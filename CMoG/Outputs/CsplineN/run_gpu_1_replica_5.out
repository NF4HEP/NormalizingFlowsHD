2023-09-26 13:23:07.541840: Importing os...
2023-09-26 13:23:07.541908: Importing sys...
2023-09-26 13:23:07.541922: Importing and initializing argparse...
Visible devices: [1]
2023-09-26 13:23:07.558874: Importing timer from timeit...
2023-09-26 13:23:07.559482: Setting env variables for tf import (only device [1] will be available)...
2023-09-26 13:23:07.559530: Importing numpy...
2023-09-26 13:23:07.719216: Importing pandas...
2023-09-26 13:23:07.919483: Importing shutil...
2023-09-26 13:23:07.919513: Importing subprocess...
2023-09-26 13:23:07.919520: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-26 13:23:09.486006: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-26 13:23:09.773005: Importing textwrap...
2023-09-26 13:23:09.773025: Importing timeit...
2023-09-26 13:23:09.773032: Importing traceback...
2023-09-26 13:23:09.773036: Importing typing...
2023-09-26 13:23:09.773043: Setting tf configs...
2023-09-26 13:23:10.037040: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-26 13:23:11.035256: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

===========
Generating train data for run 286.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_286/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_286/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_286/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_286
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.5424666   4.160326    8.947518   ...  7.3513613   3.2319064
   2.0238907 ]
 [ 1.223466    3.6614168   7.44358    ...  7.1275983   2.889486
   2.1263318 ]
 [ 2.8113456   3.2582755   7.9323573  ...  7.1118317   2.55147
   1.6773428 ]
 ...
 [ 4.8257422   5.760828    0.99088824 ...  1.1221942   6.224573
   1.3870366 ]
 [ 2.8796694   4.5901284   7.9563813  ...  7.221806    3.399475
   1.8678449 ]
 [ 4.724557    5.899211   -0.68043184 ...  1.4107605   6.846419
   1.3825576 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  2305120   
 r)                                                              
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fc560462980>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc510292e00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc510292e00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc57054a050>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc4dc559f90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc4dc55a500>, <keras.callbacks.ModelCheckpoint object at 0x7fc4dc55a650>, <keras.callbacks.EarlyStopping object at 0x7fc4dc55a860>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc4dc55a890>, <keras.callbacks.TerminateOnNaN object at 0x7fc4dc55a5c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_286/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 286/720 with hyperparameters:
timestamp = 2023-09-26 13:23:17.168012
ndims = 32
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.5424666   4.160326    8.947518    1.0939208   7.617727   -0.37824863
  9.755269    4.70193     9.9485855   6.084102    7.916266    0.31372035
  2.6670911   1.177147    2.493954    1.2494209   3.149602    6.587495
  1.3128376   6.9324527   5.4146237   2.7642589   4.1612015   1.0985487
  4.086754    9.263963    3.806251    5.6873355   2.3875675   7.3513613
  3.2319064   2.0238907 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 38: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 13:25:18.745 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1725.6587, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 121s - loss: nan - MinusLogProbMetric: 1725.6587 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 121s/epoch - 620ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 286.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_286/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_286/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_286/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_286
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.5424666   4.160326    8.947518   ...  7.3513613   3.2319064
   2.0238907 ]
 [ 1.223466    3.6614168   7.44358    ...  7.1275983   2.889486
   2.1263318 ]
 [ 2.8113456   3.2582755   7.9323573  ...  7.1118317   2.55147
   1.6773428 ]
 ...
 [ 4.8257422   5.760828    0.99088824 ...  1.1221942   6.224573
   1.3870366 ]
 [ 2.8796694   4.5901284   7.9563813  ...  7.221806    3.399475
   1.8678449 ]
 [ 4.724557    5.899211   -0.68043184 ...  1.4107605   6.846419
   1.3825576 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  2305120   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7fc9170fb3d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc916bccdf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc916bccdf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc917130040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc916878850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc916878dc0>, <keras.callbacks.ModelCheckpoint object at 0x7fc916878e80>, <keras.callbacks.EarlyStopping object at 0x7fc9168790f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc916879120>, <keras.callbacks.TerminateOnNaN object at 0x7fc916878d60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_286/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 286/720 with hyperparameters:
timestamp = 2023-09-26 13:25:27.912743
ndims = 32
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.5424666   4.160326    8.947518    1.0939208   7.617727   -0.37824863
  9.755269    4.70193     9.9485855   6.084102    7.916266    0.31372035
  2.6670911   1.177147    2.493954    1.2494209   3.149602    6.587495
  1.3128376   6.9324527   5.4146237   2.7642589   4.1612015   1.0985487
  4.086754    9.263963    3.806251    5.6873355   2.3875675   7.3513613
  3.2319064   2.0238907 ]
Epoch 1/1000
2023-09-26 13:28:26.736 
Epoch 1/1000 
	 loss: 217.8038, MinusLogProbMetric: 217.8038, val_loss: 57.3796, val_MinusLogProbMetric: 57.3796

Epoch 1: val_loss improved from inf to 57.37963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 179s - loss: 217.8038 - MinusLogProbMetric: 217.8038 - val_loss: 57.3796 - val_MinusLogProbMetric: 57.3796 - lr: 3.3333e-04 - 179s/epoch - 915ms/step
Epoch 2/1000
2023-09-26 13:29:32.997 
Epoch 2/1000 
	 loss: 61.4635, MinusLogProbMetric: 61.4635, val_loss: 42.6075, val_MinusLogProbMetric: 42.6075

Epoch 2: val_loss improved from 57.37963 to 42.60746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 61.4635 - MinusLogProbMetric: 61.4635 - val_loss: 42.6075 - val_MinusLogProbMetric: 42.6075 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 3/1000
2023-09-26 13:30:38.936 
Epoch 3/1000 
	 loss: 34.6490, MinusLogProbMetric: 34.6490, val_loss: 30.6002, val_MinusLogProbMetric: 30.6002

Epoch 3: val_loss improved from 42.60746 to 30.60021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 34.6490 - MinusLogProbMetric: 34.6490 - val_loss: 30.6002 - val_MinusLogProbMetric: 30.6002 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 4/1000
2023-09-26 13:31:45.858 
Epoch 4/1000 
	 loss: 30.1395, MinusLogProbMetric: 30.1395, val_loss: 29.1955, val_MinusLogProbMetric: 29.1955

Epoch 4: val_loss improved from 30.60021 to 29.19552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 30.1395 - MinusLogProbMetric: 30.1395 - val_loss: 29.1955 - val_MinusLogProbMetric: 29.1955 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 5/1000
2023-09-26 13:32:52.521 
Epoch 5/1000 
	 loss: 27.5565, MinusLogProbMetric: 27.5565, val_loss: 27.6509, val_MinusLogProbMetric: 27.6509

Epoch 5: val_loss improved from 29.19552 to 27.65090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 27.5565 - MinusLogProbMetric: 27.5565 - val_loss: 27.6509 - val_MinusLogProbMetric: 27.6509 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 6/1000
2023-09-26 13:33:59.842 
Epoch 6/1000 
	 loss: 26.5031, MinusLogProbMetric: 26.5031, val_loss: 25.3943, val_MinusLogProbMetric: 25.3943

Epoch 6: val_loss improved from 27.65090 to 25.39430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 26.5031 - MinusLogProbMetric: 26.5031 - val_loss: 25.3943 - val_MinusLogProbMetric: 25.3943 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 7/1000
2023-09-26 13:35:07.227 
Epoch 7/1000 
	 loss: 25.0625, MinusLogProbMetric: 25.0625, val_loss: 24.7069, val_MinusLogProbMetric: 24.7069

Epoch 7: val_loss improved from 25.39430 to 24.70692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 25.0625 - MinusLogProbMetric: 25.0625 - val_loss: 24.7069 - val_MinusLogProbMetric: 24.7069 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 8/1000
2023-09-26 13:36:14.768 
Epoch 8/1000 
	 loss: 24.0809, MinusLogProbMetric: 24.0809, val_loss: 24.7753, val_MinusLogProbMetric: 24.7753

Epoch 8: val_loss did not improve from 24.70692
196/196 - 66s - loss: 24.0809 - MinusLogProbMetric: 24.0809 - val_loss: 24.7753 - val_MinusLogProbMetric: 24.7753 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 9/1000
2023-09-26 13:37:21.032 
Epoch 9/1000 
	 loss: 23.5476, MinusLogProbMetric: 23.5476, val_loss: 24.1989, val_MinusLogProbMetric: 24.1989

Epoch 9: val_loss improved from 24.70692 to 24.19888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 23.5476 - MinusLogProbMetric: 23.5476 - val_loss: 24.1989 - val_MinusLogProbMetric: 24.1989 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 10/1000
2023-09-26 13:38:28.357 
Epoch 10/1000 
	 loss: 23.1614, MinusLogProbMetric: 23.1614, val_loss: 22.4448, val_MinusLogProbMetric: 22.4448

Epoch 10: val_loss improved from 24.19888 to 22.44485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 23.1614 - MinusLogProbMetric: 23.1614 - val_loss: 22.4448 - val_MinusLogProbMetric: 22.4448 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 11/1000
2023-09-26 13:39:36.137 
Epoch 11/1000 
	 loss: 22.6213, MinusLogProbMetric: 22.6213, val_loss: 23.2140, val_MinusLogProbMetric: 23.2140

Epoch 11: val_loss did not improve from 22.44485
196/196 - 67s - loss: 22.6213 - MinusLogProbMetric: 22.6213 - val_loss: 23.2140 - val_MinusLogProbMetric: 23.2140 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 12/1000
2023-09-26 13:40:42.665 
Epoch 12/1000 
	 loss: 22.5354, MinusLogProbMetric: 22.5354, val_loss: 22.7580, val_MinusLogProbMetric: 22.7580

Epoch 12: val_loss did not improve from 22.44485
196/196 - 67s - loss: 22.5354 - MinusLogProbMetric: 22.5354 - val_loss: 22.7580 - val_MinusLogProbMetric: 22.7580 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 13/1000
2023-09-26 13:41:48.824 
Epoch 13/1000 
	 loss: 22.0666, MinusLogProbMetric: 22.0666, val_loss: 21.5477, val_MinusLogProbMetric: 21.5477

Epoch 13: val_loss improved from 22.44485 to 21.54770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 22.0666 - MinusLogProbMetric: 22.0666 - val_loss: 21.5477 - val_MinusLogProbMetric: 21.5477 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 14/1000
2023-09-26 13:42:56.053 
Epoch 14/1000 
	 loss: 21.6612, MinusLogProbMetric: 21.6612, val_loss: 21.7531, val_MinusLogProbMetric: 21.7531

Epoch 14: val_loss did not improve from 21.54770
196/196 - 66s - loss: 21.6612 - MinusLogProbMetric: 21.6612 - val_loss: 21.7531 - val_MinusLogProbMetric: 21.7531 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 15/1000
2023-09-26 13:44:02.665 
Epoch 15/1000 
	 loss: 21.4045, MinusLogProbMetric: 21.4045, val_loss: 21.2306, val_MinusLogProbMetric: 21.2306

Epoch 15: val_loss improved from 21.54770 to 21.23060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 21.4045 - MinusLogProbMetric: 21.4045 - val_loss: 21.2306 - val_MinusLogProbMetric: 21.2306 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 16/1000
2023-09-26 13:45:10.065 
Epoch 16/1000 
	 loss: 21.1877, MinusLogProbMetric: 21.1877, val_loss: 21.7568, val_MinusLogProbMetric: 21.7568

Epoch 16: val_loss did not improve from 21.23060
196/196 - 66s - loss: 21.1877 - MinusLogProbMetric: 21.1877 - val_loss: 21.7568 - val_MinusLogProbMetric: 21.7568 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 17/1000
2023-09-26 13:46:16.476 
Epoch 17/1000 
	 loss: 20.9466, MinusLogProbMetric: 20.9466, val_loss: 21.5128, val_MinusLogProbMetric: 21.5128

Epoch 17: val_loss did not improve from 21.23060
196/196 - 66s - loss: 20.9466 - MinusLogProbMetric: 20.9466 - val_loss: 21.5128 - val_MinusLogProbMetric: 21.5128 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 18/1000
2023-09-26 13:47:23.838 
Epoch 18/1000 
	 loss: 20.8167, MinusLogProbMetric: 20.8167, val_loss: 20.9317, val_MinusLogProbMetric: 20.9317

Epoch 18: val_loss improved from 21.23060 to 20.93174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 20.8167 - MinusLogProbMetric: 20.8167 - val_loss: 20.9317 - val_MinusLogProbMetric: 20.9317 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 19/1000
2023-09-26 13:48:32.361 
Epoch 19/1000 
	 loss: 20.6053, MinusLogProbMetric: 20.6053, val_loss: 20.2149, val_MinusLogProbMetric: 20.2149

Epoch 19: val_loss improved from 20.93174 to 20.21493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 69s - loss: 20.6053 - MinusLogProbMetric: 20.6053 - val_loss: 20.2149 - val_MinusLogProbMetric: 20.2149 - lr: 3.3333e-04 - 69s/epoch - 350ms/step
Epoch 20/1000
2023-09-26 13:49:40.987 
Epoch 20/1000 
	 loss: 20.4278, MinusLogProbMetric: 20.4278, val_loss: 20.8567, val_MinusLogProbMetric: 20.8567

Epoch 20: val_loss did not improve from 20.21493
196/196 - 68s - loss: 20.4278 - MinusLogProbMetric: 20.4278 - val_loss: 20.8567 - val_MinusLogProbMetric: 20.8567 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 21/1000
2023-09-26 13:50:49.281 
Epoch 21/1000 
	 loss: 20.4080, MinusLogProbMetric: 20.4080, val_loss: 20.5346, val_MinusLogProbMetric: 20.5346

Epoch 21: val_loss did not improve from 20.21493
196/196 - 68s - loss: 20.4080 - MinusLogProbMetric: 20.4080 - val_loss: 20.5346 - val_MinusLogProbMetric: 20.5346 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 22/1000
2023-09-26 13:51:57.447 
Epoch 22/1000 
	 loss: 20.1122, MinusLogProbMetric: 20.1122, val_loss: 20.0267, val_MinusLogProbMetric: 20.0267

Epoch 22: val_loss improved from 20.21493 to 20.02674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 69s - loss: 20.1122 - MinusLogProbMetric: 20.1122 - val_loss: 20.0267 - val_MinusLogProbMetric: 20.0267 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 23/1000
2023-09-26 13:53:06.585 
Epoch 23/1000 
	 loss: 20.1057, MinusLogProbMetric: 20.1057, val_loss: 20.7796, val_MinusLogProbMetric: 20.7796

Epoch 23: val_loss did not improve from 20.02674
196/196 - 68s - loss: 20.1057 - MinusLogProbMetric: 20.1057 - val_loss: 20.7796 - val_MinusLogProbMetric: 20.7796 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 24/1000
2023-09-26 13:54:13.984 
Epoch 24/1000 
	 loss: 19.9143, MinusLogProbMetric: 19.9143, val_loss: 19.8502, val_MinusLogProbMetric: 19.8502

Epoch 24: val_loss improved from 20.02674 to 19.85018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 19.9143 - MinusLogProbMetric: 19.9143 - val_loss: 19.8502 - val_MinusLogProbMetric: 19.8502 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 25/1000
2023-09-26 13:55:21.825 
Epoch 25/1000 
	 loss: 19.8248, MinusLogProbMetric: 19.8248, val_loss: 19.8977, val_MinusLogProbMetric: 19.8977

Epoch 25: val_loss did not improve from 19.85018
196/196 - 67s - loss: 19.8248 - MinusLogProbMetric: 19.8248 - val_loss: 19.8977 - val_MinusLogProbMetric: 19.8977 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 26/1000
2023-09-26 13:56:28.728 
Epoch 26/1000 
	 loss: 19.5753, MinusLogProbMetric: 19.5753, val_loss: 19.1950, val_MinusLogProbMetric: 19.1950

Epoch 26: val_loss improved from 19.85018 to 19.19502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 19.5753 - MinusLogProbMetric: 19.5753 - val_loss: 19.1950 - val_MinusLogProbMetric: 19.1950 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 27/1000
2023-09-26 13:57:36.051 
Epoch 27/1000 
	 loss: 19.4854, MinusLogProbMetric: 19.4854, val_loss: 20.2863, val_MinusLogProbMetric: 20.2863

Epoch 27: val_loss did not improve from 19.19502
196/196 - 66s - loss: 19.4854 - MinusLogProbMetric: 19.4854 - val_loss: 20.2863 - val_MinusLogProbMetric: 20.2863 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 28/1000
2023-09-26 13:58:42.560 
Epoch 28/1000 
	 loss: 19.5004, MinusLogProbMetric: 19.5004, val_loss: 19.3197, val_MinusLogProbMetric: 19.3197

Epoch 28: val_loss did not improve from 19.19502
196/196 - 67s - loss: 19.5004 - MinusLogProbMetric: 19.5004 - val_loss: 19.3197 - val_MinusLogProbMetric: 19.3197 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 29/1000
2023-09-26 13:59:48.914 
Epoch 29/1000 
	 loss: 19.4073, MinusLogProbMetric: 19.4073, val_loss: 19.2528, val_MinusLogProbMetric: 19.2528

Epoch 29: val_loss did not improve from 19.19502
196/196 - 66s - loss: 19.4073 - MinusLogProbMetric: 19.4073 - val_loss: 19.2528 - val_MinusLogProbMetric: 19.2528 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 30/1000
2023-09-26 14:00:55.802 
Epoch 30/1000 
	 loss: 19.3339, MinusLogProbMetric: 19.3339, val_loss: 19.4234, val_MinusLogProbMetric: 19.4234

Epoch 30: val_loss did not improve from 19.19502
196/196 - 67s - loss: 19.3339 - MinusLogProbMetric: 19.3339 - val_loss: 19.4234 - val_MinusLogProbMetric: 19.4234 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 31/1000
2023-09-26 14:02:02.429 
Epoch 31/1000 
	 loss: 19.1810, MinusLogProbMetric: 19.1810, val_loss: 21.5054, val_MinusLogProbMetric: 21.5054

Epoch 31: val_loss did not improve from 19.19502
196/196 - 67s - loss: 19.1810 - MinusLogProbMetric: 19.1810 - val_loss: 21.5054 - val_MinusLogProbMetric: 21.5054 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 32/1000
2023-09-26 14:03:09.095 
Epoch 32/1000 
	 loss: 19.1859, MinusLogProbMetric: 19.1859, val_loss: 19.2025, val_MinusLogProbMetric: 19.2025

Epoch 32: val_loss did not improve from 19.19502
196/196 - 67s - loss: 19.1859 - MinusLogProbMetric: 19.1859 - val_loss: 19.2025 - val_MinusLogProbMetric: 19.2025 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 33/1000
2023-09-26 14:04:15.224 
Epoch 33/1000 
	 loss: 19.0474, MinusLogProbMetric: 19.0474, val_loss: 19.3885, val_MinusLogProbMetric: 19.3885

Epoch 33: val_loss did not improve from 19.19502
196/196 - 66s - loss: 19.0474 - MinusLogProbMetric: 19.0474 - val_loss: 19.3885 - val_MinusLogProbMetric: 19.3885 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 34/1000
2023-09-26 14:05:21.992 
Epoch 34/1000 
	 loss: 19.0103, MinusLogProbMetric: 19.0103, val_loss: 19.0661, val_MinusLogProbMetric: 19.0661

Epoch 34: val_loss improved from 19.19502 to 19.06611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 19.0103 - MinusLogProbMetric: 19.0103 - val_loss: 19.0661 - val_MinusLogProbMetric: 19.0661 - lr: 3.3333e-04 - 68s/epoch - 346ms/step
Epoch 35/1000
2023-09-26 14:06:29.135 
Epoch 35/1000 
	 loss: 18.9523, MinusLogProbMetric: 18.9523, val_loss: 18.6447, val_MinusLogProbMetric: 18.6447

Epoch 35: val_loss improved from 19.06611 to 18.64467, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 18.9523 - MinusLogProbMetric: 18.9523 - val_loss: 18.6447 - val_MinusLogProbMetric: 18.6447 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 36/1000
2023-09-26 14:07:37.313 
Epoch 36/1000 
	 loss: 18.8817, MinusLogProbMetric: 18.8817, val_loss: 18.6091, val_MinusLogProbMetric: 18.6091

Epoch 36: val_loss improved from 18.64467 to 18.60914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 18.8817 - MinusLogProbMetric: 18.8817 - val_loss: 18.6091 - val_MinusLogProbMetric: 18.6091 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 37/1000
2023-09-26 14:08:45.061 
Epoch 37/1000 
	 loss: 18.7901, MinusLogProbMetric: 18.7901, val_loss: 18.8277, val_MinusLogProbMetric: 18.8277

Epoch 37: val_loss did not improve from 18.60914
196/196 - 67s - loss: 18.7901 - MinusLogProbMetric: 18.7901 - val_loss: 18.8277 - val_MinusLogProbMetric: 18.8277 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 38/1000
2023-09-26 14:09:52.205 
Epoch 38/1000 
	 loss: 18.8165, MinusLogProbMetric: 18.8165, val_loss: 19.2368, val_MinusLogProbMetric: 19.2368

Epoch 38: val_loss did not improve from 18.60914
196/196 - 67s - loss: 18.8165 - MinusLogProbMetric: 18.8165 - val_loss: 19.2368 - val_MinusLogProbMetric: 19.2368 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 39/1000
2023-09-26 14:10:58.457 
Epoch 39/1000 
	 loss: 18.6882, MinusLogProbMetric: 18.6882, val_loss: 19.0752, val_MinusLogProbMetric: 19.0752

Epoch 39: val_loss did not improve from 18.60914
196/196 - 66s - loss: 18.6882 - MinusLogProbMetric: 18.6882 - val_loss: 19.0752 - val_MinusLogProbMetric: 19.0752 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 40/1000
2023-09-26 14:12:04.413 
Epoch 40/1000 
	 loss: 18.7598, MinusLogProbMetric: 18.7598, val_loss: 19.9793, val_MinusLogProbMetric: 19.9793

Epoch 40: val_loss did not improve from 18.60914
196/196 - 66s - loss: 18.7598 - MinusLogProbMetric: 18.7598 - val_loss: 19.9793 - val_MinusLogProbMetric: 19.9793 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 41/1000
2023-09-26 14:13:11.462 
Epoch 41/1000 
	 loss: 18.6989, MinusLogProbMetric: 18.6989, val_loss: 18.8471, val_MinusLogProbMetric: 18.8471

Epoch 41: val_loss did not improve from 18.60914
196/196 - 67s - loss: 18.6989 - MinusLogProbMetric: 18.6989 - val_loss: 18.8471 - val_MinusLogProbMetric: 18.8471 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 42/1000
2023-09-26 14:14:18.297 
Epoch 42/1000 
	 loss: 18.6086, MinusLogProbMetric: 18.6086, val_loss: 19.3909, val_MinusLogProbMetric: 19.3909

Epoch 42: val_loss did not improve from 18.60914
196/196 - 67s - loss: 18.6086 - MinusLogProbMetric: 18.6086 - val_loss: 19.3909 - val_MinusLogProbMetric: 19.3909 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 43/1000
2023-09-26 14:15:25.484 
Epoch 43/1000 
	 loss: 18.6897, MinusLogProbMetric: 18.6897, val_loss: 18.7432, val_MinusLogProbMetric: 18.7432

Epoch 43: val_loss did not improve from 18.60914
196/196 - 67s - loss: 18.6897 - MinusLogProbMetric: 18.6897 - val_loss: 18.7432 - val_MinusLogProbMetric: 18.7432 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 44/1000
2023-09-26 14:16:32.715 
Epoch 44/1000 
	 loss: 18.5734, MinusLogProbMetric: 18.5734, val_loss: 19.0041, val_MinusLogProbMetric: 19.0041

Epoch 44: val_loss did not improve from 18.60914
196/196 - 67s - loss: 18.5734 - MinusLogProbMetric: 18.5734 - val_loss: 19.0041 - val_MinusLogProbMetric: 19.0041 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 45/1000
2023-09-26 14:17:39.921 
Epoch 45/1000 
	 loss: 18.5574, MinusLogProbMetric: 18.5574, val_loss: 20.1005, val_MinusLogProbMetric: 20.1005

Epoch 45: val_loss did not improve from 18.60914
196/196 - 67s - loss: 18.5574 - MinusLogProbMetric: 18.5574 - val_loss: 20.1005 - val_MinusLogProbMetric: 20.1005 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 46/1000
2023-09-26 14:18:45.980 
Epoch 46/1000 
	 loss: 18.5788, MinusLogProbMetric: 18.5788, val_loss: 18.9207, val_MinusLogProbMetric: 18.9207

Epoch 46: val_loss did not improve from 18.60914
196/196 - 66s - loss: 18.5788 - MinusLogProbMetric: 18.5788 - val_loss: 18.9207 - val_MinusLogProbMetric: 18.9207 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 47/1000
2023-09-26 14:19:52.208 
Epoch 47/1000 
	 loss: 18.4781, MinusLogProbMetric: 18.4781, val_loss: 18.8121, val_MinusLogProbMetric: 18.8121

Epoch 47: val_loss did not improve from 18.60914
196/196 - 66s - loss: 18.4781 - MinusLogProbMetric: 18.4781 - val_loss: 18.8121 - val_MinusLogProbMetric: 18.8121 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 48/1000
2023-09-26 14:20:56.668 
Epoch 48/1000 
	 loss: 18.5004, MinusLogProbMetric: 18.5004, val_loss: 19.0315, val_MinusLogProbMetric: 19.0315

Epoch 48: val_loss did not improve from 18.60914
196/196 - 64s - loss: 18.5004 - MinusLogProbMetric: 18.5004 - val_loss: 19.0315 - val_MinusLogProbMetric: 19.0315 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 49/1000
2023-09-26 14:22:02.353 
Epoch 49/1000 
	 loss: 18.4757, MinusLogProbMetric: 18.4757, val_loss: 19.4852, val_MinusLogProbMetric: 19.4852

Epoch 49: val_loss did not improve from 18.60914
196/196 - 66s - loss: 18.4757 - MinusLogProbMetric: 18.4757 - val_loss: 19.4852 - val_MinusLogProbMetric: 19.4852 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 50/1000
2023-09-26 14:23:08.584 
Epoch 50/1000 
	 loss: 18.4371, MinusLogProbMetric: 18.4371, val_loss: 18.4479, val_MinusLogProbMetric: 18.4479

Epoch 50: val_loss improved from 18.60914 to 18.44793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 18.4371 - MinusLogProbMetric: 18.4371 - val_loss: 18.4479 - val_MinusLogProbMetric: 18.4479 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 51/1000
2023-09-26 14:24:16.603 
Epoch 51/1000 
	 loss: 18.2920, MinusLogProbMetric: 18.2920, val_loss: 19.3515, val_MinusLogProbMetric: 19.3515

Epoch 51: val_loss did not improve from 18.44793
196/196 - 67s - loss: 18.2920 - MinusLogProbMetric: 18.2920 - val_loss: 19.3515 - val_MinusLogProbMetric: 19.3515 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 52/1000
2023-09-26 14:25:22.509 
Epoch 52/1000 
	 loss: 18.3596, MinusLogProbMetric: 18.3596, val_loss: 18.3804, val_MinusLogProbMetric: 18.3804

Epoch 52: val_loss improved from 18.44793 to 18.38044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 18.3596 - MinusLogProbMetric: 18.3596 - val_loss: 18.3804 - val_MinusLogProbMetric: 18.3804 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 53/1000
2023-09-26 14:26:29.678 
Epoch 53/1000 
	 loss: 18.3127, MinusLogProbMetric: 18.3127, val_loss: 18.1729, val_MinusLogProbMetric: 18.1729

Epoch 53: val_loss improved from 18.38044 to 18.17290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 18.3127 - MinusLogProbMetric: 18.3127 - val_loss: 18.1729 - val_MinusLogProbMetric: 18.1729 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 54/1000
2023-09-26 14:27:37.155 
Epoch 54/1000 
	 loss: 18.3379, MinusLogProbMetric: 18.3379, val_loss: 18.3830, val_MinusLogProbMetric: 18.3830

Epoch 54: val_loss did not improve from 18.17290
196/196 - 66s - loss: 18.3379 - MinusLogProbMetric: 18.3379 - val_loss: 18.3830 - val_MinusLogProbMetric: 18.3830 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 55/1000
2023-09-26 14:28:41.926 
Epoch 55/1000 
	 loss: 18.2977, MinusLogProbMetric: 18.2977, val_loss: 18.1898, val_MinusLogProbMetric: 18.1898

Epoch 55: val_loss did not improve from 18.17290
196/196 - 65s - loss: 18.2977 - MinusLogProbMetric: 18.2977 - val_loss: 18.1898 - val_MinusLogProbMetric: 18.1898 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 56/1000
2023-09-26 14:29:49.286 
Epoch 56/1000 
	 loss: 18.2159, MinusLogProbMetric: 18.2159, val_loss: 18.5358, val_MinusLogProbMetric: 18.5358

Epoch 56: val_loss did not improve from 18.17290
196/196 - 67s - loss: 18.2159 - MinusLogProbMetric: 18.2159 - val_loss: 18.5358 - val_MinusLogProbMetric: 18.5358 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 57/1000
2023-09-26 14:30:56.396 
Epoch 57/1000 
	 loss: 18.2330, MinusLogProbMetric: 18.2330, val_loss: 18.6931, val_MinusLogProbMetric: 18.6931

Epoch 57: val_loss did not improve from 18.17290
196/196 - 67s - loss: 18.2330 - MinusLogProbMetric: 18.2330 - val_loss: 18.6931 - val_MinusLogProbMetric: 18.6931 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 58/1000
2023-09-26 14:32:04.839 
Epoch 58/1000 
	 loss: 18.1769, MinusLogProbMetric: 18.1769, val_loss: 18.2861, val_MinusLogProbMetric: 18.2861

Epoch 58: val_loss did not improve from 18.17290
196/196 - 68s - loss: 18.1769 - MinusLogProbMetric: 18.1769 - val_loss: 18.2861 - val_MinusLogProbMetric: 18.2861 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 59/1000
2023-09-26 14:33:12.018 
Epoch 59/1000 
	 loss: 18.1393, MinusLogProbMetric: 18.1393, val_loss: 18.4463, val_MinusLogProbMetric: 18.4463

Epoch 59: val_loss did not improve from 18.17290
196/196 - 67s - loss: 18.1393 - MinusLogProbMetric: 18.1393 - val_loss: 18.4463 - val_MinusLogProbMetric: 18.4463 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 60/1000
2023-09-26 14:34:19.055 
Epoch 60/1000 
	 loss: 18.0462, MinusLogProbMetric: 18.0462, val_loss: 18.4352, val_MinusLogProbMetric: 18.4352

Epoch 60: val_loss did not improve from 18.17290
196/196 - 67s - loss: 18.0462 - MinusLogProbMetric: 18.0462 - val_loss: 18.4352 - val_MinusLogProbMetric: 18.4352 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 61/1000
2023-09-26 14:35:25.680 
Epoch 61/1000 
	 loss: 18.0203, MinusLogProbMetric: 18.0203, val_loss: 18.3449, val_MinusLogProbMetric: 18.3449

Epoch 61: val_loss did not improve from 18.17290
196/196 - 67s - loss: 18.0203 - MinusLogProbMetric: 18.0203 - val_loss: 18.3449 - val_MinusLogProbMetric: 18.3449 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 62/1000
2023-09-26 14:36:33.115 
Epoch 62/1000 
	 loss: 18.0535, MinusLogProbMetric: 18.0535, val_loss: 18.4523, val_MinusLogProbMetric: 18.4523

Epoch 62: val_loss did not improve from 18.17290
196/196 - 67s - loss: 18.0535 - MinusLogProbMetric: 18.0535 - val_loss: 18.4523 - val_MinusLogProbMetric: 18.4523 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 63/1000
2023-09-26 14:37:40.662 
Epoch 63/1000 
	 loss: 18.0839, MinusLogProbMetric: 18.0839, val_loss: 18.4705, val_MinusLogProbMetric: 18.4705

Epoch 63: val_loss did not improve from 18.17290
196/196 - 68s - loss: 18.0839 - MinusLogProbMetric: 18.0839 - val_loss: 18.4705 - val_MinusLogProbMetric: 18.4705 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 64/1000
2023-09-26 14:38:47.533 
Epoch 64/1000 
	 loss: 18.0122, MinusLogProbMetric: 18.0122, val_loss: 18.0330, val_MinusLogProbMetric: 18.0330

Epoch 64: val_loss improved from 18.17290 to 18.03302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 18.0122 - MinusLogProbMetric: 18.0122 - val_loss: 18.0330 - val_MinusLogProbMetric: 18.0330 - lr: 3.3333e-04 - 68s/epoch - 346ms/step
Epoch 65/1000
2023-09-26 14:39:55.115 
Epoch 65/1000 
	 loss: 18.0073, MinusLogProbMetric: 18.0073, val_loss: 17.8758, val_MinusLogProbMetric: 17.8758

Epoch 65: val_loss improved from 18.03302 to 17.87584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 18.0073 - MinusLogProbMetric: 18.0073 - val_loss: 17.8758 - val_MinusLogProbMetric: 17.8758 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 66/1000
2023-09-26 14:41:02.468 
Epoch 66/1000 
	 loss: 17.9043, MinusLogProbMetric: 17.9043, val_loss: 18.2495, val_MinusLogProbMetric: 18.2495

Epoch 66: val_loss did not improve from 17.87584
196/196 - 66s - loss: 17.9043 - MinusLogProbMetric: 17.9043 - val_loss: 18.2495 - val_MinusLogProbMetric: 18.2495 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 67/1000
2023-09-26 14:42:08.725 
Epoch 67/1000 
	 loss: 17.9813, MinusLogProbMetric: 17.9813, val_loss: 19.3193, val_MinusLogProbMetric: 19.3193

Epoch 67: val_loss did not improve from 17.87584
196/196 - 66s - loss: 17.9813 - MinusLogProbMetric: 17.9813 - val_loss: 19.3193 - val_MinusLogProbMetric: 19.3193 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 68/1000
2023-09-26 14:43:15.405 
Epoch 68/1000 
	 loss: 17.9989, MinusLogProbMetric: 17.9989, val_loss: 18.1752, val_MinusLogProbMetric: 18.1752

Epoch 68: val_loss did not improve from 17.87584
196/196 - 67s - loss: 17.9989 - MinusLogProbMetric: 17.9989 - val_loss: 18.1752 - val_MinusLogProbMetric: 18.1752 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 69/1000
2023-09-26 14:44:22.089 
Epoch 69/1000 
	 loss: 17.9414, MinusLogProbMetric: 17.9414, val_loss: 18.1870, val_MinusLogProbMetric: 18.1870

Epoch 69: val_loss did not improve from 17.87584
196/196 - 67s - loss: 17.9414 - MinusLogProbMetric: 17.9414 - val_loss: 18.1870 - val_MinusLogProbMetric: 18.1870 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 70/1000
2023-09-26 14:45:28.841 
Epoch 70/1000 
	 loss: 17.9105, MinusLogProbMetric: 17.9105, val_loss: 18.2851, val_MinusLogProbMetric: 18.2851

Epoch 70: val_loss did not improve from 17.87584
196/196 - 67s - loss: 17.9105 - MinusLogProbMetric: 17.9105 - val_loss: 18.2851 - val_MinusLogProbMetric: 18.2851 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 71/1000
2023-09-26 14:46:35.618 
Epoch 71/1000 
	 loss: 17.8668, MinusLogProbMetric: 17.8668, val_loss: 18.2314, val_MinusLogProbMetric: 18.2314

Epoch 71: val_loss did not improve from 17.87584
196/196 - 67s - loss: 17.8668 - MinusLogProbMetric: 17.8668 - val_loss: 18.2314 - val_MinusLogProbMetric: 18.2314 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 72/1000
2023-09-26 14:47:42.339 
Epoch 72/1000 
	 loss: 17.9284, MinusLogProbMetric: 17.9284, val_loss: 18.5070, val_MinusLogProbMetric: 18.5070

Epoch 72: val_loss did not improve from 17.87584
196/196 - 67s - loss: 17.9284 - MinusLogProbMetric: 17.9284 - val_loss: 18.5070 - val_MinusLogProbMetric: 18.5070 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 73/1000
2023-09-26 14:48:49.178 
Epoch 73/1000 
	 loss: 17.8467, MinusLogProbMetric: 17.8467, val_loss: 18.1857, val_MinusLogProbMetric: 18.1857

Epoch 73: val_loss did not improve from 17.87584
196/196 - 67s - loss: 17.8467 - MinusLogProbMetric: 17.8467 - val_loss: 18.1857 - val_MinusLogProbMetric: 18.1857 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 74/1000
2023-09-26 14:49:55.177 
Epoch 74/1000 
	 loss: 17.7757, MinusLogProbMetric: 17.7757, val_loss: 18.1547, val_MinusLogProbMetric: 18.1547

Epoch 74: val_loss did not improve from 17.87584
196/196 - 66s - loss: 17.7757 - MinusLogProbMetric: 17.7757 - val_loss: 18.1547 - val_MinusLogProbMetric: 18.1547 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 75/1000
2023-09-26 14:51:02.083 
Epoch 75/1000 
	 loss: 17.9234, MinusLogProbMetric: 17.9234, val_loss: 17.6828, val_MinusLogProbMetric: 17.6828

Epoch 75: val_loss improved from 17.87584 to 17.68284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 17.9234 - MinusLogProbMetric: 17.9234 - val_loss: 17.6828 - val_MinusLogProbMetric: 17.6828 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 76/1000
2023-09-26 14:52:09.490 
Epoch 76/1000 
	 loss: 17.8290, MinusLogProbMetric: 17.8290, val_loss: 18.2237, val_MinusLogProbMetric: 18.2237

Epoch 76: val_loss did not improve from 17.68284
196/196 - 66s - loss: 17.8290 - MinusLogProbMetric: 17.8290 - val_loss: 18.2237 - val_MinusLogProbMetric: 18.2237 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 77/1000
2023-09-26 14:53:16.725 
Epoch 77/1000 
	 loss: 17.8107, MinusLogProbMetric: 17.8107, val_loss: 17.9146, val_MinusLogProbMetric: 17.9146

Epoch 77: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.8107 - MinusLogProbMetric: 17.8107 - val_loss: 17.9146 - val_MinusLogProbMetric: 17.9146 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 78/1000
2023-09-26 14:54:23.692 
Epoch 78/1000 
	 loss: 17.7687, MinusLogProbMetric: 17.7687, val_loss: 18.3201, val_MinusLogProbMetric: 18.3201

Epoch 78: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7687 - MinusLogProbMetric: 17.7687 - val_loss: 18.3201 - val_MinusLogProbMetric: 18.3201 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 79/1000
2023-09-26 14:55:30.976 
Epoch 79/1000 
	 loss: 17.7981, MinusLogProbMetric: 17.7981, val_loss: 17.7630, val_MinusLogProbMetric: 17.7630

Epoch 79: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7981 - MinusLogProbMetric: 17.7981 - val_loss: 17.7630 - val_MinusLogProbMetric: 17.7630 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 80/1000
2023-09-26 14:56:38.154 
Epoch 80/1000 
	 loss: 17.7430, MinusLogProbMetric: 17.7430, val_loss: 17.9417, val_MinusLogProbMetric: 17.9417

Epoch 80: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7430 - MinusLogProbMetric: 17.7430 - val_loss: 17.9417 - val_MinusLogProbMetric: 17.9417 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 81/1000
2023-09-26 14:57:46.277 
Epoch 81/1000 
	 loss: 17.7852, MinusLogProbMetric: 17.7852, val_loss: 18.0190, val_MinusLogProbMetric: 18.0190

Epoch 81: val_loss did not improve from 17.68284
196/196 - 68s - loss: 17.7852 - MinusLogProbMetric: 17.7852 - val_loss: 18.0190 - val_MinusLogProbMetric: 18.0190 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 82/1000
2023-09-26 14:58:53.823 
Epoch 82/1000 
	 loss: 17.7822, MinusLogProbMetric: 17.7822, val_loss: 17.9078, val_MinusLogProbMetric: 17.9078

Epoch 82: val_loss did not improve from 17.68284
196/196 - 68s - loss: 17.7822 - MinusLogProbMetric: 17.7822 - val_loss: 17.9078 - val_MinusLogProbMetric: 17.9078 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 83/1000
2023-09-26 15:00:01.819 
Epoch 83/1000 
	 loss: 17.7981, MinusLogProbMetric: 17.7981, val_loss: 17.9072, val_MinusLogProbMetric: 17.9072

Epoch 83: val_loss did not improve from 17.68284
196/196 - 68s - loss: 17.7981 - MinusLogProbMetric: 17.7981 - val_loss: 17.9072 - val_MinusLogProbMetric: 17.9072 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 84/1000
2023-09-26 15:01:09.074 
Epoch 84/1000 
	 loss: 17.7675, MinusLogProbMetric: 17.7675, val_loss: 17.8534, val_MinusLogProbMetric: 17.8534

Epoch 84: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7675 - MinusLogProbMetric: 17.7675 - val_loss: 17.8534 - val_MinusLogProbMetric: 17.8534 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 85/1000
2023-09-26 15:02:16.023 
Epoch 85/1000 
	 loss: 17.7287, MinusLogProbMetric: 17.7287, val_loss: 18.4107, val_MinusLogProbMetric: 18.4107

Epoch 85: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7287 - MinusLogProbMetric: 17.7287 - val_loss: 18.4107 - val_MinusLogProbMetric: 18.4107 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 86/1000
2023-09-26 15:03:22.491 
Epoch 86/1000 
	 loss: 17.7154, MinusLogProbMetric: 17.7154, val_loss: 17.9064, val_MinusLogProbMetric: 17.9064

Epoch 86: val_loss did not improve from 17.68284
196/196 - 66s - loss: 17.7154 - MinusLogProbMetric: 17.7154 - val_loss: 17.9064 - val_MinusLogProbMetric: 17.9064 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 87/1000
2023-09-26 15:04:29.009 
Epoch 87/1000 
	 loss: 17.7507, MinusLogProbMetric: 17.7507, val_loss: 18.1752, val_MinusLogProbMetric: 18.1752

Epoch 87: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7507 - MinusLogProbMetric: 17.7507 - val_loss: 18.1752 - val_MinusLogProbMetric: 18.1752 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 88/1000
2023-09-26 15:05:35.616 
Epoch 88/1000 
	 loss: 17.7123, MinusLogProbMetric: 17.7123, val_loss: 17.8074, val_MinusLogProbMetric: 17.8074

Epoch 88: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7123 - MinusLogProbMetric: 17.7123 - val_loss: 17.8074 - val_MinusLogProbMetric: 17.8074 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 89/1000
2023-09-26 15:06:42.963 
Epoch 89/1000 
	 loss: 17.7120, MinusLogProbMetric: 17.7120, val_loss: 18.3968, val_MinusLogProbMetric: 18.3968

Epoch 89: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7120 - MinusLogProbMetric: 17.7120 - val_loss: 18.3968 - val_MinusLogProbMetric: 18.3968 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 90/1000
2023-09-26 15:07:49.876 
Epoch 90/1000 
	 loss: 17.7091, MinusLogProbMetric: 17.7091, val_loss: 17.8837, val_MinusLogProbMetric: 17.8837

Epoch 90: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7091 - MinusLogProbMetric: 17.7091 - val_loss: 17.8837 - val_MinusLogProbMetric: 17.8837 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 91/1000
2023-09-26 15:08:56.498 
Epoch 91/1000 
	 loss: 17.6745, MinusLogProbMetric: 17.6745, val_loss: 17.9710, val_MinusLogProbMetric: 17.9710

Epoch 91: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.6745 - MinusLogProbMetric: 17.6745 - val_loss: 17.9710 - val_MinusLogProbMetric: 17.9710 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 92/1000
2023-09-26 15:10:03.359 
Epoch 92/1000 
	 loss: 17.7028, MinusLogProbMetric: 17.7028, val_loss: 17.9552, val_MinusLogProbMetric: 17.9552

Epoch 92: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.7028 - MinusLogProbMetric: 17.7028 - val_loss: 17.9552 - val_MinusLogProbMetric: 17.9552 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 93/1000
2023-09-26 15:11:10.143 
Epoch 93/1000 
	 loss: 17.6379, MinusLogProbMetric: 17.6379, val_loss: 18.0817, val_MinusLogProbMetric: 18.0817

Epoch 93: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.6379 - MinusLogProbMetric: 17.6379 - val_loss: 18.0817 - val_MinusLogProbMetric: 18.0817 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 94/1000
2023-09-26 15:12:16.943 
Epoch 94/1000 
	 loss: 17.6561, MinusLogProbMetric: 17.6561, val_loss: 18.2758, val_MinusLogProbMetric: 18.2758

Epoch 94: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.6561 - MinusLogProbMetric: 17.6561 - val_loss: 18.2758 - val_MinusLogProbMetric: 18.2758 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 95/1000
2023-09-26 15:13:23.343 
Epoch 95/1000 
	 loss: 17.5687, MinusLogProbMetric: 17.5687, val_loss: 17.7074, val_MinusLogProbMetric: 17.7074

Epoch 95: val_loss did not improve from 17.68284
196/196 - 66s - loss: 17.5687 - MinusLogProbMetric: 17.5687 - val_loss: 17.7074 - val_MinusLogProbMetric: 17.7074 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 96/1000
2023-09-26 15:14:30.483 
Epoch 96/1000 
	 loss: 17.5981, MinusLogProbMetric: 17.5981, val_loss: 17.7971, val_MinusLogProbMetric: 17.7971

Epoch 96: val_loss did not improve from 17.68284
196/196 - 67s - loss: 17.5981 - MinusLogProbMetric: 17.5981 - val_loss: 17.7971 - val_MinusLogProbMetric: 17.7971 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 97/1000
2023-09-26 15:15:37.651 
Epoch 97/1000 
	 loss: 17.6481, MinusLogProbMetric: 17.6481, val_loss: 17.6624, val_MinusLogProbMetric: 17.6624

Epoch 97: val_loss improved from 17.68284 to 17.66245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 17.6481 - MinusLogProbMetric: 17.6481 - val_loss: 17.6624 - val_MinusLogProbMetric: 17.6624 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 98/1000
2023-09-26 15:16:46.181 
Epoch 98/1000 
	 loss: 17.6012, MinusLogProbMetric: 17.6012, val_loss: 18.0876, val_MinusLogProbMetric: 18.0876

Epoch 98: val_loss did not improve from 17.66245
196/196 - 67s - loss: 17.6012 - MinusLogProbMetric: 17.6012 - val_loss: 18.0876 - val_MinusLogProbMetric: 18.0876 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 99/1000
2023-09-26 15:17:52.683 
Epoch 99/1000 
	 loss: 17.5942, MinusLogProbMetric: 17.5942, val_loss: 17.8926, val_MinusLogProbMetric: 17.8926

Epoch 99: val_loss did not improve from 17.66245
196/196 - 66s - loss: 17.5942 - MinusLogProbMetric: 17.5942 - val_loss: 17.8926 - val_MinusLogProbMetric: 17.8926 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 100/1000
2023-09-26 15:18:59.838 
Epoch 100/1000 
	 loss: 17.5976, MinusLogProbMetric: 17.5976, val_loss: 17.8910, val_MinusLogProbMetric: 17.8910

Epoch 100: val_loss did not improve from 17.66245
196/196 - 67s - loss: 17.5976 - MinusLogProbMetric: 17.5976 - val_loss: 17.8910 - val_MinusLogProbMetric: 17.8910 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 101/1000
2023-09-26 15:20:06.529 
Epoch 101/1000 
	 loss: 17.5398, MinusLogProbMetric: 17.5398, val_loss: 17.7490, val_MinusLogProbMetric: 17.7490

Epoch 101: val_loss did not improve from 17.66245
196/196 - 67s - loss: 17.5398 - MinusLogProbMetric: 17.5398 - val_loss: 17.7490 - val_MinusLogProbMetric: 17.7490 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 102/1000
2023-09-26 15:21:12.823 
Epoch 102/1000 
	 loss: 17.5264, MinusLogProbMetric: 17.5264, val_loss: 17.7252, val_MinusLogProbMetric: 17.7252

Epoch 102: val_loss did not improve from 17.66245
196/196 - 66s - loss: 17.5264 - MinusLogProbMetric: 17.5264 - val_loss: 17.7252 - val_MinusLogProbMetric: 17.7252 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 103/1000
2023-09-26 15:22:19.823 
Epoch 103/1000 
	 loss: 17.5945, MinusLogProbMetric: 17.5945, val_loss: 17.8109, val_MinusLogProbMetric: 17.8109

Epoch 103: val_loss did not improve from 17.66245
196/196 - 67s - loss: 17.5945 - MinusLogProbMetric: 17.5945 - val_loss: 17.8109 - val_MinusLogProbMetric: 17.8109 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 104/1000
2023-09-26 15:23:27.004 
Epoch 104/1000 
	 loss: 17.5665, MinusLogProbMetric: 17.5665, val_loss: 17.5727, val_MinusLogProbMetric: 17.5727

Epoch 104: val_loss improved from 17.66245 to 17.57273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 68s - loss: 17.5665 - MinusLogProbMetric: 17.5665 - val_loss: 17.5727 - val_MinusLogProbMetric: 17.5727 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 105/1000
2023-09-26 15:24:34.793 
Epoch 105/1000 
	 loss: 17.5711, MinusLogProbMetric: 17.5711, val_loss: 17.9282, val_MinusLogProbMetric: 17.9282

Epoch 105: val_loss did not improve from 17.57273
196/196 - 67s - loss: 17.5711 - MinusLogProbMetric: 17.5711 - val_loss: 17.9282 - val_MinusLogProbMetric: 17.9282 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 106/1000
2023-09-26 15:25:39.679 
Epoch 106/1000 
	 loss: 17.5734, MinusLogProbMetric: 17.5734, val_loss: 17.9504, val_MinusLogProbMetric: 17.9504

Epoch 106: val_loss did not improve from 17.57273
196/196 - 65s - loss: 17.5734 - MinusLogProbMetric: 17.5734 - val_loss: 17.9504 - val_MinusLogProbMetric: 17.9504 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 107/1000
2023-09-26 15:26:41.209 
Epoch 107/1000 
	 loss: 17.4649, MinusLogProbMetric: 17.4649, val_loss: 18.0080, val_MinusLogProbMetric: 18.0080

Epoch 107: val_loss did not improve from 17.57273
196/196 - 62s - loss: 17.4649 - MinusLogProbMetric: 17.4649 - val_loss: 18.0080 - val_MinusLogProbMetric: 18.0080 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 108/1000
2023-09-26 15:27:40.807 
Epoch 108/1000 
	 loss: 17.5152, MinusLogProbMetric: 17.5152, val_loss: 17.9894, val_MinusLogProbMetric: 17.9894

Epoch 108: val_loss did not improve from 17.57273
196/196 - 60s - loss: 17.5152 - MinusLogProbMetric: 17.5152 - val_loss: 17.9894 - val_MinusLogProbMetric: 17.9894 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 109/1000
2023-09-26 15:28:44.387 
Epoch 109/1000 
	 loss: 17.5447, MinusLogProbMetric: 17.5447, val_loss: 17.8098, val_MinusLogProbMetric: 17.8098

Epoch 109: val_loss did not improve from 17.57273
196/196 - 64s - loss: 17.5447 - MinusLogProbMetric: 17.5447 - val_loss: 17.8098 - val_MinusLogProbMetric: 17.8098 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 110/1000
2023-09-26 15:29:41.128 
Epoch 110/1000 
	 loss: 17.5262, MinusLogProbMetric: 17.5262, val_loss: 17.8697, val_MinusLogProbMetric: 17.8697

Epoch 110: val_loss did not improve from 17.57273
196/196 - 57s - loss: 17.5262 - MinusLogProbMetric: 17.5262 - val_loss: 17.8697 - val_MinusLogProbMetric: 17.8697 - lr: 3.3333e-04 - 57s/epoch - 289ms/step
Epoch 111/1000
2023-09-26 15:30:37.502 
Epoch 111/1000 
	 loss: 17.5052, MinusLogProbMetric: 17.5052, val_loss: 17.7362, val_MinusLogProbMetric: 17.7362

Epoch 111: val_loss did not improve from 17.57273
196/196 - 56s - loss: 17.5052 - MinusLogProbMetric: 17.5052 - val_loss: 17.7362 - val_MinusLogProbMetric: 17.7362 - lr: 3.3333e-04 - 56s/epoch - 288ms/step
Epoch 112/1000
2023-09-26 15:31:37.750 
Epoch 112/1000 
	 loss: 17.5309, MinusLogProbMetric: 17.5309, val_loss: 17.8824, val_MinusLogProbMetric: 17.8824

Epoch 112: val_loss did not improve from 17.57273
196/196 - 60s - loss: 17.5309 - MinusLogProbMetric: 17.5309 - val_loss: 17.8824 - val_MinusLogProbMetric: 17.8824 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 113/1000
2023-09-26 15:32:41.100 
Epoch 113/1000 
	 loss: 17.4921, MinusLogProbMetric: 17.4921, val_loss: 18.0644, val_MinusLogProbMetric: 18.0644

Epoch 113: val_loss did not improve from 17.57273
196/196 - 63s - loss: 17.4921 - MinusLogProbMetric: 17.4921 - val_loss: 18.0644 - val_MinusLogProbMetric: 18.0644 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 114/1000
2023-09-26 15:33:42.944 
Epoch 114/1000 
	 loss: 17.5070, MinusLogProbMetric: 17.5070, val_loss: 17.8457, val_MinusLogProbMetric: 17.8457

Epoch 114: val_loss did not improve from 17.57273
196/196 - 62s - loss: 17.5070 - MinusLogProbMetric: 17.5070 - val_loss: 17.8457 - val_MinusLogProbMetric: 17.8457 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 115/1000
2023-09-26 15:34:48.338 
Epoch 115/1000 
	 loss: 17.4480, MinusLogProbMetric: 17.4480, val_loss: 17.8656, val_MinusLogProbMetric: 17.8656

Epoch 115: val_loss did not improve from 17.57273
196/196 - 65s - loss: 17.4480 - MinusLogProbMetric: 17.4480 - val_loss: 17.8656 - val_MinusLogProbMetric: 17.8656 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 116/1000
2023-09-26 15:35:54.609 
Epoch 116/1000 
	 loss: 17.4759, MinusLogProbMetric: 17.4759, val_loss: 17.6746, val_MinusLogProbMetric: 17.6746

Epoch 116: val_loss did not improve from 17.57273
196/196 - 66s - loss: 17.4759 - MinusLogProbMetric: 17.4759 - val_loss: 17.6746 - val_MinusLogProbMetric: 17.6746 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 117/1000
2023-09-26 15:37:01.039 
Epoch 117/1000 
	 loss: 17.5047, MinusLogProbMetric: 17.5047, val_loss: 17.3940, val_MinusLogProbMetric: 17.3940

Epoch 117: val_loss improved from 17.57273 to 17.39399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 67s - loss: 17.5047 - MinusLogProbMetric: 17.5047 - val_loss: 17.3940 - val_MinusLogProbMetric: 17.3940 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 118/1000
2023-09-26 15:38:08.605 
Epoch 118/1000 
	 loss: 17.4669, MinusLogProbMetric: 17.4669, val_loss: 18.1553, val_MinusLogProbMetric: 18.1553

Epoch 118: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.4669 - MinusLogProbMetric: 17.4669 - val_loss: 18.1553 - val_MinusLogProbMetric: 18.1553 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 119/1000
2023-09-26 15:39:15.488 
Epoch 119/1000 
	 loss: 17.4986, MinusLogProbMetric: 17.4986, val_loss: 17.8181, val_MinusLogProbMetric: 17.8181

Epoch 119: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.4986 - MinusLogProbMetric: 17.4986 - val_loss: 17.8181 - val_MinusLogProbMetric: 17.8181 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 120/1000
2023-09-26 15:40:22.267 
Epoch 120/1000 
	 loss: 17.4500, MinusLogProbMetric: 17.4500, val_loss: 17.6500, val_MinusLogProbMetric: 17.6500

Epoch 120: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.4500 - MinusLogProbMetric: 17.4500 - val_loss: 17.6500 - val_MinusLogProbMetric: 17.6500 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 121/1000
2023-09-26 15:41:28.405 
Epoch 121/1000 
	 loss: 17.3886, MinusLogProbMetric: 17.3886, val_loss: 18.1070, val_MinusLogProbMetric: 18.1070

Epoch 121: val_loss did not improve from 17.39399
196/196 - 66s - loss: 17.3886 - MinusLogProbMetric: 17.3886 - val_loss: 18.1070 - val_MinusLogProbMetric: 18.1070 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 122/1000
2023-09-26 15:42:34.459 
Epoch 122/1000 
	 loss: 17.4674, MinusLogProbMetric: 17.4674, val_loss: 17.7161, val_MinusLogProbMetric: 17.7161

Epoch 122: val_loss did not improve from 17.39399
196/196 - 66s - loss: 17.4674 - MinusLogProbMetric: 17.4674 - val_loss: 17.7161 - val_MinusLogProbMetric: 17.7161 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 123/1000
2023-09-26 15:43:40.439 
Epoch 123/1000 
	 loss: 17.4372, MinusLogProbMetric: 17.4372, val_loss: 17.7785, val_MinusLogProbMetric: 17.7785

Epoch 123: val_loss did not improve from 17.39399
196/196 - 66s - loss: 17.4372 - MinusLogProbMetric: 17.4372 - val_loss: 17.7785 - val_MinusLogProbMetric: 17.7785 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 124/1000
2023-09-26 15:44:46.775 
Epoch 124/1000 
	 loss: 17.4094, MinusLogProbMetric: 17.4094, val_loss: 17.8917, val_MinusLogProbMetric: 17.8917

Epoch 124: val_loss did not improve from 17.39399
196/196 - 66s - loss: 17.4094 - MinusLogProbMetric: 17.4094 - val_loss: 17.8917 - val_MinusLogProbMetric: 17.8917 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 125/1000
2023-09-26 15:45:52.923 
Epoch 125/1000 
	 loss: 17.4257, MinusLogProbMetric: 17.4257, val_loss: 17.6771, val_MinusLogProbMetric: 17.6771

Epoch 125: val_loss did not improve from 17.39399
196/196 - 66s - loss: 17.4257 - MinusLogProbMetric: 17.4257 - val_loss: 17.6771 - val_MinusLogProbMetric: 17.6771 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 126/1000
2023-09-26 15:46:58.854 
Epoch 126/1000 
	 loss: 17.4479, MinusLogProbMetric: 17.4479, val_loss: 18.0046, val_MinusLogProbMetric: 18.0046

Epoch 126: val_loss did not improve from 17.39399
196/196 - 66s - loss: 17.4479 - MinusLogProbMetric: 17.4479 - val_loss: 18.0046 - val_MinusLogProbMetric: 18.0046 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 127/1000
2023-09-26 15:48:05.496 
Epoch 127/1000 
	 loss: 17.3843, MinusLogProbMetric: 17.3843, val_loss: 17.8755, val_MinusLogProbMetric: 17.8755

Epoch 127: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.3843 - MinusLogProbMetric: 17.3843 - val_loss: 17.8755 - val_MinusLogProbMetric: 17.8755 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 128/1000
2023-09-26 15:49:12.127 
Epoch 128/1000 
	 loss: 17.4069, MinusLogProbMetric: 17.4069, val_loss: 17.8698, val_MinusLogProbMetric: 17.8698

Epoch 128: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.4069 - MinusLogProbMetric: 17.4069 - val_loss: 17.8698 - val_MinusLogProbMetric: 17.8698 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 129/1000
2023-09-26 15:50:18.638 
Epoch 129/1000 
	 loss: 17.3921, MinusLogProbMetric: 17.3921, val_loss: 17.8473, val_MinusLogProbMetric: 17.8473

Epoch 129: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.3921 - MinusLogProbMetric: 17.3921 - val_loss: 17.8473 - val_MinusLogProbMetric: 17.8473 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 130/1000
2023-09-26 15:51:25.334 
Epoch 130/1000 
	 loss: 17.3858, MinusLogProbMetric: 17.3858, val_loss: 17.7729, val_MinusLogProbMetric: 17.7729

Epoch 130: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.3858 - MinusLogProbMetric: 17.3858 - val_loss: 17.7729 - val_MinusLogProbMetric: 17.7729 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 131/1000
2023-09-26 15:52:31.372 
Epoch 131/1000 
	 loss: 17.3353, MinusLogProbMetric: 17.3353, val_loss: 17.5131, val_MinusLogProbMetric: 17.5131

Epoch 131: val_loss did not improve from 17.39399
196/196 - 66s - loss: 17.3353 - MinusLogProbMetric: 17.3353 - val_loss: 17.5131 - val_MinusLogProbMetric: 17.5131 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 132/1000
2023-09-26 15:53:38.170 
Epoch 132/1000 
	 loss: 17.3798, MinusLogProbMetric: 17.3798, val_loss: 17.7706, val_MinusLogProbMetric: 17.7706

Epoch 132: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.3798 - MinusLogProbMetric: 17.3798 - val_loss: 17.7706 - val_MinusLogProbMetric: 17.7706 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 133/1000
2023-09-26 15:54:44.901 
Epoch 133/1000 
	 loss: 17.3669, MinusLogProbMetric: 17.3669, val_loss: 17.5533, val_MinusLogProbMetric: 17.5533

Epoch 133: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.3669 - MinusLogProbMetric: 17.3669 - val_loss: 17.5533 - val_MinusLogProbMetric: 17.5533 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 134/1000
2023-09-26 15:55:51.817 
Epoch 134/1000 
	 loss: 17.3786, MinusLogProbMetric: 17.3786, val_loss: 17.7698, val_MinusLogProbMetric: 17.7698

Epoch 134: val_loss did not improve from 17.39399
196/196 - 67s - loss: 17.3786 - MinusLogProbMetric: 17.3786 - val_loss: 17.7698 - val_MinusLogProbMetric: 17.7698 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 135/1000
2023-09-26 15:56:52.089 
Epoch 135/1000 
	 loss: 17.3699, MinusLogProbMetric: 17.3699, val_loss: 17.8402, val_MinusLogProbMetric: 17.8402

Epoch 135: val_loss did not improve from 17.39399
196/196 - 60s - loss: 17.3699 - MinusLogProbMetric: 17.3699 - val_loss: 17.8402 - val_MinusLogProbMetric: 17.8402 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 136/1000
2023-09-26 15:57:46.458 
Epoch 136/1000 
	 loss: 17.3860, MinusLogProbMetric: 17.3860, val_loss: 18.0300, val_MinusLogProbMetric: 18.0300

Epoch 136: val_loss did not improve from 17.39399
196/196 - 54s - loss: 17.3860 - MinusLogProbMetric: 17.3860 - val_loss: 18.0300 - val_MinusLogProbMetric: 18.0300 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 137/1000
2023-09-26 15:58:50.899 
Epoch 137/1000 
	 loss: 17.3774, MinusLogProbMetric: 17.3774, val_loss: 17.3645, val_MinusLogProbMetric: 17.3645

Epoch 137: val_loss improved from 17.39399 to 17.36448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 17.3774 - MinusLogProbMetric: 17.3774 - val_loss: 17.3645 - val_MinusLogProbMetric: 17.3645 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 138/1000
2023-09-26 15:59:57.063 
Epoch 138/1000 
	 loss: 17.3835, MinusLogProbMetric: 17.3835, val_loss: 17.5505, val_MinusLogProbMetric: 17.5505

Epoch 138: val_loss did not improve from 17.36448
196/196 - 65s - loss: 17.3835 - MinusLogProbMetric: 17.3835 - val_loss: 17.5505 - val_MinusLogProbMetric: 17.5505 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 139/1000
2023-09-26 16:01:03.852 
Epoch 139/1000 
	 loss: 17.3664, MinusLogProbMetric: 17.3664, val_loss: 17.8888, val_MinusLogProbMetric: 17.8888

Epoch 139: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.3664 - MinusLogProbMetric: 17.3664 - val_loss: 17.8888 - val_MinusLogProbMetric: 17.8888 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 140/1000
2023-09-26 16:02:11.035 
Epoch 140/1000 
	 loss: 17.3533, MinusLogProbMetric: 17.3533, val_loss: 17.5973, val_MinusLogProbMetric: 17.5973

Epoch 140: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.3533 - MinusLogProbMetric: 17.3533 - val_loss: 17.5973 - val_MinusLogProbMetric: 17.5973 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 141/1000
2023-09-26 16:03:17.515 
Epoch 141/1000 
	 loss: 17.3611, MinusLogProbMetric: 17.3611, val_loss: 17.5061, val_MinusLogProbMetric: 17.5061

Epoch 141: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.3611 - MinusLogProbMetric: 17.3611 - val_loss: 17.5061 - val_MinusLogProbMetric: 17.5061 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 142/1000
2023-09-26 16:04:24.604 
Epoch 142/1000 
	 loss: 17.3555, MinusLogProbMetric: 17.3555, val_loss: 17.6294, val_MinusLogProbMetric: 17.6294

Epoch 142: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.3555 - MinusLogProbMetric: 17.3555 - val_loss: 17.6294 - val_MinusLogProbMetric: 17.6294 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 143/1000
2023-09-26 16:05:31.320 
Epoch 143/1000 
	 loss: 17.2853, MinusLogProbMetric: 17.2853, val_loss: 17.9647, val_MinusLogProbMetric: 17.9647

Epoch 143: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.2853 - MinusLogProbMetric: 17.2853 - val_loss: 17.9647 - val_MinusLogProbMetric: 17.9647 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 144/1000
2023-09-26 16:06:37.913 
Epoch 144/1000 
	 loss: 17.3676, MinusLogProbMetric: 17.3676, val_loss: 17.7561, val_MinusLogProbMetric: 17.7561

Epoch 144: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.3676 - MinusLogProbMetric: 17.3676 - val_loss: 17.7561 - val_MinusLogProbMetric: 17.7561 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 145/1000
2023-09-26 16:07:44.418 
Epoch 145/1000 
	 loss: 17.3407, MinusLogProbMetric: 17.3407, val_loss: 17.5963, val_MinusLogProbMetric: 17.5963

Epoch 145: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.3407 - MinusLogProbMetric: 17.3407 - val_loss: 17.5963 - val_MinusLogProbMetric: 17.5963 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 146/1000
2023-09-26 16:08:51.894 
Epoch 146/1000 
	 loss: 17.2754, MinusLogProbMetric: 17.2754, val_loss: 17.6742, val_MinusLogProbMetric: 17.6742

Epoch 146: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.2754 - MinusLogProbMetric: 17.2754 - val_loss: 17.6742 - val_MinusLogProbMetric: 17.6742 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 147/1000
2023-09-26 16:09:58.466 
Epoch 147/1000 
	 loss: 17.3161, MinusLogProbMetric: 17.3161, val_loss: 17.5940, val_MinusLogProbMetric: 17.5940

Epoch 147: val_loss did not improve from 17.36448
196/196 - 67s - loss: 17.3161 - MinusLogProbMetric: 17.3161 - val_loss: 17.5940 - val_MinusLogProbMetric: 17.5940 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 148/1000
2023-09-26 16:11:04.841 
Epoch 148/1000 
	 loss: 17.3134, MinusLogProbMetric: 17.3134, val_loss: 17.6373, val_MinusLogProbMetric: 17.6373

Epoch 148: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.3134 - MinusLogProbMetric: 17.3134 - val_loss: 17.6373 - val_MinusLogProbMetric: 17.6373 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 149/1000
2023-09-26 16:12:11.300 
Epoch 149/1000 
	 loss: 17.3085, MinusLogProbMetric: 17.3085, val_loss: 17.6246, val_MinusLogProbMetric: 17.6246

Epoch 149: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.3085 - MinusLogProbMetric: 17.3085 - val_loss: 17.6246 - val_MinusLogProbMetric: 17.6246 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 150/1000
2023-09-26 16:13:17.624 
Epoch 150/1000 
	 loss: 17.2928, MinusLogProbMetric: 17.2928, val_loss: 17.5212, val_MinusLogProbMetric: 17.5212

Epoch 150: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2928 - MinusLogProbMetric: 17.2928 - val_loss: 17.5212 - val_MinusLogProbMetric: 17.5212 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 151/1000
2023-09-26 16:14:23.746 
Epoch 151/1000 
	 loss: 17.3308, MinusLogProbMetric: 17.3308, val_loss: 17.5147, val_MinusLogProbMetric: 17.5147

Epoch 151: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.3308 - MinusLogProbMetric: 17.3308 - val_loss: 17.5147 - val_MinusLogProbMetric: 17.5147 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 152/1000
2023-09-26 16:15:29.701 
Epoch 152/1000 
	 loss: 17.2870, MinusLogProbMetric: 17.2870, val_loss: 17.8820, val_MinusLogProbMetric: 17.8820

Epoch 152: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2870 - MinusLogProbMetric: 17.2870 - val_loss: 17.8820 - val_MinusLogProbMetric: 17.8820 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 153/1000
2023-09-26 16:16:35.392 
Epoch 153/1000 
	 loss: 17.2789, MinusLogProbMetric: 17.2789, val_loss: 17.5735, val_MinusLogProbMetric: 17.5735

Epoch 153: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2789 - MinusLogProbMetric: 17.2789 - val_loss: 17.5735 - val_MinusLogProbMetric: 17.5735 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 154/1000
2023-09-26 16:17:41.196 
Epoch 154/1000 
	 loss: 17.3205, MinusLogProbMetric: 17.3205, val_loss: 17.6591, val_MinusLogProbMetric: 17.6591

Epoch 154: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.3205 - MinusLogProbMetric: 17.3205 - val_loss: 17.6591 - val_MinusLogProbMetric: 17.6591 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 155/1000
2023-09-26 16:18:47.044 
Epoch 155/1000 
	 loss: 17.2581, MinusLogProbMetric: 17.2581, val_loss: 17.8090, val_MinusLogProbMetric: 17.8090

Epoch 155: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2581 - MinusLogProbMetric: 17.2581 - val_loss: 17.8090 - val_MinusLogProbMetric: 17.8090 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 156/1000
2023-09-26 16:19:52.690 
Epoch 156/1000 
	 loss: 17.2688, MinusLogProbMetric: 17.2688, val_loss: 17.5012, val_MinusLogProbMetric: 17.5012

Epoch 156: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2688 - MinusLogProbMetric: 17.2688 - val_loss: 17.5012 - val_MinusLogProbMetric: 17.5012 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 157/1000
2023-09-26 16:20:58.235 
Epoch 157/1000 
	 loss: 17.2680, MinusLogProbMetric: 17.2680, val_loss: 17.8117, val_MinusLogProbMetric: 17.8117

Epoch 157: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2680 - MinusLogProbMetric: 17.2680 - val_loss: 17.8117 - val_MinusLogProbMetric: 17.8117 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 158/1000
2023-09-26 16:22:03.807 
Epoch 158/1000 
	 loss: 17.2582, MinusLogProbMetric: 17.2582, val_loss: 17.5590, val_MinusLogProbMetric: 17.5590

Epoch 158: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2582 - MinusLogProbMetric: 17.2582 - val_loss: 17.5590 - val_MinusLogProbMetric: 17.5590 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 159/1000
2023-09-26 16:23:09.643 
Epoch 159/1000 
	 loss: 17.2502, MinusLogProbMetric: 17.2502, val_loss: 17.5757, val_MinusLogProbMetric: 17.5757

Epoch 159: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2502 - MinusLogProbMetric: 17.2502 - val_loss: 17.5757 - val_MinusLogProbMetric: 17.5757 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 160/1000
2023-09-26 16:24:15.349 
Epoch 160/1000 
	 loss: 17.2776, MinusLogProbMetric: 17.2776, val_loss: 17.7135, val_MinusLogProbMetric: 17.7135

Epoch 160: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2776 - MinusLogProbMetric: 17.2776 - val_loss: 17.7135 - val_MinusLogProbMetric: 17.7135 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 161/1000
2023-09-26 16:25:21.400 
Epoch 161/1000 
	 loss: 17.2594, MinusLogProbMetric: 17.2594, val_loss: 17.5047, val_MinusLogProbMetric: 17.5047

Epoch 161: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2594 - MinusLogProbMetric: 17.2594 - val_loss: 17.5047 - val_MinusLogProbMetric: 17.5047 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 162/1000
2023-09-26 16:26:27.178 
Epoch 162/1000 
	 loss: 17.2643, MinusLogProbMetric: 17.2643, val_loss: 17.8358, val_MinusLogProbMetric: 17.8358

Epoch 162: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2643 - MinusLogProbMetric: 17.2643 - val_loss: 17.8358 - val_MinusLogProbMetric: 17.8358 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 163/1000
2023-09-26 16:27:32.591 
Epoch 163/1000 
	 loss: 17.2271, MinusLogProbMetric: 17.2271, val_loss: 17.5264, val_MinusLogProbMetric: 17.5264

Epoch 163: val_loss did not improve from 17.36448
196/196 - 65s - loss: 17.2271 - MinusLogProbMetric: 17.2271 - val_loss: 17.5264 - val_MinusLogProbMetric: 17.5264 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 164/1000
2023-09-26 16:28:38.175 
Epoch 164/1000 
	 loss: 17.2404, MinusLogProbMetric: 17.2404, val_loss: 17.8232, val_MinusLogProbMetric: 17.8232

Epoch 164: val_loss did not improve from 17.36448
196/196 - 66s - loss: 17.2404 - MinusLogProbMetric: 17.2404 - val_loss: 17.8232 - val_MinusLogProbMetric: 17.8232 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 165/1000
2023-09-26 16:29:43.585 
Epoch 165/1000 
	 loss: 17.2477, MinusLogProbMetric: 17.2477, val_loss: 17.4573, val_MinusLogProbMetric: 17.4573

Epoch 165: val_loss did not improve from 17.36448
196/196 - 65s - loss: 17.2477 - MinusLogProbMetric: 17.2477 - val_loss: 17.4573 - val_MinusLogProbMetric: 17.4573 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 166/1000
2023-09-26 16:30:48.727 
Epoch 166/1000 
	 loss: 17.2299, MinusLogProbMetric: 17.2299, val_loss: 17.4287, val_MinusLogProbMetric: 17.4287

Epoch 166: val_loss did not improve from 17.36448
196/196 - 65s - loss: 17.2299 - MinusLogProbMetric: 17.2299 - val_loss: 17.4287 - val_MinusLogProbMetric: 17.4287 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 167/1000
2023-09-26 16:31:53.644 
Epoch 167/1000 
	 loss: 17.2485, MinusLogProbMetric: 17.2485, val_loss: 17.7082, val_MinusLogProbMetric: 17.7082

Epoch 167: val_loss did not improve from 17.36448
196/196 - 65s - loss: 17.2485 - MinusLogProbMetric: 17.2485 - val_loss: 17.7082 - val_MinusLogProbMetric: 17.7082 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 168/1000
2023-09-26 16:32:58.405 
Epoch 168/1000 
	 loss: 17.2782, MinusLogProbMetric: 17.2782, val_loss: 17.6952, val_MinusLogProbMetric: 17.6952

Epoch 168: val_loss did not improve from 17.36448
196/196 - 65s - loss: 17.2782 - MinusLogProbMetric: 17.2782 - val_loss: 17.6952 - val_MinusLogProbMetric: 17.6952 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 169/1000
2023-09-26 16:34:03.435 
Epoch 169/1000 
	 loss: 17.2236, MinusLogProbMetric: 17.2236, val_loss: 17.6189, val_MinusLogProbMetric: 17.6189

Epoch 169: val_loss did not improve from 17.36448
196/196 - 65s - loss: 17.2236 - MinusLogProbMetric: 17.2236 - val_loss: 17.6189 - val_MinusLogProbMetric: 17.6189 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 170/1000
2023-09-26 16:35:08.667 
Epoch 170/1000 
	 loss: 17.2173, MinusLogProbMetric: 17.2173, val_loss: 17.3291, val_MinusLogProbMetric: 17.3291

Epoch 170: val_loss improved from 17.36448 to 17.32915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 17.2173 - MinusLogProbMetric: 17.2173 - val_loss: 17.3291 - val_MinusLogProbMetric: 17.3291 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 171/1000
2023-09-26 16:36:14.768 
Epoch 171/1000 
	 loss: 17.2460, MinusLogProbMetric: 17.2460, val_loss: 17.7191, val_MinusLogProbMetric: 17.7191

Epoch 171: val_loss did not improve from 17.32915
196/196 - 65s - loss: 17.2460 - MinusLogProbMetric: 17.2460 - val_loss: 17.7191 - val_MinusLogProbMetric: 17.7191 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 172/1000
2023-09-26 16:37:19.841 
Epoch 172/1000 
	 loss: 17.2217, MinusLogProbMetric: 17.2217, val_loss: 17.5230, val_MinusLogProbMetric: 17.5230

Epoch 172: val_loss did not improve from 17.32915
196/196 - 65s - loss: 17.2217 - MinusLogProbMetric: 17.2217 - val_loss: 17.5230 - val_MinusLogProbMetric: 17.5230 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 173/1000
2023-09-26 16:38:24.895 
Epoch 173/1000 
	 loss: 17.2347, MinusLogProbMetric: 17.2347, val_loss: 17.4608, val_MinusLogProbMetric: 17.4608

Epoch 173: val_loss did not improve from 17.32915
196/196 - 65s - loss: 17.2347 - MinusLogProbMetric: 17.2347 - val_loss: 17.4608 - val_MinusLogProbMetric: 17.4608 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 174/1000
2023-09-26 16:39:30.400 
Epoch 174/1000 
	 loss: 17.2283, MinusLogProbMetric: 17.2283, val_loss: 17.7227, val_MinusLogProbMetric: 17.7227

Epoch 174: val_loss did not improve from 17.32915
196/196 - 66s - loss: 17.2283 - MinusLogProbMetric: 17.2283 - val_loss: 17.7227 - val_MinusLogProbMetric: 17.7227 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 175/1000
2023-09-26 16:40:35.229 
Epoch 175/1000 
	 loss: 17.2308, MinusLogProbMetric: 17.2308, val_loss: 17.7278, val_MinusLogProbMetric: 17.7278

Epoch 175: val_loss did not improve from 17.32915
196/196 - 65s - loss: 17.2308 - MinusLogProbMetric: 17.2308 - val_loss: 17.7278 - val_MinusLogProbMetric: 17.7278 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 176/1000
2023-09-26 16:41:40.548 
Epoch 176/1000 
	 loss: 17.2454, MinusLogProbMetric: 17.2454, val_loss: 17.3215, val_MinusLogProbMetric: 17.3215

Epoch 176: val_loss improved from 17.32915 to 17.32153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 17.2454 - MinusLogProbMetric: 17.2454 - val_loss: 17.3215 - val_MinusLogProbMetric: 17.3215 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 177/1000
2023-09-26 16:42:46.456 
Epoch 177/1000 
	 loss: 17.1765, MinusLogProbMetric: 17.1765, val_loss: 17.4174, val_MinusLogProbMetric: 17.4174

Epoch 177: val_loss did not improve from 17.32153
196/196 - 65s - loss: 17.1765 - MinusLogProbMetric: 17.1765 - val_loss: 17.4174 - val_MinusLogProbMetric: 17.4174 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 178/1000
2023-09-26 16:43:51.357 
Epoch 178/1000 
	 loss: 17.2306, MinusLogProbMetric: 17.2306, val_loss: 17.4289, val_MinusLogProbMetric: 17.4289

Epoch 178: val_loss did not improve from 17.32153
196/196 - 65s - loss: 17.2306 - MinusLogProbMetric: 17.2306 - val_loss: 17.4289 - val_MinusLogProbMetric: 17.4289 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 179/1000
2023-09-26 16:44:56.267 
Epoch 179/1000 
	 loss: 17.1533, MinusLogProbMetric: 17.1533, val_loss: 17.7153, val_MinusLogProbMetric: 17.7153

Epoch 179: val_loss did not improve from 17.32153
196/196 - 65s - loss: 17.1533 - MinusLogProbMetric: 17.1533 - val_loss: 17.7153 - val_MinusLogProbMetric: 17.7153 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 180/1000
2023-09-26 16:46:01.042 
Epoch 180/1000 
	 loss: 17.2002, MinusLogProbMetric: 17.2002, val_loss: 17.5286, val_MinusLogProbMetric: 17.5286

Epoch 180: val_loss did not improve from 17.32153
196/196 - 65s - loss: 17.2002 - MinusLogProbMetric: 17.2002 - val_loss: 17.5286 - val_MinusLogProbMetric: 17.5286 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 181/1000
2023-09-26 16:47:06.115 
Epoch 181/1000 
	 loss: 17.1879, MinusLogProbMetric: 17.1879, val_loss: 17.4337, val_MinusLogProbMetric: 17.4337

Epoch 181: val_loss did not improve from 17.32153
196/196 - 65s - loss: 17.1879 - MinusLogProbMetric: 17.1879 - val_loss: 17.4337 - val_MinusLogProbMetric: 17.4337 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 182/1000
2023-09-26 16:48:11.265 
Epoch 182/1000 
	 loss: 17.1384, MinusLogProbMetric: 17.1384, val_loss: 17.5197, val_MinusLogProbMetric: 17.5197

Epoch 182: val_loss did not improve from 17.32153
196/196 - 65s - loss: 17.1384 - MinusLogProbMetric: 17.1384 - val_loss: 17.5197 - val_MinusLogProbMetric: 17.5197 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 183/1000
2023-09-26 16:49:16.245 
Epoch 183/1000 
	 loss: 17.2104, MinusLogProbMetric: 17.2104, val_loss: 17.4412, val_MinusLogProbMetric: 17.4412

Epoch 183: val_loss did not improve from 17.32153
196/196 - 65s - loss: 17.2104 - MinusLogProbMetric: 17.2104 - val_loss: 17.4412 - val_MinusLogProbMetric: 17.4412 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 184/1000
2023-09-26 16:50:21.154 
Epoch 184/1000 
	 loss: 17.1718, MinusLogProbMetric: 17.1718, val_loss: 17.2810, val_MinusLogProbMetric: 17.2810

Epoch 184: val_loss improved from 17.32153 to 17.28099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 17.1718 - MinusLogProbMetric: 17.1718 - val_loss: 17.2810 - val_MinusLogProbMetric: 17.2810 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 185/1000
2023-09-26 16:51:26.974 
Epoch 185/1000 
	 loss: 17.1665, MinusLogProbMetric: 17.1665, val_loss: 17.5308, val_MinusLogProbMetric: 17.5308

Epoch 185: val_loss did not improve from 17.28099
196/196 - 65s - loss: 17.1665 - MinusLogProbMetric: 17.1665 - val_loss: 17.5308 - val_MinusLogProbMetric: 17.5308 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 186/1000
2023-09-26 16:52:31.996 
Epoch 186/1000 
	 loss: 17.1385, MinusLogProbMetric: 17.1385, val_loss: 17.3006, val_MinusLogProbMetric: 17.3006

Epoch 186: val_loss did not improve from 17.28099
196/196 - 65s - loss: 17.1385 - MinusLogProbMetric: 17.1385 - val_loss: 17.3006 - val_MinusLogProbMetric: 17.3006 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 187/1000
2023-09-26 16:53:36.727 
Epoch 187/1000 
	 loss: 17.2785, MinusLogProbMetric: 17.2785, val_loss: 17.6606, val_MinusLogProbMetric: 17.6606

Epoch 187: val_loss did not improve from 17.28099
196/196 - 65s - loss: 17.2785 - MinusLogProbMetric: 17.2785 - val_loss: 17.6606 - val_MinusLogProbMetric: 17.6606 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 188/1000
2023-09-26 16:54:41.837 
Epoch 188/1000 
	 loss: 17.1462, MinusLogProbMetric: 17.1462, val_loss: 18.1346, val_MinusLogProbMetric: 18.1346

Epoch 188: val_loss did not improve from 17.28099
196/196 - 65s - loss: 17.1462 - MinusLogProbMetric: 17.1462 - val_loss: 18.1346 - val_MinusLogProbMetric: 18.1346 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 189/1000
2023-09-26 16:55:46.555 
Epoch 189/1000 
	 loss: 17.1794, MinusLogProbMetric: 17.1794, val_loss: 17.4901, val_MinusLogProbMetric: 17.4901

Epoch 189: val_loss did not improve from 17.28099
196/196 - 65s - loss: 17.1794 - MinusLogProbMetric: 17.1794 - val_loss: 17.4901 - val_MinusLogProbMetric: 17.4901 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 190/1000
2023-09-26 16:56:50.526 
Epoch 190/1000 
	 loss: 17.1287, MinusLogProbMetric: 17.1287, val_loss: 17.2441, val_MinusLogProbMetric: 17.2441

Epoch 190: val_loss improved from 17.28099 to 17.24405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 17.1287 - MinusLogProbMetric: 17.1287 - val_loss: 17.2441 - val_MinusLogProbMetric: 17.2441 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 191/1000
2023-09-26 16:57:56.508 
Epoch 191/1000 
	 loss: 17.1395, MinusLogProbMetric: 17.1395, val_loss: 17.6569, val_MinusLogProbMetric: 17.6569

Epoch 191: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1395 - MinusLogProbMetric: 17.1395 - val_loss: 17.6569 - val_MinusLogProbMetric: 17.6569 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 192/1000
2023-09-26 16:59:01.462 
Epoch 192/1000 
	 loss: 17.1670, MinusLogProbMetric: 17.1670, val_loss: 17.7219, val_MinusLogProbMetric: 17.7219

Epoch 192: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1670 - MinusLogProbMetric: 17.1670 - val_loss: 17.7219 - val_MinusLogProbMetric: 17.7219 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 193/1000
2023-09-26 17:00:06.281 
Epoch 193/1000 
	 loss: 17.1867, MinusLogProbMetric: 17.1867, val_loss: 17.3220, val_MinusLogProbMetric: 17.3220

Epoch 193: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1867 - MinusLogProbMetric: 17.1867 - val_loss: 17.3220 - val_MinusLogProbMetric: 17.3220 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 194/1000
2023-09-26 17:01:11.027 
Epoch 194/1000 
	 loss: 17.0817, MinusLogProbMetric: 17.0817, val_loss: 17.6114, val_MinusLogProbMetric: 17.6114

Epoch 194: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0817 - MinusLogProbMetric: 17.0817 - val_loss: 17.6114 - val_MinusLogProbMetric: 17.6114 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 195/1000
2023-09-26 17:02:15.329 
Epoch 195/1000 
	 loss: 17.1414, MinusLogProbMetric: 17.1414, val_loss: 17.5562, val_MinusLogProbMetric: 17.5562

Epoch 195: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.1414 - MinusLogProbMetric: 17.1414 - val_loss: 17.5562 - val_MinusLogProbMetric: 17.5562 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 196/1000
2023-09-26 17:03:20.447 
Epoch 196/1000 
	 loss: 17.1316, MinusLogProbMetric: 17.1316, val_loss: 17.4609, val_MinusLogProbMetric: 17.4609

Epoch 196: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1316 - MinusLogProbMetric: 17.1316 - val_loss: 17.4609 - val_MinusLogProbMetric: 17.4609 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 197/1000
2023-09-26 17:04:24.546 
Epoch 197/1000 
	 loss: 17.1431, MinusLogProbMetric: 17.1431, val_loss: 17.4259, val_MinusLogProbMetric: 17.4259

Epoch 197: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.1431 - MinusLogProbMetric: 17.1431 - val_loss: 17.4259 - val_MinusLogProbMetric: 17.4259 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 198/1000
2023-09-26 17:05:29.131 
Epoch 198/1000 
	 loss: 17.1476, MinusLogProbMetric: 17.1476, val_loss: 17.6653, val_MinusLogProbMetric: 17.6653

Epoch 198: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1476 - MinusLogProbMetric: 17.1476 - val_loss: 17.6653 - val_MinusLogProbMetric: 17.6653 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 199/1000
2023-09-26 17:06:33.381 
Epoch 199/1000 
	 loss: 17.1251, MinusLogProbMetric: 17.1251, val_loss: 17.7463, val_MinusLogProbMetric: 17.7463

Epoch 199: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.1251 - MinusLogProbMetric: 17.1251 - val_loss: 17.7463 - val_MinusLogProbMetric: 17.7463 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 200/1000
2023-09-26 17:07:37.841 
Epoch 200/1000 
	 loss: 17.1558, MinusLogProbMetric: 17.1558, val_loss: 17.7686, val_MinusLogProbMetric: 17.7686

Epoch 200: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.1558 - MinusLogProbMetric: 17.1558 - val_loss: 17.7686 - val_MinusLogProbMetric: 17.7686 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 201/1000
2023-09-26 17:08:42.598 
Epoch 201/1000 
	 loss: 17.0986, MinusLogProbMetric: 17.0986, val_loss: 17.5387, val_MinusLogProbMetric: 17.5387

Epoch 201: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0986 - MinusLogProbMetric: 17.0986 - val_loss: 17.5387 - val_MinusLogProbMetric: 17.5387 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 202/1000
2023-09-26 17:09:47.087 
Epoch 202/1000 
	 loss: 17.1154, MinusLogProbMetric: 17.1154, val_loss: 17.4914, val_MinusLogProbMetric: 17.4914

Epoch 202: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.1154 - MinusLogProbMetric: 17.1154 - val_loss: 17.4914 - val_MinusLogProbMetric: 17.4914 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 203/1000
2023-09-26 17:10:51.741 
Epoch 203/1000 
	 loss: 17.1255, MinusLogProbMetric: 17.1255, val_loss: 17.5969, val_MinusLogProbMetric: 17.5969

Epoch 203: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1255 - MinusLogProbMetric: 17.1255 - val_loss: 17.5969 - val_MinusLogProbMetric: 17.5969 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 204/1000
2023-09-26 17:11:56.482 
Epoch 204/1000 
	 loss: 17.1091, MinusLogProbMetric: 17.1091, val_loss: 17.3343, val_MinusLogProbMetric: 17.3343

Epoch 204: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1091 - MinusLogProbMetric: 17.1091 - val_loss: 17.3343 - val_MinusLogProbMetric: 17.3343 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 205/1000
2023-09-26 17:13:00.985 
Epoch 205/1000 
	 loss: 17.1336, MinusLogProbMetric: 17.1336, val_loss: 17.5454, val_MinusLogProbMetric: 17.5454

Epoch 205: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.1336 - MinusLogProbMetric: 17.1336 - val_loss: 17.5454 - val_MinusLogProbMetric: 17.5454 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 206/1000
2023-09-26 17:14:05.671 
Epoch 206/1000 
	 loss: 17.1166, MinusLogProbMetric: 17.1166, val_loss: 17.6893, val_MinusLogProbMetric: 17.6893

Epoch 206: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1166 - MinusLogProbMetric: 17.1166 - val_loss: 17.6893 - val_MinusLogProbMetric: 17.6893 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 207/1000
2023-09-26 17:15:10.207 
Epoch 207/1000 
	 loss: 17.1549, MinusLogProbMetric: 17.1549, val_loss: 17.5562, val_MinusLogProbMetric: 17.5562

Epoch 207: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1549 - MinusLogProbMetric: 17.1549 - val_loss: 17.5562 - val_MinusLogProbMetric: 17.5562 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 208/1000
2023-09-26 17:16:14.535 
Epoch 208/1000 
	 loss: 17.1077, MinusLogProbMetric: 17.1077, val_loss: 17.4975, val_MinusLogProbMetric: 17.4975

Epoch 208: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.1077 - MinusLogProbMetric: 17.1077 - val_loss: 17.4975 - val_MinusLogProbMetric: 17.4975 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 209/1000
2023-09-26 17:17:19.243 
Epoch 209/1000 
	 loss: 17.1118, MinusLogProbMetric: 17.1118, val_loss: 17.3338, val_MinusLogProbMetric: 17.3338

Epoch 209: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1118 - MinusLogProbMetric: 17.1118 - val_loss: 17.3338 - val_MinusLogProbMetric: 17.3338 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 210/1000
2023-09-26 17:18:23.589 
Epoch 210/1000 
	 loss: 17.0779, MinusLogProbMetric: 17.0779, val_loss: 17.3265, val_MinusLogProbMetric: 17.3265

Epoch 210: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0779 - MinusLogProbMetric: 17.0779 - val_loss: 17.3265 - val_MinusLogProbMetric: 17.3265 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 211/1000
2023-09-26 17:19:28.357 
Epoch 211/1000 
	 loss: 17.1064, MinusLogProbMetric: 17.1064, val_loss: 17.3362, val_MinusLogProbMetric: 17.3362

Epoch 211: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1064 - MinusLogProbMetric: 17.1064 - val_loss: 17.3362 - val_MinusLogProbMetric: 17.3362 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 212/1000
2023-09-26 17:20:33.038 
Epoch 212/1000 
	 loss: 17.0872, MinusLogProbMetric: 17.0872, val_loss: 17.6189, val_MinusLogProbMetric: 17.6189

Epoch 212: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0872 - MinusLogProbMetric: 17.0872 - val_loss: 17.6189 - val_MinusLogProbMetric: 17.6189 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 213/1000
2023-09-26 17:21:37.975 
Epoch 213/1000 
	 loss: 17.0871, MinusLogProbMetric: 17.0871, val_loss: 17.6886, val_MinusLogProbMetric: 17.6886

Epoch 213: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0871 - MinusLogProbMetric: 17.0871 - val_loss: 17.6886 - val_MinusLogProbMetric: 17.6886 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 214/1000
2023-09-26 17:22:42.569 
Epoch 214/1000 
	 loss: 17.1016, MinusLogProbMetric: 17.1016, val_loss: 17.7395, val_MinusLogProbMetric: 17.7395

Epoch 214: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1016 - MinusLogProbMetric: 17.1016 - val_loss: 17.7395 - val_MinusLogProbMetric: 17.7395 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 215/1000
2023-09-26 17:23:46.775 
Epoch 215/1000 
	 loss: 17.0849, MinusLogProbMetric: 17.0849, val_loss: 17.4938, val_MinusLogProbMetric: 17.4938

Epoch 215: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0849 - MinusLogProbMetric: 17.0849 - val_loss: 17.4938 - val_MinusLogProbMetric: 17.4938 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 216/1000
2023-09-26 17:24:51.355 
Epoch 216/1000 
	 loss: 17.0741, MinusLogProbMetric: 17.0741, val_loss: 17.5037, val_MinusLogProbMetric: 17.5037

Epoch 216: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0741 - MinusLogProbMetric: 17.0741 - val_loss: 17.5037 - val_MinusLogProbMetric: 17.5037 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 217/1000
2023-09-26 17:25:55.629 
Epoch 217/1000 
	 loss: 17.0604, MinusLogProbMetric: 17.0604, val_loss: 17.3170, val_MinusLogProbMetric: 17.3170

Epoch 217: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0604 - MinusLogProbMetric: 17.0604 - val_loss: 17.3170 - val_MinusLogProbMetric: 17.3170 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 218/1000
2023-09-26 17:27:00.093 
Epoch 218/1000 
	 loss: 17.0928, MinusLogProbMetric: 17.0928, val_loss: 17.3425, val_MinusLogProbMetric: 17.3425

Epoch 218: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0928 - MinusLogProbMetric: 17.0928 - val_loss: 17.3425 - val_MinusLogProbMetric: 17.3425 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 219/1000
2023-09-26 17:28:04.571 
Epoch 219/1000 
	 loss: 17.0879, MinusLogProbMetric: 17.0879, val_loss: 17.4230, val_MinusLogProbMetric: 17.4230

Epoch 219: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0879 - MinusLogProbMetric: 17.0879 - val_loss: 17.4230 - val_MinusLogProbMetric: 17.4230 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 220/1000
2023-09-26 17:29:08.986 
Epoch 220/1000 
	 loss: 17.0265, MinusLogProbMetric: 17.0265, val_loss: 17.5147, val_MinusLogProbMetric: 17.5147

Epoch 220: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0265 - MinusLogProbMetric: 17.0265 - val_loss: 17.5147 - val_MinusLogProbMetric: 17.5147 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 221/1000
2023-09-26 17:30:13.970 
Epoch 221/1000 
	 loss: 17.0775, MinusLogProbMetric: 17.0775, val_loss: 17.3938, val_MinusLogProbMetric: 17.3938

Epoch 221: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0775 - MinusLogProbMetric: 17.0775 - val_loss: 17.3938 - val_MinusLogProbMetric: 17.3938 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 222/1000
2023-09-26 17:31:18.551 
Epoch 222/1000 
	 loss: 17.0846, MinusLogProbMetric: 17.0846, val_loss: 17.4902, val_MinusLogProbMetric: 17.4902

Epoch 222: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0846 - MinusLogProbMetric: 17.0846 - val_loss: 17.4902 - val_MinusLogProbMetric: 17.4902 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 223/1000
2023-09-26 17:32:23.531 
Epoch 223/1000 
	 loss: 17.0273, MinusLogProbMetric: 17.0273, val_loss: 17.3238, val_MinusLogProbMetric: 17.3238

Epoch 223: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0273 - MinusLogProbMetric: 17.0273 - val_loss: 17.3238 - val_MinusLogProbMetric: 17.3238 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 224/1000
2023-09-26 17:33:28.480 
Epoch 224/1000 
	 loss: 17.0793, MinusLogProbMetric: 17.0793, val_loss: 17.5462, val_MinusLogProbMetric: 17.5462

Epoch 224: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0793 - MinusLogProbMetric: 17.0793 - val_loss: 17.5462 - val_MinusLogProbMetric: 17.5462 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 225/1000
2023-09-26 17:34:33.219 
Epoch 225/1000 
	 loss: 17.1141, MinusLogProbMetric: 17.1141, val_loss: 17.5532, val_MinusLogProbMetric: 17.5532

Epoch 225: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1141 - MinusLogProbMetric: 17.1141 - val_loss: 17.5532 - val_MinusLogProbMetric: 17.5532 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 226/1000
2023-09-26 17:35:37.786 
Epoch 226/1000 
	 loss: 17.1213, MinusLogProbMetric: 17.1213, val_loss: 17.5663, val_MinusLogProbMetric: 17.5663

Epoch 226: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1213 - MinusLogProbMetric: 17.1213 - val_loss: 17.5663 - val_MinusLogProbMetric: 17.5663 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 227/1000
2023-09-26 17:36:42.509 
Epoch 227/1000 
	 loss: 17.1092, MinusLogProbMetric: 17.1092, val_loss: 17.5438, val_MinusLogProbMetric: 17.5438

Epoch 227: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.1092 - MinusLogProbMetric: 17.1092 - val_loss: 17.5438 - val_MinusLogProbMetric: 17.5438 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 228/1000
2023-09-26 17:37:46.822 
Epoch 228/1000 
	 loss: 17.0362, MinusLogProbMetric: 17.0362, val_loss: 17.3482, val_MinusLogProbMetric: 17.3482

Epoch 228: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0362 - MinusLogProbMetric: 17.0362 - val_loss: 17.3482 - val_MinusLogProbMetric: 17.3482 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 229/1000
2023-09-26 17:38:51.223 
Epoch 229/1000 
	 loss: 17.0292, MinusLogProbMetric: 17.0292, val_loss: 17.5417, val_MinusLogProbMetric: 17.5417

Epoch 229: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0292 - MinusLogProbMetric: 17.0292 - val_loss: 17.5417 - val_MinusLogProbMetric: 17.5417 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 230/1000
2023-09-26 17:39:55.577 
Epoch 230/1000 
	 loss: 17.0298, MinusLogProbMetric: 17.0298, val_loss: 17.6097, val_MinusLogProbMetric: 17.6097

Epoch 230: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0298 - MinusLogProbMetric: 17.0298 - val_loss: 17.6097 - val_MinusLogProbMetric: 17.6097 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 231/1000
2023-09-26 17:40:59.781 
Epoch 231/1000 
	 loss: 17.0858, MinusLogProbMetric: 17.0858, val_loss: 17.3181, val_MinusLogProbMetric: 17.3181

Epoch 231: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0858 - MinusLogProbMetric: 17.0858 - val_loss: 17.3181 - val_MinusLogProbMetric: 17.3181 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 232/1000
2023-09-26 17:42:04.209 
Epoch 232/1000 
	 loss: 17.0339, MinusLogProbMetric: 17.0339, val_loss: 17.3031, val_MinusLogProbMetric: 17.3031

Epoch 232: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0339 - MinusLogProbMetric: 17.0339 - val_loss: 17.3031 - val_MinusLogProbMetric: 17.3031 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 233/1000
2023-09-26 17:43:08.608 
Epoch 233/1000 
	 loss: 17.0285, MinusLogProbMetric: 17.0285, val_loss: 17.3114, val_MinusLogProbMetric: 17.3114

Epoch 233: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0285 - MinusLogProbMetric: 17.0285 - val_loss: 17.3114 - val_MinusLogProbMetric: 17.3114 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 234/1000
2023-09-26 17:44:13.453 
Epoch 234/1000 
	 loss: 17.0530, MinusLogProbMetric: 17.0530, val_loss: 17.5308, val_MinusLogProbMetric: 17.5308

Epoch 234: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0530 - MinusLogProbMetric: 17.0530 - val_loss: 17.5308 - val_MinusLogProbMetric: 17.5308 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 235/1000
2023-09-26 17:45:18.143 
Epoch 235/1000 
	 loss: 17.0458, MinusLogProbMetric: 17.0458, val_loss: 17.2655, val_MinusLogProbMetric: 17.2655

Epoch 235: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0458 - MinusLogProbMetric: 17.0458 - val_loss: 17.2655 - val_MinusLogProbMetric: 17.2655 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 236/1000
2023-09-26 17:46:22.744 
Epoch 236/1000 
	 loss: 16.9971, MinusLogProbMetric: 16.9971, val_loss: 17.3839, val_MinusLogProbMetric: 17.3839

Epoch 236: val_loss did not improve from 17.24405
196/196 - 65s - loss: 16.9971 - MinusLogProbMetric: 16.9971 - val_loss: 17.3839 - val_MinusLogProbMetric: 17.3839 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 237/1000
2023-09-26 17:47:26.830 
Epoch 237/1000 
	 loss: 17.0536, MinusLogProbMetric: 17.0536, val_loss: 17.5098, val_MinusLogProbMetric: 17.5098

Epoch 237: val_loss did not improve from 17.24405
196/196 - 64s - loss: 17.0536 - MinusLogProbMetric: 17.0536 - val_loss: 17.5098 - val_MinusLogProbMetric: 17.5098 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 238/1000
2023-09-26 17:48:31.829 
Epoch 238/1000 
	 loss: 17.0224, MinusLogProbMetric: 17.0224, val_loss: 17.5812, val_MinusLogProbMetric: 17.5812

Epoch 238: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0224 - MinusLogProbMetric: 17.0224 - val_loss: 17.5812 - val_MinusLogProbMetric: 17.5812 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 239/1000
2023-09-26 17:49:36.340 
Epoch 239/1000 
	 loss: 17.0279, MinusLogProbMetric: 17.0279, val_loss: 17.4365, val_MinusLogProbMetric: 17.4365

Epoch 239: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0279 - MinusLogProbMetric: 17.0279 - val_loss: 17.4365 - val_MinusLogProbMetric: 17.4365 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 240/1000
2023-09-26 17:50:41.312 
Epoch 240/1000 
	 loss: 17.0253, MinusLogProbMetric: 17.0253, val_loss: 17.7232, val_MinusLogProbMetric: 17.7232

Epoch 240: val_loss did not improve from 17.24405
196/196 - 65s - loss: 17.0253 - MinusLogProbMetric: 17.0253 - val_loss: 17.7232 - val_MinusLogProbMetric: 17.7232 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 241/1000
2023-09-26 17:51:45.590 
Epoch 241/1000 
	 loss: 16.7768, MinusLogProbMetric: 16.7768, val_loss: 17.3116, val_MinusLogProbMetric: 17.3116

Epoch 241: val_loss did not improve from 17.24405
196/196 - 64s - loss: 16.7768 - MinusLogProbMetric: 16.7768 - val_loss: 17.3116 - val_MinusLogProbMetric: 17.3116 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 242/1000
2023-09-26 17:52:49.838 
Epoch 242/1000 
	 loss: 16.7547, MinusLogProbMetric: 16.7547, val_loss: 17.3098, val_MinusLogProbMetric: 17.3098

Epoch 242: val_loss did not improve from 17.24405
196/196 - 64s - loss: 16.7547 - MinusLogProbMetric: 16.7547 - val_loss: 17.3098 - val_MinusLogProbMetric: 17.3098 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 243/1000
2023-09-26 17:53:54.880 
Epoch 243/1000 
	 loss: 16.7544, MinusLogProbMetric: 16.7544, val_loss: 17.0939, val_MinusLogProbMetric: 17.0939

Epoch 243: val_loss improved from 17.24405 to 17.09386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.7544 - MinusLogProbMetric: 16.7544 - val_loss: 17.0939 - val_MinusLogProbMetric: 17.0939 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 244/1000
2023-09-26 17:54:59.860 
Epoch 244/1000 
	 loss: 16.7464, MinusLogProbMetric: 16.7464, val_loss: 17.0811, val_MinusLogProbMetric: 17.0811

Epoch 244: val_loss improved from 17.09386 to 17.08109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.7464 - MinusLogProbMetric: 16.7464 - val_loss: 17.0811 - val_MinusLogProbMetric: 17.0811 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 245/1000
2023-09-26 17:56:05.556 
Epoch 245/1000 
	 loss: 16.7407, MinusLogProbMetric: 16.7407, val_loss: 17.2291, val_MinusLogProbMetric: 17.2291

Epoch 245: val_loss did not improve from 17.08109
196/196 - 65s - loss: 16.7407 - MinusLogProbMetric: 16.7407 - val_loss: 17.2291 - val_MinusLogProbMetric: 17.2291 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 246/1000
2023-09-26 17:57:10.254 
Epoch 246/1000 
	 loss: 16.7664, MinusLogProbMetric: 16.7664, val_loss: 17.3161, val_MinusLogProbMetric: 17.3161

Epoch 246: val_loss did not improve from 17.08109
196/196 - 65s - loss: 16.7664 - MinusLogProbMetric: 16.7664 - val_loss: 17.3161 - val_MinusLogProbMetric: 17.3161 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 247/1000
2023-09-26 17:58:14.933 
Epoch 247/1000 
	 loss: 16.7372, MinusLogProbMetric: 16.7372, val_loss: 17.0064, val_MinusLogProbMetric: 17.0064

Epoch 247: val_loss improved from 17.08109 to 17.00644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.7372 - MinusLogProbMetric: 16.7372 - val_loss: 17.0064 - val_MinusLogProbMetric: 17.0064 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 248/1000
2023-09-26 17:59:21.039 
Epoch 248/1000 
	 loss: 16.7548, MinusLogProbMetric: 16.7548, val_loss: 17.1869, val_MinusLogProbMetric: 17.1869

Epoch 248: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7548 - MinusLogProbMetric: 16.7548 - val_loss: 17.1869 - val_MinusLogProbMetric: 17.1869 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 249/1000
2023-09-26 18:00:25.976 
Epoch 249/1000 
	 loss: 16.7571, MinusLogProbMetric: 16.7571, val_loss: 17.1295, val_MinusLogProbMetric: 17.1295

Epoch 249: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7571 - MinusLogProbMetric: 16.7571 - val_loss: 17.1295 - val_MinusLogProbMetric: 17.1295 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 250/1000
2023-09-26 18:01:31.083 
Epoch 250/1000 
	 loss: 16.7272, MinusLogProbMetric: 16.7272, val_loss: 17.1544, val_MinusLogProbMetric: 17.1544

Epoch 250: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7272 - MinusLogProbMetric: 16.7272 - val_loss: 17.1544 - val_MinusLogProbMetric: 17.1544 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 251/1000
2023-09-26 18:02:35.425 
Epoch 251/1000 
	 loss: 16.7604, MinusLogProbMetric: 16.7604, val_loss: 17.1494, val_MinusLogProbMetric: 17.1494

Epoch 251: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7604 - MinusLogProbMetric: 16.7604 - val_loss: 17.1494 - val_MinusLogProbMetric: 17.1494 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 252/1000
2023-09-26 18:03:40.383 
Epoch 252/1000 
	 loss: 16.7498, MinusLogProbMetric: 16.7498, val_loss: 17.4135, val_MinusLogProbMetric: 17.4135

Epoch 252: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7498 - MinusLogProbMetric: 16.7498 - val_loss: 17.4135 - val_MinusLogProbMetric: 17.4135 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 253/1000
2023-09-26 18:04:45.166 
Epoch 253/1000 
	 loss: 16.7384, MinusLogProbMetric: 16.7384, val_loss: 17.0965, val_MinusLogProbMetric: 17.0965

Epoch 253: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7384 - MinusLogProbMetric: 16.7384 - val_loss: 17.0965 - val_MinusLogProbMetric: 17.0965 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 254/1000
2023-09-26 18:05:50.367 
Epoch 254/1000 
	 loss: 16.7352, MinusLogProbMetric: 16.7352, val_loss: 17.0537, val_MinusLogProbMetric: 17.0537

Epoch 254: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7352 - MinusLogProbMetric: 16.7352 - val_loss: 17.0537 - val_MinusLogProbMetric: 17.0537 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 255/1000
2023-09-26 18:06:55.379 
Epoch 255/1000 
	 loss: 16.7158, MinusLogProbMetric: 16.7158, val_loss: 17.3580, val_MinusLogProbMetric: 17.3580

Epoch 255: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7158 - MinusLogProbMetric: 16.7158 - val_loss: 17.3580 - val_MinusLogProbMetric: 17.3580 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 256/1000
2023-09-26 18:08:00.158 
Epoch 256/1000 
	 loss: 16.7514, MinusLogProbMetric: 16.7514, val_loss: 17.1628, val_MinusLogProbMetric: 17.1628

Epoch 256: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7514 - MinusLogProbMetric: 16.7514 - val_loss: 17.1628 - val_MinusLogProbMetric: 17.1628 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 257/1000
2023-09-26 18:09:05.376 
Epoch 257/1000 
	 loss: 16.7156, MinusLogProbMetric: 16.7156, val_loss: 17.1214, val_MinusLogProbMetric: 17.1214

Epoch 257: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7156 - MinusLogProbMetric: 16.7156 - val_loss: 17.1214 - val_MinusLogProbMetric: 17.1214 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 258/1000
2023-09-26 18:10:09.962 
Epoch 258/1000 
	 loss: 16.7530, MinusLogProbMetric: 16.7530, val_loss: 17.1866, val_MinusLogProbMetric: 17.1866

Epoch 258: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7530 - MinusLogProbMetric: 16.7530 - val_loss: 17.1866 - val_MinusLogProbMetric: 17.1866 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 259/1000
2023-09-26 18:11:14.365 
Epoch 259/1000 
	 loss: 16.7349, MinusLogProbMetric: 16.7349, val_loss: 17.1249, val_MinusLogProbMetric: 17.1249

Epoch 259: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7349 - MinusLogProbMetric: 16.7349 - val_loss: 17.1249 - val_MinusLogProbMetric: 17.1249 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 260/1000
2023-09-26 18:12:18.512 
Epoch 260/1000 
	 loss: 16.7442, MinusLogProbMetric: 16.7442, val_loss: 17.1487, val_MinusLogProbMetric: 17.1487

Epoch 260: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7442 - MinusLogProbMetric: 16.7442 - val_loss: 17.1487 - val_MinusLogProbMetric: 17.1487 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 261/1000
2023-09-26 18:13:23.455 
Epoch 261/1000 
	 loss: 16.7400, MinusLogProbMetric: 16.7400, val_loss: 17.1129, val_MinusLogProbMetric: 17.1129

Epoch 261: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7400 - MinusLogProbMetric: 16.7400 - val_loss: 17.1129 - val_MinusLogProbMetric: 17.1129 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 262/1000
2023-09-26 18:14:27.764 
Epoch 262/1000 
	 loss: 16.7368, MinusLogProbMetric: 16.7368, val_loss: 17.0940, val_MinusLogProbMetric: 17.0940

Epoch 262: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7368 - MinusLogProbMetric: 16.7368 - val_loss: 17.0940 - val_MinusLogProbMetric: 17.0940 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 263/1000
2023-09-26 18:15:32.443 
Epoch 263/1000 
	 loss: 16.7760, MinusLogProbMetric: 16.7760, val_loss: 17.2082, val_MinusLogProbMetric: 17.2082

Epoch 263: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7760 - MinusLogProbMetric: 16.7760 - val_loss: 17.2082 - val_MinusLogProbMetric: 17.2082 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 264/1000
2023-09-26 18:16:37.453 
Epoch 264/1000 
	 loss: 16.7750, MinusLogProbMetric: 16.7750, val_loss: 17.2226, val_MinusLogProbMetric: 17.2226

Epoch 264: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7750 - MinusLogProbMetric: 16.7750 - val_loss: 17.2226 - val_MinusLogProbMetric: 17.2226 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 265/1000
2023-09-26 18:17:42.236 
Epoch 265/1000 
	 loss: 16.7354, MinusLogProbMetric: 16.7354, val_loss: 17.8581, val_MinusLogProbMetric: 17.8581

Epoch 265: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7354 - MinusLogProbMetric: 16.7354 - val_loss: 17.8581 - val_MinusLogProbMetric: 17.8581 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 266/1000
2023-09-26 18:18:47.169 
Epoch 266/1000 
	 loss: 16.7291, MinusLogProbMetric: 16.7291, val_loss: 17.4749, val_MinusLogProbMetric: 17.4749

Epoch 266: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7291 - MinusLogProbMetric: 16.7291 - val_loss: 17.4749 - val_MinusLogProbMetric: 17.4749 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 267/1000
2023-09-26 18:19:51.771 
Epoch 267/1000 
	 loss: 16.7384, MinusLogProbMetric: 16.7384, val_loss: 17.1403, val_MinusLogProbMetric: 17.1403

Epoch 267: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7384 - MinusLogProbMetric: 16.7384 - val_loss: 17.1403 - val_MinusLogProbMetric: 17.1403 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 268/1000
2023-09-26 18:20:56.150 
Epoch 268/1000 
	 loss: 16.7799, MinusLogProbMetric: 16.7799, val_loss: 17.2701, val_MinusLogProbMetric: 17.2701

Epoch 268: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7799 - MinusLogProbMetric: 16.7799 - val_loss: 17.2701 - val_MinusLogProbMetric: 17.2701 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 269/1000
2023-09-26 18:22:00.853 
Epoch 269/1000 
	 loss: 16.7203, MinusLogProbMetric: 16.7203, val_loss: 17.2518, val_MinusLogProbMetric: 17.2518

Epoch 269: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7203 - MinusLogProbMetric: 16.7203 - val_loss: 17.2518 - val_MinusLogProbMetric: 17.2518 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 270/1000
2023-09-26 18:23:05.585 
Epoch 270/1000 
	 loss: 16.7459, MinusLogProbMetric: 16.7459, val_loss: 17.0975, val_MinusLogProbMetric: 17.0975

Epoch 270: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7459 - MinusLogProbMetric: 16.7459 - val_loss: 17.0975 - val_MinusLogProbMetric: 17.0975 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 271/1000
2023-09-26 18:24:10.384 
Epoch 271/1000 
	 loss: 16.7252, MinusLogProbMetric: 16.7252, val_loss: 17.3586, val_MinusLogProbMetric: 17.3586

Epoch 271: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7252 - MinusLogProbMetric: 16.7252 - val_loss: 17.3586 - val_MinusLogProbMetric: 17.3586 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 272/1000
2023-09-26 18:25:15.291 
Epoch 272/1000 
	 loss: 16.7590, MinusLogProbMetric: 16.7590, val_loss: 17.0772, val_MinusLogProbMetric: 17.0772

Epoch 272: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7590 - MinusLogProbMetric: 16.7590 - val_loss: 17.0772 - val_MinusLogProbMetric: 17.0772 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 273/1000
2023-09-26 18:26:20.240 
Epoch 273/1000 
	 loss: 16.7291, MinusLogProbMetric: 16.7291, val_loss: 17.1922, val_MinusLogProbMetric: 17.1922

Epoch 273: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7291 - MinusLogProbMetric: 16.7291 - val_loss: 17.1922 - val_MinusLogProbMetric: 17.1922 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 274/1000
2023-09-26 18:27:25.387 
Epoch 274/1000 
	 loss: 16.7164, MinusLogProbMetric: 16.7164, val_loss: 17.0783, val_MinusLogProbMetric: 17.0783

Epoch 274: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7164 - MinusLogProbMetric: 16.7164 - val_loss: 17.0783 - val_MinusLogProbMetric: 17.0783 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 275/1000
2023-09-26 18:28:30.160 
Epoch 275/1000 
	 loss: 16.7646, MinusLogProbMetric: 16.7646, val_loss: 17.2750, val_MinusLogProbMetric: 17.2750

Epoch 275: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7646 - MinusLogProbMetric: 16.7646 - val_loss: 17.2750 - val_MinusLogProbMetric: 17.2750 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 276/1000
2023-09-26 18:29:34.604 
Epoch 276/1000 
	 loss: 16.6999, MinusLogProbMetric: 16.6999, val_loss: 17.1038, val_MinusLogProbMetric: 17.1038

Epoch 276: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.6999 - MinusLogProbMetric: 16.6999 - val_loss: 17.1038 - val_MinusLogProbMetric: 17.1038 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 277/1000
2023-09-26 18:30:39.189 
Epoch 277/1000 
	 loss: 16.7460, MinusLogProbMetric: 16.7460, val_loss: 17.1676, val_MinusLogProbMetric: 17.1676

Epoch 277: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7460 - MinusLogProbMetric: 16.7460 - val_loss: 17.1676 - val_MinusLogProbMetric: 17.1676 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 278/1000
2023-09-26 18:31:43.433 
Epoch 278/1000 
	 loss: 16.7318, MinusLogProbMetric: 16.7318, val_loss: 17.0283, val_MinusLogProbMetric: 17.0283

Epoch 278: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7318 - MinusLogProbMetric: 16.7318 - val_loss: 17.0283 - val_MinusLogProbMetric: 17.0283 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 279/1000
2023-09-26 18:32:48.274 
Epoch 279/1000 
	 loss: 16.7589, MinusLogProbMetric: 16.7589, val_loss: 17.3027, val_MinusLogProbMetric: 17.3027

Epoch 279: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7589 - MinusLogProbMetric: 16.7589 - val_loss: 17.3027 - val_MinusLogProbMetric: 17.3027 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 280/1000
2023-09-26 18:33:52.547 
Epoch 280/1000 
	 loss: 16.7358, MinusLogProbMetric: 16.7358, val_loss: 17.1134, val_MinusLogProbMetric: 17.1134

Epoch 280: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7358 - MinusLogProbMetric: 16.7358 - val_loss: 17.1134 - val_MinusLogProbMetric: 17.1134 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 281/1000
2023-09-26 18:34:57.127 
Epoch 281/1000 
	 loss: 16.7232, MinusLogProbMetric: 16.7232, val_loss: 17.0939, val_MinusLogProbMetric: 17.0939

Epoch 281: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7232 - MinusLogProbMetric: 16.7232 - val_loss: 17.0939 - val_MinusLogProbMetric: 17.0939 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 282/1000
2023-09-26 18:36:01.331 
Epoch 282/1000 
	 loss: 16.6810, MinusLogProbMetric: 16.6810, val_loss: 17.1786, val_MinusLogProbMetric: 17.1786

Epoch 282: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.6810 - MinusLogProbMetric: 16.6810 - val_loss: 17.1786 - val_MinusLogProbMetric: 17.1786 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 283/1000
2023-09-26 18:37:06.006 
Epoch 283/1000 
	 loss: 16.7415, MinusLogProbMetric: 16.7415, val_loss: 17.4621, val_MinusLogProbMetric: 17.4621

Epoch 283: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7415 - MinusLogProbMetric: 16.7415 - val_loss: 17.4621 - val_MinusLogProbMetric: 17.4621 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 284/1000
2023-09-26 18:38:10.659 
Epoch 284/1000 
	 loss: 16.7270, MinusLogProbMetric: 16.7270, val_loss: 17.2505, val_MinusLogProbMetric: 17.2505

Epoch 284: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7270 - MinusLogProbMetric: 16.7270 - val_loss: 17.2505 - val_MinusLogProbMetric: 17.2505 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 285/1000
2023-09-26 18:39:15.236 
Epoch 285/1000 
	 loss: 16.7372, MinusLogProbMetric: 16.7372, val_loss: 17.1267, val_MinusLogProbMetric: 17.1267

Epoch 285: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7372 - MinusLogProbMetric: 16.7372 - val_loss: 17.1267 - val_MinusLogProbMetric: 17.1267 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 286/1000
2023-09-26 18:40:19.367 
Epoch 286/1000 
	 loss: 16.6972, MinusLogProbMetric: 16.6972, val_loss: 17.1182, val_MinusLogProbMetric: 17.1182

Epoch 286: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.6972 - MinusLogProbMetric: 16.6972 - val_loss: 17.1182 - val_MinusLogProbMetric: 17.1182 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 287/1000
2023-09-26 18:41:23.774 
Epoch 287/1000 
	 loss: 16.7175, MinusLogProbMetric: 16.7175, val_loss: 17.3099, val_MinusLogProbMetric: 17.3099

Epoch 287: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7175 - MinusLogProbMetric: 16.7175 - val_loss: 17.3099 - val_MinusLogProbMetric: 17.3099 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 288/1000
2023-09-26 18:42:28.703 
Epoch 288/1000 
	 loss: 16.7258, MinusLogProbMetric: 16.7258, val_loss: 17.1241, val_MinusLogProbMetric: 17.1241

Epoch 288: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7258 - MinusLogProbMetric: 16.7258 - val_loss: 17.1241 - val_MinusLogProbMetric: 17.1241 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 289/1000
2023-09-26 18:43:33.455 
Epoch 289/1000 
	 loss: 16.7094, MinusLogProbMetric: 16.7094, val_loss: 17.1788, val_MinusLogProbMetric: 17.1788

Epoch 289: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7094 - MinusLogProbMetric: 16.7094 - val_loss: 17.1788 - val_MinusLogProbMetric: 17.1788 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 290/1000
2023-09-26 18:44:37.932 
Epoch 290/1000 
	 loss: 16.7138, MinusLogProbMetric: 16.7138, val_loss: 17.2369, val_MinusLogProbMetric: 17.2369

Epoch 290: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7138 - MinusLogProbMetric: 16.7138 - val_loss: 17.2369 - val_MinusLogProbMetric: 17.2369 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 291/1000
2023-09-26 18:45:41.944 
Epoch 291/1000 
	 loss: 16.7123, MinusLogProbMetric: 16.7123, val_loss: 17.0812, val_MinusLogProbMetric: 17.0812

Epoch 291: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7123 - MinusLogProbMetric: 16.7123 - val_loss: 17.0812 - val_MinusLogProbMetric: 17.0812 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 292/1000
2023-09-26 18:46:46.769 
Epoch 292/1000 
	 loss: 16.7364, MinusLogProbMetric: 16.7364, val_loss: 17.1598, val_MinusLogProbMetric: 17.1598

Epoch 292: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7364 - MinusLogProbMetric: 16.7364 - val_loss: 17.1598 - val_MinusLogProbMetric: 17.1598 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 293/1000
2023-09-26 18:47:51.126 
Epoch 293/1000 
	 loss: 16.7228, MinusLogProbMetric: 16.7228, val_loss: 17.0854, val_MinusLogProbMetric: 17.0854

Epoch 293: val_loss did not improve from 17.00644
196/196 - 64s - loss: 16.7228 - MinusLogProbMetric: 16.7228 - val_loss: 17.0854 - val_MinusLogProbMetric: 17.0854 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 294/1000
2023-09-26 18:48:55.758 
Epoch 294/1000 
	 loss: 16.7012, MinusLogProbMetric: 16.7012, val_loss: 17.0350, val_MinusLogProbMetric: 17.0350

Epoch 294: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7012 - MinusLogProbMetric: 16.7012 - val_loss: 17.0350 - val_MinusLogProbMetric: 17.0350 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 295/1000
2023-09-26 18:50:00.934 
Epoch 295/1000 
	 loss: 16.7077, MinusLogProbMetric: 16.7077, val_loss: 17.0296, val_MinusLogProbMetric: 17.0296

Epoch 295: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7077 - MinusLogProbMetric: 16.7077 - val_loss: 17.0296 - val_MinusLogProbMetric: 17.0296 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 296/1000
2023-09-26 18:51:05.560 
Epoch 296/1000 
	 loss: 16.7037, MinusLogProbMetric: 16.7037, val_loss: 17.1862, val_MinusLogProbMetric: 17.1862

Epoch 296: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7037 - MinusLogProbMetric: 16.7037 - val_loss: 17.1862 - val_MinusLogProbMetric: 17.1862 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 297/1000
2023-09-26 18:52:10.142 
Epoch 297/1000 
	 loss: 16.7227, MinusLogProbMetric: 16.7227, val_loss: 17.3973, val_MinusLogProbMetric: 17.3973

Epoch 297: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.7227 - MinusLogProbMetric: 16.7227 - val_loss: 17.3973 - val_MinusLogProbMetric: 17.3973 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 298/1000
2023-09-26 18:53:14.826 
Epoch 298/1000 
	 loss: 16.5851, MinusLogProbMetric: 16.5851, val_loss: 17.0214, val_MinusLogProbMetric: 17.0214

Epoch 298: val_loss did not improve from 17.00644
196/196 - 65s - loss: 16.5851 - MinusLogProbMetric: 16.5851 - val_loss: 17.0214 - val_MinusLogProbMetric: 17.0214 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 299/1000
2023-09-26 18:54:19.304 
Epoch 299/1000 
	 loss: 16.5986, MinusLogProbMetric: 16.5986, val_loss: 16.9995, val_MinusLogProbMetric: 16.9995

Epoch 299: val_loss improved from 17.00644 to 16.99950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.5986 - MinusLogProbMetric: 16.5986 - val_loss: 16.9995 - val_MinusLogProbMetric: 16.9995 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 300/1000
2023-09-26 18:55:24.982 
Epoch 300/1000 
	 loss: 16.5485, MinusLogProbMetric: 16.5485, val_loss: 16.9865, val_MinusLogProbMetric: 16.9865

Epoch 300: val_loss improved from 16.99950 to 16.98650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.5485 - MinusLogProbMetric: 16.5485 - val_loss: 16.9865 - val_MinusLogProbMetric: 16.9865 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 301/1000
2023-09-26 18:56:30.606 
Epoch 301/1000 
	 loss: 16.5564, MinusLogProbMetric: 16.5564, val_loss: 16.9445, val_MinusLogProbMetric: 16.9445

Epoch 301: val_loss improved from 16.98650 to 16.94447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.5564 - MinusLogProbMetric: 16.5564 - val_loss: 16.9445 - val_MinusLogProbMetric: 16.9445 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 302/1000
2023-09-26 18:57:36.271 
Epoch 302/1000 
	 loss: 16.5778, MinusLogProbMetric: 16.5778, val_loss: 17.2021, val_MinusLogProbMetric: 17.2021

Epoch 302: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5778 - MinusLogProbMetric: 16.5778 - val_loss: 17.2021 - val_MinusLogProbMetric: 17.2021 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 303/1000
2023-09-26 18:58:40.825 
Epoch 303/1000 
	 loss: 16.5576, MinusLogProbMetric: 16.5576, val_loss: 16.9876, val_MinusLogProbMetric: 16.9876

Epoch 303: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5576 - MinusLogProbMetric: 16.5576 - val_loss: 16.9876 - val_MinusLogProbMetric: 16.9876 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 304/1000
2023-09-26 18:59:45.312 
Epoch 304/1000 
	 loss: 16.5844, MinusLogProbMetric: 16.5844, val_loss: 17.0029, val_MinusLogProbMetric: 17.0029

Epoch 304: val_loss did not improve from 16.94447
196/196 - 64s - loss: 16.5844 - MinusLogProbMetric: 16.5844 - val_loss: 17.0029 - val_MinusLogProbMetric: 17.0029 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 305/1000
2023-09-26 19:00:50.157 
Epoch 305/1000 
	 loss: 16.5774, MinusLogProbMetric: 16.5774, val_loss: 16.9697, val_MinusLogProbMetric: 16.9697

Epoch 305: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5774 - MinusLogProbMetric: 16.5774 - val_loss: 16.9697 - val_MinusLogProbMetric: 16.9697 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 306/1000
2023-09-26 19:01:54.903 
Epoch 306/1000 
	 loss: 16.5560, MinusLogProbMetric: 16.5560, val_loss: 17.0299, val_MinusLogProbMetric: 17.0299

Epoch 306: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5560 - MinusLogProbMetric: 16.5560 - val_loss: 17.0299 - val_MinusLogProbMetric: 17.0299 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 307/1000
2023-09-26 19:02:59.760 
Epoch 307/1000 
	 loss: 16.5907, MinusLogProbMetric: 16.5907, val_loss: 17.0976, val_MinusLogProbMetric: 17.0976

Epoch 307: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5907 - MinusLogProbMetric: 16.5907 - val_loss: 17.0976 - val_MinusLogProbMetric: 17.0976 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 308/1000
2023-09-26 19:04:04.484 
Epoch 308/1000 
	 loss: 16.5745, MinusLogProbMetric: 16.5745, val_loss: 16.9609, val_MinusLogProbMetric: 16.9609

Epoch 308: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5745 - MinusLogProbMetric: 16.5745 - val_loss: 16.9609 - val_MinusLogProbMetric: 16.9609 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 309/1000
2023-09-26 19:05:09.260 
Epoch 309/1000 
	 loss: 16.5726, MinusLogProbMetric: 16.5726, val_loss: 17.0038, val_MinusLogProbMetric: 17.0038

Epoch 309: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5726 - MinusLogProbMetric: 16.5726 - val_loss: 17.0038 - val_MinusLogProbMetric: 17.0038 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 310/1000
2023-09-26 19:06:13.986 
Epoch 310/1000 
	 loss: 16.5785, MinusLogProbMetric: 16.5785, val_loss: 16.9628, val_MinusLogProbMetric: 16.9628

Epoch 310: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5785 - MinusLogProbMetric: 16.5785 - val_loss: 16.9628 - val_MinusLogProbMetric: 16.9628 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 311/1000
2023-09-26 19:07:18.411 
Epoch 311/1000 
	 loss: 16.5700, MinusLogProbMetric: 16.5700, val_loss: 16.9911, val_MinusLogProbMetric: 16.9911

Epoch 311: val_loss did not improve from 16.94447
196/196 - 64s - loss: 16.5700 - MinusLogProbMetric: 16.5700 - val_loss: 16.9911 - val_MinusLogProbMetric: 16.9911 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 312/1000
2023-09-26 19:08:22.881 
Epoch 312/1000 
	 loss: 16.5614, MinusLogProbMetric: 16.5614, val_loss: 16.9750, val_MinusLogProbMetric: 16.9750

Epoch 312: val_loss did not improve from 16.94447
196/196 - 64s - loss: 16.5614 - MinusLogProbMetric: 16.5614 - val_loss: 16.9750 - val_MinusLogProbMetric: 16.9750 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 313/1000
2023-09-26 19:09:27.317 
Epoch 313/1000 
	 loss: 16.5675, MinusLogProbMetric: 16.5675, val_loss: 16.9663, val_MinusLogProbMetric: 16.9663

Epoch 313: val_loss did not improve from 16.94447
196/196 - 64s - loss: 16.5675 - MinusLogProbMetric: 16.5675 - val_loss: 16.9663 - val_MinusLogProbMetric: 16.9663 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 314/1000
2023-09-26 19:10:31.938 
Epoch 314/1000 
	 loss: 16.5764, MinusLogProbMetric: 16.5764, val_loss: 16.9555, val_MinusLogProbMetric: 16.9555

Epoch 314: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5764 - MinusLogProbMetric: 16.5764 - val_loss: 16.9555 - val_MinusLogProbMetric: 16.9555 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 315/1000
2023-09-26 19:11:36.976 
Epoch 315/1000 
	 loss: 16.5376, MinusLogProbMetric: 16.5376, val_loss: 17.0376, val_MinusLogProbMetric: 17.0376

Epoch 315: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5376 - MinusLogProbMetric: 16.5376 - val_loss: 17.0376 - val_MinusLogProbMetric: 17.0376 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 316/1000
2023-09-26 19:12:42.242 
Epoch 316/1000 
	 loss: 16.5840, MinusLogProbMetric: 16.5840, val_loss: 17.0369, val_MinusLogProbMetric: 17.0369

Epoch 316: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5840 - MinusLogProbMetric: 16.5840 - val_loss: 17.0369 - val_MinusLogProbMetric: 17.0369 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 317/1000
2023-09-26 19:13:47.429 
Epoch 317/1000 
	 loss: 16.5592, MinusLogProbMetric: 16.5592, val_loss: 16.9869, val_MinusLogProbMetric: 16.9869

Epoch 317: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5592 - MinusLogProbMetric: 16.5592 - val_loss: 16.9869 - val_MinusLogProbMetric: 16.9869 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 318/1000
2023-09-26 19:14:52.286 
Epoch 318/1000 
	 loss: 16.5625, MinusLogProbMetric: 16.5625, val_loss: 17.0090, val_MinusLogProbMetric: 17.0090

Epoch 318: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5625 - MinusLogProbMetric: 16.5625 - val_loss: 17.0090 - val_MinusLogProbMetric: 17.0090 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 319/1000
2023-09-26 19:15:56.904 
Epoch 319/1000 
	 loss: 16.5602, MinusLogProbMetric: 16.5602, val_loss: 17.3880, val_MinusLogProbMetric: 17.3880

Epoch 319: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5602 - MinusLogProbMetric: 16.5602 - val_loss: 17.3880 - val_MinusLogProbMetric: 17.3880 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 320/1000
2023-09-26 19:17:01.705 
Epoch 320/1000 
	 loss: 16.5894, MinusLogProbMetric: 16.5894, val_loss: 17.4268, val_MinusLogProbMetric: 17.4268

Epoch 320: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5894 - MinusLogProbMetric: 16.5894 - val_loss: 17.4268 - val_MinusLogProbMetric: 17.4268 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 321/1000
2023-09-26 19:18:04.994 
Epoch 321/1000 
	 loss: 16.6031, MinusLogProbMetric: 16.6031, val_loss: 17.1301, val_MinusLogProbMetric: 17.1301

Epoch 321: val_loss did not improve from 16.94447
196/196 - 63s - loss: 16.6031 - MinusLogProbMetric: 16.6031 - val_loss: 17.1301 - val_MinusLogProbMetric: 17.1301 - lr: 8.3333e-05 - 63s/epoch - 323ms/step
Epoch 322/1000
2023-09-26 19:19:04.486 
Epoch 322/1000 
	 loss: 16.5647, MinusLogProbMetric: 16.5647, val_loss: 17.2639, val_MinusLogProbMetric: 17.2639

Epoch 322: val_loss did not improve from 16.94447
196/196 - 59s - loss: 16.5647 - MinusLogProbMetric: 16.5647 - val_loss: 17.2639 - val_MinusLogProbMetric: 17.2639 - lr: 8.3333e-05 - 59s/epoch - 304ms/step
Epoch 323/1000
2023-09-26 19:19:59.800 
Epoch 323/1000 
	 loss: 16.5578, MinusLogProbMetric: 16.5578, val_loss: 16.9546, val_MinusLogProbMetric: 16.9546

Epoch 323: val_loss did not improve from 16.94447
196/196 - 55s - loss: 16.5578 - MinusLogProbMetric: 16.5578 - val_loss: 16.9546 - val_MinusLogProbMetric: 16.9546 - lr: 8.3333e-05 - 55s/epoch - 282ms/step
Epoch 324/1000
2023-09-26 19:21:02.218 
Epoch 324/1000 
	 loss: 16.5679, MinusLogProbMetric: 16.5679, val_loss: 17.0215, val_MinusLogProbMetric: 17.0215

Epoch 324: val_loss did not improve from 16.94447
196/196 - 62s - loss: 16.5679 - MinusLogProbMetric: 16.5679 - val_loss: 17.0215 - val_MinusLogProbMetric: 17.0215 - lr: 8.3333e-05 - 62s/epoch - 318ms/step
Epoch 325/1000
2023-09-26 19:22:01.467 
Epoch 325/1000 
	 loss: 16.5582, MinusLogProbMetric: 16.5582, val_loss: 17.0224, val_MinusLogProbMetric: 17.0224

Epoch 325: val_loss did not improve from 16.94447
196/196 - 59s - loss: 16.5582 - MinusLogProbMetric: 16.5582 - val_loss: 17.0224 - val_MinusLogProbMetric: 17.0224 - lr: 8.3333e-05 - 59s/epoch - 302ms/step
Epoch 326/1000
2023-09-26 19:22:57.981 
Epoch 326/1000 
	 loss: 16.5608, MinusLogProbMetric: 16.5608, val_loss: 16.9742, val_MinusLogProbMetric: 16.9742

Epoch 326: val_loss did not improve from 16.94447
196/196 - 57s - loss: 16.5608 - MinusLogProbMetric: 16.5608 - val_loss: 16.9742 - val_MinusLogProbMetric: 16.9742 - lr: 8.3333e-05 - 57s/epoch - 288ms/step
Epoch 327/1000
2023-09-26 19:23:56.482 
Epoch 327/1000 
	 loss: 16.5566, MinusLogProbMetric: 16.5566, val_loss: 16.9455, val_MinusLogProbMetric: 16.9455

Epoch 327: val_loss did not improve from 16.94447
196/196 - 58s - loss: 16.5566 - MinusLogProbMetric: 16.5566 - val_loss: 16.9455 - val_MinusLogProbMetric: 16.9455 - lr: 8.3333e-05 - 58s/epoch - 298ms/step
Epoch 328/1000
2023-09-26 19:25:00.303 
Epoch 328/1000 
	 loss: 16.5663, MinusLogProbMetric: 16.5663, val_loss: 16.9678, val_MinusLogProbMetric: 16.9678

Epoch 328: val_loss did not improve from 16.94447
196/196 - 64s - loss: 16.5663 - MinusLogProbMetric: 16.5663 - val_loss: 16.9678 - val_MinusLogProbMetric: 16.9678 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 329/1000
2023-09-26 19:26:03.754 
Epoch 329/1000 
	 loss: 16.5479, MinusLogProbMetric: 16.5479, val_loss: 16.9642, val_MinusLogProbMetric: 16.9642

Epoch 329: val_loss did not improve from 16.94447
196/196 - 63s - loss: 16.5479 - MinusLogProbMetric: 16.5479 - val_loss: 16.9642 - val_MinusLogProbMetric: 16.9642 - lr: 8.3333e-05 - 63s/epoch - 324ms/step
Epoch 330/1000
2023-09-26 19:27:08.294 
Epoch 330/1000 
	 loss: 16.5724, MinusLogProbMetric: 16.5724, val_loss: 16.9877, val_MinusLogProbMetric: 16.9877

Epoch 330: val_loss did not improve from 16.94447
196/196 - 65s - loss: 16.5724 - MinusLogProbMetric: 16.5724 - val_loss: 16.9877 - val_MinusLogProbMetric: 16.9877 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 331/1000
2023-09-26 19:28:10.429 
Epoch 331/1000 
	 loss: 16.5750, MinusLogProbMetric: 16.5750, val_loss: 17.0133, val_MinusLogProbMetric: 17.0133

Epoch 331: val_loss did not improve from 16.94447
196/196 - 62s - loss: 16.5750 - MinusLogProbMetric: 16.5750 - val_loss: 17.0133 - val_MinusLogProbMetric: 17.0133 - lr: 8.3333e-05 - 62s/epoch - 317ms/step
Epoch 332/1000
2023-09-26 19:29:05.551 
Epoch 332/1000 
	 loss: 16.5472, MinusLogProbMetric: 16.5472, val_loss: 16.9431, val_MinusLogProbMetric: 16.9431

Epoch 332: val_loss improved from 16.94447 to 16.94309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 56s - loss: 16.5472 - MinusLogProbMetric: 16.5472 - val_loss: 16.9431 - val_MinusLogProbMetric: 16.9431 - lr: 8.3333e-05 - 56s/epoch - 285ms/step
Epoch 333/1000
2023-09-26 19:30:05.327 
Epoch 333/1000 
	 loss: 16.6074, MinusLogProbMetric: 16.6074, val_loss: 17.0216, val_MinusLogProbMetric: 17.0216

Epoch 333: val_loss did not improve from 16.94309
196/196 - 59s - loss: 16.6074 - MinusLogProbMetric: 16.6074 - val_loss: 17.0216 - val_MinusLogProbMetric: 17.0216 - lr: 8.3333e-05 - 59s/epoch - 301ms/step
Epoch 334/1000
2023-09-26 19:31:10.026 
Epoch 334/1000 
	 loss: 16.5522, MinusLogProbMetric: 16.5522, val_loss: 16.9988, val_MinusLogProbMetric: 16.9988

Epoch 334: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5522 - MinusLogProbMetric: 16.5522 - val_loss: 16.9988 - val_MinusLogProbMetric: 16.9988 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 335/1000
2023-09-26 19:32:13.267 
Epoch 335/1000 
	 loss: 16.5416, MinusLogProbMetric: 16.5416, val_loss: 16.9854, val_MinusLogProbMetric: 16.9854

Epoch 335: val_loss did not improve from 16.94309
196/196 - 63s - loss: 16.5416 - MinusLogProbMetric: 16.5416 - val_loss: 16.9854 - val_MinusLogProbMetric: 16.9854 - lr: 8.3333e-05 - 63s/epoch - 323ms/step
Epoch 336/1000
2023-09-26 19:33:17.391 
Epoch 336/1000 
	 loss: 16.5434, MinusLogProbMetric: 16.5434, val_loss: 16.9900, val_MinusLogProbMetric: 16.9900

Epoch 336: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5434 - MinusLogProbMetric: 16.5434 - val_loss: 16.9900 - val_MinusLogProbMetric: 16.9900 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 337/1000
2023-09-26 19:34:21.698 
Epoch 337/1000 
	 loss: 16.5859, MinusLogProbMetric: 16.5859, val_loss: 17.0019, val_MinusLogProbMetric: 17.0019

Epoch 337: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5859 - MinusLogProbMetric: 16.5859 - val_loss: 17.0019 - val_MinusLogProbMetric: 17.0019 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 338/1000
2023-09-26 19:35:26.085 
Epoch 338/1000 
	 loss: 16.5476, MinusLogProbMetric: 16.5476, val_loss: 16.9930, val_MinusLogProbMetric: 16.9930

Epoch 338: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5476 - MinusLogProbMetric: 16.5476 - val_loss: 16.9930 - val_MinusLogProbMetric: 16.9930 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 339/1000
2023-09-26 19:36:30.214 
Epoch 339/1000 
	 loss: 16.5612, MinusLogProbMetric: 16.5612, val_loss: 16.9547, val_MinusLogProbMetric: 16.9547

Epoch 339: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5612 - MinusLogProbMetric: 16.5612 - val_loss: 16.9547 - val_MinusLogProbMetric: 16.9547 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 340/1000
2023-09-26 19:37:35.140 
Epoch 340/1000 
	 loss: 16.5649, MinusLogProbMetric: 16.5649, val_loss: 17.0070, val_MinusLogProbMetric: 17.0070

Epoch 340: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5649 - MinusLogProbMetric: 16.5649 - val_loss: 17.0070 - val_MinusLogProbMetric: 17.0070 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 341/1000
2023-09-26 19:38:39.337 
Epoch 341/1000 
	 loss: 16.5798, MinusLogProbMetric: 16.5798, val_loss: 16.9797, val_MinusLogProbMetric: 16.9797

Epoch 341: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5798 - MinusLogProbMetric: 16.5798 - val_loss: 16.9797 - val_MinusLogProbMetric: 16.9797 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 342/1000
2023-09-26 19:39:43.993 
Epoch 342/1000 
	 loss: 16.5519, MinusLogProbMetric: 16.5519, val_loss: 16.9814, val_MinusLogProbMetric: 16.9814

Epoch 342: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5519 - MinusLogProbMetric: 16.5519 - val_loss: 16.9814 - val_MinusLogProbMetric: 16.9814 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 343/1000
2023-09-26 19:40:48.366 
Epoch 343/1000 
	 loss: 16.5681, MinusLogProbMetric: 16.5681, val_loss: 16.9753, val_MinusLogProbMetric: 16.9753

Epoch 343: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5681 - MinusLogProbMetric: 16.5681 - val_loss: 16.9753 - val_MinusLogProbMetric: 16.9753 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 344/1000
2023-09-26 19:41:53.123 
Epoch 344/1000 
	 loss: 16.5618, MinusLogProbMetric: 16.5618, val_loss: 16.9972, val_MinusLogProbMetric: 16.9972

Epoch 344: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5618 - MinusLogProbMetric: 16.5618 - val_loss: 16.9972 - val_MinusLogProbMetric: 16.9972 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 345/1000
2023-09-26 19:42:58.368 
Epoch 345/1000 
	 loss: 16.5429, MinusLogProbMetric: 16.5429, val_loss: 16.9596, val_MinusLogProbMetric: 16.9596

Epoch 345: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5429 - MinusLogProbMetric: 16.5429 - val_loss: 16.9596 - val_MinusLogProbMetric: 16.9596 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 346/1000
2023-09-26 19:44:03.296 
Epoch 346/1000 
	 loss: 16.5746, MinusLogProbMetric: 16.5746, val_loss: 17.0116, val_MinusLogProbMetric: 17.0116

Epoch 346: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5746 - MinusLogProbMetric: 16.5746 - val_loss: 17.0116 - val_MinusLogProbMetric: 17.0116 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 347/1000
2023-09-26 19:45:07.993 
Epoch 347/1000 
	 loss: 16.5330, MinusLogProbMetric: 16.5330, val_loss: 16.9496, val_MinusLogProbMetric: 16.9496

Epoch 347: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5330 - MinusLogProbMetric: 16.5330 - val_loss: 16.9496 - val_MinusLogProbMetric: 16.9496 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 348/1000
2023-09-26 19:46:12.836 
Epoch 348/1000 
	 loss: 16.5550, MinusLogProbMetric: 16.5550, val_loss: 17.0107, val_MinusLogProbMetric: 17.0107

Epoch 348: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5550 - MinusLogProbMetric: 16.5550 - val_loss: 17.0107 - val_MinusLogProbMetric: 17.0107 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 349/1000
2023-09-26 19:47:17.955 
Epoch 349/1000 
	 loss: 16.5616, MinusLogProbMetric: 16.5616, val_loss: 16.9665, val_MinusLogProbMetric: 16.9665

Epoch 349: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5616 - MinusLogProbMetric: 16.5616 - val_loss: 16.9665 - val_MinusLogProbMetric: 16.9665 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 350/1000
2023-09-26 19:48:22.634 
Epoch 350/1000 
	 loss: 16.5531, MinusLogProbMetric: 16.5531, val_loss: 16.9597, val_MinusLogProbMetric: 16.9597

Epoch 350: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5531 - MinusLogProbMetric: 16.5531 - val_loss: 16.9597 - val_MinusLogProbMetric: 16.9597 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 351/1000
2023-09-26 19:49:27.763 
Epoch 351/1000 
	 loss: 16.5610, MinusLogProbMetric: 16.5610, val_loss: 16.9914, val_MinusLogProbMetric: 16.9914

Epoch 351: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5610 - MinusLogProbMetric: 16.5610 - val_loss: 16.9914 - val_MinusLogProbMetric: 16.9914 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 352/1000
2023-09-26 19:50:32.499 
Epoch 352/1000 
	 loss: 16.5332, MinusLogProbMetric: 16.5332, val_loss: 17.0486, val_MinusLogProbMetric: 17.0486

Epoch 352: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5332 - MinusLogProbMetric: 16.5332 - val_loss: 17.0486 - val_MinusLogProbMetric: 17.0486 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 353/1000
2023-09-26 19:51:36.957 
Epoch 353/1000 
	 loss: 16.5758, MinusLogProbMetric: 16.5758, val_loss: 17.0841, val_MinusLogProbMetric: 17.0841

Epoch 353: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5758 - MinusLogProbMetric: 16.5758 - val_loss: 17.0841 - val_MinusLogProbMetric: 17.0841 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 354/1000
2023-09-26 19:52:41.130 
Epoch 354/1000 
	 loss: 16.5570, MinusLogProbMetric: 16.5570, val_loss: 16.9699, val_MinusLogProbMetric: 16.9699

Epoch 354: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5570 - MinusLogProbMetric: 16.5570 - val_loss: 16.9699 - val_MinusLogProbMetric: 16.9699 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 355/1000
2023-09-26 19:53:45.719 
Epoch 355/1000 
	 loss: 16.5331, MinusLogProbMetric: 16.5331, val_loss: 17.0083, val_MinusLogProbMetric: 17.0083

Epoch 355: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5331 - MinusLogProbMetric: 16.5331 - val_loss: 17.0083 - val_MinusLogProbMetric: 17.0083 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 356/1000
2023-09-26 19:54:50.837 
Epoch 356/1000 
	 loss: 16.5555, MinusLogProbMetric: 16.5555, val_loss: 16.9866, val_MinusLogProbMetric: 16.9866

Epoch 356: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5555 - MinusLogProbMetric: 16.5555 - val_loss: 16.9866 - val_MinusLogProbMetric: 16.9866 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 357/1000
2023-09-26 19:55:55.264 
Epoch 357/1000 
	 loss: 16.5539, MinusLogProbMetric: 16.5539, val_loss: 16.9647, val_MinusLogProbMetric: 16.9647

Epoch 357: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5539 - MinusLogProbMetric: 16.5539 - val_loss: 16.9647 - val_MinusLogProbMetric: 16.9647 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 358/1000
2023-09-26 19:57:00.265 
Epoch 358/1000 
	 loss: 16.5315, MinusLogProbMetric: 16.5315, val_loss: 17.0299, val_MinusLogProbMetric: 17.0299

Epoch 358: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5315 - MinusLogProbMetric: 16.5315 - val_loss: 17.0299 - val_MinusLogProbMetric: 17.0299 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 359/1000
2023-09-26 19:58:04.570 
Epoch 359/1000 
	 loss: 16.5418, MinusLogProbMetric: 16.5418, val_loss: 16.9596, val_MinusLogProbMetric: 16.9596

Epoch 359: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5418 - MinusLogProbMetric: 16.5418 - val_loss: 16.9596 - val_MinusLogProbMetric: 16.9596 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 360/1000
2023-09-26 19:59:08.816 
Epoch 360/1000 
	 loss: 16.5483, MinusLogProbMetric: 16.5483, val_loss: 16.9640, val_MinusLogProbMetric: 16.9640

Epoch 360: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5483 - MinusLogProbMetric: 16.5483 - val_loss: 16.9640 - val_MinusLogProbMetric: 16.9640 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 361/1000
2023-09-26 20:00:13.707 
Epoch 361/1000 
	 loss: 16.5660, MinusLogProbMetric: 16.5660, val_loss: 16.9737, val_MinusLogProbMetric: 16.9737

Epoch 361: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5660 - MinusLogProbMetric: 16.5660 - val_loss: 16.9737 - val_MinusLogProbMetric: 16.9737 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 362/1000
2023-09-26 20:01:18.759 
Epoch 362/1000 
	 loss: 16.5383, MinusLogProbMetric: 16.5383, val_loss: 17.0075, val_MinusLogProbMetric: 17.0075

Epoch 362: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5383 - MinusLogProbMetric: 16.5383 - val_loss: 17.0075 - val_MinusLogProbMetric: 17.0075 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 363/1000
2023-09-26 20:02:23.098 
Epoch 363/1000 
	 loss: 16.5445, MinusLogProbMetric: 16.5445, val_loss: 16.9833, val_MinusLogProbMetric: 16.9833

Epoch 363: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5445 - MinusLogProbMetric: 16.5445 - val_loss: 16.9833 - val_MinusLogProbMetric: 16.9833 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 364/1000
2023-09-26 20:03:27.749 
Epoch 364/1000 
	 loss: 16.5508, MinusLogProbMetric: 16.5508, val_loss: 16.9774, val_MinusLogProbMetric: 16.9774

Epoch 364: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5508 - MinusLogProbMetric: 16.5508 - val_loss: 16.9774 - val_MinusLogProbMetric: 16.9774 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 365/1000
2023-09-26 20:04:32.819 
Epoch 365/1000 
	 loss: 16.5620, MinusLogProbMetric: 16.5620, val_loss: 16.9857, val_MinusLogProbMetric: 16.9857

Epoch 365: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5620 - MinusLogProbMetric: 16.5620 - val_loss: 16.9857 - val_MinusLogProbMetric: 16.9857 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 366/1000
2023-09-26 20:05:37.856 
Epoch 366/1000 
	 loss: 16.5447, MinusLogProbMetric: 16.5447, val_loss: 17.0115, val_MinusLogProbMetric: 17.0115

Epoch 366: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5447 - MinusLogProbMetric: 16.5447 - val_loss: 17.0115 - val_MinusLogProbMetric: 17.0115 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 367/1000
2023-09-26 20:06:42.651 
Epoch 367/1000 
	 loss: 16.5445, MinusLogProbMetric: 16.5445, val_loss: 17.1221, val_MinusLogProbMetric: 17.1221

Epoch 367: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5445 - MinusLogProbMetric: 16.5445 - val_loss: 17.1221 - val_MinusLogProbMetric: 17.1221 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 368/1000
2023-09-26 20:07:47.200 
Epoch 368/1000 
	 loss: 16.5720, MinusLogProbMetric: 16.5720, val_loss: 17.0743, val_MinusLogProbMetric: 17.0743

Epoch 368: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5720 - MinusLogProbMetric: 16.5720 - val_loss: 17.0743 - val_MinusLogProbMetric: 17.0743 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 369/1000
2023-09-26 20:08:52.038 
Epoch 369/1000 
	 loss: 16.5283, MinusLogProbMetric: 16.5283, val_loss: 16.9508, val_MinusLogProbMetric: 16.9508

Epoch 369: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5283 - MinusLogProbMetric: 16.5283 - val_loss: 16.9508 - val_MinusLogProbMetric: 16.9508 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 370/1000
2023-09-26 20:09:56.514 
Epoch 370/1000 
	 loss: 16.5616, MinusLogProbMetric: 16.5616, val_loss: 16.9779, val_MinusLogProbMetric: 16.9779

Epoch 370: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5616 - MinusLogProbMetric: 16.5616 - val_loss: 16.9779 - val_MinusLogProbMetric: 16.9779 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 371/1000
2023-09-26 20:11:00.800 
Epoch 371/1000 
	 loss: 16.5376, MinusLogProbMetric: 16.5376, val_loss: 16.9746, val_MinusLogProbMetric: 16.9746

Epoch 371: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5376 - MinusLogProbMetric: 16.5376 - val_loss: 16.9746 - val_MinusLogProbMetric: 16.9746 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 372/1000
2023-09-26 20:12:05.108 
Epoch 372/1000 
	 loss: 16.5332, MinusLogProbMetric: 16.5332, val_loss: 16.9877, val_MinusLogProbMetric: 16.9877

Epoch 372: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5332 - MinusLogProbMetric: 16.5332 - val_loss: 16.9877 - val_MinusLogProbMetric: 16.9877 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 373/1000
2023-09-26 20:13:09.328 
Epoch 373/1000 
	 loss: 16.5420, MinusLogProbMetric: 16.5420, val_loss: 17.0597, val_MinusLogProbMetric: 17.0597

Epoch 373: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5420 - MinusLogProbMetric: 16.5420 - val_loss: 17.0597 - val_MinusLogProbMetric: 17.0597 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 374/1000
2023-09-26 20:14:13.721 
Epoch 374/1000 
	 loss: 16.5370, MinusLogProbMetric: 16.5370, val_loss: 17.0003, val_MinusLogProbMetric: 17.0003

Epoch 374: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5370 - MinusLogProbMetric: 16.5370 - val_loss: 17.0003 - val_MinusLogProbMetric: 17.0003 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 375/1000
2023-09-26 20:15:18.032 
Epoch 375/1000 
	 loss: 16.5267, MinusLogProbMetric: 16.5267, val_loss: 17.1508, val_MinusLogProbMetric: 17.1508

Epoch 375: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5267 - MinusLogProbMetric: 16.5267 - val_loss: 17.1508 - val_MinusLogProbMetric: 17.1508 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 376/1000
2023-09-26 20:16:22.234 
Epoch 376/1000 
	 loss: 16.5909, MinusLogProbMetric: 16.5909, val_loss: 17.0054, val_MinusLogProbMetric: 17.0054

Epoch 376: val_loss did not improve from 16.94309
196/196 - 64s - loss: 16.5909 - MinusLogProbMetric: 16.5909 - val_loss: 17.0054 - val_MinusLogProbMetric: 17.0054 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 377/1000
2023-09-26 20:17:26.757 
Epoch 377/1000 
	 loss: 16.5516, MinusLogProbMetric: 16.5516, val_loss: 17.0148, val_MinusLogProbMetric: 17.0148

Epoch 377: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5516 - MinusLogProbMetric: 16.5516 - val_loss: 17.0148 - val_MinusLogProbMetric: 17.0148 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 378/1000
2023-09-26 20:18:31.415 
Epoch 378/1000 
	 loss: 16.5452, MinusLogProbMetric: 16.5452, val_loss: 17.0243, val_MinusLogProbMetric: 17.0243

Epoch 378: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5452 - MinusLogProbMetric: 16.5452 - val_loss: 17.0243 - val_MinusLogProbMetric: 17.0243 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 379/1000
2023-09-26 20:19:36.048 
Epoch 379/1000 
	 loss: 16.5299, MinusLogProbMetric: 16.5299, val_loss: 17.0689, val_MinusLogProbMetric: 17.0689

Epoch 379: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5299 - MinusLogProbMetric: 16.5299 - val_loss: 17.0689 - val_MinusLogProbMetric: 17.0689 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 380/1000
2023-09-26 20:20:41.183 
Epoch 380/1000 
	 loss: 16.5497, MinusLogProbMetric: 16.5497, val_loss: 17.1878, val_MinusLogProbMetric: 17.1878

Epoch 380: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5497 - MinusLogProbMetric: 16.5497 - val_loss: 17.1878 - val_MinusLogProbMetric: 17.1878 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 381/1000
2023-09-26 20:21:45.713 
Epoch 381/1000 
	 loss: 16.5431, MinusLogProbMetric: 16.5431, val_loss: 17.0180, val_MinusLogProbMetric: 17.0180

Epoch 381: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5431 - MinusLogProbMetric: 16.5431 - val_loss: 17.0180 - val_MinusLogProbMetric: 17.0180 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 382/1000
2023-09-26 20:22:50.384 
Epoch 382/1000 
	 loss: 16.5614, MinusLogProbMetric: 16.5614, val_loss: 17.0147, val_MinusLogProbMetric: 17.0147

Epoch 382: val_loss did not improve from 16.94309
196/196 - 65s - loss: 16.5614 - MinusLogProbMetric: 16.5614 - val_loss: 17.0147 - val_MinusLogProbMetric: 17.0147 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 383/1000
2023-09-26 20:23:54.629 
Epoch 383/1000 
	 loss: 16.4695, MinusLogProbMetric: 16.4695, val_loss: 16.9290, val_MinusLogProbMetric: 16.9290

Epoch 383: val_loss improved from 16.94309 to 16.92899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.4695 - MinusLogProbMetric: 16.4695 - val_loss: 16.9290 - val_MinusLogProbMetric: 16.9290 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 384/1000
2023-09-26 20:25:00.671 
Epoch 384/1000 
	 loss: 16.4633, MinusLogProbMetric: 16.4633, val_loss: 16.9486, val_MinusLogProbMetric: 16.9486

Epoch 384: val_loss did not improve from 16.92899
196/196 - 65s - loss: 16.4633 - MinusLogProbMetric: 16.4633 - val_loss: 16.9486 - val_MinusLogProbMetric: 16.9486 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 385/1000
2023-09-26 20:26:05.300 
Epoch 385/1000 
	 loss: 16.4612, MinusLogProbMetric: 16.4612, val_loss: 16.9533, val_MinusLogProbMetric: 16.9533

Epoch 385: val_loss did not improve from 16.92899
196/196 - 65s - loss: 16.4612 - MinusLogProbMetric: 16.4612 - val_loss: 16.9533 - val_MinusLogProbMetric: 16.9533 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 386/1000
2023-09-26 20:27:09.958 
Epoch 386/1000 
	 loss: 16.4735, MinusLogProbMetric: 16.4735, val_loss: 16.9274, val_MinusLogProbMetric: 16.9274

Epoch 386: val_loss improved from 16.92899 to 16.92743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.4735 - MinusLogProbMetric: 16.4735 - val_loss: 16.9274 - val_MinusLogProbMetric: 16.9274 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 387/1000
2023-09-26 20:28:15.507 
Epoch 387/1000 
	 loss: 16.4567, MinusLogProbMetric: 16.4567, val_loss: 16.9208, val_MinusLogProbMetric: 16.9208

Epoch 387: val_loss improved from 16.92743 to 16.92077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.4567 - MinusLogProbMetric: 16.4567 - val_loss: 16.9208 - val_MinusLogProbMetric: 16.9208 - lr: 4.1667e-05 - 66s/epoch - 334ms/step
Epoch 388/1000
2023-09-26 20:29:21.208 
Epoch 388/1000 
	 loss: 16.4660, MinusLogProbMetric: 16.4660, val_loss: 16.9771, val_MinusLogProbMetric: 16.9771

Epoch 388: val_loss did not improve from 16.92077
196/196 - 65s - loss: 16.4660 - MinusLogProbMetric: 16.4660 - val_loss: 16.9771 - val_MinusLogProbMetric: 16.9771 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 389/1000
2023-09-26 20:30:26.034 
Epoch 389/1000 
	 loss: 16.4685, MinusLogProbMetric: 16.4685, val_loss: 16.9584, val_MinusLogProbMetric: 16.9584

Epoch 389: val_loss did not improve from 16.92077
196/196 - 65s - loss: 16.4685 - MinusLogProbMetric: 16.4685 - val_loss: 16.9584 - val_MinusLogProbMetric: 16.9584 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 390/1000
2023-09-26 20:31:30.853 
Epoch 390/1000 
	 loss: 16.4737, MinusLogProbMetric: 16.4737, val_loss: 16.9324, val_MinusLogProbMetric: 16.9324

Epoch 390: val_loss did not improve from 16.92077
196/196 - 65s - loss: 16.4737 - MinusLogProbMetric: 16.4737 - val_loss: 16.9324 - val_MinusLogProbMetric: 16.9324 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 391/1000
2023-09-26 20:32:35.851 
Epoch 391/1000 
	 loss: 16.4697, MinusLogProbMetric: 16.4697, val_loss: 16.9560, val_MinusLogProbMetric: 16.9560

Epoch 391: val_loss did not improve from 16.92077
196/196 - 65s - loss: 16.4697 - MinusLogProbMetric: 16.4697 - val_loss: 16.9560 - val_MinusLogProbMetric: 16.9560 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 392/1000
2023-09-26 20:33:40.543 
Epoch 392/1000 
	 loss: 16.4669, MinusLogProbMetric: 16.4669, val_loss: 16.9222, val_MinusLogProbMetric: 16.9222

Epoch 392: val_loss did not improve from 16.92077
196/196 - 65s - loss: 16.4669 - MinusLogProbMetric: 16.4669 - val_loss: 16.9222 - val_MinusLogProbMetric: 16.9222 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 393/1000
2023-09-26 20:34:44.763 
Epoch 393/1000 
	 loss: 16.4666, MinusLogProbMetric: 16.4666, val_loss: 16.9312, val_MinusLogProbMetric: 16.9312

Epoch 393: val_loss did not improve from 16.92077
196/196 - 64s - loss: 16.4666 - MinusLogProbMetric: 16.4666 - val_loss: 16.9312 - val_MinusLogProbMetric: 16.9312 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 394/1000
2023-09-26 20:35:49.549 
Epoch 394/1000 
	 loss: 16.4631, MinusLogProbMetric: 16.4631, val_loss: 16.9189, val_MinusLogProbMetric: 16.9189

Epoch 394: val_loss improved from 16.92077 to 16.91890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.4631 - MinusLogProbMetric: 16.4631 - val_loss: 16.9189 - val_MinusLogProbMetric: 16.9189 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 395/1000
2023-09-26 20:36:55.198 
Epoch 395/1000 
	 loss: 16.4627, MinusLogProbMetric: 16.4627, val_loss: 16.9466, val_MinusLogProbMetric: 16.9466

Epoch 395: val_loss did not improve from 16.91890
196/196 - 65s - loss: 16.4627 - MinusLogProbMetric: 16.4627 - val_loss: 16.9466 - val_MinusLogProbMetric: 16.9466 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 396/1000
2023-09-26 20:37:59.715 
Epoch 396/1000 
	 loss: 16.4688, MinusLogProbMetric: 16.4688, val_loss: 16.9976, val_MinusLogProbMetric: 16.9976

Epoch 396: val_loss did not improve from 16.91890
196/196 - 65s - loss: 16.4688 - MinusLogProbMetric: 16.4688 - val_loss: 16.9976 - val_MinusLogProbMetric: 16.9976 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 397/1000
2023-09-26 20:39:04.318 
Epoch 397/1000 
	 loss: 16.4521, MinusLogProbMetric: 16.4521, val_loss: 16.9238, val_MinusLogProbMetric: 16.9238

Epoch 397: val_loss did not improve from 16.91890
196/196 - 65s - loss: 16.4521 - MinusLogProbMetric: 16.4521 - val_loss: 16.9238 - val_MinusLogProbMetric: 16.9238 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 398/1000
2023-09-26 20:40:09.518 
Epoch 398/1000 
	 loss: 16.4790, MinusLogProbMetric: 16.4790, val_loss: 16.9152, val_MinusLogProbMetric: 16.9152

Epoch 398: val_loss improved from 16.91890 to 16.91518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.4790 - MinusLogProbMetric: 16.4790 - val_loss: 16.9152 - val_MinusLogProbMetric: 16.9152 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 399/1000
2023-09-26 20:41:15.296 
Epoch 399/1000 
	 loss: 16.4596, MinusLogProbMetric: 16.4596, val_loss: 16.9213, val_MinusLogProbMetric: 16.9213

Epoch 399: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4596 - MinusLogProbMetric: 16.4596 - val_loss: 16.9213 - val_MinusLogProbMetric: 16.9213 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 400/1000
2023-09-26 20:42:20.127 
Epoch 400/1000 
	 loss: 16.4544, MinusLogProbMetric: 16.4544, val_loss: 16.9215, val_MinusLogProbMetric: 16.9215

Epoch 400: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4544 - MinusLogProbMetric: 16.4544 - val_loss: 16.9215 - val_MinusLogProbMetric: 16.9215 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 401/1000
2023-09-26 20:43:24.930 
Epoch 401/1000 
	 loss: 16.4624, MinusLogProbMetric: 16.4624, val_loss: 16.9348, val_MinusLogProbMetric: 16.9348

Epoch 401: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4624 - MinusLogProbMetric: 16.4624 - val_loss: 16.9348 - val_MinusLogProbMetric: 16.9348 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 402/1000
2023-09-26 20:44:29.747 
Epoch 402/1000 
	 loss: 16.4668, MinusLogProbMetric: 16.4668, val_loss: 16.9299, val_MinusLogProbMetric: 16.9299

Epoch 402: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4668 - MinusLogProbMetric: 16.4668 - val_loss: 16.9299 - val_MinusLogProbMetric: 16.9299 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 403/1000
2023-09-26 20:45:34.632 
Epoch 403/1000 
	 loss: 16.4630, MinusLogProbMetric: 16.4630, val_loss: 16.9469, val_MinusLogProbMetric: 16.9469

Epoch 403: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4630 - MinusLogProbMetric: 16.4630 - val_loss: 16.9469 - val_MinusLogProbMetric: 16.9469 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 404/1000
2023-09-26 20:46:39.626 
Epoch 404/1000 
	 loss: 16.4542, MinusLogProbMetric: 16.4542, val_loss: 16.9808, val_MinusLogProbMetric: 16.9808

Epoch 404: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4542 - MinusLogProbMetric: 16.4542 - val_loss: 16.9808 - val_MinusLogProbMetric: 16.9808 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 405/1000
2023-09-26 20:47:44.554 
Epoch 405/1000 
	 loss: 16.4633, MinusLogProbMetric: 16.4633, val_loss: 16.9527, val_MinusLogProbMetric: 16.9527

Epoch 405: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4633 - MinusLogProbMetric: 16.4633 - val_loss: 16.9527 - val_MinusLogProbMetric: 16.9527 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 406/1000
2023-09-26 20:48:49.624 
Epoch 406/1000 
	 loss: 16.4614, MinusLogProbMetric: 16.4614, val_loss: 16.9776, val_MinusLogProbMetric: 16.9776

Epoch 406: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4614 - MinusLogProbMetric: 16.4614 - val_loss: 16.9776 - val_MinusLogProbMetric: 16.9776 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 407/1000
2023-09-26 20:49:54.109 
Epoch 407/1000 
	 loss: 16.4670, MinusLogProbMetric: 16.4670, val_loss: 16.9396, val_MinusLogProbMetric: 16.9396

Epoch 407: val_loss did not improve from 16.91518
196/196 - 64s - loss: 16.4670 - MinusLogProbMetric: 16.4670 - val_loss: 16.9396 - val_MinusLogProbMetric: 16.9396 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 408/1000
2023-09-26 20:50:59.092 
Epoch 408/1000 
	 loss: 16.4670, MinusLogProbMetric: 16.4670, val_loss: 16.9257, val_MinusLogProbMetric: 16.9257

Epoch 408: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4670 - MinusLogProbMetric: 16.4670 - val_loss: 16.9257 - val_MinusLogProbMetric: 16.9257 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 409/1000
2023-09-26 20:52:03.854 
Epoch 409/1000 
	 loss: 16.4489, MinusLogProbMetric: 16.4489, val_loss: 16.9536, val_MinusLogProbMetric: 16.9536

Epoch 409: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4489 - MinusLogProbMetric: 16.4489 - val_loss: 16.9536 - val_MinusLogProbMetric: 16.9536 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 410/1000
2023-09-26 20:53:08.876 
Epoch 410/1000 
	 loss: 16.4587, MinusLogProbMetric: 16.4587, val_loss: 16.9299, val_MinusLogProbMetric: 16.9299

Epoch 410: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4587 - MinusLogProbMetric: 16.4587 - val_loss: 16.9299 - val_MinusLogProbMetric: 16.9299 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 411/1000
2023-09-26 20:54:13.474 
Epoch 411/1000 
	 loss: 16.4561, MinusLogProbMetric: 16.4561, val_loss: 16.9689, val_MinusLogProbMetric: 16.9689

Epoch 411: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4561 - MinusLogProbMetric: 16.4561 - val_loss: 16.9689 - val_MinusLogProbMetric: 16.9689 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 412/1000
2023-09-26 20:55:18.282 
Epoch 412/1000 
	 loss: 16.4631, MinusLogProbMetric: 16.4631, val_loss: 16.9903, val_MinusLogProbMetric: 16.9903

Epoch 412: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4631 - MinusLogProbMetric: 16.4631 - val_loss: 16.9903 - val_MinusLogProbMetric: 16.9903 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 413/1000
2023-09-26 20:56:23.210 
Epoch 413/1000 
	 loss: 16.4623, MinusLogProbMetric: 16.4623, val_loss: 16.9333, val_MinusLogProbMetric: 16.9333

Epoch 413: val_loss did not improve from 16.91518
196/196 - 65s - loss: 16.4623 - MinusLogProbMetric: 16.4623 - val_loss: 16.9333 - val_MinusLogProbMetric: 16.9333 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 414/1000
2023-09-26 20:57:27.673 
Epoch 414/1000 
	 loss: 16.4586, MinusLogProbMetric: 16.4586, val_loss: 16.9285, val_MinusLogProbMetric: 16.9285

Epoch 414: val_loss did not improve from 16.91518
196/196 - 64s - loss: 16.4586 - MinusLogProbMetric: 16.4586 - val_loss: 16.9285 - val_MinusLogProbMetric: 16.9285 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 415/1000
2023-09-26 20:58:32.340 
Epoch 415/1000 
	 loss: 16.4512, MinusLogProbMetric: 16.4512, val_loss: 16.9118, val_MinusLogProbMetric: 16.9118

Epoch 415: val_loss improved from 16.91518 to 16.91184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.4512 - MinusLogProbMetric: 16.4512 - val_loss: 16.9118 - val_MinusLogProbMetric: 16.9118 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 416/1000
2023-09-26 20:59:37.846 
Epoch 416/1000 
	 loss: 16.4589, MinusLogProbMetric: 16.4589, val_loss: 16.9693, val_MinusLogProbMetric: 16.9693

Epoch 416: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4589 - MinusLogProbMetric: 16.4589 - val_loss: 16.9693 - val_MinusLogProbMetric: 16.9693 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 417/1000
2023-09-26 21:00:42.503 
Epoch 417/1000 
	 loss: 16.4513, MinusLogProbMetric: 16.4513, val_loss: 16.9724, val_MinusLogProbMetric: 16.9724

Epoch 417: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4513 - MinusLogProbMetric: 16.4513 - val_loss: 16.9724 - val_MinusLogProbMetric: 16.9724 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 418/1000
2023-09-26 21:01:47.487 
Epoch 418/1000 
	 loss: 16.4536, MinusLogProbMetric: 16.4536, val_loss: 16.9397, val_MinusLogProbMetric: 16.9397

Epoch 418: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4536 - MinusLogProbMetric: 16.4536 - val_loss: 16.9397 - val_MinusLogProbMetric: 16.9397 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 419/1000
2023-09-26 21:02:52.223 
Epoch 419/1000 
	 loss: 16.4516, MinusLogProbMetric: 16.4516, val_loss: 16.9170, val_MinusLogProbMetric: 16.9170

Epoch 419: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4516 - MinusLogProbMetric: 16.4516 - val_loss: 16.9170 - val_MinusLogProbMetric: 16.9170 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 420/1000
2023-09-26 21:03:56.842 
Epoch 420/1000 
	 loss: 16.4647, MinusLogProbMetric: 16.4647, val_loss: 16.9346, val_MinusLogProbMetric: 16.9346

Epoch 420: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4647 - MinusLogProbMetric: 16.4647 - val_loss: 16.9346 - val_MinusLogProbMetric: 16.9346 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 421/1000
2023-09-26 21:05:01.173 
Epoch 421/1000 
	 loss: 16.4559, MinusLogProbMetric: 16.4559, val_loss: 16.9703, val_MinusLogProbMetric: 16.9703

Epoch 421: val_loss did not improve from 16.91184
196/196 - 64s - loss: 16.4559 - MinusLogProbMetric: 16.4559 - val_loss: 16.9703 - val_MinusLogProbMetric: 16.9703 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 422/1000
2023-09-26 21:06:05.910 
Epoch 422/1000 
	 loss: 16.4598, MinusLogProbMetric: 16.4598, val_loss: 16.9451, val_MinusLogProbMetric: 16.9451

Epoch 422: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4598 - MinusLogProbMetric: 16.4598 - val_loss: 16.9451 - val_MinusLogProbMetric: 16.9451 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 423/1000
2023-09-26 21:07:10.859 
Epoch 423/1000 
	 loss: 16.4638, MinusLogProbMetric: 16.4638, val_loss: 16.9422, val_MinusLogProbMetric: 16.9422

Epoch 423: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4638 - MinusLogProbMetric: 16.4638 - val_loss: 16.9422 - val_MinusLogProbMetric: 16.9422 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 424/1000
2023-09-26 21:08:15.378 
Epoch 424/1000 
	 loss: 16.4555, MinusLogProbMetric: 16.4555, val_loss: 16.9868, val_MinusLogProbMetric: 16.9868

Epoch 424: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4555 - MinusLogProbMetric: 16.4555 - val_loss: 16.9868 - val_MinusLogProbMetric: 16.9868 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 425/1000
2023-09-26 21:09:20.075 
Epoch 425/1000 
	 loss: 16.4549, MinusLogProbMetric: 16.4549, val_loss: 16.9445, val_MinusLogProbMetric: 16.9445

Epoch 425: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4549 - MinusLogProbMetric: 16.4549 - val_loss: 16.9445 - val_MinusLogProbMetric: 16.9445 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 426/1000
2023-09-26 21:10:24.463 
Epoch 426/1000 
	 loss: 16.4711, MinusLogProbMetric: 16.4711, val_loss: 16.9344, val_MinusLogProbMetric: 16.9344

Epoch 426: val_loss did not improve from 16.91184
196/196 - 64s - loss: 16.4711 - MinusLogProbMetric: 16.4711 - val_loss: 16.9344 - val_MinusLogProbMetric: 16.9344 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 427/1000
2023-09-26 21:11:29.234 
Epoch 427/1000 
	 loss: 16.4579, MinusLogProbMetric: 16.4579, val_loss: 16.9317, val_MinusLogProbMetric: 16.9317

Epoch 427: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4579 - MinusLogProbMetric: 16.4579 - val_loss: 16.9317 - val_MinusLogProbMetric: 16.9317 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 428/1000
2023-09-26 21:12:34.089 
Epoch 428/1000 
	 loss: 16.4515, MinusLogProbMetric: 16.4515, val_loss: 16.9250, val_MinusLogProbMetric: 16.9250

Epoch 428: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4515 - MinusLogProbMetric: 16.4515 - val_loss: 16.9250 - val_MinusLogProbMetric: 16.9250 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 429/1000
2023-09-26 21:13:38.839 
Epoch 429/1000 
	 loss: 16.4602, MinusLogProbMetric: 16.4602, val_loss: 17.0393, val_MinusLogProbMetric: 17.0393

Epoch 429: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4602 - MinusLogProbMetric: 16.4602 - val_loss: 17.0393 - val_MinusLogProbMetric: 17.0393 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 430/1000
2023-09-26 21:14:43.886 
Epoch 430/1000 
	 loss: 16.4616, MinusLogProbMetric: 16.4616, val_loss: 16.9949, val_MinusLogProbMetric: 16.9949

Epoch 430: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4616 - MinusLogProbMetric: 16.4616 - val_loss: 16.9949 - val_MinusLogProbMetric: 16.9949 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 431/1000
2023-09-26 21:15:48.639 
Epoch 431/1000 
	 loss: 16.4590, MinusLogProbMetric: 16.4590, val_loss: 16.9253, val_MinusLogProbMetric: 16.9253

Epoch 431: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4590 - MinusLogProbMetric: 16.4590 - val_loss: 16.9253 - val_MinusLogProbMetric: 16.9253 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 432/1000
2023-09-26 21:16:53.368 
Epoch 432/1000 
	 loss: 16.4563, MinusLogProbMetric: 16.4563, val_loss: 16.9537, val_MinusLogProbMetric: 16.9537

Epoch 432: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4563 - MinusLogProbMetric: 16.4563 - val_loss: 16.9537 - val_MinusLogProbMetric: 16.9537 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 433/1000
2023-09-26 21:17:57.646 
Epoch 433/1000 
	 loss: 16.4650, MinusLogProbMetric: 16.4650, val_loss: 17.2225, val_MinusLogProbMetric: 17.2225

Epoch 433: val_loss did not improve from 16.91184
196/196 - 64s - loss: 16.4650 - MinusLogProbMetric: 16.4650 - val_loss: 17.2225 - val_MinusLogProbMetric: 17.2225 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 434/1000
2023-09-26 21:19:02.526 
Epoch 434/1000 
	 loss: 16.4642, MinusLogProbMetric: 16.4642, val_loss: 16.9173, val_MinusLogProbMetric: 16.9173

Epoch 434: val_loss did not improve from 16.91184
196/196 - 65s - loss: 16.4642 - MinusLogProbMetric: 16.4642 - val_loss: 16.9173 - val_MinusLogProbMetric: 16.9173 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 435/1000
2023-09-26 21:20:07.015 
Epoch 435/1000 
	 loss: 16.4649, MinusLogProbMetric: 16.4649, val_loss: 16.9088, val_MinusLogProbMetric: 16.9088

Epoch 435: val_loss improved from 16.91184 to 16.90879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.4649 - MinusLogProbMetric: 16.4649 - val_loss: 16.9088 - val_MinusLogProbMetric: 16.9088 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 436/1000
2023-09-26 21:21:12.874 
Epoch 436/1000 
	 loss: 16.4502, MinusLogProbMetric: 16.4502, val_loss: 16.9265, val_MinusLogProbMetric: 16.9265

Epoch 436: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4502 - MinusLogProbMetric: 16.4502 - val_loss: 16.9265 - val_MinusLogProbMetric: 16.9265 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 437/1000
2023-09-26 21:22:17.495 
Epoch 437/1000 
	 loss: 16.4595, MinusLogProbMetric: 16.4595, val_loss: 16.9822, val_MinusLogProbMetric: 16.9822

Epoch 437: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4595 - MinusLogProbMetric: 16.4595 - val_loss: 16.9822 - val_MinusLogProbMetric: 16.9822 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 438/1000
2023-09-26 21:23:22.342 
Epoch 438/1000 
	 loss: 16.4505, MinusLogProbMetric: 16.4505, val_loss: 16.9410, val_MinusLogProbMetric: 16.9410

Epoch 438: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4505 - MinusLogProbMetric: 16.4505 - val_loss: 16.9410 - val_MinusLogProbMetric: 16.9410 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 439/1000
2023-09-26 21:24:26.707 
Epoch 439/1000 
	 loss: 16.4473, MinusLogProbMetric: 16.4473, val_loss: 17.0695, val_MinusLogProbMetric: 17.0695

Epoch 439: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4473 - MinusLogProbMetric: 16.4473 - val_loss: 17.0695 - val_MinusLogProbMetric: 17.0695 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 440/1000
2023-09-26 21:25:30.632 
Epoch 440/1000 
	 loss: 16.4483, MinusLogProbMetric: 16.4483, val_loss: 16.9151, val_MinusLogProbMetric: 16.9151

Epoch 440: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4483 - MinusLogProbMetric: 16.4483 - val_loss: 16.9151 - val_MinusLogProbMetric: 16.9151 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 441/1000
2023-09-26 21:26:35.082 
Epoch 441/1000 
	 loss: 16.4555, MinusLogProbMetric: 16.4555, val_loss: 16.9729, val_MinusLogProbMetric: 16.9729

Epoch 441: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4555 - MinusLogProbMetric: 16.4555 - val_loss: 16.9729 - val_MinusLogProbMetric: 16.9729 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 442/1000
2023-09-26 21:27:39.050 
Epoch 442/1000 
	 loss: 16.4673, MinusLogProbMetric: 16.4673, val_loss: 16.9113, val_MinusLogProbMetric: 16.9113

Epoch 442: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4673 - MinusLogProbMetric: 16.4673 - val_loss: 16.9113 - val_MinusLogProbMetric: 16.9113 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 443/1000
2023-09-26 21:28:43.187 
Epoch 443/1000 
	 loss: 16.4487, MinusLogProbMetric: 16.4487, val_loss: 16.9247, val_MinusLogProbMetric: 16.9247

Epoch 443: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4487 - MinusLogProbMetric: 16.4487 - val_loss: 16.9247 - val_MinusLogProbMetric: 16.9247 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 444/1000
2023-09-26 21:29:47.683 
Epoch 444/1000 
	 loss: 16.4729, MinusLogProbMetric: 16.4729, val_loss: 16.9775, val_MinusLogProbMetric: 16.9775

Epoch 444: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4729 - MinusLogProbMetric: 16.4729 - val_loss: 16.9775 - val_MinusLogProbMetric: 16.9775 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 445/1000
2023-09-26 21:30:52.562 
Epoch 445/1000 
	 loss: 16.4553, MinusLogProbMetric: 16.4553, val_loss: 16.9309, val_MinusLogProbMetric: 16.9309

Epoch 445: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4553 - MinusLogProbMetric: 16.4553 - val_loss: 16.9309 - val_MinusLogProbMetric: 16.9309 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 446/1000
2023-09-26 21:31:56.713 
Epoch 446/1000 
	 loss: 16.4450, MinusLogProbMetric: 16.4450, val_loss: 16.9186, val_MinusLogProbMetric: 16.9186

Epoch 446: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4450 - MinusLogProbMetric: 16.4450 - val_loss: 16.9186 - val_MinusLogProbMetric: 16.9186 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 447/1000
2023-09-26 21:33:01.781 
Epoch 447/1000 
	 loss: 16.4383, MinusLogProbMetric: 16.4383, val_loss: 16.9565, val_MinusLogProbMetric: 16.9565

Epoch 447: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4383 - MinusLogProbMetric: 16.4383 - val_loss: 16.9565 - val_MinusLogProbMetric: 16.9565 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 448/1000
2023-09-26 21:34:06.587 
Epoch 448/1000 
	 loss: 16.4653, MinusLogProbMetric: 16.4653, val_loss: 17.0149, val_MinusLogProbMetric: 17.0149

Epoch 448: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4653 - MinusLogProbMetric: 16.4653 - val_loss: 17.0149 - val_MinusLogProbMetric: 17.0149 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 449/1000
2023-09-26 21:35:11.170 
Epoch 449/1000 
	 loss: 16.4497, MinusLogProbMetric: 16.4497, val_loss: 16.9187, val_MinusLogProbMetric: 16.9187

Epoch 449: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4497 - MinusLogProbMetric: 16.4497 - val_loss: 16.9187 - val_MinusLogProbMetric: 16.9187 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 450/1000
2023-09-26 21:36:15.818 
Epoch 450/1000 
	 loss: 16.4574, MinusLogProbMetric: 16.4574, val_loss: 16.9571, val_MinusLogProbMetric: 16.9571

Epoch 450: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4574 - MinusLogProbMetric: 16.4574 - val_loss: 16.9571 - val_MinusLogProbMetric: 16.9571 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 451/1000
2023-09-26 21:37:20.754 
Epoch 451/1000 
	 loss: 16.4643, MinusLogProbMetric: 16.4643, val_loss: 16.9762, val_MinusLogProbMetric: 16.9762

Epoch 451: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4643 - MinusLogProbMetric: 16.4643 - val_loss: 16.9762 - val_MinusLogProbMetric: 16.9762 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 452/1000
2023-09-26 21:38:25.821 
Epoch 452/1000 
	 loss: 16.4611, MinusLogProbMetric: 16.4611, val_loss: 16.9240, val_MinusLogProbMetric: 16.9240

Epoch 452: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4611 - MinusLogProbMetric: 16.4611 - val_loss: 16.9240 - val_MinusLogProbMetric: 16.9240 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 453/1000
2023-09-26 21:39:30.830 
Epoch 453/1000 
	 loss: 16.4581, MinusLogProbMetric: 16.4581, val_loss: 16.9140, val_MinusLogProbMetric: 16.9140

Epoch 453: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4581 - MinusLogProbMetric: 16.4581 - val_loss: 16.9140 - val_MinusLogProbMetric: 16.9140 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 454/1000
2023-09-26 21:40:36.112 
Epoch 454/1000 
	 loss: 16.4812, MinusLogProbMetric: 16.4812, val_loss: 16.9458, val_MinusLogProbMetric: 16.9458

Epoch 454: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4812 - MinusLogProbMetric: 16.4812 - val_loss: 16.9458 - val_MinusLogProbMetric: 16.9458 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 455/1000
2023-09-26 21:41:40.823 
Epoch 455/1000 
	 loss: 16.4460, MinusLogProbMetric: 16.4460, val_loss: 16.9415, val_MinusLogProbMetric: 16.9415

Epoch 455: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4460 - MinusLogProbMetric: 16.4460 - val_loss: 16.9415 - val_MinusLogProbMetric: 16.9415 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 456/1000
2023-09-26 21:42:45.896 
Epoch 456/1000 
	 loss: 16.4494, MinusLogProbMetric: 16.4494, val_loss: 16.9519, val_MinusLogProbMetric: 16.9519

Epoch 456: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4494 - MinusLogProbMetric: 16.4494 - val_loss: 16.9519 - val_MinusLogProbMetric: 16.9519 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 457/1000
2023-09-26 21:43:50.968 
Epoch 457/1000 
	 loss: 16.4449, MinusLogProbMetric: 16.4449, val_loss: 16.9226, val_MinusLogProbMetric: 16.9226

Epoch 457: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4449 - MinusLogProbMetric: 16.4449 - val_loss: 16.9226 - val_MinusLogProbMetric: 16.9226 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 458/1000
2023-09-26 21:44:55.956 
Epoch 458/1000 
	 loss: 16.4463, MinusLogProbMetric: 16.4463, val_loss: 16.9568, val_MinusLogProbMetric: 16.9568

Epoch 458: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4463 - MinusLogProbMetric: 16.4463 - val_loss: 16.9568 - val_MinusLogProbMetric: 16.9568 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 459/1000
2023-09-26 21:46:01.187 
Epoch 459/1000 
	 loss: 16.4588, MinusLogProbMetric: 16.4588, val_loss: 16.9400, val_MinusLogProbMetric: 16.9400

Epoch 459: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4588 - MinusLogProbMetric: 16.4588 - val_loss: 16.9400 - val_MinusLogProbMetric: 16.9400 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 460/1000
2023-09-26 21:47:05.814 
Epoch 460/1000 
	 loss: 16.4376, MinusLogProbMetric: 16.4376, val_loss: 16.9428, val_MinusLogProbMetric: 16.9428

Epoch 460: val_loss did not improve from 16.90879
196/196 - 65s - loss: 16.4376 - MinusLogProbMetric: 16.4376 - val_loss: 16.9428 - val_MinusLogProbMetric: 16.9428 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 461/1000
2023-09-26 21:48:10.289 
Epoch 461/1000 
	 loss: 16.4603, MinusLogProbMetric: 16.4603, val_loss: 16.9214, val_MinusLogProbMetric: 16.9214

Epoch 461: val_loss did not improve from 16.90879
196/196 - 64s - loss: 16.4603 - MinusLogProbMetric: 16.4603 - val_loss: 16.9214 - val_MinusLogProbMetric: 16.9214 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 462/1000
2023-09-26 21:49:15.317 
Epoch 462/1000 
	 loss: 16.4295, MinusLogProbMetric: 16.4295, val_loss: 16.9083, val_MinusLogProbMetric: 16.9083

Epoch 462: val_loss improved from 16.90879 to 16.90826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.4295 - MinusLogProbMetric: 16.4295 - val_loss: 16.9083 - val_MinusLogProbMetric: 16.9083 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 463/1000
2023-09-26 21:50:20.951 
Epoch 463/1000 
	 loss: 16.4472, MinusLogProbMetric: 16.4472, val_loss: 17.0735, val_MinusLogProbMetric: 17.0735

Epoch 463: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4472 - MinusLogProbMetric: 16.4472 - val_loss: 17.0735 - val_MinusLogProbMetric: 17.0735 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 464/1000
2023-09-26 21:51:25.621 
Epoch 464/1000 
	 loss: 16.4423, MinusLogProbMetric: 16.4423, val_loss: 16.9602, val_MinusLogProbMetric: 16.9602

Epoch 464: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4423 - MinusLogProbMetric: 16.4423 - val_loss: 16.9602 - val_MinusLogProbMetric: 16.9602 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 465/1000
2023-09-26 21:52:29.996 
Epoch 465/1000 
	 loss: 16.4562, MinusLogProbMetric: 16.4562, val_loss: 16.9655, val_MinusLogProbMetric: 16.9655

Epoch 465: val_loss did not improve from 16.90826
196/196 - 64s - loss: 16.4562 - MinusLogProbMetric: 16.4562 - val_loss: 16.9655 - val_MinusLogProbMetric: 16.9655 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 466/1000
2023-09-26 21:53:35.013 
Epoch 466/1000 
	 loss: 16.4321, MinusLogProbMetric: 16.4321, val_loss: 16.9132, val_MinusLogProbMetric: 16.9132

Epoch 466: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4321 - MinusLogProbMetric: 16.4321 - val_loss: 16.9132 - val_MinusLogProbMetric: 16.9132 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 467/1000
2023-09-26 21:54:39.618 
Epoch 467/1000 
	 loss: 16.4382, MinusLogProbMetric: 16.4382, val_loss: 16.9349, val_MinusLogProbMetric: 16.9349

Epoch 467: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4382 - MinusLogProbMetric: 16.4382 - val_loss: 16.9349 - val_MinusLogProbMetric: 16.9349 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 468/1000
2023-09-26 21:55:44.491 
Epoch 468/1000 
	 loss: 16.4410, MinusLogProbMetric: 16.4410, val_loss: 16.9333, val_MinusLogProbMetric: 16.9333

Epoch 468: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4410 - MinusLogProbMetric: 16.4410 - val_loss: 16.9333 - val_MinusLogProbMetric: 16.9333 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 469/1000
2023-09-26 21:56:49.324 
Epoch 469/1000 
	 loss: 16.4539, MinusLogProbMetric: 16.4539, val_loss: 16.9508, val_MinusLogProbMetric: 16.9508

Epoch 469: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4539 - MinusLogProbMetric: 16.4539 - val_loss: 16.9508 - val_MinusLogProbMetric: 16.9508 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 470/1000
2023-09-26 21:57:53.650 
Epoch 470/1000 
	 loss: 16.4324, MinusLogProbMetric: 16.4324, val_loss: 16.9281, val_MinusLogProbMetric: 16.9281

Epoch 470: val_loss did not improve from 16.90826
196/196 - 64s - loss: 16.4324 - MinusLogProbMetric: 16.4324 - val_loss: 16.9281 - val_MinusLogProbMetric: 16.9281 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 471/1000
2023-09-26 21:58:58.097 
Epoch 471/1000 
	 loss: 16.4571, MinusLogProbMetric: 16.4571, val_loss: 16.9713, val_MinusLogProbMetric: 16.9713

Epoch 471: val_loss did not improve from 16.90826
196/196 - 64s - loss: 16.4571 - MinusLogProbMetric: 16.4571 - val_loss: 16.9713 - val_MinusLogProbMetric: 16.9713 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 472/1000
2023-09-26 22:00:02.791 
Epoch 472/1000 
	 loss: 16.4486, MinusLogProbMetric: 16.4486, val_loss: 17.0210, val_MinusLogProbMetric: 17.0210

Epoch 472: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4486 - MinusLogProbMetric: 16.4486 - val_loss: 17.0210 - val_MinusLogProbMetric: 17.0210 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 473/1000
2023-09-26 22:01:07.602 
Epoch 473/1000 
	 loss: 16.4413, MinusLogProbMetric: 16.4413, val_loss: 16.9229, val_MinusLogProbMetric: 16.9229

Epoch 473: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4413 - MinusLogProbMetric: 16.4413 - val_loss: 16.9229 - val_MinusLogProbMetric: 16.9229 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 474/1000
2023-09-26 22:02:12.958 
Epoch 474/1000 
	 loss: 16.4527, MinusLogProbMetric: 16.4527, val_loss: 16.9921, val_MinusLogProbMetric: 16.9921

Epoch 474: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4527 - MinusLogProbMetric: 16.4527 - val_loss: 16.9921 - val_MinusLogProbMetric: 16.9921 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 475/1000
2023-09-26 22:03:17.573 
Epoch 475/1000 
	 loss: 16.4362, MinusLogProbMetric: 16.4362, val_loss: 16.9860, val_MinusLogProbMetric: 16.9860

Epoch 475: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4362 - MinusLogProbMetric: 16.4362 - val_loss: 16.9860 - val_MinusLogProbMetric: 16.9860 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 476/1000
2023-09-26 22:04:21.849 
Epoch 476/1000 
	 loss: 16.4384, MinusLogProbMetric: 16.4384, val_loss: 16.9408, val_MinusLogProbMetric: 16.9408

Epoch 476: val_loss did not improve from 16.90826
196/196 - 64s - loss: 16.4384 - MinusLogProbMetric: 16.4384 - val_loss: 16.9408 - val_MinusLogProbMetric: 16.9408 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 477/1000
2023-09-26 22:05:26.655 
Epoch 477/1000 
	 loss: 16.4621, MinusLogProbMetric: 16.4621, val_loss: 16.9232, val_MinusLogProbMetric: 16.9232

Epoch 477: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4621 - MinusLogProbMetric: 16.4621 - val_loss: 16.9232 - val_MinusLogProbMetric: 16.9232 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 478/1000
2023-09-26 22:06:31.461 
Epoch 478/1000 
	 loss: 16.4395, MinusLogProbMetric: 16.4395, val_loss: 16.9413, val_MinusLogProbMetric: 16.9413

Epoch 478: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4395 - MinusLogProbMetric: 16.4395 - val_loss: 16.9413 - val_MinusLogProbMetric: 16.9413 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 479/1000
2023-09-26 22:07:36.108 
Epoch 479/1000 
	 loss: 16.4479, MinusLogProbMetric: 16.4479, val_loss: 16.9921, val_MinusLogProbMetric: 16.9921

Epoch 479: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4479 - MinusLogProbMetric: 16.4479 - val_loss: 16.9921 - val_MinusLogProbMetric: 16.9921 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 480/1000
2023-09-26 22:08:40.651 
Epoch 480/1000 
	 loss: 16.4607, MinusLogProbMetric: 16.4607, val_loss: 16.9320, val_MinusLogProbMetric: 16.9320

Epoch 480: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4607 - MinusLogProbMetric: 16.4607 - val_loss: 16.9320 - val_MinusLogProbMetric: 16.9320 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 481/1000
2023-09-26 22:09:45.590 
Epoch 481/1000 
	 loss: 16.4454, MinusLogProbMetric: 16.4454, val_loss: 16.9931, val_MinusLogProbMetric: 16.9931

Epoch 481: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4454 - MinusLogProbMetric: 16.4454 - val_loss: 16.9931 - val_MinusLogProbMetric: 16.9931 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 482/1000
2023-09-26 22:10:50.770 
Epoch 482/1000 
	 loss: 16.4354, MinusLogProbMetric: 16.4354, val_loss: 16.9170, val_MinusLogProbMetric: 16.9170

Epoch 482: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4354 - MinusLogProbMetric: 16.4354 - val_loss: 16.9170 - val_MinusLogProbMetric: 16.9170 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 483/1000
2023-09-26 22:11:55.292 
Epoch 483/1000 
	 loss: 16.4584, MinusLogProbMetric: 16.4584, val_loss: 16.9238, val_MinusLogProbMetric: 16.9238

Epoch 483: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4584 - MinusLogProbMetric: 16.4584 - val_loss: 16.9238 - val_MinusLogProbMetric: 16.9238 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 484/1000
2023-09-26 22:13:00.092 
Epoch 484/1000 
	 loss: 16.4411, MinusLogProbMetric: 16.4411, val_loss: 16.9221, val_MinusLogProbMetric: 16.9221

Epoch 484: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4411 - MinusLogProbMetric: 16.4411 - val_loss: 16.9221 - val_MinusLogProbMetric: 16.9221 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 485/1000
2023-09-26 22:14:04.930 
Epoch 485/1000 
	 loss: 16.4404, MinusLogProbMetric: 16.4404, val_loss: 17.1007, val_MinusLogProbMetric: 17.1007

Epoch 485: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4404 - MinusLogProbMetric: 16.4404 - val_loss: 17.1007 - val_MinusLogProbMetric: 17.1007 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 486/1000
2023-09-26 22:15:10.212 
Epoch 486/1000 
	 loss: 16.4501, MinusLogProbMetric: 16.4501, val_loss: 16.9265, val_MinusLogProbMetric: 16.9265

Epoch 486: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4501 - MinusLogProbMetric: 16.4501 - val_loss: 16.9265 - val_MinusLogProbMetric: 16.9265 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 487/1000
2023-09-26 22:16:14.781 
Epoch 487/1000 
	 loss: 16.4343, MinusLogProbMetric: 16.4343, val_loss: 16.9573, val_MinusLogProbMetric: 16.9573

Epoch 487: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4343 - MinusLogProbMetric: 16.4343 - val_loss: 16.9573 - val_MinusLogProbMetric: 16.9573 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 488/1000
2023-09-26 22:17:19.490 
Epoch 488/1000 
	 loss: 16.4526, MinusLogProbMetric: 16.4526, val_loss: 16.9102, val_MinusLogProbMetric: 16.9102

Epoch 488: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4526 - MinusLogProbMetric: 16.4526 - val_loss: 16.9102 - val_MinusLogProbMetric: 16.9102 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 489/1000
2023-09-26 22:18:24.155 
Epoch 489/1000 
	 loss: 16.4355, MinusLogProbMetric: 16.4355, val_loss: 16.9660, val_MinusLogProbMetric: 16.9660

Epoch 489: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4355 - MinusLogProbMetric: 16.4355 - val_loss: 16.9660 - val_MinusLogProbMetric: 16.9660 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 490/1000
2023-09-26 22:19:28.657 
Epoch 490/1000 
	 loss: 16.4638, MinusLogProbMetric: 16.4638, val_loss: 16.9905, val_MinusLogProbMetric: 16.9905

Epoch 490: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4638 - MinusLogProbMetric: 16.4638 - val_loss: 16.9905 - val_MinusLogProbMetric: 16.9905 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 491/1000
2023-09-26 22:20:33.218 
Epoch 491/1000 
	 loss: 16.4484, MinusLogProbMetric: 16.4484, val_loss: 16.9609, val_MinusLogProbMetric: 16.9609

Epoch 491: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4484 - MinusLogProbMetric: 16.4484 - val_loss: 16.9609 - val_MinusLogProbMetric: 16.9609 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 492/1000
2023-09-26 22:21:38.008 
Epoch 492/1000 
	 loss: 16.4293, MinusLogProbMetric: 16.4293, val_loss: 16.9468, val_MinusLogProbMetric: 16.9468

Epoch 492: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4293 - MinusLogProbMetric: 16.4293 - val_loss: 16.9468 - val_MinusLogProbMetric: 16.9468 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 493/1000
2023-09-26 22:22:42.793 
Epoch 493/1000 
	 loss: 16.4318, MinusLogProbMetric: 16.4318, val_loss: 16.9217, val_MinusLogProbMetric: 16.9217

Epoch 493: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4318 - MinusLogProbMetric: 16.4318 - val_loss: 16.9217 - val_MinusLogProbMetric: 16.9217 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 494/1000
2023-09-26 22:23:47.379 
Epoch 494/1000 
	 loss: 16.4355, MinusLogProbMetric: 16.4355, val_loss: 16.9316, val_MinusLogProbMetric: 16.9316

Epoch 494: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4355 - MinusLogProbMetric: 16.4355 - val_loss: 16.9316 - val_MinusLogProbMetric: 16.9316 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 495/1000
2023-09-26 22:24:52.215 
Epoch 495/1000 
	 loss: 16.4474, MinusLogProbMetric: 16.4474, val_loss: 16.9155, val_MinusLogProbMetric: 16.9155

Epoch 495: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4474 - MinusLogProbMetric: 16.4474 - val_loss: 16.9155 - val_MinusLogProbMetric: 16.9155 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 496/1000
2023-09-26 22:25:57.014 
Epoch 496/1000 
	 loss: 16.4344, MinusLogProbMetric: 16.4344, val_loss: 17.2504, val_MinusLogProbMetric: 17.2504

Epoch 496: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4344 - MinusLogProbMetric: 16.4344 - val_loss: 17.2504 - val_MinusLogProbMetric: 17.2504 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 497/1000
2023-09-26 22:27:01.961 
Epoch 497/1000 
	 loss: 16.4497, MinusLogProbMetric: 16.4497, val_loss: 16.9801, val_MinusLogProbMetric: 16.9801

Epoch 497: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4497 - MinusLogProbMetric: 16.4497 - val_loss: 16.9801 - val_MinusLogProbMetric: 16.9801 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 498/1000
2023-09-26 22:28:06.874 
Epoch 498/1000 
	 loss: 16.4238, MinusLogProbMetric: 16.4238, val_loss: 16.9360, val_MinusLogProbMetric: 16.9360

Epoch 498: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4238 - MinusLogProbMetric: 16.4238 - val_loss: 16.9360 - val_MinusLogProbMetric: 16.9360 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 499/1000
2023-09-26 22:29:11.428 
Epoch 499/1000 
	 loss: 16.4370, MinusLogProbMetric: 16.4370, val_loss: 16.9154, val_MinusLogProbMetric: 16.9154

Epoch 499: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4370 - MinusLogProbMetric: 16.4370 - val_loss: 16.9154 - val_MinusLogProbMetric: 16.9154 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 500/1000
2023-09-26 22:30:16.419 
Epoch 500/1000 
	 loss: 16.4575, MinusLogProbMetric: 16.4575, val_loss: 16.9160, val_MinusLogProbMetric: 16.9160

Epoch 500: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4575 - MinusLogProbMetric: 16.4575 - val_loss: 16.9160 - val_MinusLogProbMetric: 16.9160 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 501/1000
2023-09-26 22:31:20.952 
Epoch 501/1000 
	 loss: 16.4254, MinusLogProbMetric: 16.4254, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 501: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4254 - MinusLogProbMetric: 16.4254 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 502/1000
2023-09-26 22:32:22.668 
Epoch 502/1000 
	 loss: 16.4644, MinusLogProbMetric: 16.4644, val_loss: 16.9358, val_MinusLogProbMetric: 16.9358

Epoch 502: val_loss did not improve from 16.90826
196/196 - 62s - loss: 16.4644 - MinusLogProbMetric: 16.4644 - val_loss: 16.9358 - val_MinusLogProbMetric: 16.9358 - lr: 4.1667e-05 - 62s/epoch - 315ms/step
Epoch 503/1000
2023-09-26 22:33:16.738 
Epoch 503/1000 
	 loss: 16.4293, MinusLogProbMetric: 16.4293, val_loss: 17.0456, val_MinusLogProbMetric: 17.0456

Epoch 503: val_loss did not improve from 16.90826
196/196 - 54s - loss: 16.4293 - MinusLogProbMetric: 16.4293 - val_loss: 17.0456 - val_MinusLogProbMetric: 17.0456 - lr: 4.1667e-05 - 54s/epoch - 276ms/step
Epoch 504/1000
2023-09-26 22:34:09.939 
Epoch 504/1000 
	 loss: 16.4521, MinusLogProbMetric: 16.4521, val_loss: 16.9868, val_MinusLogProbMetric: 16.9868

Epoch 504: val_loss did not improve from 16.90826
196/196 - 53s - loss: 16.4521 - MinusLogProbMetric: 16.4521 - val_loss: 16.9868 - val_MinusLogProbMetric: 16.9868 - lr: 4.1667e-05 - 53s/epoch - 271ms/step
Epoch 505/1000
2023-09-26 22:35:11.530 
Epoch 505/1000 
	 loss: 16.4273, MinusLogProbMetric: 16.4273, val_loss: 16.9330, val_MinusLogProbMetric: 16.9330

Epoch 505: val_loss did not improve from 16.90826
196/196 - 62s - loss: 16.4273 - MinusLogProbMetric: 16.4273 - val_loss: 16.9330 - val_MinusLogProbMetric: 16.9330 - lr: 4.1667e-05 - 62s/epoch - 314ms/step
Epoch 506/1000
2023-09-26 22:36:09.554 
Epoch 506/1000 
	 loss: 16.4340, MinusLogProbMetric: 16.4340, val_loss: 16.9400, val_MinusLogProbMetric: 16.9400

Epoch 506: val_loss did not improve from 16.90826
196/196 - 58s - loss: 16.4340 - MinusLogProbMetric: 16.4340 - val_loss: 16.9400 - val_MinusLogProbMetric: 16.9400 - lr: 4.1667e-05 - 58s/epoch - 296ms/step
Epoch 507/1000
2023-09-26 22:37:02.285 
Epoch 507/1000 
	 loss: 16.4475, MinusLogProbMetric: 16.4475, val_loss: 16.9696, val_MinusLogProbMetric: 16.9696

Epoch 507: val_loss did not improve from 16.90826
196/196 - 53s - loss: 16.4475 - MinusLogProbMetric: 16.4475 - val_loss: 16.9696 - val_MinusLogProbMetric: 16.9696 - lr: 4.1667e-05 - 53s/epoch - 269ms/step
Epoch 508/1000
2023-09-26 22:37:58.978 
Epoch 508/1000 
	 loss: 16.4241, MinusLogProbMetric: 16.4241, val_loss: 17.0472, val_MinusLogProbMetric: 17.0472

Epoch 508: val_loss did not improve from 16.90826
196/196 - 57s - loss: 16.4241 - MinusLogProbMetric: 16.4241 - val_loss: 17.0472 - val_MinusLogProbMetric: 17.0472 - lr: 4.1667e-05 - 57s/epoch - 289ms/step
Epoch 509/1000
2023-09-26 22:39:02.399 
Epoch 509/1000 
	 loss: 16.4317, MinusLogProbMetric: 16.4317, val_loss: 17.1435, val_MinusLogProbMetric: 17.1435

Epoch 509: val_loss did not improve from 16.90826
196/196 - 63s - loss: 16.4317 - MinusLogProbMetric: 16.4317 - val_loss: 17.1435 - val_MinusLogProbMetric: 17.1435 - lr: 4.1667e-05 - 63s/epoch - 324ms/step
Epoch 510/1000
2023-09-26 22:40:06.378 
Epoch 510/1000 
	 loss: 16.4623, MinusLogProbMetric: 16.4623, val_loss: 16.9398, val_MinusLogProbMetric: 16.9398

Epoch 510: val_loss did not improve from 16.90826
196/196 - 64s - loss: 16.4623 - MinusLogProbMetric: 16.4623 - val_loss: 16.9398 - val_MinusLogProbMetric: 16.9398 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 511/1000
2023-09-26 22:41:10.677 
Epoch 511/1000 
	 loss: 16.4262, MinusLogProbMetric: 16.4262, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 511: val_loss did not improve from 16.90826
196/196 - 64s - loss: 16.4262 - MinusLogProbMetric: 16.4262 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 512/1000
2023-09-26 22:42:15.359 
Epoch 512/1000 
	 loss: 16.4212, MinusLogProbMetric: 16.4212, val_loss: 16.9757, val_MinusLogProbMetric: 16.9757

Epoch 512: val_loss did not improve from 16.90826
196/196 - 65s - loss: 16.4212 - MinusLogProbMetric: 16.4212 - val_loss: 16.9757 - val_MinusLogProbMetric: 16.9757 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 513/1000
2023-09-26 22:43:20.089 
Epoch 513/1000 
	 loss: 16.3969, MinusLogProbMetric: 16.3969, val_loss: 16.9005, val_MinusLogProbMetric: 16.9005

Epoch 513: val_loss improved from 16.90826 to 16.90050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3969 - MinusLogProbMetric: 16.3969 - val_loss: 16.9005 - val_MinusLogProbMetric: 16.9005 - lr: 2.0833e-05 - 66s/epoch - 335ms/step
Epoch 514/1000
2023-09-26 22:44:25.107 
Epoch 514/1000 
	 loss: 16.3921, MinusLogProbMetric: 16.3921, val_loss: 16.9134, val_MinusLogProbMetric: 16.9134

Epoch 514: val_loss did not improve from 16.90050
196/196 - 64s - loss: 16.3921 - MinusLogProbMetric: 16.3921 - val_loss: 16.9134 - val_MinusLogProbMetric: 16.9134 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 515/1000
2023-09-26 22:45:29.324 
Epoch 515/1000 
	 loss: 16.3969, MinusLogProbMetric: 16.3969, val_loss: 16.9099, val_MinusLogProbMetric: 16.9099

Epoch 515: val_loss did not improve from 16.90050
196/196 - 64s - loss: 16.3969 - MinusLogProbMetric: 16.3969 - val_loss: 16.9099 - val_MinusLogProbMetric: 16.9099 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 516/1000
2023-09-26 22:46:34.164 
Epoch 516/1000 
	 loss: 16.4108, MinusLogProbMetric: 16.4108, val_loss: 16.9202, val_MinusLogProbMetric: 16.9202

Epoch 516: val_loss did not improve from 16.90050
196/196 - 65s - loss: 16.4108 - MinusLogProbMetric: 16.4108 - val_loss: 16.9202 - val_MinusLogProbMetric: 16.9202 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 517/1000
2023-09-26 22:47:38.734 
Epoch 517/1000 
	 loss: 16.3985, MinusLogProbMetric: 16.3985, val_loss: 16.9111, val_MinusLogProbMetric: 16.9111

Epoch 517: val_loss did not improve from 16.90050
196/196 - 65s - loss: 16.3985 - MinusLogProbMetric: 16.3985 - val_loss: 16.9111 - val_MinusLogProbMetric: 16.9111 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 518/1000
2023-09-26 22:48:42.834 
Epoch 518/1000 
	 loss: 16.3919, MinusLogProbMetric: 16.3919, val_loss: 16.8922, val_MinusLogProbMetric: 16.8922

Epoch 518: val_loss improved from 16.90050 to 16.89215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.3919 - MinusLogProbMetric: 16.3919 - val_loss: 16.8922 - val_MinusLogProbMetric: 16.8922 - lr: 2.0833e-05 - 65s/epoch - 332ms/step
Epoch 519/1000
2023-09-26 22:49:48.412 
Epoch 519/1000 
	 loss: 16.3890, MinusLogProbMetric: 16.3890, val_loss: 16.9411, val_MinusLogProbMetric: 16.9411

Epoch 519: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3890 - MinusLogProbMetric: 16.3890 - val_loss: 16.9411 - val_MinusLogProbMetric: 16.9411 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 520/1000
2023-09-26 22:50:52.644 
Epoch 520/1000 
	 loss: 16.3977, MinusLogProbMetric: 16.3977, val_loss: 16.9419, val_MinusLogProbMetric: 16.9419

Epoch 520: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3977 - MinusLogProbMetric: 16.3977 - val_loss: 16.9419 - val_MinusLogProbMetric: 16.9419 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 521/1000
2023-09-26 22:51:56.889 
Epoch 521/1000 
	 loss: 16.3926, MinusLogProbMetric: 16.3926, val_loss: 16.9040, val_MinusLogProbMetric: 16.9040

Epoch 521: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3926 - MinusLogProbMetric: 16.3926 - val_loss: 16.9040 - val_MinusLogProbMetric: 16.9040 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 522/1000
2023-09-26 22:53:01.169 
Epoch 522/1000 
	 loss: 16.4059, MinusLogProbMetric: 16.4059, val_loss: 16.8953, val_MinusLogProbMetric: 16.8953

Epoch 522: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.4059 - MinusLogProbMetric: 16.4059 - val_loss: 16.8953 - val_MinusLogProbMetric: 16.8953 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 523/1000
2023-09-26 22:54:05.973 
Epoch 523/1000 
	 loss: 16.3962, MinusLogProbMetric: 16.3962, val_loss: 16.9390, val_MinusLogProbMetric: 16.9390

Epoch 523: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3962 - MinusLogProbMetric: 16.3962 - val_loss: 16.9390 - val_MinusLogProbMetric: 16.9390 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 524/1000
2023-09-26 22:55:10.776 
Epoch 524/1000 
	 loss: 16.3952, MinusLogProbMetric: 16.3952, val_loss: 16.8955, val_MinusLogProbMetric: 16.8955

Epoch 524: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3952 - MinusLogProbMetric: 16.3952 - val_loss: 16.8955 - val_MinusLogProbMetric: 16.8955 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 525/1000
2023-09-26 22:56:15.064 
Epoch 525/1000 
	 loss: 16.3968, MinusLogProbMetric: 16.3968, val_loss: 16.8996, val_MinusLogProbMetric: 16.8996

Epoch 525: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3968 - MinusLogProbMetric: 16.3968 - val_loss: 16.8996 - val_MinusLogProbMetric: 16.8996 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 526/1000
2023-09-26 22:57:19.265 
Epoch 526/1000 
	 loss: 16.3907, MinusLogProbMetric: 16.3907, val_loss: 16.9076, val_MinusLogProbMetric: 16.9076

Epoch 526: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3907 - MinusLogProbMetric: 16.3907 - val_loss: 16.9076 - val_MinusLogProbMetric: 16.9076 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 527/1000
2023-09-26 22:58:23.674 
Epoch 527/1000 
	 loss: 16.3941, MinusLogProbMetric: 16.3941, val_loss: 16.9035, val_MinusLogProbMetric: 16.9035

Epoch 527: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3941 - MinusLogProbMetric: 16.3941 - val_loss: 16.9035 - val_MinusLogProbMetric: 16.9035 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 528/1000
2023-09-26 22:59:28.116 
Epoch 528/1000 
	 loss: 16.3978, MinusLogProbMetric: 16.3978, val_loss: 16.9114, val_MinusLogProbMetric: 16.9114

Epoch 528: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3978 - MinusLogProbMetric: 16.3978 - val_loss: 16.9114 - val_MinusLogProbMetric: 16.9114 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 529/1000
2023-09-26 23:00:32.662 
Epoch 529/1000 
	 loss: 16.3921, MinusLogProbMetric: 16.3921, val_loss: 16.9016, val_MinusLogProbMetric: 16.9016

Epoch 529: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3921 - MinusLogProbMetric: 16.3921 - val_loss: 16.9016 - val_MinusLogProbMetric: 16.9016 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 530/1000
2023-09-26 23:01:36.785 
Epoch 530/1000 
	 loss: 16.3956, MinusLogProbMetric: 16.3956, val_loss: 16.9095, val_MinusLogProbMetric: 16.9095

Epoch 530: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3956 - MinusLogProbMetric: 16.3956 - val_loss: 16.9095 - val_MinusLogProbMetric: 16.9095 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 531/1000
2023-09-26 23:02:41.015 
Epoch 531/1000 
	 loss: 16.4000, MinusLogProbMetric: 16.4000, val_loss: 16.8953, val_MinusLogProbMetric: 16.8953

Epoch 531: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.4000 - MinusLogProbMetric: 16.4000 - val_loss: 16.8953 - val_MinusLogProbMetric: 16.8953 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 532/1000
2023-09-26 23:03:44.989 
Epoch 532/1000 
	 loss: 16.3875, MinusLogProbMetric: 16.3875, val_loss: 16.8960, val_MinusLogProbMetric: 16.8960

Epoch 532: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3875 - MinusLogProbMetric: 16.3875 - val_loss: 16.8960 - val_MinusLogProbMetric: 16.8960 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 533/1000
2023-09-26 23:04:49.211 
Epoch 533/1000 
	 loss: 16.3906, MinusLogProbMetric: 16.3906, val_loss: 16.9429, val_MinusLogProbMetric: 16.9429

Epoch 533: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3906 - MinusLogProbMetric: 16.3906 - val_loss: 16.9429 - val_MinusLogProbMetric: 16.9429 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 534/1000
2023-09-26 23:05:53.176 
Epoch 534/1000 
	 loss: 16.3902, MinusLogProbMetric: 16.3902, val_loss: 16.9291, val_MinusLogProbMetric: 16.9291

Epoch 534: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3902 - MinusLogProbMetric: 16.3902 - val_loss: 16.9291 - val_MinusLogProbMetric: 16.9291 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 535/1000
2023-09-26 23:06:57.559 
Epoch 535/1000 
	 loss: 16.3937, MinusLogProbMetric: 16.3937, val_loss: 16.9450, val_MinusLogProbMetric: 16.9450

Epoch 535: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3937 - MinusLogProbMetric: 16.3937 - val_loss: 16.9450 - val_MinusLogProbMetric: 16.9450 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 536/1000
2023-09-26 23:08:03.194 
Epoch 536/1000 
	 loss: 16.4021, MinusLogProbMetric: 16.4021, val_loss: 16.9172, val_MinusLogProbMetric: 16.9172

Epoch 536: val_loss did not improve from 16.89215
196/196 - 66s - loss: 16.4021 - MinusLogProbMetric: 16.4021 - val_loss: 16.9172 - val_MinusLogProbMetric: 16.9172 - lr: 2.0833e-05 - 66s/epoch - 335ms/step
Epoch 537/1000
2023-09-26 23:09:07.888 
Epoch 537/1000 
	 loss: 16.3932, MinusLogProbMetric: 16.3932, val_loss: 16.9090, val_MinusLogProbMetric: 16.9090

Epoch 537: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3932 - MinusLogProbMetric: 16.3932 - val_loss: 16.9090 - val_MinusLogProbMetric: 16.9090 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 538/1000
2023-09-26 23:10:12.389 
Epoch 538/1000 
	 loss: 16.3957, MinusLogProbMetric: 16.3957, val_loss: 16.9517, val_MinusLogProbMetric: 16.9517

Epoch 538: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3957 - MinusLogProbMetric: 16.3957 - val_loss: 16.9517 - val_MinusLogProbMetric: 16.9517 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 539/1000
2023-09-26 23:11:16.575 
Epoch 539/1000 
	 loss: 16.3949, MinusLogProbMetric: 16.3949, val_loss: 16.9422, val_MinusLogProbMetric: 16.9422

Epoch 539: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3949 - MinusLogProbMetric: 16.3949 - val_loss: 16.9422 - val_MinusLogProbMetric: 16.9422 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 540/1000
2023-09-26 23:12:20.923 
Epoch 540/1000 
	 loss: 16.4087, MinusLogProbMetric: 16.4087, val_loss: 16.9067, val_MinusLogProbMetric: 16.9067

Epoch 540: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.4087 - MinusLogProbMetric: 16.4087 - val_loss: 16.9067 - val_MinusLogProbMetric: 16.9067 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 541/1000
2023-09-26 23:13:25.520 
Epoch 541/1000 
	 loss: 16.3947, MinusLogProbMetric: 16.3947, val_loss: 16.9157, val_MinusLogProbMetric: 16.9157

Epoch 541: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3947 - MinusLogProbMetric: 16.3947 - val_loss: 16.9157 - val_MinusLogProbMetric: 16.9157 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 542/1000
2023-09-26 23:14:29.839 
Epoch 542/1000 
	 loss: 16.3969, MinusLogProbMetric: 16.3969, val_loss: 16.9013, val_MinusLogProbMetric: 16.9013

Epoch 542: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3969 - MinusLogProbMetric: 16.3969 - val_loss: 16.9013 - val_MinusLogProbMetric: 16.9013 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 543/1000
2023-09-26 23:15:34.354 
Epoch 543/1000 
	 loss: 16.3916, MinusLogProbMetric: 16.3916, val_loss: 16.9039, val_MinusLogProbMetric: 16.9039

Epoch 543: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3916 - MinusLogProbMetric: 16.3916 - val_loss: 16.9039 - val_MinusLogProbMetric: 16.9039 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 544/1000
2023-09-26 23:16:38.826 
Epoch 544/1000 
	 loss: 16.3958, MinusLogProbMetric: 16.3958, val_loss: 16.8956, val_MinusLogProbMetric: 16.8956

Epoch 544: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3958 - MinusLogProbMetric: 16.3958 - val_loss: 16.8956 - val_MinusLogProbMetric: 16.8956 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 545/1000
2023-09-26 23:17:43.383 
Epoch 545/1000 
	 loss: 16.3946, MinusLogProbMetric: 16.3946, val_loss: 16.9337, val_MinusLogProbMetric: 16.9337

Epoch 545: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3946 - MinusLogProbMetric: 16.3946 - val_loss: 16.9337 - val_MinusLogProbMetric: 16.9337 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 546/1000
2023-09-26 23:18:48.054 
Epoch 546/1000 
	 loss: 16.4027, MinusLogProbMetric: 16.4027, val_loss: 16.8979, val_MinusLogProbMetric: 16.8979

Epoch 546: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.4027 - MinusLogProbMetric: 16.4027 - val_loss: 16.8979 - val_MinusLogProbMetric: 16.8979 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 547/1000
2023-09-26 23:19:52.863 
Epoch 547/1000 
	 loss: 16.3900, MinusLogProbMetric: 16.3900, val_loss: 16.9075, val_MinusLogProbMetric: 16.9075

Epoch 547: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3900 - MinusLogProbMetric: 16.3900 - val_loss: 16.9075 - val_MinusLogProbMetric: 16.9075 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 548/1000
2023-09-26 23:20:57.225 
Epoch 548/1000 
	 loss: 16.3880, MinusLogProbMetric: 16.3880, val_loss: 16.8950, val_MinusLogProbMetric: 16.8950

Epoch 548: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3880 - MinusLogProbMetric: 16.3880 - val_loss: 16.8950 - val_MinusLogProbMetric: 16.8950 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 549/1000
2023-09-26 23:22:01.567 
Epoch 549/1000 
	 loss: 16.3909, MinusLogProbMetric: 16.3909, val_loss: 16.9051, val_MinusLogProbMetric: 16.9051

Epoch 549: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3909 - MinusLogProbMetric: 16.3909 - val_loss: 16.9051 - val_MinusLogProbMetric: 16.9051 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 550/1000
2023-09-26 23:23:06.018 
Epoch 550/1000 
	 loss: 16.3906, MinusLogProbMetric: 16.3906, val_loss: 16.8950, val_MinusLogProbMetric: 16.8950

Epoch 550: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3906 - MinusLogProbMetric: 16.3906 - val_loss: 16.8950 - val_MinusLogProbMetric: 16.8950 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 551/1000
2023-09-26 23:24:10.660 
Epoch 551/1000 
	 loss: 16.3913, MinusLogProbMetric: 16.3913, val_loss: 16.9702, val_MinusLogProbMetric: 16.9702

Epoch 551: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3913 - MinusLogProbMetric: 16.3913 - val_loss: 16.9702 - val_MinusLogProbMetric: 16.9702 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 552/1000
2023-09-26 23:25:15.176 
Epoch 552/1000 
	 loss: 16.3899, MinusLogProbMetric: 16.3899, val_loss: 16.9049, val_MinusLogProbMetric: 16.9049

Epoch 552: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3899 - MinusLogProbMetric: 16.3899 - val_loss: 16.9049 - val_MinusLogProbMetric: 16.9049 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 553/1000
2023-09-26 23:26:19.335 
Epoch 553/1000 
	 loss: 16.3978, MinusLogProbMetric: 16.3978, val_loss: 16.9057, val_MinusLogProbMetric: 16.9057

Epoch 553: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3978 - MinusLogProbMetric: 16.3978 - val_loss: 16.9057 - val_MinusLogProbMetric: 16.9057 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 554/1000
2023-09-26 23:27:23.240 
Epoch 554/1000 
	 loss: 16.3966, MinusLogProbMetric: 16.3966, val_loss: 16.8948, val_MinusLogProbMetric: 16.8948

Epoch 554: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3966 - MinusLogProbMetric: 16.3966 - val_loss: 16.8948 - val_MinusLogProbMetric: 16.8948 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 555/1000
2023-09-26 23:28:28.223 
Epoch 555/1000 
	 loss: 16.3942, MinusLogProbMetric: 16.3942, val_loss: 16.9127, val_MinusLogProbMetric: 16.9127

Epoch 555: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3942 - MinusLogProbMetric: 16.3942 - val_loss: 16.9127 - val_MinusLogProbMetric: 16.9127 - lr: 2.0833e-05 - 65s/epoch - 332ms/step
Epoch 556/1000
2023-09-26 23:29:33.004 
Epoch 556/1000 
	 loss: 16.3963, MinusLogProbMetric: 16.3963, val_loss: 16.9426, val_MinusLogProbMetric: 16.9426

Epoch 556: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.3963 - MinusLogProbMetric: 16.3963 - val_loss: 16.9426 - val_MinusLogProbMetric: 16.9426 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 557/1000
2023-09-26 23:30:37.091 
Epoch 557/1000 
	 loss: 16.3964, MinusLogProbMetric: 16.3964, val_loss: 16.9362, val_MinusLogProbMetric: 16.9362

Epoch 557: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3964 - MinusLogProbMetric: 16.3964 - val_loss: 16.9362 - val_MinusLogProbMetric: 16.9362 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 558/1000
2023-09-26 23:31:41.864 
Epoch 558/1000 
	 loss: 16.4007, MinusLogProbMetric: 16.4007, val_loss: 16.8927, val_MinusLogProbMetric: 16.8927

Epoch 558: val_loss did not improve from 16.89215
196/196 - 65s - loss: 16.4007 - MinusLogProbMetric: 16.4007 - val_loss: 16.8927 - val_MinusLogProbMetric: 16.8927 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 559/1000
2023-09-26 23:32:46.032 
Epoch 559/1000 
	 loss: 16.3898, MinusLogProbMetric: 16.3898, val_loss: 16.9039, val_MinusLogProbMetric: 16.9039

Epoch 559: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3898 - MinusLogProbMetric: 16.3898 - val_loss: 16.9039 - val_MinusLogProbMetric: 16.9039 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 560/1000
2023-09-26 23:33:50.472 
Epoch 560/1000 
	 loss: 16.3873, MinusLogProbMetric: 16.3873, val_loss: 16.9049, val_MinusLogProbMetric: 16.9049

Epoch 560: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3873 - MinusLogProbMetric: 16.3873 - val_loss: 16.9049 - val_MinusLogProbMetric: 16.9049 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 561/1000
2023-09-26 23:34:54.800 
Epoch 561/1000 
	 loss: 16.3904, MinusLogProbMetric: 16.3904, val_loss: 16.9838, val_MinusLogProbMetric: 16.9838

Epoch 561: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3904 - MinusLogProbMetric: 16.3904 - val_loss: 16.9838 - val_MinusLogProbMetric: 16.9838 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 562/1000
2023-09-26 23:35:58.757 
Epoch 562/1000 
	 loss: 16.3912, MinusLogProbMetric: 16.3912, val_loss: 16.9198, val_MinusLogProbMetric: 16.9198

Epoch 562: val_loss did not improve from 16.89215
196/196 - 64s - loss: 16.3912 - MinusLogProbMetric: 16.3912 - val_loss: 16.9198 - val_MinusLogProbMetric: 16.9198 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 563/1000
2023-09-26 23:37:03.049 
Epoch 563/1000 
	 loss: 16.3859, MinusLogProbMetric: 16.3859, val_loss: 16.8919, val_MinusLogProbMetric: 16.8919

Epoch 563: val_loss improved from 16.89215 to 16.89190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.3859 - MinusLogProbMetric: 16.3859 - val_loss: 16.8919 - val_MinusLogProbMetric: 16.8919 - lr: 2.0833e-05 - 65s/epoch - 333ms/step
Epoch 564/1000
2023-09-26 23:38:08.207 
Epoch 564/1000 
	 loss: 16.3970, MinusLogProbMetric: 16.3970, val_loss: 16.9625, val_MinusLogProbMetric: 16.9625

Epoch 564: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3970 - MinusLogProbMetric: 16.3970 - val_loss: 16.9625 - val_MinusLogProbMetric: 16.9625 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 565/1000
2023-09-26 23:39:12.829 
Epoch 565/1000 
	 loss: 16.3881, MinusLogProbMetric: 16.3881, val_loss: 16.8980, val_MinusLogProbMetric: 16.8980

Epoch 565: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3881 - MinusLogProbMetric: 16.3881 - val_loss: 16.8980 - val_MinusLogProbMetric: 16.8980 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 566/1000
2023-09-26 23:40:17.118 
Epoch 566/1000 
	 loss: 16.3908, MinusLogProbMetric: 16.3908, val_loss: 16.8962, val_MinusLogProbMetric: 16.8962

Epoch 566: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3908 - MinusLogProbMetric: 16.3908 - val_loss: 16.8962 - val_MinusLogProbMetric: 16.8962 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 567/1000
2023-09-26 23:41:21.234 
Epoch 567/1000 
	 loss: 16.3917, MinusLogProbMetric: 16.3917, val_loss: 16.9049, val_MinusLogProbMetric: 16.9049

Epoch 567: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3917 - MinusLogProbMetric: 16.3917 - val_loss: 16.9049 - val_MinusLogProbMetric: 16.9049 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 568/1000
2023-09-26 23:42:25.629 
Epoch 568/1000 
	 loss: 16.3876, MinusLogProbMetric: 16.3876, val_loss: 16.9121, val_MinusLogProbMetric: 16.9121

Epoch 568: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3876 - MinusLogProbMetric: 16.3876 - val_loss: 16.9121 - val_MinusLogProbMetric: 16.9121 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 569/1000
2023-09-26 23:43:29.872 
Epoch 569/1000 
	 loss: 16.3883, MinusLogProbMetric: 16.3883, val_loss: 16.8928, val_MinusLogProbMetric: 16.8928

Epoch 569: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3883 - MinusLogProbMetric: 16.3883 - val_loss: 16.8928 - val_MinusLogProbMetric: 16.8928 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 570/1000
2023-09-26 23:44:34.706 
Epoch 570/1000 
	 loss: 16.3907, MinusLogProbMetric: 16.3907, val_loss: 16.9052, val_MinusLogProbMetric: 16.9052

Epoch 570: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3907 - MinusLogProbMetric: 16.3907 - val_loss: 16.9052 - val_MinusLogProbMetric: 16.9052 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 571/1000
2023-09-26 23:45:39.095 
Epoch 571/1000 
	 loss: 16.3952, MinusLogProbMetric: 16.3952, val_loss: 16.9641, val_MinusLogProbMetric: 16.9641

Epoch 571: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3952 - MinusLogProbMetric: 16.3952 - val_loss: 16.9641 - val_MinusLogProbMetric: 16.9641 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 572/1000
2023-09-26 23:46:43.778 
Epoch 572/1000 
	 loss: 16.3936, MinusLogProbMetric: 16.3936, val_loss: 16.9254, val_MinusLogProbMetric: 16.9254

Epoch 572: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3936 - MinusLogProbMetric: 16.3936 - val_loss: 16.9254 - val_MinusLogProbMetric: 16.9254 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 573/1000
2023-09-26 23:47:47.780 
Epoch 573/1000 
	 loss: 16.3910, MinusLogProbMetric: 16.3910, val_loss: 16.8988, val_MinusLogProbMetric: 16.8988

Epoch 573: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3910 - MinusLogProbMetric: 16.3910 - val_loss: 16.8988 - val_MinusLogProbMetric: 16.8988 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 574/1000
2023-09-26 23:48:52.089 
Epoch 574/1000 
	 loss: 16.3866, MinusLogProbMetric: 16.3866, val_loss: 16.9191, val_MinusLogProbMetric: 16.9191

Epoch 574: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3866 - MinusLogProbMetric: 16.3866 - val_loss: 16.9191 - val_MinusLogProbMetric: 16.9191 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 575/1000
2023-09-26 23:49:56.192 
Epoch 575/1000 
	 loss: 16.3916, MinusLogProbMetric: 16.3916, val_loss: 16.9307, val_MinusLogProbMetric: 16.9307

Epoch 575: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3916 - MinusLogProbMetric: 16.3916 - val_loss: 16.9307 - val_MinusLogProbMetric: 16.9307 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 576/1000
2023-09-26 23:51:00.616 
Epoch 576/1000 
	 loss: 16.3860, MinusLogProbMetric: 16.3860, val_loss: 16.9205, val_MinusLogProbMetric: 16.9205

Epoch 576: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3860 - MinusLogProbMetric: 16.3860 - val_loss: 16.9205 - val_MinusLogProbMetric: 16.9205 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 577/1000
2023-09-26 23:52:05.518 
Epoch 577/1000 
	 loss: 16.3940, MinusLogProbMetric: 16.3940, val_loss: 16.9272, val_MinusLogProbMetric: 16.9272

Epoch 577: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3940 - MinusLogProbMetric: 16.3940 - val_loss: 16.9272 - val_MinusLogProbMetric: 16.9272 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 578/1000
2023-09-26 23:53:09.989 
Epoch 578/1000 
	 loss: 16.3872, MinusLogProbMetric: 16.3872, val_loss: 16.8943, val_MinusLogProbMetric: 16.8943

Epoch 578: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3872 - MinusLogProbMetric: 16.3872 - val_loss: 16.8943 - val_MinusLogProbMetric: 16.8943 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 579/1000
2023-09-26 23:54:14.564 
Epoch 579/1000 
	 loss: 16.3884, MinusLogProbMetric: 16.3884, val_loss: 16.9631, val_MinusLogProbMetric: 16.9631

Epoch 579: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3884 - MinusLogProbMetric: 16.3884 - val_loss: 16.9631 - val_MinusLogProbMetric: 16.9631 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 580/1000
2023-09-26 23:55:19.222 
Epoch 580/1000 
	 loss: 16.3958, MinusLogProbMetric: 16.3958, val_loss: 16.9775, val_MinusLogProbMetric: 16.9775

Epoch 580: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3958 - MinusLogProbMetric: 16.3958 - val_loss: 16.9775 - val_MinusLogProbMetric: 16.9775 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 581/1000
2023-09-26 23:56:23.878 
Epoch 581/1000 
	 loss: 16.3889, MinusLogProbMetric: 16.3889, val_loss: 16.9053, val_MinusLogProbMetric: 16.9053

Epoch 581: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3889 - MinusLogProbMetric: 16.3889 - val_loss: 16.9053 - val_MinusLogProbMetric: 16.9053 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 582/1000
2023-09-26 23:57:28.313 
Epoch 582/1000 
	 loss: 16.3854, MinusLogProbMetric: 16.3854, val_loss: 16.9059, val_MinusLogProbMetric: 16.9059

Epoch 582: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3854 - MinusLogProbMetric: 16.3854 - val_loss: 16.9059 - val_MinusLogProbMetric: 16.9059 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 583/1000
2023-09-26 23:58:32.826 
Epoch 583/1000 
	 loss: 16.3959, MinusLogProbMetric: 16.3959, val_loss: 16.8991, val_MinusLogProbMetric: 16.8991

Epoch 583: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3959 - MinusLogProbMetric: 16.3959 - val_loss: 16.8991 - val_MinusLogProbMetric: 16.8991 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 584/1000
2023-09-26 23:59:37.317 
Epoch 584/1000 
	 loss: 16.3903, MinusLogProbMetric: 16.3903, val_loss: 16.9052, val_MinusLogProbMetric: 16.9052

Epoch 584: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3903 - MinusLogProbMetric: 16.3903 - val_loss: 16.9052 - val_MinusLogProbMetric: 16.9052 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 585/1000
2023-09-27 00:00:42.126 
Epoch 585/1000 
	 loss: 16.3880, MinusLogProbMetric: 16.3880, val_loss: 16.9371, val_MinusLogProbMetric: 16.9371

Epoch 585: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3880 - MinusLogProbMetric: 16.3880 - val_loss: 16.9371 - val_MinusLogProbMetric: 16.9371 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 586/1000
2023-09-27 00:01:46.449 
Epoch 586/1000 
	 loss: 16.3838, MinusLogProbMetric: 16.3838, val_loss: 16.9023, val_MinusLogProbMetric: 16.9023

Epoch 586: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3838 - MinusLogProbMetric: 16.3838 - val_loss: 16.9023 - val_MinusLogProbMetric: 16.9023 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 587/1000
2023-09-27 00:02:50.767 
Epoch 587/1000 
	 loss: 16.3931, MinusLogProbMetric: 16.3931, val_loss: 16.9179, val_MinusLogProbMetric: 16.9179

Epoch 587: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3931 - MinusLogProbMetric: 16.3931 - val_loss: 16.9179 - val_MinusLogProbMetric: 16.9179 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 588/1000
2023-09-27 00:03:55.252 
Epoch 588/1000 
	 loss: 16.4052, MinusLogProbMetric: 16.4052, val_loss: 16.9188, val_MinusLogProbMetric: 16.9188

Epoch 588: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.4052 - MinusLogProbMetric: 16.4052 - val_loss: 16.9188 - val_MinusLogProbMetric: 16.9188 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 589/1000
2023-09-27 00:04:59.668 
Epoch 589/1000 
	 loss: 16.3849, MinusLogProbMetric: 16.3849, val_loss: 16.9045, val_MinusLogProbMetric: 16.9045

Epoch 589: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3849 - MinusLogProbMetric: 16.3849 - val_loss: 16.9045 - val_MinusLogProbMetric: 16.9045 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 590/1000
2023-09-27 00:06:03.901 
Epoch 590/1000 
	 loss: 16.3933, MinusLogProbMetric: 16.3933, val_loss: 16.9134, val_MinusLogProbMetric: 16.9134

Epoch 590: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3933 - MinusLogProbMetric: 16.3933 - val_loss: 16.9134 - val_MinusLogProbMetric: 16.9134 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 591/1000
2023-09-27 00:07:08.693 
Epoch 591/1000 
	 loss: 16.3853, MinusLogProbMetric: 16.3853, val_loss: 16.9018, val_MinusLogProbMetric: 16.9018

Epoch 591: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3853 - MinusLogProbMetric: 16.3853 - val_loss: 16.9018 - val_MinusLogProbMetric: 16.9018 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 592/1000
2023-09-27 00:08:13.326 
Epoch 592/1000 
	 loss: 16.3889, MinusLogProbMetric: 16.3889, val_loss: 16.9121, val_MinusLogProbMetric: 16.9121

Epoch 592: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3889 - MinusLogProbMetric: 16.3889 - val_loss: 16.9121 - val_MinusLogProbMetric: 16.9121 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 593/1000
2023-09-27 00:09:18.255 
Epoch 593/1000 
	 loss: 16.3883, MinusLogProbMetric: 16.3883, val_loss: 16.9172, val_MinusLogProbMetric: 16.9172

Epoch 593: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3883 - MinusLogProbMetric: 16.3883 - val_loss: 16.9172 - val_MinusLogProbMetric: 16.9172 - lr: 2.0833e-05 - 65s/epoch - 331ms/step
Epoch 594/1000
2023-09-27 00:10:22.591 
Epoch 594/1000 
	 loss: 16.3806, MinusLogProbMetric: 16.3806, val_loss: 16.8966, val_MinusLogProbMetric: 16.8966

Epoch 594: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3806 - MinusLogProbMetric: 16.3806 - val_loss: 16.8966 - val_MinusLogProbMetric: 16.8966 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 595/1000
2023-09-27 00:11:26.701 
Epoch 595/1000 
	 loss: 16.3878, MinusLogProbMetric: 16.3878, val_loss: 16.9534, val_MinusLogProbMetric: 16.9534

Epoch 595: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3878 - MinusLogProbMetric: 16.3878 - val_loss: 16.9534 - val_MinusLogProbMetric: 16.9534 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 596/1000
2023-09-27 00:12:31.053 
Epoch 596/1000 
	 loss: 16.4046, MinusLogProbMetric: 16.4046, val_loss: 16.9905, val_MinusLogProbMetric: 16.9905

Epoch 596: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.4046 - MinusLogProbMetric: 16.4046 - val_loss: 16.9905 - val_MinusLogProbMetric: 16.9905 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 597/1000
2023-09-27 00:13:35.603 
Epoch 597/1000 
	 loss: 16.3928, MinusLogProbMetric: 16.3928, val_loss: 16.8989, val_MinusLogProbMetric: 16.8989

Epoch 597: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3928 - MinusLogProbMetric: 16.3928 - val_loss: 16.8989 - val_MinusLogProbMetric: 16.8989 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 598/1000
2023-09-27 00:14:40.168 
Epoch 598/1000 
	 loss: 16.3959, MinusLogProbMetric: 16.3959, val_loss: 16.9246, val_MinusLogProbMetric: 16.9246

Epoch 598: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3959 - MinusLogProbMetric: 16.3959 - val_loss: 16.9246 - val_MinusLogProbMetric: 16.9246 - lr: 2.0833e-05 - 65s/epoch - 329ms/step
Epoch 599/1000
2023-09-27 00:15:44.446 
Epoch 599/1000 
	 loss: 16.3939, MinusLogProbMetric: 16.3939, val_loss: 16.9094, val_MinusLogProbMetric: 16.9094

Epoch 599: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3939 - MinusLogProbMetric: 16.3939 - val_loss: 16.9094 - val_MinusLogProbMetric: 16.9094 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 600/1000
2023-09-27 00:16:49.099 
Epoch 600/1000 
	 loss: 16.3925, MinusLogProbMetric: 16.3925, val_loss: 16.9236, val_MinusLogProbMetric: 16.9236

Epoch 600: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3925 - MinusLogProbMetric: 16.3925 - val_loss: 16.9236 - val_MinusLogProbMetric: 16.9236 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 601/1000
2023-09-27 00:17:54.106 
Epoch 601/1000 
	 loss: 16.3844, MinusLogProbMetric: 16.3844, val_loss: 16.9005, val_MinusLogProbMetric: 16.9005

Epoch 601: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3844 - MinusLogProbMetric: 16.3844 - val_loss: 16.9005 - val_MinusLogProbMetric: 16.9005 - lr: 2.0833e-05 - 65s/epoch - 332ms/step
Epoch 602/1000
2023-09-27 00:18:58.859 
Epoch 602/1000 
	 loss: 16.3831, MinusLogProbMetric: 16.3831, val_loss: 16.9002, val_MinusLogProbMetric: 16.9002

Epoch 602: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3831 - MinusLogProbMetric: 16.3831 - val_loss: 16.9002 - val_MinusLogProbMetric: 16.9002 - lr: 2.0833e-05 - 65s/epoch - 330ms/step
Epoch 603/1000
2023-09-27 00:20:03.016 
Epoch 603/1000 
	 loss: 16.3989, MinusLogProbMetric: 16.3989, val_loss: 17.1044, val_MinusLogProbMetric: 17.1044

Epoch 603: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3989 - MinusLogProbMetric: 16.3989 - val_loss: 17.1044 - val_MinusLogProbMetric: 17.1044 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 604/1000
2023-09-27 00:21:07.030 
Epoch 604/1000 
	 loss: 16.3979, MinusLogProbMetric: 16.3979, val_loss: 16.8973, val_MinusLogProbMetric: 16.8973

Epoch 604: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3979 - MinusLogProbMetric: 16.3979 - val_loss: 16.8973 - val_MinusLogProbMetric: 16.8973 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 605/1000
2023-09-27 00:22:11.122 
Epoch 605/1000 
	 loss: 16.3867, MinusLogProbMetric: 16.3867, val_loss: 16.9846, val_MinusLogProbMetric: 16.9846

Epoch 605: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3867 - MinusLogProbMetric: 16.3867 - val_loss: 16.9846 - val_MinusLogProbMetric: 16.9846 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 606/1000
2023-09-27 00:23:16.193 
Epoch 606/1000 
	 loss: 16.3912, MinusLogProbMetric: 16.3912, val_loss: 16.8990, val_MinusLogProbMetric: 16.8990

Epoch 606: val_loss did not improve from 16.89190
196/196 - 65s - loss: 16.3912 - MinusLogProbMetric: 16.3912 - val_loss: 16.8990 - val_MinusLogProbMetric: 16.8990 - lr: 2.0833e-05 - 65s/epoch - 332ms/step
Epoch 607/1000
2023-09-27 00:24:18.403 
Epoch 607/1000 
	 loss: 16.3888, MinusLogProbMetric: 16.3888, val_loss: 16.9756, val_MinusLogProbMetric: 16.9756

Epoch 607: val_loss did not improve from 16.89190
196/196 - 62s - loss: 16.3888 - MinusLogProbMetric: 16.3888 - val_loss: 16.9756 - val_MinusLogProbMetric: 16.9756 - lr: 2.0833e-05 - 62s/epoch - 317ms/step
Epoch 608/1000
2023-09-27 00:25:15.388 
Epoch 608/1000 
	 loss: 16.3855, MinusLogProbMetric: 16.3855, val_loss: 16.8935, val_MinusLogProbMetric: 16.8935

Epoch 608: val_loss did not improve from 16.89190
196/196 - 57s - loss: 16.3855 - MinusLogProbMetric: 16.3855 - val_loss: 16.8935 - val_MinusLogProbMetric: 16.8935 - lr: 2.0833e-05 - 57s/epoch - 291ms/step
Epoch 609/1000
2023-09-27 00:26:17.954 
Epoch 609/1000 
	 loss: 16.3847, MinusLogProbMetric: 16.3847, val_loss: 16.9802, val_MinusLogProbMetric: 16.9802

Epoch 609: val_loss did not improve from 16.89190
196/196 - 63s - loss: 16.3847 - MinusLogProbMetric: 16.3847 - val_loss: 16.9802 - val_MinusLogProbMetric: 16.9802 - lr: 2.0833e-05 - 63s/epoch - 319ms/step
Epoch 610/1000
2023-09-27 00:27:21.936 
Epoch 610/1000 
	 loss: 16.3854, MinusLogProbMetric: 16.3854, val_loss: 16.9056, val_MinusLogProbMetric: 16.9056

Epoch 610: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3854 - MinusLogProbMetric: 16.3854 - val_loss: 16.9056 - val_MinusLogProbMetric: 16.9056 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 611/1000
2023-09-27 00:28:25.950 
Epoch 611/1000 
	 loss: 16.3919, MinusLogProbMetric: 16.3919, val_loss: 16.9087, val_MinusLogProbMetric: 16.9087

Epoch 611: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3919 - MinusLogProbMetric: 16.3919 - val_loss: 16.9087 - val_MinusLogProbMetric: 16.9087 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 612/1000
2023-09-27 00:29:29.791 
Epoch 612/1000 
	 loss: 16.3899, MinusLogProbMetric: 16.3899, val_loss: 16.9342, val_MinusLogProbMetric: 16.9342

Epoch 612: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3899 - MinusLogProbMetric: 16.3899 - val_loss: 16.9342 - val_MinusLogProbMetric: 16.9342 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 613/1000
2023-09-27 00:30:33.848 
Epoch 613/1000 
	 loss: 16.3970, MinusLogProbMetric: 16.3970, val_loss: 16.9634, val_MinusLogProbMetric: 16.9634

Epoch 613: val_loss did not improve from 16.89190
196/196 - 64s - loss: 16.3970 - MinusLogProbMetric: 16.3970 - val_loss: 16.9634 - val_MinusLogProbMetric: 16.9634 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 614/1000
2023-09-27 00:31:37.864 
Epoch 614/1000 
	 loss: 16.3701, MinusLogProbMetric: 16.3701, val_loss: 16.8861, val_MinusLogProbMetric: 16.8861

Epoch 614: val_loss improved from 16.89190 to 16.88609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.3701 - MinusLogProbMetric: 16.3701 - val_loss: 16.8861 - val_MinusLogProbMetric: 16.8861 - lr: 1.0417e-05 - 65s/epoch - 332ms/step
Epoch 615/1000
2023-09-27 00:32:42.990 
Epoch 615/1000 
	 loss: 16.3657, MinusLogProbMetric: 16.3657, val_loss: 16.8910, val_MinusLogProbMetric: 16.8910

Epoch 615: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3657 - MinusLogProbMetric: 16.3657 - val_loss: 16.8910 - val_MinusLogProbMetric: 16.8910 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 616/1000
2023-09-27 00:33:47.123 
Epoch 616/1000 
	 loss: 16.3681, MinusLogProbMetric: 16.3681, val_loss: 16.8879, val_MinusLogProbMetric: 16.8879

Epoch 616: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3681 - MinusLogProbMetric: 16.3681 - val_loss: 16.8879 - val_MinusLogProbMetric: 16.8879 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 617/1000
2023-09-27 00:34:50.745 
Epoch 617/1000 
	 loss: 16.3664, MinusLogProbMetric: 16.3664, val_loss: 16.9011, val_MinusLogProbMetric: 16.9011

Epoch 617: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3664 - MinusLogProbMetric: 16.3664 - val_loss: 16.9011 - val_MinusLogProbMetric: 16.9011 - lr: 1.0417e-05 - 64s/epoch - 325ms/step
Epoch 618/1000
2023-09-27 00:35:55.069 
Epoch 618/1000 
	 loss: 16.3658, MinusLogProbMetric: 16.3658, val_loss: 16.8890, val_MinusLogProbMetric: 16.8890

Epoch 618: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3658 - MinusLogProbMetric: 16.3658 - val_loss: 16.8890 - val_MinusLogProbMetric: 16.8890 - lr: 1.0417e-05 - 64s/epoch - 328ms/step
Epoch 619/1000
2023-09-27 00:36:59.074 
Epoch 619/1000 
	 loss: 16.3667, MinusLogProbMetric: 16.3667, val_loss: 16.8956, val_MinusLogProbMetric: 16.8956

Epoch 619: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3667 - MinusLogProbMetric: 16.3667 - val_loss: 16.8956 - val_MinusLogProbMetric: 16.8956 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 620/1000
2023-09-27 00:38:03.269 
Epoch 620/1000 
	 loss: 16.3717, MinusLogProbMetric: 16.3717, val_loss: 16.8893, val_MinusLogProbMetric: 16.8893

Epoch 620: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3717 - MinusLogProbMetric: 16.3717 - val_loss: 16.8893 - val_MinusLogProbMetric: 16.8893 - lr: 1.0417e-05 - 64s/epoch - 328ms/step
Epoch 621/1000
2023-09-27 00:39:06.995 
Epoch 621/1000 
	 loss: 16.3643, MinusLogProbMetric: 16.3643, val_loss: 16.8951, val_MinusLogProbMetric: 16.8951

Epoch 621: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3643 - MinusLogProbMetric: 16.3643 - val_loss: 16.8951 - val_MinusLogProbMetric: 16.8951 - lr: 1.0417e-05 - 64s/epoch - 325ms/step
Epoch 622/1000
2023-09-27 00:40:11.215 
Epoch 622/1000 
	 loss: 16.3648, MinusLogProbMetric: 16.3648, val_loss: 16.8881, val_MinusLogProbMetric: 16.8881

Epoch 622: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3648 - MinusLogProbMetric: 16.3648 - val_loss: 16.8881 - val_MinusLogProbMetric: 16.8881 - lr: 1.0417e-05 - 64s/epoch - 328ms/step
Epoch 623/1000
2023-09-27 00:41:15.131 
Epoch 623/1000 
	 loss: 16.3652, MinusLogProbMetric: 16.3652, val_loss: 16.9059, val_MinusLogProbMetric: 16.9059

Epoch 623: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3652 - MinusLogProbMetric: 16.3652 - val_loss: 16.9059 - val_MinusLogProbMetric: 16.9059 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 624/1000
2023-09-27 00:42:19.101 
Epoch 624/1000 
	 loss: 16.3640, MinusLogProbMetric: 16.3640, val_loss: 16.8967, val_MinusLogProbMetric: 16.8967

Epoch 624: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3640 - MinusLogProbMetric: 16.3640 - val_loss: 16.8967 - val_MinusLogProbMetric: 16.8967 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 625/1000
2023-09-27 00:43:23.677 
Epoch 625/1000 
	 loss: 16.3660, MinusLogProbMetric: 16.3660, val_loss: 16.8876, val_MinusLogProbMetric: 16.8876

Epoch 625: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3660 - MinusLogProbMetric: 16.3660 - val_loss: 16.8876 - val_MinusLogProbMetric: 16.8876 - lr: 1.0417e-05 - 65s/epoch - 329ms/step
Epoch 626/1000
2023-09-27 00:44:27.128 
Epoch 626/1000 
	 loss: 16.3672, MinusLogProbMetric: 16.3672, val_loss: 16.8922, val_MinusLogProbMetric: 16.8922

Epoch 626: val_loss did not improve from 16.88609
196/196 - 63s - loss: 16.3672 - MinusLogProbMetric: 16.3672 - val_loss: 16.8922 - val_MinusLogProbMetric: 16.8922 - lr: 1.0417e-05 - 63s/epoch - 324ms/step
Epoch 627/1000
2023-09-27 00:45:30.555 
Epoch 627/1000 
	 loss: 16.3658, MinusLogProbMetric: 16.3658, val_loss: 16.9061, val_MinusLogProbMetric: 16.9061

Epoch 627: val_loss did not improve from 16.88609
196/196 - 63s - loss: 16.3658 - MinusLogProbMetric: 16.3658 - val_loss: 16.9061 - val_MinusLogProbMetric: 16.9061 - lr: 1.0417e-05 - 63s/epoch - 324ms/step
Epoch 628/1000
2023-09-27 00:46:33.917 
Epoch 628/1000 
	 loss: 16.3641, MinusLogProbMetric: 16.3641, val_loss: 16.8916, val_MinusLogProbMetric: 16.8916

Epoch 628: val_loss did not improve from 16.88609
196/196 - 63s - loss: 16.3641 - MinusLogProbMetric: 16.3641 - val_loss: 16.8916 - val_MinusLogProbMetric: 16.8916 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 629/1000
2023-09-27 00:47:37.913 
Epoch 629/1000 
	 loss: 16.3672, MinusLogProbMetric: 16.3672, val_loss: 16.8920, val_MinusLogProbMetric: 16.8920

Epoch 629: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3672 - MinusLogProbMetric: 16.3672 - val_loss: 16.8920 - val_MinusLogProbMetric: 16.8920 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 630/1000
2023-09-27 00:48:41.908 
Epoch 630/1000 
	 loss: 16.3684, MinusLogProbMetric: 16.3684, val_loss: 16.9391, val_MinusLogProbMetric: 16.9391

Epoch 630: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3684 - MinusLogProbMetric: 16.3684 - val_loss: 16.9391 - val_MinusLogProbMetric: 16.9391 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 631/1000
2023-09-27 00:49:45.948 
Epoch 631/1000 
	 loss: 16.3721, MinusLogProbMetric: 16.3721, val_loss: 16.8900, val_MinusLogProbMetric: 16.8900

Epoch 631: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3721 - MinusLogProbMetric: 16.3721 - val_loss: 16.8900 - val_MinusLogProbMetric: 16.8900 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 632/1000
2023-09-27 00:50:49.860 
Epoch 632/1000 
	 loss: 16.3654, MinusLogProbMetric: 16.3654, val_loss: 16.8885, val_MinusLogProbMetric: 16.8885

Epoch 632: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3654 - MinusLogProbMetric: 16.3654 - val_loss: 16.8885 - val_MinusLogProbMetric: 16.8885 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 633/1000
2023-09-27 00:51:53.822 
Epoch 633/1000 
	 loss: 16.3694, MinusLogProbMetric: 16.3694, val_loss: 16.9077, val_MinusLogProbMetric: 16.9077

Epoch 633: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3694 - MinusLogProbMetric: 16.3694 - val_loss: 16.9077 - val_MinusLogProbMetric: 16.9077 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 634/1000
2023-09-27 00:52:57.906 
Epoch 634/1000 
	 loss: 16.3644, MinusLogProbMetric: 16.3644, val_loss: 16.8906, val_MinusLogProbMetric: 16.8906

Epoch 634: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3644 - MinusLogProbMetric: 16.3644 - val_loss: 16.8906 - val_MinusLogProbMetric: 16.8906 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 635/1000
2023-09-27 00:54:02.032 
Epoch 635/1000 
	 loss: 16.3678, MinusLogProbMetric: 16.3678, val_loss: 16.9190, val_MinusLogProbMetric: 16.9190

Epoch 635: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3678 - MinusLogProbMetric: 16.3678 - val_loss: 16.9190 - val_MinusLogProbMetric: 16.9190 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 636/1000
2023-09-27 00:55:06.650 
Epoch 636/1000 
	 loss: 16.3735, MinusLogProbMetric: 16.3735, val_loss: 16.9121, val_MinusLogProbMetric: 16.9121

Epoch 636: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3735 - MinusLogProbMetric: 16.3735 - val_loss: 16.9121 - val_MinusLogProbMetric: 16.9121 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 637/1000
2023-09-27 00:56:11.809 
Epoch 637/1000 
	 loss: 16.3660, MinusLogProbMetric: 16.3660, val_loss: 16.8871, val_MinusLogProbMetric: 16.8871

Epoch 637: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3660 - MinusLogProbMetric: 16.3660 - val_loss: 16.8871 - val_MinusLogProbMetric: 16.8871 - lr: 1.0417e-05 - 65s/epoch - 332ms/step
Epoch 638/1000
2023-09-27 00:57:16.558 
Epoch 638/1000 
	 loss: 16.3706, MinusLogProbMetric: 16.3706, val_loss: 16.8977, val_MinusLogProbMetric: 16.8977

Epoch 638: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3706 - MinusLogProbMetric: 16.3706 - val_loss: 16.8977 - val_MinusLogProbMetric: 16.8977 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 639/1000
2023-09-27 00:58:21.020 
Epoch 639/1000 
	 loss: 16.3657, MinusLogProbMetric: 16.3657, val_loss: 16.8975, val_MinusLogProbMetric: 16.8975

Epoch 639: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3657 - MinusLogProbMetric: 16.3657 - val_loss: 16.8975 - val_MinusLogProbMetric: 16.8975 - lr: 1.0417e-05 - 64s/epoch - 329ms/step
Epoch 640/1000
2023-09-27 00:59:25.693 
Epoch 640/1000 
	 loss: 16.3647, MinusLogProbMetric: 16.3647, val_loss: 16.8924, val_MinusLogProbMetric: 16.8924

Epoch 640: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3647 - MinusLogProbMetric: 16.3647 - val_loss: 16.8924 - val_MinusLogProbMetric: 16.8924 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 641/1000
2023-09-27 01:00:29.731 
Epoch 641/1000 
	 loss: 16.3633, MinusLogProbMetric: 16.3633, val_loss: 16.8921, val_MinusLogProbMetric: 16.8921

Epoch 641: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3633 - MinusLogProbMetric: 16.3633 - val_loss: 16.8921 - val_MinusLogProbMetric: 16.8921 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 642/1000
2023-09-27 01:01:34.260 
Epoch 642/1000 
	 loss: 16.3649, MinusLogProbMetric: 16.3649, val_loss: 16.8934, val_MinusLogProbMetric: 16.8934

Epoch 642: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3649 - MinusLogProbMetric: 16.3649 - val_loss: 16.8934 - val_MinusLogProbMetric: 16.8934 - lr: 1.0417e-05 - 65s/epoch - 329ms/step
Epoch 643/1000
2023-09-27 01:02:39.054 
Epoch 643/1000 
	 loss: 16.3649, MinusLogProbMetric: 16.3649, val_loss: 16.8877, val_MinusLogProbMetric: 16.8877

Epoch 643: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3649 - MinusLogProbMetric: 16.3649 - val_loss: 16.8877 - val_MinusLogProbMetric: 16.8877 - lr: 1.0417e-05 - 65s/epoch - 331ms/step
Epoch 644/1000
2023-09-27 01:03:43.616 
Epoch 644/1000 
	 loss: 16.3641, MinusLogProbMetric: 16.3641, val_loss: 16.8900, val_MinusLogProbMetric: 16.8900

Epoch 644: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3641 - MinusLogProbMetric: 16.3641 - val_loss: 16.8900 - val_MinusLogProbMetric: 16.8900 - lr: 1.0417e-05 - 65s/epoch - 329ms/step
Epoch 645/1000
2023-09-27 01:04:47.852 
Epoch 645/1000 
	 loss: 16.3627, MinusLogProbMetric: 16.3627, val_loss: 16.9063, val_MinusLogProbMetric: 16.9063

Epoch 645: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3627 - MinusLogProbMetric: 16.3627 - val_loss: 16.9063 - val_MinusLogProbMetric: 16.9063 - lr: 1.0417e-05 - 64s/epoch - 328ms/step
Epoch 646/1000
2023-09-27 01:05:52.801 
Epoch 646/1000 
	 loss: 16.3694, MinusLogProbMetric: 16.3694, val_loss: 16.9010, val_MinusLogProbMetric: 16.9010

Epoch 646: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3694 - MinusLogProbMetric: 16.3694 - val_loss: 16.9010 - val_MinusLogProbMetric: 16.9010 - lr: 1.0417e-05 - 65s/epoch - 331ms/step
Epoch 647/1000
2023-09-27 01:06:56.720 
Epoch 647/1000 
	 loss: 16.3668, MinusLogProbMetric: 16.3668, val_loss: 16.8889, val_MinusLogProbMetric: 16.8889

Epoch 647: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3668 - MinusLogProbMetric: 16.3668 - val_loss: 16.8889 - val_MinusLogProbMetric: 16.8889 - lr: 1.0417e-05 - 64s/epoch - 326ms/step
Epoch 648/1000
2023-09-27 01:08:01.185 
Epoch 648/1000 
	 loss: 16.3624, MinusLogProbMetric: 16.3624, val_loss: 16.8934, val_MinusLogProbMetric: 16.8934

Epoch 648: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3624 - MinusLogProbMetric: 16.3624 - val_loss: 16.8934 - val_MinusLogProbMetric: 16.8934 - lr: 1.0417e-05 - 64s/epoch - 329ms/step
Epoch 649/1000
2023-09-27 01:09:05.893 
Epoch 649/1000 
	 loss: 16.3685, MinusLogProbMetric: 16.3685, val_loss: 16.8893, val_MinusLogProbMetric: 16.8893

Epoch 649: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3685 - MinusLogProbMetric: 16.3685 - val_loss: 16.8893 - val_MinusLogProbMetric: 16.8893 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 650/1000
2023-09-27 01:10:10.517 
Epoch 650/1000 
	 loss: 16.3651, MinusLogProbMetric: 16.3651, val_loss: 16.8938, val_MinusLogProbMetric: 16.8938

Epoch 650: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3651 - MinusLogProbMetric: 16.3651 - val_loss: 16.8938 - val_MinusLogProbMetric: 16.8938 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 651/1000
2023-09-27 01:11:15.085 
Epoch 651/1000 
	 loss: 16.3677, MinusLogProbMetric: 16.3677, val_loss: 17.0021, val_MinusLogProbMetric: 17.0021

Epoch 651: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3677 - MinusLogProbMetric: 16.3677 - val_loss: 17.0021 - val_MinusLogProbMetric: 17.0021 - lr: 1.0417e-05 - 65s/epoch - 329ms/step
Epoch 652/1000
2023-09-27 01:12:19.322 
Epoch 652/1000 
	 loss: 16.3692, MinusLogProbMetric: 16.3692, val_loss: 16.8938, val_MinusLogProbMetric: 16.8938

Epoch 652: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3692 - MinusLogProbMetric: 16.3692 - val_loss: 16.8938 - val_MinusLogProbMetric: 16.8938 - lr: 1.0417e-05 - 64s/epoch - 328ms/step
Epoch 653/1000
2023-09-27 01:13:23.901 
Epoch 653/1000 
	 loss: 16.3643, MinusLogProbMetric: 16.3643, val_loss: 16.8899, val_MinusLogProbMetric: 16.8899

Epoch 653: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3643 - MinusLogProbMetric: 16.3643 - val_loss: 16.8899 - val_MinusLogProbMetric: 16.8899 - lr: 1.0417e-05 - 65s/epoch - 329ms/step
Epoch 654/1000
2023-09-27 01:14:28.667 
Epoch 654/1000 
	 loss: 16.3645, MinusLogProbMetric: 16.3645, val_loss: 16.8876, val_MinusLogProbMetric: 16.8876

Epoch 654: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3645 - MinusLogProbMetric: 16.3645 - val_loss: 16.8876 - val_MinusLogProbMetric: 16.8876 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 655/1000
2023-09-27 01:15:33.061 
Epoch 655/1000 
	 loss: 16.3642, MinusLogProbMetric: 16.3642, val_loss: 16.8977, val_MinusLogProbMetric: 16.8977

Epoch 655: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3642 - MinusLogProbMetric: 16.3642 - val_loss: 16.8977 - val_MinusLogProbMetric: 16.8977 - lr: 1.0417e-05 - 64s/epoch - 329ms/step
Epoch 656/1000
2023-09-27 01:16:37.546 
Epoch 656/1000 
	 loss: 16.3694, MinusLogProbMetric: 16.3694, val_loss: 16.9044, val_MinusLogProbMetric: 16.9044

Epoch 656: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3694 - MinusLogProbMetric: 16.3694 - val_loss: 16.9044 - val_MinusLogProbMetric: 16.9044 - lr: 1.0417e-05 - 64s/epoch - 329ms/step
Epoch 657/1000
2023-09-27 01:17:42.081 
Epoch 657/1000 
	 loss: 16.3627, MinusLogProbMetric: 16.3627, val_loss: 16.8973, val_MinusLogProbMetric: 16.8973

Epoch 657: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3627 - MinusLogProbMetric: 16.3627 - val_loss: 16.8973 - val_MinusLogProbMetric: 16.8973 - lr: 1.0417e-05 - 65s/epoch - 329ms/step
Epoch 658/1000
2023-09-27 01:18:47.095 
Epoch 658/1000 
	 loss: 16.3667, MinusLogProbMetric: 16.3667, val_loss: 16.8986, val_MinusLogProbMetric: 16.8986

Epoch 658: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3667 - MinusLogProbMetric: 16.3667 - val_loss: 16.8986 - val_MinusLogProbMetric: 16.8986 - lr: 1.0417e-05 - 65s/epoch - 332ms/step
Epoch 659/1000
2023-09-27 01:19:51.701 
Epoch 659/1000 
	 loss: 16.3621, MinusLogProbMetric: 16.3621, val_loss: 16.8915, val_MinusLogProbMetric: 16.8915

Epoch 659: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3621 - MinusLogProbMetric: 16.3621 - val_loss: 16.8915 - val_MinusLogProbMetric: 16.8915 - lr: 1.0417e-05 - 65s/epoch - 330ms/step
Epoch 660/1000
2023-09-27 01:20:55.769 
Epoch 660/1000 
	 loss: 16.3627, MinusLogProbMetric: 16.3627, val_loss: 16.8936, val_MinusLogProbMetric: 16.8936

Epoch 660: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3627 - MinusLogProbMetric: 16.3627 - val_loss: 16.8936 - val_MinusLogProbMetric: 16.8936 - lr: 1.0417e-05 - 64s/epoch - 327ms/step
Epoch 661/1000
2023-09-27 01:22:00.739 
Epoch 661/1000 
	 loss: 16.3631, MinusLogProbMetric: 16.3631, val_loss: 16.9123, val_MinusLogProbMetric: 16.9123

Epoch 661: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3631 - MinusLogProbMetric: 16.3631 - val_loss: 16.9123 - val_MinusLogProbMetric: 16.9123 - lr: 1.0417e-05 - 65s/epoch - 331ms/step
Epoch 662/1000
2023-09-27 01:23:05.686 
Epoch 662/1000 
	 loss: 16.3709, MinusLogProbMetric: 16.3709, val_loss: 16.8911, val_MinusLogProbMetric: 16.8911

Epoch 662: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3709 - MinusLogProbMetric: 16.3709 - val_loss: 16.8911 - val_MinusLogProbMetric: 16.8911 - lr: 1.0417e-05 - 65s/epoch - 331ms/step
Epoch 663/1000
2023-09-27 01:24:10.269 
Epoch 663/1000 
	 loss: 16.3639, MinusLogProbMetric: 16.3639, val_loss: 16.9454, val_MinusLogProbMetric: 16.9454

Epoch 663: val_loss did not improve from 16.88609
196/196 - 65s - loss: 16.3639 - MinusLogProbMetric: 16.3639 - val_loss: 16.9454 - val_MinusLogProbMetric: 16.9454 - lr: 1.0417e-05 - 65s/epoch - 329ms/step
Epoch 664/1000
2023-09-27 01:25:14.604 
Epoch 664/1000 
	 loss: 16.3686, MinusLogProbMetric: 16.3686, val_loss: 16.8979, val_MinusLogProbMetric: 16.8979

Epoch 664: val_loss did not improve from 16.88609
196/196 - 64s - loss: 16.3686 - MinusLogProbMetric: 16.3686 - val_loss: 16.8979 - val_MinusLogProbMetric: 16.8979 - lr: 1.0417e-05 - 64s/epoch - 328ms/step
Epoch 665/1000
2023-09-27 01:26:18.979 
Epoch 665/1000 
	 loss: 16.3548, MinusLogProbMetric: 16.3548, val_loss: 16.8856, val_MinusLogProbMetric: 16.8856

Epoch 665: val_loss improved from 16.88609 to 16.88561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.3548 - MinusLogProbMetric: 16.3548 - val_loss: 16.8856 - val_MinusLogProbMetric: 16.8856 - lr: 5.2083e-06 - 65s/epoch - 334ms/step
Epoch 666/1000
2023-09-27 01:27:24.587 
Epoch 666/1000 
	 loss: 16.3558, MinusLogProbMetric: 16.3558, val_loss: 16.8883, val_MinusLogProbMetric: 16.8883

Epoch 666: val_loss did not improve from 16.88561
196/196 - 65s - loss: 16.3558 - MinusLogProbMetric: 16.3558 - val_loss: 16.8883 - val_MinusLogProbMetric: 16.8883 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 667/1000
2023-09-27 01:28:29.220 
Epoch 667/1000 
	 loss: 16.3547, MinusLogProbMetric: 16.3547, val_loss: 16.8838, val_MinusLogProbMetric: 16.8838

Epoch 667: val_loss improved from 16.88561 to 16.88380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3547 - MinusLogProbMetric: 16.3547 - val_loss: 16.8838 - val_MinusLogProbMetric: 16.8838 - lr: 5.2083e-06 - 66s/epoch - 336ms/step
Epoch 668/1000
2023-09-27 01:29:34.557 
Epoch 668/1000 
	 loss: 16.3545, MinusLogProbMetric: 16.3545, val_loss: 16.8856, val_MinusLogProbMetric: 16.8856

Epoch 668: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3545 - MinusLogProbMetric: 16.3545 - val_loss: 16.8856 - val_MinusLogProbMetric: 16.8856 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 669/1000
2023-09-27 01:30:39.207 
Epoch 669/1000 
	 loss: 16.3540, MinusLogProbMetric: 16.3540, val_loss: 16.8871, val_MinusLogProbMetric: 16.8871

Epoch 669: val_loss did not improve from 16.88380
196/196 - 65s - loss: 16.3540 - MinusLogProbMetric: 16.3540 - val_loss: 16.8871 - val_MinusLogProbMetric: 16.8871 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 670/1000
2023-09-27 01:31:43.453 
Epoch 670/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8841, val_MinusLogProbMetric: 16.8841

Epoch 670: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8841 - val_MinusLogProbMetric: 16.8841 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 671/1000
2023-09-27 01:32:47.998 
Epoch 671/1000 
	 loss: 16.3544, MinusLogProbMetric: 16.3544, val_loss: 16.8865, val_MinusLogProbMetric: 16.8865

Epoch 671: val_loss did not improve from 16.88380
196/196 - 65s - loss: 16.3544 - MinusLogProbMetric: 16.3544 - val_loss: 16.8865 - val_MinusLogProbMetric: 16.8865 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 672/1000
2023-09-27 01:33:52.534 
Epoch 672/1000 
	 loss: 16.3549, MinusLogProbMetric: 16.3549, val_loss: 16.9000, val_MinusLogProbMetric: 16.9000

Epoch 672: val_loss did not improve from 16.88380
196/196 - 65s - loss: 16.3549 - MinusLogProbMetric: 16.3549 - val_loss: 16.9000 - val_MinusLogProbMetric: 16.9000 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 673/1000
2023-09-27 01:34:56.933 
Epoch 673/1000 
	 loss: 16.3553, MinusLogProbMetric: 16.3553, val_loss: 16.8839, val_MinusLogProbMetric: 16.8839

Epoch 673: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3553 - MinusLogProbMetric: 16.3553 - val_loss: 16.8839 - val_MinusLogProbMetric: 16.8839 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 674/1000
2023-09-27 01:36:01.372 
Epoch 674/1000 
	 loss: 16.3545, MinusLogProbMetric: 16.3545, val_loss: 16.8845, val_MinusLogProbMetric: 16.8845

Epoch 674: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3545 - MinusLogProbMetric: 16.3545 - val_loss: 16.8845 - val_MinusLogProbMetric: 16.8845 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 675/1000
2023-09-27 01:37:06.151 
Epoch 675/1000 
	 loss: 16.3545, MinusLogProbMetric: 16.3545, val_loss: 16.8883, val_MinusLogProbMetric: 16.8883

Epoch 675: val_loss did not improve from 16.88380
196/196 - 65s - loss: 16.3545 - MinusLogProbMetric: 16.3545 - val_loss: 16.8883 - val_MinusLogProbMetric: 16.8883 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 676/1000
2023-09-27 01:38:10.699 
Epoch 676/1000 
	 loss: 16.3548, MinusLogProbMetric: 16.3548, val_loss: 16.8853, val_MinusLogProbMetric: 16.8853

Epoch 676: val_loss did not improve from 16.88380
196/196 - 65s - loss: 16.3548 - MinusLogProbMetric: 16.3548 - val_loss: 16.8853 - val_MinusLogProbMetric: 16.8853 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 677/1000
2023-09-27 01:39:15.089 
Epoch 677/1000 
	 loss: 16.3539, MinusLogProbMetric: 16.3539, val_loss: 16.8872, val_MinusLogProbMetric: 16.8872

Epoch 677: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3539 - MinusLogProbMetric: 16.3539 - val_loss: 16.8872 - val_MinusLogProbMetric: 16.8872 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 678/1000
2023-09-27 01:40:19.702 
Epoch 678/1000 
	 loss: 16.3542, MinusLogProbMetric: 16.3542, val_loss: 16.8866, val_MinusLogProbMetric: 16.8866

Epoch 678: val_loss did not improve from 16.88380
196/196 - 65s - loss: 16.3542 - MinusLogProbMetric: 16.3542 - val_loss: 16.8866 - val_MinusLogProbMetric: 16.8866 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 679/1000
2023-09-27 01:41:24.097 
Epoch 679/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8878, val_MinusLogProbMetric: 16.8878

Epoch 679: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8878 - val_MinusLogProbMetric: 16.8878 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 680/1000
2023-09-27 01:42:28.490 
Epoch 680/1000 
	 loss: 16.3550, MinusLogProbMetric: 16.3550, val_loss: 16.8865, val_MinusLogProbMetric: 16.8865

Epoch 680: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3550 - MinusLogProbMetric: 16.3550 - val_loss: 16.8865 - val_MinusLogProbMetric: 16.8865 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 681/1000
2023-09-27 01:43:32.636 
Epoch 681/1000 
	 loss: 16.3538, MinusLogProbMetric: 16.3538, val_loss: 16.8885, val_MinusLogProbMetric: 16.8885

Epoch 681: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3538 - MinusLogProbMetric: 16.3538 - val_loss: 16.8885 - val_MinusLogProbMetric: 16.8885 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 682/1000
2023-09-27 01:44:37.271 
Epoch 682/1000 
	 loss: 16.3536, MinusLogProbMetric: 16.3536, val_loss: 16.8869, val_MinusLogProbMetric: 16.8869

Epoch 682: val_loss did not improve from 16.88380
196/196 - 65s - loss: 16.3536 - MinusLogProbMetric: 16.3536 - val_loss: 16.8869 - val_MinusLogProbMetric: 16.8869 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 683/1000
2023-09-27 01:45:41.639 
Epoch 683/1000 
	 loss: 16.3542, MinusLogProbMetric: 16.3542, val_loss: 16.8864, val_MinusLogProbMetric: 16.8864

Epoch 683: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3542 - MinusLogProbMetric: 16.3542 - val_loss: 16.8864 - val_MinusLogProbMetric: 16.8864 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 684/1000
2023-09-27 01:46:46.066 
Epoch 684/1000 
	 loss: 16.3536, MinusLogProbMetric: 16.3536, val_loss: 16.9202, val_MinusLogProbMetric: 16.9202

Epoch 684: val_loss did not improve from 16.88380
196/196 - 64s - loss: 16.3536 - MinusLogProbMetric: 16.3536 - val_loss: 16.9202 - val_MinusLogProbMetric: 16.9202 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 685/1000
2023-09-27 01:47:50.875 
Epoch 685/1000 
	 loss: 16.3554, MinusLogProbMetric: 16.3554, val_loss: 16.8834, val_MinusLogProbMetric: 16.8834

Epoch 685: val_loss improved from 16.88380 to 16.88338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3554 - MinusLogProbMetric: 16.3554 - val_loss: 16.8834 - val_MinusLogProbMetric: 16.8834 - lr: 5.2083e-06 - 66s/epoch - 337ms/step
Epoch 686/1000
2023-09-27 01:48:56.377 
Epoch 686/1000 
	 loss: 16.3526, MinusLogProbMetric: 16.3526, val_loss: 16.8890, val_MinusLogProbMetric: 16.8890

Epoch 686: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3526 - MinusLogProbMetric: 16.3526 - val_loss: 16.8890 - val_MinusLogProbMetric: 16.8890 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 687/1000
2023-09-27 01:50:00.674 
Epoch 687/1000 
	 loss: 16.3568, MinusLogProbMetric: 16.3568, val_loss: 16.8944, val_MinusLogProbMetric: 16.8944

Epoch 687: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3568 - MinusLogProbMetric: 16.3568 - val_loss: 16.8944 - val_MinusLogProbMetric: 16.8944 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 688/1000
2023-09-27 01:51:04.782 
Epoch 688/1000 
	 loss: 16.3546, MinusLogProbMetric: 16.3546, val_loss: 16.8871, val_MinusLogProbMetric: 16.8871

Epoch 688: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3546 - MinusLogProbMetric: 16.3546 - val_loss: 16.8871 - val_MinusLogProbMetric: 16.8871 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 689/1000
2023-09-27 01:52:08.876 
Epoch 689/1000 
	 loss: 16.3531, MinusLogProbMetric: 16.3531, val_loss: 16.8838, val_MinusLogProbMetric: 16.8838

Epoch 689: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3531 - MinusLogProbMetric: 16.3531 - val_loss: 16.8838 - val_MinusLogProbMetric: 16.8838 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 690/1000
2023-09-27 01:53:13.137 
Epoch 690/1000 
	 loss: 16.3549, MinusLogProbMetric: 16.3549, val_loss: 16.8935, val_MinusLogProbMetric: 16.8935

Epoch 690: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3549 - MinusLogProbMetric: 16.3549 - val_loss: 16.8935 - val_MinusLogProbMetric: 16.8935 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 691/1000
2023-09-27 01:54:17.404 
Epoch 691/1000 
	 loss: 16.3556, MinusLogProbMetric: 16.3556, val_loss: 16.8880, val_MinusLogProbMetric: 16.8880

Epoch 691: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3556 - MinusLogProbMetric: 16.3556 - val_loss: 16.8880 - val_MinusLogProbMetric: 16.8880 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 692/1000
2023-09-27 01:55:21.708 
Epoch 692/1000 
	 loss: 16.3542, MinusLogProbMetric: 16.3542, val_loss: 16.8890, val_MinusLogProbMetric: 16.8890

Epoch 692: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3542 - MinusLogProbMetric: 16.3542 - val_loss: 16.8890 - val_MinusLogProbMetric: 16.8890 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 693/1000
2023-09-27 01:56:25.675 
Epoch 693/1000 
	 loss: 16.3528, MinusLogProbMetric: 16.3528, val_loss: 16.8854, val_MinusLogProbMetric: 16.8854

Epoch 693: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3528 - MinusLogProbMetric: 16.3528 - val_loss: 16.8854 - val_MinusLogProbMetric: 16.8854 - lr: 5.2083e-06 - 64s/epoch - 326ms/step
Epoch 694/1000
2023-09-27 01:57:30.268 
Epoch 694/1000 
	 loss: 16.3521, MinusLogProbMetric: 16.3521, val_loss: 16.8880, val_MinusLogProbMetric: 16.8880

Epoch 694: val_loss did not improve from 16.88338
196/196 - 65s - loss: 16.3521 - MinusLogProbMetric: 16.3521 - val_loss: 16.8880 - val_MinusLogProbMetric: 16.8880 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 695/1000
2023-09-27 01:58:34.990 
Epoch 695/1000 
	 loss: 16.3533, MinusLogProbMetric: 16.3533, val_loss: 16.8855, val_MinusLogProbMetric: 16.8855

Epoch 695: val_loss did not improve from 16.88338
196/196 - 65s - loss: 16.3533 - MinusLogProbMetric: 16.3533 - val_loss: 16.8855 - val_MinusLogProbMetric: 16.8855 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 696/1000
2023-09-27 01:59:39.159 
Epoch 696/1000 
	 loss: 16.3538, MinusLogProbMetric: 16.3538, val_loss: 16.8958, val_MinusLogProbMetric: 16.8958

Epoch 696: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3538 - MinusLogProbMetric: 16.3538 - val_loss: 16.8958 - val_MinusLogProbMetric: 16.8958 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 697/1000
2023-09-27 02:00:43.858 
Epoch 697/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8884, val_MinusLogProbMetric: 16.8884

Epoch 697: val_loss did not improve from 16.88338
196/196 - 65s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8884 - val_MinusLogProbMetric: 16.8884 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 698/1000
2023-09-27 02:01:48.053 
Epoch 698/1000 
	 loss: 16.3528, MinusLogProbMetric: 16.3528, val_loss: 16.8854, val_MinusLogProbMetric: 16.8854

Epoch 698: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3528 - MinusLogProbMetric: 16.3528 - val_loss: 16.8854 - val_MinusLogProbMetric: 16.8854 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 699/1000
2023-09-27 02:02:52.470 
Epoch 699/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8869, val_MinusLogProbMetric: 16.8869

Epoch 699: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8869 - val_MinusLogProbMetric: 16.8869 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 700/1000
2023-09-27 02:03:57.159 
Epoch 700/1000 
	 loss: 16.3540, MinusLogProbMetric: 16.3540, val_loss: 16.8967, val_MinusLogProbMetric: 16.8967

Epoch 700: val_loss did not improve from 16.88338
196/196 - 65s - loss: 16.3540 - MinusLogProbMetric: 16.3540 - val_loss: 16.8967 - val_MinusLogProbMetric: 16.8967 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 701/1000
2023-09-27 02:05:01.702 
Epoch 701/1000 
	 loss: 16.3526, MinusLogProbMetric: 16.3526, val_loss: 16.8853, val_MinusLogProbMetric: 16.8853

Epoch 701: val_loss did not improve from 16.88338
196/196 - 65s - loss: 16.3526 - MinusLogProbMetric: 16.3526 - val_loss: 16.8853 - val_MinusLogProbMetric: 16.8853 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 702/1000
2023-09-27 02:06:06.466 
Epoch 702/1000 
	 loss: 16.3547, MinusLogProbMetric: 16.3547, val_loss: 16.9131, val_MinusLogProbMetric: 16.9131

Epoch 702: val_loss did not improve from 16.88338
196/196 - 65s - loss: 16.3547 - MinusLogProbMetric: 16.3547 - val_loss: 16.9131 - val_MinusLogProbMetric: 16.9131 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 703/1000
2023-09-27 02:07:10.522 
Epoch 703/1000 
	 loss: 16.3560, MinusLogProbMetric: 16.3560, val_loss: 16.8877, val_MinusLogProbMetric: 16.8877

Epoch 703: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3560 - MinusLogProbMetric: 16.3560 - val_loss: 16.8877 - val_MinusLogProbMetric: 16.8877 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 704/1000
2023-09-27 02:08:15.129 
Epoch 704/1000 
	 loss: 16.3554, MinusLogProbMetric: 16.3554, val_loss: 16.8856, val_MinusLogProbMetric: 16.8856

Epoch 704: val_loss did not improve from 16.88338
196/196 - 65s - loss: 16.3554 - MinusLogProbMetric: 16.3554 - val_loss: 16.8856 - val_MinusLogProbMetric: 16.8856 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 705/1000
2023-09-27 02:09:19.531 
Epoch 705/1000 
	 loss: 16.3533, MinusLogProbMetric: 16.3533, val_loss: 16.8913, val_MinusLogProbMetric: 16.8913

Epoch 705: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3533 - MinusLogProbMetric: 16.3533 - val_loss: 16.8913 - val_MinusLogProbMetric: 16.8913 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 706/1000
2023-09-27 02:10:23.908 
Epoch 706/1000 
	 loss: 16.3534, MinusLogProbMetric: 16.3534, val_loss: 16.8847, val_MinusLogProbMetric: 16.8847

Epoch 706: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3534 - MinusLogProbMetric: 16.3534 - val_loss: 16.8847 - val_MinusLogProbMetric: 16.8847 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 707/1000
2023-09-27 02:11:28.098 
Epoch 707/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 16.8836, val_MinusLogProbMetric: 16.8836

Epoch 707: val_loss did not improve from 16.88338
196/196 - 64s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 16.8836 - val_MinusLogProbMetric: 16.8836 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 708/1000
2023-09-27 02:12:32.132 
Epoch 708/1000 
	 loss: 16.3539, MinusLogProbMetric: 16.3539, val_loss: 16.8833, val_MinusLogProbMetric: 16.8833

Epoch 708: val_loss improved from 16.88338 to 16.88327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 65s - loss: 16.3539 - MinusLogProbMetric: 16.3539 - val_loss: 16.8833 - val_MinusLogProbMetric: 16.8833 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 709/1000
2023-09-27 02:13:37.494 
Epoch 709/1000 
	 loss: 16.3549, MinusLogProbMetric: 16.3549, val_loss: 16.8881, val_MinusLogProbMetric: 16.8881

Epoch 709: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3549 - MinusLogProbMetric: 16.3549 - val_loss: 16.8881 - val_MinusLogProbMetric: 16.8881 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 710/1000
2023-09-27 02:14:42.231 
Epoch 710/1000 
	 loss: 16.3540, MinusLogProbMetric: 16.3540, val_loss: 16.8864, val_MinusLogProbMetric: 16.8864

Epoch 710: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3540 - MinusLogProbMetric: 16.3540 - val_loss: 16.8864 - val_MinusLogProbMetric: 16.8864 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 711/1000
2023-09-27 02:15:47.024 
Epoch 711/1000 
	 loss: 16.3567, MinusLogProbMetric: 16.3567, val_loss: 16.8847, val_MinusLogProbMetric: 16.8847

Epoch 711: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3567 - MinusLogProbMetric: 16.3567 - val_loss: 16.8847 - val_MinusLogProbMetric: 16.8847 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 712/1000
2023-09-27 02:16:51.635 
Epoch 712/1000 
	 loss: 16.3543, MinusLogProbMetric: 16.3543, val_loss: 16.8924, val_MinusLogProbMetric: 16.8924

Epoch 712: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3543 - MinusLogProbMetric: 16.3543 - val_loss: 16.8924 - val_MinusLogProbMetric: 16.8924 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 713/1000
2023-09-27 02:17:56.171 
Epoch 713/1000 
	 loss: 16.3542, MinusLogProbMetric: 16.3542, val_loss: 16.8991, val_MinusLogProbMetric: 16.8991

Epoch 713: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3542 - MinusLogProbMetric: 16.3542 - val_loss: 16.8991 - val_MinusLogProbMetric: 16.8991 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 714/1000
2023-09-27 02:19:00.881 
Epoch 714/1000 
	 loss: 16.3542, MinusLogProbMetric: 16.3542, val_loss: 16.8944, val_MinusLogProbMetric: 16.8944

Epoch 714: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3542 - MinusLogProbMetric: 16.3542 - val_loss: 16.8944 - val_MinusLogProbMetric: 16.8944 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 715/1000
2023-09-27 02:20:05.363 
Epoch 715/1000 
	 loss: 16.3538, MinusLogProbMetric: 16.3538, val_loss: 16.8852, val_MinusLogProbMetric: 16.8852

Epoch 715: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3538 - MinusLogProbMetric: 16.3538 - val_loss: 16.8852 - val_MinusLogProbMetric: 16.8852 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 716/1000
2023-09-27 02:21:09.498 
Epoch 716/1000 
	 loss: 16.3534, MinusLogProbMetric: 16.3534, val_loss: 16.9027, val_MinusLogProbMetric: 16.9027

Epoch 716: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3534 - MinusLogProbMetric: 16.3534 - val_loss: 16.9027 - val_MinusLogProbMetric: 16.9027 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 717/1000
2023-09-27 02:22:14.119 
Epoch 717/1000 
	 loss: 16.3539, MinusLogProbMetric: 16.3539, val_loss: 16.8850, val_MinusLogProbMetric: 16.8850

Epoch 717: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3539 - MinusLogProbMetric: 16.3539 - val_loss: 16.8850 - val_MinusLogProbMetric: 16.8850 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 718/1000
2023-09-27 02:23:18.424 
Epoch 718/1000 
	 loss: 16.3545, MinusLogProbMetric: 16.3545, val_loss: 16.8886, val_MinusLogProbMetric: 16.8886

Epoch 718: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3545 - MinusLogProbMetric: 16.3545 - val_loss: 16.8886 - val_MinusLogProbMetric: 16.8886 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 719/1000
2023-09-27 02:24:22.907 
Epoch 719/1000 
	 loss: 16.3532, MinusLogProbMetric: 16.3532, val_loss: 16.8890, val_MinusLogProbMetric: 16.8890

Epoch 719: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3532 - MinusLogProbMetric: 16.3532 - val_loss: 16.8890 - val_MinusLogProbMetric: 16.8890 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 720/1000
2023-09-27 02:25:27.746 
Epoch 720/1000 
	 loss: 16.3540, MinusLogProbMetric: 16.3540, val_loss: 16.8841, val_MinusLogProbMetric: 16.8841

Epoch 720: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3540 - MinusLogProbMetric: 16.3540 - val_loss: 16.8841 - val_MinusLogProbMetric: 16.8841 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 721/1000
2023-09-27 02:26:32.233 
Epoch 721/1000 
	 loss: 16.3523, MinusLogProbMetric: 16.3523, val_loss: 16.8912, val_MinusLogProbMetric: 16.8912

Epoch 721: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3523 - MinusLogProbMetric: 16.3523 - val_loss: 16.8912 - val_MinusLogProbMetric: 16.8912 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 722/1000
2023-09-27 02:27:36.614 
Epoch 722/1000 
	 loss: 16.3534, MinusLogProbMetric: 16.3534, val_loss: 16.8980, val_MinusLogProbMetric: 16.8980

Epoch 722: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3534 - MinusLogProbMetric: 16.3534 - val_loss: 16.8980 - val_MinusLogProbMetric: 16.8980 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 723/1000
2023-09-27 02:28:41.135 
Epoch 723/1000 
	 loss: 16.3534, MinusLogProbMetric: 16.3534, val_loss: 16.8853, val_MinusLogProbMetric: 16.8853

Epoch 723: val_loss did not improve from 16.88327
196/196 - 65s - loss: 16.3534 - MinusLogProbMetric: 16.3534 - val_loss: 16.8853 - val_MinusLogProbMetric: 16.8853 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 724/1000
2023-09-27 02:29:45.433 
Epoch 724/1000 
	 loss: 16.3557, MinusLogProbMetric: 16.3557, val_loss: 16.8988, val_MinusLogProbMetric: 16.8988

Epoch 724: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3557 - MinusLogProbMetric: 16.3557 - val_loss: 16.8988 - val_MinusLogProbMetric: 16.8988 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 725/1000
2023-09-27 02:30:49.533 
Epoch 725/1000 
	 loss: 16.3534, MinusLogProbMetric: 16.3534, val_loss: 16.8928, val_MinusLogProbMetric: 16.8928

Epoch 725: val_loss did not improve from 16.88327
196/196 - 64s - loss: 16.3534 - MinusLogProbMetric: 16.3534 - val_loss: 16.8928 - val_MinusLogProbMetric: 16.8928 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 726/1000
2023-09-27 02:31:48.173 
Epoch 726/1000 
	 loss: 16.3569, MinusLogProbMetric: 16.3569, val_loss: 16.8907, val_MinusLogProbMetric: 16.8907

Epoch 726: val_loss did not improve from 16.88327
196/196 - 59s - loss: 16.3569 - MinusLogProbMetric: 16.3569 - val_loss: 16.8907 - val_MinusLogProbMetric: 16.8907 - lr: 5.2083e-06 - 59s/epoch - 299ms/step
Epoch 727/1000
2023-09-27 02:32:42.556 
Epoch 727/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8827, val_MinusLogProbMetric: 16.8827

Epoch 727: val_loss improved from 16.88327 to 16.88266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 55s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8827 - val_MinusLogProbMetric: 16.8827 - lr: 5.2083e-06 - 55s/epoch - 282ms/step
Epoch 728/1000
2023-09-27 02:33:46.737 
Epoch 728/1000 
	 loss: 16.3539, MinusLogProbMetric: 16.3539, val_loss: 16.8867, val_MinusLogProbMetric: 16.8867

Epoch 728: val_loss did not improve from 16.88266
196/196 - 63s - loss: 16.3539 - MinusLogProbMetric: 16.3539 - val_loss: 16.8867 - val_MinusLogProbMetric: 16.8867 - lr: 5.2083e-06 - 63s/epoch - 322ms/step
Epoch 729/1000
2023-09-27 02:34:52.203 
Epoch 729/1000 
	 loss: 16.3539, MinusLogProbMetric: 16.3539, val_loss: 16.8894, val_MinusLogProbMetric: 16.8894

Epoch 729: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3539 - MinusLogProbMetric: 16.3539 - val_loss: 16.8894 - val_MinusLogProbMetric: 16.8894 - lr: 5.2083e-06 - 65s/epoch - 334ms/step
Epoch 730/1000
2023-09-27 02:35:57.300 
Epoch 730/1000 
	 loss: 16.3534, MinusLogProbMetric: 16.3534, val_loss: 16.8852, val_MinusLogProbMetric: 16.8852

Epoch 730: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3534 - MinusLogProbMetric: 16.3534 - val_loss: 16.8852 - val_MinusLogProbMetric: 16.8852 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 731/1000
2023-09-27 02:37:01.558 
Epoch 731/1000 
	 loss: 16.3523, MinusLogProbMetric: 16.3523, val_loss: 16.8831, val_MinusLogProbMetric: 16.8831

Epoch 731: val_loss did not improve from 16.88266
196/196 - 64s - loss: 16.3523 - MinusLogProbMetric: 16.3523 - val_loss: 16.8831 - val_MinusLogProbMetric: 16.8831 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 732/1000
2023-09-27 02:38:06.865 
Epoch 732/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 16.8962, val_MinusLogProbMetric: 16.8962

Epoch 732: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 16.8962 - val_MinusLogProbMetric: 16.8962 - lr: 5.2083e-06 - 65s/epoch - 333ms/step
Epoch 733/1000
2023-09-27 02:39:11.734 
Epoch 733/1000 
	 loss: 16.3532, MinusLogProbMetric: 16.3532, val_loss: 16.8878, val_MinusLogProbMetric: 16.8878

Epoch 733: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3532 - MinusLogProbMetric: 16.3532 - val_loss: 16.8878 - val_MinusLogProbMetric: 16.8878 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 734/1000
2023-09-27 02:40:16.403 
Epoch 734/1000 
	 loss: 16.3523, MinusLogProbMetric: 16.3523, val_loss: 16.8945, val_MinusLogProbMetric: 16.8945

Epoch 734: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3523 - MinusLogProbMetric: 16.3523 - val_loss: 16.8945 - val_MinusLogProbMetric: 16.8945 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 735/1000
2023-09-27 02:41:20.971 
Epoch 735/1000 
	 loss: 16.3517, MinusLogProbMetric: 16.3517, val_loss: 16.8830, val_MinusLogProbMetric: 16.8830

Epoch 735: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3517 - MinusLogProbMetric: 16.3517 - val_loss: 16.8830 - val_MinusLogProbMetric: 16.8830 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 736/1000
2023-09-27 02:42:25.937 
Epoch 736/1000 
	 loss: 16.3540, MinusLogProbMetric: 16.3540, val_loss: 16.8927, val_MinusLogProbMetric: 16.8927

Epoch 736: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3540 - MinusLogProbMetric: 16.3540 - val_loss: 16.8927 - val_MinusLogProbMetric: 16.8927 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 737/1000
2023-09-27 02:43:29.989 
Epoch 737/1000 
	 loss: 16.3517, MinusLogProbMetric: 16.3517, val_loss: 16.8903, val_MinusLogProbMetric: 16.8903

Epoch 737: val_loss did not improve from 16.88266
196/196 - 64s - loss: 16.3517 - MinusLogProbMetric: 16.3517 - val_loss: 16.8903 - val_MinusLogProbMetric: 16.8903 - lr: 5.2083e-06 - 64s/epoch - 327ms/step
Epoch 738/1000
2023-09-27 02:44:35.250 
Epoch 738/1000 
	 loss: 16.3519, MinusLogProbMetric: 16.3519, val_loss: 16.8970, val_MinusLogProbMetric: 16.8970

Epoch 738: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3519 - MinusLogProbMetric: 16.3519 - val_loss: 16.8970 - val_MinusLogProbMetric: 16.8970 - lr: 5.2083e-06 - 65s/epoch - 333ms/step
Epoch 739/1000
2023-09-27 02:45:39.991 
Epoch 739/1000 
	 loss: 16.3532, MinusLogProbMetric: 16.3532, val_loss: 16.8856, val_MinusLogProbMetric: 16.8856

Epoch 739: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3532 - MinusLogProbMetric: 16.3532 - val_loss: 16.8856 - val_MinusLogProbMetric: 16.8856 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 740/1000
2023-09-27 02:46:44.915 
Epoch 740/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 16.8982, val_MinusLogProbMetric: 16.8982

Epoch 740: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 16.8982 - val_MinusLogProbMetric: 16.8982 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 741/1000
2023-09-27 02:47:49.450 
Epoch 741/1000 
	 loss: 16.3545, MinusLogProbMetric: 16.3545, val_loss: 16.8850, val_MinusLogProbMetric: 16.8850

Epoch 741: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3545 - MinusLogProbMetric: 16.3545 - val_loss: 16.8850 - val_MinusLogProbMetric: 16.8850 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 742/1000
2023-09-27 02:48:54.146 
Epoch 742/1000 
	 loss: 16.3515, MinusLogProbMetric: 16.3515, val_loss: 16.8872, val_MinusLogProbMetric: 16.8872

Epoch 742: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3515 - MinusLogProbMetric: 16.3515 - val_loss: 16.8872 - val_MinusLogProbMetric: 16.8872 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 743/1000
2023-09-27 02:49:59.147 
Epoch 743/1000 
	 loss: 16.3523, MinusLogProbMetric: 16.3523, val_loss: 16.8832, val_MinusLogProbMetric: 16.8832

Epoch 743: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3523 - MinusLogProbMetric: 16.3523 - val_loss: 16.8832 - val_MinusLogProbMetric: 16.8832 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 744/1000
2023-09-27 02:51:03.701 
Epoch 744/1000 
	 loss: 16.3533, MinusLogProbMetric: 16.3533, val_loss: 16.8913, val_MinusLogProbMetric: 16.8913

Epoch 744: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3533 - MinusLogProbMetric: 16.3533 - val_loss: 16.8913 - val_MinusLogProbMetric: 16.8913 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 745/1000
2023-09-27 02:52:08.669 
Epoch 745/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8852, val_MinusLogProbMetric: 16.8852

Epoch 745: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8852 - val_MinusLogProbMetric: 16.8852 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 746/1000
2023-09-27 02:53:13.895 
Epoch 746/1000 
	 loss: 16.3537, MinusLogProbMetric: 16.3537, val_loss: 16.8916, val_MinusLogProbMetric: 16.8916

Epoch 746: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3537 - MinusLogProbMetric: 16.3537 - val_loss: 16.8916 - val_MinusLogProbMetric: 16.8916 - lr: 5.2083e-06 - 65s/epoch - 333ms/step
Epoch 747/1000
2023-09-27 02:54:18.511 
Epoch 747/1000 
	 loss: 16.3523, MinusLogProbMetric: 16.3523, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 747: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3523 - MinusLogProbMetric: 16.3523 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 748/1000
2023-09-27 02:55:22.919 
Epoch 748/1000 
	 loss: 16.3572, MinusLogProbMetric: 16.3572, val_loss: 16.8941, val_MinusLogProbMetric: 16.8941

Epoch 748: val_loss did not improve from 16.88266
196/196 - 64s - loss: 16.3572 - MinusLogProbMetric: 16.3572 - val_loss: 16.8941 - val_MinusLogProbMetric: 16.8941 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 749/1000
2023-09-27 02:56:27.755 
Epoch 749/1000 
	 loss: 16.3542, MinusLogProbMetric: 16.3542, val_loss: 16.9121, val_MinusLogProbMetric: 16.9121

Epoch 749: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3542 - MinusLogProbMetric: 16.3542 - val_loss: 16.9121 - val_MinusLogProbMetric: 16.9121 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 750/1000
2023-09-27 02:57:31.969 
Epoch 750/1000 
	 loss: 16.3530, MinusLogProbMetric: 16.3530, val_loss: 16.8942, val_MinusLogProbMetric: 16.8942

Epoch 750: val_loss did not improve from 16.88266
196/196 - 64s - loss: 16.3530 - MinusLogProbMetric: 16.3530 - val_loss: 16.8942 - val_MinusLogProbMetric: 16.8942 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 751/1000
2023-09-27 02:58:36.523 
Epoch 751/1000 
	 loss: 16.3552, MinusLogProbMetric: 16.3552, val_loss: 16.8852, val_MinusLogProbMetric: 16.8852

Epoch 751: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3552 - MinusLogProbMetric: 16.3552 - val_loss: 16.8852 - val_MinusLogProbMetric: 16.8852 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 752/1000
2023-09-27 02:59:41.421 
Epoch 752/1000 
	 loss: 16.3521, MinusLogProbMetric: 16.3521, val_loss: 16.8837, val_MinusLogProbMetric: 16.8837

Epoch 752: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3521 - MinusLogProbMetric: 16.3521 - val_loss: 16.8837 - val_MinusLogProbMetric: 16.8837 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 753/1000
2023-09-27 03:00:46.023 
Epoch 753/1000 
	 loss: 16.3523, MinusLogProbMetric: 16.3523, val_loss: 16.8845, val_MinusLogProbMetric: 16.8845

Epoch 753: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3523 - MinusLogProbMetric: 16.3523 - val_loss: 16.8845 - val_MinusLogProbMetric: 16.8845 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 754/1000
2023-09-27 03:01:51.524 
Epoch 754/1000 
	 loss: 16.3524, MinusLogProbMetric: 16.3524, val_loss: 16.8845, val_MinusLogProbMetric: 16.8845

Epoch 754: val_loss did not improve from 16.88266
196/196 - 66s - loss: 16.3524 - MinusLogProbMetric: 16.3524 - val_loss: 16.8845 - val_MinusLogProbMetric: 16.8845 - lr: 5.2083e-06 - 66s/epoch - 334ms/step
Epoch 755/1000
2023-09-27 03:02:56.061 
Epoch 755/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 16.9095, val_MinusLogProbMetric: 16.9095

Epoch 755: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 16.9095 - val_MinusLogProbMetric: 16.9095 - lr: 5.2083e-06 - 65s/epoch - 329ms/step
Epoch 756/1000
2023-09-27 03:04:01.055 
Epoch 756/1000 
	 loss: 16.3546, MinusLogProbMetric: 16.3546, val_loss: 16.8852, val_MinusLogProbMetric: 16.8852

Epoch 756: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3546 - MinusLogProbMetric: 16.3546 - val_loss: 16.8852 - val_MinusLogProbMetric: 16.8852 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 757/1000
2023-09-27 03:05:05.921 
Epoch 757/1000 
	 loss: 16.3526, MinusLogProbMetric: 16.3526, val_loss: 16.8840, val_MinusLogProbMetric: 16.8840

Epoch 757: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3526 - MinusLogProbMetric: 16.3526 - val_loss: 16.8840 - val_MinusLogProbMetric: 16.8840 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 758/1000
2023-09-27 03:06:10.812 
Epoch 758/1000 
	 loss: 16.3522, MinusLogProbMetric: 16.3522, val_loss: 16.8884, val_MinusLogProbMetric: 16.8884

Epoch 758: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3522 - MinusLogProbMetric: 16.3522 - val_loss: 16.8884 - val_MinusLogProbMetric: 16.8884 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 759/1000
2023-09-27 03:07:15.813 
Epoch 759/1000 
	 loss: 16.3510, MinusLogProbMetric: 16.3510, val_loss: 16.8856, val_MinusLogProbMetric: 16.8856

Epoch 759: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3510 - MinusLogProbMetric: 16.3510 - val_loss: 16.8856 - val_MinusLogProbMetric: 16.8856 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 760/1000
2023-09-27 03:08:20.694 
Epoch 760/1000 
	 loss: 16.3522, MinusLogProbMetric: 16.3522, val_loss: 16.8989, val_MinusLogProbMetric: 16.8989

Epoch 760: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3522 - MinusLogProbMetric: 16.3522 - val_loss: 16.8989 - val_MinusLogProbMetric: 16.8989 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 761/1000
2023-09-27 03:09:25.469 
Epoch 761/1000 
	 loss: 16.3532, MinusLogProbMetric: 16.3532, val_loss: 16.8930, val_MinusLogProbMetric: 16.8930

Epoch 761: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3532 - MinusLogProbMetric: 16.3532 - val_loss: 16.8930 - val_MinusLogProbMetric: 16.8930 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 762/1000
2023-09-27 03:10:30.284 
Epoch 762/1000 
	 loss: 16.3527, MinusLogProbMetric: 16.3527, val_loss: 16.8829, val_MinusLogProbMetric: 16.8829

Epoch 762: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3527 - MinusLogProbMetric: 16.3527 - val_loss: 16.8829 - val_MinusLogProbMetric: 16.8829 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 763/1000
2023-09-27 03:11:35.363 
Epoch 763/1000 
	 loss: 16.3518, MinusLogProbMetric: 16.3518, val_loss: 16.8982, val_MinusLogProbMetric: 16.8982

Epoch 763: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3518 - MinusLogProbMetric: 16.3518 - val_loss: 16.8982 - val_MinusLogProbMetric: 16.8982 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 764/1000
2023-09-27 03:12:40.818 
Epoch 764/1000 
	 loss: 16.3508, MinusLogProbMetric: 16.3508, val_loss: 16.8909, val_MinusLogProbMetric: 16.8909

Epoch 764: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3508 - MinusLogProbMetric: 16.3508 - val_loss: 16.8909 - val_MinusLogProbMetric: 16.8909 - lr: 5.2083e-06 - 65s/epoch - 334ms/step
Epoch 765/1000
2023-09-27 03:13:45.688 
Epoch 765/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8961, val_MinusLogProbMetric: 16.8961

Epoch 765: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8961 - val_MinusLogProbMetric: 16.8961 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 766/1000
2023-09-27 03:14:50.701 
Epoch 766/1000 
	 loss: 16.3535, MinusLogProbMetric: 16.3535, val_loss: 16.8926, val_MinusLogProbMetric: 16.8926

Epoch 766: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3535 - MinusLogProbMetric: 16.3535 - val_loss: 16.8926 - val_MinusLogProbMetric: 16.8926 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 767/1000
2023-09-27 03:15:55.724 
Epoch 767/1000 
	 loss: 16.3548, MinusLogProbMetric: 16.3548, val_loss: 16.9134, val_MinusLogProbMetric: 16.9134

Epoch 767: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3548 - MinusLogProbMetric: 16.3548 - val_loss: 16.9134 - val_MinusLogProbMetric: 16.9134 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 768/1000
2023-09-27 03:17:00.542 
Epoch 768/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 16.8865, val_MinusLogProbMetric: 16.8865

Epoch 768: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 16.8865 - val_MinusLogProbMetric: 16.8865 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 769/1000
2023-09-27 03:18:04.931 
Epoch 769/1000 
	 loss: 16.3516, MinusLogProbMetric: 16.3516, val_loss: 16.8962, val_MinusLogProbMetric: 16.8962

Epoch 769: val_loss did not improve from 16.88266
196/196 - 64s - loss: 16.3516 - MinusLogProbMetric: 16.3516 - val_loss: 16.8962 - val_MinusLogProbMetric: 16.8962 - lr: 5.2083e-06 - 64s/epoch - 328ms/step
Epoch 770/1000
2023-09-27 03:19:09.686 
Epoch 770/1000 
	 loss: 16.3544, MinusLogProbMetric: 16.3544, val_loss: 16.8870, val_MinusLogProbMetric: 16.8870

Epoch 770: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3544 - MinusLogProbMetric: 16.3544 - val_loss: 16.8870 - val_MinusLogProbMetric: 16.8870 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 771/1000
2023-09-27 03:20:14.142 
Epoch 771/1000 
	 loss: 16.3530, MinusLogProbMetric: 16.3530, val_loss: 16.8854, val_MinusLogProbMetric: 16.8854

Epoch 771: val_loss did not improve from 16.88266
196/196 - 64s - loss: 16.3530 - MinusLogProbMetric: 16.3530 - val_loss: 16.8854 - val_MinusLogProbMetric: 16.8854 - lr: 5.2083e-06 - 64s/epoch - 329ms/step
Epoch 772/1000
2023-09-27 03:21:18.972 
Epoch 772/1000 
	 loss: 16.3525, MinusLogProbMetric: 16.3525, val_loss: 16.8837, val_MinusLogProbMetric: 16.8837

Epoch 772: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3525 - MinusLogProbMetric: 16.3525 - val_loss: 16.8837 - val_MinusLogProbMetric: 16.8837 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 773/1000
2023-09-27 03:22:23.983 
Epoch 773/1000 
	 loss: 16.3522, MinusLogProbMetric: 16.3522, val_loss: 16.8876, val_MinusLogProbMetric: 16.8876

Epoch 773: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3522 - MinusLogProbMetric: 16.3522 - val_loss: 16.8876 - val_MinusLogProbMetric: 16.8876 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 774/1000
2023-09-27 03:23:28.804 
Epoch 774/1000 
	 loss: 16.3521, MinusLogProbMetric: 16.3521, val_loss: 16.8903, val_MinusLogProbMetric: 16.8903

Epoch 774: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3521 - MinusLogProbMetric: 16.3521 - val_loss: 16.8903 - val_MinusLogProbMetric: 16.8903 - lr: 5.2083e-06 - 65s/epoch - 331ms/step
Epoch 775/1000
2023-09-27 03:24:33.569 
Epoch 775/1000 
	 loss: 16.3533, MinusLogProbMetric: 16.3533, val_loss: 16.8863, val_MinusLogProbMetric: 16.8863

Epoch 775: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3533 - MinusLogProbMetric: 16.3533 - val_loss: 16.8863 - val_MinusLogProbMetric: 16.8863 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 776/1000
2023-09-27 03:25:38.189 
Epoch 776/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 16.8892, val_MinusLogProbMetric: 16.8892

Epoch 776: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 16.8892 - val_MinusLogProbMetric: 16.8892 - lr: 5.2083e-06 - 65s/epoch - 330ms/step
Epoch 777/1000
2023-09-27 03:26:43.187 
Epoch 777/1000 
	 loss: 16.3526, MinusLogProbMetric: 16.3526, val_loss: 16.8880, val_MinusLogProbMetric: 16.8880

Epoch 777: val_loss did not improve from 16.88266
196/196 - 65s - loss: 16.3526 - MinusLogProbMetric: 16.3526 - val_loss: 16.8880 - val_MinusLogProbMetric: 16.8880 - lr: 5.2083e-06 - 65s/epoch - 332ms/step
Epoch 778/1000
2023-09-27 03:27:48.291 
Epoch 778/1000 
	 loss: 16.3471, MinusLogProbMetric: 16.3471, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 778: val_loss improved from 16.88266 to 16.88250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3471 - MinusLogProbMetric: 16.3471 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 2.6042e-06 - 66s/epoch - 338ms/step
Epoch 779/1000
2023-09-27 03:28:54.480 
Epoch 779/1000 
	 loss: 16.3491, MinusLogProbMetric: 16.3491, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 779: val_loss improved from 16.88250 to 16.88246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3491 - MinusLogProbMetric: 16.3491 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 2.6042e-06 - 66s/epoch - 336ms/step
Epoch 780/1000
2023-09-27 03:30:00.415 
Epoch 780/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 16.8847, val_MinusLogProbMetric: 16.8847

Epoch 780: val_loss did not improve from 16.88246
196/196 - 65s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 16.8847 - val_MinusLogProbMetric: 16.8847 - lr: 2.6042e-06 - 65s/epoch - 332ms/step
Epoch 781/1000
2023-09-27 03:31:05.390 
Epoch 781/1000 
	 loss: 16.3464, MinusLogProbMetric: 16.3464, val_loss: 16.8824, val_MinusLogProbMetric: 16.8824

Epoch 781: val_loss improved from 16.88246 to 16.88243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3464 - MinusLogProbMetric: 16.3464 - val_loss: 16.8824 - val_MinusLogProbMetric: 16.8824 - lr: 2.6042e-06 - 66s/epoch - 335ms/step
Epoch 782/1000
2023-09-27 03:32:10.619 
Epoch 782/1000 
	 loss: 16.3469, MinusLogProbMetric: 16.3469, val_loss: 16.8823, val_MinusLogProbMetric: 16.8823

Epoch 782: val_loss improved from 16.88243 to 16.88229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3469 - MinusLogProbMetric: 16.3469 - val_loss: 16.8823 - val_MinusLogProbMetric: 16.8823 - lr: 2.6042e-06 - 66s/epoch - 335ms/step
Epoch 783/1000
2023-09-27 03:33:16.669 
Epoch 783/1000 
	 loss: 16.3472, MinusLogProbMetric: 16.3472, val_loss: 16.8813, val_MinusLogProbMetric: 16.8813

Epoch 783: val_loss improved from 16.88229 to 16.88125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_286/weights/best_weights.h5
196/196 - 66s - loss: 16.3472 - MinusLogProbMetric: 16.3472 - val_loss: 16.8813 - val_MinusLogProbMetric: 16.8813 - lr: 2.6042e-06 - 66s/epoch - 336ms/step
Epoch 784/1000
2023-09-27 03:34:23.240 
Epoch 784/1000 
	 loss: 16.3469, MinusLogProbMetric: 16.3469, val_loss: 16.8826, val_MinusLogProbMetric: 16.8826

Epoch 784: val_loss did not improve from 16.88125
196/196 - 66s - loss: 16.3469 - MinusLogProbMetric: 16.3469 - val_loss: 16.8826 - val_MinusLogProbMetric: 16.8826 - lr: 2.6042e-06 - 66s/epoch - 334ms/step
Epoch 785/1000
2023-09-27 03:35:27.876 
Epoch 785/1000 
	 loss: 16.3470, MinusLogProbMetric: 16.3470, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 785: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3470 - MinusLogProbMetric: 16.3470 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 786/1000
2023-09-27 03:36:32.972 
Epoch 786/1000 
	 loss: 16.3463, MinusLogProbMetric: 16.3463, val_loss: 16.8850, val_MinusLogProbMetric: 16.8850

Epoch 786: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3463 - MinusLogProbMetric: 16.3463 - val_loss: 16.8850 - val_MinusLogProbMetric: 16.8850 - lr: 2.6042e-06 - 65s/epoch - 332ms/step
Epoch 787/1000
2023-09-27 03:37:37.894 
Epoch 787/1000 
	 loss: 16.3464, MinusLogProbMetric: 16.3464, val_loss: 16.8851, val_MinusLogProbMetric: 16.8851

Epoch 787: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3464 - MinusLogProbMetric: 16.3464 - val_loss: 16.8851 - val_MinusLogProbMetric: 16.8851 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 788/1000
2023-09-27 03:38:43.053 
Epoch 788/1000 
	 loss: 16.3479, MinusLogProbMetric: 16.3479, val_loss: 16.8868, val_MinusLogProbMetric: 16.8868

Epoch 788: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3479 - MinusLogProbMetric: 16.3479 - val_loss: 16.8868 - val_MinusLogProbMetric: 16.8868 - lr: 2.6042e-06 - 65s/epoch - 332ms/step
Epoch 789/1000
2023-09-27 03:39:47.936 
Epoch 789/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 16.8826, val_MinusLogProbMetric: 16.8826

Epoch 789: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 16.8826 - val_MinusLogProbMetric: 16.8826 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 790/1000
2023-09-27 03:40:52.839 
Epoch 790/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 16.8832, val_MinusLogProbMetric: 16.8832

Epoch 790: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 16.8832 - val_MinusLogProbMetric: 16.8832 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 791/1000
2023-09-27 03:41:57.725 
Epoch 791/1000 
	 loss: 16.3465, MinusLogProbMetric: 16.3465, val_loss: 16.8850, val_MinusLogProbMetric: 16.8850

Epoch 791: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3465 - MinusLogProbMetric: 16.3465 - val_loss: 16.8850 - val_MinusLogProbMetric: 16.8850 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 792/1000
2023-09-27 03:43:02.130 
Epoch 792/1000 
	 loss: 16.3458, MinusLogProbMetric: 16.3458, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 792: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3458 - MinusLogProbMetric: 16.3458 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 2.6042e-06 - 64s/epoch - 329ms/step
Epoch 793/1000
2023-09-27 03:44:06.671 
Epoch 793/1000 
	 loss: 16.3462, MinusLogProbMetric: 16.3462, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 793: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3462 - MinusLogProbMetric: 16.3462 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 2.6042e-06 - 65s/epoch - 329ms/step
Epoch 794/1000
2023-09-27 03:45:11.851 
Epoch 794/1000 
	 loss: 16.3468, MinusLogProbMetric: 16.3468, val_loss: 16.8833, val_MinusLogProbMetric: 16.8833

Epoch 794: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3468 - MinusLogProbMetric: 16.3468 - val_loss: 16.8833 - val_MinusLogProbMetric: 16.8833 - lr: 2.6042e-06 - 65s/epoch - 333ms/step
Epoch 795/1000
2023-09-27 03:46:16.658 
Epoch 795/1000 
	 loss: 16.3483, MinusLogProbMetric: 16.3483, val_loss: 16.8832, val_MinusLogProbMetric: 16.8832

Epoch 795: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3483 - MinusLogProbMetric: 16.3483 - val_loss: 16.8832 - val_MinusLogProbMetric: 16.8832 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 796/1000
2023-09-27 03:47:21.483 
Epoch 796/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 16.8848, val_MinusLogProbMetric: 16.8848

Epoch 796: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 16.8848 - val_MinusLogProbMetric: 16.8848 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 797/1000
2023-09-27 03:48:26.666 
Epoch 797/1000 
	 loss: 16.3456, MinusLogProbMetric: 16.3456, val_loss: 16.8880, val_MinusLogProbMetric: 16.8880

Epoch 797: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3456 - MinusLogProbMetric: 16.3456 - val_loss: 16.8880 - val_MinusLogProbMetric: 16.8880 - lr: 2.6042e-06 - 65s/epoch - 333ms/step
Epoch 798/1000
2023-09-27 03:49:31.572 
Epoch 798/1000 
	 loss: 16.3460, MinusLogProbMetric: 16.3460, val_loss: 16.8836, val_MinusLogProbMetric: 16.8836

Epoch 798: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3460 - MinusLogProbMetric: 16.3460 - val_loss: 16.8836 - val_MinusLogProbMetric: 16.8836 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 799/1000
2023-09-27 03:50:36.474 
Epoch 799/1000 
	 loss: 16.3472, MinusLogProbMetric: 16.3472, val_loss: 16.8929, val_MinusLogProbMetric: 16.8929

Epoch 799: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3472 - MinusLogProbMetric: 16.3472 - val_loss: 16.8929 - val_MinusLogProbMetric: 16.8929 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 800/1000
2023-09-27 03:51:41.896 
Epoch 800/1000 
	 loss: 16.3486, MinusLogProbMetric: 16.3486, val_loss: 16.8952, val_MinusLogProbMetric: 16.8952

Epoch 800: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3486 - MinusLogProbMetric: 16.3486 - val_loss: 16.8952 - val_MinusLogProbMetric: 16.8952 - lr: 2.6042e-06 - 65s/epoch - 334ms/step
Epoch 801/1000
2023-09-27 03:52:46.711 
Epoch 801/1000 
	 loss: 16.3469, MinusLogProbMetric: 16.3469, val_loss: 16.8871, val_MinusLogProbMetric: 16.8871

Epoch 801: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3469 - MinusLogProbMetric: 16.3469 - val_loss: 16.8871 - val_MinusLogProbMetric: 16.8871 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 802/1000
2023-09-27 03:53:51.520 
Epoch 802/1000 
	 loss: 16.3468, MinusLogProbMetric: 16.3468, val_loss: 16.8849, val_MinusLogProbMetric: 16.8849

Epoch 802: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3468 - MinusLogProbMetric: 16.3468 - val_loss: 16.8849 - val_MinusLogProbMetric: 16.8849 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 803/1000
2023-09-27 03:54:56.127 
Epoch 803/1000 
	 loss: 16.3462, MinusLogProbMetric: 16.3462, val_loss: 16.8852, val_MinusLogProbMetric: 16.8852

Epoch 803: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3462 - MinusLogProbMetric: 16.3462 - val_loss: 16.8852 - val_MinusLogProbMetric: 16.8852 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 804/1000
2023-09-27 03:56:01.327 
Epoch 804/1000 
	 loss: 16.3461, MinusLogProbMetric: 16.3461, val_loss: 16.8838, val_MinusLogProbMetric: 16.8838

Epoch 804: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3461 - MinusLogProbMetric: 16.3461 - val_loss: 16.8838 - val_MinusLogProbMetric: 16.8838 - lr: 2.6042e-06 - 65s/epoch - 333ms/step
Epoch 805/1000
2023-09-27 03:57:06.266 
Epoch 805/1000 
	 loss: 16.3494, MinusLogProbMetric: 16.3494, val_loss: 16.8845, val_MinusLogProbMetric: 16.8845

Epoch 805: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3494 - MinusLogProbMetric: 16.3494 - val_loss: 16.8845 - val_MinusLogProbMetric: 16.8845 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 806/1000
2023-09-27 03:58:11.454 
Epoch 806/1000 
	 loss: 16.3468, MinusLogProbMetric: 16.3468, val_loss: 16.8839, val_MinusLogProbMetric: 16.8839

Epoch 806: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3468 - MinusLogProbMetric: 16.3468 - val_loss: 16.8839 - val_MinusLogProbMetric: 16.8839 - lr: 2.6042e-06 - 65s/epoch - 333ms/step
Epoch 807/1000
2023-09-27 03:59:16.223 
Epoch 807/1000 
	 loss: 16.3475, MinusLogProbMetric: 16.3475, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 807: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3475 - MinusLogProbMetric: 16.3475 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 808/1000
2023-09-27 04:00:21.137 
Epoch 808/1000 
	 loss: 16.3459, MinusLogProbMetric: 16.3459, val_loss: 16.8857, val_MinusLogProbMetric: 16.8857

Epoch 808: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3459 - MinusLogProbMetric: 16.3459 - val_loss: 16.8857 - val_MinusLogProbMetric: 16.8857 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 809/1000
2023-09-27 04:01:25.824 
Epoch 809/1000 
	 loss: 16.3469, MinusLogProbMetric: 16.3469, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 809: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3469 - MinusLogProbMetric: 16.3469 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 810/1000
2023-09-27 04:02:30.255 
Epoch 810/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 16.8842, val_MinusLogProbMetric: 16.8842

Epoch 810: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 16.8842 - val_MinusLogProbMetric: 16.8842 - lr: 2.6042e-06 - 64s/epoch - 329ms/step
Epoch 811/1000
2023-09-27 04:03:35.135 
Epoch 811/1000 
	 loss: 16.3468, MinusLogProbMetric: 16.3468, val_loss: 16.8822, val_MinusLogProbMetric: 16.8822

Epoch 811: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3468 - MinusLogProbMetric: 16.3468 - val_loss: 16.8822 - val_MinusLogProbMetric: 16.8822 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 812/1000
2023-09-27 04:04:40.206 
Epoch 812/1000 
	 loss: 16.3460, MinusLogProbMetric: 16.3460, val_loss: 16.8873, val_MinusLogProbMetric: 16.8873

Epoch 812: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3460 - MinusLogProbMetric: 16.3460 - val_loss: 16.8873 - val_MinusLogProbMetric: 16.8873 - lr: 2.6042e-06 - 65s/epoch - 332ms/step
Epoch 813/1000
2023-09-27 04:05:45.408 
Epoch 813/1000 
	 loss: 16.3456, MinusLogProbMetric: 16.3456, val_loss: 16.8832, val_MinusLogProbMetric: 16.8832

Epoch 813: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3456 - MinusLogProbMetric: 16.3456 - val_loss: 16.8832 - val_MinusLogProbMetric: 16.8832 - lr: 2.6042e-06 - 65s/epoch - 333ms/step
Epoch 814/1000
2023-09-27 04:06:50.980 
Epoch 814/1000 
	 loss: 16.3479, MinusLogProbMetric: 16.3479, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 814: val_loss did not improve from 16.88125
196/196 - 66s - loss: 16.3479 - MinusLogProbMetric: 16.3479 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 2.6042e-06 - 66s/epoch - 335ms/step
Epoch 815/1000
2023-09-27 04:07:55.791 
Epoch 815/1000 
	 loss: 16.3467, MinusLogProbMetric: 16.3467, val_loss: 16.8837, val_MinusLogProbMetric: 16.8837

Epoch 815: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3467 - MinusLogProbMetric: 16.3467 - val_loss: 16.8837 - val_MinusLogProbMetric: 16.8837 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 816/1000
2023-09-27 04:09:00.699 
Epoch 816/1000 
	 loss: 16.3467, MinusLogProbMetric: 16.3467, val_loss: 16.8830, val_MinusLogProbMetric: 16.8830

Epoch 816: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3467 - MinusLogProbMetric: 16.3467 - val_loss: 16.8830 - val_MinusLogProbMetric: 16.8830 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 817/1000
2023-09-27 04:10:05.416 
Epoch 817/1000 
	 loss: 16.3459, MinusLogProbMetric: 16.3459, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 817: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3459 - MinusLogProbMetric: 16.3459 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 818/1000
2023-09-27 04:11:10.035 
Epoch 818/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 818: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 819/1000
2023-09-27 04:12:14.529 
Epoch 819/1000 
	 loss: 16.3459, MinusLogProbMetric: 16.3459, val_loss: 16.8838, val_MinusLogProbMetric: 16.8838

Epoch 819: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3459 - MinusLogProbMetric: 16.3459 - val_loss: 16.8838 - val_MinusLogProbMetric: 16.8838 - lr: 2.6042e-06 - 64s/epoch - 329ms/step
Epoch 820/1000
2023-09-27 04:13:18.768 
Epoch 820/1000 
	 loss: 16.3455, MinusLogProbMetric: 16.3455, val_loss: 16.8873, val_MinusLogProbMetric: 16.8873

Epoch 820: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3455 - MinusLogProbMetric: 16.3455 - val_loss: 16.8873 - val_MinusLogProbMetric: 16.8873 - lr: 2.6042e-06 - 64s/epoch - 328ms/step
Epoch 821/1000
2023-09-27 04:14:23.724 
Epoch 821/1000 
	 loss: 16.3462, MinusLogProbMetric: 16.3462, val_loss: 16.8833, val_MinusLogProbMetric: 16.8833

Epoch 821: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3462 - MinusLogProbMetric: 16.3462 - val_loss: 16.8833 - val_MinusLogProbMetric: 16.8833 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 822/1000
2023-09-27 04:15:28.628 
Epoch 822/1000 
	 loss: 16.3464, MinusLogProbMetric: 16.3464, val_loss: 16.8836, val_MinusLogProbMetric: 16.8836

Epoch 822: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3464 - MinusLogProbMetric: 16.3464 - val_loss: 16.8836 - val_MinusLogProbMetric: 16.8836 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 823/1000
2023-09-27 04:16:32.766 
Epoch 823/1000 
	 loss: 16.3454, MinusLogProbMetric: 16.3454, val_loss: 16.8836, val_MinusLogProbMetric: 16.8836

Epoch 823: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3454 - MinusLogProbMetric: 16.3454 - val_loss: 16.8836 - val_MinusLogProbMetric: 16.8836 - lr: 2.6042e-06 - 64s/epoch - 327ms/step
Epoch 824/1000
2023-09-27 04:17:37.424 
Epoch 824/1000 
	 loss: 16.3474, MinusLogProbMetric: 16.3474, val_loss: 16.8840, val_MinusLogProbMetric: 16.8840

Epoch 824: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3474 - MinusLogProbMetric: 16.3474 - val_loss: 16.8840 - val_MinusLogProbMetric: 16.8840 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 825/1000
2023-09-27 04:18:42.388 
Epoch 825/1000 
	 loss: 16.3468, MinusLogProbMetric: 16.3468, val_loss: 16.8848, val_MinusLogProbMetric: 16.8848

Epoch 825: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3468 - MinusLogProbMetric: 16.3468 - val_loss: 16.8848 - val_MinusLogProbMetric: 16.8848 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 826/1000
2023-09-27 04:19:47.623 
Epoch 826/1000 
	 loss: 16.3470, MinusLogProbMetric: 16.3470, val_loss: 16.8832, val_MinusLogProbMetric: 16.8832

Epoch 826: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3470 - MinusLogProbMetric: 16.3470 - val_loss: 16.8832 - val_MinusLogProbMetric: 16.8832 - lr: 2.6042e-06 - 65s/epoch - 333ms/step
Epoch 827/1000
2023-09-27 04:20:52.598 
Epoch 827/1000 
	 loss: 16.3458, MinusLogProbMetric: 16.3458, val_loss: 16.8833, val_MinusLogProbMetric: 16.8833

Epoch 827: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3458 - MinusLogProbMetric: 16.3458 - val_loss: 16.8833 - val_MinusLogProbMetric: 16.8833 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 828/1000
2023-09-27 04:21:57.745 
Epoch 828/1000 
	 loss: 16.3467, MinusLogProbMetric: 16.3467, val_loss: 16.8816, val_MinusLogProbMetric: 16.8816

Epoch 828: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3467 - MinusLogProbMetric: 16.3467 - val_loss: 16.8816 - val_MinusLogProbMetric: 16.8816 - lr: 2.6042e-06 - 65s/epoch - 332ms/step
Epoch 829/1000
2023-09-27 04:23:02.708 
Epoch 829/1000 
	 loss: 16.3477, MinusLogProbMetric: 16.3477, val_loss: 16.8830, val_MinusLogProbMetric: 16.8830

Epoch 829: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3477 - MinusLogProbMetric: 16.3477 - val_loss: 16.8830 - val_MinusLogProbMetric: 16.8830 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 830/1000
2023-09-27 04:24:06.982 
Epoch 830/1000 
	 loss: 16.3467, MinusLogProbMetric: 16.3467, val_loss: 16.8831, val_MinusLogProbMetric: 16.8831

Epoch 830: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3467 - MinusLogProbMetric: 16.3467 - val_loss: 16.8831 - val_MinusLogProbMetric: 16.8831 - lr: 2.6042e-06 - 64s/epoch - 328ms/step
Epoch 831/1000
2023-09-27 04:25:11.922 
Epoch 831/1000 
	 loss: 16.3464, MinusLogProbMetric: 16.3464, val_loss: 16.8824, val_MinusLogProbMetric: 16.8824

Epoch 831: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3464 - MinusLogProbMetric: 16.3464 - val_loss: 16.8824 - val_MinusLogProbMetric: 16.8824 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 832/1000
2023-09-27 04:26:16.549 
Epoch 832/1000 
	 loss: 16.3461, MinusLogProbMetric: 16.3461, val_loss: 16.8857, val_MinusLogProbMetric: 16.8857

Epoch 832: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3461 - MinusLogProbMetric: 16.3461 - val_loss: 16.8857 - val_MinusLogProbMetric: 16.8857 - lr: 2.6042e-06 - 65s/epoch - 330ms/step
Epoch 833/1000
2023-09-27 04:27:21.368 
Epoch 833/1000 
	 loss: 16.3461, MinusLogProbMetric: 16.3461, val_loss: 16.8952, val_MinusLogProbMetric: 16.8952

Epoch 833: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3461 - MinusLogProbMetric: 16.3461 - val_loss: 16.8952 - val_MinusLogProbMetric: 16.8952 - lr: 2.6042e-06 - 65s/epoch - 331ms/step
Epoch 834/1000
2023-09-27 04:28:25.969 
Epoch 834/1000 
	 loss: 16.3445, MinusLogProbMetric: 16.3445, val_loss: 16.8813, val_MinusLogProbMetric: 16.8813

Epoch 834: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3445 - MinusLogProbMetric: 16.3445 - val_loss: 16.8813 - val_MinusLogProbMetric: 16.8813 - lr: 1.3021e-06 - 65s/epoch - 330ms/step
Epoch 835/1000
2023-09-27 04:29:30.496 
Epoch 835/1000 
	 loss: 16.3442, MinusLogProbMetric: 16.3442, val_loss: 16.8838, val_MinusLogProbMetric: 16.8838

Epoch 835: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3442 - MinusLogProbMetric: 16.3442 - val_loss: 16.8838 - val_MinusLogProbMetric: 16.8838 - lr: 1.3021e-06 - 65s/epoch - 329ms/step
Epoch 836/1000
2023-09-27 04:30:35.302 
Epoch 836/1000 
	 loss: 16.3443, MinusLogProbMetric: 16.3443, val_loss: 16.8816, val_MinusLogProbMetric: 16.8816

Epoch 836: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3443 - MinusLogProbMetric: 16.3443 - val_loss: 16.8816 - val_MinusLogProbMetric: 16.8816 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 837/1000
2023-09-27 04:31:39.786 
Epoch 837/1000 
	 loss: 16.3437, MinusLogProbMetric: 16.3437, val_loss: 16.8852, val_MinusLogProbMetric: 16.8852

Epoch 837: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3437 - MinusLogProbMetric: 16.3437 - val_loss: 16.8852 - val_MinusLogProbMetric: 16.8852 - lr: 1.3021e-06 - 64s/epoch - 329ms/step
Epoch 838/1000
2023-09-27 04:32:44.425 
Epoch 838/1000 
	 loss: 16.3436, MinusLogProbMetric: 16.3436, val_loss: 16.8857, val_MinusLogProbMetric: 16.8857

Epoch 838: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3436 - MinusLogProbMetric: 16.3436 - val_loss: 16.8857 - val_MinusLogProbMetric: 16.8857 - lr: 1.3021e-06 - 65s/epoch - 330ms/step
Epoch 839/1000
2023-09-27 04:33:49.320 
Epoch 839/1000 
	 loss: 16.3438, MinusLogProbMetric: 16.3438, val_loss: 16.8824, val_MinusLogProbMetric: 16.8824

Epoch 839: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3438 - MinusLogProbMetric: 16.3438 - val_loss: 16.8824 - val_MinusLogProbMetric: 16.8824 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 840/1000
2023-09-27 04:34:54.351 
Epoch 840/1000 
	 loss: 16.3434, MinusLogProbMetric: 16.3434, val_loss: 16.8819, val_MinusLogProbMetric: 16.8819

Epoch 840: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3434 - MinusLogProbMetric: 16.3434 - val_loss: 16.8819 - val_MinusLogProbMetric: 16.8819 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 841/1000
2023-09-27 04:35:58.893 
Epoch 841/1000 
	 loss: 16.3435, MinusLogProbMetric: 16.3435, val_loss: 16.8819, val_MinusLogProbMetric: 16.8819

Epoch 841: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3435 - MinusLogProbMetric: 16.3435 - val_loss: 16.8819 - val_MinusLogProbMetric: 16.8819 - lr: 1.3021e-06 - 65s/epoch - 329ms/step
Epoch 842/1000
2023-09-27 04:37:03.866 
Epoch 842/1000 
	 loss: 16.3441, MinusLogProbMetric: 16.3441, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 842: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3441 - MinusLogProbMetric: 16.3441 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 843/1000
2023-09-27 04:38:08.170 
Epoch 843/1000 
	 loss: 16.3441, MinusLogProbMetric: 16.3441, val_loss: 16.8827, val_MinusLogProbMetric: 16.8827

Epoch 843: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3441 - MinusLogProbMetric: 16.3441 - val_loss: 16.8827 - val_MinusLogProbMetric: 16.8827 - lr: 1.3021e-06 - 64s/epoch - 328ms/step
Epoch 844/1000
2023-09-27 04:39:12.977 
Epoch 844/1000 
	 loss: 16.3441, MinusLogProbMetric: 16.3441, val_loss: 16.8824, val_MinusLogProbMetric: 16.8824

Epoch 844: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3441 - MinusLogProbMetric: 16.3441 - val_loss: 16.8824 - val_MinusLogProbMetric: 16.8824 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 845/1000
2023-09-27 04:40:17.492 
Epoch 845/1000 
	 loss: 16.3439, MinusLogProbMetric: 16.3439, val_loss: 16.8823, val_MinusLogProbMetric: 16.8823

Epoch 845: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3439 - MinusLogProbMetric: 16.3439 - val_loss: 16.8823 - val_MinusLogProbMetric: 16.8823 - lr: 1.3021e-06 - 65s/epoch - 329ms/step
Epoch 846/1000
2023-09-27 04:41:22.333 
Epoch 846/1000 
	 loss: 16.3435, MinusLogProbMetric: 16.3435, val_loss: 16.8816, val_MinusLogProbMetric: 16.8816

Epoch 846: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3435 - MinusLogProbMetric: 16.3435 - val_loss: 16.8816 - val_MinusLogProbMetric: 16.8816 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 847/1000
2023-09-27 04:42:27.453 
Epoch 847/1000 
	 loss: 16.3450, MinusLogProbMetric: 16.3450, val_loss: 16.8820, val_MinusLogProbMetric: 16.8820

Epoch 847: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3450 - MinusLogProbMetric: 16.3450 - val_loss: 16.8820 - val_MinusLogProbMetric: 16.8820 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 848/1000
2023-09-27 04:43:32.562 
Epoch 848/1000 
	 loss: 16.3443, MinusLogProbMetric: 16.3443, val_loss: 16.8824, val_MinusLogProbMetric: 16.8824

Epoch 848: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3443 - MinusLogProbMetric: 16.3443 - val_loss: 16.8824 - val_MinusLogProbMetric: 16.8824 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 849/1000
2023-09-27 04:44:37.454 
Epoch 849/1000 
	 loss: 16.3435, MinusLogProbMetric: 16.3435, val_loss: 16.8835, val_MinusLogProbMetric: 16.8835

Epoch 849: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3435 - MinusLogProbMetric: 16.3435 - val_loss: 16.8835 - val_MinusLogProbMetric: 16.8835 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 850/1000
2023-09-27 04:45:41.915 
Epoch 850/1000 
	 loss: 16.3446, MinusLogProbMetric: 16.3446, val_loss: 16.8826, val_MinusLogProbMetric: 16.8826

Epoch 850: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3446 - MinusLogProbMetric: 16.3446 - val_loss: 16.8826 - val_MinusLogProbMetric: 16.8826 - lr: 1.3021e-06 - 64s/epoch - 329ms/step
Epoch 851/1000
2023-09-27 04:46:46.931 
Epoch 851/1000 
	 loss: 16.3443, MinusLogProbMetric: 16.3443, val_loss: 16.8829, val_MinusLogProbMetric: 16.8829

Epoch 851: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3443 - MinusLogProbMetric: 16.3443 - val_loss: 16.8829 - val_MinusLogProbMetric: 16.8829 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 852/1000
2023-09-27 04:47:51.615 
Epoch 852/1000 
	 loss: 16.3434, MinusLogProbMetric: 16.3434, val_loss: 16.8826, val_MinusLogProbMetric: 16.8826

Epoch 852: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3434 - MinusLogProbMetric: 16.3434 - val_loss: 16.8826 - val_MinusLogProbMetric: 16.8826 - lr: 1.3021e-06 - 65s/epoch - 330ms/step
Epoch 853/1000
2023-09-27 04:48:56.224 
Epoch 853/1000 
	 loss: 16.3436, MinusLogProbMetric: 16.3436, val_loss: 16.8837, val_MinusLogProbMetric: 16.8837

Epoch 853: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3436 - MinusLogProbMetric: 16.3436 - val_loss: 16.8837 - val_MinusLogProbMetric: 16.8837 - lr: 1.3021e-06 - 65s/epoch - 330ms/step
Epoch 854/1000
2023-09-27 04:50:00.808 
Epoch 854/1000 
	 loss: 16.3439, MinusLogProbMetric: 16.3439, val_loss: 16.8823, val_MinusLogProbMetric: 16.8823

Epoch 854: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3439 - MinusLogProbMetric: 16.3439 - val_loss: 16.8823 - val_MinusLogProbMetric: 16.8823 - lr: 1.3021e-06 - 65s/epoch - 329ms/step
Epoch 855/1000
2023-09-27 04:51:05.817 
Epoch 855/1000 
	 loss: 16.3435, MinusLogProbMetric: 16.3435, val_loss: 16.8849, val_MinusLogProbMetric: 16.8849

Epoch 855: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3435 - MinusLogProbMetric: 16.3435 - val_loss: 16.8849 - val_MinusLogProbMetric: 16.8849 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 856/1000
2023-09-27 04:52:10.832 
Epoch 856/1000 
	 loss: 16.3432, MinusLogProbMetric: 16.3432, val_loss: 16.8818, val_MinusLogProbMetric: 16.8818

Epoch 856: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3432 - MinusLogProbMetric: 16.3432 - val_loss: 16.8818 - val_MinusLogProbMetric: 16.8818 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 857/1000
2023-09-27 04:53:15.319 
Epoch 857/1000 
	 loss: 16.3437, MinusLogProbMetric: 16.3437, val_loss: 16.8844, val_MinusLogProbMetric: 16.8844

Epoch 857: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3437 - MinusLogProbMetric: 16.3437 - val_loss: 16.8844 - val_MinusLogProbMetric: 16.8844 - lr: 1.3021e-06 - 64s/epoch - 329ms/step
Epoch 858/1000
2023-09-27 04:54:20.479 
Epoch 858/1000 
	 loss: 16.3435, MinusLogProbMetric: 16.3435, val_loss: 16.8827, val_MinusLogProbMetric: 16.8827

Epoch 858: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3435 - MinusLogProbMetric: 16.3435 - val_loss: 16.8827 - val_MinusLogProbMetric: 16.8827 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 859/1000
2023-09-27 04:55:25.338 
Epoch 859/1000 
	 loss: 16.3438, MinusLogProbMetric: 16.3438, val_loss: 16.8819, val_MinusLogProbMetric: 16.8819

Epoch 859: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3438 - MinusLogProbMetric: 16.3438 - val_loss: 16.8819 - val_MinusLogProbMetric: 16.8819 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 860/1000
2023-09-27 04:56:30.439 
Epoch 860/1000 
	 loss: 16.3439, MinusLogProbMetric: 16.3439, val_loss: 16.8821, val_MinusLogProbMetric: 16.8821

Epoch 860: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3439 - MinusLogProbMetric: 16.3439 - val_loss: 16.8821 - val_MinusLogProbMetric: 16.8821 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 861/1000
2023-09-27 04:57:33.659 
Epoch 861/1000 
	 loss: 16.3437, MinusLogProbMetric: 16.3437, val_loss: 16.8843, val_MinusLogProbMetric: 16.8843

Epoch 861: val_loss did not improve from 16.88125
196/196 - 63s - loss: 16.3437 - MinusLogProbMetric: 16.3437 - val_loss: 16.8843 - val_MinusLogProbMetric: 16.8843 - lr: 1.3021e-06 - 63s/epoch - 323ms/step
Epoch 862/1000
2023-09-27 04:58:32.717 
Epoch 862/1000 
	 loss: 16.3453, MinusLogProbMetric: 16.3453, val_loss: 16.8813, val_MinusLogProbMetric: 16.8813

Epoch 862: val_loss did not improve from 16.88125
196/196 - 59s - loss: 16.3453 - MinusLogProbMetric: 16.3453 - val_loss: 16.8813 - val_MinusLogProbMetric: 16.8813 - lr: 1.3021e-06 - 59s/epoch - 301ms/step
Epoch 863/1000
2023-09-27 04:59:36.768 
Epoch 863/1000 
	 loss: 16.3442, MinusLogProbMetric: 16.3442, val_loss: 16.8821, val_MinusLogProbMetric: 16.8821

Epoch 863: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3442 - MinusLogProbMetric: 16.3442 - val_loss: 16.8821 - val_MinusLogProbMetric: 16.8821 - lr: 1.3021e-06 - 64s/epoch - 327ms/step
Epoch 864/1000
2023-09-27 05:00:41.283 
Epoch 864/1000 
	 loss: 16.3438, MinusLogProbMetric: 16.3438, val_loss: 16.8814, val_MinusLogProbMetric: 16.8814

Epoch 864: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3438 - MinusLogProbMetric: 16.3438 - val_loss: 16.8814 - val_MinusLogProbMetric: 16.8814 - lr: 1.3021e-06 - 65s/epoch - 329ms/step
Epoch 865/1000
2023-09-27 05:01:46.137 
Epoch 865/1000 
	 loss: 16.3436, MinusLogProbMetric: 16.3436, val_loss: 16.8841, val_MinusLogProbMetric: 16.8841

Epoch 865: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3436 - MinusLogProbMetric: 16.3436 - val_loss: 16.8841 - val_MinusLogProbMetric: 16.8841 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 866/1000
2023-09-27 05:02:51.339 
Epoch 866/1000 
	 loss: 16.3438, MinusLogProbMetric: 16.3438, val_loss: 16.8814, val_MinusLogProbMetric: 16.8814

Epoch 866: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3438 - MinusLogProbMetric: 16.3438 - val_loss: 16.8814 - val_MinusLogProbMetric: 16.8814 - lr: 1.3021e-06 - 65s/epoch - 333ms/step
Epoch 867/1000
2023-09-27 05:03:56.179 
Epoch 867/1000 
	 loss: 16.3443, MinusLogProbMetric: 16.3443, val_loss: 16.8827, val_MinusLogProbMetric: 16.8827

Epoch 867: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3443 - MinusLogProbMetric: 16.3443 - val_loss: 16.8827 - val_MinusLogProbMetric: 16.8827 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 868/1000
2023-09-27 05:05:01.084 
Epoch 868/1000 
	 loss: 16.3442, MinusLogProbMetric: 16.3442, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 868: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3442 - MinusLogProbMetric: 16.3442 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 869/1000
2023-09-27 05:06:05.875 
Epoch 869/1000 
	 loss: 16.3440, MinusLogProbMetric: 16.3440, val_loss: 16.8826, val_MinusLogProbMetric: 16.8826

Epoch 869: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3440 - MinusLogProbMetric: 16.3440 - val_loss: 16.8826 - val_MinusLogProbMetric: 16.8826 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 870/1000
2023-09-27 05:07:10.708 
Epoch 870/1000 
	 loss: 16.3443, MinusLogProbMetric: 16.3443, val_loss: 16.8833, val_MinusLogProbMetric: 16.8833

Epoch 870: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3443 - MinusLogProbMetric: 16.3443 - val_loss: 16.8833 - val_MinusLogProbMetric: 16.8833 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 871/1000
2023-09-27 05:08:15.464 
Epoch 871/1000 
	 loss: 16.3434, MinusLogProbMetric: 16.3434, val_loss: 16.8897, val_MinusLogProbMetric: 16.8897

Epoch 871: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3434 - MinusLogProbMetric: 16.3434 - val_loss: 16.8897 - val_MinusLogProbMetric: 16.8897 - lr: 1.3021e-06 - 65s/epoch - 330ms/step
Epoch 872/1000
2023-09-27 05:09:20.420 
Epoch 872/1000 
	 loss: 16.3444, MinusLogProbMetric: 16.3444, val_loss: 16.8846, val_MinusLogProbMetric: 16.8846

Epoch 872: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3444 - MinusLogProbMetric: 16.3444 - val_loss: 16.8846 - val_MinusLogProbMetric: 16.8846 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 873/1000
2023-09-27 05:10:25.028 
Epoch 873/1000 
	 loss: 16.3431, MinusLogProbMetric: 16.3431, val_loss: 16.8822, val_MinusLogProbMetric: 16.8822

Epoch 873: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3431 - MinusLogProbMetric: 16.3431 - val_loss: 16.8822 - val_MinusLogProbMetric: 16.8822 - lr: 1.3021e-06 - 65s/epoch - 330ms/step
Epoch 874/1000
2023-09-27 05:11:30.047 
Epoch 874/1000 
	 loss: 16.3435, MinusLogProbMetric: 16.3435, val_loss: 16.8823, val_MinusLogProbMetric: 16.8823

Epoch 874: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3435 - MinusLogProbMetric: 16.3435 - val_loss: 16.8823 - val_MinusLogProbMetric: 16.8823 - lr: 1.3021e-06 - 65s/epoch - 332ms/step
Epoch 875/1000
2023-09-27 05:12:34.885 
Epoch 875/1000 
	 loss: 16.3433, MinusLogProbMetric: 16.3433, val_loss: 16.8831, val_MinusLogProbMetric: 16.8831

Epoch 875: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3433 - MinusLogProbMetric: 16.3433 - val_loss: 16.8831 - val_MinusLogProbMetric: 16.8831 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 876/1000
2023-09-27 05:13:39.795 
Epoch 876/1000 
	 loss: 16.3440, MinusLogProbMetric: 16.3440, val_loss: 16.8814, val_MinusLogProbMetric: 16.8814

Epoch 876: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3440 - MinusLogProbMetric: 16.3440 - val_loss: 16.8814 - val_MinusLogProbMetric: 16.8814 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 877/1000
2023-09-27 05:14:44.584 
Epoch 877/1000 
	 loss: 16.3442, MinusLogProbMetric: 16.3442, val_loss: 16.8819, val_MinusLogProbMetric: 16.8819

Epoch 877: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3442 - MinusLogProbMetric: 16.3442 - val_loss: 16.8819 - val_MinusLogProbMetric: 16.8819 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 878/1000
2023-09-27 05:15:49.127 
Epoch 878/1000 
	 loss: 16.3453, MinusLogProbMetric: 16.3453, val_loss: 16.8825, val_MinusLogProbMetric: 16.8825

Epoch 878: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3453 - MinusLogProbMetric: 16.3453 - val_loss: 16.8825 - val_MinusLogProbMetric: 16.8825 - lr: 1.3021e-06 - 65s/epoch - 329ms/step
Epoch 879/1000
2023-09-27 05:16:53.314 
Epoch 879/1000 
	 loss: 16.3437, MinusLogProbMetric: 16.3437, val_loss: 16.8878, val_MinusLogProbMetric: 16.8878

Epoch 879: val_loss did not improve from 16.88125
196/196 - 64s - loss: 16.3437 - MinusLogProbMetric: 16.3437 - val_loss: 16.8878 - val_MinusLogProbMetric: 16.8878 - lr: 1.3021e-06 - 64s/epoch - 327ms/step
Epoch 880/1000
2023-09-27 05:17:58.121 
Epoch 880/1000 
	 loss: 16.3444, MinusLogProbMetric: 16.3444, val_loss: 16.8837, val_MinusLogProbMetric: 16.8837

Epoch 880: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3444 - MinusLogProbMetric: 16.3444 - val_loss: 16.8837 - val_MinusLogProbMetric: 16.8837 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 881/1000
2023-09-27 05:19:02.840 
Epoch 881/1000 
	 loss: 16.3438, MinusLogProbMetric: 16.3438, val_loss: 16.8877, val_MinusLogProbMetric: 16.8877

Epoch 881: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3438 - MinusLogProbMetric: 16.3438 - val_loss: 16.8877 - val_MinusLogProbMetric: 16.8877 - lr: 1.3021e-06 - 65s/epoch - 330ms/step
Epoch 882/1000
2023-09-27 05:20:07.787 
Epoch 882/1000 
	 loss: 16.3440, MinusLogProbMetric: 16.3440, val_loss: 16.8822, val_MinusLogProbMetric: 16.8822

Epoch 882: val_loss did not improve from 16.88125
196/196 - 65s - loss: 16.3440 - MinusLogProbMetric: 16.3440 - val_loss: 16.8822 - val_MinusLogProbMetric: 16.8822 - lr: 1.3021e-06 - 65s/epoch - 331ms/step
Epoch 883/1000
2023-09-27 05:21:12.486 
Epoch 883/1000 
	 loss: 16.3436, MinusLogProbMetric: 16.3436, val_loss: 16.8822, val_MinusLogProbMetric: 16.8822

Epoch 883: val_loss did not improve from 16.88125
Restoring model weights from the end of the best epoch: 783.
196/196 - 65s - loss: 16.3436 - MinusLogProbMetric: 16.3436 - val_loss: 16.8822 - val_MinusLogProbMetric: 16.8822 - lr: 1.3021e-06 - 65s/epoch - 333ms/step
Epoch 883: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 24.668973572028335 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 13.349390521005262 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 10.438973534968682 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 12.319359818997327 seconds.
Training succeeded with seed 541.
Model trained in 57345.32 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 62.39 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 62.86 s.
===========
Run 286/720 done in 57544.10 s.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

===========
Generating train data for run 300.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_300/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_300/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_300/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_300
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1399280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,399,280
Trainable params: 1,399,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7fc324642590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc2a4153f40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc2a4153f40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc8e35f90f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc29c45d570>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc29c45dae0>, <keras.callbacks.ModelCheckpoint object at 0x7fc29c45dba0>, <keras.callbacks.EarlyStopping object at 0x7fc29c45de10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc29c45de40>, <keras.callbacks.TerminateOnNaN object at 0x7fc29c45da80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_300/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 300/720 with hyperparameters:
timestamp = 2023-09-27 05:22:21.861978
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1399280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
2023-09-27 05:24:19.798 
Epoch 1/1000 
	 loss: 59.4338, MinusLogProbMetric: 59.4338, val_loss: 25.1230, val_MinusLogProbMetric: 25.1230

Epoch 1: val_loss improved from inf to 25.12296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 119s - loss: 59.4338 - MinusLogProbMetric: 59.4338 - val_loss: 25.1230 - val_MinusLogProbMetric: 25.1230 - lr: 0.0010 - 119s/epoch - 605ms/step
Epoch 2/1000
2023-09-27 05:25:03.043 
Epoch 2/1000 
	 loss: 24.3562, MinusLogProbMetric: 24.3562, val_loss: 23.5843, val_MinusLogProbMetric: 23.5843

Epoch 2: val_loss improved from 25.12296 to 23.58430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 24.3562 - MinusLogProbMetric: 24.3562 - val_loss: 23.5843 - val_MinusLogProbMetric: 23.5843 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 3/1000
2023-09-27 05:25:46.059 
Epoch 3/1000 
	 loss: 22.3562, MinusLogProbMetric: 22.3562, val_loss: 21.2001, val_MinusLogProbMetric: 21.2001

Epoch 3: val_loss improved from 23.58430 to 21.20011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 22.3562 - MinusLogProbMetric: 22.3562 - val_loss: 21.2001 - val_MinusLogProbMetric: 21.2001 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 4/1000
2023-09-27 05:26:28.696 
Epoch 4/1000 
	 loss: 21.6214, MinusLogProbMetric: 21.6214, val_loss: 21.3425, val_MinusLogProbMetric: 21.3425

Epoch 4: val_loss did not improve from 21.20011
196/196 - 42s - loss: 21.6214 - MinusLogProbMetric: 21.6214 - val_loss: 21.3425 - val_MinusLogProbMetric: 21.3425 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 5/1000
2023-09-27 05:27:11.204 
Epoch 5/1000 
	 loss: 20.6396, MinusLogProbMetric: 20.6396, val_loss: 20.2652, val_MinusLogProbMetric: 20.2652

Epoch 5: val_loss improved from 21.20011 to 20.26520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 20.6396 - MinusLogProbMetric: 20.6396 - val_loss: 20.2652 - val_MinusLogProbMetric: 20.2652 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 6/1000
2023-09-27 05:27:53.890 
Epoch 6/1000 
	 loss: 20.2708, MinusLogProbMetric: 20.2708, val_loss: 21.3289, val_MinusLogProbMetric: 21.3289

Epoch 6: val_loss did not improve from 20.26520
196/196 - 42s - loss: 20.2708 - MinusLogProbMetric: 20.2708 - val_loss: 21.3289 - val_MinusLogProbMetric: 21.3289 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 7/1000
2023-09-27 05:28:36.142 
Epoch 7/1000 
	 loss: 19.8602, MinusLogProbMetric: 19.8602, val_loss: 22.4568, val_MinusLogProbMetric: 22.4568

Epoch 7: val_loss did not improve from 20.26520
196/196 - 42s - loss: 19.8602 - MinusLogProbMetric: 19.8602 - val_loss: 22.4568 - val_MinusLogProbMetric: 22.4568 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 8/1000
2023-09-27 05:29:18.401 
Epoch 8/1000 
	 loss: 19.8724, MinusLogProbMetric: 19.8724, val_loss: 21.8331, val_MinusLogProbMetric: 21.8331

Epoch 8: val_loss did not improve from 20.26520
196/196 - 42s - loss: 19.8724 - MinusLogProbMetric: 19.8724 - val_loss: 21.8331 - val_MinusLogProbMetric: 21.8331 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 9/1000
2023-09-27 05:30:00.554 
Epoch 9/1000 
	 loss: 19.5657, MinusLogProbMetric: 19.5657, val_loss: 19.3703, val_MinusLogProbMetric: 19.3703

Epoch 9: val_loss improved from 20.26520 to 19.37034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 19.5657 - MinusLogProbMetric: 19.5657 - val_loss: 19.3703 - val_MinusLogProbMetric: 19.3703 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 10/1000
2023-09-27 05:30:43.328 
Epoch 10/1000 
	 loss: 19.2742, MinusLogProbMetric: 19.2742, val_loss: 18.9264, val_MinusLogProbMetric: 18.9264

Epoch 10: val_loss improved from 19.37034 to 18.92638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 19.2742 - MinusLogProbMetric: 19.2742 - val_loss: 18.9264 - val_MinusLogProbMetric: 18.9264 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 11/1000
2023-09-27 05:31:25.987 
Epoch 11/1000 
	 loss: 18.9917, MinusLogProbMetric: 18.9917, val_loss: 19.1951, val_MinusLogProbMetric: 19.1951

Epoch 11: val_loss did not improve from 18.92638
196/196 - 42s - loss: 18.9917 - MinusLogProbMetric: 18.9917 - val_loss: 19.1951 - val_MinusLogProbMetric: 19.1951 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 12/1000
2023-09-27 05:32:06.392 
Epoch 12/1000 
	 loss: 18.7204, MinusLogProbMetric: 18.7204, val_loss: 19.8487, val_MinusLogProbMetric: 19.8487

Epoch 12: val_loss did not improve from 18.92638
196/196 - 40s - loss: 18.7204 - MinusLogProbMetric: 18.7204 - val_loss: 19.8487 - val_MinusLogProbMetric: 19.8487 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 13/1000
2023-09-27 05:32:45.025 
Epoch 13/1000 
	 loss: 18.7038, MinusLogProbMetric: 18.7038, val_loss: 19.5904, val_MinusLogProbMetric: 19.5904

Epoch 13: val_loss did not improve from 18.92638
196/196 - 39s - loss: 18.7038 - MinusLogProbMetric: 18.7038 - val_loss: 19.5904 - val_MinusLogProbMetric: 19.5904 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 14/1000
2023-09-27 05:33:18.564 
Epoch 14/1000 
	 loss: 18.5628, MinusLogProbMetric: 18.5628, val_loss: 18.4872, val_MinusLogProbMetric: 18.4872

Epoch 14: val_loss improved from 18.92638 to 18.48724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 34s - loss: 18.5628 - MinusLogProbMetric: 18.5628 - val_loss: 18.4872 - val_MinusLogProbMetric: 18.4872 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 15/1000
2023-09-27 05:33:53.182 
Epoch 15/1000 
	 loss: 18.5245, MinusLogProbMetric: 18.5245, val_loss: 18.8440, val_MinusLogProbMetric: 18.8440

Epoch 15: val_loss did not improve from 18.48724
196/196 - 34s - loss: 18.5245 - MinusLogProbMetric: 18.5245 - val_loss: 18.8440 - val_MinusLogProbMetric: 18.8440 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 16/1000
2023-09-27 05:34:33.711 
Epoch 16/1000 
	 loss: 18.4029, MinusLogProbMetric: 18.4029, val_loss: 19.4063, val_MinusLogProbMetric: 19.4063

Epoch 16: val_loss did not improve from 18.48724
196/196 - 41s - loss: 18.4029 - MinusLogProbMetric: 18.4029 - val_loss: 19.4063 - val_MinusLogProbMetric: 19.4063 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 17/1000
2023-09-27 05:35:12.423 
Epoch 17/1000 
	 loss: 18.4032, MinusLogProbMetric: 18.4032, val_loss: 18.1929, val_MinusLogProbMetric: 18.1929

Epoch 17: val_loss improved from 18.48724 to 18.19294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 39s - loss: 18.4032 - MinusLogProbMetric: 18.4032 - val_loss: 18.1929 - val_MinusLogProbMetric: 18.1929 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 18/1000
2023-09-27 05:35:46.483 
Epoch 18/1000 
	 loss: 18.2005, MinusLogProbMetric: 18.2005, val_loss: 18.6524, val_MinusLogProbMetric: 18.6524

Epoch 18: val_loss did not improve from 18.19294
196/196 - 34s - loss: 18.2005 - MinusLogProbMetric: 18.2005 - val_loss: 18.6524 - val_MinusLogProbMetric: 18.6524 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 19/1000
2023-09-27 05:36:19.961 
Epoch 19/1000 
	 loss: 18.2204, MinusLogProbMetric: 18.2204, val_loss: 18.9234, val_MinusLogProbMetric: 18.9234

Epoch 19: val_loss did not improve from 18.19294
196/196 - 33s - loss: 18.2204 - MinusLogProbMetric: 18.2204 - val_loss: 18.9234 - val_MinusLogProbMetric: 18.9234 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 20/1000
2023-09-27 05:36:59.066 
Epoch 20/1000 
	 loss: 18.1636, MinusLogProbMetric: 18.1636, val_loss: 18.3192, val_MinusLogProbMetric: 18.3192

Epoch 20: val_loss did not improve from 18.19294
196/196 - 39s - loss: 18.1636 - MinusLogProbMetric: 18.1636 - val_loss: 18.3192 - val_MinusLogProbMetric: 18.3192 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 21/1000
2023-09-27 05:37:41.209 
Epoch 21/1000 
	 loss: 18.1961, MinusLogProbMetric: 18.1961, val_loss: 18.1422, val_MinusLogProbMetric: 18.1422

Epoch 21: val_loss improved from 18.19294 to 18.14215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 18.1961 - MinusLogProbMetric: 18.1961 - val_loss: 18.1422 - val_MinusLogProbMetric: 18.1422 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 22/1000
2023-09-27 05:38:22.371 
Epoch 22/1000 
	 loss: 17.9938, MinusLogProbMetric: 17.9938, val_loss: 19.1722, val_MinusLogProbMetric: 19.1722

Epoch 22: val_loss did not improve from 18.14215
196/196 - 40s - loss: 17.9938 - MinusLogProbMetric: 17.9938 - val_loss: 19.1722 - val_MinusLogProbMetric: 19.1722 - lr: 0.0010 - 40s/epoch - 207ms/step
Epoch 23/1000
2023-09-27 05:39:03.958 
Epoch 23/1000 
	 loss: 18.0258, MinusLogProbMetric: 18.0258, val_loss: 18.0315, val_MinusLogProbMetric: 18.0315

Epoch 23: val_loss improved from 18.14215 to 18.03154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 42s - loss: 18.0258 - MinusLogProbMetric: 18.0258 - val_loss: 18.0315 - val_MinusLogProbMetric: 18.0315 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 24/1000
2023-09-27 05:39:46.387 
Epoch 24/1000 
	 loss: 17.8865, MinusLogProbMetric: 17.8865, val_loss: 18.6357, val_MinusLogProbMetric: 18.6357

Epoch 24: val_loss did not improve from 18.03154
196/196 - 42s - loss: 17.8865 - MinusLogProbMetric: 17.8865 - val_loss: 18.6357 - val_MinusLogProbMetric: 18.6357 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 25/1000
2023-09-27 05:40:27.658 
Epoch 25/1000 
	 loss: 17.8648, MinusLogProbMetric: 17.8648, val_loss: 18.1295, val_MinusLogProbMetric: 18.1295

Epoch 25: val_loss did not improve from 18.03154
196/196 - 41s - loss: 17.8648 - MinusLogProbMetric: 17.8648 - val_loss: 18.1295 - val_MinusLogProbMetric: 18.1295 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 26/1000
2023-09-27 05:41:09.283 
Epoch 26/1000 
	 loss: 17.8838, MinusLogProbMetric: 17.8838, val_loss: 18.9980, val_MinusLogProbMetric: 18.9980

Epoch 26: val_loss did not improve from 18.03154
196/196 - 42s - loss: 17.8838 - MinusLogProbMetric: 17.8838 - val_loss: 18.9980 - val_MinusLogProbMetric: 18.9980 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 27/1000
2023-09-27 05:41:50.825 
Epoch 27/1000 
	 loss: 17.8487, MinusLogProbMetric: 17.8487, val_loss: 18.1471, val_MinusLogProbMetric: 18.1471

Epoch 27: val_loss did not improve from 18.03154
196/196 - 42s - loss: 17.8487 - MinusLogProbMetric: 17.8487 - val_loss: 18.1471 - val_MinusLogProbMetric: 18.1471 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 28/1000
2023-09-27 05:42:32.296 
Epoch 28/1000 
	 loss: 17.8695, MinusLogProbMetric: 17.8695, val_loss: 20.5586, val_MinusLogProbMetric: 20.5586

Epoch 28: val_loss did not improve from 18.03154
196/196 - 41s - loss: 17.8695 - MinusLogProbMetric: 17.8695 - val_loss: 20.5586 - val_MinusLogProbMetric: 20.5586 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 29/1000
2023-09-27 05:43:14.053 
Epoch 29/1000 
	 loss: 17.8296, MinusLogProbMetric: 17.8296, val_loss: 18.3916, val_MinusLogProbMetric: 18.3916

Epoch 29: val_loss did not improve from 18.03154
196/196 - 42s - loss: 17.8296 - MinusLogProbMetric: 17.8296 - val_loss: 18.3916 - val_MinusLogProbMetric: 18.3916 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 30/1000
2023-09-27 05:43:55.138 
Epoch 30/1000 
	 loss: 17.6355, MinusLogProbMetric: 17.6355, val_loss: 17.8349, val_MinusLogProbMetric: 17.8349

Epoch 30: val_loss improved from 18.03154 to 17.83486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 42s - loss: 17.6355 - MinusLogProbMetric: 17.6355 - val_loss: 17.8349 - val_MinusLogProbMetric: 17.8349 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 31/1000
2023-09-27 05:44:37.179 
Epoch 31/1000 
	 loss: 17.6396, MinusLogProbMetric: 17.6396, val_loss: 17.8067, val_MinusLogProbMetric: 17.8067

Epoch 31: val_loss improved from 17.83486 to 17.80672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 42s - loss: 17.6396 - MinusLogProbMetric: 17.6396 - val_loss: 17.8067 - val_MinusLogProbMetric: 17.8067 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 32/1000
2023-09-27 05:45:19.680 
Epoch 32/1000 
	 loss: 17.6092, MinusLogProbMetric: 17.6092, val_loss: 18.5355, val_MinusLogProbMetric: 18.5355

Epoch 32: val_loss did not improve from 17.80672
196/196 - 42s - loss: 17.6092 - MinusLogProbMetric: 17.6092 - val_loss: 18.5355 - val_MinusLogProbMetric: 18.5355 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 33/1000
2023-09-27 05:46:00.973 
Epoch 33/1000 
	 loss: 17.6732, MinusLogProbMetric: 17.6732, val_loss: 17.8406, val_MinusLogProbMetric: 17.8406

Epoch 33: val_loss did not improve from 17.80672
196/196 - 41s - loss: 17.6732 - MinusLogProbMetric: 17.6732 - val_loss: 17.8406 - val_MinusLogProbMetric: 17.8406 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 34/1000
2023-09-27 05:46:43.033 
Epoch 34/1000 
	 loss: 17.5479, MinusLogProbMetric: 17.5479, val_loss: 17.7829, val_MinusLogProbMetric: 17.7829

Epoch 34: val_loss improved from 17.80672 to 17.78285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 17.5479 - MinusLogProbMetric: 17.5479 - val_loss: 17.7829 - val_MinusLogProbMetric: 17.7829 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 35/1000
2023-09-27 05:47:25.408 
Epoch 35/1000 
	 loss: 17.5399, MinusLogProbMetric: 17.5399, val_loss: 17.8299, val_MinusLogProbMetric: 17.8299

Epoch 35: val_loss did not improve from 17.78285
196/196 - 42s - loss: 17.5399 - MinusLogProbMetric: 17.5399 - val_loss: 17.8299 - val_MinusLogProbMetric: 17.8299 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 36/1000
2023-09-27 05:48:07.334 
Epoch 36/1000 
	 loss: 17.7368, MinusLogProbMetric: 17.7368, val_loss: 18.1530, val_MinusLogProbMetric: 18.1530

Epoch 36: val_loss did not improve from 17.78285
196/196 - 42s - loss: 17.7368 - MinusLogProbMetric: 17.7368 - val_loss: 18.1530 - val_MinusLogProbMetric: 18.1530 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 37/1000
2023-09-27 05:48:49.133 
Epoch 37/1000 
	 loss: 17.5320, MinusLogProbMetric: 17.5320, val_loss: 17.6390, val_MinusLogProbMetric: 17.6390

Epoch 37: val_loss improved from 17.78285 to 17.63898, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 17.5320 - MinusLogProbMetric: 17.5320 - val_loss: 17.6390 - val_MinusLogProbMetric: 17.6390 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 38/1000
2023-09-27 05:49:31.384 
Epoch 38/1000 
	 loss: 17.5095, MinusLogProbMetric: 17.5095, val_loss: 17.7909, val_MinusLogProbMetric: 17.7909

Epoch 38: val_loss did not improve from 17.63898
196/196 - 41s - loss: 17.5095 - MinusLogProbMetric: 17.5095 - val_loss: 17.7909 - val_MinusLogProbMetric: 17.7909 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 39/1000
2023-09-27 05:50:13.075 
Epoch 39/1000 
	 loss: 17.4126, MinusLogProbMetric: 17.4126, val_loss: 18.6691, val_MinusLogProbMetric: 18.6691

Epoch 39: val_loss did not improve from 17.63898
196/196 - 42s - loss: 17.4126 - MinusLogProbMetric: 17.4126 - val_loss: 18.6691 - val_MinusLogProbMetric: 18.6691 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 40/1000
2023-09-27 05:50:54.683 
Epoch 40/1000 
	 loss: 17.4464, MinusLogProbMetric: 17.4464, val_loss: 17.5259, val_MinusLogProbMetric: 17.5259

Epoch 40: val_loss improved from 17.63898 to 17.52590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 42s - loss: 17.4464 - MinusLogProbMetric: 17.4464 - val_loss: 17.5259 - val_MinusLogProbMetric: 17.5259 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 41/1000
2023-09-27 05:51:37.061 
Epoch 41/1000 
	 loss: 17.5188, MinusLogProbMetric: 17.5188, val_loss: 17.6308, val_MinusLogProbMetric: 17.6308

Epoch 41: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.5188 - MinusLogProbMetric: 17.5188 - val_loss: 17.6308 - val_MinusLogProbMetric: 17.6308 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 42/1000
2023-09-27 05:52:18.803 
Epoch 42/1000 
	 loss: 17.3720, MinusLogProbMetric: 17.3720, val_loss: 17.8843, val_MinusLogProbMetric: 17.8843

Epoch 42: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.3720 - MinusLogProbMetric: 17.3720 - val_loss: 17.8843 - val_MinusLogProbMetric: 17.8843 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 43/1000
2023-09-27 05:53:00.762 
Epoch 43/1000 
	 loss: 17.3543, MinusLogProbMetric: 17.3543, val_loss: 19.5401, val_MinusLogProbMetric: 19.5401

Epoch 43: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.3543 - MinusLogProbMetric: 17.3543 - val_loss: 19.5401 - val_MinusLogProbMetric: 19.5401 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 44/1000
2023-09-27 05:53:42.457 
Epoch 44/1000 
	 loss: 17.3818, MinusLogProbMetric: 17.3818, val_loss: 17.6404, val_MinusLogProbMetric: 17.6404

Epoch 44: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.3818 - MinusLogProbMetric: 17.3818 - val_loss: 17.6404 - val_MinusLogProbMetric: 17.6404 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 45/1000
2023-09-27 05:54:24.432 
Epoch 45/1000 
	 loss: 17.3062, MinusLogProbMetric: 17.3062, val_loss: 17.6438, val_MinusLogProbMetric: 17.6438

Epoch 45: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.3062 - MinusLogProbMetric: 17.3062 - val_loss: 17.6438 - val_MinusLogProbMetric: 17.6438 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 46/1000
2023-09-27 05:55:05.925 
Epoch 46/1000 
	 loss: 17.2966, MinusLogProbMetric: 17.2966, val_loss: 17.6081, val_MinusLogProbMetric: 17.6081

Epoch 46: val_loss did not improve from 17.52590
196/196 - 41s - loss: 17.2966 - MinusLogProbMetric: 17.2966 - val_loss: 17.6081 - val_MinusLogProbMetric: 17.6081 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 47/1000
2023-09-27 05:55:47.792 
Epoch 47/1000 
	 loss: 17.2404, MinusLogProbMetric: 17.2404, val_loss: 17.6975, val_MinusLogProbMetric: 17.6975

Epoch 47: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.2404 - MinusLogProbMetric: 17.2404 - val_loss: 17.6975 - val_MinusLogProbMetric: 17.6975 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 48/1000
2023-09-27 05:56:29.707 
Epoch 48/1000 
	 loss: 17.3660, MinusLogProbMetric: 17.3660, val_loss: 17.6658, val_MinusLogProbMetric: 17.6658

Epoch 48: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.3660 - MinusLogProbMetric: 17.3660 - val_loss: 17.6658 - val_MinusLogProbMetric: 17.6658 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 49/1000
2023-09-27 05:57:11.250 
Epoch 49/1000 
	 loss: 17.2744, MinusLogProbMetric: 17.2744, val_loss: 17.6808, val_MinusLogProbMetric: 17.6808

Epoch 49: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.2744 - MinusLogProbMetric: 17.2744 - val_loss: 17.6808 - val_MinusLogProbMetric: 17.6808 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 50/1000
2023-09-27 05:57:53.170 
Epoch 50/1000 
	 loss: 17.2592, MinusLogProbMetric: 17.2592, val_loss: 17.6573, val_MinusLogProbMetric: 17.6573

Epoch 50: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.2592 - MinusLogProbMetric: 17.2592 - val_loss: 17.6573 - val_MinusLogProbMetric: 17.6573 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 51/1000
2023-09-27 05:58:34.621 
Epoch 51/1000 
	 loss: 17.2501, MinusLogProbMetric: 17.2501, val_loss: 17.5898, val_MinusLogProbMetric: 17.5898

Epoch 51: val_loss did not improve from 17.52590
196/196 - 41s - loss: 17.2501 - MinusLogProbMetric: 17.2501 - val_loss: 17.5898 - val_MinusLogProbMetric: 17.5898 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 52/1000
2023-09-27 05:59:15.994 
Epoch 52/1000 
	 loss: 17.2044, MinusLogProbMetric: 17.2044, val_loss: 17.6574, val_MinusLogProbMetric: 17.6574

Epoch 52: val_loss did not improve from 17.52590
196/196 - 41s - loss: 17.2044 - MinusLogProbMetric: 17.2044 - val_loss: 17.6574 - val_MinusLogProbMetric: 17.6574 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 53/1000
2023-09-27 05:59:57.489 
Epoch 53/1000 
	 loss: 17.2313, MinusLogProbMetric: 17.2313, val_loss: 17.5831, val_MinusLogProbMetric: 17.5831

Epoch 53: val_loss did not improve from 17.52590
196/196 - 41s - loss: 17.2313 - MinusLogProbMetric: 17.2313 - val_loss: 17.5831 - val_MinusLogProbMetric: 17.5831 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 54/1000
2023-09-27 06:00:39.144 
Epoch 54/1000 
	 loss: 17.1583, MinusLogProbMetric: 17.1583, val_loss: 18.2619, val_MinusLogProbMetric: 18.2619

Epoch 54: val_loss did not improve from 17.52590
196/196 - 42s - loss: 17.1583 - MinusLogProbMetric: 17.1583 - val_loss: 18.2619 - val_MinusLogProbMetric: 18.2619 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 55/1000
2023-09-27 06:01:20.619 
Epoch 55/1000 
	 loss: 17.2168, MinusLogProbMetric: 17.2168, val_loss: 17.5531, val_MinusLogProbMetric: 17.5531

Epoch 55: val_loss did not improve from 17.52590
196/196 - 41s - loss: 17.2168 - MinusLogProbMetric: 17.2168 - val_loss: 17.5531 - val_MinusLogProbMetric: 17.5531 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 56/1000
2023-09-27 06:02:02.396 
Epoch 56/1000 
	 loss: 17.1002, MinusLogProbMetric: 17.1002, val_loss: 17.4018, val_MinusLogProbMetric: 17.4018

Epoch 56: val_loss improved from 17.52590 to 17.40179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 42s - loss: 17.1002 - MinusLogProbMetric: 17.1002 - val_loss: 17.4018 - val_MinusLogProbMetric: 17.4018 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 57/1000
2023-09-27 06:02:44.909 
Epoch 57/1000 
	 loss: 17.1276, MinusLogProbMetric: 17.1276, val_loss: 17.3996, val_MinusLogProbMetric: 17.3996

Epoch 57: val_loss improved from 17.40179 to 17.39957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 43s - loss: 17.1276 - MinusLogProbMetric: 17.1276 - val_loss: 17.3996 - val_MinusLogProbMetric: 17.3996 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 58/1000
2023-09-27 06:03:26.908 
Epoch 58/1000 
	 loss: 17.1249, MinusLogProbMetric: 17.1249, val_loss: 17.4614, val_MinusLogProbMetric: 17.4614

Epoch 58: val_loss did not improve from 17.39957
196/196 - 41s - loss: 17.1249 - MinusLogProbMetric: 17.1249 - val_loss: 17.4614 - val_MinusLogProbMetric: 17.4614 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 59/1000
2023-09-27 06:04:08.921 
Epoch 59/1000 
	 loss: 17.1002, MinusLogProbMetric: 17.1002, val_loss: 17.8686, val_MinusLogProbMetric: 17.8686

Epoch 59: val_loss did not improve from 17.39957
196/196 - 42s - loss: 17.1002 - MinusLogProbMetric: 17.1002 - val_loss: 17.8686 - val_MinusLogProbMetric: 17.8686 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 60/1000
2023-09-27 06:04:50.507 
Epoch 60/1000 
	 loss: 17.0606, MinusLogProbMetric: 17.0606, val_loss: 17.5675, val_MinusLogProbMetric: 17.5675

Epoch 60: val_loss did not improve from 17.39957
196/196 - 42s - loss: 17.0606 - MinusLogProbMetric: 17.0606 - val_loss: 17.5675 - val_MinusLogProbMetric: 17.5675 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 61/1000
2023-09-27 06:05:32.238 
Epoch 61/1000 
	 loss: 17.0527, MinusLogProbMetric: 17.0527, val_loss: 17.3923, val_MinusLogProbMetric: 17.3923

Epoch 61: val_loss improved from 17.39957 to 17.39227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 42s - loss: 17.0527 - MinusLogProbMetric: 17.0527 - val_loss: 17.3923 - val_MinusLogProbMetric: 17.3923 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 62/1000
2023-09-27 06:06:14.750 
Epoch 62/1000 
	 loss: 17.0022, MinusLogProbMetric: 17.0022, val_loss: 17.5114, val_MinusLogProbMetric: 17.5114

Epoch 62: val_loss did not improve from 17.39227
196/196 - 42s - loss: 17.0022 - MinusLogProbMetric: 17.0022 - val_loss: 17.5114 - val_MinusLogProbMetric: 17.5114 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 63/1000
2023-09-27 06:06:56.229 
Epoch 63/1000 
	 loss: 16.9895, MinusLogProbMetric: 16.9895, val_loss: 18.4093, val_MinusLogProbMetric: 18.4093

Epoch 63: val_loss did not improve from 17.39227
196/196 - 41s - loss: 16.9895 - MinusLogProbMetric: 16.9895 - val_loss: 18.4093 - val_MinusLogProbMetric: 18.4093 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 64/1000
2023-09-27 06:07:37.691 
Epoch 64/1000 
	 loss: 17.1154, MinusLogProbMetric: 17.1154, val_loss: 17.3066, val_MinusLogProbMetric: 17.3066

Epoch 64: val_loss improved from 17.39227 to 17.30657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_300/weights/best_weights.h5
196/196 - 42s - loss: 17.1154 - MinusLogProbMetric: 17.1154 - val_loss: 17.3066 - val_MinusLogProbMetric: 17.3066 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 65/1000
2023-09-27 06:08:20.223 
Epoch 65/1000 
	 loss: 16.9965, MinusLogProbMetric: 16.9965, val_loss: 17.4378, val_MinusLogProbMetric: 17.4378

Epoch 65: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.9965 - MinusLogProbMetric: 16.9965 - val_loss: 17.4378 - val_MinusLogProbMetric: 17.4378 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 66/1000
2023-09-27 06:09:01.895 
Epoch 66/1000 
	 loss: 17.0199, MinusLogProbMetric: 17.0199, val_loss: 17.8689, val_MinusLogProbMetric: 17.8689

Epoch 66: val_loss did not improve from 17.30657
196/196 - 42s - loss: 17.0199 - MinusLogProbMetric: 17.0199 - val_loss: 17.8689 - val_MinusLogProbMetric: 17.8689 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 67/1000
2023-09-27 06:09:43.730 
Epoch 67/1000 
	 loss: 17.0341, MinusLogProbMetric: 17.0341, val_loss: 17.3257, val_MinusLogProbMetric: 17.3257

Epoch 67: val_loss did not improve from 17.30657
196/196 - 42s - loss: 17.0341 - MinusLogProbMetric: 17.0341 - val_loss: 17.3257 - val_MinusLogProbMetric: 17.3257 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 68/1000
2023-09-27 06:10:25.309 
Epoch 68/1000 
	 loss: 17.0321, MinusLogProbMetric: 17.0321, val_loss: 17.4441, val_MinusLogProbMetric: 17.4441

Epoch 68: val_loss did not improve from 17.30657
196/196 - 42s - loss: 17.0321 - MinusLogProbMetric: 17.0321 - val_loss: 17.4441 - val_MinusLogProbMetric: 17.4441 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 69/1000
2023-09-27 06:11:06.628 
Epoch 69/1000 
	 loss: 16.9326, MinusLogProbMetric: 16.9326, val_loss: 17.4770, val_MinusLogProbMetric: 17.4770

Epoch 69: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.9326 - MinusLogProbMetric: 16.9326 - val_loss: 17.4770 - val_MinusLogProbMetric: 17.4770 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 70/1000
2023-09-27 06:11:48.225 
Epoch 70/1000 
	 loss: 16.9006, MinusLogProbMetric: 16.9006, val_loss: 17.4854, val_MinusLogProbMetric: 17.4854

Epoch 70: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.9006 - MinusLogProbMetric: 16.9006 - val_loss: 17.4854 - val_MinusLogProbMetric: 17.4854 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 71/1000
2023-09-27 06:12:29.442 
Epoch 71/1000 
	 loss: 16.9702, MinusLogProbMetric: 16.9702, val_loss: 17.6058, val_MinusLogProbMetric: 17.6058

Epoch 71: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.9702 - MinusLogProbMetric: 16.9702 - val_loss: 17.6058 - val_MinusLogProbMetric: 17.6058 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 72/1000
2023-09-27 06:13:10.953 
Epoch 72/1000 
	 loss: 16.8938, MinusLogProbMetric: 16.8938, val_loss: 17.5097, val_MinusLogProbMetric: 17.5097

Epoch 72: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.8938 - MinusLogProbMetric: 16.8938 - val_loss: 17.5097 - val_MinusLogProbMetric: 17.5097 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 73/1000
2023-09-27 06:13:52.625 
Epoch 73/1000 
	 loss: 16.9433, MinusLogProbMetric: 16.9433, val_loss: 17.7670, val_MinusLogProbMetric: 17.7670

Epoch 73: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.9433 - MinusLogProbMetric: 16.9433 - val_loss: 17.7670 - val_MinusLogProbMetric: 17.7670 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 74/1000
2023-09-27 06:14:34.258 
Epoch 74/1000 
	 loss: 16.8415, MinusLogProbMetric: 16.8415, val_loss: 18.1888, val_MinusLogProbMetric: 18.1888

Epoch 74: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.8415 - MinusLogProbMetric: 16.8415 - val_loss: 18.1888 - val_MinusLogProbMetric: 18.1888 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 75/1000
2023-09-27 06:15:15.769 
Epoch 75/1000 
	 loss: 16.8910, MinusLogProbMetric: 16.8910, val_loss: 18.1878, val_MinusLogProbMetric: 18.1878

Epoch 75: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.8910 - MinusLogProbMetric: 16.8910 - val_loss: 18.1878 - val_MinusLogProbMetric: 18.1878 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 76/1000
2023-09-27 06:15:57.346 
Epoch 76/1000 
	 loss: 16.8353, MinusLogProbMetric: 16.8353, val_loss: 17.3612, val_MinusLogProbMetric: 17.3612

Epoch 76: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.8353 - MinusLogProbMetric: 16.8353 - val_loss: 17.3612 - val_MinusLogProbMetric: 17.3612 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 77/1000
2023-09-27 06:16:38.938 
Epoch 77/1000 
	 loss: 16.8590, MinusLogProbMetric: 16.8590, val_loss: 17.5746, val_MinusLogProbMetric: 17.5746

Epoch 77: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.8590 - MinusLogProbMetric: 16.8590 - val_loss: 17.5746 - val_MinusLogProbMetric: 17.5746 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 78/1000
2023-09-27 06:17:21.062 
Epoch 78/1000 
	 loss: 16.9200, MinusLogProbMetric: 16.9200, val_loss: 17.4524, val_MinusLogProbMetric: 17.4524

Epoch 78: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.9200 - MinusLogProbMetric: 16.9200 - val_loss: 17.4524 - val_MinusLogProbMetric: 17.4524 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 79/1000
2023-09-27 06:18:02.628 
Epoch 79/1000 
	 loss: 16.7913, MinusLogProbMetric: 16.7913, val_loss: 17.4317, val_MinusLogProbMetric: 17.4317

Epoch 79: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.7913 - MinusLogProbMetric: 16.7913 - val_loss: 17.4317 - val_MinusLogProbMetric: 17.4317 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 80/1000
2023-09-27 06:18:44.152 
Epoch 80/1000 
	 loss: 16.8279, MinusLogProbMetric: 16.8279, val_loss: 17.3957, val_MinusLogProbMetric: 17.3957

Epoch 80: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.8279 - MinusLogProbMetric: 16.8279 - val_loss: 17.3957 - val_MinusLogProbMetric: 17.3957 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 81/1000
2023-09-27 06:19:25.712 
Epoch 81/1000 
	 loss: 16.8131, MinusLogProbMetric: 16.8131, val_loss: 17.7370, val_MinusLogProbMetric: 17.7370

Epoch 81: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.8131 - MinusLogProbMetric: 16.8131 - val_loss: 17.7370 - val_MinusLogProbMetric: 17.7370 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 82/1000
2023-09-27 06:20:07.065 
Epoch 82/1000 
	 loss: 16.8078, MinusLogProbMetric: 16.8078, val_loss: 17.4856, val_MinusLogProbMetric: 17.4856

Epoch 82: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.8078 - MinusLogProbMetric: 16.8078 - val_loss: 17.4856 - val_MinusLogProbMetric: 17.4856 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 83/1000
2023-09-27 06:20:48.631 
Epoch 83/1000 
	 loss: 16.7277, MinusLogProbMetric: 16.7277, val_loss: 17.6237, val_MinusLogProbMetric: 17.6237

Epoch 83: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.7277 - MinusLogProbMetric: 16.7277 - val_loss: 17.6237 - val_MinusLogProbMetric: 17.6237 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 84/1000
2023-09-27 06:21:30.052 
Epoch 84/1000 
	 loss: 16.7347, MinusLogProbMetric: 16.7347, val_loss: 17.6853, val_MinusLogProbMetric: 17.6853

Epoch 84: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.7347 - MinusLogProbMetric: 16.7347 - val_loss: 17.6853 - val_MinusLogProbMetric: 17.6853 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 85/1000
2023-09-27 06:22:11.596 
Epoch 85/1000 
	 loss: 16.7817, MinusLogProbMetric: 16.7817, val_loss: 17.4579, val_MinusLogProbMetric: 17.4579

Epoch 85: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.7817 - MinusLogProbMetric: 16.7817 - val_loss: 17.4579 - val_MinusLogProbMetric: 17.4579 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 86/1000
2023-09-27 06:22:53.216 
Epoch 86/1000 
	 loss: 16.7501, MinusLogProbMetric: 16.7501, val_loss: 17.4008, val_MinusLogProbMetric: 17.4008

Epoch 86: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.7501 - MinusLogProbMetric: 16.7501 - val_loss: 17.4008 - val_MinusLogProbMetric: 17.4008 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 87/1000
2023-09-27 06:23:34.686 
Epoch 87/1000 
	 loss: 16.7392, MinusLogProbMetric: 16.7392, val_loss: 17.3828, val_MinusLogProbMetric: 17.3828

Epoch 87: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.7392 - MinusLogProbMetric: 16.7392 - val_loss: 17.3828 - val_MinusLogProbMetric: 17.3828 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 88/1000
2023-09-27 06:24:16.250 
Epoch 88/1000 
	 loss: 16.6958, MinusLogProbMetric: 16.6958, val_loss: 17.3879, val_MinusLogProbMetric: 17.3879

Epoch 88: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6958 - MinusLogProbMetric: 16.6958 - val_loss: 17.3879 - val_MinusLogProbMetric: 17.3879 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 89/1000
2023-09-27 06:24:57.700 
Epoch 89/1000 
	 loss: 16.7074, MinusLogProbMetric: 16.7074, val_loss: 17.8095, val_MinusLogProbMetric: 17.8095

Epoch 89: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.7074 - MinusLogProbMetric: 16.7074 - val_loss: 17.8095 - val_MinusLogProbMetric: 17.8095 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 90/1000
2023-09-27 06:25:39.289 
Epoch 90/1000 
	 loss: 16.7715, MinusLogProbMetric: 16.7715, val_loss: 17.5562, val_MinusLogProbMetric: 17.5562

Epoch 90: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.7715 - MinusLogProbMetric: 16.7715 - val_loss: 17.5562 - val_MinusLogProbMetric: 17.5562 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 91/1000
2023-09-27 06:26:20.716 
Epoch 91/1000 
	 loss: 16.6911, MinusLogProbMetric: 16.6911, val_loss: 17.5190, val_MinusLogProbMetric: 17.5190

Epoch 91: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.6911 - MinusLogProbMetric: 16.6911 - val_loss: 17.5190 - val_MinusLogProbMetric: 17.5190 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 92/1000
2023-09-27 06:27:02.244 
Epoch 92/1000 
	 loss: 16.6824, MinusLogProbMetric: 16.6824, val_loss: 17.9439, val_MinusLogProbMetric: 17.9439

Epoch 92: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6824 - MinusLogProbMetric: 16.6824 - val_loss: 17.9439 - val_MinusLogProbMetric: 17.9439 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 93/1000
2023-09-27 06:27:43.768 
Epoch 93/1000 
	 loss: 16.6952, MinusLogProbMetric: 16.6952, val_loss: 17.4081, val_MinusLogProbMetric: 17.4081

Epoch 93: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6952 - MinusLogProbMetric: 16.6952 - val_loss: 17.4081 - val_MinusLogProbMetric: 17.4081 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 94/1000
2023-09-27 06:28:25.475 
Epoch 94/1000 
	 loss: 16.6915, MinusLogProbMetric: 16.6915, val_loss: 17.4868, val_MinusLogProbMetric: 17.4868

Epoch 94: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6915 - MinusLogProbMetric: 16.6915 - val_loss: 17.4868 - val_MinusLogProbMetric: 17.4868 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 95/1000
2023-09-27 06:29:07.097 
Epoch 95/1000 
	 loss: 16.6212, MinusLogProbMetric: 16.6212, val_loss: 17.6306, val_MinusLogProbMetric: 17.6306

Epoch 95: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6212 - MinusLogProbMetric: 16.6212 - val_loss: 17.6306 - val_MinusLogProbMetric: 17.6306 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 96/1000
2023-09-27 06:29:48.717 
Epoch 96/1000 
	 loss: 16.6350, MinusLogProbMetric: 16.6350, val_loss: 17.4884, val_MinusLogProbMetric: 17.4884

Epoch 96: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6350 - MinusLogProbMetric: 16.6350 - val_loss: 17.4884 - val_MinusLogProbMetric: 17.4884 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 97/1000
2023-09-27 06:30:30.235 
Epoch 97/1000 
	 loss: 16.6336, MinusLogProbMetric: 16.6336, val_loss: 17.5173, val_MinusLogProbMetric: 17.5173

Epoch 97: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6336 - MinusLogProbMetric: 16.6336 - val_loss: 17.5173 - val_MinusLogProbMetric: 17.5173 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 98/1000
2023-09-27 06:31:11.760 
Epoch 98/1000 
	 loss: 16.6160, MinusLogProbMetric: 16.6160, val_loss: 17.5827, val_MinusLogProbMetric: 17.5827

Epoch 98: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6160 - MinusLogProbMetric: 16.6160 - val_loss: 17.5827 - val_MinusLogProbMetric: 17.5827 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 99/1000
2023-09-27 06:31:53.116 
Epoch 99/1000 
	 loss: 16.6017, MinusLogProbMetric: 16.6017, val_loss: 17.6244, val_MinusLogProbMetric: 17.6244

Epoch 99: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.6017 - MinusLogProbMetric: 16.6017 - val_loss: 17.6244 - val_MinusLogProbMetric: 17.6244 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 100/1000
2023-09-27 06:32:34.520 
Epoch 100/1000 
	 loss: 16.6239, MinusLogProbMetric: 16.6239, val_loss: 17.8415, val_MinusLogProbMetric: 17.8415

Epoch 100: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.6239 - MinusLogProbMetric: 16.6239 - val_loss: 17.8415 - val_MinusLogProbMetric: 17.8415 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 101/1000
2023-09-27 06:33:16.256 
Epoch 101/1000 
	 loss: 16.5844, MinusLogProbMetric: 16.5844, val_loss: 17.7781, val_MinusLogProbMetric: 17.7781

Epoch 101: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.5844 - MinusLogProbMetric: 16.5844 - val_loss: 17.7781 - val_MinusLogProbMetric: 17.7781 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 102/1000
2023-09-27 06:33:57.824 
Epoch 102/1000 
	 loss: 16.6355, MinusLogProbMetric: 16.6355, val_loss: 17.3755, val_MinusLogProbMetric: 17.3755

Epoch 102: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6355 - MinusLogProbMetric: 16.6355 - val_loss: 17.3755 - val_MinusLogProbMetric: 17.3755 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 103/1000
2023-09-27 06:34:39.321 
Epoch 103/1000 
	 loss: 16.5648, MinusLogProbMetric: 16.5648, val_loss: 17.7190, val_MinusLogProbMetric: 17.7190

Epoch 103: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.5648 - MinusLogProbMetric: 16.5648 - val_loss: 17.7190 - val_MinusLogProbMetric: 17.7190 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 104/1000
2023-09-27 06:35:20.968 
Epoch 104/1000 
	 loss: 16.6072, MinusLogProbMetric: 16.6072, val_loss: 17.4727, val_MinusLogProbMetric: 17.4727

Epoch 104: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.6072 - MinusLogProbMetric: 16.6072 - val_loss: 17.4727 - val_MinusLogProbMetric: 17.4727 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 105/1000
2023-09-27 06:36:02.340 
Epoch 105/1000 
	 loss: 16.5771, MinusLogProbMetric: 16.5771, val_loss: 17.6530, val_MinusLogProbMetric: 17.6530

Epoch 105: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.5771 - MinusLogProbMetric: 16.5771 - val_loss: 17.6530 - val_MinusLogProbMetric: 17.6530 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 106/1000
2023-09-27 06:36:43.954 
Epoch 106/1000 
	 loss: 16.5728, MinusLogProbMetric: 16.5728, val_loss: 17.5571, val_MinusLogProbMetric: 17.5571

Epoch 106: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.5728 - MinusLogProbMetric: 16.5728 - val_loss: 17.5571 - val_MinusLogProbMetric: 17.5571 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 107/1000
2023-09-27 06:37:25.763 
Epoch 107/1000 
	 loss: 16.5297, MinusLogProbMetric: 16.5297, val_loss: 17.4199, val_MinusLogProbMetric: 17.4199

Epoch 107: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.5297 - MinusLogProbMetric: 16.5297 - val_loss: 17.4199 - val_MinusLogProbMetric: 17.4199 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 108/1000
2023-09-27 06:38:07.786 
Epoch 108/1000 
	 loss: 16.5416, MinusLogProbMetric: 16.5416, val_loss: 17.5980, val_MinusLogProbMetric: 17.5980

Epoch 108: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.5416 - MinusLogProbMetric: 16.5416 - val_loss: 17.5980 - val_MinusLogProbMetric: 17.5980 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 109/1000
2023-09-27 06:38:49.543 
Epoch 109/1000 
	 loss: 16.5028, MinusLogProbMetric: 16.5028, val_loss: 17.4462, val_MinusLogProbMetric: 17.4462

Epoch 109: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.5028 - MinusLogProbMetric: 16.5028 - val_loss: 17.4462 - val_MinusLogProbMetric: 17.4462 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 110/1000
2023-09-27 06:39:31.198 
Epoch 110/1000 
	 loss: 16.5484, MinusLogProbMetric: 16.5484, val_loss: 17.6385, val_MinusLogProbMetric: 17.6385

Epoch 110: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.5484 - MinusLogProbMetric: 16.5484 - val_loss: 17.6385 - val_MinusLogProbMetric: 17.6385 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 111/1000
2023-09-27 06:40:12.558 
Epoch 111/1000 
	 loss: 16.5142, MinusLogProbMetric: 16.5142, val_loss: 17.4337, val_MinusLogProbMetric: 17.4337

Epoch 111: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.5142 - MinusLogProbMetric: 16.5142 - val_loss: 17.4337 - val_MinusLogProbMetric: 17.4337 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 112/1000
2023-09-27 06:40:54.175 
Epoch 112/1000 
	 loss: 16.4843, MinusLogProbMetric: 16.4843, val_loss: 17.5434, val_MinusLogProbMetric: 17.5434

Epoch 112: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.4843 - MinusLogProbMetric: 16.4843 - val_loss: 17.5434 - val_MinusLogProbMetric: 17.5434 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 113/1000
2023-09-27 06:41:35.913 
Epoch 113/1000 
	 loss: 16.5066, MinusLogProbMetric: 16.5066, val_loss: 17.5071, val_MinusLogProbMetric: 17.5071

Epoch 113: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.5066 - MinusLogProbMetric: 16.5066 - val_loss: 17.5071 - val_MinusLogProbMetric: 17.5071 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 114/1000
2023-09-27 06:42:17.505 
Epoch 114/1000 
	 loss: 16.4980, MinusLogProbMetric: 16.4980, val_loss: 17.7098, val_MinusLogProbMetric: 17.7098

Epoch 114: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.4980 - MinusLogProbMetric: 16.4980 - val_loss: 17.7098 - val_MinusLogProbMetric: 17.7098 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 115/1000
2023-09-27 06:42:58.969 
Epoch 115/1000 
	 loss: 16.2084, MinusLogProbMetric: 16.2084, val_loss: 17.5010, val_MinusLogProbMetric: 17.5010

Epoch 115: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.2084 - MinusLogProbMetric: 16.2084 - val_loss: 17.5010 - val_MinusLogProbMetric: 17.5010 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 116/1000
2023-09-27 06:43:40.391 
Epoch 116/1000 
	 loss: 16.1661, MinusLogProbMetric: 16.1661, val_loss: 17.4282, val_MinusLogProbMetric: 17.4282

Epoch 116: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1661 - MinusLogProbMetric: 16.1661 - val_loss: 17.4282 - val_MinusLogProbMetric: 17.4282 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 117/1000
2023-09-27 06:44:22.037 
Epoch 117/1000 
	 loss: 16.1592, MinusLogProbMetric: 16.1592, val_loss: 17.3669, val_MinusLogProbMetric: 17.3669

Epoch 117: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1592 - MinusLogProbMetric: 16.1592 - val_loss: 17.3669 - val_MinusLogProbMetric: 17.3669 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 118/1000
2023-09-27 06:45:03.200 
Epoch 118/1000 
	 loss: 16.1768, MinusLogProbMetric: 16.1768, val_loss: 17.4150, val_MinusLogProbMetric: 17.4150

Epoch 118: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1768 - MinusLogProbMetric: 16.1768 - val_loss: 17.4150 - val_MinusLogProbMetric: 17.4150 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 119/1000
2023-09-27 06:45:44.710 
Epoch 119/1000 
	 loss: 16.1553, MinusLogProbMetric: 16.1553, val_loss: 17.3721, val_MinusLogProbMetric: 17.3721

Epoch 119: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1553 - MinusLogProbMetric: 16.1553 - val_loss: 17.3721 - val_MinusLogProbMetric: 17.3721 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 120/1000
2023-09-27 06:46:26.409 
Epoch 120/1000 
	 loss: 16.1418, MinusLogProbMetric: 16.1418, val_loss: 17.4122, val_MinusLogProbMetric: 17.4122

Epoch 120: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1418 - MinusLogProbMetric: 16.1418 - val_loss: 17.4122 - val_MinusLogProbMetric: 17.4122 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 121/1000
2023-09-27 06:47:07.791 
Epoch 121/1000 
	 loss: 16.1442, MinusLogProbMetric: 16.1442, val_loss: 17.5120, val_MinusLogProbMetric: 17.5120

Epoch 121: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1442 - MinusLogProbMetric: 16.1442 - val_loss: 17.5120 - val_MinusLogProbMetric: 17.5120 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 122/1000
2023-09-27 06:47:49.390 
Epoch 122/1000 
	 loss: 16.1505, MinusLogProbMetric: 16.1505, val_loss: 17.4684, val_MinusLogProbMetric: 17.4684

Epoch 122: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1505 - MinusLogProbMetric: 16.1505 - val_loss: 17.4684 - val_MinusLogProbMetric: 17.4684 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 123/1000
2023-09-27 06:48:31.157 
Epoch 123/1000 
	 loss: 16.1449, MinusLogProbMetric: 16.1449, val_loss: 17.4229, val_MinusLogProbMetric: 17.4229

Epoch 123: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1449 - MinusLogProbMetric: 16.1449 - val_loss: 17.4229 - val_MinusLogProbMetric: 17.4229 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 124/1000
2023-09-27 06:49:12.508 
Epoch 124/1000 
	 loss: 16.1323, MinusLogProbMetric: 16.1323, val_loss: 17.5459, val_MinusLogProbMetric: 17.5459

Epoch 124: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1323 - MinusLogProbMetric: 16.1323 - val_loss: 17.5459 - val_MinusLogProbMetric: 17.5459 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 125/1000
2023-09-27 06:49:53.894 
Epoch 125/1000 
	 loss: 16.1593, MinusLogProbMetric: 16.1593, val_loss: 17.4660, val_MinusLogProbMetric: 17.4660

Epoch 125: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1593 - MinusLogProbMetric: 16.1593 - val_loss: 17.4660 - val_MinusLogProbMetric: 17.4660 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 126/1000
2023-09-27 06:50:35.514 
Epoch 126/1000 
	 loss: 16.1156, MinusLogProbMetric: 16.1156, val_loss: 17.3892, val_MinusLogProbMetric: 17.3892

Epoch 126: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1156 - MinusLogProbMetric: 16.1156 - val_loss: 17.3892 - val_MinusLogProbMetric: 17.3892 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 127/1000
2023-09-27 06:51:16.401 
Epoch 127/1000 
	 loss: 16.1243, MinusLogProbMetric: 16.1243, val_loss: 17.4503, val_MinusLogProbMetric: 17.4503

Epoch 127: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1243 - MinusLogProbMetric: 16.1243 - val_loss: 17.4503 - val_MinusLogProbMetric: 17.4503 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 128/1000
2023-09-27 06:51:57.762 
Epoch 128/1000 
	 loss: 16.1105, MinusLogProbMetric: 16.1105, val_loss: 17.5377, val_MinusLogProbMetric: 17.5377

Epoch 128: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1105 - MinusLogProbMetric: 16.1105 - val_loss: 17.5377 - val_MinusLogProbMetric: 17.5377 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 129/1000
2023-09-27 06:52:39.355 
Epoch 129/1000 
	 loss: 16.1176, MinusLogProbMetric: 16.1176, val_loss: 17.4487, val_MinusLogProbMetric: 17.4487

Epoch 129: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1176 - MinusLogProbMetric: 16.1176 - val_loss: 17.4487 - val_MinusLogProbMetric: 17.4487 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 130/1000
2023-09-27 06:53:20.911 
Epoch 130/1000 
	 loss: 16.1135, MinusLogProbMetric: 16.1135, val_loss: 17.5106, val_MinusLogProbMetric: 17.5106

Epoch 130: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.1135 - MinusLogProbMetric: 16.1135 - val_loss: 17.5106 - val_MinusLogProbMetric: 17.5106 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 131/1000
2023-09-27 06:54:02.757 
Epoch 131/1000 
	 loss: 16.0966, MinusLogProbMetric: 16.0966, val_loss: 17.4430, val_MinusLogProbMetric: 17.4430

Epoch 131: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0966 - MinusLogProbMetric: 16.0966 - val_loss: 17.4430 - val_MinusLogProbMetric: 17.4430 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 132/1000
2023-09-27 06:54:44.088 
Epoch 132/1000 
	 loss: 16.1109, MinusLogProbMetric: 16.1109, val_loss: 17.4819, val_MinusLogProbMetric: 17.4819

Epoch 132: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1109 - MinusLogProbMetric: 16.1109 - val_loss: 17.4819 - val_MinusLogProbMetric: 17.4819 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 133/1000
2023-09-27 06:55:25.558 
Epoch 133/1000 
	 loss: 16.0924, MinusLogProbMetric: 16.0924, val_loss: 17.4314, val_MinusLogProbMetric: 17.4314

Epoch 133: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.0924 - MinusLogProbMetric: 16.0924 - val_loss: 17.4314 - val_MinusLogProbMetric: 17.4314 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 134/1000
2023-09-27 06:56:07.044 
Epoch 134/1000 
	 loss: 16.0897, MinusLogProbMetric: 16.0897, val_loss: 17.5290, val_MinusLogProbMetric: 17.5290

Epoch 134: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.0897 - MinusLogProbMetric: 16.0897 - val_loss: 17.5290 - val_MinusLogProbMetric: 17.5290 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 135/1000
2023-09-27 06:56:48.595 
Epoch 135/1000 
	 loss: 16.0991, MinusLogProbMetric: 16.0991, val_loss: 17.6031, val_MinusLogProbMetric: 17.6031

Epoch 135: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0991 - MinusLogProbMetric: 16.0991 - val_loss: 17.6031 - val_MinusLogProbMetric: 17.6031 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 136/1000
2023-09-27 06:57:30.560 
Epoch 136/1000 
	 loss: 16.0901, MinusLogProbMetric: 16.0901, val_loss: 17.4788, val_MinusLogProbMetric: 17.4788

Epoch 136: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0901 - MinusLogProbMetric: 16.0901 - val_loss: 17.4788 - val_MinusLogProbMetric: 17.4788 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 137/1000
2023-09-27 06:58:12.263 
Epoch 137/1000 
	 loss: 16.0662, MinusLogProbMetric: 16.0662, val_loss: 17.6221, val_MinusLogProbMetric: 17.6221

Epoch 137: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0662 - MinusLogProbMetric: 16.0662 - val_loss: 17.6221 - val_MinusLogProbMetric: 17.6221 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 138/1000
2023-09-27 06:58:53.759 
Epoch 138/1000 
	 loss: 16.1119, MinusLogProbMetric: 16.1119, val_loss: 17.7494, val_MinusLogProbMetric: 17.7494

Epoch 138: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.1119 - MinusLogProbMetric: 16.1119 - val_loss: 17.7494 - val_MinusLogProbMetric: 17.7494 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 139/1000
2023-09-27 06:59:35.647 
Epoch 139/1000 
	 loss: 16.0827, MinusLogProbMetric: 16.0827, val_loss: 17.5232, val_MinusLogProbMetric: 17.5232

Epoch 139: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0827 - MinusLogProbMetric: 16.0827 - val_loss: 17.5232 - val_MinusLogProbMetric: 17.5232 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 140/1000
2023-09-27 07:00:17.425 
Epoch 140/1000 
	 loss: 16.0624, MinusLogProbMetric: 16.0624, val_loss: 17.6055, val_MinusLogProbMetric: 17.6055

Epoch 140: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0624 - MinusLogProbMetric: 16.0624 - val_loss: 17.6055 - val_MinusLogProbMetric: 17.6055 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 141/1000
2023-09-27 07:00:59.241 
Epoch 141/1000 
	 loss: 16.0644, MinusLogProbMetric: 16.0644, val_loss: 17.4714, val_MinusLogProbMetric: 17.4714

Epoch 141: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0644 - MinusLogProbMetric: 16.0644 - val_loss: 17.4714 - val_MinusLogProbMetric: 17.4714 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 142/1000
2023-09-27 07:01:40.917 
Epoch 142/1000 
	 loss: 16.0351, MinusLogProbMetric: 16.0351, val_loss: 17.5230, val_MinusLogProbMetric: 17.5230

Epoch 142: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0351 - MinusLogProbMetric: 16.0351 - val_loss: 17.5230 - val_MinusLogProbMetric: 17.5230 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 143/1000
2023-09-27 07:02:22.723 
Epoch 143/1000 
	 loss: 16.0430, MinusLogProbMetric: 16.0430, val_loss: 17.5072, val_MinusLogProbMetric: 17.5072

Epoch 143: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0430 - MinusLogProbMetric: 16.0430 - val_loss: 17.5072 - val_MinusLogProbMetric: 17.5072 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 144/1000
2023-09-27 07:03:04.701 
Epoch 144/1000 
	 loss: 16.0451, MinusLogProbMetric: 16.0451, val_loss: 17.6269, val_MinusLogProbMetric: 17.6269

Epoch 144: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0451 - MinusLogProbMetric: 16.0451 - val_loss: 17.6269 - val_MinusLogProbMetric: 17.6269 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 145/1000
2023-09-27 07:03:46.121 
Epoch 145/1000 
	 loss: 16.0549, MinusLogProbMetric: 16.0549, val_loss: 17.5813, val_MinusLogProbMetric: 17.5813

Epoch 145: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.0549 - MinusLogProbMetric: 16.0549 - val_loss: 17.5813 - val_MinusLogProbMetric: 17.5813 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 146/1000
2023-09-27 07:04:27.799 
Epoch 146/1000 
	 loss: 16.0282, MinusLogProbMetric: 16.0282, val_loss: 17.6328, val_MinusLogProbMetric: 17.6328

Epoch 146: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0282 - MinusLogProbMetric: 16.0282 - val_loss: 17.6328 - val_MinusLogProbMetric: 17.6328 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 147/1000
2023-09-27 07:05:09.693 
Epoch 147/1000 
	 loss: 16.0390, MinusLogProbMetric: 16.0390, val_loss: 17.8228, val_MinusLogProbMetric: 17.8228

Epoch 147: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0390 - MinusLogProbMetric: 16.0390 - val_loss: 17.8228 - val_MinusLogProbMetric: 17.8228 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 148/1000
2023-09-27 07:05:51.465 
Epoch 148/1000 
	 loss: 16.0520, MinusLogProbMetric: 16.0520, val_loss: 17.5573, val_MinusLogProbMetric: 17.5573

Epoch 148: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0520 - MinusLogProbMetric: 16.0520 - val_loss: 17.5573 - val_MinusLogProbMetric: 17.5573 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 149/1000
2023-09-27 07:06:32.870 
Epoch 149/1000 
	 loss: 16.0129, MinusLogProbMetric: 16.0129, val_loss: 17.5862, val_MinusLogProbMetric: 17.5862

Epoch 149: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.0129 - MinusLogProbMetric: 16.0129 - val_loss: 17.5862 - val_MinusLogProbMetric: 17.5862 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 150/1000
2023-09-27 07:07:14.114 
Epoch 150/1000 
	 loss: 16.0049, MinusLogProbMetric: 16.0049, val_loss: 17.5479, val_MinusLogProbMetric: 17.5479

Epoch 150: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.0049 - MinusLogProbMetric: 16.0049 - val_loss: 17.5479 - val_MinusLogProbMetric: 17.5479 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 151/1000
2023-09-27 07:07:55.479 
Epoch 151/1000 
	 loss: 16.0163, MinusLogProbMetric: 16.0163, val_loss: 17.6490, val_MinusLogProbMetric: 17.6490

Epoch 151: val_loss did not improve from 17.30657
196/196 - 41s - loss: 16.0163 - MinusLogProbMetric: 16.0163 - val_loss: 17.6490 - val_MinusLogProbMetric: 17.6490 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 152/1000
2023-09-27 07:08:37.066 
Epoch 152/1000 
	 loss: 16.0105, MinusLogProbMetric: 16.0105, val_loss: 17.6119, val_MinusLogProbMetric: 17.6119

Epoch 152: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0105 - MinusLogProbMetric: 16.0105 - val_loss: 17.6119 - val_MinusLogProbMetric: 17.6119 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 153/1000
2023-09-27 07:09:18.601 
Epoch 153/1000 
	 loss: 15.9969, MinusLogProbMetric: 15.9969, val_loss: 17.7611, val_MinusLogProbMetric: 17.7611

Epoch 153: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9969 - MinusLogProbMetric: 15.9969 - val_loss: 17.7611 - val_MinusLogProbMetric: 17.7611 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 154/1000
2023-09-27 07:10:00.481 
Epoch 154/1000 
	 loss: 16.0082, MinusLogProbMetric: 16.0082, val_loss: 17.6628, val_MinusLogProbMetric: 17.6628

Epoch 154: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0082 - MinusLogProbMetric: 16.0082 - val_loss: 17.6628 - val_MinusLogProbMetric: 17.6628 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 155/1000
2023-09-27 07:10:42.219 
Epoch 155/1000 
	 loss: 16.0079, MinusLogProbMetric: 16.0079, val_loss: 17.6508, val_MinusLogProbMetric: 17.6508

Epoch 155: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0079 - MinusLogProbMetric: 16.0079 - val_loss: 17.6508 - val_MinusLogProbMetric: 17.6508 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 156/1000
2023-09-27 07:11:23.836 
Epoch 156/1000 
	 loss: 15.9940, MinusLogProbMetric: 15.9940, val_loss: 17.5867, val_MinusLogProbMetric: 17.5867

Epoch 156: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9940 - MinusLogProbMetric: 15.9940 - val_loss: 17.5867 - val_MinusLogProbMetric: 17.5867 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 157/1000
2023-09-27 07:12:05.598 
Epoch 157/1000 
	 loss: 15.9948, MinusLogProbMetric: 15.9948, val_loss: 17.5771, val_MinusLogProbMetric: 17.5771

Epoch 157: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9948 - MinusLogProbMetric: 15.9948 - val_loss: 17.5771 - val_MinusLogProbMetric: 17.5771 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 158/1000
2023-09-27 07:12:47.616 
Epoch 158/1000 
	 loss: 16.0004, MinusLogProbMetric: 16.0004, val_loss: 17.6776, val_MinusLogProbMetric: 17.6776

Epoch 158: val_loss did not improve from 17.30657
196/196 - 42s - loss: 16.0004 - MinusLogProbMetric: 16.0004 - val_loss: 17.6776 - val_MinusLogProbMetric: 17.6776 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 159/1000
2023-09-27 07:13:29.336 
Epoch 159/1000 
	 loss: 15.9893, MinusLogProbMetric: 15.9893, val_loss: 17.6490, val_MinusLogProbMetric: 17.6490

Epoch 159: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9893 - MinusLogProbMetric: 15.9893 - val_loss: 17.6490 - val_MinusLogProbMetric: 17.6490 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 160/1000
2023-09-27 07:14:11.357 
Epoch 160/1000 
	 loss: 15.9930, MinusLogProbMetric: 15.9930, val_loss: 17.7252, val_MinusLogProbMetric: 17.7252

Epoch 160: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9930 - MinusLogProbMetric: 15.9930 - val_loss: 17.7252 - val_MinusLogProbMetric: 17.7252 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 161/1000
2023-09-27 07:14:52.939 
Epoch 161/1000 
	 loss: 15.9609, MinusLogProbMetric: 15.9609, val_loss: 17.6941, val_MinusLogProbMetric: 17.6941

Epoch 161: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9609 - MinusLogProbMetric: 15.9609 - val_loss: 17.6941 - val_MinusLogProbMetric: 17.6941 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 162/1000
2023-09-27 07:15:34.672 
Epoch 162/1000 
	 loss: 15.9795, MinusLogProbMetric: 15.9795, val_loss: 17.5788, val_MinusLogProbMetric: 17.5788

Epoch 162: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9795 - MinusLogProbMetric: 15.9795 - val_loss: 17.5788 - val_MinusLogProbMetric: 17.5788 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 163/1000
2023-09-27 07:16:16.271 
Epoch 163/1000 
	 loss: 15.9715, MinusLogProbMetric: 15.9715, val_loss: 17.6787, val_MinusLogProbMetric: 17.6787

Epoch 163: val_loss did not improve from 17.30657
196/196 - 42s - loss: 15.9715 - MinusLogProbMetric: 15.9715 - val_loss: 17.6787 - val_MinusLogProbMetric: 17.6787 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 164/1000
2023-09-27 07:16:57.944 
Epoch 164/1000 
	 loss: 15.9391, MinusLogProbMetric: 15.9391, val_loss: 17.6650, val_MinusLogProbMetric: 17.6650

Epoch 164: val_loss did not improve from 17.30657
Restoring model weights from the end of the best epoch: 64.
196/196 - 42s - loss: 15.9391 - MinusLogProbMetric: 15.9391 - val_loss: 17.6650 - val_MinusLogProbMetric: 17.6650 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 164: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 15.223335562972352 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 9.31063974899007 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 7.025733968010172 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 7.970367901027203 seconds.
Training succeeded with seed 869.
Model trained in 6876.67 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 40.79 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 41.06 s.
===========
Run 300/720 done in 6923.45 s.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

===========
Generating train data for run 302.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_302/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_302/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_302/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_302
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  2305120   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7fc229509510>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc10066bb50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc10066bb50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc10045ae90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1004a5de0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1003166b0>, <keras.callbacks.ModelCheckpoint object at 0x7fc1003175b0>, <keras.callbacks.EarlyStopping object at 0x7fc1003163b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1003160b0>, <keras.callbacks.TerminateOnNaN object at 0x7fc1004ddf30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_302/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 302/720 with hyperparameters:
timestamp = 2023-09-27 07:17:51.719756
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 54: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 07:20:13.490 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1261.0519, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 1261.0519 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 142s/epoch - 723ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 302.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_302/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_302/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_302/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_302
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  2305120   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7fc3247a2590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc324785ed0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc324785ed0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc917f12950>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc274230cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc274231240>, <keras.callbacks.ModelCheckpoint object at 0x7fc274231300>, <keras.callbacks.EarlyStopping object at 0x7fc274231570>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc2742315a0>, <keras.callbacks.TerminateOnNaN object at 0x7fc2742311e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_302/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 302/720 with hyperparameters:
timestamp = 2023-09-27 07:20:23.574793
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
2023-09-27 07:23:35.074 
Epoch 1/1000 
	 loss: 179.3762, MinusLogProbMetric: 179.3762, val_loss: 52.4163, val_MinusLogProbMetric: 52.4163

Epoch 1: val_loss improved from inf to 52.41628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 192s - loss: 179.3762 - MinusLogProbMetric: 179.3762 - val_loss: 52.4163 - val_MinusLogProbMetric: 52.4163 - lr: 3.3333e-04 - 192s/epoch - 981ms/step
Epoch 2/1000
2023-09-27 07:24:42.044 
Epoch 2/1000 
	 loss: 41.7859, MinusLogProbMetric: 41.7859, val_loss: 36.3499, val_MinusLogProbMetric: 36.3499

Epoch 2: val_loss improved from 52.41628 to 36.34992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 67s - loss: 41.7859 - MinusLogProbMetric: 41.7859 - val_loss: 36.3499 - val_MinusLogProbMetric: 36.3499 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 3/1000
2023-09-27 07:25:48.331 
Epoch 3/1000 
	 loss: 32.7983, MinusLogProbMetric: 32.7983, val_loss: 31.0101, val_MinusLogProbMetric: 31.0101

Epoch 3: val_loss improved from 36.34992 to 31.01011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 67s - loss: 32.7983 - MinusLogProbMetric: 32.7983 - val_loss: 31.0101 - val_MinusLogProbMetric: 31.0101 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 4/1000
2023-09-27 07:26:54.936 
Epoch 4/1000 
	 loss: 28.6026, MinusLogProbMetric: 28.6026, val_loss: 26.9859, val_MinusLogProbMetric: 26.9859

Epoch 4: val_loss improved from 31.01011 to 26.98594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 67s - loss: 28.6026 - MinusLogProbMetric: 28.6026 - val_loss: 26.9859 - val_MinusLogProbMetric: 26.9859 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 5/1000
2023-09-27 07:28:01.382 
Epoch 5/1000 
	 loss: 26.6029, MinusLogProbMetric: 26.6029, val_loss: 28.5337, val_MinusLogProbMetric: 28.5337

Epoch 5: val_loss did not improve from 26.98594
196/196 - 65s - loss: 26.6029 - MinusLogProbMetric: 26.6029 - val_loss: 28.5337 - val_MinusLogProbMetric: 28.5337 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 6/1000
2023-09-27 07:29:06.673 
Epoch 6/1000 
	 loss: 25.6160, MinusLogProbMetric: 25.6160, val_loss: 26.4180, val_MinusLogProbMetric: 26.4180

Epoch 6: val_loss improved from 26.98594 to 26.41799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 25.6160 - MinusLogProbMetric: 25.6160 - val_loss: 26.4180 - val_MinusLogProbMetric: 26.4180 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 7/1000
2023-09-27 07:30:12.805 
Epoch 7/1000 
	 loss: 24.5882, MinusLogProbMetric: 24.5882, val_loss: 24.3813, val_MinusLogProbMetric: 24.3813

Epoch 7: val_loss improved from 26.41799 to 24.38130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 24.5882 - MinusLogProbMetric: 24.5882 - val_loss: 24.3813 - val_MinusLogProbMetric: 24.3813 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 8/1000
2023-09-27 07:31:19.177 
Epoch 8/1000 
	 loss: 23.4788, MinusLogProbMetric: 23.4788, val_loss: 23.3916, val_MinusLogProbMetric: 23.3916

Epoch 8: val_loss improved from 24.38130 to 23.39161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 23.4788 - MinusLogProbMetric: 23.4788 - val_loss: 23.3916 - val_MinusLogProbMetric: 23.3916 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 9/1000
2023-09-27 07:32:25.385 
Epoch 9/1000 
	 loss: 23.2316, MinusLogProbMetric: 23.2316, val_loss: 23.8104, val_MinusLogProbMetric: 23.8104

Epoch 9: val_loss did not improve from 23.39161
196/196 - 65s - loss: 23.2316 - MinusLogProbMetric: 23.2316 - val_loss: 23.8104 - val_MinusLogProbMetric: 23.8104 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 10/1000
2023-09-27 07:33:30.521 
Epoch 10/1000 
	 loss: 22.9594, MinusLogProbMetric: 22.9594, val_loss: 22.7491, val_MinusLogProbMetric: 22.7491

Epoch 10: val_loss improved from 23.39161 to 22.74914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 22.9594 - MinusLogProbMetric: 22.9594 - val_loss: 22.7491 - val_MinusLogProbMetric: 22.7491 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 11/1000
2023-09-27 07:34:36.530 
Epoch 11/1000 
	 loss: 22.4331, MinusLogProbMetric: 22.4331, val_loss: 22.7094, val_MinusLogProbMetric: 22.7094

Epoch 11: val_loss improved from 22.74914 to 22.70944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 22.4331 - MinusLogProbMetric: 22.4331 - val_loss: 22.7094 - val_MinusLogProbMetric: 22.7094 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 12/1000
2023-09-27 07:35:42.577 
Epoch 12/1000 
	 loss: 22.1056, MinusLogProbMetric: 22.1056, val_loss: 22.6871, val_MinusLogProbMetric: 22.6871

Epoch 12: val_loss improved from 22.70944 to 22.68710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 22.1056 - MinusLogProbMetric: 22.1056 - val_loss: 22.6871 - val_MinusLogProbMetric: 22.6871 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 13/1000
2023-09-27 07:36:48.732 
Epoch 13/1000 
	 loss: 21.7004, MinusLogProbMetric: 21.7004, val_loss: 21.6432, val_MinusLogProbMetric: 21.6432

Epoch 13: val_loss improved from 22.68710 to 21.64316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 21.7004 - MinusLogProbMetric: 21.7004 - val_loss: 21.6432 - val_MinusLogProbMetric: 21.6432 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 14/1000
2023-09-27 07:37:55.260 
Epoch 14/1000 
	 loss: 21.5747, MinusLogProbMetric: 21.5747, val_loss: 22.8587, val_MinusLogProbMetric: 22.8587

Epoch 14: val_loss did not improve from 21.64316
196/196 - 65s - loss: 21.5747 - MinusLogProbMetric: 21.5747 - val_loss: 22.8587 - val_MinusLogProbMetric: 22.8587 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 15/1000
2023-09-27 07:39:00.477 
Epoch 15/1000 
	 loss: 21.5024, MinusLogProbMetric: 21.5024, val_loss: 21.5952, val_MinusLogProbMetric: 21.5952

Epoch 15: val_loss improved from 21.64316 to 21.59517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 21.5024 - MinusLogProbMetric: 21.5024 - val_loss: 21.5952 - val_MinusLogProbMetric: 21.5952 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 16/1000
2023-09-27 07:40:06.380 
Epoch 16/1000 
	 loss: 21.2047, MinusLogProbMetric: 21.2047, val_loss: 21.1332, val_MinusLogProbMetric: 21.1332

Epoch 16: val_loss improved from 21.59517 to 21.13317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 21.2047 - MinusLogProbMetric: 21.2047 - val_loss: 21.1332 - val_MinusLogProbMetric: 21.1332 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 17/1000
2023-09-27 07:41:12.795 
Epoch 17/1000 
	 loss: 21.0864, MinusLogProbMetric: 21.0864, val_loss: 22.1292, val_MinusLogProbMetric: 22.1292

Epoch 17: val_loss did not improve from 21.13317
196/196 - 66s - loss: 21.0864 - MinusLogProbMetric: 21.0864 - val_loss: 22.1292 - val_MinusLogProbMetric: 22.1292 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 18/1000
2023-09-27 07:42:18.044 
Epoch 18/1000 
	 loss: 20.9041, MinusLogProbMetric: 20.9041, val_loss: 21.2302, val_MinusLogProbMetric: 21.2302

Epoch 18: val_loss did not improve from 21.13317
196/196 - 65s - loss: 20.9041 - MinusLogProbMetric: 20.9041 - val_loss: 21.2302 - val_MinusLogProbMetric: 21.2302 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 19/1000
2023-09-27 07:43:23.078 
Epoch 19/1000 
	 loss: 20.7883, MinusLogProbMetric: 20.7883, val_loss: 21.1242, val_MinusLogProbMetric: 21.1242

Epoch 19: val_loss improved from 21.13317 to 21.12419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 20.7883 - MinusLogProbMetric: 20.7883 - val_loss: 21.1242 - val_MinusLogProbMetric: 21.1242 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 20/1000
2023-09-27 07:44:28.976 
Epoch 20/1000 
	 loss: 20.6793, MinusLogProbMetric: 20.6793, val_loss: 20.6471, val_MinusLogProbMetric: 20.6471

Epoch 20: val_loss improved from 21.12419 to 20.64706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 20.6793 - MinusLogProbMetric: 20.6793 - val_loss: 20.6471 - val_MinusLogProbMetric: 20.6471 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 21/1000
2023-09-27 07:45:34.660 
Epoch 21/1000 
	 loss: 20.4861, MinusLogProbMetric: 20.4861, val_loss: 19.8052, val_MinusLogProbMetric: 19.8052

Epoch 21: val_loss improved from 20.64706 to 19.80523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 20.4861 - MinusLogProbMetric: 20.4861 - val_loss: 19.8052 - val_MinusLogProbMetric: 19.8052 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 22/1000
2023-09-27 07:46:41.454 
Epoch 22/1000 
	 loss: 20.3430, MinusLogProbMetric: 20.3430, val_loss: 20.1006, val_MinusLogProbMetric: 20.1006

Epoch 22: val_loss did not improve from 19.80523
196/196 - 66s - loss: 20.3430 - MinusLogProbMetric: 20.3430 - val_loss: 20.1006 - val_MinusLogProbMetric: 20.1006 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 23/1000
2023-09-27 07:47:46.856 
Epoch 23/1000 
	 loss: 20.2729, MinusLogProbMetric: 20.2729, val_loss: 19.5933, val_MinusLogProbMetric: 19.5933

Epoch 23: val_loss improved from 19.80523 to 19.59327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 20.2729 - MinusLogProbMetric: 20.2729 - val_loss: 19.5933 - val_MinusLogProbMetric: 19.5933 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 24/1000
2023-09-27 07:48:53.509 
Epoch 24/1000 
	 loss: 20.6233, MinusLogProbMetric: 20.6233, val_loss: 20.4940, val_MinusLogProbMetric: 20.4940

Epoch 24: val_loss did not improve from 19.59327
196/196 - 66s - loss: 20.6233 - MinusLogProbMetric: 20.6233 - val_loss: 20.4940 - val_MinusLogProbMetric: 20.4940 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 25/1000
2023-09-27 07:49:59.382 
Epoch 25/1000 
	 loss: 20.1939, MinusLogProbMetric: 20.1939, val_loss: 20.2330, val_MinusLogProbMetric: 20.2330

Epoch 25: val_loss did not improve from 19.59327
196/196 - 66s - loss: 20.1939 - MinusLogProbMetric: 20.1939 - val_loss: 20.2330 - val_MinusLogProbMetric: 20.2330 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 26/1000
2023-09-27 07:51:04.912 
Epoch 26/1000 
	 loss: 20.0533, MinusLogProbMetric: 20.0533, val_loss: 19.9921, val_MinusLogProbMetric: 19.9921

Epoch 26: val_loss did not improve from 19.59327
196/196 - 66s - loss: 20.0533 - MinusLogProbMetric: 20.0533 - val_loss: 19.9921 - val_MinusLogProbMetric: 19.9921 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 27/1000
2023-09-27 07:52:10.019 
Epoch 27/1000 
	 loss: 20.0750, MinusLogProbMetric: 20.0750, val_loss: 20.0259, val_MinusLogProbMetric: 20.0259

Epoch 27: val_loss did not improve from 19.59327
196/196 - 65s - loss: 20.0750 - MinusLogProbMetric: 20.0750 - val_loss: 20.0259 - val_MinusLogProbMetric: 20.0259 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 28/1000
2023-09-27 07:53:15.015 
Epoch 28/1000 
	 loss: 20.5012, MinusLogProbMetric: 20.5012, val_loss: 20.4305, val_MinusLogProbMetric: 20.4305

Epoch 28: val_loss did not improve from 19.59327
196/196 - 65s - loss: 20.5012 - MinusLogProbMetric: 20.5012 - val_loss: 20.4305 - val_MinusLogProbMetric: 20.4305 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 29/1000
2023-09-27 07:54:20.287 
Epoch 29/1000 
	 loss: 20.2039, MinusLogProbMetric: 20.2039, val_loss: 20.1011, val_MinusLogProbMetric: 20.1011

Epoch 29: val_loss did not improve from 19.59327
196/196 - 65s - loss: 20.2039 - MinusLogProbMetric: 20.2039 - val_loss: 20.1011 - val_MinusLogProbMetric: 20.1011 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 30/1000
2023-09-27 07:55:26.052 
Epoch 30/1000 
	 loss: 19.8191, MinusLogProbMetric: 19.8191, val_loss: 19.7787, val_MinusLogProbMetric: 19.7787

Epoch 30: val_loss did not improve from 19.59327
196/196 - 66s - loss: 19.8191 - MinusLogProbMetric: 19.8191 - val_loss: 19.7787 - val_MinusLogProbMetric: 19.7787 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 31/1000
2023-09-27 07:56:31.547 
Epoch 31/1000 
	 loss: 19.7594, MinusLogProbMetric: 19.7594, val_loss: 19.9878, val_MinusLogProbMetric: 19.9878

Epoch 31: val_loss did not improve from 19.59327
196/196 - 65s - loss: 19.7594 - MinusLogProbMetric: 19.7594 - val_loss: 19.9878 - val_MinusLogProbMetric: 19.9878 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 32/1000
2023-09-27 07:57:36.991 
Epoch 32/1000 
	 loss: 19.5584, MinusLogProbMetric: 19.5584, val_loss: 20.5449, val_MinusLogProbMetric: 20.5449

Epoch 32: val_loss did not improve from 19.59327
196/196 - 65s - loss: 19.5584 - MinusLogProbMetric: 19.5584 - val_loss: 20.5449 - val_MinusLogProbMetric: 20.5449 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 33/1000
2023-09-27 07:58:42.249 
Epoch 33/1000 
	 loss: 19.8665, MinusLogProbMetric: 19.8665, val_loss: 19.9401, val_MinusLogProbMetric: 19.9401

Epoch 33: val_loss did not improve from 19.59327
196/196 - 65s - loss: 19.8665 - MinusLogProbMetric: 19.8665 - val_loss: 19.9401 - val_MinusLogProbMetric: 19.9401 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 34/1000
2023-09-27 07:59:47.480 
Epoch 34/1000 
	 loss: 19.5101, MinusLogProbMetric: 19.5101, val_loss: 19.2540, val_MinusLogProbMetric: 19.2540

Epoch 34: val_loss improved from 19.59327 to 19.25397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 19.5101 - MinusLogProbMetric: 19.5101 - val_loss: 19.2540 - val_MinusLogProbMetric: 19.2540 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 35/1000
2023-09-27 08:00:53.812 
Epoch 35/1000 
	 loss: 19.4626, MinusLogProbMetric: 19.4626, val_loss: 20.2263, val_MinusLogProbMetric: 20.2263

Epoch 35: val_loss did not improve from 19.25397
196/196 - 65s - loss: 19.4626 - MinusLogProbMetric: 19.4626 - val_loss: 20.2263 - val_MinusLogProbMetric: 20.2263 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 36/1000
2023-09-27 08:01:59.536 
Epoch 36/1000 
	 loss: 19.3741, MinusLogProbMetric: 19.3741, val_loss: 19.3145, val_MinusLogProbMetric: 19.3145

Epoch 36: val_loss did not improve from 19.25397
196/196 - 66s - loss: 19.3741 - MinusLogProbMetric: 19.3741 - val_loss: 19.3145 - val_MinusLogProbMetric: 19.3145 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 37/1000
2023-09-27 08:03:05.418 
Epoch 37/1000 
	 loss: 19.4917, MinusLogProbMetric: 19.4917, val_loss: 19.5840, val_MinusLogProbMetric: 19.5840

Epoch 37: val_loss did not improve from 19.25397
196/196 - 66s - loss: 19.4917 - MinusLogProbMetric: 19.4917 - val_loss: 19.5840 - val_MinusLogProbMetric: 19.5840 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 38/1000
2023-09-27 08:04:11.178 
Epoch 38/1000 
	 loss: 19.1360, MinusLogProbMetric: 19.1360, val_loss: 18.9320, val_MinusLogProbMetric: 18.9320

Epoch 38: val_loss improved from 19.25397 to 18.93197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 67s - loss: 19.1360 - MinusLogProbMetric: 19.1360 - val_loss: 18.9320 - val_MinusLogProbMetric: 18.9320 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 39/1000
2023-09-27 08:05:17.727 
Epoch 39/1000 
	 loss: 19.2171, MinusLogProbMetric: 19.2171, val_loss: 19.3776, val_MinusLogProbMetric: 19.3776

Epoch 39: val_loss did not improve from 18.93197
196/196 - 66s - loss: 19.2171 - MinusLogProbMetric: 19.2171 - val_loss: 19.3776 - val_MinusLogProbMetric: 19.3776 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 40/1000
2023-09-27 08:06:23.309 
Epoch 40/1000 
	 loss: 19.2121, MinusLogProbMetric: 19.2121, val_loss: 19.3604, val_MinusLogProbMetric: 19.3604

Epoch 40: val_loss did not improve from 18.93197
196/196 - 66s - loss: 19.2121 - MinusLogProbMetric: 19.2121 - val_loss: 19.3604 - val_MinusLogProbMetric: 19.3604 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 41/1000
2023-09-27 08:07:28.350 
Epoch 41/1000 
	 loss: 19.2301, MinusLogProbMetric: 19.2301, val_loss: 19.0853, val_MinusLogProbMetric: 19.0853

Epoch 41: val_loss did not improve from 18.93197
196/196 - 65s - loss: 19.2301 - MinusLogProbMetric: 19.2301 - val_loss: 19.0853 - val_MinusLogProbMetric: 19.0853 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 42/1000
2023-09-27 08:08:33.532 
Epoch 42/1000 
	 loss: 18.9441, MinusLogProbMetric: 18.9441, val_loss: 19.2781, val_MinusLogProbMetric: 19.2781

Epoch 42: val_loss did not improve from 18.93197
196/196 - 65s - loss: 18.9441 - MinusLogProbMetric: 18.9441 - val_loss: 19.2781 - val_MinusLogProbMetric: 19.2781 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 43/1000
2023-09-27 08:09:39.031 
Epoch 43/1000 
	 loss: 18.9649, MinusLogProbMetric: 18.9649, val_loss: 18.6721, val_MinusLogProbMetric: 18.6721

Epoch 43: val_loss improved from 18.93197 to 18.67214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 18.9649 - MinusLogProbMetric: 18.9649 - val_loss: 18.6721 - val_MinusLogProbMetric: 18.6721 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 44/1000
2023-09-27 08:10:45.630 
Epoch 44/1000 
	 loss: 19.0075, MinusLogProbMetric: 19.0075, val_loss: 19.6422, val_MinusLogProbMetric: 19.6422

Epoch 44: val_loss did not improve from 18.67214
196/196 - 66s - loss: 19.0075 - MinusLogProbMetric: 19.0075 - val_loss: 19.6422 - val_MinusLogProbMetric: 19.6422 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 45/1000
2023-09-27 08:11:50.735 
Epoch 45/1000 
	 loss: 19.0183, MinusLogProbMetric: 19.0183, val_loss: 21.4036, val_MinusLogProbMetric: 21.4036

Epoch 45: val_loss did not improve from 18.67214
196/196 - 65s - loss: 19.0183 - MinusLogProbMetric: 19.0183 - val_loss: 21.4036 - val_MinusLogProbMetric: 21.4036 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 46/1000
2023-09-27 08:12:56.011 
Epoch 46/1000 
	 loss: 18.7995, MinusLogProbMetric: 18.7995, val_loss: 18.8302, val_MinusLogProbMetric: 18.8302

Epoch 46: val_loss did not improve from 18.67214
196/196 - 65s - loss: 18.7995 - MinusLogProbMetric: 18.7995 - val_loss: 18.8302 - val_MinusLogProbMetric: 18.8302 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 47/1000
2023-09-27 08:14:01.199 
Epoch 47/1000 
	 loss: 18.9322, MinusLogProbMetric: 18.9322, val_loss: 18.6056, val_MinusLogProbMetric: 18.6056

Epoch 47: val_loss improved from 18.67214 to 18.60561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 18.9322 - MinusLogProbMetric: 18.9322 - val_loss: 18.6056 - val_MinusLogProbMetric: 18.6056 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 48/1000
2023-09-27 08:15:07.533 
Epoch 48/1000 
	 loss: 18.8016, MinusLogProbMetric: 18.8016, val_loss: 18.8024, val_MinusLogProbMetric: 18.8024

Epoch 48: val_loss did not improve from 18.60561
196/196 - 65s - loss: 18.8016 - MinusLogProbMetric: 18.8016 - val_loss: 18.8024 - val_MinusLogProbMetric: 18.8024 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 49/1000
2023-09-27 08:16:12.655 
Epoch 49/1000 
	 loss: 18.7402, MinusLogProbMetric: 18.7402, val_loss: 21.7698, val_MinusLogProbMetric: 21.7698

Epoch 49: val_loss did not improve from 18.60561
196/196 - 65s - loss: 18.7402 - MinusLogProbMetric: 18.7402 - val_loss: 21.7698 - val_MinusLogProbMetric: 21.7698 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 50/1000
2023-09-27 08:17:18.331 
Epoch 50/1000 
	 loss: 18.6491, MinusLogProbMetric: 18.6491, val_loss: 18.5371, val_MinusLogProbMetric: 18.5371

Epoch 50: val_loss improved from 18.60561 to 18.53707, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 67s - loss: 18.6491 - MinusLogProbMetric: 18.6491 - val_loss: 18.5371 - val_MinusLogProbMetric: 18.5371 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 51/1000
2023-09-27 08:18:24.515 
Epoch 51/1000 
	 loss: 18.7133, MinusLogProbMetric: 18.7133, val_loss: 18.8192, val_MinusLogProbMetric: 18.8192

Epoch 51: val_loss did not improve from 18.53707
196/196 - 65s - loss: 18.7133 - MinusLogProbMetric: 18.7133 - val_loss: 18.8192 - val_MinusLogProbMetric: 18.8192 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 52/1000
2023-09-27 08:19:29.887 
Epoch 52/1000 
	 loss: 18.7992, MinusLogProbMetric: 18.7992, val_loss: 18.8490, val_MinusLogProbMetric: 18.8490

Epoch 52: val_loss did not improve from 18.53707
196/196 - 65s - loss: 18.7992 - MinusLogProbMetric: 18.7992 - val_loss: 18.8490 - val_MinusLogProbMetric: 18.8490 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 53/1000
2023-09-27 08:20:35.046 
Epoch 53/1000 
	 loss: 18.6821, MinusLogProbMetric: 18.6821, val_loss: 19.1540, val_MinusLogProbMetric: 19.1540

Epoch 53: val_loss did not improve from 18.53707
196/196 - 65s - loss: 18.6821 - MinusLogProbMetric: 18.6821 - val_loss: 19.1540 - val_MinusLogProbMetric: 19.1540 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 54/1000
2023-09-27 08:21:40.582 
Epoch 54/1000 
	 loss: 18.6384, MinusLogProbMetric: 18.6384, val_loss: 19.0300, val_MinusLogProbMetric: 19.0300

Epoch 54: val_loss did not improve from 18.53707
196/196 - 66s - loss: 18.6384 - MinusLogProbMetric: 18.6384 - val_loss: 19.0300 - val_MinusLogProbMetric: 19.0300 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 55/1000
2023-09-27 08:22:45.936 
Epoch 55/1000 
	 loss: 18.6070, MinusLogProbMetric: 18.6070, val_loss: 18.8273, val_MinusLogProbMetric: 18.8273

Epoch 55: val_loss did not improve from 18.53707
196/196 - 65s - loss: 18.6070 - MinusLogProbMetric: 18.6070 - val_loss: 18.8273 - val_MinusLogProbMetric: 18.8273 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 56/1000
2023-09-27 08:23:51.516 
Epoch 56/1000 
	 loss: 18.5552, MinusLogProbMetric: 18.5552, val_loss: 19.0868, val_MinusLogProbMetric: 19.0868

Epoch 56: val_loss did not improve from 18.53707
196/196 - 66s - loss: 18.5552 - MinusLogProbMetric: 18.5552 - val_loss: 19.0868 - val_MinusLogProbMetric: 19.0868 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 57/1000
2023-09-27 08:24:56.704 
Epoch 57/1000 
	 loss: 18.5300, MinusLogProbMetric: 18.5300, val_loss: 18.4806, val_MinusLogProbMetric: 18.4806

Epoch 57: val_loss improved from 18.53707 to 18.48065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 18.5300 - MinusLogProbMetric: 18.5300 - val_loss: 18.4806 - val_MinusLogProbMetric: 18.4806 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 58/1000
2023-09-27 08:26:03.432 
Epoch 58/1000 
	 loss: 18.4777, MinusLogProbMetric: 18.4777, val_loss: 18.3647, val_MinusLogProbMetric: 18.3647

Epoch 58: val_loss improved from 18.48065 to 18.36474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 67s - loss: 18.4777 - MinusLogProbMetric: 18.4777 - val_loss: 18.3647 - val_MinusLogProbMetric: 18.3647 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 59/1000
2023-09-27 08:27:09.533 
Epoch 59/1000 
	 loss: 18.4522, MinusLogProbMetric: 18.4522, val_loss: 19.1347, val_MinusLogProbMetric: 19.1347

Epoch 59: val_loss did not improve from 18.36474
196/196 - 65s - loss: 18.4522 - MinusLogProbMetric: 18.4522 - val_loss: 19.1347 - val_MinusLogProbMetric: 19.1347 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 60/1000
2023-09-27 08:28:14.956 
Epoch 60/1000 
	 loss: 18.4807, MinusLogProbMetric: 18.4807, val_loss: 18.3028, val_MinusLogProbMetric: 18.3028

Epoch 60: val_loss improved from 18.36474 to 18.30284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 18.4807 - MinusLogProbMetric: 18.4807 - val_loss: 18.3028 - val_MinusLogProbMetric: 18.3028 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 61/1000
2023-09-27 08:29:21.617 
Epoch 61/1000 
	 loss: 18.5145, MinusLogProbMetric: 18.5145, val_loss: 18.9963, val_MinusLogProbMetric: 18.9963

Epoch 61: val_loss did not improve from 18.30284
196/196 - 66s - loss: 18.5145 - MinusLogProbMetric: 18.5145 - val_loss: 18.9963 - val_MinusLogProbMetric: 18.9963 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 62/1000
2023-09-27 08:30:26.679 
Epoch 62/1000 
	 loss: 18.4997, MinusLogProbMetric: 18.4997, val_loss: 18.7195, val_MinusLogProbMetric: 18.7195

Epoch 62: val_loss did not improve from 18.30284
196/196 - 65s - loss: 18.4997 - MinusLogProbMetric: 18.4997 - val_loss: 18.7195 - val_MinusLogProbMetric: 18.7195 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 63/1000
2023-09-27 08:31:32.406 
Epoch 63/1000 
	 loss: 18.4415, MinusLogProbMetric: 18.4415, val_loss: 18.8036, val_MinusLogProbMetric: 18.8036

Epoch 63: val_loss did not improve from 18.30284
196/196 - 66s - loss: 18.4415 - MinusLogProbMetric: 18.4415 - val_loss: 18.8036 - val_MinusLogProbMetric: 18.8036 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 64/1000
2023-09-27 08:32:37.837 
Epoch 64/1000 
	 loss: 18.3544, MinusLogProbMetric: 18.3544, val_loss: 18.3104, val_MinusLogProbMetric: 18.3104

Epoch 64: val_loss did not improve from 18.30284
196/196 - 65s - loss: 18.3544 - MinusLogProbMetric: 18.3544 - val_loss: 18.3104 - val_MinusLogProbMetric: 18.3104 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 65/1000
2023-09-27 08:33:42.862 
Epoch 65/1000 
	 loss: 18.3636, MinusLogProbMetric: 18.3636, val_loss: 18.7371, val_MinusLogProbMetric: 18.7371

Epoch 65: val_loss did not improve from 18.30284
196/196 - 65s - loss: 18.3636 - MinusLogProbMetric: 18.3636 - val_loss: 18.7371 - val_MinusLogProbMetric: 18.7371 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 66/1000
2023-09-27 08:34:48.447 
Epoch 66/1000 
	 loss: 18.2977, MinusLogProbMetric: 18.2977, val_loss: 18.6911, val_MinusLogProbMetric: 18.6911

Epoch 66: val_loss did not improve from 18.30284
196/196 - 66s - loss: 18.2977 - MinusLogProbMetric: 18.2977 - val_loss: 18.6911 - val_MinusLogProbMetric: 18.6911 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 67/1000
2023-09-27 08:35:53.827 
Epoch 67/1000 
	 loss: 18.3932, MinusLogProbMetric: 18.3932, val_loss: 18.6084, val_MinusLogProbMetric: 18.6084

Epoch 67: val_loss did not improve from 18.30284
196/196 - 65s - loss: 18.3932 - MinusLogProbMetric: 18.3932 - val_loss: 18.6084 - val_MinusLogProbMetric: 18.6084 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 68/1000
2023-09-27 08:36:59.533 
Epoch 68/1000 
	 loss: 18.2019, MinusLogProbMetric: 18.2019, val_loss: 18.2381, val_MinusLogProbMetric: 18.2381

Epoch 68: val_loss improved from 18.30284 to 18.23812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 67s - loss: 18.2019 - MinusLogProbMetric: 18.2019 - val_loss: 18.2381 - val_MinusLogProbMetric: 18.2381 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 69/1000
2023-09-27 08:38:05.572 
Epoch 69/1000 
	 loss: 18.2429, MinusLogProbMetric: 18.2429, val_loss: 18.1391, val_MinusLogProbMetric: 18.1391

Epoch 69: val_loss improved from 18.23812 to 18.13910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 18.2429 - MinusLogProbMetric: 18.2429 - val_loss: 18.1391 - val_MinusLogProbMetric: 18.1391 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 70/1000
2023-09-27 08:39:11.851 
Epoch 70/1000 
	 loss: 18.2095, MinusLogProbMetric: 18.2095, val_loss: 18.0333, val_MinusLogProbMetric: 18.0333

Epoch 70: val_loss improved from 18.13910 to 18.03333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 66s - loss: 18.2095 - MinusLogProbMetric: 18.2095 - val_loss: 18.0333 - val_MinusLogProbMetric: 18.0333 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 71/1000
2023-09-27 08:40:18.251 
Epoch 71/1000 
	 loss: 18.3429, MinusLogProbMetric: 18.3429, val_loss: 18.3097, val_MinusLogProbMetric: 18.3097

Epoch 71: val_loss did not improve from 18.03333
196/196 - 65s - loss: 18.3429 - MinusLogProbMetric: 18.3429 - val_loss: 18.3097 - val_MinusLogProbMetric: 18.3097 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 72/1000
2023-09-27 08:41:23.782 
Epoch 72/1000 
	 loss: 18.2064, MinusLogProbMetric: 18.2064, val_loss: 18.7375, val_MinusLogProbMetric: 18.7375

Epoch 72: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.2064 - MinusLogProbMetric: 18.2064 - val_loss: 18.7375 - val_MinusLogProbMetric: 18.7375 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 73/1000
2023-09-27 08:42:29.154 
Epoch 73/1000 
	 loss: 18.1949, MinusLogProbMetric: 18.1949, val_loss: 19.6791, val_MinusLogProbMetric: 19.6791

Epoch 73: val_loss did not improve from 18.03333
196/196 - 65s - loss: 18.1949 - MinusLogProbMetric: 18.1949 - val_loss: 19.6791 - val_MinusLogProbMetric: 19.6791 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 74/1000
2023-09-27 08:43:34.753 
Epoch 74/1000 
	 loss: 18.2387, MinusLogProbMetric: 18.2387, val_loss: 18.9548, val_MinusLogProbMetric: 18.9548

Epoch 74: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.2387 - MinusLogProbMetric: 18.2387 - val_loss: 18.9548 - val_MinusLogProbMetric: 18.9548 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 75/1000
2023-09-27 08:44:40.520 
Epoch 75/1000 
	 loss: 18.2076, MinusLogProbMetric: 18.2076, val_loss: 18.0443, val_MinusLogProbMetric: 18.0443

Epoch 75: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.2076 - MinusLogProbMetric: 18.2076 - val_loss: 18.0443 - val_MinusLogProbMetric: 18.0443 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 76/1000
2023-09-27 08:45:45.750 
Epoch 76/1000 
	 loss: 18.0960, MinusLogProbMetric: 18.0960, val_loss: 18.3678, val_MinusLogProbMetric: 18.3678

Epoch 76: val_loss did not improve from 18.03333
196/196 - 65s - loss: 18.0960 - MinusLogProbMetric: 18.0960 - val_loss: 18.3678 - val_MinusLogProbMetric: 18.3678 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 77/1000
2023-09-27 08:46:51.459 
Epoch 77/1000 
	 loss: 18.0697, MinusLogProbMetric: 18.0697, val_loss: 18.1756, val_MinusLogProbMetric: 18.1756

Epoch 77: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.0697 - MinusLogProbMetric: 18.0697 - val_loss: 18.1756 - val_MinusLogProbMetric: 18.1756 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 78/1000
2023-09-27 08:47:56.881 
Epoch 78/1000 
	 loss: 18.1615, MinusLogProbMetric: 18.1615, val_loss: 18.7593, val_MinusLogProbMetric: 18.7593

Epoch 78: val_loss did not improve from 18.03333
196/196 - 65s - loss: 18.1615 - MinusLogProbMetric: 18.1615 - val_loss: 18.7593 - val_MinusLogProbMetric: 18.7593 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 79/1000
2023-09-27 08:49:02.127 
Epoch 79/1000 
	 loss: 18.0251, MinusLogProbMetric: 18.0251, val_loss: 18.1001, val_MinusLogProbMetric: 18.1001

Epoch 79: val_loss did not improve from 18.03333
196/196 - 65s - loss: 18.0251 - MinusLogProbMetric: 18.0251 - val_loss: 18.1001 - val_MinusLogProbMetric: 18.1001 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 80/1000
2023-09-27 08:50:08.043 
Epoch 80/1000 
	 loss: 18.1376, MinusLogProbMetric: 18.1376, val_loss: 18.8813, val_MinusLogProbMetric: 18.8813

Epoch 80: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.1376 - MinusLogProbMetric: 18.1376 - val_loss: 18.8813 - val_MinusLogProbMetric: 18.8813 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 81/1000
2023-09-27 08:51:13.875 
Epoch 81/1000 
	 loss: 18.0255, MinusLogProbMetric: 18.0255, val_loss: 19.3645, val_MinusLogProbMetric: 19.3645

Epoch 81: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.0255 - MinusLogProbMetric: 18.0255 - val_loss: 19.3645 - val_MinusLogProbMetric: 19.3645 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 82/1000
2023-09-27 08:52:19.688 
Epoch 82/1000 
	 loss: 18.1828, MinusLogProbMetric: 18.1828, val_loss: 18.2619, val_MinusLogProbMetric: 18.2619

Epoch 82: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.1828 - MinusLogProbMetric: 18.1828 - val_loss: 18.2619 - val_MinusLogProbMetric: 18.2619 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 83/1000
2023-09-27 08:53:25.147 
Epoch 83/1000 
	 loss: 18.0039, MinusLogProbMetric: 18.0039, val_loss: 18.0540, val_MinusLogProbMetric: 18.0540

Epoch 83: val_loss did not improve from 18.03333
196/196 - 65s - loss: 18.0039 - MinusLogProbMetric: 18.0039 - val_loss: 18.0540 - val_MinusLogProbMetric: 18.0540 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 84/1000
2023-09-27 08:54:30.983 
Epoch 84/1000 
	 loss: 18.0705, MinusLogProbMetric: 18.0705, val_loss: 18.6688, val_MinusLogProbMetric: 18.6688

Epoch 84: val_loss did not improve from 18.03333
196/196 - 66s - loss: 18.0705 - MinusLogProbMetric: 18.0705 - val_loss: 18.6688 - val_MinusLogProbMetric: 18.6688 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 85/1000
2023-09-27 08:55:37.541 
Epoch 85/1000 
	 loss: 18.0003, MinusLogProbMetric: 18.0003, val_loss: 18.6728, val_MinusLogProbMetric: 18.6728

Epoch 85: val_loss did not improve from 18.03333
196/196 - 67s - loss: 18.0003 - MinusLogProbMetric: 18.0003 - val_loss: 18.6728 - val_MinusLogProbMetric: 18.6728 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 86/1000
2023-09-27 08:56:40.243 
Epoch 86/1000 
	 loss: 18.0157, MinusLogProbMetric: 18.0157, val_loss: 17.8384, val_MinusLogProbMetric: 17.8384

Epoch 86: val_loss improved from 18.03333 to 17.83842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 64s - loss: 18.0157 - MinusLogProbMetric: 18.0157 - val_loss: 17.8384 - val_MinusLogProbMetric: 17.8384 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 87/1000
2023-09-27 08:57:38.995 
Epoch 87/1000 
	 loss: 17.9175, MinusLogProbMetric: 17.9175, val_loss: 17.9573, val_MinusLogProbMetric: 17.9573

Epoch 87: val_loss did not improve from 17.83842
196/196 - 58s - loss: 17.9175 - MinusLogProbMetric: 17.9175 - val_loss: 17.9573 - val_MinusLogProbMetric: 17.9573 - lr: 3.3333e-04 - 58s/epoch - 295ms/step
Epoch 88/1000
2023-09-27 08:58:41.293 
Epoch 88/1000 
	 loss: 17.9952, MinusLogProbMetric: 17.9952, val_loss: 17.8895, val_MinusLogProbMetric: 17.8895

Epoch 88: val_loss did not improve from 17.83842
196/196 - 62s - loss: 17.9952 - MinusLogProbMetric: 17.9952 - val_loss: 17.8895 - val_MinusLogProbMetric: 17.8895 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 89/1000
2023-09-27 08:59:45.158 
Epoch 89/1000 
	 loss: 17.9713, MinusLogProbMetric: 17.9713, val_loss: 18.3671, val_MinusLogProbMetric: 18.3671

Epoch 89: val_loss did not improve from 17.83842
196/196 - 64s - loss: 17.9713 - MinusLogProbMetric: 17.9713 - val_loss: 18.3671 - val_MinusLogProbMetric: 18.3671 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 90/1000
2023-09-27 09:00:47.561 
Epoch 90/1000 
	 loss: 17.9214, MinusLogProbMetric: 17.9214, val_loss: 19.3834, val_MinusLogProbMetric: 19.3834

Epoch 90: val_loss did not improve from 17.83842
196/196 - 62s - loss: 17.9214 - MinusLogProbMetric: 17.9214 - val_loss: 19.3834 - val_MinusLogProbMetric: 19.3834 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 91/1000
2023-09-27 09:01:47.219 
Epoch 91/1000 
	 loss: 17.9265, MinusLogProbMetric: 17.9265, val_loss: 18.2637, val_MinusLogProbMetric: 18.2637

Epoch 91: val_loss did not improve from 17.83842
196/196 - 60s - loss: 17.9265 - MinusLogProbMetric: 17.9265 - val_loss: 18.2637 - val_MinusLogProbMetric: 18.2637 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 92/1000
2023-09-27 09:02:49.627 
Epoch 92/1000 
	 loss: 17.9187, MinusLogProbMetric: 17.9187, val_loss: 18.7544, val_MinusLogProbMetric: 18.7544

Epoch 92: val_loss did not improve from 17.83842
196/196 - 62s - loss: 17.9187 - MinusLogProbMetric: 17.9187 - val_loss: 18.7544 - val_MinusLogProbMetric: 18.7544 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 93/1000
2023-09-27 09:03:55.236 
Epoch 93/1000 
	 loss: 17.8711, MinusLogProbMetric: 17.8711, val_loss: 17.9391, val_MinusLogProbMetric: 17.9391

Epoch 93: val_loss did not improve from 17.83842
196/196 - 66s - loss: 17.8711 - MinusLogProbMetric: 17.8711 - val_loss: 17.9391 - val_MinusLogProbMetric: 17.9391 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 94/1000
2023-09-27 09:05:00.027 
Epoch 94/1000 
	 loss: 17.8787, MinusLogProbMetric: 17.8787, val_loss: 18.4458, val_MinusLogProbMetric: 18.4458

Epoch 94: val_loss did not improve from 17.83842
196/196 - 65s - loss: 17.8787 - MinusLogProbMetric: 17.8787 - val_loss: 18.4458 - val_MinusLogProbMetric: 18.4458 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 95/1000
2023-09-27 09:06:04.963 
Epoch 95/1000 
	 loss: 17.9181, MinusLogProbMetric: 17.9181, val_loss: 17.9616, val_MinusLogProbMetric: 17.9616

Epoch 95: val_loss did not improve from 17.83842
196/196 - 65s - loss: 17.9181 - MinusLogProbMetric: 17.9181 - val_loss: 17.9616 - val_MinusLogProbMetric: 17.9616 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 96/1000
2023-09-27 09:07:09.722 
Epoch 96/1000 
	 loss: 17.8629, MinusLogProbMetric: 17.8629, val_loss: 18.5227, val_MinusLogProbMetric: 18.5227

Epoch 96: val_loss did not improve from 17.83842
196/196 - 65s - loss: 17.8629 - MinusLogProbMetric: 17.8629 - val_loss: 18.5227 - val_MinusLogProbMetric: 18.5227 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 97/1000
2023-09-27 09:08:15.638 
Epoch 97/1000 
	 loss: 17.9066, MinusLogProbMetric: 17.9066, val_loss: 18.0671, val_MinusLogProbMetric: 18.0671

Epoch 97: val_loss did not improve from 17.83842
196/196 - 66s - loss: 17.9066 - MinusLogProbMetric: 17.9066 - val_loss: 18.0671 - val_MinusLogProbMetric: 18.0671 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 98/1000
2023-09-27 09:09:22.029 
Epoch 98/1000 
	 loss: 17.8147, MinusLogProbMetric: 17.8147, val_loss: 19.7859, val_MinusLogProbMetric: 19.7859

Epoch 98: val_loss did not improve from 17.83842
196/196 - 66s - loss: 17.8147 - MinusLogProbMetric: 17.8147 - val_loss: 19.7859 - val_MinusLogProbMetric: 19.7859 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 99/1000
2023-09-27 09:10:27.478 
Epoch 99/1000 
	 loss: 17.8640, MinusLogProbMetric: 17.8640, val_loss: 18.6753, val_MinusLogProbMetric: 18.6753

Epoch 99: val_loss did not improve from 17.83842
196/196 - 65s - loss: 17.8640 - MinusLogProbMetric: 17.8640 - val_loss: 18.6753 - val_MinusLogProbMetric: 18.6753 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 100/1000
2023-09-27 09:11:33.077 
Epoch 100/1000 
	 loss: 17.8820, MinusLogProbMetric: 17.8820, val_loss: 18.0927, val_MinusLogProbMetric: 18.0927

Epoch 100: val_loss did not improve from 17.83842
196/196 - 66s - loss: 17.8820 - MinusLogProbMetric: 17.8820 - val_loss: 18.0927 - val_MinusLogProbMetric: 18.0927 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 101/1000
2023-09-27 09:12:38.990 
Epoch 101/1000 
	 loss: 17.8541, MinusLogProbMetric: 17.8541, val_loss: 18.2793, val_MinusLogProbMetric: 18.2793

Epoch 101: val_loss did not improve from 17.83842
196/196 - 66s - loss: 17.8541 - MinusLogProbMetric: 17.8541 - val_loss: 18.2793 - val_MinusLogProbMetric: 18.2793 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 102/1000
2023-09-27 09:13:44.241 
Epoch 102/1000 
	 loss: 17.7581, MinusLogProbMetric: 17.7581, val_loss: 18.0538, val_MinusLogProbMetric: 18.0538

Epoch 102: val_loss did not improve from 17.83842
196/196 - 65s - loss: 17.7581 - MinusLogProbMetric: 17.7581 - val_loss: 18.0538 - val_MinusLogProbMetric: 18.0538 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 103/1000
2023-09-27 09:14:49.803 
Epoch 103/1000 
	 loss: 17.8204, MinusLogProbMetric: 17.8204, val_loss: 18.7783, val_MinusLogProbMetric: 18.7783

Epoch 103: val_loss did not improve from 17.83842
196/196 - 66s - loss: 17.8204 - MinusLogProbMetric: 17.8204 - val_loss: 18.7783 - val_MinusLogProbMetric: 18.7783 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 104/1000
2023-09-27 09:15:56.042 
Epoch 104/1000 
	 loss: 17.8224, MinusLogProbMetric: 17.8224, val_loss: 19.5556, val_MinusLogProbMetric: 19.5556

Epoch 104: val_loss did not improve from 17.83842
196/196 - 66s - loss: 17.8224 - MinusLogProbMetric: 17.8224 - val_loss: 19.5556 - val_MinusLogProbMetric: 19.5556 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 105/1000
2023-09-27 09:17:00.820 
Epoch 105/1000 
	 loss: 17.8117, MinusLogProbMetric: 17.8117, val_loss: 18.1730, val_MinusLogProbMetric: 18.1730

Epoch 105: val_loss did not improve from 17.83842
196/196 - 65s - loss: 17.8117 - MinusLogProbMetric: 17.8117 - val_loss: 18.1730 - val_MinusLogProbMetric: 18.1730 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 106/1000
2023-09-27 09:18:02.201 
Epoch 106/1000 
	 loss: 17.8564, MinusLogProbMetric: 17.8564, val_loss: 17.9334, val_MinusLogProbMetric: 17.9334

Epoch 106: val_loss did not improve from 17.83842
196/196 - 61s - loss: 17.8564 - MinusLogProbMetric: 17.8564 - val_loss: 17.9334 - val_MinusLogProbMetric: 17.9334 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 107/1000
2023-09-27 09:19:01.486 
Epoch 107/1000 
	 loss: 17.7024, MinusLogProbMetric: 17.7024, val_loss: 17.8848, val_MinusLogProbMetric: 17.8848

Epoch 107: val_loss did not improve from 17.83842
196/196 - 59s - loss: 17.7024 - MinusLogProbMetric: 17.7024 - val_loss: 17.8848 - val_MinusLogProbMetric: 17.8848 - lr: 3.3333e-04 - 59s/epoch - 302ms/step
Epoch 108/1000
2023-09-27 09:20:05.494 
Epoch 108/1000 
	 loss: 17.6972, MinusLogProbMetric: 17.6972, val_loss: 17.9029, val_MinusLogProbMetric: 17.9029

Epoch 108: val_loss did not improve from 17.83842
196/196 - 64s - loss: 17.6972 - MinusLogProbMetric: 17.6972 - val_loss: 17.9029 - val_MinusLogProbMetric: 17.9029 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 109/1000
2023-09-27 09:21:10.934 
Epoch 109/1000 
	 loss: 17.7398, MinusLogProbMetric: 17.7398, val_loss: 18.1610, val_MinusLogProbMetric: 18.1610

Epoch 109: val_loss did not improve from 17.83842
196/196 - 65s - loss: 17.7398 - MinusLogProbMetric: 17.7398 - val_loss: 18.1610 - val_MinusLogProbMetric: 18.1610 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 110/1000
2023-09-27 09:22:13.156 
Epoch 110/1000 
	 loss: 17.7744, MinusLogProbMetric: 17.7744, val_loss: 18.3762, val_MinusLogProbMetric: 18.3762

Epoch 110: val_loss did not improve from 17.83842
196/196 - 62s - loss: 17.7744 - MinusLogProbMetric: 17.7744 - val_loss: 18.3762 - val_MinusLogProbMetric: 18.3762 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 111/1000
2023-09-27 09:23:13.093 
Epoch 111/1000 
	 loss: 17.8121, MinusLogProbMetric: 17.8121, val_loss: 17.8607, val_MinusLogProbMetric: 17.8607

Epoch 111: val_loss did not improve from 17.83842
196/196 - 60s - loss: 17.8121 - MinusLogProbMetric: 17.8121 - val_loss: 17.8607 - val_MinusLogProbMetric: 17.8607 - lr: 3.3333e-04 - 60s/epoch - 306ms/step
Epoch 112/1000
2023-09-27 09:24:09.301 
Epoch 112/1000 
	 loss: 17.6976, MinusLogProbMetric: 17.6976, val_loss: 17.9569, val_MinusLogProbMetric: 17.9569

Epoch 112: val_loss did not improve from 17.83842
196/196 - 56s - loss: 17.6976 - MinusLogProbMetric: 17.6976 - val_loss: 17.9569 - val_MinusLogProbMetric: 17.9569 - lr: 3.3333e-04 - 56s/epoch - 287ms/step
Epoch 113/1000
2023-09-27 09:25:10.549 
Epoch 113/1000 
	 loss: 17.6356, MinusLogProbMetric: 17.6356, val_loss: 17.9314, val_MinusLogProbMetric: 17.9314

Epoch 113: val_loss did not improve from 17.83842
196/196 - 61s - loss: 17.6356 - MinusLogProbMetric: 17.6356 - val_loss: 17.9314 - val_MinusLogProbMetric: 17.9314 - lr: 3.3333e-04 - 61s/epoch - 312ms/step
Epoch 114/1000
2023-09-27 09:26:10.949 
Epoch 114/1000 
	 loss: 17.7138, MinusLogProbMetric: 17.7138, val_loss: 18.1288, val_MinusLogProbMetric: 18.1288

Epoch 114: val_loss did not improve from 17.83842
196/196 - 60s - loss: 17.7138 - MinusLogProbMetric: 17.7138 - val_loss: 18.1288 - val_MinusLogProbMetric: 18.1288 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 115/1000
2023-09-27 09:27:07.697 
Epoch 115/1000 
	 loss: 17.7001, MinusLogProbMetric: 17.7001, val_loss: 17.6831, val_MinusLogProbMetric: 17.6831

Epoch 115: val_loss improved from 17.83842 to 17.68311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 58s - loss: 17.7001 - MinusLogProbMetric: 17.7001 - val_loss: 17.6831 - val_MinusLogProbMetric: 17.6831 - lr: 3.3333e-04 - 58s/epoch - 294ms/step
Epoch 116/1000
2023-09-27 09:28:08.733 
Epoch 116/1000 
	 loss: 17.7333, MinusLogProbMetric: 17.7333, val_loss: 17.8128, val_MinusLogProbMetric: 17.8128

Epoch 116: val_loss did not improve from 17.68311
196/196 - 60s - loss: 17.7333 - MinusLogProbMetric: 17.7333 - val_loss: 17.8128 - val_MinusLogProbMetric: 17.8128 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 117/1000
2023-09-27 09:29:13.616 
Epoch 117/1000 
	 loss: 17.7698, MinusLogProbMetric: 17.7698, val_loss: 18.0124, val_MinusLogProbMetric: 18.0124

Epoch 117: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.7698 - MinusLogProbMetric: 17.7698 - val_loss: 18.0124 - val_MinusLogProbMetric: 18.0124 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 118/1000
2023-09-27 09:30:17.752 
Epoch 118/1000 
	 loss: 17.5892, MinusLogProbMetric: 17.5892, val_loss: 18.5657, val_MinusLogProbMetric: 18.5657

Epoch 118: val_loss did not improve from 17.68311
196/196 - 64s - loss: 17.5892 - MinusLogProbMetric: 17.5892 - val_loss: 18.5657 - val_MinusLogProbMetric: 18.5657 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 119/1000
2023-09-27 09:31:23.099 
Epoch 119/1000 
	 loss: 17.6658, MinusLogProbMetric: 17.6658, val_loss: 17.8245, val_MinusLogProbMetric: 17.8245

Epoch 119: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.6658 - MinusLogProbMetric: 17.6658 - val_loss: 17.8245 - val_MinusLogProbMetric: 17.8245 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 120/1000
2023-09-27 09:32:28.032 
Epoch 120/1000 
	 loss: 17.6121, MinusLogProbMetric: 17.6121, val_loss: 17.7773, val_MinusLogProbMetric: 17.7773

Epoch 120: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.6121 - MinusLogProbMetric: 17.6121 - val_loss: 17.7773 - val_MinusLogProbMetric: 17.7773 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 121/1000
2023-09-27 09:33:32.758 
Epoch 121/1000 
	 loss: 17.6581, MinusLogProbMetric: 17.6581, val_loss: 17.7528, val_MinusLogProbMetric: 17.7528

Epoch 121: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.6581 - MinusLogProbMetric: 17.6581 - val_loss: 17.7528 - val_MinusLogProbMetric: 17.7528 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 122/1000
2023-09-27 09:34:37.241 
Epoch 122/1000 
	 loss: 17.6090, MinusLogProbMetric: 17.6090, val_loss: 17.9873, val_MinusLogProbMetric: 17.9873

Epoch 122: val_loss did not improve from 17.68311
196/196 - 64s - loss: 17.6090 - MinusLogProbMetric: 17.6090 - val_loss: 17.9873 - val_MinusLogProbMetric: 17.9873 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 123/1000
2023-09-27 09:35:41.834 
Epoch 123/1000 
	 loss: 17.7235, MinusLogProbMetric: 17.7235, val_loss: 18.2658, val_MinusLogProbMetric: 18.2658

Epoch 123: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.7235 - MinusLogProbMetric: 17.7235 - val_loss: 18.2658 - val_MinusLogProbMetric: 18.2658 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 124/1000
2023-09-27 09:36:46.594 
Epoch 124/1000 
	 loss: 17.6193, MinusLogProbMetric: 17.6193, val_loss: 18.5399, val_MinusLogProbMetric: 18.5399

Epoch 124: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.6193 - MinusLogProbMetric: 17.6193 - val_loss: 18.5399 - val_MinusLogProbMetric: 18.5399 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 125/1000
2023-09-27 09:37:50.966 
Epoch 125/1000 
	 loss: 17.6472, MinusLogProbMetric: 17.6472, val_loss: 18.0040, val_MinusLogProbMetric: 18.0040

Epoch 125: val_loss did not improve from 17.68311
196/196 - 64s - loss: 17.6472 - MinusLogProbMetric: 17.6472 - val_loss: 18.0040 - val_MinusLogProbMetric: 18.0040 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 126/1000
2023-09-27 09:38:55.235 
Epoch 126/1000 
	 loss: 17.5538, MinusLogProbMetric: 17.5538, val_loss: 18.3813, val_MinusLogProbMetric: 18.3813

Epoch 126: val_loss did not improve from 17.68311
196/196 - 64s - loss: 17.5538 - MinusLogProbMetric: 17.5538 - val_loss: 18.3813 - val_MinusLogProbMetric: 18.3813 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 127/1000
2023-09-27 09:39:59.037 
Epoch 127/1000 
	 loss: 17.6162, MinusLogProbMetric: 17.6162, val_loss: 17.8627, val_MinusLogProbMetric: 17.8627

Epoch 127: val_loss did not improve from 17.68311
196/196 - 64s - loss: 17.6162 - MinusLogProbMetric: 17.6162 - val_loss: 17.8627 - val_MinusLogProbMetric: 17.8627 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 128/1000
2023-09-27 09:41:03.645 
Epoch 128/1000 
	 loss: 17.5654, MinusLogProbMetric: 17.5654, val_loss: 18.5760, val_MinusLogProbMetric: 18.5760

Epoch 128: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.5654 - MinusLogProbMetric: 17.5654 - val_loss: 18.5760 - val_MinusLogProbMetric: 18.5760 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 129/1000
2023-09-27 09:42:08.232 
Epoch 129/1000 
	 loss: 17.5401, MinusLogProbMetric: 17.5401, val_loss: 18.0872, val_MinusLogProbMetric: 18.0872

Epoch 129: val_loss did not improve from 17.68311
196/196 - 65s - loss: 17.5401 - MinusLogProbMetric: 17.5401 - val_loss: 18.0872 - val_MinusLogProbMetric: 18.0872 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 130/1000
2023-09-27 09:43:12.509 
Epoch 130/1000 
	 loss: 17.5691, MinusLogProbMetric: 17.5691, val_loss: 17.6335, val_MinusLogProbMetric: 17.6335

Epoch 130: val_loss improved from 17.68311 to 17.63350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 17.5691 - MinusLogProbMetric: 17.5691 - val_loss: 17.6335 - val_MinusLogProbMetric: 17.6335 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 131/1000
2023-09-27 09:44:17.617 
Epoch 131/1000 
	 loss: 17.7313, MinusLogProbMetric: 17.7313, val_loss: 17.8069, val_MinusLogProbMetric: 17.8069

Epoch 131: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.7313 - MinusLogProbMetric: 17.7313 - val_loss: 17.8069 - val_MinusLogProbMetric: 17.8069 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 132/1000
2023-09-27 09:45:21.506 
Epoch 132/1000 
	 loss: 17.5683, MinusLogProbMetric: 17.5683, val_loss: 18.0900, val_MinusLogProbMetric: 18.0900

Epoch 132: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5683 - MinusLogProbMetric: 17.5683 - val_loss: 18.0900 - val_MinusLogProbMetric: 18.0900 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 133/1000
2023-09-27 09:46:25.519 
Epoch 133/1000 
	 loss: 17.5130, MinusLogProbMetric: 17.5130, val_loss: 17.6805, val_MinusLogProbMetric: 17.6805

Epoch 133: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5130 - MinusLogProbMetric: 17.5130 - val_loss: 17.6805 - val_MinusLogProbMetric: 17.6805 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 134/1000
2023-09-27 09:47:29.430 
Epoch 134/1000 
	 loss: 17.5859, MinusLogProbMetric: 17.5859, val_loss: 17.7217, val_MinusLogProbMetric: 17.7217

Epoch 134: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5859 - MinusLogProbMetric: 17.5859 - val_loss: 17.7217 - val_MinusLogProbMetric: 17.7217 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 135/1000
2023-09-27 09:48:33.761 
Epoch 135/1000 
	 loss: 17.5480, MinusLogProbMetric: 17.5480, val_loss: 18.0969, val_MinusLogProbMetric: 18.0969

Epoch 135: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5480 - MinusLogProbMetric: 17.5480 - val_loss: 18.0969 - val_MinusLogProbMetric: 18.0969 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 136/1000
2023-09-27 09:49:38.100 
Epoch 136/1000 
	 loss: 17.5877, MinusLogProbMetric: 17.5877, val_loss: 17.9112, val_MinusLogProbMetric: 17.9112

Epoch 136: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5877 - MinusLogProbMetric: 17.5877 - val_loss: 17.9112 - val_MinusLogProbMetric: 17.9112 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 137/1000
2023-09-27 09:50:41.952 
Epoch 137/1000 
	 loss: 17.5603, MinusLogProbMetric: 17.5603, val_loss: 18.1638, val_MinusLogProbMetric: 18.1638

Epoch 137: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5603 - MinusLogProbMetric: 17.5603 - val_loss: 18.1638 - val_MinusLogProbMetric: 18.1638 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 138/1000
2023-09-27 09:51:45.640 
Epoch 138/1000 
	 loss: 17.5090, MinusLogProbMetric: 17.5090, val_loss: 18.2030, val_MinusLogProbMetric: 18.2030

Epoch 138: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5090 - MinusLogProbMetric: 17.5090 - val_loss: 18.2030 - val_MinusLogProbMetric: 18.2030 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 139/1000
2023-09-27 09:52:49.371 
Epoch 139/1000 
	 loss: 17.5849, MinusLogProbMetric: 17.5849, val_loss: 17.6459, val_MinusLogProbMetric: 17.6459

Epoch 139: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5849 - MinusLogProbMetric: 17.5849 - val_loss: 17.6459 - val_MinusLogProbMetric: 17.6459 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 140/1000
2023-09-27 09:53:53.782 
Epoch 140/1000 
	 loss: 17.5592, MinusLogProbMetric: 17.5592, val_loss: 17.8331, val_MinusLogProbMetric: 17.8331

Epoch 140: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5592 - MinusLogProbMetric: 17.5592 - val_loss: 17.8331 - val_MinusLogProbMetric: 17.8331 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 141/1000
2023-09-27 09:54:58.433 
Epoch 141/1000 
	 loss: 17.4712, MinusLogProbMetric: 17.4712, val_loss: 18.0058, val_MinusLogProbMetric: 18.0058

Epoch 141: val_loss did not improve from 17.63350
196/196 - 65s - loss: 17.4712 - MinusLogProbMetric: 17.4712 - val_loss: 18.0058 - val_MinusLogProbMetric: 18.0058 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 142/1000
2023-09-27 09:56:02.602 
Epoch 142/1000 
	 loss: 17.5107, MinusLogProbMetric: 17.5107, val_loss: 17.7746, val_MinusLogProbMetric: 17.7746

Epoch 142: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5107 - MinusLogProbMetric: 17.5107 - val_loss: 17.7746 - val_MinusLogProbMetric: 17.7746 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 143/1000
2023-09-27 09:57:06.740 
Epoch 143/1000 
	 loss: 17.5720, MinusLogProbMetric: 17.5720, val_loss: 17.8064, val_MinusLogProbMetric: 17.8064

Epoch 143: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5720 - MinusLogProbMetric: 17.5720 - val_loss: 17.8064 - val_MinusLogProbMetric: 17.8064 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 144/1000
2023-09-27 09:58:10.599 
Epoch 144/1000 
	 loss: 17.5626, MinusLogProbMetric: 17.5626, val_loss: 17.8402, val_MinusLogProbMetric: 17.8402

Epoch 144: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5626 - MinusLogProbMetric: 17.5626 - val_loss: 17.8402 - val_MinusLogProbMetric: 17.8402 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 145/1000
2023-09-27 09:59:14.972 
Epoch 145/1000 
	 loss: 17.5094, MinusLogProbMetric: 17.5094, val_loss: 17.9300, val_MinusLogProbMetric: 17.9300

Epoch 145: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5094 - MinusLogProbMetric: 17.5094 - val_loss: 17.9300 - val_MinusLogProbMetric: 17.9300 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 146/1000
2023-09-27 10:00:19.526 
Epoch 146/1000 
	 loss: 17.5513, MinusLogProbMetric: 17.5513, val_loss: 18.1338, val_MinusLogProbMetric: 18.1338

Epoch 146: val_loss did not improve from 17.63350
196/196 - 65s - loss: 17.5513 - MinusLogProbMetric: 17.5513 - val_loss: 18.1338 - val_MinusLogProbMetric: 18.1338 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 147/1000
2023-09-27 10:01:24.104 
Epoch 147/1000 
	 loss: 17.4451, MinusLogProbMetric: 17.4451, val_loss: 17.7343, val_MinusLogProbMetric: 17.7343

Epoch 147: val_loss did not improve from 17.63350
196/196 - 65s - loss: 17.4451 - MinusLogProbMetric: 17.4451 - val_loss: 17.7343 - val_MinusLogProbMetric: 17.7343 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 148/1000
2023-09-27 10:02:28.032 
Epoch 148/1000 
	 loss: 17.4616, MinusLogProbMetric: 17.4616, val_loss: 18.0459, val_MinusLogProbMetric: 18.0459

Epoch 148: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.4616 - MinusLogProbMetric: 17.4616 - val_loss: 18.0459 - val_MinusLogProbMetric: 18.0459 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 149/1000
2023-09-27 10:03:32.511 
Epoch 149/1000 
	 loss: 17.5044, MinusLogProbMetric: 17.5044, val_loss: 17.8573, val_MinusLogProbMetric: 17.8573

Epoch 149: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.5044 - MinusLogProbMetric: 17.5044 - val_loss: 17.8573 - val_MinusLogProbMetric: 17.8573 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 150/1000
2023-09-27 10:04:36.749 
Epoch 150/1000 
	 loss: 17.4346, MinusLogProbMetric: 17.4346, val_loss: 18.2254, val_MinusLogProbMetric: 18.2254

Epoch 150: val_loss did not improve from 17.63350
196/196 - 64s - loss: 17.4346 - MinusLogProbMetric: 17.4346 - val_loss: 18.2254 - val_MinusLogProbMetric: 18.2254 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 151/1000
2023-09-27 10:05:40.866 
Epoch 151/1000 
	 loss: 17.4051, MinusLogProbMetric: 17.4051, val_loss: 17.6044, val_MinusLogProbMetric: 17.6044

Epoch 151: val_loss improved from 17.63350 to 17.60441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 17.4051 - MinusLogProbMetric: 17.4051 - val_loss: 17.6044 - val_MinusLogProbMetric: 17.6044 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 152/1000
2023-09-27 10:06:42.166 
Epoch 152/1000 
	 loss: 17.4754, MinusLogProbMetric: 17.4754, val_loss: 17.7047, val_MinusLogProbMetric: 17.7047

Epoch 152: val_loss did not improve from 17.60441
196/196 - 60s - loss: 17.4754 - MinusLogProbMetric: 17.4754 - val_loss: 17.7047 - val_MinusLogProbMetric: 17.7047 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 153/1000
2023-09-27 10:07:41.254 
Epoch 153/1000 
	 loss: 17.4635, MinusLogProbMetric: 17.4635, val_loss: 17.8941, val_MinusLogProbMetric: 17.8941

Epoch 153: val_loss did not improve from 17.60441
196/196 - 59s - loss: 17.4635 - MinusLogProbMetric: 17.4635 - val_loss: 17.8941 - val_MinusLogProbMetric: 17.8941 - lr: 3.3333e-04 - 59s/epoch - 301ms/step
Epoch 154/1000
2023-09-27 10:08:42.998 
Epoch 154/1000 
	 loss: 17.3786, MinusLogProbMetric: 17.3786, val_loss: 17.5960, val_MinusLogProbMetric: 17.5960

Epoch 154: val_loss improved from 17.60441 to 17.59605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 63s - loss: 17.3786 - MinusLogProbMetric: 17.3786 - val_loss: 17.5960 - val_MinusLogProbMetric: 17.5960 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 155/1000
2023-09-27 10:09:48.804 
Epoch 155/1000 
	 loss: 17.3984, MinusLogProbMetric: 17.3984, val_loss: 17.6366, val_MinusLogProbMetric: 17.6366

Epoch 155: val_loss did not improve from 17.59605
196/196 - 65s - loss: 17.3984 - MinusLogProbMetric: 17.3984 - val_loss: 17.6366 - val_MinusLogProbMetric: 17.6366 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 156/1000
2023-09-27 10:10:52.878 
Epoch 156/1000 
	 loss: 17.4342, MinusLogProbMetric: 17.4342, val_loss: 17.6497, val_MinusLogProbMetric: 17.6497

Epoch 156: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.4342 - MinusLogProbMetric: 17.4342 - val_loss: 17.6497 - val_MinusLogProbMetric: 17.6497 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 157/1000
2023-09-27 10:11:56.819 
Epoch 157/1000 
	 loss: 17.4499, MinusLogProbMetric: 17.4499, val_loss: 17.9546, val_MinusLogProbMetric: 17.9546

Epoch 157: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.4499 - MinusLogProbMetric: 17.4499 - val_loss: 17.9546 - val_MinusLogProbMetric: 17.9546 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 158/1000
2023-09-27 10:13:00.928 
Epoch 158/1000 
	 loss: 17.3976, MinusLogProbMetric: 17.3976, val_loss: 17.7720, val_MinusLogProbMetric: 17.7720

Epoch 158: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.3976 - MinusLogProbMetric: 17.3976 - val_loss: 17.7720 - val_MinusLogProbMetric: 17.7720 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 159/1000
2023-09-27 10:14:04.755 
Epoch 159/1000 
	 loss: 17.4371, MinusLogProbMetric: 17.4371, val_loss: 17.8766, val_MinusLogProbMetric: 17.8766

Epoch 159: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.4371 - MinusLogProbMetric: 17.4371 - val_loss: 17.8766 - val_MinusLogProbMetric: 17.8766 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 160/1000
2023-09-27 10:15:08.465 
Epoch 160/1000 
	 loss: 17.4171, MinusLogProbMetric: 17.4171, val_loss: 17.7653, val_MinusLogProbMetric: 17.7653

Epoch 160: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.4171 - MinusLogProbMetric: 17.4171 - val_loss: 17.7653 - val_MinusLogProbMetric: 17.7653 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 161/1000
2023-09-27 10:16:12.868 
Epoch 161/1000 
	 loss: 17.3785, MinusLogProbMetric: 17.3785, val_loss: 17.7262, val_MinusLogProbMetric: 17.7262

Epoch 161: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.3785 - MinusLogProbMetric: 17.3785 - val_loss: 17.7262 - val_MinusLogProbMetric: 17.7262 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 162/1000
2023-09-27 10:17:17.016 
Epoch 162/1000 
	 loss: 17.3593, MinusLogProbMetric: 17.3593, val_loss: 17.6535, val_MinusLogProbMetric: 17.6535

Epoch 162: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.3593 - MinusLogProbMetric: 17.3593 - val_loss: 17.6535 - val_MinusLogProbMetric: 17.6535 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 163/1000
2023-09-27 10:18:20.839 
Epoch 163/1000 
	 loss: 17.3459, MinusLogProbMetric: 17.3459, val_loss: 17.7420, val_MinusLogProbMetric: 17.7420

Epoch 163: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.3459 - MinusLogProbMetric: 17.3459 - val_loss: 17.7420 - val_MinusLogProbMetric: 17.7420 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 164/1000
2023-09-27 10:19:25.110 
Epoch 164/1000 
	 loss: 17.3944, MinusLogProbMetric: 17.3944, val_loss: 19.0318, val_MinusLogProbMetric: 19.0318

Epoch 164: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.3944 - MinusLogProbMetric: 17.3944 - val_loss: 19.0318 - val_MinusLogProbMetric: 19.0318 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 165/1000
2023-09-27 10:20:29.304 
Epoch 165/1000 
	 loss: 17.3788, MinusLogProbMetric: 17.3788, val_loss: 18.0758, val_MinusLogProbMetric: 18.0758

Epoch 165: val_loss did not improve from 17.59605
196/196 - 64s - loss: 17.3788 - MinusLogProbMetric: 17.3788 - val_loss: 18.0758 - val_MinusLogProbMetric: 18.0758 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 166/1000
2023-09-27 10:21:33.090 
Epoch 166/1000 
	 loss: 17.3190, MinusLogProbMetric: 17.3190, val_loss: 17.5745, val_MinusLogProbMetric: 17.5745

Epoch 166: val_loss improved from 17.59605 to 17.57452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 17.3190 - MinusLogProbMetric: 17.3190 - val_loss: 17.5745 - val_MinusLogProbMetric: 17.5745 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 167/1000
2023-09-27 10:22:38.244 
Epoch 167/1000 
	 loss: 17.5169, MinusLogProbMetric: 17.5169, val_loss: 17.8588, val_MinusLogProbMetric: 17.8588

Epoch 167: val_loss did not improve from 17.57452
196/196 - 64s - loss: 17.5169 - MinusLogProbMetric: 17.5169 - val_loss: 17.8588 - val_MinusLogProbMetric: 17.8588 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 168/1000
2023-09-27 10:23:42.373 
Epoch 168/1000 
	 loss: 17.4042, MinusLogProbMetric: 17.4042, val_loss: 17.8164, val_MinusLogProbMetric: 17.8164

Epoch 168: val_loss did not improve from 17.57452
196/196 - 64s - loss: 17.4042 - MinusLogProbMetric: 17.4042 - val_loss: 17.8164 - val_MinusLogProbMetric: 17.8164 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 169/1000
2023-09-27 10:24:46.612 
Epoch 169/1000 
	 loss: 17.3130, MinusLogProbMetric: 17.3130, val_loss: 17.8598, val_MinusLogProbMetric: 17.8598

Epoch 169: val_loss did not improve from 17.57452
196/196 - 64s - loss: 17.3130 - MinusLogProbMetric: 17.3130 - val_loss: 17.8598 - val_MinusLogProbMetric: 17.8598 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 170/1000
2023-09-27 10:25:50.512 
Epoch 170/1000 
	 loss: 17.3392, MinusLogProbMetric: 17.3392, val_loss: 17.9670, val_MinusLogProbMetric: 17.9670

Epoch 170: val_loss did not improve from 17.57452
196/196 - 64s - loss: 17.3392 - MinusLogProbMetric: 17.3392 - val_loss: 17.9670 - val_MinusLogProbMetric: 17.9670 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 171/1000
2023-09-27 10:26:54.349 
Epoch 171/1000 
	 loss: 17.3381, MinusLogProbMetric: 17.3381, val_loss: 17.5642, val_MinusLogProbMetric: 17.5642

Epoch 171: val_loss improved from 17.57452 to 17.56421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 17.3381 - MinusLogProbMetric: 17.3381 - val_loss: 17.5642 - val_MinusLogProbMetric: 17.5642 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 172/1000
2023-09-27 10:27:59.518 
Epoch 172/1000 
	 loss: 17.3392, MinusLogProbMetric: 17.3392, val_loss: 18.3188, val_MinusLogProbMetric: 18.3188

Epoch 172: val_loss did not improve from 17.56421
196/196 - 64s - loss: 17.3392 - MinusLogProbMetric: 17.3392 - val_loss: 18.3188 - val_MinusLogProbMetric: 18.3188 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 173/1000
2023-09-27 10:29:03.532 
Epoch 173/1000 
	 loss: 17.3998, MinusLogProbMetric: 17.3998, val_loss: 18.0029, val_MinusLogProbMetric: 18.0029

Epoch 173: val_loss did not improve from 17.56421
196/196 - 64s - loss: 17.3998 - MinusLogProbMetric: 17.3998 - val_loss: 18.0029 - val_MinusLogProbMetric: 18.0029 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 174/1000
2023-09-27 10:30:07.829 
Epoch 174/1000 
	 loss: 17.3361, MinusLogProbMetric: 17.3361, val_loss: 18.1222, val_MinusLogProbMetric: 18.1222

Epoch 174: val_loss did not improve from 17.56421
196/196 - 64s - loss: 17.3361 - MinusLogProbMetric: 17.3361 - val_loss: 18.1222 - val_MinusLogProbMetric: 18.1222 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 175/1000
2023-09-27 10:31:11.806 
Epoch 175/1000 
	 loss: 17.3195, MinusLogProbMetric: 17.3195, val_loss: 17.4644, val_MinusLogProbMetric: 17.4644

Epoch 175: val_loss improved from 17.56421 to 17.46442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 17.3195 - MinusLogProbMetric: 17.3195 - val_loss: 17.4644 - val_MinusLogProbMetric: 17.4644 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 176/1000
2023-09-27 10:32:17.095 
Epoch 176/1000 
	 loss: 17.3030, MinusLogProbMetric: 17.3030, val_loss: 17.7067, val_MinusLogProbMetric: 17.7067

Epoch 176: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3030 - MinusLogProbMetric: 17.3030 - val_loss: 17.7067 - val_MinusLogProbMetric: 17.7067 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 177/1000
2023-09-27 10:33:21.245 
Epoch 177/1000 
	 loss: 17.3650, MinusLogProbMetric: 17.3650, val_loss: 17.6854, val_MinusLogProbMetric: 17.6854

Epoch 177: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3650 - MinusLogProbMetric: 17.3650 - val_loss: 17.6854 - val_MinusLogProbMetric: 17.6854 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 178/1000
2023-09-27 10:34:25.317 
Epoch 178/1000 
	 loss: 17.3476, MinusLogProbMetric: 17.3476, val_loss: 17.9710, val_MinusLogProbMetric: 17.9710

Epoch 178: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3476 - MinusLogProbMetric: 17.3476 - val_loss: 17.9710 - val_MinusLogProbMetric: 17.9710 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 179/1000
2023-09-27 10:35:29.270 
Epoch 179/1000 
	 loss: 17.3920, MinusLogProbMetric: 17.3920, val_loss: 17.6450, val_MinusLogProbMetric: 17.6450

Epoch 179: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3920 - MinusLogProbMetric: 17.3920 - val_loss: 17.6450 - val_MinusLogProbMetric: 17.6450 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 180/1000
2023-09-27 10:36:33.200 
Epoch 180/1000 
	 loss: 17.2808, MinusLogProbMetric: 17.2808, val_loss: 17.7237, val_MinusLogProbMetric: 17.7237

Epoch 180: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2808 - MinusLogProbMetric: 17.2808 - val_loss: 17.7237 - val_MinusLogProbMetric: 17.7237 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 181/1000
2023-09-27 10:37:37.240 
Epoch 181/1000 
	 loss: 17.3040, MinusLogProbMetric: 17.3040, val_loss: 17.5658, val_MinusLogProbMetric: 17.5658

Epoch 181: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3040 - MinusLogProbMetric: 17.3040 - val_loss: 17.5658 - val_MinusLogProbMetric: 17.5658 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 182/1000
2023-09-27 10:38:41.199 
Epoch 182/1000 
	 loss: 17.3408, MinusLogProbMetric: 17.3408, val_loss: 17.6133, val_MinusLogProbMetric: 17.6133

Epoch 182: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3408 - MinusLogProbMetric: 17.3408 - val_loss: 17.6133 - val_MinusLogProbMetric: 17.6133 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 183/1000
2023-09-27 10:39:45.171 
Epoch 183/1000 
	 loss: 17.2907, MinusLogProbMetric: 17.2907, val_loss: 17.8944, val_MinusLogProbMetric: 17.8944

Epoch 183: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2907 - MinusLogProbMetric: 17.2907 - val_loss: 17.8944 - val_MinusLogProbMetric: 17.8944 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 184/1000
2023-09-27 10:40:49.247 
Epoch 184/1000 
	 loss: 17.2880, MinusLogProbMetric: 17.2880, val_loss: 17.7062, val_MinusLogProbMetric: 17.7062

Epoch 184: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2880 - MinusLogProbMetric: 17.2880 - val_loss: 17.7062 - val_MinusLogProbMetric: 17.7062 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 185/1000
2023-09-27 10:41:53.366 
Epoch 185/1000 
	 loss: 17.2653, MinusLogProbMetric: 17.2653, val_loss: 17.6603, val_MinusLogProbMetric: 17.6603

Epoch 185: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2653 - MinusLogProbMetric: 17.2653 - val_loss: 17.6603 - val_MinusLogProbMetric: 17.6603 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 186/1000
2023-09-27 10:42:56.930 
Epoch 186/1000 
	 loss: 17.2517, MinusLogProbMetric: 17.2517, val_loss: 17.5013, val_MinusLogProbMetric: 17.5013

Epoch 186: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2517 - MinusLogProbMetric: 17.2517 - val_loss: 17.5013 - val_MinusLogProbMetric: 17.5013 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 187/1000
2023-09-27 10:44:01.166 
Epoch 187/1000 
	 loss: 17.3428, MinusLogProbMetric: 17.3428, val_loss: 17.6237, val_MinusLogProbMetric: 17.6237

Epoch 187: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3428 - MinusLogProbMetric: 17.3428 - val_loss: 17.6237 - val_MinusLogProbMetric: 17.6237 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 188/1000
2023-09-27 10:45:04.876 
Epoch 188/1000 
	 loss: 17.2900, MinusLogProbMetric: 17.2900, val_loss: 17.9063, val_MinusLogProbMetric: 17.9063

Epoch 188: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2900 - MinusLogProbMetric: 17.2900 - val_loss: 17.9063 - val_MinusLogProbMetric: 17.9063 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 189/1000
2023-09-27 10:46:08.521 
Epoch 189/1000 
	 loss: 17.2813, MinusLogProbMetric: 17.2813, val_loss: 18.0329, val_MinusLogProbMetric: 18.0329

Epoch 189: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2813 - MinusLogProbMetric: 17.2813 - val_loss: 18.0329 - val_MinusLogProbMetric: 18.0329 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 190/1000
2023-09-27 10:47:12.826 
Epoch 190/1000 
	 loss: 17.3144, MinusLogProbMetric: 17.3144, val_loss: 17.7015, val_MinusLogProbMetric: 17.7015

Epoch 190: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.3144 - MinusLogProbMetric: 17.3144 - val_loss: 17.7015 - val_MinusLogProbMetric: 17.7015 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 191/1000
2023-09-27 10:48:16.681 
Epoch 191/1000 
	 loss: 17.2535, MinusLogProbMetric: 17.2535, val_loss: 17.7673, val_MinusLogProbMetric: 17.7673

Epoch 191: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2535 - MinusLogProbMetric: 17.2535 - val_loss: 17.7673 - val_MinusLogProbMetric: 17.7673 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 192/1000
2023-09-27 10:49:20.468 
Epoch 192/1000 
	 loss: 17.2203, MinusLogProbMetric: 17.2203, val_loss: 17.5824, val_MinusLogProbMetric: 17.5824

Epoch 192: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2203 - MinusLogProbMetric: 17.2203 - val_loss: 17.5824 - val_MinusLogProbMetric: 17.5824 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 193/1000
2023-09-27 10:50:24.501 
Epoch 193/1000 
	 loss: 17.2873, MinusLogProbMetric: 17.2873, val_loss: 17.5919, val_MinusLogProbMetric: 17.5919

Epoch 193: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2873 - MinusLogProbMetric: 17.2873 - val_loss: 17.5919 - val_MinusLogProbMetric: 17.5919 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 194/1000
2023-09-27 10:51:28.192 
Epoch 194/1000 
	 loss: 17.2192, MinusLogProbMetric: 17.2192, val_loss: 17.5701, val_MinusLogProbMetric: 17.5701

Epoch 194: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2192 - MinusLogProbMetric: 17.2192 - val_loss: 17.5701 - val_MinusLogProbMetric: 17.5701 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 195/1000
2023-09-27 10:52:31.849 
Epoch 195/1000 
	 loss: 17.2658, MinusLogProbMetric: 17.2658, val_loss: 17.8078, val_MinusLogProbMetric: 17.8078

Epoch 195: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2658 - MinusLogProbMetric: 17.2658 - val_loss: 17.8078 - val_MinusLogProbMetric: 17.8078 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 196/1000
2023-09-27 10:53:35.509 
Epoch 196/1000 
	 loss: 17.2740, MinusLogProbMetric: 17.2740, val_loss: 17.6950, val_MinusLogProbMetric: 17.6950

Epoch 196: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2740 - MinusLogProbMetric: 17.2740 - val_loss: 17.6950 - val_MinusLogProbMetric: 17.6950 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 197/1000
2023-09-27 10:54:39.267 
Epoch 197/1000 
	 loss: 17.2440, MinusLogProbMetric: 17.2440, val_loss: 17.7529, val_MinusLogProbMetric: 17.7529

Epoch 197: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2440 - MinusLogProbMetric: 17.2440 - val_loss: 17.7529 - val_MinusLogProbMetric: 17.7529 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 198/1000
2023-09-27 10:55:42.811 
Epoch 198/1000 
	 loss: 17.2484, MinusLogProbMetric: 17.2484, val_loss: 17.7294, val_MinusLogProbMetric: 17.7294

Epoch 198: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2484 - MinusLogProbMetric: 17.2484 - val_loss: 17.7294 - val_MinusLogProbMetric: 17.7294 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 199/1000
2023-09-27 10:56:46.767 
Epoch 199/1000 
	 loss: 17.2248, MinusLogProbMetric: 17.2248, val_loss: 18.1133, val_MinusLogProbMetric: 18.1133

Epoch 199: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2248 - MinusLogProbMetric: 17.2248 - val_loss: 18.1133 - val_MinusLogProbMetric: 18.1133 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 200/1000
2023-09-27 10:57:50.969 
Epoch 200/1000 
	 loss: 17.2250, MinusLogProbMetric: 17.2250, val_loss: 17.7946, val_MinusLogProbMetric: 17.7946

Epoch 200: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2250 - MinusLogProbMetric: 17.2250 - val_loss: 17.7946 - val_MinusLogProbMetric: 17.7946 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 201/1000
2023-09-27 10:58:54.668 
Epoch 201/1000 
	 loss: 17.2156, MinusLogProbMetric: 17.2156, val_loss: 17.7912, val_MinusLogProbMetric: 17.7912

Epoch 201: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2156 - MinusLogProbMetric: 17.2156 - val_loss: 17.7912 - val_MinusLogProbMetric: 17.7912 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 202/1000
2023-09-27 10:59:58.551 
Epoch 202/1000 
	 loss: 17.2220, MinusLogProbMetric: 17.2220, val_loss: 17.6153, val_MinusLogProbMetric: 17.6153

Epoch 202: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2220 - MinusLogProbMetric: 17.2220 - val_loss: 17.6153 - val_MinusLogProbMetric: 17.6153 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 203/1000
2023-09-27 11:01:02.247 
Epoch 203/1000 
	 loss: 17.1978, MinusLogProbMetric: 17.1978, val_loss: 17.5397, val_MinusLogProbMetric: 17.5397

Epoch 203: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.1978 - MinusLogProbMetric: 17.1978 - val_loss: 17.5397 - val_MinusLogProbMetric: 17.5397 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 204/1000
2023-09-27 11:02:06.098 
Epoch 204/1000 
	 loss: 17.1978, MinusLogProbMetric: 17.1978, val_loss: 17.9336, val_MinusLogProbMetric: 17.9336

Epoch 204: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.1978 - MinusLogProbMetric: 17.1978 - val_loss: 17.9336 - val_MinusLogProbMetric: 17.9336 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 205/1000
2023-09-27 11:03:10.228 
Epoch 205/1000 
	 loss: 17.1793, MinusLogProbMetric: 17.1793, val_loss: 17.6947, val_MinusLogProbMetric: 17.6947

Epoch 205: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.1793 - MinusLogProbMetric: 17.1793 - val_loss: 17.6947 - val_MinusLogProbMetric: 17.6947 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 206/1000
2023-09-27 11:04:14.171 
Epoch 206/1000 
	 loss: 17.2434, MinusLogProbMetric: 17.2434, val_loss: 17.5997, val_MinusLogProbMetric: 17.5997

Epoch 206: val_loss did not improve from 17.46442
196/196 - 64s - loss: 17.2434 - MinusLogProbMetric: 17.2434 - val_loss: 17.5997 - val_MinusLogProbMetric: 17.5997 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 207/1000
2023-09-27 11:05:17.213 
Epoch 207/1000 
	 loss: 17.1601, MinusLogProbMetric: 17.1601, val_loss: 17.5691, val_MinusLogProbMetric: 17.5691

Epoch 207: val_loss did not improve from 17.46442
196/196 - 63s - loss: 17.1601 - MinusLogProbMetric: 17.1601 - val_loss: 17.5691 - val_MinusLogProbMetric: 17.5691 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 208/1000
2023-09-27 11:06:20.533 
Epoch 208/1000 
	 loss: 17.1961, MinusLogProbMetric: 17.1961, val_loss: 17.3861, val_MinusLogProbMetric: 17.3861

Epoch 208: val_loss improved from 17.46442 to 17.38609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 64s - loss: 17.1961 - MinusLogProbMetric: 17.1961 - val_loss: 17.3861 - val_MinusLogProbMetric: 17.3861 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 209/1000
2023-09-27 11:07:25.564 
Epoch 209/1000 
	 loss: 17.1575, MinusLogProbMetric: 17.1575, val_loss: 17.5396, val_MinusLogProbMetric: 17.5396

Epoch 209: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1575 - MinusLogProbMetric: 17.1575 - val_loss: 17.5396 - val_MinusLogProbMetric: 17.5396 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 210/1000
2023-09-27 11:08:29.314 
Epoch 210/1000 
	 loss: 17.1762, MinusLogProbMetric: 17.1762, val_loss: 17.6818, val_MinusLogProbMetric: 17.6818

Epoch 210: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1762 - MinusLogProbMetric: 17.1762 - val_loss: 17.6818 - val_MinusLogProbMetric: 17.6818 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 211/1000
2023-09-27 11:09:33.093 
Epoch 211/1000 
	 loss: 17.1457, MinusLogProbMetric: 17.1457, val_loss: 17.5362, val_MinusLogProbMetric: 17.5362

Epoch 211: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1457 - MinusLogProbMetric: 17.1457 - val_loss: 17.5362 - val_MinusLogProbMetric: 17.5362 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 212/1000
2023-09-27 11:10:36.927 
Epoch 212/1000 
	 loss: 17.1766, MinusLogProbMetric: 17.1766, val_loss: 17.5794, val_MinusLogProbMetric: 17.5794

Epoch 212: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1766 - MinusLogProbMetric: 17.1766 - val_loss: 17.5794 - val_MinusLogProbMetric: 17.5794 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 213/1000
2023-09-27 11:11:37.523 
Epoch 213/1000 
	 loss: 17.1792, MinusLogProbMetric: 17.1792, val_loss: 17.6648, val_MinusLogProbMetric: 17.6648

Epoch 213: val_loss did not improve from 17.38609
196/196 - 61s - loss: 17.1792 - MinusLogProbMetric: 17.1792 - val_loss: 17.6648 - val_MinusLogProbMetric: 17.6648 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 214/1000
2023-09-27 11:12:41.123 
Epoch 214/1000 
	 loss: 17.1182, MinusLogProbMetric: 17.1182, val_loss: 17.7599, val_MinusLogProbMetric: 17.7599

Epoch 214: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1182 - MinusLogProbMetric: 17.1182 - val_loss: 17.7599 - val_MinusLogProbMetric: 17.7599 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 215/1000
2023-09-27 11:13:44.652 
Epoch 215/1000 
	 loss: 17.1663, MinusLogProbMetric: 17.1663, val_loss: 17.6889, val_MinusLogProbMetric: 17.6889

Epoch 215: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1663 - MinusLogProbMetric: 17.1663 - val_loss: 17.6889 - val_MinusLogProbMetric: 17.6889 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 216/1000
2023-09-27 11:14:48.010 
Epoch 216/1000 
	 loss: 17.1901, MinusLogProbMetric: 17.1901, val_loss: 17.5943, val_MinusLogProbMetric: 17.5943

Epoch 216: val_loss did not improve from 17.38609
196/196 - 63s - loss: 17.1901 - MinusLogProbMetric: 17.1901 - val_loss: 17.5943 - val_MinusLogProbMetric: 17.5943 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 217/1000
2023-09-27 11:15:51.339 
Epoch 217/1000 
	 loss: 17.1771, MinusLogProbMetric: 17.1771, val_loss: 17.5688, val_MinusLogProbMetric: 17.5688

Epoch 217: val_loss did not improve from 17.38609
196/196 - 63s - loss: 17.1771 - MinusLogProbMetric: 17.1771 - val_loss: 17.5688 - val_MinusLogProbMetric: 17.5688 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 218/1000
2023-09-27 11:16:55.234 
Epoch 218/1000 
	 loss: 17.1258, MinusLogProbMetric: 17.1258, val_loss: 17.5696, val_MinusLogProbMetric: 17.5696

Epoch 218: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1258 - MinusLogProbMetric: 17.1258 - val_loss: 17.5696 - val_MinusLogProbMetric: 17.5696 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 219/1000
2023-09-27 11:17:59.224 
Epoch 219/1000 
	 loss: 17.1344, MinusLogProbMetric: 17.1344, val_loss: 17.6305, val_MinusLogProbMetric: 17.6305

Epoch 219: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1344 - MinusLogProbMetric: 17.1344 - val_loss: 17.6305 - val_MinusLogProbMetric: 17.6305 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 220/1000
2023-09-27 11:19:02.678 
Epoch 220/1000 
	 loss: 17.1868, MinusLogProbMetric: 17.1868, val_loss: 17.9442, val_MinusLogProbMetric: 17.9442

Epoch 220: val_loss did not improve from 17.38609
196/196 - 63s - loss: 17.1868 - MinusLogProbMetric: 17.1868 - val_loss: 17.9442 - val_MinusLogProbMetric: 17.9442 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 221/1000
2023-09-27 11:20:06.772 
Epoch 221/1000 
	 loss: 17.1626, MinusLogProbMetric: 17.1626, val_loss: 17.7437, val_MinusLogProbMetric: 17.7437

Epoch 221: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1626 - MinusLogProbMetric: 17.1626 - val_loss: 17.7437 - val_MinusLogProbMetric: 17.7437 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 222/1000
2023-09-27 11:21:10.416 
Epoch 222/1000 
	 loss: 17.1639, MinusLogProbMetric: 17.1639, val_loss: 17.6059, val_MinusLogProbMetric: 17.6059

Epoch 222: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1639 - MinusLogProbMetric: 17.1639 - val_loss: 17.6059 - val_MinusLogProbMetric: 17.6059 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 223/1000
2023-09-27 11:22:14.265 
Epoch 223/1000 
	 loss: 17.1238, MinusLogProbMetric: 17.1238, val_loss: 17.8704, val_MinusLogProbMetric: 17.8704

Epoch 223: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1238 - MinusLogProbMetric: 17.1238 - val_loss: 17.8704 - val_MinusLogProbMetric: 17.8704 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 224/1000
2023-09-27 11:23:18.491 
Epoch 224/1000 
	 loss: 17.1610, MinusLogProbMetric: 17.1610, val_loss: 17.6487, val_MinusLogProbMetric: 17.6487

Epoch 224: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1610 - MinusLogProbMetric: 17.1610 - val_loss: 17.6487 - val_MinusLogProbMetric: 17.6487 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 225/1000
2023-09-27 11:24:22.190 
Epoch 225/1000 
	 loss: 17.1268, MinusLogProbMetric: 17.1268, val_loss: 17.5107, val_MinusLogProbMetric: 17.5107

Epoch 225: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1268 - MinusLogProbMetric: 17.1268 - val_loss: 17.5107 - val_MinusLogProbMetric: 17.5107 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 226/1000
2023-09-27 11:25:25.863 
Epoch 226/1000 
	 loss: 17.0959, MinusLogProbMetric: 17.0959, val_loss: 17.5733, val_MinusLogProbMetric: 17.5733

Epoch 226: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.0959 - MinusLogProbMetric: 17.0959 - val_loss: 17.5733 - val_MinusLogProbMetric: 17.5733 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 227/1000
2023-09-27 11:26:29.521 
Epoch 227/1000 
	 loss: 17.1112, MinusLogProbMetric: 17.1112, val_loss: 17.4572, val_MinusLogProbMetric: 17.4572

Epoch 227: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1112 - MinusLogProbMetric: 17.1112 - val_loss: 17.4572 - val_MinusLogProbMetric: 17.4572 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 228/1000
2023-09-27 11:27:33.515 
Epoch 228/1000 
	 loss: 17.1437, MinusLogProbMetric: 17.1437, val_loss: 17.4596, val_MinusLogProbMetric: 17.4596

Epoch 228: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.1437 - MinusLogProbMetric: 17.1437 - val_loss: 17.4596 - val_MinusLogProbMetric: 17.4596 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 229/1000
2023-09-27 11:28:37.590 
Epoch 229/1000 
	 loss: 17.0975, MinusLogProbMetric: 17.0975, val_loss: 17.5763, val_MinusLogProbMetric: 17.5763

Epoch 229: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.0975 - MinusLogProbMetric: 17.0975 - val_loss: 17.5763 - val_MinusLogProbMetric: 17.5763 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 230/1000
2023-09-27 11:29:40.949 
Epoch 230/1000 
	 loss: 17.1465, MinusLogProbMetric: 17.1465, val_loss: 17.7414, val_MinusLogProbMetric: 17.7414

Epoch 230: val_loss did not improve from 17.38609
196/196 - 63s - loss: 17.1465 - MinusLogProbMetric: 17.1465 - val_loss: 17.7414 - val_MinusLogProbMetric: 17.7414 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 231/1000
2023-09-27 11:30:44.691 
Epoch 231/1000 
	 loss: 17.0782, MinusLogProbMetric: 17.0782, val_loss: 17.4631, val_MinusLogProbMetric: 17.4631

Epoch 231: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.0782 - MinusLogProbMetric: 17.0782 - val_loss: 17.4631 - val_MinusLogProbMetric: 17.4631 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 232/1000
2023-09-27 11:31:48.701 
Epoch 232/1000 
	 loss: 17.0933, MinusLogProbMetric: 17.0933, val_loss: 17.8715, val_MinusLogProbMetric: 17.8715

Epoch 232: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.0933 - MinusLogProbMetric: 17.0933 - val_loss: 17.8715 - val_MinusLogProbMetric: 17.8715 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 233/1000
2023-09-27 11:32:52.620 
Epoch 233/1000 
	 loss: 17.0575, MinusLogProbMetric: 17.0575, val_loss: 17.5233, val_MinusLogProbMetric: 17.5233

Epoch 233: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.0575 - MinusLogProbMetric: 17.0575 - val_loss: 17.5233 - val_MinusLogProbMetric: 17.5233 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 234/1000
2023-09-27 11:33:56.755 
Epoch 234/1000 
	 loss: 17.0664, MinusLogProbMetric: 17.0664, val_loss: 17.6200, val_MinusLogProbMetric: 17.6200

Epoch 234: val_loss did not improve from 17.38609
196/196 - 64s - loss: 17.0664 - MinusLogProbMetric: 17.0664 - val_loss: 17.6200 - val_MinusLogProbMetric: 17.6200 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 235/1000
2023-09-27 11:35:01.032 
Epoch 235/1000 
	 loss: 17.0640, MinusLogProbMetric: 17.0640, val_loss: 17.3448, val_MinusLogProbMetric: 17.3448

Epoch 235: val_loss improved from 17.38609 to 17.34477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 17.0640 - MinusLogProbMetric: 17.0640 - val_loss: 17.3448 - val_MinusLogProbMetric: 17.3448 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 236/1000
2023-09-27 11:36:05.859 
Epoch 236/1000 
	 loss: 17.0899, MinusLogProbMetric: 17.0899, val_loss: 17.6660, val_MinusLogProbMetric: 17.6660

Epoch 236: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0899 - MinusLogProbMetric: 17.0899 - val_loss: 17.6660 - val_MinusLogProbMetric: 17.6660 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 237/1000
2023-09-27 11:37:09.744 
Epoch 237/1000 
	 loss: 17.1093, MinusLogProbMetric: 17.1093, val_loss: 17.5647, val_MinusLogProbMetric: 17.5647

Epoch 237: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.1093 - MinusLogProbMetric: 17.1093 - val_loss: 17.5647 - val_MinusLogProbMetric: 17.5647 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 238/1000
2023-09-27 11:38:13.998 
Epoch 238/1000 
	 loss: 17.0883, MinusLogProbMetric: 17.0883, val_loss: 17.5954, val_MinusLogProbMetric: 17.5954

Epoch 238: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0883 - MinusLogProbMetric: 17.0883 - val_loss: 17.5954 - val_MinusLogProbMetric: 17.5954 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 239/1000
2023-09-27 11:39:17.815 
Epoch 239/1000 
	 loss: 17.0682, MinusLogProbMetric: 17.0682, val_loss: 17.6004, val_MinusLogProbMetric: 17.6004

Epoch 239: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0682 - MinusLogProbMetric: 17.0682 - val_loss: 17.6004 - val_MinusLogProbMetric: 17.6004 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 240/1000
2023-09-27 11:40:21.803 
Epoch 240/1000 
	 loss: 17.0918, MinusLogProbMetric: 17.0918, val_loss: 17.5127, val_MinusLogProbMetric: 17.5127

Epoch 240: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0918 - MinusLogProbMetric: 17.0918 - val_loss: 17.5127 - val_MinusLogProbMetric: 17.5127 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 241/1000
2023-09-27 11:41:25.848 
Epoch 241/1000 
	 loss: 17.1315, MinusLogProbMetric: 17.1315, val_loss: 17.4238, val_MinusLogProbMetric: 17.4238

Epoch 241: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.1315 - MinusLogProbMetric: 17.1315 - val_loss: 17.4238 - val_MinusLogProbMetric: 17.4238 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 242/1000
2023-09-27 11:42:30.190 
Epoch 242/1000 
	 loss: 17.0598, MinusLogProbMetric: 17.0598, val_loss: 17.5134, val_MinusLogProbMetric: 17.5134

Epoch 242: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0598 - MinusLogProbMetric: 17.0598 - val_loss: 17.5134 - val_MinusLogProbMetric: 17.5134 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 243/1000
2023-09-27 11:43:34.072 
Epoch 243/1000 
	 loss: 17.0472, MinusLogProbMetric: 17.0472, val_loss: 17.5054, val_MinusLogProbMetric: 17.5054

Epoch 243: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0472 - MinusLogProbMetric: 17.0472 - val_loss: 17.5054 - val_MinusLogProbMetric: 17.5054 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 244/1000
2023-09-27 11:44:37.953 
Epoch 244/1000 
	 loss: 17.1135, MinusLogProbMetric: 17.1135, val_loss: 18.0574, val_MinusLogProbMetric: 18.0574

Epoch 244: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.1135 - MinusLogProbMetric: 17.1135 - val_loss: 18.0574 - val_MinusLogProbMetric: 18.0574 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 245/1000
2023-09-27 11:45:42.100 
Epoch 245/1000 
	 loss: 17.0678, MinusLogProbMetric: 17.0678, val_loss: 17.5423, val_MinusLogProbMetric: 17.5423

Epoch 245: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0678 - MinusLogProbMetric: 17.0678 - val_loss: 17.5423 - val_MinusLogProbMetric: 17.5423 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 246/1000
2023-09-27 11:46:45.921 
Epoch 246/1000 
	 loss: 17.1146, MinusLogProbMetric: 17.1146, val_loss: 17.5164, val_MinusLogProbMetric: 17.5164

Epoch 246: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.1146 - MinusLogProbMetric: 17.1146 - val_loss: 17.5164 - val_MinusLogProbMetric: 17.5164 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 247/1000
2023-09-27 11:47:49.831 
Epoch 247/1000 
	 loss: 17.0629, MinusLogProbMetric: 17.0629, val_loss: 17.6605, val_MinusLogProbMetric: 17.6605

Epoch 247: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0629 - MinusLogProbMetric: 17.0629 - val_loss: 17.6605 - val_MinusLogProbMetric: 17.6605 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 248/1000
2023-09-27 11:48:53.902 
Epoch 248/1000 
	 loss: 17.0603, MinusLogProbMetric: 17.0603, val_loss: 17.4441, val_MinusLogProbMetric: 17.4441

Epoch 248: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0603 - MinusLogProbMetric: 17.0603 - val_loss: 17.4441 - val_MinusLogProbMetric: 17.4441 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 249/1000
2023-09-27 11:49:58.040 
Epoch 249/1000 
	 loss: 17.0707, MinusLogProbMetric: 17.0707, val_loss: 17.5148, val_MinusLogProbMetric: 17.5148

Epoch 249: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0707 - MinusLogProbMetric: 17.0707 - val_loss: 17.5148 - val_MinusLogProbMetric: 17.5148 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 250/1000
2023-09-27 11:51:02.163 
Epoch 250/1000 
	 loss: 17.1168, MinusLogProbMetric: 17.1168, val_loss: 17.7757, val_MinusLogProbMetric: 17.7757

Epoch 250: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.1168 - MinusLogProbMetric: 17.1168 - val_loss: 17.7757 - val_MinusLogProbMetric: 17.7757 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 251/1000
2023-09-27 11:52:06.397 
Epoch 251/1000 
	 loss: 17.0359, MinusLogProbMetric: 17.0359, val_loss: 17.6745, val_MinusLogProbMetric: 17.6745

Epoch 251: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0359 - MinusLogProbMetric: 17.0359 - val_loss: 17.6745 - val_MinusLogProbMetric: 17.6745 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 252/1000
2023-09-27 11:53:10.266 
Epoch 252/1000 
	 loss: 17.0535, MinusLogProbMetric: 17.0535, val_loss: 17.7177, val_MinusLogProbMetric: 17.7177

Epoch 252: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0535 - MinusLogProbMetric: 17.0535 - val_loss: 17.7177 - val_MinusLogProbMetric: 17.7177 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 253/1000
2023-09-27 11:54:14.526 
Epoch 253/1000 
	 loss: 17.0583, MinusLogProbMetric: 17.0583, val_loss: 18.1508, val_MinusLogProbMetric: 18.1508

Epoch 253: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0583 - MinusLogProbMetric: 17.0583 - val_loss: 18.1508 - val_MinusLogProbMetric: 18.1508 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 254/1000
2023-09-27 11:55:18.143 
Epoch 254/1000 
	 loss: 17.0977, MinusLogProbMetric: 17.0977, val_loss: 17.8697, val_MinusLogProbMetric: 17.8697

Epoch 254: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0977 - MinusLogProbMetric: 17.0977 - val_loss: 17.8697 - val_MinusLogProbMetric: 17.8697 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 255/1000
2023-09-27 11:56:22.272 
Epoch 255/1000 
	 loss: 17.0399, MinusLogProbMetric: 17.0399, val_loss: 17.8856, val_MinusLogProbMetric: 17.8856

Epoch 255: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0399 - MinusLogProbMetric: 17.0399 - val_loss: 17.8856 - val_MinusLogProbMetric: 17.8856 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 256/1000
2023-09-27 11:57:26.156 
Epoch 256/1000 
	 loss: 16.9837, MinusLogProbMetric: 16.9837, val_loss: 18.0697, val_MinusLogProbMetric: 18.0697

Epoch 256: val_loss did not improve from 17.34477
196/196 - 64s - loss: 16.9837 - MinusLogProbMetric: 16.9837 - val_loss: 18.0697 - val_MinusLogProbMetric: 18.0697 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 257/1000
2023-09-27 11:58:30.210 
Epoch 257/1000 
	 loss: 17.0499, MinusLogProbMetric: 17.0499, val_loss: 17.5353, val_MinusLogProbMetric: 17.5353

Epoch 257: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0499 - MinusLogProbMetric: 17.0499 - val_loss: 17.5353 - val_MinusLogProbMetric: 17.5353 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 258/1000
2023-09-27 11:59:34.240 
Epoch 258/1000 
	 loss: 17.0537, MinusLogProbMetric: 17.0537, val_loss: 17.6913, val_MinusLogProbMetric: 17.6913

Epoch 258: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0537 - MinusLogProbMetric: 17.0537 - val_loss: 17.6913 - val_MinusLogProbMetric: 17.6913 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 259/1000
2023-09-27 12:00:37.999 
Epoch 259/1000 
	 loss: 17.0229, MinusLogProbMetric: 17.0229, val_loss: 17.3623, val_MinusLogProbMetric: 17.3623

Epoch 259: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0229 - MinusLogProbMetric: 17.0229 - val_loss: 17.3623 - val_MinusLogProbMetric: 17.3623 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 260/1000
2023-09-27 12:01:41.780 
Epoch 260/1000 
	 loss: 17.0104, MinusLogProbMetric: 17.0104, val_loss: 17.4218, val_MinusLogProbMetric: 17.4218

Epoch 260: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0104 - MinusLogProbMetric: 17.0104 - val_loss: 17.4218 - val_MinusLogProbMetric: 17.4218 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 261/1000
2023-09-27 12:02:45.865 
Epoch 261/1000 
	 loss: 17.0191, MinusLogProbMetric: 17.0191, val_loss: 17.6325, val_MinusLogProbMetric: 17.6325

Epoch 261: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0191 - MinusLogProbMetric: 17.0191 - val_loss: 17.6325 - val_MinusLogProbMetric: 17.6325 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 262/1000
2023-09-27 12:03:49.952 
Epoch 262/1000 
	 loss: 17.0396, MinusLogProbMetric: 17.0396, val_loss: 17.6988, val_MinusLogProbMetric: 17.6988

Epoch 262: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0396 - MinusLogProbMetric: 17.0396 - val_loss: 17.6988 - val_MinusLogProbMetric: 17.6988 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 263/1000
2023-09-27 12:04:54.255 
Epoch 263/1000 
	 loss: 17.0085, MinusLogProbMetric: 17.0085, val_loss: 17.9476, val_MinusLogProbMetric: 17.9476

Epoch 263: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0085 - MinusLogProbMetric: 17.0085 - val_loss: 17.9476 - val_MinusLogProbMetric: 17.9476 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 264/1000
2023-09-27 12:05:58.153 
Epoch 264/1000 
	 loss: 17.0896, MinusLogProbMetric: 17.0896, val_loss: 17.4985, val_MinusLogProbMetric: 17.4985

Epoch 264: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0896 - MinusLogProbMetric: 17.0896 - val_loss: 17.4985 - val_MinusLogProbMetric: 17.4985 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 265/1000
2023-09-27 12:07:02.473 
Epoch 265/1000 
	 loss: 16.9944, MinusLogProbMetric: 16.9944, val_loss: 17.9444, val_MinusLogProbMetric: 17.9444

Epoch 265: val_loss did not improve from 17.34477
196/196 - 64s - loss: 16.9944 - MinusLogProbMetric: 16.9944 - val_loss: 17.9444 - val_MinusLogProbMetric: 17.9444 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 266/1000
2023-09-27 12:08:06.537 
Epoch 266/1000 
	 loss: 17.0162, MinusLogProbMetric: 17.0162, val_loss: 17.6468, val_MinusLogProbMetric: 17.6468

Epoch 266: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0162 - MinusLogProbMetric: 17.0162 - val_loss: 17.6468 - val_MinusLogProbMetric: 17.6468 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 267/1000
2023-09-27 12:09:10.603 
Epoch 267/1000 
	 loss: 17.0025, MinusLogProbMetric: 17.0025, val_loss: 18.3135, val_MinusLogProbMetric: 18.3135

Epoch 267: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0025 - MinusLogProbMetric: 17.0025 - val_loss: 18.3135 - val_MinusLogProbMetric: 18.3135 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 268/1000
2023-09-27 12:10:15.169 
Epoch 268/1000 
	 loss: 17.0685, MinusLogProbMetric: 17.0685, val_loss: 17.5010, val_MinusLogProbMetric: 17.5010

Epoch 268: val_loss did not improve from 17.34477
196/196 - 65s - loss: 17.0685 - MinusLogProbMetric: 17.0685 - val_loss: 17.5010 - val_MinusLogProbMetric: 17.5010 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 269/1000
2023-09-27 12:11:19.462 
Epoch 269/1000 
	 loss: 16.9965, MinusLogProbMetric: 16.9965, val_loss: 17.5009, val_MinusLogProbMetric: 17.5009

Epoch 269: val_loss did not improve from 17.34477
196/196 - 64s - loss: 16.9965 - MinusLogProbMetric: 16.9965 - val_loss: 17.5009 - val_MinusLogProbMetric: 17.5009 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 270/1000
2023-09-27 12:12:23.628 
Epoch 270/1000 
	 loss: 17.0497, MinusLogProbMetric: 17.0497, val_loss: 17.7113, val_MinusLogProbMetric: 17.7113

Epoch 270: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0497 - MinusLogProbMetric: 17.0497 - val_loss: 17.7113 - val_MinusLogProbMetric: 17.7113 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 271/1000
2023-09-27 12:13:27.855 
Epoch 271/1000 
	 loss: 16.9520, MinusLogProbMetric: 16.9520, val_loss: 17.6097, val_MinusLogProbMetric: 17.6097

Epoch 271: val_loss did not improve from 17.34477
196/196 - 64s - loss: 16.9520 - MinusLogProbMetric: 16.9520 - val_loss: 17.6097 - val_MinusLogProbMetric: 17.6097 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 272/1000
2023-09-27 12:14:31.744 
Epoch 272/1000 
	 loss: 17.0035, MinusLogProbMetric: 17.0035, val_loss: 18.3440, val_MinusLogProbMetric: 18.3440

Epoch 272: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0035 - MinusLogProbMetric: 17.0035 - val_loss: 18.3440 - val_MinusLogProbMetric: 18.3440 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 273/1000
2023-09-27 12:15:35.772 
Epoch 273/1000 
	 loss: 17.0043, MinusLogProbMetric: 17.0043, val_loss: 17.8475, val_MinusLogProbMetric: 17.8475

Epoch 273: val_loss did not improve from 17.34477
196/196 - 64s - loss: 17.0043 - MinusLogProbMetric: 17.0043 - val_loss: 17.8475 - val_MinusLogProbMetric: 17.8475 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 274/1000
2023-09-27 12:16:39.865 
Epoch 274/1000 
	 loss: 16.9800, MinusLogProbMetric: 16.9800, val_loss: 17.3178, val_MinusLogProbMetric: 17.3178

Epoch 274: val_loss improved from 17.34477 to 17.31776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 16.9800 - MinusLogProbMetric: 16.9800 - val_loss: 17.3178 - val_MinusLogProbMetric: 17.3178 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 275/1000
2023-09-27 12:17:45.029 
Epoch 275/1000 
	 loss: 16.9630, MinusLogProbMetric: 16.9630, val_loss: 17.6159, val_MinusLogProbMetric: 17.6159

Epoch 275: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9630 - MinusLogProbMetric: 16.9630 - val_loss: 17.6159 - val_MinusLogProbMetric: 17.6159 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 276/1000
2023-09-27 12:18:49.239 
Epoch 276/1000 
	 loss: 17.0077, MinusLogProbMetric: 17.0077, val_loss: 18.1287, val_MinusLogProbMetric: 18.1287

Epoch 276: val_loss did not improve from 17.31776
196/196 - 64s - loss: 17.0077 - MinusLogProbMetric: 17.0077 - val_loss: 18.1287 - val_MinusLogProbMetric: 18.1287 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 277/1000
2023-09-27 12:19:53.520 
Epoch 277/1000 
	 loss: 16.9407, MinusLogProbMetric: 16.9407, val_loss: 17.5548, val_MinusLogProbMetric: 17.5548

Epoch 277: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9407 - MinusLogProbMetric: 16.9407 - val_loss: 17.5548 - val_MinusLogProbMetric: 17.5548 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 278/1000
2023-09-27 12:20:57.701 
Epoch 278/1000 
	 loss: 17.0103, MinusLogProbMetric: 17.0103, val_loss: 17.5552, val_MinusLogProbMetric: 17.5552

Epoch 278: val_loss did not improve from 17.31776
196/196 - 64s - loss: 17.0103 - MinusLogProbMetric: 17.0103 - val_loss: 17.5552 - val_MinusLogProbMetric: 17.5552 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 279/1000
2023-09-27 12:22:01.634 
Epoch 279/1000 
	 loss: 16.9600, MinusLogProbMetric: 16.9600, val_loss: 17.5478, val_MinusLogProbMetric: 17.5478

Epoch 279: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9600 - MinusLogProbMetric: 16.9600 - val_loss: 17.5478 - val_MinusLogProbMetric: 17.5478 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 280/1000
2023-09-27 12:23:06.047 
Epoch 280/1000 
	 loss: 16.9722, MinusLogProbMetric: 16.9722, val_loss: 17.4596, val_MinusLogProbMetric: 17.4596

Epoch 280: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9722 - MinusLogProbMetric: 16.9722 - val_loss: 17.4596 - val_MinusLogProbMetric: 17.4596 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 281/1000
2023-09-27 12:24:09.947 
Epoch 281/1000 
	 loss: 16.9336, MinusLogProbMetric: 16.9336, val_loss: 17.3409, val_MinusLogProbMetric: 17.3409

Epoch 281: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9336 - MinusLogProbMetric: 16.9336 - val_loss: 17.3409 - val_MinusLogProbMetric: 17.3409 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 282/1000
2023-09-27 12:25:13.377 
Epoch 282/1000 
	 loss: 16.9984, MinusLogProbMetric: 16.9984, val_loss: 17.4786, val_MinusLogProbMetric: 17.4786

Epoch 282: val_loss did not improve from 17.31776
196/196 - 63s - loss: 16.9984 - MinusLogProbMetric: 16.9984 - val_loss: 17.4786 - val_MinusLogProbMetric: 17.4786 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 283/1000
2023-09-27 12:26:17.868 
Epoch 283/1000 
	 loss: 16.9504, MinusLogProbMetric: 16.9504, val_loss: 17.5982, val_MinusLogProbMetric: 17.5982

Epoch 283: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9504 - MinusLogProbMetric: 16.9504 - val_loss: 17.5982 - val_MinusLogProbMetric: 17.5982 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 284/1000
2023-09-27 12:27:21.668 
Epoch 284/1000 
	 loss: 16.9628, MinusLogProbMetric: 16.9628, val_loss: 17.4496, val_MinusLogProbMetric: 17.4496

Epoch 284: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9628 - MinusLogProbMetric: 16.9628 - val_loss: 17.4496 - val_MinusLogProbMetric: 17.4496 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 285/1000
2023-09-27 12:28:26.028 
Epoch 285/1000 
	 loss: 16.9333, MinusLogProbMetric: 16.9333, val_loss: 17.4633, val_MinusLogProbMetric: 17.4633

Epoch 285: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9333 - MinusLogProbMetric: 16.9333 - val_loss: 17.4633 - val_MinusLogProbMetric: 17.4633 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 286/1000
2023-09-27 12:29:30.084 
Epoch 286/1000 
	 loss: 16.9380, MinusLogProbMetric: 16.9380, val_loss: 17.6296, val_MinusLogProbMetric: 17.6296

Epoch 286: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9380 - MinusLogProbMetric: 16.9380 - val_loss: 17.6296 - val_MinusLogProbMetric: 17.6296 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 287/1000
2023-09-27 12:30:34.129 
Epoch 287/1000 
	 loss: 16.9564, MinusLogProbMetric: 16.9564, val_loss: 17.6068, val_MinusLogProbMetric: 17.6068

Epoch 287: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9564 - MinusLogProbMetric: 16.9564 - val_loss: 17.6068 - val_MinusLogProbMetric: 17.6068 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 288/1000
2023-09-27 12:31:38.356 
Epoch 288/1000 
	 loss: 16.9499, MinusLogProbMetric: 16.9499, val_loss: 17.5264, val_MinusLogProbMetric: 17.5264

Epoch 288: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9499 - MinusLogProbMetric: 16.9499 - val_loss: 17.5264 - val_MinusLogProbMetric: 17.5264 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 289/1000
2023-09-27 12:32:42.281 
Epoch 289/1000 
	 loss: 16.9477, MinusLogProbMetric: 16.9477, val_loss: 17.5249, val_MinusLogProbMetric: 17.5249

Epoch 289: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9477 - MinusLogProbMetric: 16.9477 - val_loss: 17.5249 - val_MinusLogProbMetric: 17.5249 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 290/1000
2023-09-27 12:33:45.784 
Epoch 290/1000 
	 loss: 16.9591, MinusLogProbMetric: 16.9591, val_loss: 17.5131, val_MinusLogProbMetric: 17.5131

Epoch 290: val_loss did not improve from 17.31776
196/196 - 64s - loss: 16.9591 - MinusLogProbMetric: 16.9591 - val_loss: 17.5131 - val_MinusLogProbMetric: 17.5131 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 291/1000
2023-09-27 12:34:49.285 
Epoch 291/1000 
	 loss: 16.9252, MinusLogProbMetric: 16.9252, val_loss: 17.5662, val_MinusLogProbMetric: 17.5662

Epoch 291: val_loss did not improve from 17.31776
196/196 - 63s - loss: 16.9252 - MinusLogProbMetric: 16.9252 - val_loss: 17.5662 - val_MinusLogProbMetric: 17.5662 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 292/1000
2023-09-27 12:35:53.391 
Epoch 292/1000 
	 loss: 16.9659, MinusLogProbMetric: 16.9659, val_loss: 17.3102, val_MinusLogProbMetric: 17.3102

Epoch 292: val_loss improved from 17.31776 to 17.31018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 16.9659 - MinusLogProbMetric: 16.9659 - val_loss: 17.3102 - val_MinusLogProbMetric: 17.3102 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 293/1000
2023-09-27 12:36:58.155 
Epoch 293/1000 
	 loss: 16.9613, MinusLogProbMetric: 16.9613, val_loss: 17.3616, val_MinusLogProbMetric: 17.3616

Epoch 293: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9613 - MinusLogProbMetric: 16.9613 - val_loss: 17.3616 - val_MinusLogProbMetric: 17.3616 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 294/1000
2023-09-27 12:38:01.650 
Epoch 294/1000 
	 loss: 16.9593, MinusLogProbMetric: 16.9593, val_loss: 17.4696, val_MinusLogProbMetric: 17.4696

Epoch 294: val_loss did not improve from 17.31018
196/196 - 63s - loss: 16.9593 - MinusLogProbMetric: 16.9593 - val_loss: 17.4696 - val_MinusLogProbMetric: 17.4696 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 295/1000
2023-09-27 12:39:05.934 
Epoch 295/1000 
	 loss: 16.9471, MinusLogProbMetric: 16.9471, val_loss: 17.7165, val_MinusLogProbMetric: 17.7165

Epoch 295: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9471 - MinusLogProbMetric: 16.9471 - val_loss: 17.7165 - val_MinusLogProbMetric: 17.7165 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 296/1000
2023-09-27 12:40:10.070 
Epoch 296/1000 
	 loss: 16.9276, MinusLogProbMetric: 16.9276, val_loss: 17.7781, val_MinusLogProbMetric: 17.7781

Epoch 296: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9276 - MinusLogProbMetric: 16.9276 - val_loss: 17.7781 - val_MinusLogProbMetric: 17.7781 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 297/1000
2023-09-27 12:41:13.859 
Epoch 297/1000 
	 loss: 16.9332, MinusLogProbMetric: 16.9332, val_loss: 17.5446, val_MinusLogProbMetric: 17.5446

Epoch 297: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9332 - MinusLogProbMetric: 16.9332 - val_loss: 17.5446 - val_MinusLogProbMetric: 17.5446 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 298/1000
2023-09-27 12:42:17.979 
Epoch 298/1000 
	 loss: 16.8762, MinusLogProbMetric: 16.8762, val_loss: 17.4935, val_MinusLogProbMetric: 17.4935

Epoch 298: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.8762 - MinusLogProbMetric: 16.8762 - val_loss: 17.4935 - val_MinusLogProbMetric: 17.4935 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 299/1000
2023-09-27 12:43:21.815 
Epoch 299/1000 
	 loss: 16.9594, MinusLogProbMetric: 16.9594, val_loss: 17.5309, val_MinusLogProbMetric: 17.5309

Epoch 299: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9594 - MinusLogProbMetric: 16.9594 - val_loss: 17.5309 - val_MinusLogProbMetric: 17.5309 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 300/1000
2023-09-27 12:44:25.782 
Epoch 300/1000 
	 loss: 16.9279, MinusLogProbMetric: 16.9279, val_loss: 17.5021, val_MinusLogProbMetric: 17.5021

Epoch 300: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9279 - MinusLogProbMetric: 16.9279 - val_loss: 17.5021 - val_MinusLogProbMetric: 17.5021 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 301/1000
2023-09-27 12:45:29.695 
Epoch 301/1000 
	 loss: 16.9481, MinusLogProbMetric: 16.9481, val_loss: 18.1797, val_MinusLogProbMetric: 18.1797

Epoch 301: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9481 - MinusLogProbMetric: 16.9481 - val_loss: 18.1797 - val_MinusLogProbMetric: 18.1797 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 302/1000
2023-09-27 12:46:34.045 
Epoch 302/1000 
	 loss: 16.9279, MinusLogProbMetric: 16.9279, val_loss: 17.4359, val_MinusLogProbMetric: 17.4359

Epoch 302: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9279 - MinusLogProbMetric: 16.9279 - val_loss: 17.4359 - val_MinusLogProbMetric: 17.4359 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 303/1000
2023-09-27 12:47:38.478 
Epoch 303/1000 
	 loss: 16.9055, MinusLogProbMetric: 16.9055, val_loss: 17.3874, val_MinusLogProbMetric: 17.3874

Epoch 303: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.9055 - MinusLogProbMetric: 16.9055 - val_loss: 17.3874 - val_MinusLogProbMetric: 17.3874 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 304/1000
2023-09-27 12:48:42.707 
Epoch 304/1000 
	 loss: 16.8805, MinusLogProbMetric: 16.8805, val_loss: 17.4971, val_MinusLogProbMetric: 17.4971

Epoch 304: val_loss did not improve from 17.31018
196/196 - 64s - loss: 16.8805 - MinusLogProbMetric: 16.8805 - val_loss: 17.4971 - val_MinusLogProbMetric: 17.4971 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 305/1000
2023-09-27 12:49:46.011 
Epoch 305/1000 
	 loss: 16.8730, MinusLogProbMetric: 16.8730, val_loss: 18.1911, val_MinusLogProbMetric: 18.1911

Epoch 305: val_loss did not improve from 17.31018
196/196 - 63s - loss: 16.8730 - MinusLogProbMetric: 16.8730 - val_loss: 18.1911 - val_MinusLogProbMetric: 18.1911 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 306/1000
2023-09-27 12:50:49.088 
Epoch 306/1000 
	 loss: 16.9417, MinusLogProbMetric: 16.9417, val_loss: 17.8857, val_MinusLogProbMetric: 17.8857

Epoch 306: val_loss did not improve from 17.31018
196/196 - 63s - loss: 16.9417 - MinusLogProbMetric: 16.9417 - val_loss: 17.8857 - val_MinusLogProbMetric: 17.8857 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 307/1000
2023-09-27 12:51:50.594 
Epoch 307/1000 
	 loss: 16.9063, MinusLogProbMetric: 16.9063, val_loss: 17.2897, val_MinusLogProbMetric: 17.2897

Epoch 307: val_loss improved from 17.31018 to 17.28974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 62s - loss: 16.9063 - MinusLogProbMetric: 16.9063 - val_loss: 17.2897 - val_MinusLogProbMetric: 17.2897 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 308/1000
2023-09-27 12:52:55.784 
Epoch 308/1000 
	 loss: 16.8876, MinusLogProbMetric: 16.8876, val_loss: 17.5412, val_MinusLogProbMetric: 17.5412

Epoch 308: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8876 - MinusLogProbMetric: 16.8876 - val_loss: 17.5412 - val_MinusLogProbMetric: 17.5412 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 309/1000
2023-09-27 12:53:59.808 
Epoch 309/1000 
	 loss: 16.9134, MinusLogProbMetric: 16.9134, val_loss: 18.1909, val_MinusLogProbMetric: 18.1909

Epoch 309: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.9134 - MinusLogProbMetric: 16.9134 - val_loss: 18.1909 - val_MinusLogProbMetric: 18.1909 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 310/1000
2023-09-27 12:55:03.724 
Epoch 310/1000 
	 loss: 16.9492, MinusLogProbMetric: 16.9492, val_loss: 17.5798, val_MinusLogProbMetric: 17.5798

Epoch 310: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.9492 - MinusLogProbMetric: 16.9492 - val_loss: 17.5798 - val_MinusLogProbMetric: 17.5798 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 311/1000
2023-09-27 12:56:07.612 
Epoch 311/1000 
	 loss: 16.8763, MinusLogProbMetric: 16.8763, val_loss: 17.6354, val_MinusLogProbMetric: 17.6354

Epoch 311: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8763 - MinusLogProbMetric: 16.8763 - val_loss: 17.6354 - val_MinusLogProbMetric: 17.6354 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 312/1000
2023-09-27 12:57:11.497 
Epoch 312/1000 
	 loss: 16.9151, MinusLogProbMetric: 16.9151, val_loss: 17.4692, val_MinusLogProbMetric: 17.4692

Epoch 312: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.9151 - MinusLogProbMetric: 16.9151 - val_loss: 17.4692 - val_MinusLogProbMetric: 17.4692 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 313/1000
2023-09-27 12:58:15.220 
Epoch 313/1000 
	 loss: 16.8433, MinusLogProbMetric: 16.8433, val_loss: 17.5871, val_MinusLogProbMetric: 17.5871

Epoch 313: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8433 - MinusLogProbMetric: 16.8433 - val_loss: 17.5871 - val_MinusLogProbMetric: 17.5871 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 314/1000
2023-09-27 12:59:19.029 
Epoch 314/1000 
	 loss: 16.8920, MinusLogProbMetric: 16.8920, val_loss: 18.1706, val_MinusLogProbMetric: 18.1706

Epoch 314: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8920 - MinusLogProbMetric: 16.8920 - val_loss: 18.1706 - val_MinusLogProbMetric: 18.1706 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 315/1000
2023-09-27 13:00:23.004 
Epoch 315/1000 
	 loss: 16.8898, MinusLogProbMetric: 16.8898, val_loss: 17.6212, val_MinusLogProbMetric: 17.6212

Epoch 315: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8898 - MinusLogProbMetric: 16.8898 - val_loss: 17.6212 - val_MinusLogProbMetric: 17.6212 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 316/1000
2023-09-27 13:01:27.155 
Epoch 316/1000 
	 loss: 16.8657, MinusLogProbMetric: 16.8657, val_loss: 17.5010, val_MinusLogProbMetric: 17.5010

Epoch 316: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8657 - MinusLogProbMetric: 16.8657 - val_loss: 17.5010 - val_MinusLogProbMetric: 17.5010 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 317/1000
2023-09-27 13:02:30.938 
Epoch 317/1000 
	 loss: 16.8462, MinusLogProbMetric: 16.8462, val_loss: 18.0100, val_MinusLogProbMetric: 18.0100

Epoch 317: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8462 - MinusLogProbMetric: 16.8462 - val_loss: 18.0100 - val_MinusLogProbMetric: 18.0100 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 318/1000
2023-09-27 13:03:34.723 
Epoch 318/1000 
	 loss: 16.8900, MinusLogProbMetric: 16.8900, val_loss: 17.5447, val_MinusLogProbMetric: 17.5447

Epoch 318: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8900 - MinusLogProbMetric: 16.8900 - val_loss: 17.5447 - val_MinusLogProbMetric: 17.5447 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 319/1000
2023-09-27 13:04:39.152 
Epoch 319/1000 
	 loss: 16.8906, MinusLogProbMetric: 16.8906, val_loss: 17.6205, val_MinusLogProbMetric: 17.6205

Epoch 319: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8906 - MinusLogProbMetric: 16.8906 - val_loss: 17.6205 - val_MinusLogProbMetric: 17.6205 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 320/1000
2023-09-27 13:05:43.446 
Epoch 320/1000 
	 loss: 16.8655, MinusLogProbMetric: 16.8655, val_loss: 17.6270, val_MinusLogProbMetric: 17.6270

Epoch 320: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8655 - MinusLogProbMetric: 16.8655 - val_loss: 17.6270 - val_MinusLogProbMetric: 17.6270 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 321/1000
2023-09-27 13:06:47.109 
Epoch 321/1000 
	 loss: 16.9099, MinusLogProbMetric: 16.9099, val_loss: 17.6300, val_MinusLogProbMetric: 17.6300

Epoch 321: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.9099 - MinusLogProbMetric: 16.9099 - val_loss: 17.6300 - val_MinusLogProbMetric: 17.6300 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 322/1000
2023-09-27 13:07:51.254 
Epoch 322/1000 
	 loss: 16.8699, MinusLogProbMetric: 16.8699, val_loss: 17.4847, val_MinusLogProbMetric: 17.4847

Epoch 322: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8699 - MinusLogProbMetric: 16.8699 - val_loss: 17.4847 - val_MinusLogProbMetric: 17.4847 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 323/1000
2023-09-27 13:08:55.634 
Epoch 323/1000 
	 loss: 16.9034, MinusLogProbMetric: 16.9034, val_loss: 17.4260, val_MinusLogProbMetric: 17.4260

Epoch 323: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.9034 - MinusLogProbMetric: 16.9034 - val_loss: 17.4260 - val_MinusLogProbMetric: 17.4260 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 324/1000
2023-09-27 13:10:00.157 
Epoch 324/1000 
	 loss: 16.8947, MinusLogProbMetric: 16.8947, val_loss: 17.4522, val_MinusLogProbMetric: 17.4522

Epoch 324: val_loss did not improve from 17.28974
196/196 - 65s - loss: 16.8947 - MinusLogProbMetric: 16.8947 - val_loss: 17.4522 - val_MinusLogProbMetric: 17.4522 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 325/1000
2023-09-27 13:11:03.912 
Epoch 325/1000 
	 loss: 16.8546, MinusLogProbMetric: 16.8546, val_loss: 17.3735, val_MinusLogProbMetric: 17.3735

Epoch 325: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8546 - MinusLogProbMetric: 16.8546 - val_loss: 17.3735 - val_MinusLogProbMetric: 17.3735 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 326/1000
2023-09-27 13:12:07.925 
Epoch 326/1000 
	 loss: 16.8948, MinusLogProbMetric: 16.8948, val_loss: 17.4150, val_MinusLogProbMetric: 17.4150

Epoch 326: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8948 - MinusLogProbMetric: 16.8948 - val_loss: 17.4150 - val_MinusLogProbMetric: 17.4150 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 327/1000
2023-09-27 13:13:12.424 
Epoch 327/1000 
	 loss: 16.8922, MinusLogProbMetric: 16.8922, val_loss: 17.4838, val_MinusLogProbMetric: 17.4838

Epoch 327: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8922 - MinusLogProbMetric: 16.8922 - val_loss: 17.4838 - val_MinusLogProbMetric: 17.4838 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 328/1000
2023-09-27 13:14:16.672 
Epoch 328/1000 
	 loss: 16.8170, MinusLogProbMetric: 16.8170, val_loss: 17.5658, val_MinusLogProbMetric: 17.5658

Epoch 328: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8170 - MinusLogProbMetric: 16.8170 - val_loss: 17.5658 - val_MinusLogProbMetric: 17.5658 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 329/1000
2023-09-27 13:15:20.547 
Epoch 329/1000 
	 loss: 16.8701, MinusLogProbMetric: 16.8701, val_loss: 17.4932, val_MinusLogProbMetric: 17.4932

Epoch 329: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8701 - MinusLogProbMetric: 16.8701 - val_loss: 17.4932 - val_MinusLogProbMetric: 17.4932 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 330/1000
2023-09-27 13:16:24.452 
Epoch 330/1000 
	 loss: 16.8634, MinusLogProbMetric: 16.8634, val_loss: 17.7364, val_MinusLogProbMetric: 17.7364

Epoch 330: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8634 - MinusLogProbMetric: 16.8634 - val_loss: 17.7364 - val_MinusLogProbMetric: 17.7364 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 331/1000
2023-09-27 13:17:28.685 
Epoch 331/1000 
	 loss: 16.8345, MinusLogProbMetric: 16.8345, val_loss: 17.5030, val_MinusLogProbMetric: 17.5030

Epoch 331: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8345 - MinusLogProbMetric: 16.8345 - val_loss: 17.5030 - val_MinusLogProbMetric: 17.5030 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 332/1000
2023-09-27 13:18:32.566 
Epoch 332/1000 
	 loss: 16.7824, MinusLogProbMetric: 16.7824, val_loss: 17.8016, val_MinusLogProbMetric: 17.8016

Epoch 332: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.7824 - MinusLogProbMetric: 16.7824 - val_loss: 17.8016 - val_MinusLogProbMetric: 17.8016 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 333/1000
2023-09-27 13:19:36.862 
Epoch 333/1000 
	 loss: 16.8717, MinusLogProbMetric: 16.8717, val_loss: 17.6511, val_MinusLogProbMetric: 17.6511

Epoch 333: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8717 - MinusLogProbMetric: 16.8717 - val_loss: 17.6511 - val_MinusLogProbMetric: 17.6511 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 334/1000
2023-09-27 13:20:41.216 
Epoch 334/1000 
	 loss: 16.7861, MinusLogProbMetric: 16.7861, val_loss: 17.6444, val_MinusLogProbMetric: 17.6444

Epoch 334: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.7861 - MinusLogProbMetric: 16.7861 - val_loss: 17.6444 - val_MinusLogProbMetric: 17.6444 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 335/1000
2023-09-27 13:21:44.657 
Epoch 335/1000 
	 loss: 16.8283, MinusLogProbMetric: 16.8283, val_loss: 18.3787, val_MinusLogProbMetric: 18.3787

Epoch 335: val_loss did not improve from 17.28974
196/196 - 63s - loss: 16.8283 - MinusLogProbMetric: 16.8283 - val_loss: 18.3787 - val_MinusLogProbMetric: 18.3787 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 336/1000
2023-09-27 13:22:48.557 
Epoch 336/1000 
	 loss: 16.9218, MinusLogProbMetric: 16.9218, val_loss: 17.3590, val_MinusLogProbMetric: 17.3590

Epoch 336: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.9218 - MinusLogProbMetric: 16.9218 - val_loss: 17.3590 - val_MinusLogProbMetric: 17.3590 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 337/1000
2023-09-27 13:23:52.896 
Epoch 337/1000 
	 loss: 16.8430, MinusLogProbMetric: 16.8430, val_loss: 17.5630, val_MinusLogProbMetric: 17.5630

Epoch 337: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8430 - MinusLogProbMetric: 16.8430 - val_loss: 17.5630 - val_MinusLogProbMetric: 17.5630 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 338/1000
2023-09-27 13:24:56.995 
Epoch 338/1000 
	 loss: 16.8335, MinusLogProbMetric: 16.8335, val_loss: 17.6131, val_MinusLogProbMetric: 17.6131

Epoch 338: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8335 - MinusLogProbMetric: 16.8335 - val_loss: 17.6131 - val_MinusLogProbMetric: 17.6131 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 339/1000
2023-09-27 13:26:00.966 
Epoch 339/1000 
	 loss: 16.8131, MinusLogProbMetric: 16.8131, val_loss: 17.4681, val_MinusLogProbMetric: 17.4681

Epoch 339: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8131 - MinusLogProbMetric: 16.8131 - val_loss: 17.4681 - val_MinusLogProbMetric: 17.4681 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 340/1000
2023-09-27 13:27:04.999 
Epoch 340/1000 
	 loss: 16.8242, MinusLogProbMetric: 16.8242, val_loss: 17.3595, val_MinusLogProbMetric: 17.3595

Epoch 340: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8242 - MinusLogProbMetric: 16.8242 - val_loss: 17.3595 - val_MinusLogProbMetric: 17.3595 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 341/1000
2023-09-27 13:28:09.561 
Epoch 341/1000 
	 loss: 16.8445, MinusLogProbMetric: 16.8445, val_loss: 17.9424, val_MinusLogProbMetric: 17.9424

Epoch 341: val_loss did not improve from 17.28974
196/196 - 65s - loss: 16.8445 - MinusLogProbMetric: 16.8445 - val_loss: 17.9424 - val_MinusLogProbMetric: 17.9424 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 342/1000
2023-09-27 13:29:13.416 
Epoch 342/1000 
	 loss: 16.8721, MinusLogProbMetric: 16.8721, val_loss: 17.6450, val_MinusLogProbMetric: 17.6450

Epoch 342: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8721 - MinusLogProbMetric: 16.8721 - val_loss: 17.6450 - val_MinusLogProbMetric: 17.6450 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 343/1000
2023-09-27 13:30:17.285 
Epoch 343/1000 
	 loss: 16.8518, MinusLogProbMetric: 16.8518, val_loss: 17.4133, val_MinusLogProbMetric: 17.4133

Epoch 343: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8518 - MinusLogProbMetric: 16.8518 - val_loss: 17.4133 - val_MinusLogProbMetric: 17.4133 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 344/1000
2023-09-27 13:31:21.051 
Epoch 344/1000 
	 loss: 16.7962, MinusLogProbMetric: 16.7962, val_loss: 17.5719, val_MinusLogProbMetric: 17.5719

Epoch 344: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.7962 - MinusLogProbMetric: 16.7962 - val_loss: 17.5719 - val_MinusLogProbMetric: 17.5719 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 345/1000
2023-09-27 13:32:25.108 
Epoch 345/1000 
	 loss: 16.8221, MinusLogProbMetric: 16.8221, val_loss: 17.9346, val_MinusLogProbMetric: 17.9346

Epoch 345: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.8221 - MinusLogProbMetric: 16.8221 - val_loss: 17.9346 - val_MinusLogProbMetric: 17.9346 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 346/1000
2023-09-27 13:33:28.526 
Epoch 346/1000 
	 loss: 16.8310, MinusLogProbMetric: 16.8310, val_loss: 17.7400, val_MinusLogProbMetric: 17.7400

Epoch 346: val_loss did not improve from 17.28974
196/196 - 63s - loss: 16.8310 - MinusLogProbMetric: 16.8310 - val_loss: 17.7400 - val_MinusLogProbMetric: 17.7400 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 347/1000
2023-09-27 13:34:32.733 
Epoch 347/1000 
	 loss: 16.7981, MinusLogProbMetric: 16.7981, val_loss: 17.7344, val_MinusLogProbMetric: 17.7344

Epoch 347: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.7981 - MinusLogProbMetric: 16.7981 - val_loss: 17.7344 - val_MinusLogProbMetric: 17.7344 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 348/1000
2023-09-27 13:35:37.031 
Epoch 348/1000 
	 loss: 16.7966, MinusLogProbMetric: 16.7966, val_loss: 17.7031, val_MinusLogProbMetric: 17.7031

Epoch 348: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.7966 - MinusLogProbMetric: 16.7966 - val_loss: 17.7031 - val_MinusLogProbMetric: 17.7031 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 349/1000
2023-09-27 13:36:40.828 
Epoch 349/1000 
	 loss: 16.7822, MinusLogProbMetric: 16.7822, val_loss: 17.5330, val_MinusLogProbMetric: 17.5330

Epoch 349: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.7822 - MinusLogProbMetric: 16.7822 - val_loss: 17.5330 - val_MinusLogProbMetric: 17.5330 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 350/1000
2023-09-27 13:37:44.767 
Epoch 350/1000 
	 loss: 16.7974, MinusLogProbMetric: 16.7974, val_loss: 17.3875, val_MinusLogProbMetric: 17.3875

Epoch 350: val_loss did not improve from 17.28974
196/196 - 64s - loss: 16.7974 - MinusLogProbMetric: 16.7974 - val_loss: 17.3875 - val_MinusLogProbMetric: 17.3875 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 351/1000
2023-09-27 13:38:49.052 
Epoch 351/1000 
	 loss: 16.8089, MinusLogProbMetric: 16.8089, val_loss: 17.2448, val_MinusLogProbMetric: 17.2448

Epoch 351: val_loss improved from 17.28974 to 17.24485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 16.8089 - MinusLogProbMetric: 16.8089 - val_loss: 17.2448 - val_MinusLogProbMetric: 17.2448 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 352/1000
2023-09-27 13:39:53.914 
Epoch 352/1000 
	 loss: 16.8031, MinusLogProbMetric: 16.8031, val_loss: 18.5683, val_MinusLogProbMetric: 18.5683

Epoch 352: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.8031 - MinusLogProbMetric: 16.8031 - val_loss: 18.5683 - val_MinusLogProbMetric: 18.5683 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 353/1000
2023-09-27 13:40:57.947 
Epoch 353/1000 
	 loss: 16.8402, MinusLogProbMetric: 16.8402, val_loss: 17.4645, val_MinusLogProbMetric: 17.4645

Epoch 353: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.8402 - MinusLogProbMetric: 16.8402 - val_loss: 17.4645 - val_MinusLogProbMetric: 17.4645 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 354/1000
2023-09-27 13:42:02.024 
Epoch 354/1000 
	 loss: 16.8762, MinusLogProbMetric: 16.8762, val_loss: 17.5048, val_MinusLogProbMetric: 17.5048

Epoch 354: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.8762 - MinusLogProbMetric: 16.8762 - val_loss: 17.5048 - val_MinusLogProbMetric: 17.5048 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 355/1000
2023-09-27 13:43:05.556 
Epoch 355/1000 
	 loss: 16.7440, MinusLogProbMetric: 16.7440, val_loss: 17.5398, val_MinusLogProbMetric: 17.5398

Epoch 355: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7440 - MinusLogProbMetric: 16.7440 - val_loss: 17.5398 - val_MinusLogProbMetric: 17.5398 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 356/1000
2023-09-27 13:44:09.378 
Epoch 356/1000 
	 loss: 16.7937, MinusLogProbMetric: 16.7937, val_loss: 17.7076, val_MinusLogProbMetric: 17.7076

Epoch 356: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7937 - MinusLogProbMetric: 16.7937 - val_loss: 17.7076 - val_MinusLogProbMetric: 17.7076 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 357/1000
2023-09-27 13:45:09.033 
Epoch 357/1000 
	 loss: 16.7958, MinusLogProbMetric: 16.7958, val_loss: 17.5577, val_MinusLogProbMetric: 17.5577

Epoch 357: val_loss did not improve from 17.24485
196/196 - 60s - loss: 16.7958 - MinusLogProbMetric: 16.7958 - val_loss: 17.5577 - val_MinusLogProbMetric: 17.5577 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 358/1000
2023-09-27 13:46:05.473 
Epoch 358/1000 
	 loss: 16.7790, MinusLogProbMetric: 16.7790, val_loss: 17.5834, val_MinusLogProbMetric: 17.5834

Epoch 358: val_loss did not improve from 17.24485
196/196 - 56s - loss: 16.7790 - MinusLogProbMetric: 16.7790 - val_loss: 17.5834 - val_MinusLogProbMetric: 17.5834 - lr: 3.3333e-04 - 56s/epoch - 288ms/step
Epoch 359/1000
2023-09-27 13:47:09.674 
Epoch 359/1000 
	 loss: 16.8610, MinusLogProbMetric: 16.8610, val_loss: 17.3841, val_MinusLogProbMetric: 17.3841

Epoch 359: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.8610 - MinusLogProbMetric: 16.8610 - val_loss: 17.3841 - val_MinusLogProbMetric: 17.3841 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 360/1000
2023-09-27 13:48:13.112 
Epoch 360/1000 
	 loss: 16.7245, MinusLogProbMetric: 16.7245, val_loss: 17.5087, val_MinusLogProbMetric: 17.5087

Epoch 360: val_loss did not improve from 17.24485
196/196 - 63s - loss: 16.7245 - MinusLogProbMetric: 16.7245 - val_loss: 17.5087 - val_MinusLogProbMetric: 17.5087 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 361/1000
2023-09-27 13:49:17.224 
Epoch 361/1000 
	 loss: 16.7601, MinusLogProbMetric: 16.7601, val_loss: 17.7541, val_MinusLogProbMetric: 17.7541

Epoch 361: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7601 - MinusLogProbMetric: 16.7601 - val_loss: 17.7541 - val_MinusLogProbMetric: 17.7541 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 362/1000
2023-09-27 13:50:20.862 
Epoch 362/1000 
	 loss: 16.7792, MinusLogProbMetric: 16.7792, val_loss: 17.5536, val_MinusLogProbMetric: 17.5536

Epoch 362: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7792 - MinusLogProbMetric: 16.7792 - val_loss: 17.5536 - val_MinusLogProbMetric: 17.5536 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 363/1000
2023-09-27 13:51:24.830 
Epoch 363/1000 
	 loss: 16.7737, MinusLogProbMetric: 16.7737, val_loss: 18.1174, val_MinusLogProbMetric: 18.1174

Epoch 363: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7737 - MinusLogProbMetric: 16.7737 - val_loss: 18.1174 - val_MinusLogProbMetric: 18.1174 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 364/1000
2023-09-27 13:52:28.411 
Epoch 364/1000 
	 loss: 16.7702, MinusLogProbMetric: 16.7702, val_loss: 17.4277, val_MinusLogProbMetric: 17.4277

Epoch 364: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7702 - MinusLogProbMetric: 16.7702 - val_loss: 17.4277 - val_MinusLogProbMetric: 17.4277 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 365/1000
2023-09-27 13:53:32.678 
Epoch 365/1000 
	 loss: 16.8734, MinusLogProbMetric: 16.8734, val_loss: 17.7545, val_MinusLogProbMetric: 17.7545

Epoch 365: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.8734 - MinusLogProbMetric: 16.8734 - val_loss: 17.7545 - val_MinusLogProbMetric: 17.7545 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 366/1000
2023-09-27 13:54:37.139 
Epoch 366/1000 
	 loss: 16.7455, MinusLogProbMetric: 16.7455, val_loss: 17.4639, val_MinusLogProbMetric: 17.4639

Epoch 366: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7455 - MinusLogProbMetric: 16.7455 - val_loss: 17.4639 - val_MinusLogProbMetric: 17.4639 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 367/1000
2023-09-27 13:55:41.260 
Epoch 367/1000 
	 loss: 16.8291, MinusLogProbMetric: 16.8291, val_loss: 17.4375, val_MinusLogProbMetric: 17.4375

Epoch 367: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.8291 - MinusLogProbMetric: 16.8291 - val_loss: 17.4375 - val_MinusLogProbMetric: 17.4375 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 368/1000
2023-09-27 13:56:45.169 
Epoch 368/1000 
	 loss: 16.7487, MinusLogProbMetric: 16.7487, val_loss: 17.5749, val_MinusLogProbMetric: 17.5749

Epoch 368: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7487 - MinusLogProbMetric: 16.7487 - val_loss: 17.5749 - val_MinusLogProbMetric: 17.5749 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 369/1000
2023-09-27 13:57:49.230 
Epoch 369/1000 
	 loss: 16.7512, MinusLogProbMetric: 16.7512, val_loss: 17.7813, val_MinusLogProbMetric: 17.7813

Epoch 369: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7512 - MinusLogProbMetric: 16.7512 - val_loss: 17.7813 - val_MinusLogProbMetric: 17.7813 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 370/1000
2023-09-27 13:58:53.285 
Epoch 370/1000 
	 loss: 16.7920, MinusLogProbMetric: 16.7920, val_loss: 17.8080, val_MinusLogProbMetric: 17.8080

Epoch 370: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7920 - MinusLogProbMetric: 16.7920 - val_loss: 17.8080 - val_MinusLogProbMetric: 17.8080 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 371/1000
2023-09-27 13:59:57.293 
Epoch 371/1000 
	 loss: 16.7733, MinusLogProbMetric: 16.7733, val_loss: 17.5508, val_MinusLogProbMetric: 17.5508

Epoch 371: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7733 - MinusLogProbMetric: 16.7733 - val_loss: 17.5508 - val_MinusLogProbMetric: 17.5508 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 372/1000
2023-09-27 14:01:01.390 
Epoch 372/1000 
	 loss: 16.7631, MinusLogProbMetric: 16.7631, val_loss: 17.6045, val_MinusLogProbMetric: 17.6045

Epoch 372: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7631 - MinusLogProbMetric: 16.7631 - val_loss: 17.6045 - val_MinusLogProbMetric: 17.6045 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 373/1000
2023-09-27 14:02:05.384 
Epoch 373/1000 
	 loss: 16.7415, MinusLogProbMetric: 16.7415, val_loss: 17.5104, val_MinusLogProbMetric: 17.5104

Epoch 373: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7415 - MinusLogProbMetric: 16.7415 - val_loss: 17.5104 - val_MinusLogProbMetric: 17.5104 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 374/1000
2023-09-27 14:03:09.213 
Epoch 374/1000 
	 loss: 16.7902, MinusLogProbMetric: 16.7902, val_loss: 17.8358, val_MinusLogProbMetric: 17.8358

Epoch 374: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7902 - MinusLogProbMetric: 16.7902 - val_loss: 17.8358 - val_MinusLogProbMetric: 17.8358 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 375/1000
2023-09-27 14:04:12.878 
Epoch 375/1000 
	 loss: 16.7428, MinusLogProbMetric: 16.7428, val_loss: 18.0864, val_MinusLogProbMetric: 18.0864

Epoch 375: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7428 - MinusLogProbMetric: 16.7428 - val_loss: 18.0864 - val_MinusLogProbMetric: 18.0864 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 376/1000
2023-09-27 14:05:16.952 
Epoch 376/1000 
	 loss: 16.7411, MinusLogProbMetric: 16.7411, val_loss: 17.5500, val_MinusLogProbMetric: 17.5500

Epoch 376: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7411 - MinusLogProbMetric: 16.7411 - val_loss: 17.5500 - val_MinusLogProbMetric: 17.5500 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 377/1000
2023-09-27 14:06:20.836 
Epoch 377/1000 
	 loss: 16.7113, MinusLogProbMetric: 16.7113, val_loss: 17.3941, val_MinusLogProbMetric: 17.3941

Epoch 377: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7113 - MinusLogProbMetric: 16.7113 - val_loss: 17.3941 - val_MinusLogProbMetric: 17.3941 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 378/1000
2023-09-27 14:07:24.812 
Epoch 378/1000 
	 loss: 16.7538, MinusLogProbMetric: 16.7538, val_loss: 17.6501, val_MinusLogProbMetric: 17.6501

Epoch 378: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7538 - MinusLogProbMetric: 16.7538 - val_loss: 17.6501 - val_MinusLogProbMetric: 17.6501 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 379/1000
2023-09-27 14:08:29.061 
Epoch 379/1000 
	 loss: 16.6840, MinusLogProbMetric: 16.6840, val_loss: 17.6475, val_MinusLogProbMetric: 17.6475

Epoch 379: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.6840 - MinusLogProbMetric: 16.6840 - val_loss: 17.6475 - val_MinusLogProbMetric: 17.6475 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 380/1000
2023-09-27 14:09:32.792 
Epoch 380/1000 
	 loss: 16.7243, MinusLogProbMetric: 16.7243, val_loss: 17.5183, val_MinusLogProbMetric: 17.5183

Epoch 380: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7243 - MinusLogProbMetric: 16.7243 - val_loss: 17.5183 - val_MinusLogProbMetric: 17.5183 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 381/1000
2023-09-27 14:10:36.893 
Epoch 381/1000 
	 loss: 16.7242, MinusLogProbMetric: 16.7242, val_loss: 17.3186, val_MinusLogProbMetric: 17.3186

Epoch 381: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7242 - MinusLogProbMetric: 16.7242 - val_loss: 17.3186 - val_MinusLogProbMetric: 17.3186 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 382/1000
2023-09-27 14:11:41.130 
Epoch 382/1000 
	 loss: 16.7523, MinusLogProbMetric: 16.7523, val_loss: 17.6441, val_MinusLogProbMetric: 17.6441

Epoch 382: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7523 - MinusLogProbMetric: 16.7523 - val_loss: 17.6441 - val_MinusLogProbMetric: 17.6441 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 383/1000
2023-09-27 14:12:45.191 
Epoch 383/1000 
	 loss: 16.7675, MinusLogProbMetric: 16.7675, val_loss: 17.4291, val_MinusLogProbMetric: 17.4291

Epoch 383: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7675 - MinusLogProbMetric: 16.7675 - val_loss: 17.4291 - val_MinusLogProbMetric: 17.4291 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 384/1000
2023-09-27 14:13:49.224 
Epoch 384/1000 
	 loss: 16.7336, MinusLogProbMetric: 16.7336, val_loss: 17.4004, val_MinusLogProbMetric: 17.4004

Epoch 384: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7336 - MinusLogProbMetric: 16.7336 - val_loss: 17.4004 - val_MinusLogProbMetric: 17.4004 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 385/1000
2023-09-27 14:14:53.112 
Epoch 385/1000 
	 loss: 16.7367, MinusLogProbMetric: 16.7367, val_loss: 17.5324, val_MinusLogProbMetric: 17.5324

Epoch 385: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7367 - MinusLogProbMetric: 16.7367 - val_loss: 17.5324 - val_MinusLogProbMetric: 17.5324 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 386/1000
2023-09-27 14:15:57.073 
Epoch 386/1000 
	 loss: 16.7372, MinusLogProbMetric: 16.7372, val_loss: 17.7784, val_MinusLogProbMetric: 17.7784

Epoch 386: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7372 - MinusLogProbMetric: 16.7372 - val_loss: 17.7784 - val_MinusLogProbMetric: 17.7784 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 387/1000
2023-09-27 14:17:00.796 
Epoch 387/1000 
	 loss: 16.7512, MinusLogProbMetric: 16.7512, val_loss: 17.5722, val_MinusLogProbMetric: 17.5722

Epoch 387: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7512 - MinusLogProbMetric: 16.7512 - val_loss: 17.5722 - val_MinusLogProbMetric: 17.5722 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 388/1000
2023-09-27 14:18:05.148 
Epoch 388/1000 
	 loss: 16.7286, MinusLogProbMetric: 16.7286, val_loss: 17.4788, val_MinusLogProbMetric: 17.4788

Epoch 388: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7286 - MinusLogProbMetric: 16.7286 - val_loss: 17.4788 - val_MinusLogProbMetric: 17.4788 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 389/1000
2023-09-27 14:19:08.981 
Epoch 389/1000 
	 loss: 16.7009, MinusLogProbMetric: 16.7009, val_loss: 17.5844, val_MinusLogProbMetric: 17.5844

Epoch 389: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7009 - MinusLogProbMetric: 16.7009 - val_loss: 17.5844 - val_MinusLogProbMetric: 17.5844 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 390/1000
2023-09-27 14:20:12.802 
Epoch 390/1000 
	 loss: 16.7225, MinusLogProbMetric: 16.7225, val_loss: 17.6087, val_MinusLogProbMetric: 17.6087

Epoch 390: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7225 - MinusLogProbMetric: 16.7225 - val_loss: 17.6087 - val_MinusLogProbMetric: 17.6087 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 391/1000
2023-09-27 14:21:16.233 
Epoch 391/1000 
	 loss: 16.7120, MinusLogProbMetric: 16.7120, val_loss: 17.7453, val_MinusLogProbMetric: 17.7453

Epoch 391: val_loss did not improve from 17.24485
196/196 - 63s - loss: 16.7120 - MinusLogProbMetric: 16.7120 - val_loss: 17.7453 - val_MinusLogProbMetric: 17.7453 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 392/1000
2023-09-27 14:22:20.273 
Epoch 392/1000 
	 loss: 16.7509, MinusLogProbMetric: 16.7509, val_loss: 17.4864, val_MinusLogProbMetric: 17.4864

Epoch 392: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7509 - MinusLogProbMetric: 16.7509 - val_loss: 17.4864 - val_MinusLogProbMetric: 17.4864 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 393/1000
2023-09-27 14:23:23.798 
Epoch 393/1000 
	 loss: 16.7082, MinusLogProbMetric: 16.7082, val_loss: 17.6949, val_MinusLogProbMetric: 17.6949

Epoch 393: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7082 - MinusLogProbMetric: 16.7082 - val_loss: 17.6949 - val_MinusLogProbMetric: 17.6949 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 394/1000
2023-09-27 14:24:27.300 
Epoch 394/1000 
	 loss: 16.7328, MinusLogProbMetric: 16.7328, val_loss: 17.7005, val_MinusLogProbMetric: 17.7005

Epoch 394: val_loss did not improve from 17.24485
196/196 - 63s - loss: 16.7328 - MinusLogProbMetric: 16.7328 - val_loss: 17.7005 - val_MinusLogProbMetric: 17.7005 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 395/1000
2023-09-27 14:25:31.142 
Epoch 395/1000 
	 loss: 16.7265, MinusLogProbMetric: 16.7265, val_loss: 17.5351, val_MinusLogProbMetric: 17.5351

Epoch 395: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7265 - MinusLogProbMetric: 16.7265 - val_loss: 17.5351 - val_MinusLogProbMetric: 17.5351 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 396/1000
2023-09-27 14:26:34.733 
Epoch 396/1000 
	 loss: 16.7201, MinusLogProbMetric: 16.7201, val_loss: 17.4024, val_MinusLogProbMetric: 17.4024

Epoch 396: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7201 - MinusLogProbMetric: 16.7201 - val_loss: 17.4024 - val_MinusLogProbMetric: 17.4024 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 397/1000
2023-09-27 14:27:38.527 
Epoch 397/1000 
	 loss: 16.7152, MinusLogProbMetric: 16.7152, val_loss: 17.7899, val_MinusLogProbMetric: 17.7899

Epoch 397: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7152 - MinusLogProbMetric: 16.7152 - val_loss: 17.7899 - val_MinusLogProbMetric: 17.7899 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 398/1000
2023-09-27 14:28:42.462 
Epoch 398/1000 
	 loss: 16.7098, MinusLogProbMetric: 16.7098, val_loss: 17.4303, val_MinusLogProbMetric: 17.4303

Epoch 398: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7098 - MinusLogProbMetric: 16.7098 - val_loss: 17.4303 - val_MinusLogProbMetric: 17.4303 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 399/1000
2023-09-27 14:29:46.551 
Epoch 399/1000 
	 loss: 16.7173, MinusLogProbMetric: 16.7173, val_loss: 17.7041, val_MinusLogProbMetric: 17.7041

Epoch 399: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7173 - MinusLogProbMetric: 16.7173 - val_loss: 17.7041 - val_MinusLogProbMetric: 17.7041 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 400/1000
2023-09-27 14:30:50.918 
Epoch 400/1000 
	 loss: 16.7227, MinusLogProbMetric: 16.7227, val_loss: 17.5986, val_MinusLogProbMetric: 17.5986

Epoch 400: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.7227 - MinusLogProbMetric: 16.7227 - val_loss: 17.5986 - val_MinusLogProbMetric: 17.5986 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 401/1000
2023-09-27 14:31:54.733 
Epoch 401/1000 
	 loss: 16.6894, MinusLogProbMetric: 16.6894, val_loss: 17.5508, val_MinusLogProbMetric: 17.5508

Epoch 401: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.6894 - MinusLogProbMetric: 16.6894 - val_loss: 17.5508 - val_MinusLogProbMetric: 17.5508 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 402/1000
2023-09-27 14:32:58.713 
Epoch 402/1000 
	 loss: 16.4025, MinusLogProbMetric: 16.4025, val_loss: 17.2918, val_MinusLogProbMetric: 17.2918

Epoch 402: val_loss did not improve from 17.24485
196/196 - 64s - loss: 16.4025 - MinusLogProbMetric: 16.4025 - val_loss: 17.2918 - val_MinusLogProbMetric: 17.2918 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 403/1000
2023-09-27 14:34:02.293 
Epoch 403/1000 
	 loss: 16.3756, MinusLogProbMetric: 16.3756, val_loss: 17.1733, val_MinusLogProbMetric: 17.1733

Epoch 403: val_loss improved from 17.24485 to 17.17334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 64s - loss: 16.3756 - MinusLogProbMetric: 16.3756 - val_loss: 17.1733 - val_MinusLogProbMetric: 17.1733 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 404/1000
2023-09-27 14:35:06.999 
Epoch 404/1000 
	 loss: 16.3846, MinusLogProbMetric: 16.3846, val_loss: 17.3089, val_MinusLogProbMetric: 17.3089

Epoch 404: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3846 - MinusLogProbMetric: 16.3846 - val_loss: 17.3089 - val_MinusLogProbMetric: 17.3089 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 405/1000
2023-09-27 14:36:10.889 
Epoch 405/1000 
	 loss: 16.3765, MinusLogProbMetric: 16.3765, val_loss: 17.1961, val_MinusLogProbMetric: 17.1961

Epoch 405: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3765 - MinusLogProbMetric: 16.3765 - val_loss: 17.1961 - val_MinusLogProbMetric: 17.1961 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 406/1000
2023-09-27 14:37:14.835 
Epoch 406/1000 
	 loss: 16.3816, MinusLogProbMetric: 16.3816, val_loss: 17.2920, val_MinusLogProbMetric: 17.2920

Epoch 406: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3816 - MinusLogProbMetric: 16.3816 - val_loss: 17.2920 - val_MinusLogProbMetric: 17.2920 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 407/1000
2023-09-27 14:38:18.648 
Epoch 407/1000 
	 loss: 16.3867, MinusLogProbMetric: 16.3867, val_loss: 17.2524, val_MinusLogProbMetric: 17.2524

Epoch 407: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3867 - MinusLogProbMetric: 16.3867 - val_loss: 17.2524 - val_MinusLogProbMetric: 17.2524 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 408/1000
2023-09-27 14:39:22.738 
Epoch 408/1000 
	 loss: 16.3729, MinusLogProbMetric: 16.3729, val_loss: 17.1883, val_MinusLogProbMetric: 17.1883

Epoch 408: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3729 - MinusLogProbMetric: 16.3729 - val_loss: 17.1883 - val_MinusLogProbMetric: 17.1883 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 409/1000
2023-09-27 14:40:26.525 
Epoch 409/1000 
	 loss: 16.4009, MinusLogProbMetric: 16.4009, val_loss: 17.2585, val_MinusLogProbMetric: 17.2585

Epoch 409: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.4009 - MinusLogProbMetric: 16.4009 - val_loss: 17.2585 - val_MinusLogProbMetric: 17.2585 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 410/1000
2023-09-27 14:41:30.350 
Epoch 410/1000 
	 loss: 16.3829, MinusLogProbMetric: 16.3829, val_loss: 17.2272, val_MinusLogProbMetric: 17.2272

Epoch 410: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3829 - MinusLogProbMetric: 16.3829 - val_loss: 17.2272 - val_MinusLogProbMetric: 17.2272 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 411/1000
2023-09-27 14:42:34.591 
Epoch 411/1000 
	 loss: 16.4179, MinusLogProbMetric: 16.4179, val_loss: 17.2120, val_MinusLogProbMetric: 17.2120

Epoch 411: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.4179 - MinusLogProbMetric: 16.4179 - val_loss: 17.2120 - val_MinusLogProbMetric: 17.2120 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 412/1000
2023-09-27 14:43:38.338 
Epoch 412/1000 
	 loss: 16.3829, MinusLogProbMetric: 16.3829, val_loss: 17.3266, val_MinusLogProbMetric: 17.3266

Epoch 412: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3829 - MinusLogProbMetric: 16.3829 - val_loss: 17.3266 - val_MinusLogProbMetric: 17.3266 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 413/1000
2023-09-27 14:44:42.535 
Epoch 413/1000 
	 loss: 16.3819, MinusLogProbMetric: 16.3819, val_loss: 17.2679, val_MinusLogProbMetric: 17.2679

Epoch 413: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3819 - MinusLogProbMetric: 16.3819 - val_loss: 17.2679 - val_MinusLogProbMetric: 17.2679 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 414/1000
2023-09-27 14:45:46.039 
Epoch 414/1000 
	 loss: 16.3902, MinusLogProbMetric: 16.3902, val_loss: 17.3102, val_MinusLogProbMetric: 17.3102

Epoch 414: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3902 - MinusLogProbMetric: 16.3902 - val_loss: 17.3102 - val_MinusLogProbMetric: 17.3102 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 415/1000
2023-09-27 14:46:49.424 
Epoch 415/1000 
	 loss: 16.3912, MinusLogProbMetric: 16.3912, val_loss: 17.2553, val_MinusLogProbMetric: 17.2553

Epoch 415: val_loss did not improve from 17.17334
196/196 - 63s - loss: 16.3912 - MinusLogProbMetric: 16.3912 - val_loss: 17.2553 - val_MinusLogProbMetric: 17.2553 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 416/1000
2023-09-27 14:47:53.410 
Epoch 416/1000 
	 loss: 16.4006, MinusLogProbMetric: 16.4006, val_loss: 17.3389, val_MinusLogProbMetric: 17.3389

Epoch 416: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.4006 - MinusLogProbMetric: 16.4006 - val_loss: 17.3389 - val_MinusLogProbMetric: 17.3389 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 417/1000
2023-09-27 14:48:56.996 
Epoch 417/1000 
	 loss: 16.3677, MinusLogProbMetric: 16.3677, val_loss: 17.3658, val_MinusLogProbMetric: 17.3658

Epoch 417: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3677 - MinusLogProbMetric: 16.3677 - val_loss: 17.3658 - val_MinusLogProbMetric: 17.3658 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 418/1000
2023-09-27 14:50:00.780 
Epoch 418/1000 
	 loss: 16.3771, MinusLogProbMetric: 16.3771, val_loss: 17.1936, val_MinusLogProbMetric: 17.1936

Epoch 418: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3771 - MinusLogProbMetric: 16.3771 - val_loss: 17.1936 - val_MinusLogProbMetric: 17.1936 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 419/1000
2023-09-27 14:51:04.981 
Epoch 419/1000 
	 loss: 16.3646, MinusLogProbMetric: 16.3646, val_loss: 17.2927, val_MinusLogProbMetric: 17.2927

Epoch 419: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3646 - MinusLogProbMetric: 16.3646 - val_loss: 17.2927 - val_MinusLogProbMetric: 17.2927 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 420/1000
2023-09-27 14:52:08.887 
Epoch 420/1000 
	 loss: 16.3675, MinusLogProbMetric: 16.3675, val_loss: 17.2454, val_MinusLogProbMetric: 17.2454

Epoch 420: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3675 - MinusLogProbMetric: 16.3675 - val_loss: 17.2454 - val_MinusLogProbMetric: 17.2454 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 421/1000
2023-09-27 14:53:12.986 
Epoch 421/1000 
	 loss: 16.3805, MinusLogProbMetric: 16.3805, val_loss: 17.2619, val_MinusLogProbMetric: 17.2619

Epoch 421: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3805 - MinusLogProbMetric: 16.3805 - val_loss: 17.2619 - val_MinusLogProbMetric: 17.2619 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 422/1000
2023-09-27 14:54:16.951 
Epoch 422/1000 
	 loss: 16.3767, MinusLogProbMetric: 16.3767, val_loss: 17.1796, val_MinusLogProbMetric: 17.1796

Epoch 422: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3767 - MinusLogProbMetric: 16.3767 - val_loss: 17.1796 - val_MinusLogProbMetric: 17.1796 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 423/1000
2023-09-27 14:55:20.952 
Epoch 423/1000 
	 loss: 16.3904, MinusLogProbMetric: 16.3904, val_loss: 17.2650, val_MinusLogProbMetric: 17.2650

Epoch 423: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3904 - MinusLogProbMetric: 16.3904 - val_loss: 17.2650 - val_MinusLogProbMetric: 17.2650 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 424/1000
2023-09-27 14:56:24.598 
Epoch 424/1000 
	 loss: 16.3950, MinusLogProbMetric: 16.3950, val_loss: 17.2639, val_MinusLogProbMetric: 17.2639

Epoch 424: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3950 - MinusLogProbMetric: 16.3950 - val_loss: 17.2639 - val_MinusLogProbMetric: 17.2639 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 425/1000
2023-09-27 14:57:28.335 
Epoch 425/1000 
	 loss: 16.3780, MinusLogProbMetric: 16.3780, val_loss: 17.2824, val_MinusLogProbMetric: 17.2824

Epoch 425: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3780 - MinusLogProbMetric: 16.3780 - val_loss: 17.2824 - val_MinusLogProbMetric: 17.2824 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 426/1000
2023-09-27 14:58:31.960 
Epoch 426/1000 
	 loss: 16.4136, MinusLogProbMetric: 16.4136, val_loss: 17.2454, val_MinusLogProbMetric: 17.2454

Epoch 426: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.4136 - MinusLogProbMetric: 16.4136 - val_loss: 17.2454 - val_MinusLogProbMetric: 17.2454 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 427/1000
2023-09-27 14:59:36.057 
Epoch 427/1000 
	 loss: 16.3905, MinusLogProbMetric: 16.3905, val_loss: 17.2573, val_MinusLogProbMetric: 17.2573

Epoch 427: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3905 - MinusLogProbMetric: 16.3905 - val_loss: 17.2573 - val_MinusLogProbMetric: 17.2573 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 428/1000
2023-09-27 15:00:40.271 
Epoch 428/1000 
	 loss: 16.3688, MinusLogProbMetric: 16.3688, val_loss: 17.2056, val_MinusLogProbMetric: 17.2056

Epoch 428: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3688 - MinusLogProbMetric: 16.3688 - val_loss: 17.2056 - val_MinusLogProbMetric: 17.2056 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 429/1000
2023-09-27 15:01:44.387 
Epoch 429/1000 
	 loss: 16.3730, MinusLogProbMetric: 16.3730, val_loss: 17.1960, val_MinusLogProbMetric: 17.1960

Epoch 429: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3730 - MinusLogProbMetric: 16.3730 - val_loss: 17.1960 - val_MinusLogProbMetric: 17.1960 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 430/1000
2023-09-27 15:02:48.611 
Epoch 430/1000 
	 loss: 16.3615, MinusLogProbMetric: 16.3615, val_loss: 17.2339, val_MinusLogProbMetric: 17.2339

Epoch 430: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3615 - MinusLogProbMetric: 16.3615 - val_loss: 17.2339 - val_MinusLogProbMetric: 17.2339 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 431/1000
2023-09-27 15:03:52.272 
Epoch 431/1000 
	 loss: 16.3785, MinusLogProbMetric: 16.3785, val_loss: 17.3469, val_MinusLogProbMetric: 17.3469

Epoch 431: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3785 - MinusLogProbMetric: 16.3785 - val_loss: 17.3469 - val_MinusLogProbMetric: 17.3469 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 432/1000
2023-09-27 15:04:55.879 
Epoch 432/1000 
	 loss: 16.3604, MinusLogProbMetric: 16.3604, val_loss: 17.2499, val_MinusLogProbMetric: 17.2499

Epoch 432: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3604 - MinusLogProbMetric: 16.3604 - val_loss: 17.2499 - val_MinusLogProbMetric: 17.2499 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 433/1000
2023-09-27 15:05:59.737 
Epoch 433/1000 
	 loss: 16.3651, MinusLogProbMetric: 16.3651, val_loss: 17.2319, val_MinusLogProbMetric: 17.2319

Epoch 433: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3651 - MinusLogProbMetric: 16.3651 - val_loss: 17.2319 - val_MinusLogProbMetric: 17.2319 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 434/1000
2023-09-27 15:07:04.149 
Epoch 434/1000 
	 loss: 16.3848, MinusLogProbMetric: 16.3848, val_loss: 17.2828, val_MinusLogProbMetric: 17.2828

Epoch 434: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3848 - MinusLogProbMetric: 16.3848 - val_loss: 17.2828 - val_MinusLogProbMetric: 17.2828 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 435/1000
2023-09-27 15:08:07.662 
Epoch 435/1000 
	 loss: 16.3905, MinusLogProbMetric: 16.3905, val_loss: 17.2384, val_MinusLogProbMetric: 17.2384

Epoch 435: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3905 - MinusLogProbMetric: 16.3905 - val_loss: 17.2384 - val_MinusLogProbMetric: 17.2384 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 436/1000
2023-09-27 15:09:11.979 
Epoch 436/1000 
	 loss: 16.3664, MinusLogProbMetric: 16.3664, val_loss: 17.2767, val_MinusLogProbMetric: 17.2767

Epoch 436: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3664 - MinusLogProbMetric: 16.3664 - val_loss: 17.2767 - val_MinusLogProbMetric: 17.2767 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 437/1000
2023-09-27 15:10:16.289 
Epoch 437/1000 
	 loss: 16.3538, MinusLogProbMetric: 16.3538, val_loss: 17.1906, val_MinusLogProbMetric: 17.1906

Epoch 437: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3538 - MinusLogProbMetric: 16.3538 - val_loss: 17.1906 - val_MinusLogProbMetric: 17.1906 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 438/1000
2023-09-27 15:11:20.136 
Epoch 438/1000 
	 loss: 16.3423, MinusLogProbMetric: 16.3423, val_loss: 17.3423, val_MinusLogProbMetric: 17.3423

Epoch 438: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3423 - MinusLogProbMetric: 16.3423 - val_loss: 17.3423 - val_MinusLogProbMetric: 17.3423 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 439/1000
2023-09-27 15:12:23.940 
Epoch 439/1000 
	 loss: 16.3888, MinusLogProbMetric: 16.3888, val_loss: 17.3095, val_MinusLogProbMetric: 17.3095

Epoch 439: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3888 - MinusLogProbMetric: 16.3888 - val_loss: 17.3095 - val_MinusLogProbMetric: 17.3095 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 440/1000
2023-09-27 15:13:27.538 
Epoch 440/1000 
	 loss: 16.3627, MinusLogProbMetric: 16.3627, val_loss: 17.2168, val_MinusLogProbMetric: 17.2168

Epoch 440: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3627 - MinusLogProbMetric: 16.3627 - val_loss: 17.2168 - val_MinusLogProbMetric: 17.2168 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 441/1000
2023-09-27 15:14:31.600 
Epoch 441/1000 
	 loss: 16.3572, MinusLogProbMetric: 16.3572, val_loss: 17.2072, val_MinusLogProbMetric: 17.2072

Epoch 441: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3572 - MinusLogProbMetric: 16.3572 - val_loss: 17.2072 - val_MinusLogProbMetric: 17.2072 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 442/1000
2023-09-27 15:15:35.736 
Epoch 442/1000 
	 loss: 16.3555, MinusLogProbMetric: 16.3555, val_loss: 17.3190, val_MinusLogProbMetric: 17.3190

Epoch 442: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3555 - MinusLogProbMetric: 16.3555 - val_loss: 17.3190 - val_MinusLogProbMetric: 17.3190 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 443/1000
2023-09-27 15:16:39.707 
Epoch 443/1000 
	 loss: 16.3787, MinusLogProbMetric: 16.3787, val_loss: 17.3718, val_MinusLogProbMetric: 17.3718

Epoch 443: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3787 - MinusLogProbMetric: 16.3787 - val_loss: 17.3718 - val_MinusLogProbMetric: 17.3718 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 444/1000
2023-09-27 15:17:43.936 
Epoch 444/1000 
	 loss: 16.3698, MinusLogProbMetric: 16.3698, val_loss: 17.3483, val_MinusLogProbMetric: 17.3483

Epoch 444: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3698 - MinusLogProbMetric: 16.3698 - val_loss: 17.3483 - val_MinusLogProbMetric: 17.3483 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 445/1000
2023-09-27 15:18:47.623 
Epoch 445/1000 
	 loss: 16.3782, MinusLogProbMetric: 16.3782, val_loss: 17.2523, val_MinusLogProbMetric: 17.2523

Epoch 445: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3782 - MinusLogProbMetric: 16.3782 - val_loss: 17.2523 - val_MinusLogProbMetric: 17.2523 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 446/1000
2023-09-27 15:19:51.226 
Epoch 446/1000 
	 loss: 16.3591, MinusLogProbMetric: 16.3591, val_loss: 17.3531, val_MinusLogProbMetric: 17.3531

Epoch 446: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3591 - MinusLogProbMetric: 16.3591 - val_loss: 17.3531 - val_MinusLogProbMetric: 17.3531 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 447/1000
2023-09-27 15:20:54.588 
Epoch 447/1000 
	 loss: 16.3785, MinusLogProbMetric: 16.3785, val_loss: 17.3437, val_MinusLogProbMetric: 17.3437

Epoch 447: val_loss did not improve from 17.17334
196/196 - 63s - loss: 16.3785 - MinusLogProbMetric: 16.3785 - val_loss: 17.3437 - val_MinusLogProbMetric: 17.3437 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 448/1000
2023-09-27 15:21:58.265 
Epoch 448/1000 
	 loss: 16.3666, MinusLogProbMetric: 16.3666, val_loss: 17.4423, val_MinusLogProbMetric: 17.4423

Epoch 448: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3666 - MinusLogProbMetric: 16.3666 - val_loss: 17.4423 - val_MinusLogProbMetric: 17.4423 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 449/1000
2023-09-27 15:23:01.808 
Epoch 449/1000 
	 loss: 16.3888, MinusLogProbMetric: 16.3888, val_loss: 17.3383, val_MinusLogProbMetric: 17.3383

Epoch 449: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3888 - MinusLogProbMetric: 16.3888 - val_loss: 17.3383 - val_MinusLogProbMetric: 17.3383 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 450/1000
2023-09-27 15:24:05.914 
Epoch 450/1000 
	 loss: 16.3527, MinusLogProbMetric: 16.3527, val_loss: 17.2591, val_MinusLogProbMetric: 17.2591

Epoch 450: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3527 - MinusLogProbMetric: 16.3527 - val_loss: 17.2591 - val_MinusLogProbMetric: 17.2591 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 451/1000
2023-09-27 15:25:09.787 
Epoch 451/1000 
	 loss: 16.3555, MinusLogProbMetric: 16.3555, val_loss: 17.2179, val_MinusLogProbMetric: 17.2179

Epoch 451: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.3555 - MinusLogProbMetric: 16.3555 - val_loss: 17.2179 - val_MinusLogProbMetric: 17.2179 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 452/1000
2023-09-27 15:26:14.365 
Epoch 452/1000 
	 loss: 16.3260, MinusLogProbMetric: 16.3260, val_loss: 17.2358, val_MinusLogProbMetric: 17.2358

Epoch 452: val_loss did not improve from 17.17334
196/196 - 65s - loss: 16.3260 - MinusLogProbMetric: 16.3260 - val_loss: 17.2358 - val_MinusLogProbMetric: 17.2358 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 453/1000
2023-09-27 15:27:19.194 
Epoch 453/1000 
	 loss: 16.3588, MinusLogProbMetric: 16.3588, val_loss: 17.2952, val_MinusLogProbMetric: 17.2952

Epoch 453: val_loss did not improve from 17.17334
196/196 - 65s - loss: 16.3588 - MinusLogProbMetric: 16.3588 - val_loss: 17.2952 - val_MinusLogProbMetric: 17.2952 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 454/1000
2023-09-27 15:28:23.009 
Epoch 454/1000 
	 loss: 16.2326, MinusLogProbMetric: 16.2326, val_loss: 17.1814, val_MinusLogProbMetric: 17.1814

Epoch 454: val_loss did not improve from 17.17334
196/196 - 64s - loss: 16.2326 - MinusLogProbMetric: 16.2326 - val_loss: 17.1814 - val_MinusLogProbMetric: 17.1814 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 455/1000
2023-09-27 15:29:27.058 
Epoch 455/1000 
	 loss: 16.2234, MinusLogProbMetric: 16.2234, val_loss: 17.1677, val_MinusLogProbMetric: 17.1677

Epoch 455: val_loss improved from 17.17334 to 17.16770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 16.2234 - MinusLogProbMetric: 16.2234 - val_loss: 17.1677 - val_MinusLogProbMetric: 17.1677 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 456/1000
2023-09-27 15:30:31.951 
Epoch 456/1000 
	 loss: 16.2135, MinusLogProbMetric: 16.2135, val_loss: 17.2011, val_MinusLogProbMetric: 17.2011

Epoch 456: val_loss did not improve from 17.16770
196/196 - 64s - loss: 16.2135 - MinusLogProbMetric: 16.2135 - val_loss: 17.2011 - val_MinusLogProbMetric: 17.2011 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 457/1000
2023-09-27 15:31:35.495 
Epoch 457/1000 
	 loss: 16.2259, MinusLogProbMetric: 16.2259, val_loss: 17.1844, val_MinusLogProbMetric: 17.1844

Epoch 457: val_loss did not improve from 17.16770
196/196 - 64s - loss: 16.2259 - MinusLogProbMetric: 16.2259 - val_loss: 17.1844 - val_MinusLogProbMetric: 17.1844 - lr: 8.3333e-05 - 64s/epoch - 324ms/step
Epoch 458/1000
2023-09-27 15:32:39.316 
Epoch 458/1000 
	 loss: 16.2191, MinusLogProbMetric: 16.2191, val_loss: 17.2023, val_MinusLogProbMetric: 17.2023

Epoch 458: val_loss did not improve from 17.16770
196/196 - 64s - loss: 16.2191 - MinusLogProbMetric: 16.2191 - val_loss: 17.2023 - val_MinusLogProbMetric: 17.2023 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 459/1000
2023-09-27 15:33:42.890 
Epoch 459/1000 
	 loss: 16.2132, MinusLogProbMetric: 16.2132, val_loss: 17.1394, val_MinusLogProbMetric: 17.1394

Epoch 459: val_loss improved from 17.16770 to 17.13944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 16.2132 - MinusLogProbMetric: 16.2132 - val_loss: 17.1394 - val_MinusLogProbMetric: 17.1394 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 460/1000
2023-09-27 15:34:43.836 
Epoch 460/1000 
	 loss: 16.2120, MinusLogProbMetric: 16.2120, val_loss: 17.1651, val_MinusLogProbMetric: 17.1651

Epoch 460: val_loss did not improve from 17.13944
196/196 - 60s - loss: 16.2120 - MinusLogProbMetric: 16.2120 - val_loss: 17.1651 - val_MinusLogProbMetric: 17.1651 - lr: 8.3333e-05 - 60s/epoch - 306ms/step
Epoch 461/1000
2023-09-27 15:35:38.445 
Epoch 461/1000 
	 loss: 16.2245, MinusLogProbMetric: 16.2245, val_loss: 17.1334, val_MinusLogProbMetric: 17.1334

Epoch 461: val_loss improved from 17.13944 to 17.13338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 55s - loss: 16.2245 - MinusLogProbMetric: 16.2245 - val_loss: 17.1334 - val_MinusLogProbMetric: 17.1334 - lr: 8.3333e-05 - 55s/epoch - 283ms/step
Epoch 462/1000
2023-09-27 15:36:40.565 
Epoch 462/1000 
	 loss: 16.2257, MinusLogProbMetric: 16.2257, val_loss: 17.1641, val_MinusLogProbMetric: 17.1641

Epoch 462: val_loss did not improve from 17.13338
196/196 - 61s - loss: 16.2257 - MinusLogProbMetric: 16.2257 - val_loss: 17.1641 - val_MinusLogProbMetric: 17.1641 - lr: 8.3333e-05 - 61s/epoch - 312ms/step
Epoch 463/1000
2023-09-27 15:37:38.169 
Epoch 463/1000 
	 loss: 16.2107, MinusLogProbMetric: 16.2107, val_loss: 17.1999, val_MinusLogProbMetric: 17.1999

Epoch 463: val_loss did not improve from 17.13338
196/196 - 58s - loss: 16.2107 - MinusLogProbMetric: 16.2107 - val_loss: 17.1999 - val_MinusLogProbMetric: 17.1999 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 464/1000
2023-09-27 15:38:35.801 
Epoch 464/1000 
	 loss: 16.2140, MinusLogProbMetric: 16.2140, val_loss: 17.2046, val_MinusLogProbMetric: 17.2046

Epoch 464: val_loss did not improve from 17.13338
196/196 - 58s - loss: 16.2140 - MinusLogProbMetric: 16.2140 - val_loss: 17.2046 - val_MinusLogProbMetric: 17.2046 - lr: 8.3333e-05 - 58s/epoch - 294ms/step
Epoch 465/1000
2023-09-27 15:39:39.488 
Epoch 465/1000 
	 loss: 16.2046, MinusLogProbMetric: 16.2046, val_loss: 17.1497, val_MinusLogProbMetric: 17.1497

Epoch 465: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2046 - MinusLogProbMetric: 16.2046 - val_loss: 17.1497 - val_MinusLogProbMetric: 17.1497 - lr: 8.3333e-05 - 64s/epoch - 325ms/step
Epoch 466/1000
2023-09-27 15:40:41.899 
Epoch 466/1000 
	 loss: 16.2128, MinusLogProbMetric: 16.2128, val_loss: 17.1857, val_MinusLogProbMetric: 17.1857

Epoch 466: val_loss did not improve from 17.13338
196/196 - 62s - loss: 16.2128 - MinusLogProbMetric: 16.2128 - val_loss: 17.1857 - val_MinusLogProbMetric: 17.1857 - lr: 8.3333e-05 - 62s/epoch - 318ms/step
Epoch 467/1000
2023-09-27 15:41:45.452 
Epoch 467/1000 
	 loss: 16.2144, MinusLogProbMetric: 16.2144, val_loss: 17.1527, val_MinusLogProbMetric: 17.1527

Epoch 467: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2144 - MinusLogProbMetric: 16.2144 - val_loss: 17.1527 - val_MinusLogProbMetric: 17.1527 - lr: 8.3333e-05 - 64s/epoch - 324ms/step
Epoch 468/1000
2023-09-27 15:42:49.645 
Epoch 468/1000 
	 loss: 16.2169, MinusLogProbMetric: 16.2169, val_loss: 17.2111, val_MinusLogProbMetric: 17.2111

Epoch 468: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2169 - MinusLogProbMetric: 16.2169 - val_loss: 17.2111 - val_MinusLogProbMetric: 17.2111 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 469/1000
2023-09-27 15:43:53.454 
Epoch 469/1000 
	 loss: 16.2055, MinusLogProbMetric: 16.2055, val_loss: 17.2507, val_MinusLogProbMetric: 17.2507

Epoch 469: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2055 - MinusLogProbMetric: 16.2055 - val_loss: 17.2507 - val_MinusLogProbMetric: 17.2507 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 470/1000
2023-09-27 15:44:57.352 
Epoch 470/1000 
	 loss: 16.2228, MinusLogProbMetric: 16.2228, val_loss: 17.1688, val_MinusLogProbMetric: 17.1688

Epoch 470: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2228 - MinusLogProbMetric: 16.2228 - val_loss: 17.1688 - val_MinusLogProbMetric: 17.1688 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 471/1000
2023-09-27 15:46:00.904 
Epoch 471/1000 
	 loss: 16.2284, MinusLogProbMetric: 16.2284, val_loss: 17.1409, val_MinusLogProbMetric: 17.1409

Epoch 471: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2284 - MinusLogProbMetric: 16.2284 - val_loss: 17.1409 - val_MinusLogProbMetric: 17.1409 - lr: 8.3333e-05 - 64s/epoch - 324ms/step
Epoch 472/1000
2023-09-27 15:47:04.661 
Epoch 472/1000 
	 loss: 16.2003, MinusLogProbMetric: 16.2003, val_loss: 17.1577, val_MinusLogProbMetric: 17.1577

Epoch 472: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2003 - MinusLogProbMetric: 16.2003 - val_loss: 17.1577 - val_MinusLogProbMetric: 17.1577 - lr: 8.3333e-05 - 64s/epoch - 325ms/step
Epoch 473/1000
2023-09-27 15:48:08.085 
Epoch 473/1000 
	 loss: 16.2028, MinusLogProbMetric: 16.2028, val_loss: 17.1942, val_MinusLogProbMetric: 17.1942

Epoch 473: val_loss did not improve from 17.13338
196/196 - 63s - loss: 16.2028 - MinusLogProbMetric: 16.2028 - val_loss: 17.1942 - val_MinusLogProbMetric: 17.1942 - lr: 8.3333e-05 - 63s/epoch - 324ms/step
Epoch 474/1000
2023-09-27 15:49:02.773 
Epoch 474/1000 
	 loss: 16.2394, MinusLogProbMetric: 16.2394, val_loss: 17.1591, val_MinusLogProbMetric: 17.1591

Epoch 474: val_loss did not improve from 17.13338
196/196 - 55s - loss: 16.2394 - MinusLogProbMetric: 16.2394 - val_loss: 17.1591 - val_MinusLogProbMetric: 17.1591 - lr: 8.3333e-05 - 55s/epoch - 279ms/step
Epoch 475/1000
2023-09-27 15:50:03.692 
Epoch 475/1000 
	 loss: 16.2050, MinusLogProbMetric: 16.2050, val_loss: 17.1418, val_MinusLogProbMetric: 17.1418

Epoch 475: val_loss did not improve from 17.13338
196/196 - 61s - loss: 16.2050 - MinusLogProbMetric: 16.2050 - val_loss: 17.1418 - val_MinusLogProbMetric: 17.1418 - lr: 8.3333e-05 - 61s/epoch - 311ms/step
Epoch 476/1000
2023-09-27 15:51:01.214 
Epoch 476/1000 
	 loss: 16.2030, MinusLogProbMetric: 16.2030, val_loss: 17.1402, val_MinusLogProbMetric: 17.1402

Epoch 476: val_loss did not improve from 17.13338
196/196 - 58s - loss: 16.2030 - MinusLogProbMetric: 16.2030 - val_loss: 17.1402 - val_MinusLogProbMetric: 17.1402 - lr: 8.3333e-05 - 58s/epoch - 293ms/step
Epoch 477/1000
2023-09-27 15:51:55.876 
Epoch 477/1000 
	 loss: 16.2239, MinusLogProbMetric: 16.2239, val_loss: 17.2588, val_MinusLogProbMetric: 17.2588

Epoch 477: val_loss did not improve from 17.13338
196/196 - 55s - loss: 16.2239 - MinusLogProbMetric: 16.2239 - val_loss: 17.2588 - val_MinusLogProbMetric: 17.2588 - lr: 8.3333e-05 - 55s/epoch - 279ms/step
Epoch 478/1000
2023-09-27 15:52:59.799 
Epoch 478/1000 
	 loss: 16.1985, MinusLogProbMetric: 16.1985, val_loss: 17.1769, val_MinusLogProbMetric: 17.1769

Epoch 478: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1985 - MinusLogProbMetric: 16.1985 - val_loss: 17.1769 - val_MinusLogProbMetric: 17.1769 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 479/1000
2023-09-27 15:54:03.669 
Epoch 479/1000 
	 loss: 16.1939, MinusLogProbMetric: 16.1939, val_loss: 17.1642, val_MinusLogProbMetric: 17.1642

Epoch 479: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1939 - MinusLogProbMetric: 16.1939 - val_loss: 17.1642 - val_MinusLogProbMetric: 17.1642 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 480/1000
2023-09-27 15:55:04.770 
Epoch 480/1000 
	 loss: 16.2067, MinusLogProbMetric: 16.2067, val_loss: 17.1819, val_MinusLogProbMetric: 17.1819

Epoch 480: val_loss did not improve from 17.13338
196/196 - 61s - loss: 16.2067 - MinusLogProbMetric: 16.2067 - val_loss: 17.1819 - val_MinusLogProbMetric: 17.1819 - lr: 8.3333e-05 - 61s/epoch - 312ms/step
Epoch 481/1000
2023-09-27 15:56:04.563 
Epoch 481/1000 
	 loss: 16.2135, MinusLogProbMetric: 16.2135, val_loss: 17.1510, val_MinusLogProbMetric: 17.1510

Epoch 481: val_loss did not improve from 17.13338
196/196 - 60s - loss: 16.2135 - MinusLogProbMetric: 16.2135 - val_loss: 17.1510 - val_MinusLogProbMetric: 17.1510 - lr: 8.3333e-05 - 60s/epoch - 305ms/step
Epoch 482/1000
2023-09-27 15:57:08.779 
Epoch 482/1000 
	 loss: 16.1993, MinusLogProbMetric: 16.1993, val_loss: 17.2288, val_MinusLogProbMetric: 17.2288

Epoch 482: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1993 - MinusLogProbMetric: 16.1993 - val_loss: 17.2288 - val_MinusLogProbMetric: 17.2288 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 483/1000
2023-09-27 15:58:12.832 
Epoch 483/1000 
	 loss: 16.2061, MinusLogProbMetric: 16.2061, val_loss: 17.2486, val_MinusLogProbMetric: 17.2486

Epoch 483: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2061 - MinusLogProbMetric: 16.2061 - val_loss: 17.2486 - val_MinusLogProbMetric: 17.2486 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 484/1000
2023-09-27 15:59:16.948 
Epoch 484/1000 
	 loss: 16.2164, MinusLogProbMetric: 16.2164, val_loss: 17.1859, val_MinusLogProbMetric: 17.1859

Epoch 484: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2164 - MinusLogProbMetric: 16.2164 - val_loss: 17.1859 - val_MinusLogProbMetric: 17.1859 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 485/1000
2023-09-27 16:00:19.839 
Epoch 485/1000 
	 loss: 16.2114, MinusLogProbMetric: 16.2114, val_loss: 17.1821, val_MinusLogProbMetric: 17.1821

Epoch 485: val_loss did not improve from 17.13338
196/196 - 63s - loss: 16.2114 - MinusLogProbMetric: 16.2114 - val_loss: 17.1821 - val_MinusLogProbMetric: 17.1821 - lr: 8.3333e-05 - 63s/epoch - 321ms/step
Epoch 486/1000
2023-09-27 16:01:18.770 
Epoch 486/1000 
	 loss: 16.2046, MinusLogProbMetric: 16.2046, val_loss: 17.2204, val_MinusLogProbMetric: 17.2204

Epoch 486: val_loss did not improve from 17.13338
196/196 - 59s - loss: 16.2046 - MinusLogProbMetric: 16.2046 - val_loss: 17.2204 - val_MinusLogProbMetric: 17.2204 - lr: 8.3333e-05 - 59s/epoch - 301ms/step
Epoch 487/1000
2023-09-27 16:02:22.813 
Epoch 487/1000 
	 loss: 16.2162, MinusLogProbMetric: 16.2162, val_loss: 17.1858, val_MinusLogProbMetric: 17.1858

Epoch 487: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2162 - MinusLogProbMetric: 16.2162 - val_loss: 17.1858 - val_MinusLogProbMetric: 17.1858 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 488/1000
2023-09-27 16:03:26.203 
Epoch 488/1000 
	 loss: 16.2002, MinusLogProbMetric: 16.2002, val_loss: 17.1505, val_MinusLogProbMetric: 17.1505

Epoch 488: val_loss did not improve from 17.13338
196/196 - 63s - loss: 16.2002 - MinusLogProbMetric: 16.2002 - val_loss: 17.1505 - val_MinusLogProbMetric: 17.1505 - lr: 8.3333e-05 - 63s/epoch - 323ms/step
Epoch 489/1000
2023-09-27 16:04:30.236 
Epoch 489/1000 
	 loss: 16.1982, MinusLogProbMetric: 16.1982, val_loss: 17.3225, val_MinusLogProbMetric: 17.3225

Epoch 489: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1982 - MinusLogProbMetric: 16.1982 - val_loss: 17.3225 - val_MinusLogProbMetric: 17.3225 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 490/1000
2023-09-27 16:05:34.381 
Epoch 490/1000 
	 loss: 16.2081, MinusLogProbMetric: 16.2081, val_loss: 17.2012, val_MinusLogProbMetric: 17.2012

Epoch 490: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2081 - MinusLogProbMetric: 16.2081 - val_loss: 17.2012 - val_MinusLogProbMetric: 17.2012 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 491/1000
2023-09-27 16:06:38.281 
Epoch 491/1000 
	 loss: 16.2065, MinusLogProbMetric: 16.2065, val_loss: 17.1958, val_MinusLogProbMetric: 17.1958

Epoch 491: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2065 - MinusLogProbMetric: 16.2065 - val_loss: 17.1958 - val_MinusLogProbMetric: 17.1958 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 492/1000
2023-09-27 16:07:40.958 
Epoch 492/1000 
	 loss: 16.1974, MinusLogProbMetric: 16.1974, val_loss: 17.2125, val_MinusLogProbMetric: 17.2125

Epoch 492: val_loss did not improve from 17.13338
196/196 - 63s - loss: 16.1974 - MinusLogProbMetric: 16.1974 - val_loss: 17.2125 - val_MinusLogProbMetric: 17.2125 - lr: 8.3333e-05 - 63s/epoch - 320ms/step
Epoch 493/1000
2023-09-27 16:08:40.312 
Epoch 493/1000 
	 loss: 16.1982, MinusLogProbMetric: 16.1982, val_loss: 17.2106, val_MinusLogProbMetric: 17.2106

Epoch 493: val_loss did not improve from 17.13338
196/196 - 59s - loss: 16.1982 - MinusLogProbMetric: 16.1982 - val_loss: 17.2106 - val_MinusLogProbMetric: 17.2106 - lr: 8.3333e-05 - 59s/epoch - 303ms/step
Epoch 494/1000
2023-09-27 16:09:42.329 
Epoch 494/1000 
	 loss: 16.2014, MinusLogProbMetric: 16.2014, val_loss: 17.2032, val_MinusLogProbMetric: 17.2032

Epoch 494: val_loss did not improve from 17.13338
196/196 - 62s - loss: 16.2014 - MinusLogProbMetric: 16.2014 - val_loss: 17.2032 - val_MinusLogProbMetric: 17.2032 - lr: 8.3333e-05 - 62s/epoch - 316ms/step
Epoch 495/1000
2023-09-27 16:10:44.477 
Epoch 495/1000 
	 loss: 16.2035, MinusLogProbMetric: 16.2035, val_loss: 17.1853, val_MinusLogProbMetric: 17.1853

Epoch 495: val_loss did not improve from 17.13338
196/196 - 62s - loss: 16.2035 - MinusLogProbMetric: 16.2035 - val_loss: 17.1853 - val_MinusLogProbMetric: 17.1853 - lr: 8.3333e-05 - 62s/epoch - 317ms/step
Epoch 496/1000
2023-09-27 16:11:46.352 
Epoch 496/1000 
	 loss: 16.1885, MinusLogProbMetric: 16.1885, val_loss: 17.2061, val_MinusLogProbMetric: 17.2061

Epoch 496: val_loss did not improve from 17.13338
196/196 - 62s - loss: 16.1885 - MinusLogProbMetric: 16.1885 - val_loss: 17.2061 - val_MinusLogProbMetric: 17.2061 - lr: 8.3333e-05 - 62s/epoch - 316ms/step
Epoch 497/1000
2023-09-27 16:12:49.866 
Epoch 497/1000 
	 loss: 16.1909, MinusLogProbMetric: 16.1909, val_loss: 17.1477, val_MinusLogProbMetric: 17.1477

Epoch 497: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1909 - MinusLogProbMetric: 16.1909 - val_loss: 17.1477 - val_MinusLogProbMetric: 17.1477 - lr: 8.3333e-05 - 64s/epoch - 324ms/step
Epoch 498/1000
2023-09-27 16:13:50.648 
Epoch 498/1000 
	 loss: 16.2155, MinusLogProbMetric: 16.2155, val_loss: 17.1846, val_MinusLogProbMetric: 17.1846

Epoch 498: val_loss did not improve from 17.13338
196/196 - 61s - loss: 16.2155 - MinusLogProbMetric: 16.2155 - val_loss: 17.1846 - val_MinusLogProbMetric: 17.1846 - lr: 8.3333e-05 - 61s/epoch - 310ms/step
Epoch 499/1000
2023-09-27 16:14:50.015 
Epoch 499/1000 
	 loss: 16.1955, MinusLogProbMetric: 16.1955, val_loss: 17.1989, val_MinusLogProbMetric: 17.1989

Epoch 499: val_loss did not improve from 17.13338
196/196 - 59s - loss: 16.1955 - MinusLogProbMetric: 16.1955 - val_loss: 17.1989 - val_MinusLogProbMetric: 17.1989 - lr: 8.3333e-05 - 59s/epoch - 303ms/step
Epoch 500/1000
2023-09-27 16:15:53.574 
Epoch 500/1000 
	 loss: 16.2003, MinusLogProbMetric: 16.2003, val_loss: 17.1683, val_MinusLogProbMetric: 17.1683

Epoch 500: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.2003 - MinusLogProbMetric: 16.2003 - val_loss: 17.1683 - val_MinusLogProbMetric: 17.1683 - lr: 8.3333e-05 - 64s/epoch - 324ms/step
Epoch 501/1000
2023-09-27 16:16:57.276 
Epoch 501/1000 
	 loss: 16.1911, MinusLogProbMetric: 16.1911, val_loss: 17.2195, val_MinusLogProbMetric: 17.2195

Epoch 501: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1911 - MinusLogProbMetric: 16.1911 - val_loss: 17.2195 - val_MinusLogProbMetric: 17.2195 - lr: 8.3333e-05 - 64s/epoch - 325ms/step
Epoch 502/1000
2023-09-27 16:18:01.340 
Epoch 502/1000 
	 loss: 16.1903, MinusLogProbMetric: 16.1903, val_loss: 17.1486, val_MinusLogProbMetric: 17.1486

Epoch 502: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1903 - MinusLogProbMetric: 16.1903 - val_loss: 17.1486 - val_MinusLogProbMetric: 17.1486 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 503/1000
2023-09-27 16:19:00.464 
Epoch 503/1000 
	 loss: 16.1879, MinusLogProbMetric: 16.1879, val_loss: 17.3875, val_MinusLogProbMetric: 17.3875

Epoch 503: val_loss did not improve from 17.13338
196/196 - 59s - loss: 16.1879 - MinusLogProbMetric: 16.1879 - val_loss: 17.3875 - val_MinusLogProbMetric: 17.3875 - lr: 8.3333e-05 - 59s/epoch - 302ms/step
Epoch 504/1000
2023-09-27 16:20:02.151 
Epoch 504/1000 
	 loss: 16.2024, MinusLogProbMetric: 16.2024, val_loss: 17.1930, val_MinusLogProbMetric: 17.1930

Epoch 504: val_loss did not improve from 17.13338
196/196 - 62s - loss: 16.2024 - MinusLogProbMetric: 16.2024 - val_loss: 17.1930 - val_MinusLogProbMetric: 17.1930 - lr: 8.3333e-05 - 62s/epoch - 315ms/step
Epoch 505/1000
2023-09-27 16:21:06.021 
Epoch 505/1000 
	 loss: 16.1896, MinusLogProbMetric: 16.1896, val_loss: 17.2254, val_MinusLogProbMetric: 17.2254

Epoch 505: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1896 - MinusLogProbMetric: 16.1896 - val_loss: 17.2254 - val_MinusLogProbMetric: 17.2254 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 506/1000
2023-09-27 16:22:08.522 
Epoch 506/1000 
	 loss: 16.2053, MinusLogProbMetric: 16.2053, val_loss: 17.1901, val_MinusLogProbMetric: 17.1901

Epoch 506: val_loss did not improve from 17.13338
196/196 - 62s - loss: 16.2053 - MinusLogProbMetric: 16.2053 - val_loss: 17.1901 - val_MinusLogProbMetric: 17.1901 - lr: 8.3333e-05 - 62s/epoch - 319ms/step
Epoch 507/1000
2023-09-27 16:23:06.638 
Epoch 507/1000 
	 loss: 16.1860, MinusLogProbMetric: 16.1860, val_loss: 17.1579, val_MinusLogProbMetric: 17.1579

Epoch 507: val_loss did not improve from 17.13338
196/196 - 58s - loss: 16.1860 - MinusLogProbMetric: 16.1860 - val_loss: 17.1579 - val_MinusLogProbMetric: 17.1579 - lr: 8.3333e-05 - 58s/epoch - 296ms/step
Epoch 508/1000
2023-09-27 16:24:10.711 
Epoch 508/1000 
	 loss: 16.1830, MinusLogProbMetric: 16.1830, val_loss: 17.2494, val_MinusLogProbMetric: 17.2494

Epoch 508: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1830 - MinusLogProbMetric: 16.1830 - val_loss: 17.2494 - val_MinusLogProbMetric: 17.2494 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 509/1000
2023-09-27 16:25:13.924 
Epoch 509/1000 
	 loss: 16.1992, MinusLogProbMetric: 16.1992, val_loss: 17.1543, val_MinusLogProbMetric: 17.1543

Epoch 509: val_loss did not improve from 17.13338
196/196 - 63s - loss: 16.1992 - MinusLogProbMetric: 16.1992 - val_loss: 17.1543 - val_MinusLogProbMetric: 17.1543 - lr: 8.3333e-05 - 63s/epoch - 323ms/step
Epoch 510/1000
2023-09-27 16:26:17.679 
Epoch 510/1000 
	 loss: 16.1932, MinusLogProbMetric: 16.1932, val_loss: 17.1599, val_MinusLogProbMetric: 17.1599

Epoch 510: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1932 - MinusLogProbMetric: 16.1932 - val_loss: 17.1599 - val_MinusLogProbMetric: 17.1599 - lr: 8.3333e-05 - 64s/epoch - 325ms/step
Epoch 511/1000
2023-09-27 16:27:21.325 
Epoch 511/1000 
	 loss: 16.1926, MinusLogProbMetric: 16.1926, val_loss: 17.2051, val_MinusLogProbMetric: 17.2051

Epoch 511: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1926 - MinusLogProbMetric: 16.1926 - val_loss: 17.2051 - val_MinusLogProbMetric: 17.2051 - lr: 8.3333e-05 - 64s/epoch - 325ms/step
Epoch 512/1000
2023-09-27 16:28:25.209 
Epoch 512/1000 
	 loss: 16.1331, MinusLogProbMetric: 16.1331, val_loss: 17.1440, val_MinusLogProbMetric: 17.1440

Epoch 512: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1331 - MinusLogProbMetric: 16.1331 - val_loss: 17.1440 - val_MinusLogProbMetric: 17.1440 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 513/1000
2023-09-27 16:29:29.191 
Epoch 513/1000 
	 loss: 16.1271, MinusLogProbMetric: 16.1271, val_loss: 17.1812, val_MinusLogProbMetric: 17.1812

Epoch 513: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1271 - MinusLogProbMetric: 16.1271 - val_loss: 17.1812 - val_MinusLogProbMetric: 17.1812 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 514/1000
2023-09-27 16:30:33.250 
Epoch 514/1000 
	 loss: 16.1378, MinusLogProbMetric: 16.1378, val_loss: 17.1487, val_MinusLogProbMetric: 17.1487

Epoch 514: val_loss did not improve from 17.13338
196/196 - 64s - loss: 16.1378 - MinusLogProbMetric: 16.1378 - val_loss: 17.1487 - val_MinusLogProbMetric: 17.1487 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 515/1000
2023-09-27 16:31:31.298 
Epoch 515/1000 
	 loss: 16.1246, MinusLogProbMetric: 16.1246, val_loss: 17.1439, val_MinusLogProbMetric: 17.1439

Epoch 515: val_loss did not improve from 17.13338
196/196 - 58s - loss: 16.1246 - MinusLogProbMetric: 16.1246 - val_loss: 17.1439 - val_MinusLogProbMetric: 17.1439 - lr: 4.1667e-05 - 58s/epoch - 296ms/step
Epoch 516/1000
2023-09-27 16:32:31.348 
Epoch 516/1000 
	 loss: 16.1232, MinusLogProbMetric: 16.1232, val_loss: 17.1486, val_MinusLogProbMetric: 17.1486

Epoch 516: val_loss did not improve from 17.13338
196/196 - 60s - loss: 16.1232 - MinusLogProbMetric: 16.1232 - val_loss: 17.1486 - val_MinusLogProbMetric: 17.1486 - lr: 4.1667e-05 - 60s/epoch - 306ms/step
Epoch 517/1000
2023-09-27 16:33:33.430 
Epoch 517/1000 
	 loss: 16.1275, MinusLogProbMetric: 16.1275, val_loss: 17.1729, val_MinusLogProbMetric: 17.1729

Epoch 517: val_loss did not improve from 17.13338
196/196 - 62s - loss: 16.1275 - MinusLogProbMetric: 16.1275 - val_loss: 17.1729 - val_MinusLogProbMetric: 17.1729 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 518/1000
2023-09-27 16:34:35.689 
Epoch 518/1000 
	 loss: 16.1288, MinusLogProbMetric: 16.1288, val_loss: 17.1310, val_MinusLogProbMetric: 17.1310

Epoch 518: val_loss improved from 17.13338 to 17.13098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 63s - loss: 16.1288 - MinusLogProbMetric: 16.1288 - val_loss: 17.1310 - val_MinusLogProbMetric: 17.1310 - lr: 4.1667e-05 - 63s/epoch - 322ms/step
Epoch 519/1000
2023-09-27 16:35:40.390 
Epoch 519/1000 
	 loss: 16.1261, MinusLogProbMetric: 16.1261, val_loss: 17.1505, val_MinusLogProbMetric: 17.1505

Epoch 519: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1261 - MinusLogProbMetric: 16.1261 - val_loss: 17.1505 - val_MinusLogProbMetric: 17.1505 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 520/1000
2023-09-27 16:36:44.245 
Epoch 520/1000 
	 loss: 16.1289, MinusLogProbMetric: 16.1289, val_loss: 17.1413, val_MinusLogProbMetric: 17.1413

Epoch 520: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1289 - MinusLogProbMetric: 16.1289 - val_loss: 17.1413 - val_MinusLogProbMetric: 17.1413 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 521/1000
2023-09-27 16:37:47.480 
Epoch 521/1000 
	 loss: 16.1286, MinusLogProbMetric: 16.1286, val_loss: 17.1513, val_MinusLogProbMetric: 17.1513

Epoch 521: val_loss did not improve from 17.13098
196/196 - 63s - loss: 16.1286 - MinusLogProbMetric: 16.1286 - val_loss: 17.1513 - val_MinusLogProbMetric: 17.1513 - lr: 4.1667e-05 - 63s/epoch - 323ms/step
Epoch 522/1000
2023-09-27 16:38:51.314 
Epoch 522/1000 
	 loss: 16.1248, MinusLogProbMetric: 16.1248, val_loss: 17.1598, val_MinusLogProbMetric: 17.1598

Epoch 522: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1248 - MinusLogProbMetric: 16.1248 - val_loss: 17.1598 - val_MinusLogProbMetric: 17.1598 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 523/1000
2023-09-27 16:39:55.061 
Epoch 523/1000 
	 loss: 16.1239, MinusLogProbMetric: 16.1239, val_loss: 17.1358, val_MinusLogProbMetric: 17.1358

Epoch 523: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1239 - MinusLogProbMetric: 16.1239 - val_loss: 17.1358 - val_MinusLogProbMetric: 17.1358 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 524/1000
2023-09-27 16:40:59.278 
Epoch 524/1000 
	 loss: 16.1231, MinusLogProbMetric: 16.1231, val_loss: 17.1430, val_MinusLogProbMetric: 17.1430

Epoch 524: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1231 - MinusLogProbMetric: 16.1231 - val_loss: 17.1430 - val_MinusLogProbMetric: 17.1430 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 525/1000
2023-09-27 16:42:03.154 
Epoch 525/1000 
	 loss: 16.1204, MinusLogProbMetric: 16.1204, val_loss: 17.1518, val_MinusLogProbMetric: 17.1518

Epoch 525: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1204 - MinusLogProbMetric: 16.1204 - val_loss: 17.1518 - val_MinusLogProbMetric: 17.1518 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 526/1000
2023-09-27 16:43:07.301 
Epoch 526/1000 
	 loss: 16.1221, MinusLogProbMetric: 16.1221, val_loss: 17.1599, val_MinusLogProbMetric: 17.1599

Epoch 526: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1221 - MinusLogProbMetric: 16.1221 - val_loss: 17.1599 - val_MinusLogProbMetric: 17.1599 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 527/1000
2023-09-27 16:44:11.415 
Epoch 527/1000 
	 loss: 16.1302, MinusLogProbMetric: 16.1302, val_loss: 17.1869, val_MinusLogProbMetric: 17.1869

Epoch 527: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1302 - MinusLogProbMetric: 16.1302 - val_loss: 17.1869 - val_MinusLogProbMetric: 17.1869 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 528/1000
2023-09-27 16:45:15.239 
Epoch 528/1000 
	 loss: 16.1321, MinusLogProbMetric: 16.1321, val_loss: 17.1458, val_MinusLogProbMetric: 17.1458

Epoch 528: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1321 - MinusLogProbMetric: 16.1321 - val_loss: 17.1458 - val_MinusLogProbMetric: 17.1458 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 529/1000
2023-09-27 16:46:19.188 
Epoch 529/1000 
	 loss: 16.1229, MinusLogProbMetric: 16.1229, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 529: val_loss did not improve from 17.13098
196/196 - 64s - loss: 16.1229 - MinusLogProbMetric: 16.1229 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 530/1000
2023-09-27 16:47:23.194 
Epoch 530/1000 
	 loss: 16.1256, MinusLogProbMetric: 16.1256, val_loss: 17.1283, val_MinusLogProbMetric: 17.1283

Epoch 530: val_loss improved from 17.13098 to 17.12829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_302/weights/best_weights.h5
196/196 - 65s - loss: 16.1256 - MinusLogProbMetric: 16.1256 - val_loss: 17.1283 - val_MinusLogProbMetric: 17.1283 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 531/1000
2023-09-27 16:48:27.921 
Epoch 531/1000 
	 loss: 16.1226, MinusLogProbMetric: 16.1226, val_loss: 17.1454, val_MinusLogProbMetric: 17.1454

Epoch 531: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1226 - MinusLogProbMetric: 16.1226 - val_loss: 17.1454 - val_MinusLogProbMetric: 17.1454 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 532/1000
2023-09-27 16:49:32.144 
Epoch 532/1000 
	 loss: 16.1272, MinusLogProbMetric: 16.1272, val_loss: 17.1611, val_MinusLogProbMetric: 17.1611

Epoch 532: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1272 - MinusLogProbMetric: 16.1272 - val_loss: 17.1611 - val_MinusLogProbMetric: 17.1611 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 533/1000
2023-09-27 16:50:36.346 
Epoch 533/1000 
	 loss: 16.1194, MinusLogProbMetric: 16.1194, val_loss: 17.1348, val_MinusLogProbMetric: 17.1348

Epoch 533: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1194 - MinusLogProbMetric: 16.1194 - val_loss: 17.1348 - val_MinusLogProbMetric: 17.1348 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 534/1000
2023-09-27 16:51:40.544 
Epoch 534/1000 
	 loss: 16.1173, MinusLogProbMetric: 16.1173, val_loss: 17.1560, val_MinusLogProbMetric: 17.1560

Epoch 534: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1173 - MinusLogProbMetric: 16.1173 - val_loss: 17.1560 - val_MinusLogProbMetric: 17.1560 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 535/1000
2023-09-27 16:52:44.689 
Epoch 535/1000 
	 loss: 16.1181, MinusLogProbMetric: 16.1181, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 535: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1181 - MinusLogProbMetric: 16.1181 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 536/1000
2023-09-27 16:53:48.381 
Epoch 536/1000 
	 loss: 16.1159, MinusLogProbMetric: 16.1159, val_loss: 17.1392, val_MinusLogProbMetric: 17.1392

Epoch 536: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1159 - MinusLogProbMetric: 16.1159 - val_loss: 17.1392 - val_MinusLogProbMetric: 17.1392 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 537/1000
2023-09-27 16:54:52.524 
Epoch 537/1000 
	 loss: 16.1284, MinusLogProbMetric: 16.1284, val_loss: 17.1613, val_MinusLogProbMetric: 17.1613

Epoch 537: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1284 - MinusLogProbMetric: 16.1284 - val_loss: 17.1613 - val_MinusLogProbMetric: 17.1613 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 538/1000
2023-09-27 16:55:56.182 
Epoch 538/1000 
	 loss: 16.1183, MinusLogProbMetric: 16.1183, val_loss: 17.1807, val_MinusLogProbMetric: 17.1807

Epoch 538: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1183 - MinusLogProbMetric: 16.1183 - val_loss: 17.1807 - val_MinusLogProbMetric: 17.1807 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 539/1000
2023-09-27 16:57:00.123 
Epoch 539/1000 
	 loss: 16.1251, MinusLogProbMetric: 16.1251, val_loss: 17.1699, val_MinusLogProbMetric: 17.1699

Epoch 539: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1251 - MinusLogProbMetric: 16.1251 - val_loss: 17.1699 - val_MinusLogProbMetric: 17.1699 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 540/1000
2023-09-27 16:58:04.023 
Epoch 540/1000 
	 loss: 16.1185, MinusLogProbMetric: 16.1185, val_loss: 17.1573, val_MinusLogProbMetric: 17.1573

Epoch 540: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1185 - MinusLogProbMetric: 16.1185 - val_loss: 17.1573 - val_MinusLogProbMetric: 17.1573 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 541/1000
2023-09-27 16:59:08.306 
Epoch 541/1000 
	 loss: 16.1255, MinusLogProbMetric: 16.1255, val_loss: 17.2093, val_MinusLogProbMetric: 17.2093

Epoch 541: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1255 - MinusLogProbMetric: 16.1255 - val_loss: 17.2093 - val_MinusLogProbMetric: 17.2093 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 542/1000
2023-09-27 17:00:12.166 
Epoch 542/1000 
	 loss: 16.1273, MinusLogProbMetric: 16.1273, val_loss: 17.1639, val_MinusLogProbMetric: 17.1639

Epoch 542: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1273 - MinusLogProbMetric: 16.1273 - val_loss: 17.1639 - val_MinusLogProbMetric: 17.1639 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 543/1000
2023-09-27 17:01:16.107 
Epoch 543/1000 
	 loss: 16.1271, MinusLogProbMetric: 16.1271, val_loss: 17.1376, val_MinusLogProbMetric: 17.1376

Epoch 543: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1271 - MinusLogProbMetric: 16.1271 - val_loss: 17.1376 - val_MinusLogProbMetric: 17.1376 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 544/1000
2023-09-27 17:02:19.908 
Epoch 544/1000 
	 loss: 16.1178, MinusLogProbMetric: 16.1178, val_loss: 17.1775, val_MinusLogProbMetric: 17.1775

Epoch 544: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1178 - MinusLogProbMetric: 16.1178 - val_loss: 17.1775 - val_MinusLogProbMetric: 17.1775 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 545/1000
2023-09-27 17:03:23.821 
Epoch 545/1000 
	 loss: 16.1263, MinusLogProbMetric: 16.1263, val_loss: 17.1301, val_MinusLogProbMetric: 17.1301

Epoch 545: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1263 - MinusLogProbMetric: 16.1263 - val_loss: 17.1301 - val_MinusLogProbMetric: 17.1301 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 546/1000
2023-09-27 17:04:27.900 
Epoch 546/1000 
	 loss: 16.1161, MinusLogProbMetric: 16.1161, val_loss: 17.1411, val_MinusLogProbMetric: 17.1411

Epoch 546: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1161 - MinusLogProbMetric: 16.1161 - val_loss: 17.1411 - val_MinusLogProbMetric: 17.1411 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 547/1000
2023-09-27 17:05:31.792 
Epoch 547/1000 
	 loss: 16.1240, MinusLogProbMetric: 16.1240, val_loss: 17.1412, val_MinusLogProbMetric: 17.1412

Epoch 547: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1240 - MinusLogProbMetric: 16.1240 - val_loss: 17.1412 - val_MinusLogProbMetric: 17.1412 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 548/1000
2023-09-27 17:06:32.215 
Epoch 548/1000 
	 loss: 16.1205, MinusLogProbMetric: 16.1205, val_loss: 17.1435, val_MinusLogProbMetric: 17.1435

Epoch 548: val_loss did not improve from 17.12829
196/196 - 60s - loss: 16.1205 - MinusLogProbMetric: 16.1205 - val_loss: 17.1435 - val_MinusLogProbMetric: 17.1435 - lr: 4.1667e-05 - 60s/epoch - 308ms/step
Epoch 549/1000
2023-09-27 17:07:29.207 
Epoch 549/1000 
	 loss: 16.1193, MinusLogProbMetric: 16.1193, val_loss: 17.1415, val_MinusLogProbMetric: 17.1415

Epoch 549: val_loss did not improve from 17.12829
196/196 - 57s - loss: 16.1193 - MinusLogProbMetric: 16.1193 - val_loss: 17.1415 - val_MinusLogProbMetric: 17.1415 - lr: 4.1667e-05 - 57s/epoch - 291ms/step
Epoch 550/1000
2023-09-27 17:08:23.642 
Epoch 550/1000 
	 loss: 16.1182, MinusLogProbMetric: 16.1182, val_loss: 17.1529, val_MinusLogProbMetric: 17.1529

Epoch 550: val_loss did not improve from 17.12829
196/196 - 54s - loss: 16.1182 - MinusLogProbMetric: 16.1182 - val_loss: 17.1529 - val_MinusLogProbMetric: 17.1529 - lr: 4.1667e-05 - 54s/epoch - 278ms/step
Epoch 551/1000
2023-09-27 17:09:23.818 
Epoch 551/1000 
	 loss: 16.1147, MinusLogProbMetric: 16.1147, val_loss: 17.1476, val_MinusLogProbMetric: 17.1476

Epoch 551: val_loss did not improve from 17.12829
196/196 - 60s - loss: 16.1147 - MinusLogProbMetric: 16.1147 - val_loss: 17.1476 - val_MinusLogProbMetric: 17.1476 - lr: 4.1667e-05 - 60s/epoch - 307ms/step
Epoch 552/1000
2023-09-27 17:10:23.068 
Epoch 552/1000 
	 loss: 16.1178, MinusLogProbMetric: 16.1178, val_loss: 17.1415, val_MinusLogProbMetric: 17.1415

Epoch 552: val_loss did not improve from 17.12829
196/196 - 59s - loss: 16.1178 - MinusLogProbMetric: 16.1178 - val_loss: 17.1415 - val_MinusLogProbMetric: 17.1415 - lr: 4.1667e-05 - 59s/epoch - 302ms/step
Epoch 553/1000
2023-09-27 17:11:18.206 
Epoch 553/1000 
	 loss: 16.1171, MinusLogProbMetric: 16.1171, val_loss: 17.1837, val_MinusLogProbMetric: 17.1837

Epoch 553: val_loss did not improve from 17.12829
196/196 - 55s - loss: 16.1171 - MinusLogProbMetric: 16.1171 - val_loss: 17.1837 - val_MinusLogProbMetric: 17.1837 - lr: 4.1667e-05 - 55s/epoch - 281ms/step
Epoch 554/1000
2023-09-27 17:12:18.382 
Epoch 554/1000 
	 loss: 16.1133, MinusLogProbMetric: 16.1133, val_loss: 17.1514, val_MinusLogProbMetric: 17.1514

Epoch 554: val_loss did not improve from 17.12829
196/196 - 60s - loss: 16.1133 - MinusLogProbMetric: 16.1133 - val_loss: 17.1514 - val_MinusLogProbMetric: 17.1514 - lr: 4.1667e-05 - 60s/epoch - 307ms/step
Epoch 555/1000
2023-09-27 17:13:22.762 
Epoch 555/1000 
	 loss: 16.1153, MinusLogProbMetric: 16.1153, val_loss: 17.1581, val_MinusLogProbMetric: 17.1581

Epoch 555: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1153 - MinusLogProbMetric: 16.1153 - val_loss: 17.1581 - val_MinusLogProbMetric: 17.1581 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 556/1000
2023-09-27 17:14:26.397 
Epoch 556/1000 
	 loss: 16.1158, MinusLogProbMetric: 16.1158, val_loss: 17.1442, val_MinusLogProbMetric: 17.1442

Epoch 556: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1158 - MinusLogProbMetric: 16.1158 - val_loss: 17.1442 - val_MinusLogProbMetric: 17.1442 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 557/1000
2023-09-27 17:15:30.449 
Epoch 557/1000 
	 loss: 16.1184, MinusLogProbMetric: 16.1184, val_loss: 17.1462, val_MinusLogProbMetric: 17.1462

Epoch 557: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1184 - MinusLogProbMetric: 16.1184 - val_loss: 17.1462 - val_MinusLogProbMetric: 17.1462 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 558/1000
2023-09-27 17:16:34.411 
Epoch 558/1000 
	 loss: 16.1199, MinusLogProbMetric: 16.1199, val_loss: 17.1410, val_MinusLogProbMetric: 17.1410

Epoch 558: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1199 - MinusLogProbMetric: 16.1199 - val_loss: 17.1410 - val_MinusLogProbMetric: 17.1410 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 559/1000
2023-09-27 17:17:38.705 
Epoch 559/1000 
	 loss: 16.1178, MinusLogProbMetric: 16.1178, val_loss: 17.1522, val_MinusLogProbMetric: 17.1522

Epoch 559: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1178 - MinusLogProbMetric: 16.1178 - val_loss: 17.1522 - val_MinusLogProbMetric: 17.1522 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 560/1000
2023-09-27 17:18:42.354 
Epoch 560/1000 
	 loss: 16.1173, MinusLogProbMetric: 16.1173, val_loss: 17.1689, val_MinusLogProbMetric: 17.1689

Epoch 560: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1173 - MinusLogProbMetric: 16.1173 - val_loss: 17.1689 - val_MinusLogProbMetric: 17.1689 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 561/1000
2023-09-27 17:19:45.986 
Epoch 561/1000 
	 loss: 16.1251, MinusLogProbMetric: 16.1251, val_loss: 17.1832, val_MinusLogProbMetric: 17.1832

Epoch 561: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1251 - MinusLogProbMetric: 16.1251 - val_loss: 17.1832 - val_MinusLogProbMetric: 17.1832 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 562/1000
2023-09-27 17:20:49.651 
Epoch 562/1000 
	 loss: 16.1173, MinusLogProbMetric: 16.1173, val_loss: 17.1532, val_MinusLogProbMetric: 17.1532

Epoch 562: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1173 - MinusLogProbMetric: 16.1173 - val_loss: 17.1532 - val_MinusLogProbMetric: 17.1532 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 563/1000
2023-09-27 17:21:53.725 
Epoch 563/1000 
	 loss: 16.1098, MinusLogProbMetric: 16.1098, val_loss: 17.1701, val_MinusLogProbMetric: 17.1701

Epoch 563: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1098 - MinusLogProbMetric: 16.1098 - val_loss: 17.1701 - val_MinusLogProbMetric: 17.1701 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 564/1000
2023-09-27 17:22:57.564 
Epoch 564/1000 
	 loss: 16.1188, MinusLogProbMetric: 16.1188, val_loss: 17.1657, val_MinusLogProbMetric: 17.1657

Epoch 564: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1188 - MinusLogProbMetric: 16.1188 - val_loss: 17.1657 - val_MinusLogProbMetric: 17.1657 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 565/1000
2023-09-27 17:24:01.771 
Epoch 565/1000 
	 loss: 16.1144, MinusLogProbMetric: 16.1144, val_loss: 17.1415, val_MinusLogProbMetric: 17.1415

Epoch 565: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1144 - MinusLogProbMetric: 16.1144 - val_loss: 17.1415 - val_MinusLogProbMetric: 17.1415 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 566/1000
2023-09-27 17:25:05.725 
Epoch 566/1000 
	 loss: 16.1158, MinusLogProbMetric: 16.1158, val_loss: 17.1458, val_MinusLogProbMetric: 17.1458

Epoch 566: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1158 - MinusLogProbMetric: 16.1158 - val_loss: 17.1458 - val_MinusLogProbMetric: 17.1458 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 567/1000
2023-09-27 17:26:09.692 
Epoch 567/1000 
	 loss: 16.1242, MinusLogProbMetric: 16.1242, val_loss: 17.1579, val_MinusLogProbMetric: 17.1579

Epoch 567: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1242 - MinusLogProbMetric: 16.1242 - val_loss: 17.1579 - val_MinusLogProbMetric: 17.1579 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 568/1000
2023-09-27 17:27:14.065 
Epoch 568/1000 
	 loss: 16.1165, MinusLogProbMetric: 16.1165, val_loss: 17.1520, val_MinusLogProbMetric: 17.1520

Epoch 568: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1165 - MinusLogProbMetric: 16.1165 - val_loss: 17.1520 - val_MinusLogProbMetric: 17.1520 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 569/1000
2023-09-27 17:28:18.371 
Epoch 569/1000 
	 loss: 16.1173, MinusLogProbMetric: 16.1173, val_loss: 17.1473, val_MinusLogProbMetric: 17.1473

Epoch 569: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1173 - MinusLogProbMetric: 16.1173 - val_loss: 17.1473 - val_MinusLogProbMetric: 17.1473 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 570/1000
2023-09-27 17:29:22.297 
Epoch 570/1000 
	 loss: 16.1136, MinusLogProbMetric: 16.1136, val_loss: 17.1895, val_MinusLogProbMetric: 17.1895

Epoch 570: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1136 - MinusLogProbMetric: 16.1136 - val_loss: 17.1895 - val_MinusLogProbMetric: 17.1895 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 571/1000
2023-09-27 17:30:26.343 
Epoch 571/1000 
	 loss: 16.1179, MinusLogProbMetric: 16.1179, val_loss: 17.1578, val_MinusLogProbMetric: 17.1578

Epoch 571: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1179 - MinusLogProbMetric: 16.1179 - val_loss: 17.1578 - val_MinusLogProbMetric: 17.1578 - lr: 4.1667e-05 - 64s/epoch - 327ms/step
Epoch 572/1000
2023-09-27 17:31:30.671 
Epoch 572/1000 
	 loss: 16.1143, MinusLogProbMetric: 16.1143, val_loss: 17.1403, val_MinusLogProbMetric: 17.1403

Epoch 572: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1143 - MinusLogProbMetric: 16.1143 - val_loss: 17.1403 - val_MinusLogProbMetric: 17.1403 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 573/1000
2023-09-27 17:32:35.042 
Epoch 573/1000 
	 loss: 16.1127, MinusLogProbMetric: 16.1127, val_loss: 17.1584, val_MinusLogProbMetric: 17.1584

Epoch 573: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1127 - MinusLogProbMetric: 16.1127 - val_loss: 17.1584 - val_MinusLogProbMetric: 17.1584 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 574/1000
2023-09-27 17:33:38.548 
Epoch 574/1000 
	 loss: 16.1191, MinusLogProbMetric: 16.1191, val_loss: 17.1446, val_MinusLogProbMetric: 17.1446

Epoch 574: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1191 - MinusLogProbMetric: 16.1191 - val_loss: 17.1446 - val_MinusLogProbMetric: 17.1446 - lr: 4.1667e-05 - 64s/epoch - 324ms/step
Epoch 575/1000
2023-09-27 17:34:43.035 
Epoch 575/1000 
	 loss: 16.1144, MinusLogProbMetric: 16.1144, val_loss: 17.1836, val_MinusLogProbMetric: 17.1836

Epoch 575: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1144 - MinusLogProbMetric: 16.1144 - val_loss: 17.1836 - val_MinusLogProbMetric: 17.1836 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 576/1000
2023-09-27 17:35:47.248 
Epoch 576/1000 
	 loss: 16.1164, MinusLogProbMetric: 16.1164, val_loss: 17.1492, val_MinusLogProbMetric: 17.1492

Epoch 576: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1164 - MinusLogProbMetric: 16.1164 - val_loss: 17.1492 - val_MinusLogProbMetric: 17.1492 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 577/1000
2023-09-27 17:36:51.095 
Epoch 577/1000 
	 loss: 16.1081, MinusLogProbMetric: 16.1081, val_loss: 17.1539, val_MinusLogProbMetric: 17.1539

Epoch 577: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1081 - MinusLogProbMetric: 16.1081 - val_loss: 17.1539 - val_MinusLogProbMetric: 17.1539 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 578/1000
2023-09-27 17:37:55.007 
Epoch 578/1000 
	 loss: 16.1152, MinusLogProbMetric: 16.1152, val_loss: 17.2385, val_MinusLogProbMetric: 17.2385

Epoch 578: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1152 - MinusLogProbMetric: 16.1152 - val_loss: 17.2385 - val_MinusLogProbMetric: 17.2385 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 579/1000
2023-09-27 17:38:58.673 
Epoch 579/1000 
	 loss: 16.1136, MinusLogProbMetric: 16.1136, val_loss: 17.1501, val_MinusLogProbMetric: 17.1501

Epoch 579: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1136 - MinusLogProbMetric: 16.1136 - val_loss: 17.1501 - val_MinusLogProbMetric: 17.1501 - lr: 4.1667e-05 - 64s/epoch - 325ms/step
Epoch 580/1000
2023-09-27 17:40:02.618 
Epoch 580/1000 
	 loss: 16.1143, MinusLogProbMetric: 16.1143, val_loss: 17.1574, val_MinusLogProbMetric: 17.1574

Epoch 580: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.1143 - MinusLogProbMetric: 16.1143 - val_loss: 17.1574 - val_MinusLogProbMetric: 17.1574 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 581/1000
2023-09-27 17:41:05.580 
Epoch 581/1000 
	 loss: 16.0824, MinusLogProbMetric: 16.0824, val_loss: 17.1420, val_MinusLogProbMetric: 17.1420

Epoch 581: val_loss did not improve from 17.12829
196/196 - 63s - loss: 16.0824 - MinusLogProbMetric: 16.0824 - val_loss: 17.1420 - val_MinusLogProbMetric: 17.1420 - lr: 2.0833e-05 - 63s/epoch - 321ms/step
Epoch 582/1000
2023-09-27 17:42:09.863 
Epoch 582/1000 
	 loss: 16.0837, MinusLogProbMetric: 16.0837, val_loss: 17.1409, val_MinusLogProbMetric: 17.1409

Epoch 582: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0837 - MinusLogProbMetric: 16.0837 - val_loss: 17.1409 - val_MinusLogProbMetric: 17.1409 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 583/1000
2023-09-27 17:43:13.592 
Epoch 583/1000 
	 loss: 16.0812, MinusLogProbMetric: 16.0812, val_loss: 17.1350, val_MinusLogProbMetric: 17.1350

Epoch 583: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0812 - MinusLogProbMetric: 16.0812 - val_loss: 17.1350 - val_MinusLogProbMetric: 17.1350 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 584/1000
2023-09-27 17:44:17.287 
Epoch 584/1000 
	 loss: 16.0787, MinusLogProbMetric: 16.0787, val_loss: 17.1376, val_MinusLogProbMetric: 17.1376

Epoch 584: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0787 - MinusLogProbMetric: 16.0787 - val_loss: 17.1376 - val_MinusLogProbMetric: 17.1376 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 585/1000
2023-09-27 17:45:21.495 
Epoch 585/1000 
	 loss: 16.0793, MinusLogProbMetric: 16.0793, val_loss: 17.1294, val_MinusLogProbMetric: 17.1294

Epoch 585: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0793 - MinusLogProbMetric: 16.0793 - val_loss: 17.1294 - val_MinusLogProbMetric: 17.1294 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 586/1000
2023-09-27 17:46:25.204 
Epoch 586/1000 
	 loss: 16.0791, MinusLogProbMetric: 16.0791, val_loss: 17.1340, val_MinusLogProbMetric: 17.1340

Epoch 586: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0791 - MinusLogProbMetric: 16.0791 - val_loss: 17.1340 - val_MinusLogProbMetric: 17.1340 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 587/1000
2023-09-27 17:47:29.223 
Epoch 587/1000 
	 loss: 16.0835, MinusLogProbMetric: 16.0835, val_loss: 17.1413, val_MinusLogProbMetric: 17.1413

Epoch 587: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0835 - MinusLogProbMetric: 16.0835 - val_loss: 17.1413 - val_MinusLogProbMetric: 17.1413 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 588/1000
2023-09-27 17:48:32.645 
Epoch 588/1000 
	 loss: 16.0807, MinusLogProbMetric: 16.0807, val_loss: 17.1385, val_MinusLogProbMetric: 17.1385

Epoch 588: val_loss did not improve from 17.12829
196/196 - 63s - loss: 16.0807 - MinusLogProbMetric: 16.0807 - val_loss: 17.1385 - val_MinusLogProbMetric: 17.1385 - lr: 2.0833e-05 - 63s/epoch - 324ms/step
Epoch 589/1000
2023-09-27 17:49:36.841 
Epoch 589/1000 
	 loss: 16.0781, MinusLogProbMetric: 16.0781, val_loss: 17.1413, val_MinusLogProbMetric: 17.1413

Epoch 589: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0781 - MinusLogProbMetric: 16.0781 - val_loss: 17.1413 - val_MinusLogProbMetric: 17.1413 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 590/1000
2023-09-27 17:50:40.538 
Epoch 590/1000 
	 loss: 16.0800, MinusLogProbMetric: 16.0800, val_loss: 17.1551, val_MinusLogProbMetric: 17.1551

Epoch 590: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0800 - MinusLogProbMetric: 16.0800 - val_loss: 17.1551 - val_MinusLogProbMetric: 17.1551 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 591/1000
2023-09-27 17:51:44.512 
Epoch 591/1000 
	 loss: 16.0771, MinusLogProbMetric: 16.0771, val_loss: 17.1399, val_MinusLogProbMetric: 17.1399

Epoch 591: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0771 - MinusLogProbMetric: 16.0771 - val_loss: 17.1399 - val_MinusLogProbMetric: 17.1399 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 592/1000
2023-09-27 17:52:48.138 
Epoch 592/1000 
	 loss: 16.0811, MinusLogProbMetric: 16.0811, val_loss: 17.1371, val_MinusLogProbMetric: 17.1371

Epoch 592: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0811 - MinusLogProbMetric: 16.0811 - val_loss: 17.1371 - val_MinusLogProbMetric: 17.1371 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 593/1000
2023-09-27 17:53:52.172 
Epoch 593/1000 
	 loss: 16.0777, MinusLogProbMetric: 16.0777, val_loss: 17.1624, val_MinusLogProbMetric: 17.1624

Epoch 593: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0777 - MinusLogProbMetric: 16.0777 - val_loss: 17.1624 - val_MinusLogProbMetric: 17.1624 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 594/1000
2023-09-27 17:54:56.003 
Epoch 594/1000 
	 loss: 16.0768, MinusLogProbMetric: 16.0768, val_loss: 17.1667, val_MinusLogProbMetric: 17.1667

Epoch 594: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0768 - MinusLogProbMetric: 16.0768 - val_loss: 17.1667 - val_MinusLogProbMetric: 17.1667 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 595/1000
2023-09-27 17:55:59.751 
Epoch 595/1000 
	 loss: 16.0784, MinusLogProbMetric: 16.0784, val_loss: 17.1410, val_MinusLogProbMetric: 17.1410

Epoch 595: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0784 - MinusLogProbMetric: 16.0784 - val_loss: 17.1410 - val_MinusLogProbMetric: 17.1410 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 596/1000
2023-09-27 17:57:03.577 
Epoch 596/1000 
	 loss: 16.0765, MinusLogProbMetric: 16.0765, val_loss: 17.1337, val_MinusLogProbMetric: 17.1337

Epoch 596: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0765 - MinusLogProbMetric: 16.0765 - val_loss: 17.1337 - val_MinusLogProbMetric: 17.1337 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 597/1000
2023-09-27 17:58:06.861 
Epoch 597/1000 
	 loss: 16.0740, MinusLogProbMetric: 16.0740, val_loss: 17.1396, val_MinusLogProbMetric: 17.1396

Epoch 597: val_loss did not improve from 17.12829
196/196 - 63s - loss: 16.0740 - MinusLogProbMetric: 16.0740 - val_loss: 17.1396 - val_MinusLogProbMetric: 17.1396 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 598/1000
2023-09-27 17:59:10.943 
Epoch 598/1000 
	 loss: 16.0762, MinusLogProbMetric: 16.0762, val_loss: 17.1512, val_MinusLogProbMetric: 17.1512

Epoch 598: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0762 - MinusLogProbMetric: 16.0762 - val_loss: 17.1512 - val_MinusLogProbMetric: 17.1512 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 599/1000
2023-09-27 18:00:14.486 
Epoch 599/1000 
	 loss: 16.0760, MinusLogProbMetric: 16.0760, val_loss: 17.1345, val_MinusLogProbMetric: 17.1345

Epoch 599: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0760 - MinusLogProbMetric: 16.0760 - val_loss: 17.1345 - val_MinusLogProbMetric: 17.1345 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 600/1000
2023-09-27 18:01:18.340 
Epoch 600/1000 
	 loss: 16.0856, MinusLogProbMetric: 16.0856, val_loss: 17.1385, val_MinusLogProbMetric: 17.1385

Epoch 600: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0856 - MinusLogProbMetric: 16.0856 - val_loss: 17.1385 - val_MinusLogProbMetric: 17.1385 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 601/1000
2023-09-27 18:02:21.895 
Epoch 601/1000 
	 loss: 16.0799, MinusLogProbMetric: 16.0799, val_loss: 17.2217, val_MinusLogProbMetric: 17.2217

Epoch 601: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0799 - MinusLogProbMetric: 16.0799 - val_loss: 17.2217 - val_MinusLogProbMetric: 17.2217 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 602/1000
2023-09-27 18:03:26.238 
Epoch 602/1000 
	 loss: 16.0863, MinusLogProbMetric: 16.0863, val_loss: 17.1383, val_MinusLogProbMetric: 17.1383

Epoch 602: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0863 - MinusLogProbMetric: 16.0863 - val_loss: 17.1383 - val_MinusLogProbMetric: 17.1383 - lr: 2.0833e-05 - 64s/epoch - 328ms/step
Epoch 603/1000
2023-09-27 18:04:30.356 
Epoch 603/1000 
	 loss: 16.0755, MinusLogProbMetric: 16.0755, val_loss: 17.1573, val_MinusLogProbMetric: 17.1573

Epoch 603: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0755 - MinusLogProbMetric: 16.0755 - val_loss: 17.1573 - val_MinusLogProbMetric: 17.1573 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 604/1000
2023-09-27 18:05:34.071 
Epoch 604/1000 
	 loss: 16.0786, MinusLogProbMetric: 16.0786, val_loss: 17.1355, val_MinusLogProbMetric: 17.1355

Epoch 604: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0786 - MinusLogProbMetric: 16.0786 - val_loss: 17.1355 - val_MinusLogProbMetric: 17.1355 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 605/1000
2023-09-27 18:06:37.986 
Epoch 605/1000 
	 loss: 16.0822, MinusLogProbMetric: 16.0822, val_loss: 17.1562, val_MinusLogProbMetric: 17.1562

Epoch 605: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0822 - MinusLogProbMetric: 16.0822 - val_loss: 17.1562 - val_MinusLogProbMetric: 17.1562 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 606/1000
2023-09-27 18:07:41.971 
Epoch 606/1000 
	 loss: 16.0796, MinusLogProbMetric: 16.0796, val_loss: 17.1411, val_MinusLogProbMetric: 17.1411

Epoch 606: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0796 - MinusLogProbMetric: 16.0796 - val_loss: 17.1411 - val_MinusLogProbMetric: 17.1411 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 607/1000
2023-09-27 18:08:45.345 
Epoch 607/1000 
	 loss: 16.0804, MinusLogProbMetric: 16.0804, val_loss: 17.1405, val_MinusLogProbMetric: 17.1405

Epoch 607: val_loss did not improve from 17.12829
196/196 - 63s - loss: 16.0804 - MinusLogProbMetric: 16.0804 - val_loss: 17.1405 - val_MinusLogProbMetric: 17.1405 - lr: 2.0833e-05 - 63s/epoch - 323ms/step
Epoch 608/1000
2023-09-27 18:09:49.365 
Epoch 608/1000 
	 loss: 16.0757, MinusLogProbMetric: 16.0757, val_loss: 17.1359, val_MinusLogProbMetric: 17.1359

Epoch 608: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0757 - MinusLogProbMetric: 16.0757 - val_loss: 17.1359 - val_MinusLogProbMetric: 17.1359 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 609/1000
2023-09-27 18:10:52.927 
Epoch 609/1000 
	 loss: 16.0783, MinusLogProbMetric: 16.0783, val_loss: 17.1617, val_MinusLogProbMetric: 17.1617

Epoch 609: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0783 - MinusLogProbMetric: 16.0783 - val_loss: 17.1617 - val_MinusLogProbMetric: 17.1617 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 610/1000
2023-09-27 18:11:56.978 
Epoch 610/1000 
	 loss: 16.0781, MinusLogProbMetric: 16.0781, val_loss: 17.1365, val_MinusLogProbMetric: 17.1365

Epoch 610: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0781 - MinusLogProbMetric: 16.0781 - val_loss: 17.1365 - val_MinusLogProbMetric: 17.1365 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 611/1000
2023-09-27 18:13:00.421 
Epoch 611/1000 
	 loss: 16.0844, MinusLogProbMetric: 16.0844, val_loss: 17.1577, val_MinusLogProbMetric: 17.1577

Epoch 611: val_loss did not improve from 17.12829
196/196 - 63s - loss: 16.0844 - MinusLogProbMetric: 16.0844 - val_loss: 17.1577 - val_MinusLogProbMetric: 17.1577 - lr: 2.0833e-05 - 63s/epoch - 324ms/step
Epoch 612/1000
2023-09-27 18:14:04.466 
Epoch 612/1000 
	 loss: 16.0809, MinusLogProbMetric: 16.0809, val_loss: 17.1583, val_MinusLogProbMetric: 17.1583

Epoch 612: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0809 - MinusLogProbMetric: 16.0809 - val_loss: 17.1583 - val_MinusLogProbMetric: 17.1583 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 613/1000
2023-09-27 18:15:08.116 
Epoch 613/1000 
	 loss: 16.0746, MinusLogProbMetric: 16.0746, val_loss: 17.1512, val_MinusLogProbMetric: 17.1512

Epoch 613: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0746 - MinusLogProbMetric: 16.0746 - val_loss: 17.1512 - val_MinusLogProbMetric: 17.1512 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 614/1000
2023-09-27 18:16:11.632 
Epoch 614/1000 
	 loss: 16.0761, MinusLogProbMetric: 16.0761, val_loss: 17.1390, val_MinusLogProbMetric: 17.1390

Epoch 614: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0761 - MinusLogProbMetric: 16.0761 - val_loss: 17.1390 - val_MinusLogProbMetric: 17.1390 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 615/1000
2023-09-27 18:17:15.176 
Epoch 615/1000 
	 loss: 16.0767, MinusLogProbMetric: 16.0767, val_loss: 17.1461, val_MinusLogProbMetric: 17.1461

Epoch 615: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0767 - MinusLogProbMetric: 16.0767 - val_loss: 17.1461 - val_MinusLogProbMetric: 17.1461 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 616/1000
2023-09-27 18:18:18.650 
Epoch 616/1000 
	 loss: 16.0755, MinusLogProbMetric: 16.0755, val_loss: 17.1393, val_MinusLogProbMetric: 17.1393

Epoch 616: val_loss did not improve from 17.12829
196/196 - 63s - loss: 16.0755 - MinusLogProbMetric: 16.0755 - val_loss: 17.1393 - val_MinusLogProbMetric: 17.1393 - lr: 2.0833e-05 - 63s/epoch - 324ms/step
Epoch 617/1000
2023-09-27 18:19:22.228 
Epoch 617/1000 
	 loss: 16.0761, MinusLogProbMetric: 16.0761, val_loss: 17.1379, val_MinusLogProbMetric: 17.1379

Epoch 617: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0761 - MinusLogProbMetric: 16.0761 - val_loss: 17.1379 - val_MinusLogProbMetric: 17.1379 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 618/1000
2023-09-27 18:20:25.847 
Epoch 618/1000 
	 loss: 16.0737, MinusLogProbMetric: 16.0737, val_loss: 17.1387, val_MinusLogProbMetric: 17.1387

Epoch 618: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0737 - MinusLogProbMetric: 16.0737 - val_loss: 17.1387 - val_MinusLogProbMetric: 17.1387 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 619/1000
2023-09-27 18:21:29.699 
Epoch 619/1000 
	 loss: 16.0763, MinusLogProbMetric: 16.0763, val_loss: 17.1472, val_MinusLogProbMetric: 17.1472

Epoch 619: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0763 - MinusLogProbMetric: 16.0763 - val_loss: 17.1472 - val_MinusLogProbMetric: 17.1472 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 620/1000
2023-09-27 18:22:33.463 
Epoch 620/1000 
	 loss: 16.0763, MinusLogProbMetric: 16.0763, val_loss: 17.1363, val_MinusLogProbMetric: 17.1363

Epoch 620: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0763 - MinusLogProbMetric: 16.0763 - val_loss: 17.1363 - val_MinusLogProbMetric: 17.1363 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 621/1000
2023-09-27 18:23:37.403 
Epoch 621/1000 
	 loss: 16.0753, MinusLogProbMetric: 16.0753, val_loss: 17.1433, val_MinusLogProbMetric: 17.1433

Epoch 621: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0753 - MinusLogProbMetric: 16.0753 - val_loss: 17.1433 - val_MinusLogProbMetric: 17.1433 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 622/1000
2023-09-27 18:24:41.478 
Epoch 622/1000 
	 loss: 16.0721, MinusLogProbMetric: 16.0721, val_loss: 17.1375, val_MinusLogProbMetric: 17.1375

Epoch 622: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0721 - MinusLogProbMetric: 16.0721 - val_loss: 17.1375 - val_MinusLogProbMetric: 17.1375 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 623/1000
2023-09-27 18:25:45.080 
Epoch 623/1000 
	 loss: 16.0767, MinusLogProbMetric: 16.0767, val_loss: 17.1484, val_MinusLogProbMetric: 17.1484

Epoch 623: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0767 - MinusLogProbMetric: 16.0767 - val_loss: 17.1484 - val_MinusLogProbMetric: 17.1484 - lr: 2.0833e-05 - 64s/epoch - 324ms/step
Epoch 624/1000
2023-09-27 18:26:49.052 
Epoch 624/1000 
	 loss: 16.0762, MinusLogProbMetric: 16.0762, val_loss: 17.1354, val_MinusLogProbMetric: 17.1354

Epoch 624: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0762 - MinusLogProbMetric: 16.0762 - val_loss: 17.1354 - val_MinusLogProbMetric: 17.1354 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 625/1000
2023-09-27 18:27:52.528 
Epoch 625/1000 
	 loss: 16.0750, MinusLogProbMetric: 16.0750, val_loss: 17.1489, val_MinusLogProbMetric: 17.1489

Epoch 625: val_loss did not improve from 17.12829
196/196 - 63s - loss: 16.0750 - MinusLogProbMetric: 16.0750 - val_loss: 17.1489 - val_MinusLogProbMetric: 17.1489 - lr: 2.0833e-05 - 63s/epoch - 324ms/step
Epoch 626/1000
2023-09-27 18:28:56.432 
Epoch 626/1000 
	 loss: 16.0756, MinusLogProbMetric: 16.0756, val_loss: 17.1445, val_MinusLogProbMetric: 17.1445

Epoch 626: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0756 - MinusLogProbMetric: 16.0756 - val_loss: 17.1445 - val_MinusLogProbMetric: 17.1445 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 627/1000
2023-09-27 18:30:00.343 
Epoch 627/1000 
	 loss: 16.0712, MinusLogProbMetric: 16.0712, val_loss: 17.1427, val_MinusLogProbMetric: 17.1427

Epoch 627: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0712 - MinusLogProbMetric: 16.0712 - val_loss: 17.1427 - val_MinusLogProbMetric: 17.1427 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 628/1000
2023-09-27 18:31:04.037 
Epoch 628/1000 
	 loss: 16.0785, MinusLogProbMetric: 16.0785, val_loss: 17.1423, val_MinusLogProbMetric: 17.1423

Epoch 628: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0785 - MinusLogProbMetric: 16.0785 - val_loss: 17.1423 - val_MinusLogProbMetric: 17.1423 - lr: 2.0833e-05 - 64s/epoch - 325ms/step
Epoch 629/1000
2023-09-27 18:32:07.906 
Epoch 629/1000 
	 loss: 16.0724, MinusLogProbMetric: 16.0724, val_loss: 17.1428, val_MinusLogProbMetric: 17.1428

Epoch 629: val_loss did not improve from 17.12829
196/196 - 64s - loss: 16.0724 - MinusLogProbMetric: 16.0724 - val_loss: 17.1428 - val_MinusLogProbMetric: 17.1428 - lr: 2.0833e-05 - 64s/epoch - 326ms/step
Epoch 630/1000
2023-09-27 18:33:11.769 
Epoch 630/1000 
	 loss: 16.0730, MinusLogProbMetric: 16.0730, val_loss: 17.1365, val_MinusLogProbMetric: 17.1365

Epoch 630: val_loss did not improve from 17.12829
Restoring model weights from the end of the best epoch: 530.
196/196 - 64s - loss: 16.0730 - MinusLogProbMetric: 16.0730 - val_loss: 17.1365 - val_MinusLogProbMetric: 17.1365 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 630: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 22.229877897014376 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 11.919265922973864 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 9.481694512011018 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 10.95786575297825 seconds.
Training succeeded with seed 869.
Model trained in 40368.96 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 56.14 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 56.43 s.
===========
Run 302/720 done in 40589.38 s.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

===========
Generating train data for run 312.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_312/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_312/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_312/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_312
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  2798560   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,798,560
Trainable params: 2,798,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7fc1ee5bbf10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc1ee6eb5b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc1ee6eb5b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc1eeed04c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1ee5eb700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1ee5ebc70>, <keras.callbacks.ModelCheckpoint object at 0x7fc1ee5ebd30>, <keras.callbacks.EarlyStopping object at 0x7fc1ee5ebfa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1ee5ebfd0>, <keras.callbacks.TerminateOnNaN object at 0x7fc1ee5ebc10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_312/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 312/720 with hyperparameters:
timestamp = 2023-09-27 18:34:20.251965
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2798560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 53: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 18:37:23.768 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2013.6731, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 183s - loss: nan - MinusLogProbMetric: 2013.6731 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 183s/epoch - 935ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 312.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_312/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_312/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_312/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_312
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  2798560   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,798,560
Trainable params: 2,798,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7fc1edf40a60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc1ed7c0160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc1ed7c0160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc0745868f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1ed566a70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1ed566fe0>, <keras.callbacks.ModelCheckpoint object at 0x7fc1ed5670a0>, <keras.callbacks.EarlyStopping object at 0x7fc1ed567310>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1ed567340>, <keras.callbacks.TerminateOnNaN object at 0x7fc1ed566f80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_312/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 312/720 with hyperparameters:
timestamp = 2023-09-27 18:37:33.125971
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2798560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 18:41:26.731 
Epoch 1/1000 
	 loss: 568.9821, MinusLogProbMetric: 568.9821, val_loss: 215.2085, val_MinusLogProbMetric: 215.2085

Epoch 1: val_loss improved from inf to 215.20845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 235s - loss: 568.9821 - MinusLogProbMetric: 568.9821 - val_loss: 215.2085 - val_MinusLogProbMetric: 215.2085 - lr: 3.3333e-04 - 235s/epoch - 1s/step
Epoch 2/1000
2023-09-27 18:42:48.227 
Epoch 2/1000 
	 loss: 128.9789, MinusLogProbMetric: 128.9789, val_loss: 80.5122, val_MinusLogProbMetric: 80.5122

Epoch 2: val_loss improved from 215.20845 to 80.51218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 128.9789 - MinusLogProbMetric: 128.9789 - val_loss: 80.5122 - val_MinusLogProbMetric: 80.5122 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 3/1000
2023-09-27 18:44:08.772 
Epoch 3/1000 
	 loss: 123.2942, MinusLogProbMetric: 123.2942, val_loss: 159.4945, val_MinusLogProbMetric: 159.4945

Epoch 3: val_loss did not improve from 80.51218
196/196 - 79s - loss: 123.2942 - MinusLogProbMetric: 123.2942 - val_loss: 159.4945 - val_MinusLogProbMetric: 159.4945 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 4/1000
2023-09-27 18:45:28.497 
Epoch 4/1000 
	 loss: 101.3747, MinusLogProbMetric: 101.3747, val_loss: 73.4291, val_MinusLogProbMetric: 73.4291

Epoch 4: val_loss improved from 80.51218 to 73.42913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 101.3747 - MinusLogProbMetric: 101.3747 - val_loss: 73.4291 - val_MinusLogProbMetric: 73.4291 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 5/1000
2023-09-27 18:46:49.659 
Epoch 5/1000 
	 loss: 58.2666, MinusLogProbMetric: 58.2666, val_loss: 49.2028, val_MinusLogProbMetric: 49.2028

Epoch 5: val_loss improved from 73.42913 to 49.20276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 58.2666 - MinusLogProbMetric: 58.2666 - val_loss: 49.2028 - val_MinusLogProbMetric: 49.2028 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 6/1000
2023-09-27 18:48:10.271 
Epoch 6/1000 
	 loss: 83.2448, MinusLogProbMetric: 83.2448, val_loss: 75.3607, val_MinusLogProbMetric: 75.3607

Epoch 6: val_loss did not improve from 49.20276
196/196 - 79s - loss: 83.2448 - MinusLogProbMetric: 83.2448 - val_loss: 75.3607 - val_MinusLogProbMetric: 75.3607 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 7/1000
2023-09-27 18:49:29.447 
Epoch 7/1000 
	 loss: 70.6959, MinusLogProbMetric: 70.6959, val_loss: 65.9432, val_MinusLogProbMetric: 65.9432

Epoch 7: val_loss did not improve from 49.20276
196/196 - 79s - loss: 70.6959 - MinusLogProbMetric: 70.6959 - val_loss: 65.9432 - val_MinusLogProbMetric: 65.9432 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 8/1000
2023-09-27 18:50:48.746 
Epoch 8/1000 
	 loss: 70.6869, MinusLogProbMetric: 70.6869, val_loss: 61.0055, val_MinusLogProbMetric: 61.0055

Epoch 8: val_loss did not improve from 49.20276
196/196 - 79s - loss: 70.6869 - MinusLogProbMetric: 70.6869 - val_loss: 61.0055 - val_MinusLogProbMetric: 61.0055 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 9/1000
2023-09-27 18:52:08.672 
Epoch 9/1000 
	 loss: 57.3296, MinusLogProbMetric: 57.3296, val_loss: 52.3743, val_MinusLogProbMetric: 52.3743

Epoch 9: val_loss did not improve from 49.20276
196/196 - 80s - loss: 57.3296 - MinusLogProbMetric: 57.3296 - val_loss: 52.3743 - val_MinusLogProbMetric: 52.3743 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 10/1000
2023-09-27 18:53:29.495 
Epoch 10/1000 
	 loss: 51.8744, MinusLogProbMetric: 51.8744, val_loss: 50.3660, val_MinusLogProbMetric: 50.3660

Epoch 10: val_loss did not improve from 49.20276
196/196 - 81s - loss: 51.8744 - MinusLogProbMetric: 51.8744 - val_loss: 50.3660 - val_MinusLogProbMetric: 50.3660 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 11/1000
2023-09-27 18:54:49.245 
Epoch 11/1000 
	 loss: 49.2667, MinusLogProbMetric: 49.2667, val_loss: 49.9778, val_MinusLogProbMetric: 49.9778

Epoch 11: val_loss did not improve from 49.20276
196/196 - 80s - loss: 49.2667 - MinusLogProbMetric: 49.2667 - val_loss: 49.9778 - val_MinusLogProbMetric: 49.9778 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 12/1000
2023-09-27 18:56:08.402 
Epoch 12/1000 
	 loss: 48.0976, MinusLogProbMetric: 48.0976, val_loss: 47.5434, val_MinusLogProbMetric: 47.5434

Epoch 12: val_loss improved from 49.20276 to 47.54342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 80s - loss: 48.0976 - MinusLogProbMetric: 48.0976 - val_loss: 47.5434 - val_MinusLogProbMetric: 47.5434 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 13/1000
2023-09-27 18:57:29.376 
Epoch 13/1000 
	 loss: 83.5573, MinusLogProbMetric: 83.5573, val_loss: 55.6241, val_MinusLogProbMetric: 55.6241

Epoch 13: val_loss did not improve from 47.54342
196/196 - 80s - loss: 83.5573 - MinusLogProbMetric: 83.5573 - val_loss: 55.6241 - val_MinusLogProbMetric: 55.6241 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 14/1000
2023-09-27 18:58:48.814 
Epoch 14/1000 
	 loss: 50.8622, MinusLogProbMetric: 50.8622, val_loss: 46.6665, val_MinusLogProbMetric: 46.6665

Epoch 14: val_loss improved from 47.54342 to 46.66650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 50.8622 - MinusLogProbMetric: 50.8622 - val_loss: 46.6665 - val_MinusLogProbMetric: 46.6665 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 15/1000
2023-09-27 19:00:10.066 
Epoch 15/1000 
	 loss: 45.0790, MinusLogProbMetric: 45.0790, val_loss: 46.1446, val_MinusLogProbMetric: 46.1446

Epoch 15: val_loss improved from 46.66650 to 46.14460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 45.0790 - MinusLogProbMetric: 45.0790 - val_loss: 46.1446 - val_MinusLogProbMetric: 46.1446 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 16/1000
2023-09-27 19:01:30.454 
Epoch 16/1000 
	 loss: 43.5030, MinusLogProbMetric: 43.5030, val_loss: 42.8099, val_MinusLogProbMetric: 42.8099

Epoch 16: val_loss improved from 46.14460 to 42.80986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 80s - loss: 43.5030 - MinusLogProbMetric: 43.5030 - val_loss: 42.8099 - val_MinusLogProbMetric: 42.8099 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 17/1000
2023-09-27 19:02:50.961 
Epoch 17/1000 
	 loss: 41.8209, MinusLogProbMetric: 41.8209, val_loss: 41.7442, val_MinusLogProbMetric: 41.7442

Epoch 17: val_loss improved from 42.80986 to 41.74422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 41.8209 - MinusLogProbMetric: 41.8209 - val_loss: 41.7442 - val_MinusLogProbMetric: 41.7442 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 18/1000
2023-09-27 19:04:11.628 
Epoch 18/1000 
	 loss: 41.0893, MinusLogProbMetric: 41.0893, val_loss: 41.0430, val_MinusLogProbMetric: 41.0430

Epoch 18: val_loss improved from 41.74422 to 41.04296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 41.0893 - MinusLogProbMetric: 41.0893 - val_loss: 41.0430 - val_MinusLogProbMetric: 41.0430 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 19/1000
2023-09-27 19:05:32.122 
Epoch 19/1000 
	 loss: 41.3534, MinusLogProbMetric: 41.3534, val_loss: 42.5605, val_MinusLogProbMetric: 42.5605

Epoch 19: val_loss did not improve from 41.04296
196/196 - 79s - loss: 41.3534 - MinusLogProbMetric: 41.3534 - val_loss: 42.5605 - val_MinusLogProbMetric: 42.5605 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 20/1000
2023-09-27 19:06:51.628 
Epoch 20/1000 
	 loss: 106.1382, MinusLogProbMetric: 106.1382, val_loss: 126.0814, val_MinusLogProbMetric: 126.0814

Epoch 20: val_loss did not improve from 41.04296
196/196 - 80s - loss: 106.1382 - MinusLogProbMetric: 106.1382 - val_loss: 126.0814 - val_MinusLogProbMetric: 126.0814 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 21/1000
2023-09-27 19:08:10.687 
Epoch 21/1000 
	 loss: 90.3251, MinusLogProbMetric: 90.3251, val_loss: 96.3076, val_MinusLogProbMetric: 96.3076

Epoch 21: val_loss did not improve from 41.04296
196/196 - 79s - loss: 90.3251 - MinusLogProbMetric: 90.3251 - val_loss: 96.3076 - val_MinusLogProbMetric: 96.3076 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 22/1000
2023-09-27 19:09:30.360 
Epoch 22/1000 
	 loss: 53.6807, MinusLogProbMetric: 53.6807, val_loss: 37.4010, val_MinusLogProbMetric: 37.4010

Epoch 22: val_loss improved from 41.04296 to 37.40095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 53.6807 - MinusLogProbMetric: 53.6807 - val_loss: 37.4010 - val_MinusLogProbMetric: 37.4010 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 23/1000
2023-09-27 19:10:51.732 
Epoch 23/1000 
	 loss: 33.6124, MinusLogProbMetric: 33.6124, val_loss: 31.7739, val_MinusLogProbMetric: 31.7739

Epoch 23: val_loss improved from 37.40095 to 31.77394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 33.6124 - MinusLogProbMetric: 33.6124 - val_loss: 31.7739 - val_MinusLogProbMetric: 31.7739 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 24/1000
2023-09-27 19:12:12.832 
Epoch 24/1000 
	 loss: 30.3990, MinusLogProbMetric: 30.3990, val_loss: 28.9364, val_MinusLogProbMetric: 28.9364

Epoch 24: val_loss improved from 31.77394 to 28.93637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 30.3990 - MinusLogProbMetric: 30.3990 - val_loss: 28.9364 - val_MinusLogProbMetric: 28.9364 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 25/1000
2023-09-27 19:13:33.608 
Epoch 25/1000 
	 loss: 26.8549, MinusLogProbMetric: 26.8549, val_loss: 25.4374, val_MinusLogProbMetric: 25.4374

Epoch 25: val_loss improved from 28.93637 to 25.43742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 26.8549 - MinusLogProbMetric: 26.8549 - val_loss: 25.4374 - val_MinusLogProbMetric: 25.4374 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 26/1000
2023-09-27 19:14:54.314 
Epoch 26/1000 
	 loss: 25.4425, MinusLogProbMetric: 25.4425, val_loss: 26.2016, val_MinusLogProbMetric: 26.2016

Epoch 26: val_loss did not improve from 25.43742
196/196 - 79s - loss: 25.4425 - MinusLogProbMetric: 25.4425 - val_loss: 26.2016 - val_MinusLogProbMetric: 26.2016 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 27/1000
2023-09-27 19:16:13.728 
Epoch 27/1000 
	 loss: 24.5938, MinusLogProbMetric: 24.5938, val_loss: 25.1456, val_MinusLogProbMetric: 25.1456

Epoch 27: val_loss improved from 25.43742 to 25.14556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 24.5938 - MinusLogProbMetric: 24.5938 - val_loss: 25.1456 - val_MinusLogProbMetric: 25.1456 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 28/1000
2023-09-27 19:17:34.181 
Epoch 28/1000 
	 loss: 24.0656, MinusLogProbMetric: 24.0656, val_loss: 22.9873, val_MinusLogProbMetric: 22.9873

Epoch 28: val_loss improved from 25.14556 to 22.98726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 24.0656 - MinusLogProbMetric: 24.0656 - val_loss: 22.9873 - val_MinusLogProbMetric: 22.9873 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 29/1000
2023-09-27 19:18:55.363 
Epoch 29/1000 
	 loss: 23.9246, MinusLogProbMetric: 23.9246, val_loss: 24.2551, val_MinusLogProbMetric: 24.2551

Epoch 29: val_loss did not improve from 22.98726
196/196 - 80s - loss: 23.9246 - MinusLogProbMetric: 23.9246 - val_loss: 24.2551 - val_MinusLogProbMetric: 24.2551 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 30/1000
2023-09-27 19:20:13.640 
Epoch 30/1000 
	 loss: 23.1472, MinusLogProbMetric: 23.1472, val_loss: 22.7071, val_MinusLogProbMetric: 22.7071

Epoch 30: val_loss improved from 22.98726 to 22.70714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 80s - loss: 23.1472 - MinusLogProbMetric: 23.1472 - val_loss: 22.7071 - val_MinusLogProbMetric: 22.7071 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 31/1000
2023-09-27 19:21:35.014 
Epoch 31/1000 
	 loss: 22.5917, MinusLogProbMetric: 22.5917, val_loss: 22.7002, val_MinusLogProbMetric: 22.7002

Epoch 31: val_loss improved from 22.70714 to 22.70019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 22.5917 - MinusLogProbMetric: 22.5917 - val_loss: 22.7002 - val_MinusLogProbMetric: 22.7002 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 32/1000
2023-09-27 19:22:55.633 
Epoch 32/1000 
	 loss: 22.1651, MinusLogProbMetric: 22.1651, val_loss: 22.3483, val_MinusLogProbMetric: 22.3483

Epoch 32: val_loss improved from 22.70019 to 22.34832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 22.1651 - MinusLogProbMetric: 22.1651 - val_loss: 22.3483 - val_MinusLogProbMetric: 22.3483 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 33/1000
2023-09-27 19:24:16.691 
Epoch 33/1000 
	 loss: 22.0098, MinusLogProbMetric: 22.0098, val_loss: 22.6422, val_MinusLogProbMetric: 22.6422

Epoch 33: val_loss did not improve from 22.34832
196/196 - 80s - loss: 22.0098 - MinusLogProbMetric: 22.0098 - val_loss: 22.6422 - val_MinusLogProbMetric: 22.6422 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 34/1000
2023-09-27 19:25:36.719 
Epoch 34/1000 
	 loss: 22.4789, MinusLogProbMetric: 22.4789, val_loss: 24.0745, val_MinusLogProbMetric: 24.0745

Epoch 34: val_loss did not improve from 22.34832
196/196 - 80s - loss: 22.4789 - MinusLogProbMetric: 22.4789 - val_loss: 24.0745 - val_MinusLogProbMetric: 24.0745 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 35/1000
2023-09-27 19:26:49.261 
Epoch 35/1000 
	 loss: 21.8083, MinusLogProbMetric: 21.8083, val_loss: 21.7035, val_MinusLogProbMetric: 21.7035

Epoch 35: val_loss improved from 22.34832 to 21.70351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 74s - loss: 21.8083 - MinusLogProbMetric: 21.8083 - val_loss: 21.7035 - val_MinusLogProbMetric: 21.7035 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 36/1000
2023-09-27 19:27:59.402 
Epoch 36/1000 
	 loss: 21.4511, MinusLogProbMetric: 21.4511, val_loss: 20.9378, val_MinusLogProbMetric: 20.9378

Epoch 36: val_loss improved from 21.70351 to 20.93780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 70s - loss: 21.4511 - MinusLogProbMetric: 21.4511 - val_loss: 20.9378 - val_MinusLogProbMetric: 20.9378 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 37/1000
2023-09-27 19:29:19.728 
Epoch 37/1000 
	 loss: 21.1373, MinusLogProbMetric: 21.1373, val_loss: 20.6606, val_MinusLogProbMetric: 20.6606

Epoch 37: val_loss improved from 20.93780 to 20.66064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 80s - loss: 21.1373 - MinusLogProbMetric: 21.1373 - val_loss: 20.6606 - val_MinusLogProbMetric: 20.6606 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 38/1000
2023-09-27 19:30:41.252 
Epoch 38/1000 
	 loss: 21.0183, MinusLogProbMetric: 21.0183, val_loss: 21.0093, val_MinusLogProbMetric: 21.0093

Epoch 38: val_loss did not improve from 20.66064
196/196 - 80s - loss: 21.0183 - MinusLogProbMetric: 21.0183 - val_loss: 21.0093 - val_MinusLogProbMetric: 21.0093 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 39/1000
2023-09-27 19:32:01.399 
Epoch 39/1000 
	 loss: 20.5995, MinusLogProbMetric: 20.5995, val_loss: 20.5277, val_MinusLogProbMetric: 20.5277

Epoch 39: val_loss improved from 20.66064 to 20.52773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 20.5995 - MinusLogProbMetric: 20.5995 - val_loss: 20.5277 - val_MinusLogProbMetric: 20.5277 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 40/1000
2023-09-27 19:33:23.138 
Epoch 40/1000 
	 loss: 20.5544, MinusLogProbMetric: 20.5544, val_loss: 20.6521, val_MinusLogProbMetric: 20.6521

Epoch 40: val_loss did not improve from 20.52773
196/196 - 80s - loss: 20.5544 - MinusLogProbMetric: 20.5544 - val_loss: 20.6521 - val_MinusLogProbMetric: 20.6521 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 41/1000
2023-09-27 19:34:43.768 
Epoch 41/1000 
	 loss: 20.4179, MinusLogProbMetric: 20.4179, val_loss: 19.8124, val_MinusLogProbMetric: 19.8124

Epoch 41: val_loss improved from 20.52773 to 19.81243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 20.4179 - MinusLogProbMetric: 20.4179 - val_loss: 19.8124 - val_MinusLogProbMetric: 19.8124 - lr: 3.3333e-04 - 82s/epoch - 419ms/step
Epoch 42/1000
2023-09-27 19:36:05.278 
Epoch 42/1000 
	 loss: 20.3320, MinusLogProbMetric: 20.3320, val_loss: 20.2805, val_MinusLogProbMetric: 20.2805

Epoch 42: val_loss did not improve from 19.81243
196/196 - 80s - loss: 20.3320 - MinusLogProbMetric: 20.3320 - val_loss: 20.2805 - val_MinusLogProbMetric: 20.2805 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 43/1000
2023-09-27 19:37:24.951 
Epoch 43/1000 
	 loss: 20.0697, MinusLogProbMetric: 20.0697, val_loss: 19.8593, val_MinusLogProbMetric: 19.8593

Epoch 43: val_loss did not improve from 19.81243
196/196 - 80s - loss: 20.0697 - MinusLogProbMetric: 20.0697 - val_loss: 19.8593 - val_MinusLogProbMetric: 19.8593 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 44/1000
2023-09-27 19:38:44.728 
Epoch 44/1000 
	 loss: 20.0015, MinusLogProbMetric: 20.0015, val_loss: 22.2505, val_MinusLogProbMetric: 22.2505

Epoch 44: val_loss did not improve from 19.81243
196/196 - 80s - loss: 20.0015 - MinusLogProbMetric: 20.0015 - val_loss: 22.2505 - val_MinusLogProbMetric: 22.2505 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 45/1000
2023-09-27 19:40:04.429 
Epoch 45/1000 
	 loss: 19.8496, MinusLogProbMetric: 19.8496, val_loss: 19.8221, val_MinusLogProbMetric: 19.8221

Epoch 45: val_loss did not improve from 19.81243
196/196 - 80s - loss: 19.8496 - MinusLogProbMetric: 19.8496 - val_loss: 19.8221 - val_MinusLogProbMetric: 19.8221 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 46/1000
2023-09-27 19:41:24.357 
Epoch 46/1000 
	 loss: 19.8819, MinusLogProbMetric: 19.8819, val_loss: 19.7918, val_MinusLogProbMetric: 19.7918

Epoch 46: val_loss improved from 19.81243 to 19.79185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 19.8819 - MinusLogProbMetric: 19.8819 - val_loss: 19.7918 - val_MinusLogProbMetric: 19.7918 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 47/1000
2023-09-27 19:42:45.499 
Epoch 47/1000 
	 loss: 19.8077, MinusLogProbMetric: 19.8077, val_loss: 19.4031, val_MinusLogProbMetric: 19.4031

Epoch 47: val_loss improved from 19.79185 to 19.40306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 19.8077 - MinusLogProbMetric: 19.8077 - val_loss: 19.4031 - val_MinusLogProbMetric: 19.4031 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 48/1000
2023-09-27 19:44:06.249 
Epoch 48/1000 
	 loss: 19.7792, MinusLogProbMetric: 19.7792, val_loss: 20.2602, val_MinusLogProbMetric: 20.2602

Epoch 48: val_loss did not improve from 19.40306
196/196 - 80s - loss: 19.7792 - MinusLogProbMetric: 19.7792 - val_loss: 20.2602 - val_MinusLogProbMetric: 20.2602 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 49/1000
2023-09-27 19:45:26.313 
Epoch 49/1000 
	 loss: 19.7287, MinusLogProbMetric: 19.7287, val_loss: 20.0407, val_MinusLogProbMetric: 20.0407

Epoch 49: val_loss did not improve from 19.40306
196/196 - 80s - loss: 19.7287 - MinusLogProbMetric: 19.7287 - val_loss: 20.0407 - val_MinusLogProbMetric: 20.0407 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 50/1000
2023-09-27 19:46:46.144 
Epoch 50/1000 
	 loss: 19.6417, MinusLogProbMetric: 19.6417, val_loss: 19.9125, val_MinusLogProbMetric: 19.9125

Epoch 50: val_loss did not improve from 19.40306
196/196 - 80s - loss: 19.6417 - MinusLogProbMetric: 19.6417 - val_loss: 19.9125 - val_MinusLogProbMetric: 19.9125 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 51/1000
2023-09-27 19:48:06.425 
Epoch 51/1000 
	 loss: 19.8503, MinusLogProbMetric: 19.8503, val_loss: 19.3661, val_MinusLogProbMetric: 19.3661

Epoch 51: val_loss improved from 19.40306 to 19.36612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 19.8503 - MinusLogProbMetric: 19.8503 - val_loss: 19.3661 - val_MinusLogProbMetric: 19.3661 - lr: 3.3333e-04 - 82s/epoch - 417ms/step
Epoch 52/1000
2023-09-27 19:49:27.215 
Epoch 52/1000 
	 loss: 19.4604, MinusLogProbMetric: 19.4604, val_loss: 19.9268, val_MinusLogProbMetric: 19.9268

Epoch 52: val_loss did not improve from 19.36612
196/196 - 79s - loss: 19.4604 - MinusLogProbMetric: 19.4604 - val_loss: 19.9268 - val_MinusLogProbMetric: 19.9268 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 53/1000
2023-09-27 19:50:46.722 
Epoch 53/1000 
	 loss: 19.4872, MinusLogProbMetric: 19.4872, val_loss: 20.0091, val_MinusLogProbMetric: 20.0091

Epoch 53: val_loss did not improve from 19.36612
196/196 - 80s - loss: 19.4872 - MinusLogProbMetric: 19.4872 - val_loss: 20.0091 - val_MinusLogProbMetric: 20.0091 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 54/1000
2023-09-27 19:52:07.194 
Epoch 54/1000 
	 loss: 19.4239, MinusLogProbMetric: 19.4239, val_loss: 20.3534, val_MinusLogProbMetric: 20.3534

Epoch 54: val_loss did not improve from 19.36612
196/196 - 80s - loss: 19.4239 - MinusLogProbMetric: 19.4239 - val_loss: 20.3534 - val_MinusLogProbMetric: 20.3534 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 55/1000
2023-09-27 19:53:28.080 
Epoch 55/1000 
	 loss: 19.5944, MinusLogProbMetric: 19.5944, val_loss: 20.4582, val_MinusLogProbMetric: 20.4582

Epoch 55: val_loss did not improve from 19.36612
196/196 - 81s - loss: 19.5944 - MinusLogProbMetric: 19.5944 - val_loss: 20.4582 - val_MinusLogProbMetric: 20.4582 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 56/1000
2023-09-27 19:54:47.905 
Epoch 56/1000 
	 loss: 19.3728, MinusLogProbMetric: 19.3728, val_loss: 19.3811, val_MinusLogProbMetric: 19.3811

Epoch 56: val_loss did not improve from 19.36612
196/196 - 80s - loss: 19.3728 - MinusLogProbMetric: 19.3728 - val_loss: 19.3811 - val_MinusLogProbMetric: 19.3811 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 57/1000
2023-09-27 19:56:07.519 
Epoch 57/1000 
	 loss: 19.2792, MinusLogProbMetric: 19.2792, val_loss: 19.2289, val_MinusLogProbMetric: 19.2289

Epoch 57: val_loss improved from 19.36612 to 19.22887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 19.2792 - MinusLogProbMetric: 19.2792 - val_loss: 19.2289 - val_MinusLogProbMetric: 19.2289 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 58/1000
2023-09-27 19:57:28.922 
Epoch 58/1000 
	 loss: 19.2474, MinusLogProbMetric: 19.2474, val_loss: 19.3702, val_MinusLogProbMetric: 19.3702

Epoch 58: val_loss did not improve from 19.22887
196/196 - 80s - loss: 19.2474 - MinusLogProbMetric: 19.2474 - val_loss: 19.3702 - val_MinusLogProbMetric: 19.3702 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 59/1000
2023-09-27 19:58:49.123 
Epoch 59/1000 
	 loss: 19.1448, MinusLogProbMetric: 19.1448, val_loss: 20.7238, val_MinusLogProbMetric: 20.7238

Epoch 59: val_loss did not improve from 19.22887
196/196 - 80s - loss: 19.1448 - MinusLogProbMetric: 19.1448 - val_loss: 20.7238 - val_MinusLogProbMetric: 20.7238 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 60/1000
2023-09-27 20:00:09.395 
Epoch 60/1000 
	 loss: 19.2125, MinusLogProbMetric: 19.2125, val_loss: 19.1927, val_MinusLogProbMetric: 19.1927

Epoch 60: val_loss improved from 19.22887 to 19.19266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 19.2125 - MinusLogProbMetric: 19.2125 - val_loss: 19.1927 - val_MinusLogProbMetric: 19.1927 - lr: 3.3333e-04 - 81s/epoch - 416ms/step
Epoch 61/1000
2023-09-27 20:01:30.493 
Epoch 61/1000 
	 loss: 19.1768, MinusLogProbMetric: 19.1768, val_loss: 18.8643, val_MinusLogProbMetric: 18.8643

Epoch 61: val_loss improved from 19.19266 to 18.86432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 19.1768 - MinusLogProbMetric: 19.1768 - val_loss: 18.8643 - val_MinusLogProbMetric: 18.8643 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 62/1000
2023-09-27 20:02:52.467 
Epoch 62/1000 
	 loss: 19.1787, MinusLogProbMetric: 19.1787, val_loss: 19.5018, val_MinusLogProbMetric: 19.5018

Epoch 62: val_loss did not improve from 18.86432
196/196 - 80s - loss: 19.1787 - MinusLogProbMetric: 19.1787 - val_loss: 19.5018 - val_MinusLogProbMetric: 19.5018 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 63/1000
2023-09-27 20:04:13.524 
Epoch 63/1000 
	 loss: 19.0192, MinusLogProbMetric: 19.0192, val_loss: 19.2806, val_MinusLogProbMetric: 19.2806

Epoch 63: val_loss did not improve from 18.86432
196/196 - 81s - loss: 19.0192 - MinusLogProbMetric: 19.0192 - val_loss: 19.2806 - val_MinusLogProbMetric: 19.2806 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 64/1000
2023-09-27 20:05:33.596 
Epoch 64/1000 
	 loss: 19.1161, MinusLogProbMetric: 19.1161, val_loss: 19.4926, val_MinusLogProbMetric: 19.4926

Epoch 64: val_loss did not improve from 18.86432
196/196 - 80s - loss: 19.1161 - MinusLogProbMetric: 19.1161 - val_loss: 19.4926 - val_MinusLogProbMetric: 19.4926 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 65/1000
2023-09-27 20:06:53.445 
Epoch 65/1000 
	 loss: 18.9861, MinusLogProbMetric: 18.9861, val_loss: 19.3305, val_MinusLogProbMetric: 19.3305

Epoch 65: val_loss did not improve from 18.86432
196/196 - 80s - loss: 18.9861 - MinusLogProbMetric: 18.9861 - val_loss: 19.3305 - val_MinusLogProbMetric: 19.3305 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 66/1000
2023-09-27 20:08:13.299 
Epoch 66/1000 
	 loss: 18.9796, MinusLogProbMetric: 18.9796, val_loss: 19.0338, val_MinusLogProbMetric: 19.0338

Epoch 66: val_loss did not improve from 18.86432
196/196 - 80s - loss: 18.9796 - MinusLogProbMetric: 18.9796 - val_loss: 19.0338 - val_MinusLogProbMetric: 19.0338 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 67/1000
2023-09-27 20:09:33.878 
Epoch 67/1000 
	 loss: 19.0014, MinusLogProbMetric: 19.0014, val_loss: 19.6343, val_MinusLogProbMetric: 19.6343

Epoch 67: val_loss did not improve from 18.86432
196/196 - 81s - loss: 19.0014 - MinusLogProbMetric: 19.0014 - val_loss: 19.6343 - val_MinusLogProbMetric: 19.6343 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 68/1000
2023-09-27 20:10:54.346 
Epoch 68/1000 
	 loss: 18.9757, MinusLogProbMetric: 18.9757, val_loss: 19.0427, val_MinusLogProbMetric: 19.0427

Epoch 68: val_loss did not improve from 18.86432
196/196 - 80s - loss: 18.9757 - MinusLogProbMetric: 18.9757 - val_loss: 19.0427 - val_MinusLogProbMetric: 19.0427 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 69/1000
2023-09-27 20:12:14.072 
Epoch 69/1000 
	 loss: 19.0401, MinusLogProbMetric: 19.0401, val_loss: 19.8155, val_MinusLogProbMetric: 19.8155

Epoch 69: val_loss did not improve from 18.86432
196/196 - 80s - loss: 19.0401 - MinusLogProbMetric: 19.0401 - val_loss: 19.8155 - val_MinusLogProbMetric: 19.8155 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 70/1000
2023-09-27 20:13:33.856 
Epoch 70/1000 
	 loss: 18.9549, MinusLogProbMetric: 18.9549, val_loss: 20.2470, val_MinusLogProbMetric: 20.2470

Epoch 70: val_loss did not improve from 18.86432
196/196 - 80s - loss: 18.9549 - MinusLogProbMetric: 18.9549 - val_loss: 20.2470 - val_MinusLogProbMetric: 20.2470 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 71/1000
2023-09-27 20:14:54.113 
Epoch 71/1000 
	 loss: 18.8823, MinusLogProbMetric: 18.8823, val_loss: 18.4700, val_MinusLogProbMetric: 18.4700

Epoch 71: val_loss improved from 18.86432 to 18.46996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 18.8823 - MinusLogProbMetric: 18.8823 - val_loss: 18.4700 - val_MinusLogProbMetric: 18.4700 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 72/1000
2023-09-27 20:16:15.033 
Epoch 72/1000 
	 loss: 18.9193, MinusLogProbMetric: 18.9193, val_loss: 18.5821, val_MinusLogProbMetric: 18.5821

Epoch 72: val_loss did not improve from 18.46996
196/196 - 80s - loss: 18.9193 - MinusLogProbMetric: 18.9193 - val_loss: 18.5821 - val_MinusLogProbMetric: 18.5821 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 73/1000
2023-09-27 20:17:35.134 
Epoch 73/1000 
	 loss: 18.8674, MinusLogProbMetric: 18.8674, val_loss: 19.3264, val_MinusLogProbMetric: 19.3264

Epoch 73: val_loss did not improve from 18.46996
196/196 - 80s - loss: 18.8674 - MinusLogProbMetric: 18.8674 - val_loss: 19.3264 - val_MinusLogProbMetric: 19.3264 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 74/1000
2023-09-27 20:18:54.617 
Epoch 74/1000 
	 loss: 18.8734, MinusLogProbMetric: 18.8734, val_loss: 19.6343, val_MinusLogProbMetric: 19.6343

Epoch 74: val_loss did not improve from 18.46996
196/196 - 79s - loss: 18.8734 - MinusLogProbMetric: 18.8734 - val_loss: 19.6343 - val_MinusLogProbMetric: 19.6343 - lr: 3.3333e-04 - 79s/epoch - 406ms/step
Epoch 75/1000
2023-09-27 20:20:14.432 
Epoch 75/1000 
	 loss: 18.7325, MinusLogProbMetric: 18.7325, val_loss: 19.3189, val_MinusLogProbMetric: 19.3189

Epoch 75: val_loss did not improve from 18.46996
196/196 - 80s - loss: 18.7325 - MinusLogProbMetric: 18.7325 - val_loss: 19.3189 - val_MinusLogProbMetric: 19.3189 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 76/1000
2023-09-27 20:21:34.610 
Epoch 76/1000 
	 loss: 18.7949, MinusLogProbMetric: 18.7949, val_loss: 18.9477, val_MinusLogProbMetric: 18.9477

Epoch 76: val_loss did not improve from 18.46996
196/196 - 80s - loss: 18.7949 - MinusLogProbMetric: 18.7949 - val_loss: 18.9477 - val_MinusLogProbMetric: 18.9477 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 77/1000
2023-09-27 20:22:54.315 
Epoch 77/1000 
	 loss: 18.7347, MinusLogProbMetric: 18.7347, val_loss: 18.9678, val_MinusLogProbMetric: 18.9678

Epoch 77: val_loss did not improve from 18.46996
196/196 - 80s - loss: 18.7347 - MinusLogProbMetric: 18.7347 - val_loss: 18.9678 - val_MinusLogProbMetric: 18.9678 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 78/1000
2023-09-27 20:24:14.218 
Epoch 78/1000 
	 loss: 18.6516, MinusLogProbMetric: 18.6516, val_loss: 18.9534, val_MinusLogProbMetric: 18.9534

Epoch 78: val_loss did not improve from 18.46996
196/196 - 80s - loss: 18.6516 - MinusLogProbMetric: 18.6516 - val_loss: 18.9534 - val_MinusLogProbMetric: 18.9534 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 79/1000
2023-09-27 20:25:34.754 
Epoch 79/1000 
	 loss: 18.7632, MinusLogProbMetric: 18.7632, val_loss: 18.6640, val_MinusLogProbMetric: 18.6640

Epoch 79: val_loss did not improve from 18.46996
196/196 - 81s - loss: 18.7632 - MinusLogProbMetric: 18.7632 - val_loss: 18.6640 - val_MinusLogProbMetric: 18.6640 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 80/1000
2023-09-27 20:26:53.867 
Epoch 80/1000 
	 loss: 18.7415, MinusLogProbMetric: 18.7415, val_loss: 20.4885, val_MinusLogProbMetric: 20.4885

Epoch 80: val_loss did not improve from 18.46996
196/196 - 79s - loss: 18.7415 - MinusLogProbMetric: 18.7415 - val_loss: 20.4885 - val_MinusLogProbMetric: 20.4885 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 81/1000
2023-09-27 20:28:14.152 
Epoch 81/1000 
	 loss: 18.7310, MinusLogProbMetric: 18.7310, val_loss: 18.5390, val_MinusLogProbMetric: 18.5390

Epoch 81: val_loss did not improve from 18.46996
196/196 - 80s - loss: 18.7310 - MinusLogProbMetric: 18.7310 - val_loss: 18.5390 - val_MinusLogProbMetric: 18.5390 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 82/1000
2023-09-27 20:29:34.279 
Epoch 82/1000 
	 loss: 18.6964, MinusLogProbMetric: 18.6964, val_loss: 18.4068, val_MinusLogProbMetric: 18.4068

Epoch 82: val_loss improved from 18.46996 to 18.40679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 18.6964 - MinusLogProbMetric: 18.6964 - val_loss: 18.4068 - val_MinusLogProbMetric: 18.4068 - lr: 3.3333e-04 - 82s/epoch - 416ms/step
Epoch 83/1000
2023-09-27 20:30:56.447 
Epoch 83/1000 
	 loss: 18.6349, MinusLogProbMetric: 18.6349, val_loss: 18.9067, val_MinusLogProbMetric: 18.9067

Epoch 83: val_loss did not improve from 18.40679
196/196 - 81s - loss: 18.6349 - MinusLogProbMetric: 18.6349 - val_loss: 18.9067 - val_MinusLogProbMetric: 18.9067 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 84/1000
2023-09-27 20:32:15.732 
Epoch 84/1000 
	 loss: 18.6591, MinusLogProbMetric: 18.6591, val_loss: 18.4506, val_MinusLogProbMetric: 18.4506

Epoch 84: val_loss did not improve from 18.40679
196/196 - 79s - loss: 18.6591 - MinusLogProbMetric: 18.6591 - val_loss: 18.4506 - val_MinusLogProbMetric: 18.4506 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 85/1000
2023-09-27 20:33:35.890 
Epoch 85/1000 
	 loss: 18.7106, MinusLogProbMetric: 18.7106, val_loss: 19.1146, val_MinusLogProbMetric: 19.1146

Epoch 85: val_loss did not improve from 18.40679
196/196 - 80s - loss: 18.7106 - MinusLogProbMetric: 18.7106 - val_loss: 19.1146 - val_MinusLogProbMetric: 19.1146 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 86/1000
2023-09-27 20:34:56.419 
Epoch 86/1000 
	 loss: 18.5651, MinusLogProbMetric: 18.5651, val_loss: 18.9125, val_MinusLogProbMetric: 18.9125

Epoch 86: val_loss did not improve from 18.40679
196/196 - 81s - loss: 18.5651 - MinusLogProbMetric: 18.5651 - val_loss: 18.9125 - val_MinusLogProbMetric: 18.9125 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 87/1000
2023-09-27 20:36:16.443 
Epoch 87/1000 
	 loss: 18.6999, MinusLogProbMetric: 18.6999, val_loss: 18.4700, val_MinusLogProbMetric: 18.4700

Epoch 87: val_loss did not improve from 18.40679
196/196 - 80s - loss: 18.6999 - MinusLogProbMetric: 18.6999 - val_loss: 18.4700 - val_MinusLogProbMetric: 18.4700 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 88/1000
2023-09-27 20:37:36.399 
Epoch 88/1000 
	 loss: 18.4892, MinusLogProbMetric: 18.4892, val_loss: 19.4476, val_MinusLogProbMetric: 19.4476

Epoch 88: val_loss did not improve from 18.40679
196/196 - 80s - loss: 18.4892 - MinusLogProbMetric: 18.4892 - val_loss: 19.4476 - val_MinusLogProbMetric: 19.4476 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 89/1000
2023-09-27 20:38:56.027 
Epoch 89/1000 
	 loss: 18.5578, MinusLogProbMetric: 18.5578, val_loss: 18.7260, val_MinusLogProbMetric: 18.7260

Epoch 89: val_loss did not improve from 18.40679
196/196 - 80s - loss: 18.5578 - MinusLogProbMetric: 18.5578 - val_loss: 18.7260 - val_MinusLogProbMetric: 18.7260 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 90/1000
2023-09-27 20:40:15.653 
Epoch 90/1000 
	 loss: 18.5537, MinusLogProbMetric: 18.5537, val_loss: 19.2758, val_MinusLogProbMetric: 19.2758

Epoch 90: val_loss did not improve from 18.40679
196/196 - 80s - loss: 18.5537 - MinusLogProbMetric: 18.5537 - val_loss: 19.2758 - val_MinusLogProbMetric: 19.2758 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 91/1000
2023-09-27 20:41:35.033 
Epoch 91/1000 
	 loss: 18.4064, MinusLogProbMetric: 18.4064, val_loss: 18.5659, val_MinusLogProbMetric: 18.5659

Epoch 91: val_loss did not improve from 18.40679
196/196 - 79s - loss: 18.4064 - MinusLogProbMetric: 18.4064 - val_loss: 18.5659 - val_MinusLogProbMetric: 18.5659 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 92/1000
2023-09-27 20:42:53.527 
Epoch 92/1000 
	 loss: 18.6090, MinusLogProbMetric: 18.6090, val_loss: 18.7656, val_MinusLogProbMetric: 18.7656

Epoch 92: val_loss did not improve from 18.40679
196/196 - 78s - loss: 18.6090 - MinusLogProbMetric: 18.6090 - val_loss: 18.7656 - val_MinusLogProbMetric: 18.7656 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 93/1000
2023-09-27 20:44:08.133 
Epoch 93/1000 
	 loss: 18.4306, MinusLogProbMetric: 18.4306, val_loss: 19.0295, val_MinusLogProbMetric: 19.0295

Epoch 93: val_loss did not improve from 18.40679
196/196 - 75s - loss: 18.4306 - MinusLogProbMetric: 18.4306 - val_loss: 19.0295 - val_MinusLogProbMetric: 19.0295 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 94/1000
2023-09-27 20:45:27.871 
Epoch 94/1000 
	 loss: 18.4346, MinusLogProbMetric: 18.4346, val_loss: 19.0290, val_MinusLogProbMetric: 19.0290

Epoch 94: val_loss did not improve from 18.40679
196/196 - 80s - loss: 18.4346 - MinusLogProbMetric: 18.4346 - val_loss: 19.0290 - val_MinusLogProbMetric: 19.0290 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 95/1000
2023-09-27 20:46:47.261 
Epoch 95/1000 
	 loss: 18.4669, MinusLogProbMetric: 18.4669, val_loss: 19.8300, val_MinusLogProbMetric: 19.8300

Epoch 95: val_loss did not improve from 18.40679
196/196 - 79s - loss: 18.4669 - MinusLogProbMetric: 18.4669 - val_loss: 19.8300 - val_MinusLogProbMetric: 19.8300 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 96/1000
2023-09-27 20:48:07.820 
Epoch 96/1000 
	 loss: 18.5144, MinusLogProbMetric: 18.5144, val_loss: 18.8657, val_MinusLogProbMetric: 18.8657

Epoch 96: val_loss did not improve from 18.40679
196/196 - 81s - loss: 18.5144 - MinusLogProbMetric: 18.5144 - val_loss: 18.8657 - val_MinusLogProbMetric: 18.8657 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 97/1000
2023-09-27 20:49:27.778 
Epoch 97/1000 
	 loss: 18.4724, MinusLogProbMetric: 18.4724, val_loss: 19.0982, val_MinusLogProbMetric: 19.0982

Epoch 97: val_loss did not improve from 18.40679
196/196 - 80s - loss: 18.4724 - MinusLogProbMetric: 18.4724 - val_loss: 19.0982 - val_MinusLogProbMetric: 19.0982 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 98/1000
2023-09-27 20:50:48.295 
Epoch 98/1000 
	 loss: 18.4367, MinusLogProbMetric: 18.4367, val_loss: 18.5519, val_MinusLogProbMetric: 18.5519

Epoch 98: val_loss did not improve from 18.40679
196/196 - 81s - loss: 18.4367 - MinusLogProbMetric: 18.4367 - val_loss: 18.5519 - val_MinusLogProbMetric: 18.5519 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 99/1000
2023-09-27 20:52:07.865 
Epoch 99/1000 
	 loss: 18.4085, MinusLogProbMetric: 18.4085, val_loss: 18.3129, val_MinusLogProbMetric: 18.3129

Epoch 99: val_loss improved from 18.40679 to 18.31289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 18.4085 - MinusLogProbMetric: 18.4085 - val_loss: 18.3129 - val_MinusLogProbMetric: 18.3129 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 100/1000
2023-09-27 20:53:28.869 
Epoch 100/1000 
	 loss: 18.3918, MinusLogProbMetric: 18.3918, val_loss: 18.7942, val_MinusLogProbMetric: 18.7942

Epoch 100: val_loss did not improve from 18.31289
196/196 - 80s - loss: 18.3918 - MinusLogProbMetric: 18.3918 - val_loss: 18.7942 - val_MinusLogProbMetric: 18.7942 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 101/1000
2023-09-27 20:54:48.419 
Epoch 101/1000 
	 loss: 18.3738, MinusLogProbMetric: 18.3738, val_loss: 19.3234, val_MinusLogProbMetric: 19.3234

Epoch 101: val_loss did not improve from 18.31289
196/196 - 80s - loss: 18.3738 - MinusLogProbMetric: 18.3738 - val_loss: 19.3234 - val_MinusLogProbMetric: 19.3234 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 102/1000
2023-09-27 20:56:08.142 
Epoch 102/1000 
	 loss: 18.3441, MinusLogProbMetric: 18.3441, val_loss: 18.3429, val_MinusLogProbMetric: 18.3429

Epoch 102: val_loss did not improve from 18.31289
196/196 - 80s - loss: 18.3441 - MinusLogProbMetric: 18.3441 - val_loss: 18.3429 - val_MinusLogProbMetric: 18.3429 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 103/1000
2023-09-27 20:57:28.124 
Epoch 103/1000 
	 loss: 18.4566, MinusLogProbMetric: 18.4566, val_loss: 18.5842, val_MinusLogProbMetric: 18.5842

Epoch 103: val_loss did not improve from 18.31289
196/196 - 80s - loss: 18.4566 - MinusLogProbMetric: 18.4566 - val_loss: 18.5842 - val_MinusLogProbMetric: 18.5842 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 104/1000
2023-09-27 20:58:47.570 
Epoch 104/1000 
	 loss: 18.2720, MinusLogProbMetric: 18.2720, val_loss: 19.1306, val_MinusLogProbMetric: 19.1306

Epoch 104: val_loss did not improve from 18.31289
196/196 - 79s - loss: 18.2720 - MinusLogProbMetric: 18.2720 - val_loss: 19.1306 - val_MinusLogProbMetric: 19.1306 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 105/1000
2023-09-27 21:00:07.391 
Epoch 105/1000 
	 loss: 18.3834, MinusLogProbMetric: 18.3834, val_loss: 19.1145, val_MinusLogProbMetric: 19.1145

Epoch 105: val_loss did not improve from 18.31289
196/196 - 80s - loss: 18.3834 - MinusLogProbMetric: 18.3834 - val_loss: 19.1145 - val_MinusLogProbMetric: 19.1145 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 106/1000
2023-09-27 21:01:27.376 
Epoch 106/1000 
	 loss: 18.2445, MinusLogProbMetric: 18.2445, val_loss: 18.8149, val_MinusLogProbMetric: 18.8149

Epoch 106: val_loss did not improve from 18.31289
196/196 - 80s - loss: 18.2445 - MinusLogProbMetric: 18.2445 - val_loss: 18.8149 - val_MinusLogProbMetric: 18.8149 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 107/1000
2023-09-27 21:02:47.385 
Epoch 107/1000 
	 loss: 18.3061, MinusLogProbMetric: 18.3061, val_loss: 18.8525, val_MinusLogProbMetric: 18.8525

Epoch 107: val_loss did not improve from 18.31289
196/196 - 80s - loss: 18.3061 - MinusLogProbMetric: 18.3061 - val_loss: 18.8525 - val_MinusLogProbMetric: 18.8525 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 108/1000
2023-09-27 21:04:08.053 
Epoch 108/1000 
	 loss: 18.2895, MinusLogProbMetric: 18.2895, val_loss: 18.4332, val_MinusLogProbMetric: 18.4332

Epoch 108: val_loss did not improve from 18.31289
196/196 - 81s - loss: 18.2895 - MinusLogProbMetric: 18.2895 - val_loss: 18.4332 - val_MinusLogProbMetric: 18.4332 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 109/1000
2023-09-27 21:05:28.675 
Epoch 109/1000 
	 loss: 18.3398, MinusLogProbMetric: 18.3398, val_loss: 18.3028, val_MinusLogProbMetric: 18.3028

Epoch 109: val_loss improved from 18.31289 to 18.30275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 18.3398 - MinusLogProbMetric: 18.3398 - val_loss: 18.3028 - val_MinusLogProbMetric: 18.3028 - lr: 3.3333e-04 - 82s/epoch - 418ms/step
Epoch 110/1000
2023-09-27 21:06:50.035 
Epoch 110/1000 
	 loss: 18.2872, MinusLogProbMetric: 18.2872, val_loss: 18.1834, val_MinusLogProbMetric: 18.1834

Epoch 110: val_loss improved from 18.30275 to 18.18337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 18.2872 - MinusLogProbMetric: 18.2872 - val_loss: 18.1834 - val_MinusLogProbMetric: 18.1834 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 111/1000
2023-09-27 21:08:11.434 
Epoch 111/1000 
	 loss: 18.2906, MinusLogProbMetric: 18.2906, val_loss: 18.0097, val_MinusLogProbMetric: 18.0097

Epoch 111: val_loss improved from 18.18337 to 18.00973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 18.2906 - MinusLogProbMetric: 18.2906 - val_loss: 18.0097 - val_MinusLogProbMetric: 18.0097 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 112/1000
2023-09-27 21:09:32.571 
Epoch 112/1000 
	 loss: 18.2171, MinusLogProbMetric: 18.2171, val_loss: 18.3172, val_MinusLogProbMetric: 18.3172

Epoch 112: val_loss did not improve from 18.00973
196/196 - 80s - loss: 18.2171 - MinusLogProbMetric: 18.2171 - val_loss: 18.3172 - val_MinusLogProbMetric: 18.3172 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 113/1000
2023-09-27 21:10:52.030 
Epoch 113/1000 
	 loss: 18.3290, MinusLogProbMetric: 18.3290, val_loss: 18.0911, val_MinusLogProbMetric: 18.0911

Epoch 113: val_loss did not improve from 18.00973
196/196 - 79s - loss: 18.3290 - MinusLogProbMetric: 18.3290 - val_loss: 18.0911 - val_MinusLogProbMetric: 18.0911 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 114/1000
2023-09-27 21:12:11.963 
Epoch 114/1000 
	 loss: 18.1766, MinusLogProbMetric: 18.1766, val_loss: 18.2928, val_MinusLogProbMetric: 18.2928

Epoch 114: val_loss did not improve from 18.00973
196/196 - 80s - loss: 18.1766 - MinusLogProbMetric: 18.1766 - val_loss: 18.2928 - val_MinusLogProbMetric: 18.2928 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 115/1000
2023-09-27 21:13:31.777 
Epoch 115/1000 
	 loss: 18.3786, MinusLogProbMetric: 18.3786, val_loss: 19.3101, val_MinusLogProbMetric: 19.3101

Epoch 115: val_loss did not improve from 18.00973
196/196 - 80s - loss: 18.3786 - MinusLogProbMetric: 18.3786 - val_loss: 19.3101 - val_MinusLogProbMetric: 19.3101 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 116/1000
2023-09-27 21:14:51.743 
Epoch 116/1000 
	 loss: 18.1926, MinusLogProbMetric: 18.1926, val_loss: 18.2427, val_MinusLogProbMetric: 18.2427

Epoch 116: val_loss did not improve from 18.00973
196/196 - 80s - loss: 18.1926 - MinusLogProbMetric: 18.1926 - val_loss: 18.2427 - val_MinusLogProbMetric: 18.2427 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 117/1000
2023-09-27 21:16:11.745 
Epoch 117/1000 
	 loss: 18.2357, MinusLogProbMetric: 18.2357, val_loss: 18.4405, val_MinusLogProbMetric: 18.4405

Epoch 117: val_loss did not improve from 18.00973
196/196 - 80s - loss: 18.2357 - MinusLogProbMetric: 18.2357 - val_loss: 18.4405 - val_MinusLogProbMetric: 18.4405 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 118/1000
2023-09-27 21:17:31.952 
Epoch 118/1000 
	 loss: 18.2034, MinusLogProbMetric: 18.2034, val_loss: 18.3895, val_MinusLogProbMetric: 18.3895

Epoch 118: val_loss did not improve from 18.00973
196/196 - 80s - loss: 18.2034 - MinusLogProbMetric: 18.2034 - val_loss: 18.3895 - val_MinusLogProbMetric: 18.3895 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 119/1000
2023-09-27 21:18:51.901 
Epoch 119/1000 
	 loss: 18.1651, MinusLogProbMetric: 18.1651, val_loss: 17.8697, val_MinusLogProbMetric: 17.8697

Epoch 119: val_loss improved from 18.00973 to 17.86975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 18.1651 - MinusLogProbMetric: 18.1651 - val_loss: 17.8697 - val_MinusLogProbMetric: 17.8697 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 120/1000
2023-09-27 21:20:12.901 
Epoch 120/1000 
	 loss: 18.1594, MinusLogProbMetric: 18.1594, val_loss: 18.8017, val_MinusLogProbMetric: 18.8017

Epoch 120: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1594 - MinusLogProbMetric: 18.1594 - val_loss: 18.8017 - val_MinusLogProbMetric: 18.8017 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 121/1000
2023-09-27 21:21:32.465 
Epoch 121/1000 
	 loss: 18.3678, MinusLogProbMetric: 18.3678, val_loss: 18.6221, val_MinusLogProbMetric: 18.6221

Epoch 121: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.3678 - MinusLogProbMetric: 18.3678 - val_loss: 18.6221 - val_MinusLogProbMetric: 18.6221 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 122/1000
2023-09-27 21:22:52.546 
Epoch 122/1000 
	 loss: 18.2084, MinusLogProbMetric: 18.2084, val_loss: 18.6105, val_MinusLogProbMetric: 18.6105

Epoch 122: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.2084 - MinusLogProbMetric: 18.2084 - val_loss: 18.6105 - val_MinusLogProbMetric: 18.6105 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 123/1000
2023-09-27 21:24:12.801 
Epoch 123/1000 
	 loss: 18.1618, MinusLogProbMetric: 18.1618, val_loss: 18.5653, val_MinusLogProbMetric: 18.5653

Epoch 123: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1618 - MinusLogProbMetric: 18.1618 - val_loss: 18.5653 - val_MinusLogProbMetric: 18.5653 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 124/1000
2023-09-27 21:25:33.053 
Epoch 124/1000 
	 loss: 18.1624, MinusLogProbMetric: 18.1624, val_loss: 18.3819, val_MinusLogProbMetric: 18.3819

Epoch 124: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1624 - MinusLogProbMetric: 18.1624 - val_loss: 18.3819 - val_MinusLogProbMetric: 18.3819 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 125/1000
2023-09-27 21:26:53.414 
Epoch 125/1000 
	 loss: 18.0627, MinusLogProbMetric: 18.0627, val_loss: 18.0899, val_MinusLogProbMetric: 18.0899

Epoch 125: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0627 - MinusLogProbMetric: 18.0627 - val_loss: 18.0899 - val_MinusLogProbMetric: 18.0899 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 126/1000
2023-09-27 21:28:13.946 
Epoch 126/1000 
	 loss: 18.0985, MinusLogProbMetric: 18.0985, val_loss: 18.0491, val_MinusLogProbMetric: 18.0491

Epoch 126: val_loss did not improve from 17.86975
196/196 - 81s - loss: 18.0985 - MinusLogProbMetric: 18.0985 - val_loss: 18.0491 - val_MinusLogProbMetric: 18.0491 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 127/1000
2023-09-27 21:29:33.828 
Epoch 127/1000 
	 loss: 18.1713, MinusLogProbMetric: 18.1713, val_loss: 18.6464, val_MinusLogProbMetric: 18.6464

Epoch 127: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1713 - MinusLogProbMetric: 18.1713 - val_loss: 18.6464 - val_MinusLogProbMetric: 18.6464 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 128/1000
2023-09-27 21:30:53.670 
Epoch 128/1000 
	 loss: 18.1330, MinusLogProbMetric: 18.1330, val_loss: 18.6081, val_MinusLogProbMetric: 18.6081

Epoch 128: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1330 - MinusLogProbMetric: 18.1330 - val_loss: 18.6081 - val_MinusLogProbMetric: 18.6081 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 129/1000
2023-09-27 21:32:13.450 
Epoch 129/1000 
	 loss: 18.0944, MinusLogProbMetric: 18.0944, val_loss: 18.6790, val_MinusLogProbMetric: 18.6790

Epoch 129: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0944 - MinusLogProbMetric: 18.0944 - val_loss: 18.6790 - val_MinusLogProbMetric: 18.6790 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 130/1000
2023-09-27 21:33:33.802 
Epoch 130/1000 
	 loss: 18.0679, MinusLogProbMetric: 18.0679, val_loss: 18.1746, val_MinusLogProbMetric: 18.1746

Epoch 130: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0679 - MinusLogProbMetric: 18.0679 - val_loss: 18.1746 - val_MinusLogProbMetric: 18.1746 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 131/1000
2023-09-27 21:34:53.958 
Epoch 131/1000 
	 loss: 18.1138, MinusLogProbMetric: 18.1138, val_loss: 18.5206, val_MinusLogProbMetric: 18.5206

Epoch 131: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1138 - MinusLogProbMetric: 18.1138 - val_loss: 18.5206 - val_MinusLogProbMetric: 18.5206 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 132/1000
2023-09-27 21:36:14.014 
Epoch 132/1000 
	 loss: 18.0076, MinusLogProbMetric: 18.0076, val_loss: 17.9868, val_MinusLogProbMetric: 17.9868

Epoch 132: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0076 - MinusLogProbMetric: 18.0076 - val_loss: 17.9868 - val_MinusLogProbMetric: 17.9868 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 133/1000
2023-09-27 21:37:33.921 
Epoch 133/1000 
	 loss: 18.1341, MinusLogProbMetric: 18.1341, val_loss: 18.4605, val_MinusLogProbMetric: 18.4605

Epoch 133: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1341 - MinusLogProbMetric: 18.1341 - val_loss: 18.4605 - val_MinusLogProbMetric: 18.4605 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 134/1000
2023-09-27 21:38:53.801 
Epoch 134/1000 
	 loss: 18.0349, MinusLogProbMetric: 18.0349, val_loss: 18.4544, val_MinusLogProbMetric: 18.4544

Epoch 134: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0349 - MinusLogProbMetric: 18.0349 - val_loss: 18.4544 - val_MinusLogProbMetric: 18.4544 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 135/1000
2023-09-27 21:40:14.444 
Epoch 135/1000 
	 loss: 18.1337, MinusLogProbMetric: 18.1337, val_loss: 18.4438, val_MinusLogProbMetric: 18.4438

Epoch 135: val_loss did not improve from 17.86975
196/196 - 81s - loss: 18.1337 - MinusLogProbMetric: 18.1337 - val_loss: 18.4438 - val_MinusLogProbMetric: 18.4438 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 136/1000
2023-09-27 21:41:35.367 
Epoch 136/1000 
	 loss: 18.1266, MinusLogProbMetric: 18.1266, val_loss: 18.1272, val_MinusLogProbMetric: 18.1272

Epoch 136: val_loss did not improve from 17.86975
196/196 - 81s - loss: 18.1266 - MinusLogProbMetric: 18.1266 - val_loss: 18.1272 - val_MinusLogProbMetric: 18.1272 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 137/1000
2023-09-27 21:42:55.627 
Epoch 137/1000 
	 loss: 18.1520, MinusLogProbMetric: 18.1520, val_loss: 19.1091, val_MinusLogProbMetric: 19.1091

Epoch 137: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1520 - MinusLogProbMetric: 18.1520 - val_loss: 19.1091 - val_MinusLogProbMetric: 19.1091 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 138/1000
2023-09-27 21:44:15.641 
Epoch 138/1000 
	 loss: 18.0029, MinusLogProbMetric: 18.0029, val_loss: 18.6854, val_MinusLogProbMetric: 18.6854

Epoch 138: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0029 - MinusLogProbMetric: 18.0029 - val_loss: 18.6854 - val_MinusLogProbMetric: 18.6854 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 139/1000
2023-09-27 21:45:35.415 
Epoch 139/1000 
	 loss: 18.1494, MinusLogProbMetric: 18.1494, val_loss: 18.4948, val_MinusLogProbMetric: 18.4948

Epoch 139: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1494 - MinusLogProbMetric: 18.1494 - val_loss: 18.4948 - val_MinusLogProbMetric: 18.4948 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 140/1000
2023-09-27 21:46:56.033 
Epoch 140/1000 
	 loss: 18.0931, MinusLogProbMetric: 18.0931, val_loss: 18.5244, val_MinusLogProbMetric: 18.5244

Epoch 140: val_loss did not improve from 17.86975
196/196 - 81s - loss: 18.0931 - MinusLogProbMetric: 18.0931 - val_loss: 18.5244 - val_MinusLogProbMetric: 18.5244 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 141/1000
2023-09-27 21:48:16.435 
Epoch 141/1000 
	 loss: 18.0516, MinusLogProbMetric: 18.0516, val_loss: 17.9828, val_MinusLogProbMetric: 17.9828

Epoch 141: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0516 - MinusLogProbMetric: 18.0516 - val_loss: 17.9828 - val_MinusLogProbMetric: 17.9828 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 142/1000
2023-09-27 21:49:36.869 
Epoch 142/1000 
	 loss: 18.0358, MinusLogProbMetric: 18.0358, val_loss: 18.1452, val_MinusLogProbMetric: 18.1452

Epoch 142: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0358 - MinusLogProbMetric: 18.0358 - val_loss: 18.1452 - val_MinusLogProbMetric: 18.1452 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 143/1000
2023-09-27 21:50:57.202 
Epoch 143/1000 
	 loss: 18.0779, MinusLogProbMetric: 18.0779, val_loss: 17.9830, val_MinusLogProbMetric: 17.9830

Epoch 143: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0779 - MinusLogProbMetric: 18.0779 - val_loss: 17.9830 - val_MinusLogProbMetric: 17.9830 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 144/1000
2023-09-27 21:52:17.593 
Epoch 144/1000 
	 loss: 18.1395, MinusLogProbMetric: 18.1395, val_loss: 18.2796, val_MinusLogProbMetric: 18.2796

Epoch 144: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.1395 - MinusLogProbMetric: 18.1395 - val_loss: 18.2796 - val_MinusLogProbMetric: 18.2796 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 145/1000
2023-09-27 21:53:37.661 
Epoch 145/1000 
	 loss: 18.0055, MinusLogProbMetric: 18.0055, val_loss: 18.5149, val_MinusLogProbMetric: 18.5149

Epoch 145: val_loss did not improve from 17.86975
196/196 - 80s - loss: 18.0055 - MinusLogProbMetric: 18.0055 - val_loss: 18.5149 - val_MinusLogProbMetric: 18.5149 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 146/1000
2023-09-27 21:54:57.598 
Epoch 146/1000 
	 loss: 18.0034, MinusLogProbMetric: 18.0034, val_loss: 17.7395, val_MinusLogProbMetric: 17.7395

Epoch 146: val_loss improved from 17.86975 to 17.73946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 18.0034 - MinusLogProbMetric: 18.0034 - val_loss: 17.7395 - val_MinusLogProbMetric: 17.7395 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 147/1000
2023-09-27 21:56:18.930 
Epoch 147/1000 
	 loss: 17.9894, MinusLogProbMetric: 17.9894, val_loss: 17.8956, val_MinusLogProbMetric: 17.8956

Epoch 147: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9894 - MinusLogProbMetric: 17.9894 - val_loss: 17.8956 - val_MinusLogProbMetric: 17.8956 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 148/1000
2023-09-27 21:57:39.645 
Epoch 148/1000 
	 loss: 17.9809, MinusLogProbMetric: 17.9809, val_loss: 18.5031, val_MinusLogProbMetric: 18.5031

Epoch 148: val_loss did not improve from 17.73946
196/196 - 81s - loss: 17.9809 - MinusLogProbMetric: 17.9809 - val_loss: 18.5031 - val_MinusLogProbMetric: 18.5031 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 149/1000
2023-09-27 21:59:00.165 
Epoch 149/1000 
	 loss: 18.0239, MinusLogProbMetric: 18.0239, val_loss: 18.0625, val_MinusLogProbMetric: 18.0625

Epoch 149: val_loss did not improve from 17.73946
196/196 - 81s - loss: 18.0239 - MinusLogProbMetric: 18.0239 - val_loss: 18.0625 - val_MinusLogProbMetric: 18.0625 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 150/1000
2023-09-27 22:00:20.260 
Epoch 150/1000 
	 loss: 18.0056, MinusLogProbMetric: 18.0056, val_loss: 18.3943, val_MinusLogProbMetric: 18.3943

Epoch 150: val_loss did not improve from 17.73946
196/196 - 80s - loss: 18.0056 - MinusLogProbMetric: 18.0056 - val_loss: 18.3943 - val_MinusLogProbMetric: 18.3943 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 151/1000
2023-09-27 22:01:40.537 
Epoch 151/1000 
	 loss: 17.9154, MinusLogProbMetric: 17.9154, val_loss: 17.9700, val_MinusLogProbMetric: 17.9700

Epoch 151: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9154 - MinusLogProbMetric: 17.9154 - val_loss: 17.9700 - val_MinusLogProbMetric: 17.9700 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 152/1000
2023-09-27 22:03:00.712 
Epoch 152/1000 
	 loss: 17.9412, MinusLogProbMetric: 17.9412, val_loss: 18.3963, val_MinusLogProbMetric: 18.3963

Epoch 152: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9412 - MinusLogProbMetric: 17.9412 - val_loss: 18.3963 - val_MinusLogProbMetric: 18.3963 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 153/1000
2023-09-27 22:04:20.536 
Epoch 153/1000 
	 loss: 17.9676, MinusLogProbMetric: 17.9676, val_loss: 17.9441, val_MinusLogProbMetric: 17.9441

Epoch 153: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9676 - MinusLogProbMetric: 17.9676 - val_loss: 17.9441 - val_MinusLogProbMetric: 17.9441 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 154/1000
2023-09-27 22:05:40.657 
Epoch 154/1000 
	 loss: 17.9427, MinusLogProbMetric: 17.9427, val_loss: 17.8227, val_MinusLogProbMetric: 17.8227

Epoch 154: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9427 - MinusLogProbMetric: 17.9427 - val_loss: 17.8227 - val_MinusLogProbMetric: 17.8227 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 155/1000
2023-09-27 22:07:00.201 
Epoch 155/1000 
	 loss: 17.9136, MinusLogProbMetric: 17.9136, val_loss: 18.5672, val_MinusLogProbMetric: 18.5672

Epoch 155: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9136 - MinusLogProbMetric: 17.9136 - val_loss: 18.5672 - val_MinusLogProbMetric: 18.5672 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 156/1000
2023-09-27 22:08:20.614 
Epoch 156/1000 
	 loss: 17.9212, MinusLogProbMetric: 17.9212, val_loss: 18.2002, val_MinusLogProbMetric: 18.2002

Epoch 156: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9212 - MinusLogProbMetric: 17.9212 - val_loss: 18.2002 - val_MinusLogProbMetric: 18.2002 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 157/1000
2023-09-27 22:09:35.813 
Epoch 157/1000 
	 loss: 17.9941, MinusLogProbMetric: 17.9941, val_loss: 17.9020, val_MinusLogProbMetric: 17.9020

Epoch 157: val_loss did not improve from 17.73946
196/196 - 75s - loss: 17.9941 - MinusLogProbMetric: 17.9941 - val_loss: 17.9020 - val_MinusLogProbMetric: 17.9020 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 158/1000
2023-09-27 22:10:49.563 
Epoch 158/1000 
	 loss: 17.8876, MinusLogProbMetric: 17.8876, val_loss: 17.9734, val_MinusLogProbMetric: 17.9734

Epoch 158: val_loss did not improve from 17.73946
196/196 - 74s - loss: 17.8876 - MinusLogProbMetric: 17.8876 - val_loss: 17.9734 - val_MinusLogProbMetric: 17.9734 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 159/1000
2023-09-27 22:12:08.614 
Epoch 159/1000 
	 loss: 17.9285, MinusLogProbMetric: 17.9285, val_loss: 17.8455, val_MinusLogProbMetric: 17.8455

Epoch 159: val_loss did not improve from 17.73946
196/196 - 79s - loss: 17.9285 - MinusLogProbMetric: 17.9285 - val_loss: 17.8455 - val_MinusLogProbMetric: 17.8455 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 160/1000
2023-09-27 22:13:28.263 
Epoch 160/1000 
	 loss: 17.9063, MinusLogProbMetric: 17.9063, val_loss: 18.1088, val_MinusLogProbMetric: 18.1088

Epoch 160: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9063 - MinusLogProbMetric: 17.9063 - val_loss: 18.1088 - val_MinusLogProbMetric: 18.1088 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 161/1000
2023-09-27 22:14:48.772 
Epoch 161/1000 
	 loss: 17.9214, MinusLogProbMetric: 17.9214, val_loss: 18.5917, val_MinusLogProbMetric: 18.5917

Epoch 161: val_loss did not improve from 17.73946
196/196 - 81s - loss: 17.9214 - MinusLogProbMetric: 17.9214 - val_loss: 18.5917 - val_MinusLogProbMetric: 18.5917 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 162/1000
2023-09-27 22:16:09.582 
Epoch 162/1000 
	 loss: 18.1379, MinusLogProbMetric: 18.1379, val_loss: 18.2462, val_MinusLogProbMetric: 18.2462

Epoch 162: val_loss did not improve from 17.73946
196/196 - 81s - loss: 18.1379 - MinusLogProbMetric: 18.1379 - val_loss: 18.2462 - val_MinusLogProbMetric: 18.2462 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 163/1000
2023-09-27 22:17:30.227 
Epoch 163/1000 
	 loss: 17.9385, MinusLogProbMetric: 17.9385, val_loss: 18.3324, val_MinusLogProbMetric: 18.3324

Epoch 163: val_loss did not improve from 17.73946
196/196 - 81s - loss: 17.9385 - MinusLogProbMetric: 17.9385 - val_loss: 18.3324 - val_MinusLogProbMetric: 18.3324 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 164/1000
2023-09-27 22:18:49.960 
Epoch 164/1000 
	 loss: 17.8816, MinusLogProbMetric: 17.8816, val_loss: 18.3696, val_MinusLogProbMetric: 18.3696

Epoch 164: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.8816 - MinusLogProbMetric: 17.8816 - val_loss: 18.3696 - val_MinusLogProbMetric: 18.3696 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 165/1000
2023-09-27 22:20:10.317 
Epoch 165/1000 
	 loss: 17.8753, MinusLogProbMetric: 17.8753, val_loss: 17.8360, val_MinusLogProbMetric: 17.8360

Epoch 165: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.8753 - MinusLogProbMetric: 17.8753 - val_loss: 17.8360 - val_MinusLogProbMetric: 17.8360 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 166/1000
2023-09-27 22:21:30.693 
Epoch 166/1000 
	 loss: 17.8333, MinusLogProbMetric: 17.8333, val_loss: 17.9071, val_MinusLogProbMetric: 17.9071

Epoch 166: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.8333 - MinusLogProbMetric: 17.8333 - val_loss: 17.9071 - val_MinusLogProbMetric: 17.9071 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 167/1000
2023-09-27 22:22:50.482 
Epoch 167/1000 
	 loss: 17.9145, MinusLogProbMetric: 17.9145, val_loss: 18.1585, val_MinusLogProbMetric: 18.1585

Epoch 167: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9145 - MinusLogProbMetric: 17.9145 - val_loss: 18.1585 - val_MinusLogProbMetric: 18.1585 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 168/1000
2023-09-27 22:24:10.889 
Epoch 168/1000 
	 loss: 17.9151, MinusLogProbMetric: 17.9151, val_loss: 18.0484, val_MinusLogProbMetric: 18.0484

Epoch 168: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9151 - MinusLogProbMetric: 17.9151 - val_loss: 18.0484 - val_MinusLogProbMetric: 18.0484 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 169/1000
2023-09-27 22:25:30.529 
Epoch 169/1000 
	 loss: 17.8547, MinusLogProbMetric: 17.8547, val_loss: 17.8616, val_MinusLogProbMetric: 17.8616

Epoch 169: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.8547 - MinusLogProbMetric: 17.8547 - val_loss: 17.8616 - val_MinusLogProbMetric: 17.8616 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 170/1000
2023-09-27 22:26:50.407 
Epoch 170/1000 
	 loss: 17.8344, MinusLogProbMetric: 17.8344, val_loss: 18.8827, val_MinusLogProbMetric: 18.8827

Epoch 170: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.8344 - MinusLogProbMetric: 17.8344 - val_loss: 18.8827 - val_MinusLogProbMetric: 18.8827 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 171/1000
2023-09-27 22:28:10.878 
Epoch 171/1000 
	 loss: 17.9111, MinusLogProbMetric: 17.9111, val_loss: 18.1241, val_MinusLogProbMetric: 18.1241

Epoch 171: val_loss did not improve from 17.73946
196/196 - 80s - loss: 17.9111 - MinusLogProbMetric: 17.9111 - val_loss: 18.1241 - val_MinusLogProbMetric: 18.1241 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 172/1000
2023-09-27 22:29:29.949 
Epoch 172/1000 
	 loss: 17.7527, MinusLogProbMetric: 17.7527, val_loss: 17.7031, val_MinusLogProbMetric: 17.7031

Epoch 172: val_loss improved from 17.73946 to 17.70309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 80s - loss: 17.7527 - MinusLogProbMetric: 17.7527 - val_loss: 17.7031 - val_MinusLogProbMetric: 17.7031 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 173/1000
2023-09-27 22:30:51.053 
Epoch 173/1000 
	 loss: 17.7351, MinusLogProbMetric: 17.7351, val_loss: 18.2899, val_MinusLogProbMetric: 18.2899

Epoch 173: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.7351 - MinusLogProbMetric: 17.7351 - val_loss: 18.2899 - val_MinusLogProbMetric: 18.2899 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 174/1000
2023-09-27 22:32:11.042 
Epoch 174/1000 
	 loss: 17.7842, MinusLogProbMetric: 17.7842, val_loss: 18.6016, val_MinusLogProbMetric: 18.6016

Epoch 174: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.7842 - MinusLogProbMetric: 17.7842 - val_loss: 18.6016 - val_MinusLogProbMetric: 18.6016 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 175/1000
2023-09-27 22:33:31.753 
Epoch 175/1000 
	 loss: 17.8168, MinusLogProbMetric: 17.8168, val_loss: 17.9210, val_MinusLogProbMetric: 17.9210

Epoch 175: val_loss did not improve from 17.70309
196/196 - 81s - loss: 17.8168 - MinusLogProbMetric: 17.8168 - val_loss: 17.9210 - val_MinusLogProbMetric: 17.9210 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 176/1000
2023-09-27 22:34:51.846 
Epoch 176/1000 
	 loss: 17.7418, MinusLogProbMetric: 17.7418, val_loss: 17.8452, val_MinusLogProbMetric: 17.8452

Epoch 176: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.7418 - MinusLogProbMetric: 17.7418 - val_loss: 17.8452 - val_MinusLogProbMetric: 17.8452 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 177/1000
2023-09-27 22:36:11.289 
Epoch 177/1000 
	 loss: 17.7789, MinusLogProbMetric: 17.7789, val_loss: 18.1451, val_MinusLogProbMetric: 18.1451

Epoch 177: val_loss did not improve from 17.70309
196/196 - 79s - loss: 17.7789 - MinusLogProbMetric: 17.7789 - val_loss: 18.1451 - val_MinusLogProbMetric: 18.1451 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 178/1000
2023-09-27 22:37:31.036 
Epoch 178/1000 
	 loss: 17.7474, MinusLogProbMetric: 17.7474, val_loss: 18.1026, val_MinusLogProbMetric: 18.1026

Epoch 178: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.7474 - MinusLogProbMetric: 17.7474 - val_loss: 18.1026 - val_MinusLogProbMetric: 18.1026 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 179/1000
2023-09-27 22:38:51.428 
Epoch 179/1000 
	 loss: 17.7462, MinusLogProbMetric: 17.7462, val_loss: 18.2035, val_MinusLogProbMetric: 18.2035

Epoch 179: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.7462 - MinusLogProbMetric: 17.7462 - val_loss: 18.2035 - val_MinusLogProbMetric: 18.2035 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 180/1000
2023-09-27 22:40:11.213 
Epoch 180/1000 
	 loss: 17.7262, MinusLogProbMetric: 17.7262, val_loss: 18.1485, val_MinusLogProbMetric: 18.1485

Epoch 180: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.7262 - MinusLogProbMetric: 17.7262 - val_loss: 18.1485 - val_MinusLogProbMetric: 18.1485 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 181/1000
2023-09-27 22:41:31.025 
Epoch 181/1000 
	 loss: 19.3542, MinusLogProbMetric: 19.3542, val_loss: 18.5909, val_MinusLogProbMetric: 18.5909

Epoch 181: val_loss did not improve from 17.70309
196/196 - 80s - loss: 19.3542 - MinusLogProbMetric: 19.3542 - val_loss: 18.5909 - val_MinusLogProbMetric: 18.5909 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 182/1000
2023-09-27 22:42:50.834 
Epoch 182/1000 
	 loss: 18.0102, MinusLogProbMetric: 18.0102, val_loss: 17.8732, val_MinusLogProbMetric: 17.8732

Epoch 182: val_loss did not improve from 17.70309
196/196 - 80s - loss: 18.0102 - MinusLogProbMetric: 18.0102 - val_loss: 17.8732 - val_MinusLogProbMetric: 17.8732 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 183/1000
2023-09-27 22:44:10.601 
Epoch 183/1000 
	 loss: 17.8331, MinusLogProbMetric: 17.8331, val_loss: 17.8458, val_MinusLogProbMetric: 17.8458

Epoch 183: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.8331 - MinusLogProbMetric: 17.8331 - val_loss: 17.8458 - val_MinusLogProbMetric: 17.8458 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 184/1000
2023-09-27 22:45:30.390 
Epoch 184/1000 
	 loss: 17.7900, MinusLogProbMetric: 17.7900, val_loss: 17.8208, val_MinusLogProbMetric: 17.8208

Epoch 184: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.7900 - MinusLogProbMetric: 17.7900 - val_loss: 17.8208 - val_MinusLogProbMetric: 17.8208 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 185/1000
2023-09-27 22:46:50.010 
Epoch 185/1000 
	 loss: 17.8089, MinusLogProbMetric: 17.8089, val_loss: 18.0121, val_MinusLogProbMetric: 18.0121

Epoch 185: val_loss did not improve from 17.70309
196/196 - 80s - loss: 17.8089 - MinusLogProbMetric: 17.8089 - val_loss: 18.0121 - val_MinusLogProbMetric: 18.0121 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 186/1000
2023-09-27 22:48:09.922 
Epoch 186/1000 
	 loss: 17.9448, MinusLogProbMetric: 17.9448, val_loss: 17.6635, val_MinusLogProbMetric: 17.6635

Epoch 186: val_loss improved from 17.70309 to 17.66350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 17.9448 - MinusLogProbMetric: 17.9448 - val_loss: 17.6635 - val_MinusLogProbMetric: 17.6635 - lr: 3.3333e-04 - 83s/epoch - 426ms/step
Epoch 187/1000
2023-09-27 22:49:33.992 
Epoch 187/1000 
	 loss: 17.6831, MinusLogProbMetric: 17.6831, val_loss: 18.0670, val_MinusLogProbMetric: 18.0670

Epoch 187: val_loss did not improve from 17.66350
196/196 - 81s - loss: 17.6831 - MinusLogProbMetric: 17.6831 - val_loss: 18.0670 - val_MinusLogProbMetric: 18.0670 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 188/1000
2023-09-27 22:50:54.166 
Epoch 188/1000 
	 loss: 17.7049, MinusLogProbMetric: 17.7049, val_loss: 17.9642, val_MinusLogProbMetric: 17.9642

Epoch 188: val_loss did not improve from 17.66350
196/196 - 80s - loss: 17.7049 - MinusLogProbMetric: 17.7049 - val_loss: 17.9642 - val_MinusLogProbMetric: 17.9642 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 189/1000
2023-09-27 22:52:14.516 
Epoch 189/1000 
	 loss: 17.7048, MinusLogProbMetric: 17.7048, val_loss: 17.7329, val_MinusLogProbMetric: 17.7329

Epoch 189: val_loss did not improve from 17.66350
196/196 - 80s - loss: 17.7048 - MinusLogProbMetric: 17.7048 - val_loss: 17.7329 - val_MinusLogProbMetric: 17.7329 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 190/1000
2023-09-27 22:53:34.485 
Epoch 190/1000 
	 loss: 17.7829, MinusLogProbMetric: 17.7829, val_loss: 17.9727, val_MinusLogProbMetric: 17.9727

Epoch 190: val_loss did not improve from 17.66350
196/196 - 80s - loss: 17.7829 - MinusLogProbMetric: 17.7829 - val_loss: 17.9727 - val_MinusLogProbMetric: 17.9727 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 191/1000
2023-09-27 22:54:55.083 
Epoch 191/1000 
	 loss: 17.7045, MinusLogProbMetric: 17.7045, val_loss: 17.8280, val_MinusLogProbMetric: 17.8280

Epoch 191: val_loss did not improve from 17.66350
196/196 - 81s - loss: 17.7045 - MinusLogProbMetric: 17.7045 - val_loss: 17.8280 - val_MinusLogProbMetric: 17.8280 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 192/1000
2023-09-27 22:56:15.182 
Epoch 192/1000 
	 loss: 17.7102, MinusLogProbMetric: 17.7102, val_loss: 17.4724, val_MinusLogProbMetric: 17.4724

Epoch 192: val_loss improved from 17.66350 to 17.47239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 17.7102 - MinusLogProbMetric: 17.7102 - val_loss: 17.4724 - val_MinusLogProbMetric: 17.4724 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 193/1000
2023-09-27 22:57:36.247 
Epoch 193/1000 
	 loss: 18.1061, MinusLogProbMetric: 18.1061, val_loss: 18.0214, val_MinusLogProbMetric: 18.0214

Epoch 193: val_loss did not improve from 17.47239
196/196 - 80s - loss: 18.1061 - MinusLogProbMetric: 18.1061 - val_loss: 18.0214 - val_MinusLogProbMetric: 18.0214 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 194/1000
2023-09-27 22:58:56.082 
Epoch 194/1000 
	 loss: 17.7455, MinusLogProbMetric: 17.7455, val_loss: 17.7010, val_MinusLogProbMetric: 17.7010

Epoch 194: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.7455 - MinusLogProbMetric: 17.7455 - val_loss: 17.7010 - val_MinusLogProbMetric: 17.7010 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 195/1000
2023-09-27 23:00:16.527 
Epoch 195/1000 
	 loss: 17.6887, MinusLogProbMetric: 17.6887, val_loss: 17.5844, val_MinusLogProbMetric: 17.5844

Epoch 195: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6887 - MinusLogProbMetric: 17.6887 - val_loss: 17.5844 - val_MinusLogProbMetric: 17.5844 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 196/1000
2023-09-27 23:01:36.851 
Epoch 196/1000 
	 loss: 17.7314, MinusLogProbMetric: 17.7314, val_loss: 17.7891, val_MinusLogProbMetric: 17.7891

Epoch 196: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.7314 - MinusLogProbMetric: 17.7314 - val_loss: 17.7891 - val_MinusLogProbMetric: 17.7891 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 197/1000
2023-09-27 23:02:56.904 
Epoch 197/1000 
	 loss: 17.6810, MinusLogProbMetric: 17.6810, val_loss: 18.1522, val_MinusLogProbMetric: 18.1522

Epoch 197: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6810 - MinusLogProbMetric: 17.6810 - val_loss: 18.1522 - val_MinusLogProbMetric: 18.1522 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 198/1000
2023-09-27 23:04:17.095 
Epoch 198/1000 
	 loss: 17.6605, MinusLogProbMetric: 17.6605, val_loss: 17.8670, val_MinusLogProbMetric: 17.8670

Epoch 198: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6605 - MinusLogProbMetric: 17.6605 - val_loss: 17.8670 - val_MinusLogProbMetric: 17.8670 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 199/1000
2023-09-27 23:05:37.475 
Epoch 199/1000 
	 loss: 17.6570, MinusLogProbMetric: 17.6570, val_loss: 17.6652, val_MinusLogProbMetric: 17.6652

Epoch 199: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6570 - MinusLogProbMetric: 17.6570 - val_loss: 17.6652 - val_MinusLogProbMetric: 17.6652 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 200/1000
2023-09-27 23:06:57.701 
Epoch 200/1000 
	 loss: 17.6519, MinusLogProbMetric: 17.6519, val_loss: 17.7537, val_MinusLogProbMetric: 17.7537

Epoch 200: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6519 - MinusLogProbMetric: 17.6519 - val_loss: 17.7537 - val_MinusLogProbMetric: 17.7537 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 201/1000
2023-09-27 23:08:17.882 
Epoch 201/1000 
	 loss: 17.6081, MinusLogProbMetric: 17.6081, val_loss: 17.9578, val_MinusLogProbMetric: 17.9578

Epoch 201: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6081 - MinusLogProbMetric: 17.6081 - val_loss: 17.9578 - val_MinusLogProbMetric: 17.9578 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 202/1000
2023-09-27 23:09:37.905 
Epoch 202/1000 
	 loss: 17.5624, MinusLogProbMetric: 17.5624, val_loss: 18.2334, val_MinusLogProbMetric: 18.2334

Epoch 202: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5624 - MinusLogProbMetric: 17.5624 - val_loss: 18.2334 - val_MinusLogProbMetric: 18.2334 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 203/1000
2023-09-27 23:10:58.113 
Epoch 203/1000 
	 loss: 18.1010, MinusLogProbMetric: 18.1010, val_loss: 18.3116, val_MinusLogProbMetric: 18.3116

Epoch 203: val_loss did not improve from 17.47239
196/196 - 80s - loss: 18.1010 - MinusLogProbMetric: 18.1010 - val_loss: 18.3116 - val_MinusLogProbMetric: 18.3116 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 204/1000
2023-09-27 23:12:18.246 
Epoch 204/1000 
	 loss: 17.7530, MinusLogProbMetric: 17.7530, val_loss: 17.8074, val_MinusLogProbMetric: 17.8074

Epoch 204: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.7530 - MinusLogProbMetric: 17.7530 - val_loss: 17.8074 - val_MinusLogProbMetric: 17.8074 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 205/1000
2023-09-27 23:13:38.173 
Epoch 205/1000 
	 loss: 17.6137, MinusLogProbMetric: 17.6137, val_loss: 17.8995, val_MinusLogProbMetric: 17.8995

Epoch 205: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6137 - MinusLogProbMetric: 17.6137 - val_loss: 17.8995 - val_MinusLogProbMetric: 17.8995 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 206/1000
2023-09-27 23:14:58.488 
Epoch 206/1000 
	 loss: 17.5961, MinusLogProbMetric: 17.5961, val_loss: 17.7262, val_MinusLogProbMetric: 17.7262

Epoch 206: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5961 - MinusLogProbMetric: 17.5961 - val_loss: 17.7262 - val_MinusLogProbMetric: 17.7262 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 207/1000
2023-09-27 23:16:18.499 
Epoch 207/1000 
	 loss: 17.6135, MinusLogProbMetric: 17.6135, val_loss: 17.7881, val_MinusLogProbMetric: 17.7881

Epoch 207: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6135 - MinusLogProbMetric: 17.6135 - val_loss: 17.7881 - val_MinusLogProbMetric: 17.7881 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 208/1000
2023-09-27 23:17:38.512 
Epoch 208/1000 
	 loss: 17.5965, MinusLogProbMetric: 17.5965, val_loss: 17.8263, val_MinusLogProbMetric: 17.8263

Epoch 208: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5965 - MinusLogProbMetric: 17.5965 - val_loss: 17.8263 - val_MinusLogProbMetric: 17.8263 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 209/1000
2023-09-27 23:18:58.880 
Epoch 209/1000 
	 loss: 17.7492, MinusLogProbMetric: 17.7492, val_loss: 17.8672, val_MinusLogProbMetric: 17.8672

Epoch 209: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.7492 - MinusLogProbMetric: 17.7492 - val_loss: 17.8672 - val_MinusLogProbMetric: 17.8672 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 210/1000
2023-09-27 23:20:18.640 
Epoch 210/1000 
	 loss: 17.6271, MinusLogProbMetric: 17.6271, val_loss: 18.1988, val_MinusLogProbMetric: 18.1988

Epoch 210: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6271 - MinusLogProbMetric: 17.6271 - val_loss: 18.1988 - val_MinusLogProbMetric: 18.1988 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 211/1000
2023-09-27 23:21:37.910 
Epoch 211/1000 
	 loss: 17.6591, MinusLogProbMetric: 17.6591, val_loss: 17.6828, val_MinusLogProbMetric: 17.6828

Epoch 211: val_loss did not improve from 17.47239
196/196 - 79s - loss: 17.6591 - MinusLogProbMetric: 17.6591 - val_loss: 17.6828 - val_MinusLogProbMetric: 17.6828 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 212/1000
2023-09-27 23:22:58.343 
Epoch 212/1000 
	 loss: 17.6576, MinusLogProbMetric: 17.6576, val_loss: 18.1113, val_MinusLogProbMetric: 18.1113

Epoch 212: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6576 - MinusLogProbMetric: 17.6576 - val_loss: 18.1113 - val_MinusLogProbMetric: 18.1113 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 213/1000
2023-09-27 23:24:18.534 
Epoch 213/1000 
	 loss: 17.6698, MinusLogProbMetric: 17.6698, val_loss: 17.6205, val_MinusLogProbMetric: 17.6205

Epoch 213: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6698 - MinusLogProbMetric: 17.6698 - val_loss: 17.6205 - val_MinusLogProbMetric: 17.6205 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 214/1000
2023-09-27 23:25:33.468 
Epoch 214/1000 
	 loss: 17.5523, MinusLogProbMetric: 17.5523, val_loss: 17.6618, val_MinusLogProbMetric: 17.6618

Epoch 214: val_loss did not improve from 17.47239
196/196 - 75s - loss: 17.5523 - MinusLogProbMetric: 17.5523 - val_loss: 17.6618 - val_MinusLogProbMetric: 17.6618 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 215/1000
2023-09-27 23:26:43.385 
Epoch 215/1000 
	 loss: 17.5903, MinusLogProbMetric: 17.5903, val_loss: 17.6469, val_MinusLogProbMetric: 17.6469

Epoch 215: val_loss did not improve from 17.47239
196/196 - 70s - loss: 17.5903 - MinusLogProbMetric: 17.5903 - val_loss: 17.6469 - val_MinusLogProbMetric: 17.6469 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 216/1000
2023-09-27 23:28:02.228 
Epoch 216/1000 
	 loss: 17.5925, MinusLogProbMetric: 17.5925, val_loss: 17.6268, val_MinusLogProbMetric: 17.6268

Epoch 216: val_loss did not improve from 17.47239
196/196 - 79s - loss: 17.5925 - MinusLogProbMetric: 17.5925 - val_loss: 17.6268 - val_MinusLogProbMetric: 17.6268 - lr: 3.3333e-04 - 79s/epoch - 402ms/step
Epoch 217/1000
2023-09-27 23:29:22.023 
Epoch 217/1000 
	 loss: 17.7077, MinusLogProbMetric: 17.7077, val_loss: 17.6071, val_MinusLogProbMetric: 17.6071

Epoch 217: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.7077 - MinusLogProbMetric: 17.7077 - val_loss: 17.6071 - val_MinusLogProbMetric: 17.6071 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 218/1000
2023-09-27 23:30:42.768 
Epoch 218/1000 
	 loss: 17.5929, MinusLogProbMetric: 17.5929, val_loss: 17.9254, val_MinusLogProbMetric: 17.9254

Epoch 218: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5929 - MinusLogProbMetric: 17.5929 - val_loss: 17.9254 - val_MinusLogProbMetric: 17.9254 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 219/1000
2023-09-27 23:32:04.016 
Epoch 219/1000 
	 loss: 17.5508, MinusLogProbMetric: 17.5508, val_loss: 17.7508, val_MinusLogProbMetric: 17.7508

Epoch 219: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5508 - MinusLogProbMetric: 17.5508 - val_loss: 17.7508 - val_MinusLogProbMetric: 17.7508 - lr: 3.3333e-04 - 81s/epoch - 415ms/step
Epoch 220/1000
2023-09-27 23:33:25.122 
Epoch 220/1000 
	 loss: 17.5832, MinusLogProbMetric: 17.5832, val_loss: 18.1324, val_MinusLogProbMetric: 18.1324

Epoch 220: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5832 - MinusLogProbMetric: 17.5832 - val_loss: 18.1324 - val_MinusLogProbMetric: 18.1324 - lr: 3.3333e-04 - 81s/epoch - 414ms/step
Epoch 221/1000
2023-09-27 23:34:45.633 
Epoch 221/1000 
	 loss: 17.5748, MinusLogProbMetric: 17.5748, val_loss: 17.7628, val_MinusLogProbMetric: 17.7628

Epoch 221: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5748 - MinusLogProbMetric: 17.5748 - val_loss: 17.7628 - val_MinusLogProbMetric: 17.7628 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 222/1000
2023-09-27 23:36:05.723 
Epoch 222/1000 
	 loss: 17.5648, MinusLogProbMetric: 17.5648, val_loss: 17.9667, val_MinusLogProbMetric: 17.9667

Epoch 222: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5648 - MinusLogProbMetric: 17.5648 - val_loss: 17.9667 - val_MinusLogProbMetric: 17.9667 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 223/1000
2023-09-27 23:37:26.088 
Epoch 223/1000 
	 loss: 17.6972, MinusLogProbMetric: 17.6972, val_loss: 17.9914, val_MinusLogProbMetric: 17.9914

Epoch 223: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.6972 - MinusLogProbMetric: 17.6972 - val_loss: 17.9914 - val_MinusLogProbMetric: 17.9914 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 224/1000
2023-09-27 23:38:46.974 
Epoch 224/1000 
	 loss: 17.6203, MinusLogProbMetric: 17.6203, val_loss: 18.3729, val_MinusLogProbMetric: 18.3729

Epoch 224: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.6203 - MinusLogProbMetric: 17.6203 - val_loss: 18.3729 - val_MinusLogProbMetric: 18.3729 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 225/1000
2023-09-27 23:40:07.466 
Epoch 225/1000 
	 loss: 17.5663, MinusLogProbMetric: 17.5663, val_loss: 18.1343, val_MinusLogProbMetric: 18.1343

Epoch 225: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5663 - MinusLogProbMetric: 17.5663 - val_loss: 18.1343 - val_MinusLogProbMetric: 18.1343 - lr: 3.3333e-04 - 80s/epoch - 411ms/step
Epoch 226/1000
2023-09-27 23:41:27.921 
Epoch 226/1000 
	 loss: 17.5497, MinusLogProbMetric: 17.5497, val_loss: 17.6040, val_MinusLogProbMetric: 17.6040

Epoch 226: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5497 - MinusLogProbMetric: 17.5497 - val_loss: 17.6040 - val_MinusLogProbMetric: 17.6040 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 227/1000
2023-09-27 23:42:47.865 
Epoch 227/1000 
	 loss: 17.5450, MinusLogProbMetric: 17.5450, val_loss: 18.2609, val_MinusLogProbMetric: 18.2609

Epoch 227: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5450 - MinusLogProbMetric: 17.5450 - val_loss: 18.2609 - val_MinusLogProbMetric: 18.2609 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 228/1000
2023-09-27 23:44:08.723 
Epoch 228/1000 
	 loss: 17.5272, MinusLogProbMetric: 17.5272, val_loss: 18.0006, val_MinusLogProbMetric: 18.0006

Epoch 228: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5272 - MinusLogProbMetric: 17.5272 - val_loss: 18.0006 - val_MinusLogProbMetric: 18.0006 - lr: 3.3333e-04 - 81s/epoch - 413ms/step
Epoch 229/1000
2023-09-27 23:45:29.026 
Epoch 229/1000 
	 loss: 17.5922, MinusLogProbMetric: 17.5922, val_loss: 17.7661, val_MinusLogProbMetric: 17.7661

Epoch 229: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5922 - MinusLogProbMetric: 17.5922 - val_loss: 17.7661 - val_MinusLogProbMetric: 17.7661 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 230/1000
2023-09-27 23:46:49.400 
Epoch 230/1000 
	 loss: 17.5631, MinusLogProbMetric: 17.5631, val_loss: 18.0370, val_MinusLogProbMetric: 18.0370

Epoch 230: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5631 - MinusLogProbMetric: 17.5631 - val_loss: 18.0370 - val_MinusLogProbMetric: 18.0370 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 231/1000
2023-09-27 23:48:09.294 
Epoch 231/1000 
	 loss: 17.5076, MinusLogProbMetric: 17.5076, val_loss: 18.4510, val_MinusLogProbMetric: 18.4510

Epoch 231: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5076 - MinusLogProbMetric: 17.5076 - val_loss: 18.4510 - val_MinusLogProbMetric: 18.4510 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 232/1000
2023-09-27 23:49:29.634 
Epoch 232/1000 
	 loss: 17.4948, MinusLogProbMetric: 17.4948, val_loss: 17.7745, val_MinusLogProbMetric: 17.7745

Epoch 232: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.4948 - MinusLogProbMetric: 17.4948 - val_loss: 17.7745 - val_MinusLogProbMetric: 17.7745 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 233/1000
2023-09-27 23:50:49.766 
Epoch 233/1000 
	 loss: 17.5222, MinusLogProbMetric: 17.5222, val_loss: 17.4776, val_MinusLogProbMetric: 17.4776

Epoch 233: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5222 - MinusLogProbMetric: 17.5222 - val_loss: 17.4776 - val_MinusLogProbMetric: 17.4776 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 234/1000
2023-09-27 23:52:09.942 
Epoch 234/1000 
	 loss: 17.4829, MinusLogProbMetric: 17.4829, val_loss: 18.1695, val_MinusLogProbMetric: 18.1695

Epoch 234: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.4829 - MinusLogProbMetric: 17.4829 - val_loss: 18.1695 - val_MinusLogProbMetric: 18.1695 - lr: 3.3333e-04 - 80s/epoch - 409ms/step
Epoch 235/1000
2023-09-27 23:53:29.528 
Epoch 235/1000 
	 loss: 17.7737, MinusLogProbMetric: 17.7737, val_loss: 17.6942, val_MinusLogProbMetric: 17.6942

Epoch 235: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.7737 - MinusLogProbMetric: 17.7737 - val_loss: 17.6942 - val_MinusLogProbMetric: 17.6942 - lr: 3.3333e-04 - 80s/epoch - 406ms/step
Epoch 236/1000
2023-09-27 23:54:50.145 
Epoch 236/1000 
	 loss: 17.5395, MinusLogProbMetric: 17.5395, val_loss: 17.8224, val_MinusLogProbMetric: 17.8224

Epoch 236: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5395 - MinusLogProbMetric: 17.5395 - val_loss: 17.8224 - val_MinusLogProbMetric: 17.8224 - lr: 3.3333e-04 - 81s/epoch - 411ms/step
Epoch 237/1000
2023-09-27 23:56:09.939 
Epoch 237/1000 
	 loss: 17.5019, MinusLogProbMetric: 17.5019, val_loss: 17.6154, val_MinusLogProbMetric: 17.6154

Epoch 237: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5019 - MinusLogProbMetric: 17.5019 - val_loss: 17.6154 - val_MinusLogProbMetric: 17.6154 - lr: 3.3333e-04 - 80s/epoch - 407ms/step
Epoch 238/1000
2023-09-27 23:57:30.280 
Epoch 238/1000 
	 loss: 17.5062, MinusLogProbMetric: 17.5062, val_loss: 17.7047, val_MinusLogProbMetric: 17.7047

Epoch 238: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5062 - MinusLogProbMetric: 17.5062 - val_loss: 17.7047 - val_MinusLogProbMetric: 17.7047 - lr: 3.3333e-04 - 80s/epoch - 410ms/step
Epoch 239/1000
2023-09-27 23:58:50.285 
Epoch 239/1000 
	 loss: 17.5434, MinusLogProbMetric: 17.5434, val_loss: 17.7376, val_MinusLogProbMetric: 17.7376

Epoch 239: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.5434 - MinusLogProbMetric: 17.5434 - val_loss: 17.7376 - val_MinusLogProbMetric: 17.7376 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 240/1000
2023-09-28 00:00:10.981 
Epoch 240/1000 
	 loss: 17.5195, MinusLogProbMetric: 17.5195, val_loss: 17.7558, val_MinusLogProbMetric: 17.7558

Epoch 240: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5195 - MinusLogProbMetric: 17.5195 - val_loss: 17.7558 - val_MinusLogProbMetric: 17.7558 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 241/1000
2023-09-28 00:01:30.998 
Epoch 241/1000 
	 loss: 17.4807, MinusLogProbMetric: 17.4807, val_loss: 17.7275, val_MinusLogProbMetric: 17.7275

Epoch 241: val_loss did not improve from 17.47239
196/196 - 80s - loss: 17.4807 - MinusLogProbMetric: 17.4807 - val_loss: 17.7275 - val_MinusLogProbMetric: 17.7275 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 242/1000
2023-09-28 00:02:51.667 
Epoch 242/1000 
	 loss: 17.5559, MinusLogProbMetric: 17.5559, val_loss: 17.9143, val_MinusLogProbMetric: 17.9143

Epoch 242: val_loss did not improve from 17.47239
196/196 - 81s - loss: 17.5559 - MinusLogProbMetric: 17.5559 - val_loss: 17.9143 - val_MinusLogProbMetric: 17.9143 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 243/1000
2023-09-28 00:04:12.404 
Epoch 243/1000 
	 loss: 17.1348, MinusLogProbMetric: 17.1348, val_loss: 17.2177, val_MinusLogProbMetric: 17.2177

Epoch 243: val_loss improved from 17.47239 to 17.21770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 17.1348 - MinusLogProbMetric: 17.1348 - val_loss: 17.2177 - val_MinusLogProbMetric: 17.2177 - lr: 1.6667e-04 - 82s/epoch - 418ms/step
Epoch 244/1000
2023-09-28 00:05:34.175 
Epoch 244/1000 
	 loss: 17.1032, MinusLogProbMetric: 17.1032, val_loss: 17.1604, val_MinusLogProbMetric: 17.1604

Epoch 244: val_loss improved from 17.21770 to 17.16045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 17.1032 - MinusLogProbMetric: 17.1032 - val_loss: 17.1604 - val_MinusLogProbMetric: 17.1604 - lr: 1.6667e-04 - 82s/epoch - 417ms/step
Epoch 245/1000
2023-09-28 00:06:56.241 
Epoch 245/1000 
	 loss: 17.7002, MinusLogProbMetric: 17.7002, val_loss: 17.5309, val_MinusLogProbMetric: 17.5309

Epoch 245: val_loss did not improve from 17.16045
196/196 - 81s - loss: 17.7002 - MinusLogProbMetric: 17.7002 - val_loss: 17.5309 - val_MinusLogProbMetric: 17.5309 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 246/1000
2023-09-28 00:08:16.838 
Epoch 246/1000 
	 loss: 17.3718, MinusLogProbMetric: 17.3718, val_loss: 17.4963, val_MinusLogProbMetric: 17.4963

Epoch 246: val_loss did not improve from 17.16045
196/196 - 81s - loss: 17.3718 - MinusLogProbMetric: 17.3718 - val_loss: 17.4963 - val_MinusLogProbMetric: 17.4963 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 247/1000
2023-09-28 00:09:37.262 
Epoch 247/1000 
	 loss: 17.3718, MinusLogProbMetric: 17.3718, val_loss: 17.4681, val_MinusLogProbMetric: 17.4681

Epoch 247: val_loss did not improve from 17.16045
196/196 - 80s - loss: 17.3718 - MinusLogProbMetric: 17.3718 - val_loss: 17.4681 - val_MinusLogProbMetric: 17.4681 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 248/1000
2023-09-28 00:10:57.796 
Epoch 248/1000 
	 loss: 17.3552, MinusLogProbMetric: 17.3552, val_loss: 17.5234, val_MinusLogProbMetric: 17.5234

Epoch 248: val_loss did not improve from 17.16045
196/196 - 81s - loss: 17.3552 - MinusLogProbMetric: 17.3552 - val_loss: 17.5234 - val_MinusLogProbMetric: 17.5234 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 249/1000
2023-09-28 00:12:17.910 
Epoch 249/1000 
	 loss: 17.3558, MinusLogProbMetric: 17.3558, val_loss: 17.4773, val_MinusLogProbMetric: 17.4773

Epoch 249: val_loss did not improve from 17.16045
196/196 - 80s - loss: 17.3558 - MinusLogProbMetric: 17.3558 - val_loss: 17.4773 - val_MinusLogProbMetric: 17.4773 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 250/1000
2023-09-28 00:13:38.164 
Epoch 250/1000 
	 loss: 17.3499, MinusLogProbMetric: 17.3499, val_loss: 17.4062, val_MinusLogProbMetric: 17.4062

Epoch 250: val_loss did not improve from 17.16045
196/196 - 80s - loss: 17.3499 - MinusLogProbMetric: 17.3499 - val_loss: 17.4062 - val_MinusLogProbMetric: 17.4062 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 251/1000
2023-09-28 00:14:58.662 
Epoch 251/1000 
	 loss: 17.3343, MinusLogProbMetric: 17.3343, val_loss: 17.5134, val_MinusLogProbMetric: 17.5134

Epoch 251: val_loss did not improve from 17.16045
196/196 - 80s - loss: 17.3343 - MinusLogProbMetric: 17.3343 - val_loss: 17.5134 - val_MinusLogProbMetric: 17.5134 - lr: 1.6667e-04 - 80s/epoch - 411ms/step
Epoch 252/1000
2023-09-28 00:16:18.788 
Epoch 252/1000 
	 loss: 17.3112, MinusLogProbMetric: 17.3112, val_loss: 17.4598, val_MinusLogProbMetric: 17.4598

Epoch 252: val_loss did not improve from 17.16045
196/196 - 80s - loss: 17.3112 - MinusLogProbMetric: 17.3112 - val_loss: 17.4598 - val_MinusLogProbMetric: 17.4598 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 253/1000
2023-09-28 00:17:39.002 
Epoch 253/1000 
	 loss: 17.3473, MinusLogProbMetric: 17.3473, val_loss: 17.2665, val_MinusLogProbMetric: 17.2665

Epoch 253: val_loss did not improve from 17.16045
196/196 - 80s - loss: 17.3473 - MinusLogProbMetric: 17.3473 - val_loss: 17.2665 - val_MinusLogProbMetric: 17.2665 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 254/1000
2023-09-28 00:18:59.013 
Epoch 254/1000 
	 loss: 17.1153, MinusLogProbMetric: 17.1153, val_loss: 17.1876, val_MinusLogProbMetric: 17.1876

Epoch 254: val_loss did not improve from 17.16045
196/196 - 80s - loss: 17.1153 - MinusLogProbMetric: 17.1153 - val_loss: 17.1876 - val_MinusLogProbMetric: 17.1876 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 255/1000
2023-09-28 00:20:19.109 
Epoch 255/1000 
	 loss: 17.1089, MinusLogProbMetric: 17.1089, val_loss: 17.1505, val_MinusLogProbMetric: 17.1505

Epoch 255: val_loss improved from 17.16045 to 17.15054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 17.1089 - MinusLogProbMetric: 17.1089 - val_loss: 17.1505 - val_MinusLogProbMetric: 17.1505 - lr: 1.6667e-04 - 82s/epoch - 416ms/step
Epoch 256/1000
2023-09-28 00:21:40.188 
Epoch 256/1000 
	 loss: 17.1076, MinusLogProbMetric: 17.1076, val_loss: 17.2329, val_MinusLogProbMetric: 17.2329

Epoch 256: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.1076 - MinusLogProbMetric: 17.1076 - val_loss: 17.2329 - val_MinusLogProbMetric: 17.2329 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 257/1000
2023-09-28 00:23:00.858 
Epoch 257/1000 
	 loss: 17.2086, MinusLogProbMetric: 17.2086, val_loss: 17.5335, val_MinusLogProbMetric: 17.5335

Epoch 257: val_loss did not improve from 17.15054
196/196 - 81s - loss: 17.2086 - MinusLogProbMetric: 17.2086 - val_loss: 17.5335 - val_MinusLogProbMetric: 17.5335 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 258/1000
2023-09-28 00:24:21.451 
Epoch 258/1000 
	 loss: 17.0858, MinusLogProbMetric: 17.0858, val_loss: 17.2445, val_MinusLogProbMetric: 17.2445

Epoch 258: val_loss did not improve from 17.15054
196/196 - 81s - loss: 17.0858 - MinusLogProbMetric: 17.0858 - val_loss: 17.2445 - val_MinusLogProbMetric: 17.2445 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 259/1000
2023-09-28 00:25:41.962 
Epoch 259/1000 
	 loss: 17.0678, MinusLogProbMetric: 17.0678, val_loss: 17.2618, val_MinusLogProbMetric: 17.2618

Epoch 259: val_loss did not improve from 17.15054
196/196 - 81s - loss: 17.0678 - MinusLogProbMetric: 17.0678 - val_loss: 17.2618 - val_MinusLogProbMetric: 17.2618 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 260/1000
2023-09-28 00:27:02.105 
Epoch 260/1000 
	 loss: 17.0717, MinusLogProbMetric: 17.0717, val_loss: 17.1967, val_MinusLogProbMetric: 17.1967

Epoch 260: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0717 - MinusLogProbMetric: 17.0717 - val_loss: 17.1967 - val_MinusLogProbMetric: 17.1967 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 261/1000
2023-09-28 00:28:21.921 
Epoch 261/1000 
	 loss: 17.1234, MinusLogProbMetric: 17.1234, val_loss: 17.1679, val_MinusLogProbMetric: 17.1679

Epoch 261: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.1234 - MinusLogProbMetric: 17.1234 - val_loss: 17.1679 - val_MinusLogProbMetric: 17.1679 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 262/1000
2023-09-28 00:29:42.631 
Epoch 262/1000 
	 loss: 17.2674, MinusLogProbMetric: 17.2674, val_loss: 17.5193, val_MinusLogProbMetric: 17.5193

Epoch 262: val_loss did not improve from 17.15054
196/196 - 81s - loss: 17.2674 - MinusLogProbMetric: 17.2674 - val_loss: 17.5193 - val_MinusLogProbMetric: 17.5193 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 263/1000
2023-09-28 00:31:03.374 
Epoch 263/1000 
	 loss: 17.1216, MinusLogProbMetric: 17.1216, val_loss: 17.3678, val_MinusLogProbMetric: 17.3678

Epoch 263: val_loss did not improve from 17.15054
196/196 - 81s - loss: 17.1216 - MinusLogProbMetric: 17.1216 - val_loss: 17.3678 - val_MinusLogProbMetric: 17.3678 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 264/1000
2023-09-28 00:32:23.365 
Epoch 264/1000 
	 loss: 17.0670, MinusLogProbMetric: 17.0670, val_loss: 17.1523, val_MinusLogProbMetric: 17.1523

Epoch 264: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0670 - MinusLogProbMetric: 17.0670 - val_loss: 17.1523 - val_MinusLogProbMetric: 17.1523 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 265/1000
2023-09-28 00:33:43.237 
Epoch 265/1000 
	 loss: 17.0822, MinusLogProbMetric: 17.0822, val_loss: 17.3229, val_MinusLogProbMetric: 17.3229

Epoch 265: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0822 - MinusLogProbMetric: 17.0822 - val_loss: 17.3229 - val_MinusLogProbMetric: 17.3229 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 266/1000
2023-09-28 00:35:03.395 
Epoch 266/1000 
	 loss: 17.0905, MinusLogProbMetric: 17.0905, val_loss: 17.3635, val_MinusLogProbMetric: 17.3635

Epoch 266: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0905 - MinusLogProbMetric: 17.0905 - val_loss: 17.3635 - val_MinusLogProbMetric: 17.3635 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 267/1000
2023-09-28 00:36:23.680 
Epoch 267/1000 
	 loss: 17.1041, MinusLogProbMetric: 17.1041, val_loss: 17.3718, val_MinusLogProbMetric: 17.3718

Epoch 267: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.1041 - MinusLogProbMetric: 17.1041 - val_loss: 17.3718 - val_MinusLogProbMetric: 17.3718 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 268/1000
2023-09-28 00:37:34.619 
Epoch 268/1000 
	 loss: 17.0871, MinusLogProbMetric: 17.0871, val_loss: 17.1765, val_MinusLogProbMetric: 17.1765

Epoch 268: val_loss did not improve from 17.15054
196/196 - 71s - loss: 17.0871 - MinusLogProbMetric: 17.0871 - val_loss: 17.1765 - val_MinusLogProbMetric: 17.1765 - lr: 1.6667e-04 - 71s/epoch - 362ms/step
Epoch 269/1000
2023-09-28 00:38:42.477 
Epoch 269/1000 
	 loss: 17.0643, MinusLogProbMetric: 17.0643, val_loss: 17.2173, val_MinusLogProbMetric: 17.2173

Epoch 269: val_loss did not improve from 17.15054
196/196 - 68s - loss: 17.0643 - MinusLogProbMetric: 17.0643 - val_loss: 17.2173 - val_MinusLogProbMetric: 17.2173 - lr: 1.6667e-04 - 68s/epoch - 346ms/step
Epoch 270/1000
2023-09-28 00:39:55.095 
Epoch 270/1000 
	 loss: 17.0803, MinusLogProbMetric: 17.0803, val_loss: 17.4086, val_MinusLogProbMetric: 17.4086

Epoch 270: val_loss did not improve from 17.15054
196/196 - 73s - loss: 17.0803 - MinusLogProbMetric: 17.0803 - val_loss: 17.4086 - val_MinusLogProbMetric: 17.4086 - lr: 1.6667e-04 - 73s/epoch - 370ms/step
Epoch 271/1000
2023-09-28 00:41:00.570 
Epoch 271/1000 
	 loss: 17.0738, MinusLogProbMetric: 17.0738, val_loss: 17.3387, val_MinusLogProbMetric: 17.3387

Epoch 271: val_loss did not improve from 17.15054
196/196 - 65s - loss: 17.0738 - MinusLogProbMetric: 17.0738 - val_loss: 17.3387 - val_MinusLogProbMetric: 17.3387 - lr: 1.6667e-04 - 65s/epoch - 334ms/step
Epoch 272/1000
2023-09-28 00:42:19.718 
Epoch 272/1000 
	 loss: 17.0806, MinusLogProbMetric: 17.0806, val_loss: 17.1791, val_MinusLogProbMetric: 17.1791

Epoch 272: val_loss did not improve from 17.15054
196/196 - 79s - loss: 17.0806 - MinusLogProbMetric: 17.0806 - val_loss: 17.1791 - val_MinusLogProbMetric: 17.1791 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 273/1000
2023-09-28 00:43:38.416 
Epoch 273/1000 
	 loss: 17.0747, MinusLogProbMetric: 17.0747, val_loss: 17.1736, val_MinusLogProbMetric: 17.1736

Epoch 273: val_loss did not improve from 17.15054
196/196 - 79s - loss: 17.0747 - MinusLogProbMetric: 17.0747 - val_loss: 17.1736 - val_MinusLogProbMetric: 17.1736 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 274/1000
2023-09-28 00:44:57.635 
Epoch 274/1000 
	 loss: 17.0628, MinusLogProbMetric: 17.0628, val_loss: 17.3154, val_MinusLogProbMetric: 17.3154

Epoch 274: val_loss did not improve from 17.15054
196/196 - 79s - loss: 17.0628 - MinusLogProbMetric: 17.0628 - val_loss: 17.3154 - val_MinusLogProbMetric: 17.3154 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 275/1000
2023-09-28 00:46:17.737 
Epoch 275/1000 
	 loss: 17.0834, MinusLogProbMetric: 17.0834, val_loss: 17.1735, val_MinusLogProbMetric: 17.1735

Epoch 275: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0834 - MinusLogProbMetric: 17.0834 - val_loss: 17.1735 - val_MinusLogProbMetric: 17.1735 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 276/1000
2023-09-28 00:47:37.655 
Epoch 276/1000 
	 loss: 17.0711, MinusLogProbMetric: 17.0711, val_loss: 17.4587, val_MinusLogProbMetric: 17.4587

Epoch 276: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0711 - MinusLogProbMetric: 17.0711 - val_loss: 17.4587 - val_MinusLogProbMetric: 17.4587 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 277/1000
2023-09-28 00:48:57.596 
Epoch 277/1000 
	 loss: 17.1280, MinusLogProbMetric: 17.1280, val_loss: 17.2253, val_MinusLogProbMetric: 17.2253

Epoch 277: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.1280 - MinusLogProbMetric: 17.1280 - val_loss: 17.2253 - val_MinusLogProbMetric: 17.2253 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 278/1000
2023-09-28 00:50:17.209 
Epoch 278/1000 
	 loss: 17.0905, MinusLogProbMetric: 17.0905, val_loss: 17.1856, val_MinusLogProbMetric: 17.1856

Epoch 278: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0905 - MinusLogProbMetric: 17.0905 - val_loss: 17.1856 - val_MinusLogProbMetric: 17.1856 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 279/1000
2023-09-28 00:51:37.366 
Epoch 279/1000 
	 loss: 17.0636, MinusLogProbMetric: 17.0636, val_loss: 17.2660, val_MinusLogProbMetric: 17.2660

Epoch 279: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0636 - MinusLogProbMetric: 17.0636 - val_loss: 17.2660 - val_MinusLogProbMetric: 17.2660 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 280/1000
2023-09-28 00:52:57.480 
Epoch 280/1000 
	 loss: 17.1127, MinusLogProbMetric: 17.1127, val_loss: 17.1814, val_MinusLogProbMetric: 17.1814

Epoch 280: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.1127 - MinusLogProbMetric: 17.1127 - val_loss: 17.1814 - val_MinusLogProbMetric: 17.1814 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 281/1000
2023-09-28 00:54:16.674 
Epoch 281/1000 
	 loss: 17.0843, MinusLogProbMetric: 17.0843, val_loss: 17.1986, val_MinusLogProbMetric: 17.1986

Epoch 281: val_loss did not improve from 17.15054
196/196 - 79s - loss: 17.0843 - MinusLogProbMetric: 17.0843 - val_loss: 17.1986 - val_MinusLogProbMetric: 17.1986 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 282/1000
2023-09-28 00:55:36.724 
Epoch 282/1000 
	 loss: 17.0667, MinusLogProbMetric: 17.0667, val_loss: 17.3422, val_MinusLogProbMetric: 17.3422

Epoch 282: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0667 - MinusLogProbMetric: 17.0667 - val_loss: 17.3422 - val_MinusLogProbMetric: 17.3422 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 283/1000
2023-09-28 00:56:56.582 
Epoch 283/1000 
	 loss: 17.0552, MinusLogProbMetric: 17.0552, val_loss: 17.1669, val_MinusLogProbMetric: 17.1669

Epoch 283: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0552 - MinusLogProbMetric: 17.0552 - val_loss: 17.1669 - val_MinusLogProbMetric: 17.1669 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 284/1000
2023-09-28 00:58:16.251 
Epoch 284/1000 
	 loss: 17.0593, MinusLogProbMetric: 17.0593, val_loss: 17.1760, val_MinusLogProbMetric: 17.1760

Epoch 284: val_loss did not improve from 17.15054
196/196 - 80s - loss: 17.0593 - MinusLogProbMetric: 17.0593 - val_loss: 17.1760 - val_MinusLogProbMetric: 17.1760 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 285/1000
2023-09-28 00:59:36.043 
Epoch 285/1000 
	 loss: 17.0760, MinusLogProbMetric: 17.0760, val_loss: 17.1358, val_MinusLogProbMetric: 17.1358

Epoch 285: val_loss improved from 17.15054 to 17.13580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 17.0760 - MinusLogProbMetric: 17.0760 - val_loss: 17.1358 - val_MinusLogProbMetric: 17.1358 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 286/1000
2023-09-28 01:00:56.907 
Epoch 286/1000 
	 loss: 17.0740, MinusLogProbMetric: 17.0740, val_loss: 17.4234, val_MinusLogProbMetric: 17.4234

Epoch 286: val_loss did not improve from 17.13580
196/196 - 79s - loss: 17.0740 - MinusLogProbMetric: 17.0740 - val_loss: 17.4234 - val_MinusLogProbMetric: 17.4234 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 287/1000
2023-09-28 01:02:16.356 
Epoch 287/1000 
	 loss: 17.0747, MinusLogProbMetric: 17.0747, val_loss: 17.5573, val_MinusLogProbMetric: 17.5573

Epoch 287: val_loss did not improve from 17.13580
196/196 - 79s - loss: 17.0747 - MinusLogProbMetric: 17.0747 - val_loss: 17.5573 - val_MinusLogProbMetric: 17.5573 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 288/1000
2023-09-28 01:03:35.770 
Epoch 288/1000 
	 loss: 17.0413, MinusLogProbMetric: 17.0413, val_loss: 17.5067, val_MinusLogProbMetric: 17.5067

Epoch 288: val_loss did not improve from 17.13580
196/196 - 79s - loss: 17.0413 - MinusLogProbMetric: 17.0413 - val_loss: 17.5067 - val_MinusLogProbMetric: 17.5067 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 289/1000
2023-09-28 01:04:55.441 
Epoch 289/1000 
	 loss: 17.0969, MinusLogProbMetric: 17.0969, val_loss: 17.2298, val_MinusLogProbMetric: 17.2298

Epoch 289: val_loss did not improve from 17.13580
196/196 - 80s - loss: 17.0969 - MinusLogProbMetric: 17.0969 - val_loss: 17.2298 - val_MinusLogProbMetric: 17.2298 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 290/1000
2023-09-28 01:06:14.629 
Epoch 290/1000 
	 loss: 17.0353, MinusLogProbMetric: 17.0353, val_loss: 17.1675, val_MinusLogProbMetric: 17.1675

Epoch 290: val_loss did not improve from 17.13580
196/196 - 79s - loss: 17.0353 - MinusLogProbMetric: 17.0353 - val_loss: 17.1675 - val_MinusLogProbMetric: 17.1675 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 291/1000
2023-09-28 01:07:34.262 
Epoch 291/1000 
	 loss: 17.1540, MinusLogProbMetric: 17.1540, val_loss: 17.2222, val_MinusLogProbMetric: 17.2222

Epoch 291: val_loss did not improve from 17.13580
196/196 - 80s - loss: 17.1540 - MinusLogProbMetric: 17.1540 - val_loss: 17.2222 - val_MinusLogProbMetric: 17.2222 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 292/1000
2023-09-28 01:08:54.749 
Epoch 292/1000 
	 loss: 17.0607, MinusLogProbMetric: 17.0607, val_loss: 17.3344, val_MinusLogProbMetric: 17.3344

Epoch 292: val_loss did not improve from 17.13580
196/196 - 80s - loss: 17.0607 - MinusLogProbMetric: 17.0607 - val_loss: 17.3344 - val_MinusLogProbMetric: 17.3344 - lr: 1.6667e-04 - 80s/epoch - 411ms/step
Epoch 293/1000
2023-09-28 01:10:15.287 
Epoch 293/1000 
	 loss: 17.0403, MinusLogProbMetric: 17.0403, val_loss: 17.3380, val_MinusLogProbMetric: 17.3380

Epoch 293: val_loss did not improve from 17.13580
196/196 - 81s - loss: 17.0403 - MinusLogProbMetric: 17.0403 - val_loss: 17.3380 - val_MinusLogProbMetric: 17.3380 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 294/1000
2023-09-28 01:11:35.244 
Epoch 294/1000 
	 loss: 17.0291, MinusLogProbMetric: 17.0291, val_loss: 17.1716, val_MinusLogProbMetric: 17.1716

Epoch 294: val_loss did not improve from 17.13580
196/196 - 80s - loss: 17.0291 - MinusLogProbMetric: 17.0291 - val_loss: 17.1716 - val_MinusLogProbMetric: 17.1716 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 295/1000
2023-09-28 01:12:55.058 
Epoch 295/1000 
	 loss: 17.1032, MinusLogProbMetric: 17.1032, val_loss: 17.1756, val_MinusLogProbMetric: 17.1756

Epoch 295: val_loss did not improve from 17.13580
196/196 - 80s - loss: 17.1032 - MinusLogProbMetric: 17.1032 - val_loss: 17.1756 - val_MinusLogProbMetric: 17.1756 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 296/1000
2023-09-28 01:14:15.567 
Epoch 296/1000 
	 loss: 17.0284, MinusLogProbMetric: 17.0284, val_loss: 17.1171, val_MinusLogProbMetric: 17.1171

Epoch 296: val_loss improved from 17.13580 to 17.11715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 17.0284 - MinusLogProbMetric: 17.0284 - val_loss: 17.1171 - val_MinusLogProbMetric: 17.1171 - lr: 1.6667e-04 - 82s/epoch - 417ms/step
Epoch 297/1000
2023-09-28 01:15:36.916 
Epoch 297/1000 
	 loss: 17.0486, MinusLogProbMetric: 17.0486, val_loss: 17.9298, val_MinusLogProbMetric: 17.9298

Epoch 297: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0486 - MinusLogProbMetric: 17.0486 - val_loss: 17.9298 - val_MinusLogProbMetric: 17.9298 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 298/1000
2023-09-28 01:16:56.768 
Epoch 298/1000 
	 loss: 17.1022, MinusLogProbMetric: 17.1022, val_loss: 17.2755, val_MinusLogProbMetric: 17.2755

Epoch 298: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.1022 - MinusLogProbMetric: 17.1022 - val_loss: 17.2755 - val_MinusLogProbMetric: 17.2755 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 299/1000
2023-09-28 01:18:16.338 
Epoch 299/1000 
	 loss: 17.0867, MinusLogProbMetric: 17.0867, val_loss: 17.2413, val_MinusLogProbMetric: 17.2413

Epoch 299: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0867 - MinusLogProbMetric: 17.0867 - val_loss: 17.2413 - val_MinusLogProbMetric: 17.2413 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 300/1000
2023-09-28 01:19:35.921 
Epoch 300/1000 
	 loss: 17.0625, MinusLogProbMetric: 17.0625, val_loss: 17.2199, val_MinusLogProbMetric: 17.2199

Epoch 300: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0625 - MinusLogProbMetric: 17.0625 - val_loss: 17.2199 - val_MinusLogProbMetric: 17.2199 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 301/1000
2023-09-28 01:20:56.399 
Epoch 301/1000 
	 loss: 17.4244, MinusLogProbMetric: 17.4244, val_loss: 17.1190, val_MinusLogProbMetric: 17.1190

Epoch 301: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.4244 - MinusLogProbMetric: 17.4244 - val_loss: 17.1190 - val_MinusLogProbMetric: 17.1190 - lr: 1.6667e-04 - 80s/epoch - 411ms/step
Epoch 302/1000
2023-09-28 01:22:15.931 
Epoch 302/1000 
	 loss: 17.0483, MinusLogProbMetric: 17.0483, val_loss: 17.3931, val_MinusLogProbMetric: 17.3931

Epoch 302: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0483 - MinusLogProbMetric: 17.0483 - val_loss: 17.3931 - val_MinusLogProbMetric: 17.3931 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 303/1000
2023-09-28 01:23:35.669 
Epoch 303/1000 
	 loss: 17.0854, MinusLogProbMetric: 17.0854, val_loss: 17.1593, val_MinusLogProbMetric: 17.1593

Epoch 303: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0854 - MinusLogProbMetric: 17.0854 - val_loss: 17.1593 - val_MinusLogProbMetric: 17.1593 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 304/1000
2023-09-28 01:24:55.803 
Epoch 304/1000 
	 loss: 17.0123, MinusLogProbMetric: 17.0123, val_loss: 17.6297, val_MinusLogProbMetric: 17.6297

Epoch 304: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0123 - MinusLogProbMetric: 17.0123 - val_loss: 17.6297 - val_MinusLogProbMetric: 17.6297 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 305/1000
2023-09-28 01:26:16.058 
Epoch 305/1000 
	 loss: 17.0604, MinusLogProbMetric: 17.0604, val_loss: 17.4432, val_MinusLogProbMetric: 17.4432

Epoch 305: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0604 - MinusLogProbMetric: 17.0604 - val_loss: 17.4432 - val_MinusLogProbMetric: 17.4432 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 306/1000
2023-09-28 01:27:35.609 
Epoch 306/1000 
	 loss: 17.0581, MinusLogProbMetric: 17.0581, val_loss: 17.3494, val_MinusLogProbMetric: 17.3494

Epoch 306: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0581 - MinusLogProbMetric: 17.0581 - val_loss: 17.3494 - val_MinusLogProbMetric: 17.3494 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 307/1000
2023-09-28 01:28:55.215 
Epoch 307/1000 
	 loss: 17.0711, MinusLogProbMetric: 17.0711, val_loss: 17.3479, val_MinusLogProbMetric: 17.3479

Epoch 307: val_loss did not improve from 17.11715
196/196 - 80s - loss: 17.0711 - MinusLogProbMetric: 17.0711 - val_loss: 17.3479 - val_MinusLogProbMetric: 17.3479 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 308/1000
2023-09-28 01:30:15.006 
Epoch 308/1000 
	 loss: 17.0087, MinusLogProbMetric: 17.0087, val_loss: 17.0725, val_MinusLogProbMetric: 17.0725

Epoch 308: val_loss improved from 17.11715 to 17.07253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 17.0087 - MinusLogProbMetric: 17.0087 - val_loss: 17.0725 - val_MinusLogProbMetric: 17.0725 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 309/1000
2023-09-28 01:31:35.638 
Epoch 309/1000 
	 loss: 17.0536, MinusLogProbMetric: 17.0536, val_loss: 17.1419, val_MinusLogProbMetric: 17.1419

Epoch 309: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0536 - MinusLogProbMetric: 17.0536 - val_loss: 17.1419 - val_MinusLogProbMetric: 17.1419 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 310/1000
2023-09-28 01:32:55.376 
Epoch 310/1000 
	 loss: 17.0486, MinusLogProbMetric: 17.0486, val_loss: 17.3010, val_MinusLogProbMetric: 17.3010

Epoch 310: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0486 - MinusLogProbMetric: 17.0486 - val_loss: 17.3010 - val_MinusLogProbMetric: 17.3010 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 311/1000
2023-09-28 01:34:15.156 
Epoch 311/1000 
	 loss: 17.0394, MinusLogProbMetric: 17.0394, val_loss: 17.1870, val_MinusLogProbMetric: 17.1870

Epoch 311: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0394 - MinusLogProbMetric: 17.0394 - val_loss: 17.1870 - val_MinusLogProbMetric: 17.1870 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 312/1000
2023-09-28 01:35:34.611 
Epoch 312/1000 
	 loss: 17.0475, MinusLogProbMetric: 17.0475, val_loss: 17.2084, val_MinusLogProbMetric: 17.2084

Epoch 312: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0475 - MinusLogProbMetric: 17.0475 - val_loss: 17.2084 - val_MinusLogProbMetric: 17.2084 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 313/1000
2023-09-28 01:36:54.359 
Epoch 313/1000 
	 loss: 17.0419, MinusLogProbMetric: 17.0419, val_loss: 17.0742, val_MinusLogProbMetric: 17.0742

Epoch 313: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0419 - MinusLogProbMetric: 17.0419 - val_loss: 17.0742 - val_MinusLogProbMetric: 17.0742 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 314/1000
2023-09-28 01:38:14.617 
Epoch 314/1000 
	 loss: 17.0500, MinusLogProbMetric: 17.0500, val_loss: 17.1354, val_MinusLogProbMetric: 17.1354

Epoch 314: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0500 - MinusLogProbMetric: 17.0500 - val_loss: 17.1354 - val_MinusLogProbMetric: 17.1354 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 315/1000
2023-09-28 01:39:33.060 
Epoch 315/1000 
	 loss: 17.0642, MinusLogProbMetric: 17.0642, val_loss: 17.2845, val_MinusLogProbMetric: 17.2845

Epoch 315: val_loss did not improve from 17.07253
196/196 - 78s - loss: 17.0642 - MinusLogProbMetric: 17.0642 - val_loss: 17.2845 - val_MinusLogProbMetric: 17.2845 - lr: 1.6667e-04 - 78s/epoch - 400ms/step
Epoch 316/1000
2023-09-28 01:40:41.973 
Epoch 316/1000 
	 loss: 17.0395, MinusLogProbMetric: 17.0395, val_loss: 17.0746, val_MinusLogProbMetric: 17.0746

Epoch 316: val_loss did not improve from 17.07253
196/196 - 69s - loss: 17.0395 - MinusLogProbMetric: 17.0395 - val_loss: 17.0746 - val_MinusLogProbMetric: 17.0746 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 317/1000
2023-09-28 01:41:45.451 
Epoch 317/1000 
	 loss: 17.1039, MinusLogProbMetric: 17.1039, val_loss: 17.4754, val_MinusLogProbMetric: 17.4754

Epoch 317: val_loss did not improve from 17.07253
196/196 - 63s - loss: 17.1039 - MinusLogProbMetric: 17.1039 - val_loss: 17.4754 - val_MinusLogProbMetric: 17.4754 - lr: 1.6667e-04 - 63s/epoch - 324ms/step
Epoch 318/1000
2023-09-28 01:42:58.380 
Epoch 318/1000 
	 loss: 17.0583, MinusLogProbMetric: 17.0583, val_loss: 17.1955, val_MinusLogProbMetric: 17.1955

Epoch 318: val_loss did not improve from 17.07253
196/196 - 73s - loss: 17.0583 - MinusLogProbMetric: 17.0583 - val_loss: 17.1955 - val_MinusLogProbMetric: 17.1955 - lr: 1.6667e-04 - 73s/epoch - 372ms/step
Epoch 319/1000
2023-09-28 01:44:01.965 
Epoch 319/1000 
	 loss: 17.0619, MinusLogProbMetric: 17.0619, val_loss: 17.1862, val_MinusLogProbMetric: 17.1862

Epoch 319: val_loss did not improve from 17.07253
196/196 - 64s - loss: 17.0619 - MinusLogProbMetric: 17.0619 - val_loss: 17.1862 - val_MinusLogProbMetric: 17.1862 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 320/1000
2023-09-28 01:45:19.068 
Epoch 320/1000 
	 loss: 17.0400, MinusLogProbMetric: 17.0400, val_loss: 17.4277, val_MinusLogProbMetric: 17.4277

Epoch 320: val_loss did not improve from 17.07253
196/196 - 77s - loss: 17.0400 - MinusLogProbMetric: 17.0400 - val_loss: 17.4277 - val_MinusLogProbMetric: 17.4277 - lr: 1.6667e-04 - 77s/epoch - 393ms/step
Epoch 321/1000
2023-09-28 01:46:38.187 
Epoch 321/1000 
	 loss: 17.0309, MinusLogProbMetric: 17.0309, val_loss: 17.3018, val_MinusLogProbMetric: 17.3018

Epoch 321: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0309 - MinusLogProbMetric: 17.0309 - val_loss: 17.3018 - val_MinusLogProbMetric: 17.3018 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 322/1000
2023-09-28 01:47:57.373 
Epoch 322/1000 
	 loss: 17.0957, MinusLogProbMetric: 17.0957, val_loss: 17.3277, val_MinusLogProbMetric: 17.3277

Epoch 322: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0957 - MinusLogProbMetric: 17.0957 - val_loss: 17.3277 - val_MinusLogProbMetric: 17.3277 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 323/1000
2023-09-28 01:49:16.636 
Epoch 323/1000 
	 loss: 17.0581, MinusLogProbMetric: 17.0581, val_loss: 17.3397, val_MinusLogProbMetric: 17.3397

Epoch 323: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0581 - MinusLogProbMetric: 17.0581 - val_loss: 17.3397 - val_MinusLogProbMetric: 17.3397 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 324/1000
2023-09-28 01:50:36.587 
Epoch 324/1000 
	 loss: 17.0440, MinusLogProbMetric: 17.0440, val_loss: 17.2707, val_MinusLogProbMetric: 17.2707

Epoch 324: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0440 - MinusLogProbMetric: 17.0440 - val_loss: 17.2707 - val_MinusLogProbMetric: 17.2707 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 325/1000
2023-09-28 01:51:56.919 
Epoch 325/1000 
	 loss: 17.1222, MinusLogProbMetric: 17.1222, val_loss: 17.2018, val_MinusLogProbMetric: 17.2018

Epoch 325: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.1222 - MinusLogProbMetric: 17.1222 - val_loss: 17.2018 - val_MinusLogProbMetric: 17.2018 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 326/1000
2023-09-28 01:53:16.108 
Epoch 326/1000 
	 loss: 17.0247, MinusLogProbMetric: 17.0247, val_loss: 17.1168, val_MinusLogProbMetric: 17.1168

Epoch 326: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0247 - MinusLogProbMetric: 17.0247 - val_loss: 17.1168 - val_MinusLogProbMetric: 17.1168 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 327/1000
2023-09-28 01:54:34.980 
Epoch 327/1000 
	 loss: 17.0496, MinusLogProbMetric: 17.0496, val_loss: 17.2918, val_MinusLogProbMetric: 17.2918

Epoch 327: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0496 - MinusLogProbMetric: 17.0496 - val_loss: 17.2918 - val_MinusLogProbMetric: 17.2918 - lr: 1.6667e-04 - 79s/epoch - 402ms/step
Epoch 328/1000
2023-09-28 01:55:54.823 
Epoch 328/1000 
	 loss: 17.0607, MinusLogProbMetric: 17.0607, val_loss: 17.4498, val_MinusLogProbMetric: 17.4498

Epoch 328: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0607 - MinusLogProbMetric: 17.0607 - val_loss: 17.4498 - val_MinusLogProbMetric: 17.4498 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 329/1000
2023-09-28 01:57:14.818 
Epoch 329/1000 
	 loss: 17.0417, MinusLogProbMetric: 17.0417, val_loss: 17.0749, val_MinusLogProbMetric: 17.0749

Epoch 329: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0417 - MinusLogProbMetric: 17.0417 - val_loss: 17.0749 - val_MinusLogProbMetric: 17.0749 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 330/1000
2023-09-28 01:58:34.893 
Epoch 330/1000 
	 loss: 17.0200, MinusLogProbMetric: 17.0200, val_loss: 17.1267, val_MinusLogProbMetric: 17.1267

Epoch 330: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0200 - MinusLogProbMetric: 17.0200 - val_loss: 17.1267 - val_MinusLogProbMetric: 17.1267 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 331/1000
2023-09-28 01:59:54.693 
Epoch 331/1000 
	 loss: 17.0204, MinusLogProbMetric: 17.0204, val_loss: 17.1190, val_MinusLogProbMetric: 17.1190

Epoch 331: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0204 - MinusLogProbMetric: 17.0204 - val_loss: 17.1190 - val_MinusLogProbMetric: 17.1190 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 332/1000
2023-09-28 02:01:15.222 
Epoch 332/1000 
	 loss: 17.0162, MinusLogProbMetric: 17.0162, val_loss: 17.3027, val_MinusLogProbMetric: 17.3027

Epoch 332: val_loss did not improve from 17.07253
196/196 - 81s - loss: 17.0162 - MinusLogProbMetric: 17.0162 - val_loss: 17.3027 - val_MinusLogProbMetric: 17.3027 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 333/1000
2023-09-28 02:02:35.307 
Epoch 333/1000 
	 loss: 17.0758, MinusLogProbMetric: 17.0758, val_loss: 17.3548, val_MinusLogProbMetric: 17.3548

Epoch 333: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0758 - MinusLogProbMetric: 17.0758 - val_loss: 17.3548 - val_MinusLogProbMetric: 17.3548 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 334/1000
2023-09-28 02:03:55.330 
Epoch 334/1000 
	 loss: 17.0159, MinusLogProbMetric: 17.0159, val_loss: 17.1809, val_MinusLogProbMetric: 17.1809

Epoch 334: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0159 - MinusLogProbMetric: 17.0159 - val_loss: 17.1809 - val_MinusLogProbMetric: 17.1809 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 335/1000
2023-09-28 02:05:15.381 
Epoch 335/1000 
	 loss: 17.0636, MinusLogProbMetric: 17.0636, val_loss: 17.2238, val_MinusLogProbMetric: 17.2238

Epoch 335: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0636 - MinusLogProbMetric: 17.0636 - val_loss: 17.2238 - val_MinusLogProbMetric: 17.2238 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 336/1000
2023-09-28 02:06:35.233 
Epoch 336/1000 
	 loss: 17.0421, MinusLogProbMetric: 17.0421, val_loss: 17.1205, val_MinusLogProbMetric: 17.1205

Epoch 336: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0421 - MinusLogProbMetric: 17.0421 - val_loss: 17.1205 - val_MinusLogProbMetric: 17.1205 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 337/1000
2023-09-28 02:07:55.277 
Epoch 337/1000 
	 loss: 17.0392, MinusLogProbMetric: 17.0392, val_loss: 17.0797, val_MinusLogProbMetric: 17.0797

Epoch 337: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0392 - MinusLogProbMetric: 17.0392 - val_loss: 17.0797 - val_MinusLogProbMetric: 17.0797 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 338/1000
2023-09-28 02:09:15.123 
Epoch 338/1000 
	 loss: 17.0485, MinusLogProbMetric: 17.0485, val_loss: 18.6889, val_MinusLogProbMetric: 18.6889

Epoch 338: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0485 - MinusLogProbMetric: 17.0485 - val_loss: 18.6889 - val_MinusLogProbMetric: 18.6889 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 339/1000
2023-09-28 02:10:34.884 
Epoch 339/1000 
	 loss: 17.1242, MinusLogProbMetric: 17.1242, val_loss: 17.2233, val_MinusLogProbMetric: 17.2233

Epoch 339: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.1242 - MinusLogProbMetric: 17.1242 - val_loss: 17.2233 - val_MinusLogProbMetric: 17.2233 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 340/1000
2023-09-28 02:11:55.400 
Epoch 340/1000 
	 loss: 16.9892, MinusLogProbMetric: 16.9892, val_loss: 17.1249, val_MinusLogProbMetric: 17.1249

Epoch 340: val_loss did not improve from 17.07253
196/196 - 81s - loss: 16.9892 - MinusLogProbMetric: 16.9892 - val_loss: 17.1249 - val_MinusLogProbMetric: 17.1249 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 341/1000
2023-09-28 02:13:15.571 
Epoch 341/1000 
	 loss: 16.9927, MinusLogProbMetric: 16.9927, val_loss: 17.2768, val_MinusLogProbMetric: 17.2768

Epoch 341: val_loss did not improve from 17.07253
196/196 - 80s - loss: 16.9927 - MinusLogProbMetric: 16.9927 - val_loss: 17.2768 - val_MinusLogProbMetric: 17.2768 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 342/1000
2023-09-28 02:14:36.230 
Epoch 342/1000 
	 loss: 17.0275, MinusLogProbMetric: 17.0275, val_loss: 17.2160, val_MinusLogProbMetric: 17.2160

Epoch 342: val_loss did not improve from 17.07253
196/196 - 81s - loss: 17.0275 - MinusLogProbMetric: 17.0275 - val_loss: 17.2160 - val_MinusLogProbMetric: 17.2160 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 343/1000
2023-09-28 02:15:56.320 
Epoch 343/1000 
	 loss: 17.0436, MinusLogProbMetric: 17.0436, val_loss: 17.2435, val_MinusLogProbMetric: 17.2435

Epoch 343: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0436 - MinusLogProbMetric: 17.0436 - val_loss: 17.2435 - val_MinusLogProbMetric: 17.2435 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 344/1000
2023-09-28 02:17:16.222 
Epoch 344/1000 
	 loss: 17.0288, MinusLogProbMetric: 17.0288, val_loss: 17.1629, val_MinusLogProbMetric: 17.1629

Epoch 344: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0288 - MinusLogProbMetric: 17.0288 - val_loss: 17.1629 - val_MinusLogProbMetric: 17.1629 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 345/1000
2023-09-28 02:18:36.659 
Epoch 345/1000 
	 loss: 17.0272, MinusLogProbMetric: 17.0272, val_loss: 17.1050, val_MinusLogProbMetric: 17.1050

Epoch 345: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0272 - MinusLogProbMetric: 17.0272 - val_loss: 17.1050 - val_MinusLogProbMetric: 17.1050 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 346/1000
2023-09-28 02:19:55.671 
Epoch 346/1000 
	 loss: 16.9993, MinusLogProbMetric: 16.9993, val_loss: 17.1634, val_MinusLogProbMetric: 17.1634

Epoch 346: val_loss did not improve from 17.07253
196/196 - 79s - loss: 16.9993 - MinusLogProbMetric: 16.9993 - val_loss: 17.1634 - val_MinusLogProbMetric: 17.1634 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 347/1000
2023-09-28 02:21:15.765 
Epoch 347/1000 
	 loss: 17.0648, MinusLogProbMetric: 17.0648, val_loss: 17.3802, val_MinusLogProbMetric: 17.3802

Epoch 347: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0648 - MinusLogProbMetric: 17.0648 - val_loss: 17.3802 - val_MinusLogProbMetric: 17.3802 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 348/1000
2023-09-28 02:22:35.479 
Epoch 348/1000 
	 loss: 17.0195, MinusLogProbMetric: 17.0195, val_loss: 17.1084, val_MinusLogProbMetric: 17.1084

Epoch 348: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0195 - MinusLogProbMetric: 17.0195 - val_loss: 17.1084 - val_MinusLogProbMetric: 17.1084 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 349/1000
2023-09-28 02:23:55.615 
Epoch 349/1000 
	 loss: 17.0192, MinusLogProbMetric: 17.0192, val_loss: 17.2132, val_MinusLogProbMetric: 17.2132

Epoch 349: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0192 - MinusLogProbMetric: 17.0192 - val_loss: 17.2132 - val_MinusLogProbMetric: 17.2132 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 350/1000
2023-09-28 02:25:15.519 
Epoch 350/1000 
	 loss: 17.0142, MinusLogProbMetric: 17.0142, val_loss: 17.2008, val_MinusLogProbMetric: 17.2008

Epoch 350: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0142 - MinusLogProbMetric: 17.0142 - val_loss: 17.2008 - val_MinusLogProbMetric: 17.2008 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 351/1000
2023-09-28 02:26:35.527 
Epoch 351/1000 
	 loss: 17.0192, MinusLogProbMetric: 17.0192, val_loss: 17.1061, val_MinusLogProbMetric: 17.1061

Epoch 351: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0192 - MinusLogProbMetric: 17.0192 - val_loss: 17.1061 - val_MinusLogProbMetric: 17.1061 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 352/1000
2023-09-28 02:27:55.121 
Epoch 352/1000 
	 loss: 17.0481, MinusLogProbMetric: 17.0481, val_loss: 17.2301, val_MinusLogProbMetric: 17.2301

Epoch 352: val_loss did not improve from 17.07253
196/196 - 80s - loss: 17.0481 - MinusLogProbMetric: 17.0481 - val_loss: 17.2301 - val_MinusLogProbMetric: 17.2301 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 353/1000
2023-09-28 02:29:14.477 
Epoch 353/1000 
	 loss: 17.0098, MinusLogProbMetric: 17.0098, val_loss: 17.1488, val_MinusLogProbMetric: 17.1488

Epoch 353: val_loss did not improve from 17.07253
196/196 - 79s - loss: 17.0098 - MinusLogProbMetric: 17.0098 - val_loss: 17.1488 - val_MinusLogProbMetric: 17.1488 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 354/1000
2023-09-28 02:30:33.368 
Epoch 354/1000 
	 loss: 16.9860, MinusLogProbMetric: 16.9860, val_loss: 17.0485, val_MinusLogProbMetric: 17.0485

Epoch 354: val_loss improved from 17.07253 to 17.04847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 80s - loss: 16.9860 - MinusLogProbMetric: 16.9860 - val_loss: 17.0485 - val_MinusLogProbMetric: 17.0485 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 355/1000
2023-09-28 02:31:53.947 
Epoch 355/1000 
	 loss: 17.0278, MinusLogProbMetric: 17.0278, val_loss: 17.4630, val_MinusLogProbMetric: 17.4630

Epoch 355: val_loss did not improve from 17.04847
196/196 - 79s - loss: 17.0278 - MinusLogProbMetric: 17.0278 - val_loss: 17.4630 - val_MinusLogProbMetric: 17.4630 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 356/1000
2023-09-28 02:33:13.858 
Epoch 356/1000 
	 loss: 17.0531, MinusLogProbMetric: 17.0531, val_loss: 17.3973, val_MinusLogProbMetric: 17.3973

Epoch 356: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0531 - MinusLogProbMetric: 17.0531 - val_loss: 17.3973 - val_MinusLogProbMetric: 17.3973 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 357/1000
2023-09-28 02:34:33.441 
Epoch 357/1000 
	 loss: 16.9953, MinusLogProbMetric: 16.9953, val_loss: 17.1940, val_MinusLogProbMetric: 17.1940

Epoch 357: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9953 - MinusLogProbMetric: 16.9953 - val_loss: 17.1940 - val_MinusLogProbMetric: 17.1940 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 358/1000
2023-09-28 02:35:53.203 
Epoch 358/1000 
	 loss: 16.9988, MinusLogProbMetric: 16.9988, val_loss: 17.2193, val_MinusLogProbMetric: 17.2193

Epoch 358: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9988 - MinusLogProbMetric: 16.9988 - val_loss: 17.2193 - val_MinusLogProbMetric: 17.2193 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 359/1000
2023-09-28 02:37:12.863 
Epoch 359/1000 
	 loss: 17.0581, MinusLogProbMetric: 17.0581, val_loss: 17.2074, val_MinusLogProbMetric: 17.2074

Epoch 359: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0581 - MinusLogProbMetric: 17.0581 - val_loss: 17.2074 - val_MinusLogProbMetric: 17.2074 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 360/1000
2023-09-28 02:38:33.202 
Epoch 360/1000 
	 loss: 16.9805, MinusLogProbMetric: 16.9805, val_loss: 17.1012, val_MinusLogProbMetric: 17.1012

Epoch 360: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9805 - MinusLogProbMetric: 16.9805 - val_loss: 17.1012 - val_MinusLogProbMetric: 17.1012 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 361/1000
2023-09-28 02:39:52.768 
Epoch 361/1000 
	 loss: 17.0072, MinusLogProbMetric: 17.0072, val_loss: 17.1867, val_MinusLogProbMetric: 17.1867

Epoch 361: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0072 - MinusLogProbMetric: 17.0072 - val_loss: 17.1867 - val_MinusLogProbMetric: 17.1867 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 362/1000
2023-09-28 02:41:12.472 
Epoch 362/1000 
	 loss: 17.0294, MinusLogProbMetric: 17.0294, val_loss: 17.2120, val_MinusLogProbMetric: 17.2120

Epoch 362: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0294 - MinusLogProbMetric: 17.0294 - val_loss: 17.2120 - val_MinusLogProbMetric: 17.2120 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 363/1000
2023-09-28 02:42:32.725 
Epoch 363/1000 
	 loss: 17.0235, MinusLogProbMetric: 17.0235, val_loss: 17.3674, val_MinusLogProbMetric: 17.3674

Epoch 363: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0235 - MinusLogProbMetric: 17.0235 - val_loss: 17.3674 - val_MinusLogProbMetric: 17.3674 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 364/1000
2023-09-28 02:43:52.838 
Epoch 364/1000 
	 loss: 17.0799, MinusLogProbMetric: 17.0799, val_loss: 17.1625, val_MinusLogProbMetric: 17.1625

Epoch 364: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0799 - MinusLogProbMetric: 17.0799 - val_loss: 17.1625 - val_MinusLogProbMetric: 17.1625 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 365/1000
2023-09-28 02:45:12.276 
Epoch 365/1000 
	 loss: 16.9958, MinusLogProbMetric: 16.9958, val_loss: 17.1614, val_MinusLogProbMetric: 17.1614

Epoch 365: val_loss did not improve from 17.04847
196/196 - 79s - loss: 16.9958 - MinusLogProbMetric: 16.9958 - val_loss: 17.1614 - val_MinusLogProbMetric: 17.1614 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 366/1000
2023-09-28 02:46:31.753 
Epoch 366/1000 
	 loss: 17.0406, MinusLogProbMetric: 17.0406, val_loss: 17.2418, val_MinusLogProbMetric: 17.2418

Epoch 366: val_loss did not improve from 17.04847
196/196 - 79s - loss: 17.0406 - MinusLogProbMetric: 17.0406 - val_loss: 17.2418 - val_MinusLogProbMetric: 17.2418 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 367/1000
2023-09-28 02:47:51.902 
Epoch 367/1000 
	 loss: 17.0379, MinusLogProbMetric: 17.0379, val_loss: 17.0976, val_MinusLogProbMetric: 17.0976

Epoch 367: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0379 - MinusLogProbMetric: 17.0379 - val_loss: 17.0976 - val_MinusLogProbMetric: 17.0976 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 368/1000
2023-09-28 02:49:12.236 
Epoch 368/1000 
	 loss: 17.0629, MinusLogProbMetric: 17.0629, val_loss: 17.2398, val_MinusLogProbMetric: 17.2398

Epoch 368: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0629 - MinusLogProbMetric: 17.0629 - val_loss: 17.2398 - val_MinusLogProbMetric: 17.2398 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 369/1000
2023-09-28 02:50:32.752 
Epoch 369/1000 
	 loss: 16.9809, MinusLogProbMetric: 16.9809, val_loss: 17.3515, val_MinusLogProbMetric: 17.3515

Epoch 369: val_loss did not improve from 17.04847
196/196 - 81s - loss: 16.9809 - MinusLogProbMetric: 16.9809 - val_loss: 17.3515 - val_MinusLogProbMetric: 17.3515 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 370/1000
2023-09-28 02:51:52.377 
Epoch 370/1000 
	 loss: 17.0220, MinusLogProbMetric: 17.0220, val_loss: 17.2088, val_MinusLogProbMetric: 17.2088

Epoch 370: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0220 - MinusLogProbMetric: 17.0220 - val_loss: 17.2088 - val_MinusLogProbMetric: 17.2088 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 371/1000
2023-09-28 02:53:11.818 
Epoch 371/1000 
	 loss: 17.0471, MinusLogProbMetric: 17.0471, val_loss: 17.1842, val_MinusLogProbMetric: 17.1842

Epoch 371: val_loss did not improve from 17.04847
196/196 - 79s - loss: 17.0471 - MinusLogProbMetric: 17.0471 - val_loss: 17.1842 - val_MinusLogProbMetric: 17.1842 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 372/1000
2023-09-28 02:54:32.444 
Epoch 372/1000 
	 loss: 16.9843, MinusLogProbMetric: 16.9843, val_loss: 17.1793, val_MinusLogProbMetric: 17.1793

Epoch 372: val_loss did not improve from 17.04847
196/196 - 81s - loss: 16.9843 - MinusLogProbMetric: 16.9843 - val_loss: 17.1793 - val_MinusLogProbMetric: 17.1793 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 373/1000
2023-09-28 02:55:52.123 
Epoch 373/1000 
	 loss: 17.0963, MinusLogProbMetric: 17.0963, val_loss: 17.1417, val_MinusLogProbMetric: 17.1417

Epoch 373: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0963 - MinusLogProbMetric: 17.0963 - val_loss: 17.1417 - val_MinusLogProbMetric: 17.1417 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 374/1000
2023-09-28 02:57:11.752 
Epoch 374/1000 
	 loss: 16.9669, MinusLogProbMetric: 16.9669, val_loss: 17.2898, val_MinusLogProbMetric: 17.2898

Epoch 374: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9669 - MinusLogProbMetric: 16.9669 - val_loss: 17.2898 - val_MinusLogProbMetric: 17.2898 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 375/1000
2023-09-28 02:58:32.105 
Epoch 375/1000 
	 loss: 17.0010, MinusLogProbMetric: 17.0010, val_loss: 17.1241, val_MinusLogProbMetric: 17.1241

Epoch 375: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0010 - MinusLogProbMetric: 17.0010 - val_loss: 17.1241 - val_MinusLogProbMetric: 17.1241 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 376/1000
2023-09-28 02:59:52.692 
Epoch 376/1000 
	 loss: 16.9773, MinusLogProbMetric: 16.9773, val_loss: 17.2000, val_MinusLogProbMetric: 17.2000

Epoch 376: val_loss did not improve from 17.04847
196/196 - 81s - loss: 16.9773 - MinusLogProbMetric: 16.9773 - val_loss: 17.2000 - val_MinusLogProbMetric: 17.2000 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 377/1000
2023-09-28 03:01:12.815 
Epoch 377/1000 
	 loss: 16.9994, MinusLogProbMetric: 16.9994, val_loss: 17.2077, val_MinusLogProbMetric: 17.2077

Epoch 377: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9994 - MinusLogProbMetric: 16.9994 - val_loss: 17.2077 - val_MinusLogProbMetric: 17.2077 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 378/1000
2023-09-28 03:02:33.175 
Epoch 378/1000 
	 loss: 17.0140, MinusLogProbMetric: 17.0140, val_loss: 17.3232, val_MinusLogProbMetric: 17.3232

Epoch 378: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0140 - MinusLogProbMetric: 17.0140 - val_loss: 17.3232 - val_MinusLogProbMetric: 17.3232 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 379/1000
2023-09-28 03:03:53.103 
Epoch 379/1000 
	 loss: 17.0161, MinusLogProbMetric: 17.0161, val_loss: 17.2836, val_MinusLogProbMetric: 17.2836

Epoch 379: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0161 - MinusLogProbMetric: 17.0161 - val_loss: 17.2836 - val_MinusLogProbMetric: 17.2836 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 380/1000
2023-09-28 03:05:12.667 
Epoch 380/1000 
	 loss: 17.0503, MinusLogProbMetric: 17.0503, val_loss: 17.1236, val_MinusLogProbMetric: 17.1236

Epoch 380: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0503 - MinusLogProbMetric: 17.0503 - val_loss: 17.1236 - val_MinusLogProbMetric: 17.1236 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 381/1000
2023-09-28 03:06:32.449 
Epoch 381/1000 
	 loss: 16.9733, MinusLogProbMetric: 16.9733, val_loss: 17.4141, val_MinusLogProbMetric: 17.4141

Epoch 381: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9733 - MinusLogProbMetric: 16.9733 - val_loss: 17.4141 - val_MinusLogProbMetric: 17.4141 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 382/1000
2023-09-28 03:07:52.473 
Epoch 382/1000 
	 loss: 17.0274, MinusLogProbMetric: 17.0274, val_loss: 17.2038, val_MinusLogProbMetric: 17.2038

Epoch 382: val_loss did not improve from 17.04847
196/196 - 80s - loss: 17.0274 - MinusLogProbMetric: 17.0274 - val_loss: 17.2038 - val_MinusLogProbMetric: 17.2038 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 383/1000
2023-09-28 03:09:12.943 
Epoch 383/1000 
	 loss: 16.9698, MinusLogProbMetric: 16.9698, val_loss: 17.3029, val_MinusLogProbMetric: 17.3029

Epoch 383: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9698 - MinusLogProbMetric: 16.9698 - val_loss: 17.3029 - val_MinusLogProbMetric: 17.3029 - lr: 1.6667e-04 - 80s/epoch - 411ms/step
Epoch 384/1000
2023-09-28 03:10:32.648 
Epoch 384/1000 
	 loss: 16.9976, MinusLogProbMetric: 16.9976, val_loss: 17.1470, val_MinusLogProbMetric: 17.1470

Epoch 384: val_loss did not improve from 17.04847
196/196 - 80s - loss: 16.9976 - MinusLogProbMetric: 16.9976 - val_loss: 17.1470 - val_MinusLogProbMetric: 17.1470 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 385/1000
2023-09-28 03:11:52.802 
Epoch 385/1000 
	 loss: 17.0297, MinusLogProbMetric: 17.0297, val_loss: 17.0443, val_MinusLogProbMetric: 17.0443

Epoch 385: val_loss improved from 17.04847 to 17.04426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 17.0297 - MinusLogProbMetric: 17.0297 - val_loss: 17.0443 - val_MinusLogProbMetric: 17.0443 - lr: 1.6667e-04 - 81s/epoch - 415ms/step
Epoch 386/1000
2023-09-28 03:13:14.143 
Epoch 386/1000 
	 loss: 16.9945, MinusLogProbMetric: 16.9945, val_loss: 17.1073, val_MinusLogProbMetric: 17.1073

Epoch 386: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9945 - MinusLogProbMetric: 16.9945 - val_loss: 17.1073 - val_MinusLogProbMetric: 17.1073 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 387/1000
2023-09-28 03:14:34.058 
Epoch 387/1000 
	 loss: 16.9777, MinusLogProbMetric: 16.9777, val_loss: 17.1330, val_MinusLogProbMetric: 17.1330

Epoch 387: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9777 - MinusLogProbMetric: 16.9777 - val_loss: 17.1330 - val_MinusLogProbMetric: 17.1330 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 388/1000
2023-09-28 03:15:54.218 
Epoch 388/1000 
	 loss: 16.9833, MinusLogProbMetric: 16.9833, val_loss: 17.1351, val_MinusLogProbMetric: 17.1351

Epoch 388: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9833 - MinusLogProbMetric: 16.9833 - val_loss: 17.1351 - val_MinusLogProbMetric: 17.1351 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 389/1000
2023-09-28 03:17:13.992 
Epoch 389/1000 
	 loss: 17.0128, MinusLogProbMetric: 17.0128, val_loss: 17.1342, val_MinusLogProbMetric: 17.1342

Epoch 389: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0128 - MinusLogProbMetric: 17.0128 - val_loss: 17.1342 - val_MinusLogProbMetric: 17.1342 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 390/1000
2023-09-28 03:18:34.246 
Epoch 390/1000 
	 loss: 17.1280, MinusLogProbMetric: 17.1280, val_loss: 17.3542, val_MinusLogProbMetric: 17.3542

Epoch 390: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.1280 - MinusLogProbMetric: 17.1280 - val_loss: 17.3542 - val_MinusLogProbMetric: 17.3542 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 391/1000
2023-09-28 03:19:55.047 
Epoch 391/1000 
	 loss: 17.0022, MinusLogProbMetric: 17.0022, val_loss: 17.1196, val_MinusLogProbMetric: 17.1196

Epoch 391: val_loss did not improve from 17.04426
196/196 - 81s - loss: 17.0022 - MinusLogProbMetric: 17.0022 - val_loss: 17.1196 - val_MinusLogProbMetric: 17.1196 - lr: 1.6667e-04 - 81s/epoch - 412ms/step
Epoch 392/1000
2023-09-28 03:21:15.589 
Epoch 392/1000 
	 loss: 16.9749, MinusLogProbMetric: 16.9749, val_loss: 17.1852, val_MinusLogProbMetric: 17.1852

Epoch 392: val_loss did not improve from 17.04426
196/196 - 81s - loss: 16.9749 - MinusLogProbMetric: 16.9749 - val_loss: 17.1852 - val_MinusLogProbMetric: 17.1852 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 393/1000
2023-09-28 03:22:35.616 
Epoch 393/1000 
	 loss: 16.9970, MinusLogProbMetric: 16.9970, val_loss: 17.1268, val_MinusLogProbMetric: 17.1268

Epoch 393: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9970 - MinusLogProbMetric: 16.9970 - val_loss: 17.1268 - val_MinusLogProbMetric: 17.1268 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 394/1000
2023-09-28 03:23:55.889 
Epoch 394/1000 
	 loss: 16.9600, MinusLogProbMetric: 16.9600, val_loss: 17.0597, val_MinusLogProbMetric: 17.0597

Epoch 394: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9600 - MinusLogProbMetric: 16.9600 - val_loss: 17.0597 - val_MinusLogProbMetric: 17.0597 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 395/1000
2023-09-28 03:25:16.072 
Epoch 395/1000 
	 loss: 17.0318, MinusLogProbMetric: 17.0318, val_loss: 17.2842, val_MinusLogProbMetric: 17.2842

Epoch 395: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0318 - MinusLogProbMetric: 17.0318 - val_loss: 17.2842 - val_MinusLogProbMetric: 17.2842 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 396/1000
2023-09-28 03:26:36.427 
Epoch 396/1000 
	 loss: 17.0074, MinusLogProbMetric: 17.0074, val_loss: 17.0486, val_MinusLogProbMetric: 17.0486

Epoch 396: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0074 - MinusLogProbMetric: 17.0074 - val_loss: 17.0486 - val_MinusLogProbMetric: 17.0486 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 397/1000
2023-09-28 03:27:55.650 
Epoch 397/1000 
	 loss: 16.9781, MinusLogProbMetric: 16.9781, val_loss: 17.2547, val_MinusLogProbMetric: 17.2547

Epoch 397: val_loss did not improve from 17.04426
196/196 - 79s - loss: 16.9781 - MinusLogProbMetric: 16.9781 - val_loss: 17.2547 - val_MinusLogProbMetric: 17.2547 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 398/1000
2023-09-28 03:29:15.810 
Epoch 398/1000 
	 loss: 17.0110, MinusLogProbMetric: 17.0110, val_loss: 17.1268, val_MinusLogProbMetric: 17.1268

Epoch 398: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0110 - MinusLogProbMetric: 17.0110 - val_loss: 17.1268 - val_MinusLogProbMetric: 17.1268 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 399/1000
2023-09-28 03:30:35.655 
Epoch 399/1000 
	 loss: 16.9842, MinusLogProbMetric: 16.9842, val_loss: 17.1709, val_MinusLogProbMetric: 17.1709

Epoch 399: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9842 - MinusLogProbMetric: 16.9842 - val_loss: 17.1709 - val_MinusLogProbMetric: 17.1709 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 400/1000
2023-09-28 03:31:55.915 
Epoch 400/1000 
	 loss: 16.9668, MinusLogProbMetric: 16.9668, val_loss: 17.0957, val_MinusLogProbMetric: 17.0957

Epoch 400: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9668 - MinusLogProbMetric: 16.9668 - val_loss: 17.0957 - val_MinusLogProbMetric: 17.0957 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 401/1000
2023-09-28 03:33:15.540 
Epoch 401/1000 
	 loss: 17.0022, MinusLogProbMetric: 17.0022, val_loss: 17.2201, val_MinusLogProbMetric: 17.2201

Epoch 401: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0022 - MinusLogProbMetric: 17.0022 - val_loss: 17.2201 - val_MinusLogProbMetric: 17.2201 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 402/1000
2023-09-28 03:34:35.878 
Epoch 402/1000 
	 loss: 17.0076, MinusLogProbMetric: 17.0076, val_loss: 17.1451, val_MinusLogProbMetric: 17.1451

Epoch 402: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0076 - MinusLogProbMetric: 17.0076 - val_loss: 17.1451 - val_MinusLogProbMetric: 17.1451 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 403/1000
2023-09-28 03:35:55.303 
Epoch 403/1000 
	 loss: 16.9622, MinusLogProbMetric: 16.9622, val_loss: 17.1576, val_MinusLogProbMetric: 17.1576

Epoch 403: val_loss did not improve from 17.04426
196/196 - 79s - loss: 16.9622 - MinusLogProbMetric: 16.9622 - val_loss: 17.1576 - val_MinusLogProbMetric: 17.1576 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 404/1000
2023-09-28 03:37:14.999 
Epoch 404/1000 
	 loss: 17.0011, MinusLogProbMetric: 17.0011, val_loss: 17.1923, val_MinusLogProbMetric: 17.1923

Epoch 404: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0011 - MinusLogProbMetric: 17.0011 - val_loss: 17.1923 - val_MinusLogProbMetric: 17.1923 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 405/1000
2023-09-28 03:38:35.180 
Epoch 405/1000 
	 loss: 16.9693, MinusLogProbMetric: 16.9693, val_loss: 17.1110, val_MinusLogProbMetric: 17.1110

Epoch 405: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9693 - MinusLogProbMetric: 16.9693 - val_loss: 17.1110 - val_MinusLogProbMetric: 17.1110 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 406/1000
2023-09-28 03:39:55.128 
Epoch 406/1000 
	 loss: 17.0097, MinusLogProbMetric: 17.0097, val_loss: 17.3081, val_MinusLogProbMetric: 17.3081

Epoch 406: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0097 - MinusLogProbMetric: 17.0097 - val_loss: 17.3081 - val_MinusLogProbMetric: 17.3081 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 407/1000
2023-09-28 03:41:15.043 
Epoch 407/1000 
	 loss: 17.0258, MinusLogProbMetric: 17.0258, val_loss: 17.0835, val_MinusLogProbMetric: 17.0835

Epoch 407: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0258 - MinusLogProbMetric: 17.0258 - val_loss: 17.0835 - val_MinusLogProbMetric: 17.0835 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 408/1000
2023-09-28 03:42:33.995 
Epoch 408/1000 
	 loss: 17.0163, MinusLogProbMetric: 17.0163, val_loss: 17.1651, val_MinusLogProbMetric: 17.1651

Epoch 408: val_loss did not improve from 17.04426
196/196 - 79s - loss: 17.0163 - MinusLogProbMetric: 17.0163 - val_loss: 17.1651 - val_MinusLogProbMetric: 17.1651 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 409/1000
2023-09-28 03:43:53.680 
Epoch 409/1000 
	 loss: 16.9708, MinusLogProbMetric: 16.9708, val_loss: 17.1122, val_MinusLogProbMetric: 17.1122

Epoch 409: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9708 - MinusLogProbMetric: 16.9708 - val_loss: 17.1122 - val_MinusLogProbMetric: 17.1122 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 410/1000
2023-09-28 03:45:13.846 
Epoch 410/1000 
	 loss: 16.9854, MinusLogProbMetric: 16.9854, val_loss: 17.2059, val_MinusLogProbMetric: 17.2059

Epoch 410: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9854 - MinusLogProbMetric: 16.9854 - val_loss: 17.2059 - val_MinusLogProbMetric: 17.2059 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 411/1000
2023-09-28 03:46:34.712 
Epoch 411/1000 
	 loss: 16.9862, MinusLogProbMetric: 16.9862, val_loss: 17.2410, val_MinusLogProbMetric: 17.2410

Epoch 411: val_loss did not improve from 17.04426
196/196 - 81s - loss: 16.9862 - MinusLogProbMetric: 16.9862 - val_loss: 17.2410 - val_MinusLogProbMetric: 17.2410 - lr: 1.6667e-04 - 81s/epoch - 413ms/step
Epoch 412/1000
2023-09-28 03:47:54.573 
Epoch 412/1000 
	 loss: 16.9781, MinusLogProbMetric: 16.9781, val_loss: 17.2712, val_MinusLogProbMetric: 17.2712

Epoch 412: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9781 - MinusLogProbMetric: 16.9781 - val_loss: 17.2712 - val_MinusLogProbMetric: 17.2712 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 413/1000
2023-09-28 03:49:14.545 
Epoch 413/1000 
	 loss: 16.9814, MinusLogProbMetric: 16.9814, val_loss: 17.1063, val_MinusLogProbMetric: 17.1063

Epoch 413: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9814 - MinusLogProbMetric: 16.9814 - val_loss: 17.1063 - val_MinusLogProbMetric: 17.1063 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 414/1000
2023-09-28 03:50:34.981 
Epoch 414/1000 
	 loss: 16.9579, MinusLogProbMetric: 16.9579, val_loss: 17.2270, val_MinusLogProbMetric: 17.2270

Epoch 414: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9579 - MinusLogProbMetric: 16.9579 - val_loss: 17.2270 - val_MinusLogProbMetric: 17.2270 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 415/1000
2023-09-28 03:51:54.924 
Epoch 415/1000 
	 loss: 16.9788, MinusLogProbMetric: 16.9788, val_loss: 17.2199, val_MinusLogProbMetric: 17.2199

Epoch 415: val_loss did not improve from 17.04426
196/196 - 80s - loss: 16.9788 - MinusLogProbMetric: 16.9788 - val_loss: 17.2199 - val_MinusLogProbMetric: 17.2199 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 416/1000
2023-09-28 03:53:14.813 
Epoch 416/1000 
	 loss: 17.0037, MinusLogProbMetric: 17.0037, val_loss: 17.1107, val_MinusLogProbMetric: 17.1107

Epoch 416: val_loss did not improve from 17.04426
196/196 - 80s - loss: 17.0037 - MinusLogProbMetric: 17.0037 - val_loss: 17.1107 - val_MinusLogProbMetric: 17.1107 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 417/1000
2023-09-28 03:54:33.996 
Epoch 417/1000 
	 loss: 16.9720, MinusLogProbMetric: 16.9720, val_loss: 17.2406, val_MinusLogProbMetric: 17.2406

Epoch 417: val_loss did not improve from 17.04426
196/196 - 79s - loss: 16.9720 - MinusLogProbMetric: 16.9720 - val_loss: 17.2406 - val_MinusLogProbMetric: 17.2406 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 418/1000
2023-09-28 03:55:54.240 
Epoch 418/1000 
	 loss: 16.9893, MinusLogProbMetric: 16.9893, val_loss: 17.0099, val_MinusLogProbMetric: 17.0099

Epoch 418: val_loss improved from 17.04426 to 17.00993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 16.9893 - MinusLogProbMetric: 16.9893 - val_loss: 17.0099 - val_MinusLogProbMetric: 17.0099 - lr: 1.6667e-04 - 82s/epoch - 417ms/step
Epoch 419/1000
2023-09-28 03:57:15.714 
Epoch 419/1000 
	 loss: 16.9936, MinusLogProbMetric: 16.9936, val_loss: 17.0724, val_MinusLogProbMetric: 17.0724

Epoch 419: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9936 - MinusLogProbMetric: 16.9936 - val_loss: 17.0724 - val_MinusLogProbMetric: 17.0724 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 420/1000
2023-09-28 03:58:35.787 
Epoch 420/1000 
	 loss: 17.0418, MinusLogProbMetric: 17.0418, val_loss: 17.0923, val_MinusLogProbMetric: 17.0923

Epoch 420: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.0418 - MinusLogProbMetric: 17.0418 - val_loss: 17.0923 - val_MinusLogProbMetric: 17.0923 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 421/1000
2023-09-28 03:59:55.967 
Epoch 421/1000 
	 loss: 16.9663, MinusLogProbMetric: 16.9663, val_loss: 17.1941, val_MinusLogProbMetric: 17.1941

Epoch 421: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9663 - MinusLogProbMetric: 16.9663 - val_loss: 17.1941 - val_MinusLogProbMetric: 17.1941 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 422/1000
2023-09-28 04:01:15.907 
Epoch 422/1000 
	 loss: 16.9431, MinusLogProbMetric: 16.9431, val_loss: 17.0730, val_MinusLogProbMetric: 17.0730

Epoch 422: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9431 - MinusLogProbMetric: 16.9431 - val_loss: 17.0730 - val_MinusLogProbMetric: 17.0730 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 423/1000
2023-09-28 04:02:36.083 
Epoch 423/1000 
	 loss: 17.0163, MinusLogProbMetric: 17.0163, val_loss: 17.0889, val_MinusLogProbMetric: 17.0889

Epoch 423: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.0163 - MinusLogProbMetric: 17.0163 - val_loss: 17.0889 - val_MinusLogProbMetric: 17.0889 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 424/1000
2023-09-28 04:03:56.344 
Epoch 424/1000 
	 loss: 16.9815, MinusLogProbMetric: 16.9815, val_loss: 17.0906, val_MinusLogProbMetric: 17.0906

Epoch 424: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9815 - MinusLogProbMetric: 16.9815 - val_loss: 17.0906 - val_MinusLogProbMetric: 17.0906 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 425/1000
2023-09-28 04:05:15.954 
Epoch 425/1000 
	 loss: 16.9722, MinusLogProbMetric: 16.9722, val_loss: 17.2984, val_MinusLogProbMetric: 17.2984

Epoch 425: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9722 - MinusLogProbMetric: 16.9722 - val_loss: 17.2984 - val_MinusLogProbMetric: 17.2984 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 426/1000
2023-09-28 04:06:35.674 
Epoch 426/1000 
	 loss: 16.9850, MinusLogProbMetric: 16.9850, val_loss: 17.1089, val_MinusLogProbMetric: 17.1089

Epoch 426: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9850 - MinusLogProbMetric: 16.9850 - val_loss: 17.1089 - val_MinusLogProbMetric: 17.1089 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 427/1000
2023-09-28 04:07:55.262 
Epoch 427/1000 
	 loss: 16.9905, MinusLogProbMetric: 16.9905, val_loss: 17.2354, val_MinusLogProbMetric: 17.2354

Epoch 427: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9905 - MinusLogProbMetric: 16.9905 - val_loss: 17.2354 - val_MinusLogProbMetric: 17.2354 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 428/1000
2023-09-28 04:09:15.357 
Epoch 428/1000 
	 loss: 16.9574, MinusLogProbMetric: 16.9574, val_loss: 17.3917, val_MinusLogProbMetric: 17.3917

Epoch 428: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9574 - MinusLogProbMetric: 16.9574 - val_loss: 17.3917 - val_MinusLogProbMetric: 17.3917 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 429/1000
2023-09-28 04:10:34.977 
Epoch 429/1000 
	 loss: 16.9811, MinusLogProbMetric: 16.9811, val_loss: 17.2725, val_MinusLogProbMetric: 17.2725

Epoch 429: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9811 - MinusLogProbMetric: 16.9811 - val_loss: 17.2725 - val_MinusLogProbMetric: 17.2725 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 430/1000
2023-09-28 04:11:54.538 
Epoch 430/1000 
	 loss: 16.9973, MinusLogProbMetric: 16.9973, val_loss: 17.2990, val_MinusLogProbMetric: 17.2990

Epoch 430: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9973 - MinusLogProbMetric: 16.9973 - val_loss: 17.2990 - val_MinusLogProbMetric: 17.2990 - lr: 1.6667e-04 - 80s/epoch - 406ms/step
Epoch 431/1000
2023-09-28 04:13:14.028 
Epoch 431/1000 
	 loss: 16.9865, MinusLogProbMetric: 16.9865, val_loss: 17.0645, val_MinusLogProbMetric: 17.0645

Epoch 431: val_loss did not improve from 17.00993
196/196 - 79s - loss: 16.9865 - MinusLogProbMetric: 16.9865 - val_loss: 17.0645 - val_MinusLogProbMetric: 17.0645 - lr: 1.6667e-04 - 79s/epoch - 406ms/step
Epoch 432/1000
2023-09-28 04:14:33.711 
Epoch 432/1000 
	 loss: 16.9453, MinusLogProbMetric: 16.9453, val_loss: 17.1278, val_MinusLogProbMetric: 17.1278

Epoch 432: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9453 - MinusLogProbMetric: 16.9453 - val_loss: 17.1278 - val_MinusLogProbMetric: 17.1278 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 433/1000
2023-09-28 04:15:53.826 
Epoch 433/1000 
	 loss: 16.9901, MinusLogProbMetric: 16.9901, val_loss: 17.3381, val_MinusLogProbMetric: 17.3381

Epoch 433: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9901 - MinusLogProbMetric: 16.9901 - val_loss: 17.3381 - val_MinusLogProbMetric: 17.3381 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 434/1000
2023-09-28 04:17:13.970 
Epoch 434/1000 
	 loss: 16.9462, MinusLogProbMetric: 16.9462, val_loss: 17.2099, val_MinusLogProbMetric: 17.2099

Epoch 434: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9462 - MinusLogProbMetric: 16.9462 - val_loss: 17.2099 - val_MinusLogProbMetric: 17.2099 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 435/1000
2023-09-28 04:18:34.423 
Epoch 435/1000 
	 loss: 16.9802, MinusLogProbMetric: 16.9802, val_loss: 17.3685, val_MinusLogProbMetric: 17.3685

Epoch 435: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9802 - MinusLogProbMetric: 16.9802 - val_loss: 17.3685 - val_MinusLogProbMetric: 17.3685 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 436/1000
2023-09-28 04:19:54.377 
Epoch 436/1000 
	 loss: 17.3611, MinusLogProbMetric: 17.3611, val_loss: 17.1626, val_MinusLogProbMetric: 17.1626

Epoch 436: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.3611 - MinusLogProbMetric: 17.3611 - val_loss: 17.1626 - val_MinusLogProbMetric: 17.1626 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 437/1000
2023-09-28 04:21:14.691 
Epoch 437/1000 
	 loss: 16.9413, MinusLogProbMetric: 16.9413, val_loss: 17.0916, val_MinusLogProbMetric: 17.0916

Epoch 437: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9413 - MinusLogProbMetric: 16.9413 - val_loss: 17.0916 - val_MinusLogProbMetric: 17.0916 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 438/1000
2023-09-28 04:22:34.637 
Epoch 438/1000 
	 loss: 16.9675, MinusLogProbMetric: 16.9675, val_loss: 17.0460, val_MinusLogProbMetric: 17.0460

Epoch 438: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9675 - MinusLogProbMetric: 16.9675 - val_loss: 17.0460 - val_MinusLogProbMetric: 17.0460 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 439/1000
2023-09-28 04:23:54.324 
Epoch 439/1000 
	 loss: 16.9799, MinusLogProbMetric: 16.9799, val_loss: 17.0887, val_MinusLogProbMetric: 17.0887

Epoch 439: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9799 - MinusLogProbMetric: 16.9799 - val_loss: 17.0887 - val_MinusLogProbMetric: 17.0887 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 440/1000
2023-09-28 04:25:14.040 
Epoch 440/1000 
	 loss: 16.9684, MinusLogProbMetric: 16.9684, val_loss: 17.1925, val_MinusLogProbMetric: 17.1925

Epoch 440: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9684 - MinusLogProbMetric: 16.9684 - val_loss: 17.1925 - val_MinusLogProbMetric: 17.1925 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 441/1000
2023-09-28 04:26:34.353 
Epoch 441/1000 
	 loss: 16.9549, MinusLogProbMetric: 16.9549, val_loss: 17.2787, val_MinusLogProbMetric: 17.2787

Epoch 441: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9549 - MinusLogProbMetric: 16.9549 - val_loss: 17.2787 - val_MinusLogProbMetric: 17.2787 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 442/1000
2023-09-28 04:27:54.409 
Epoch 442/1000 
	 loss: 16.9779, MinusLogProbMetric: 16.9779, val_loss: 17.1909, val_MinusLogProbMetric: 17.1909

Epoch 442: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9779 - MinusLogProbMetric: 16.9779 - val_loss: 17.1909 - val_MinusLogProbMetric: 17.1909 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 443/1000
2023-09-28 04:29:14.254 
Epoch 443/1000 
	 loss: 16.9143, MinusLogProbMetric: 16.9143, val_loss: 17.4266, val_MinusLogProbMetric: 17.4266

Epoch 443: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9143 - MinusLogProbMetric: 16.9143 - val_loss: 17.4266 - val_MinusLogProbMetric: 17.4266 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 444/1000
2023-09-28 04:30:33.476 
Epoch 444/1000 
	 loss: 17.0693, MinusLogProbMetric: 17.0693, val_loss: 17.3990, val_MinusLogProbMetric: 17.3990

Epoch 444: val_loss did not improve from 17.00993
196/196 - 79s - loss: 17.0693 - MinusLogProbMetric: 17.0693 - val_loss: 17.3990 - val_MinusLogProbMetric: 17.3990 - lr: 1.6667e-04 - 79s/epoch - 404ms/step
Epoch 445/1000
2023-09-28 04:31:53.767 
Epoch 445/1000 
	 loss: 16.9454, MinusLogProbMetric: 16.9454, val_loss: 17.1224, val_MinusLogProbMetric: 17.1224

Epoch 445: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9454 - MinusLogProbMetric: 16.9454 - val_loss: 17.1224 - val_MinusLogProbMetric: 17.1224 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 446/1000
2023-09-28 04:33:13.643 
Epoch 446/1000 
	 loss: 17.0027, MinusLogProbMetric: 17.0027, val_loss: 17.2640, val_MinusLogProbMetric: 17.2640

Epoch 446: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.0027 - MinusLogProbMetric: 17.0027 - val_loss: 17.2640 - val_MinusLogProbMetric: 17.2640 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 447/1000
2023-09-28 04:34:33.984 
Epoch 447/1000 
	 loss: 16.9709, MinusLogProbMetric: 16.9709, val_loss: 17.2851, val_MinusLogProbMetric: 17.2851

Epoch 447: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9709 - MinusLogProbMetric: 16.9709 - val_loss: 17.2851 - val_MinusLogProbMetric: 17.2851 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 448/1000
2023-09-28 04:35:53.962 
Epoch 448/1000 
	 loss: 16.9954, MinusLogProbMetric: 16.9954, val_loss: 17.2503, val_MinusLogProbMetric: 17.2503

Epoch 448: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9954 - MinusLogProbMetric: 16.9954 - val_loss: 17.2503 - val_MinusLogProbMetric: 17.2503 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 449/1000
2023-09-28 04:37:14.044 
Epoch 449/1000 
	 loss: 16.9948, MinusLogProbMetric: 16.9948, val_loss: 17.1125, val_MinusLogProbMetric: 17.1125

Epoch 449: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9948 - MinusLogProbMetric: 16.9948 - val_loss: 17.1125 - val_MinusLogProbMetric: 17.1125 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 450/1000
2023-09-28 04:38:34.396 
Epoch 450/1000 
	 loss: 16.9282, MinusLogProbMetric: 16.9282, val_loss: 17.2566, val_MinusLogProbMetric: 17.2566

Epoch 450: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9282 - MinusLogProbMetric: 16.9282 - val_loss: 17.2566 - val_MinusLogProbMetric: 17.2566 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 451/1000
2023-09-28 04:39:54.607 
Epoch 451/1000 
	 loss: 17.0050, MinusLogProbMetric: 17.0050, val_loss: 17.1441, val_MinusLogProbMetric: 17.1441

Epoch 451: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.0050 - MinusLogProbMetric: 17.0050 - val_loss: 17.1441 - val_MinusLogProbMetric: 17.1441 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 452/1000
2023-09-28 04:41:15.756 
Epoch 452/1000 
	 loss: 17.0244, MinusLogProbMetric: 17.0244, val_loss: 17.1526, val_MinusLogProbMetric: 17.1526

Epoch 452: val_loss did not improve from 17.00993
196/196 - 81s - loss: 17.0244 - MinusLogProbMetric: 17.0244 - val_loss: 17.1526 - val_MinusLogProbMetric: 17.1526 - lr: 1.6667e-04 - 81s/epoch - 414ms/step
Epoch 453/1000
2023-09-28 04:42:35.749 
Epoch 453/1000 
	 loss: 16.9582, MinusLogProbMetric: 16.9582, val_loss: 17.0260, val_MinusLogProbMetric: 17.0260

Epoch 453: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9582 - MinusLogProbMetric: 16.9582 - val_loss: 17.0260 - val_MinusLogProbMetric: 17.0260 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 454/1000
2023-09-28 04:43:55.850 
Epoch 454/1000 
	 loss: 16.9952, MinusLogProbMetric: 16.9952, val_loss: 17.0351, val_MinusLogProbMetric: 17.0351

Epoch 454: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9952 - MinusLogProbMetric: 16.9952 - val_loss: 17.0351 - val_MinusLogProbMetric: 17.0351 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 455/1000
2023-09-28 04:45:16.154 
Epoch 455/1000 
	 loss: 16.9691, MinusLogProbMetric: 16.9691, val_loss: 17.2426, val_MinusLogProbMetric: 17.2426

Epoch 455: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9691 - MinusLogProbMetric: 16.9691 - val_loss: 17.2426 - val_MinusLogProbMetric: 17.2426 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 456/1000
2023-09-28 04:46:36.408 
Epoch 456/1000 
	 loss: 17.0142, MinusLogProbMetric: 17.0142, val_loss: 17.0705, val_MinusLogProbMetric: 17.0705

Epoch 456: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.0142 - MinusLogProbMetric: 17.0142 - val_loss: 17.0705 - val_MinusLogProbMetric: 17.0705 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 457/1000
2023-09-28 04:47:56.179 
Epoch 457/1000 
	 loss: 16.9325, MinusLogProbMetric: 16.9325, val_loss: 17.0836, val_MinusLogProbMetric: 17.0836

Epoch 457: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9325 - MinusLogProbMetric: 16.9325 - val_loss: 17.0836 - val_MinusLogProbMetric: 17.0836 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 458/1000
2023-09-28 04:49:16.608 
Epoch 458/1000 
	 loss: 16.9310, MinusLogProbMetric: 16.9310, val_loss: 17.5044, val_MinusLogProbMetric: 17.5044

Epoch 458: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9310 - MinusLogProbMetric: 16.9310 - val_loss: 17.5044 - val_MinusLogProbMetric: 17.5044 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 459/1000
2023-09-28 04:50:35.949 
Epoch 459/1000 
	 loss: 16.9917, MinusLogProbMetric: 16.9917, val_loss: 17.1505, val_MinusLogProbMetric: 17.1505

Epoch 459: val_loss did not improve from 17.00993
196/196 - 79s - loss: 16.9917 - MinusLogProbMetric: 16.9917 - val_loss: 17.1505 - val_MinusLogProbMetric: 17.1505 - lr: 1.6667e-04 - 79s/epoch - 405ms/step
Epoch 460/1000
2023-09-28 04:51:55.980 
Epoch 460/1000 
	 loss: 16.9716, MinusLogProbMetric: 16.9716, val_loss: 17.2267, val_MinusLogProbMetric: 17.2267

Epoch 460: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9716 - MinusLogProbMetric: 16.9716 - val_loss: 17.2267 - val_MinusLogProbMetric: 17.2267 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 461/1000
2023-09-28 04:53:16.256 
Epoch 461/1000 
	 loss: 16.9673, MinusLogProbMetric: 16.9673, val_loss: 17.1725, val_MinusLogProbMetric: 17.1725

Epoch 461: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9673 - MinusLogProbMetric: 16.9673 - val_loss: 17.1725 - val_MinusLogProbMetric: 17.1725 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 462/1000
2023-09-28 04:54:36.422 
Epoch 462/1000 
	 loss: 16.9301, MinusLogProbMetric: 16.9301, val_loss: 17.2703, val_MinusLogProbMetric: 17.2703

Epoch 462: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9301 - MinusLogProbMetric: 16.9301 - val_loss: 17.2703 - val_MinusLogProbMetric: 17.2703 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 463/1000
2023-09-28 04:55:57.038 
Epoch 463/1000 
	 loss: 16.9520, MinusLogProbMetric: 16.9520, val_loss: 17.2218, val_MinusLogProbMetric: 17.2218

Epoch 463: val_loss did not improve from 17.00993
196/196 - 81s - loss: 16.9520 - MinusLogProbMetric: 16.9520 - val_loss: 17.2218 - val_MinusLogProbMetric: 17.2218 - lr: 1.6667e-04 - 81s/epoch - 411ms/step
Epoch 464/1000
2023-09-28 04:57:17.336 
Epoch 464/1000 
	 loss: 17.0547, MinusLogProbMetric: 17.0547, val_loss: 17.2563, val_MinusLogProbMetric: 17.2563

Epoch 464: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.0547 - MinusLogProbMetric: 17.0547 - val_loss: 17.2563 - val_MinusLogProbMetric: 17.2563 - lr: 1.6667e-04 - 80s/epoch - 410ms/step
Epoch 465/1000
2023-09-28 04:58:37.190 
Epoch 465/1000 
	 loss: 16.9625, MinusLogProbMetric: 16.9625, val_loss: 17.1951, val_MinusLogProbMetric: 17.1951

Epoch 465: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9625 - MinusLogProbMetric: 16.9625 - val_loss: 17.1951 - val_MinusLogProbMetric: 17.1951 - lr: 1.6667e-04 - 80s/epoch - 407ms/step
Epoch 466/1000
2023-09-28 04:59:57.359 
Epoch 466/1000 
	 loss: 16.9749, MinusLogProbMetric: 16.9749, val_loss: 17.0822, val_MinusLogProbMetric: 17.0822

Epoch 466: val_loss did not improve from 17.00993
196/196 - 80s - loss: 16.9749 - MinusLogProbMetric: 16.9749 - val_loss: 17.0822 - val_MinusLogProbMetric: 17.0822 - lr: 1.6667e-04 - 80s/epoch - 409ms/step
Epoch 467/1000
2023-09-28 05:01:17.291 
Epoch 467/1000 
	 loss: 17.1337, MinusLogProbMetric: 17.1337, val_loss: 17.9807, val_MinusLogProbMetric: 17.9807

Epoch 467: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.1337 - MinusLogProbMetric: 17.1337 - val_loss: 17.9807 - val_MinusLogProbMetric: 17.9807 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 468/1000
2023-09-28 05:02:37.244 
Epoch 468/1000 
	 loss: 17.0386, MinusLogProbMetric: 17.0386, val_loss: 17.0908, val_MinusLogProbMetric: 17.0908

Epoch 468: val_loss did not improve from 17.00993
196/196 - 80s - loss: 17.0386 - MinusLogProbMetric: 17.0386 - val_loss: 17.0908 - val_MinusLogProbMetric: 17.0908 - lr: 1.6667e-04 - 80s/epoch - 408ms/step
Epoch 469/1000
2023-09-28 05:03:57.326 
Epoch 469/1000 
	 loss: 16.8040, MinusLogProbMetric: 16.8040, val_loss: 16.9605, val_MinusLogProbMetric: 16.9605

Epoch 469: val_loss improved from 17.00993 to 16.96054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.8040 - MinusLogProbMetric: 16.8040 - val_loss: 16.9605 - val_MinusLogProbMetric: 16.9605 - lr: 8.3333e-05 - 81s/epoch - 416ms/step
Epoch 470/1000
2023-09-28 05:05:18.960 
Epoch 470/1000 
	 loss: 16.8028, MinusLogProbMetric: 16.8028, val_loss: 17.0150, val_MinusLogProbMetric: 17.0150

Epoch 470: val_loss did not improve from 16.96054
196/196 - 80s - loss: 16.8028 - MinusLogProbMetric: 16.8028 - val_loss: 17.0150 - val_MinusLogProbMetric: 17.0150 - lr: 8.3333e-05 - 80s/epoch - 410ms/step
Epoch 471/1000
2023-09-28 05:06:39.137 
Epoch 471/1000 
	 loss: 16.7981, MinusLogProbMetric: 16.7981, val_loss: 16.9680, val_MinusLogProbMetric: 16.9680

Epoch 471: val_loss did not improve from 16.96054
196/196 - 80s - loss: 16.7981 - MinusLogProbMetric: 16.7981 - val_loss: 16.9680 - val_MinusLogProbMetric: 16.9680 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 472/1000
2023-09-28 05:07:59.943 
Epoch 472/1000 
	 loss: 16.8020, MinusLogProbMetric: 16.8020, val_loss: 16.9533, val_MinusLogProbMetric: 16.9533

Epoch 472: val_loss improved from 16.96054 to 16.95332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 16.8020 - MinusLogProbMetric: 16.8020 - val_loss: 16.9533 - val_MinusLogProbMetric: 16.9533 - lr: 8.3333e-05 - 82s/epoch - 419ms/step
Epoch 473/1000
2023-09-28 05:09:22.235 
Epoch 473/1000 
	 loss: 16.8033, MinusLogProbMetric: 16.8033, val_loss: 16.9701, val_MinusLogProbMetric: 16.9701

Epoch 473: val_loss did not improve from 16.95332
196/196 - 81s - loss: 16.8033 - MinusLogProbMetric: 16.8033 - val_loss: 16.9701 - val_MinusLogProbMetric: 16.9701 - lr: 8.3333e-05 - 81s/epoch - 413ms/step
Epoch 474/1000
2023-09-28 05:10:42.434 
Epoch 474/1000 
	 loss: 16.8047, MinusLogProbMetric: 16.8047, val_loss: 16.9965, val_MinusLogProbMetric: 16.9965

Epoch 474: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.8047 - MinusLogProbMetric: 16.8047 - val_loss: 16.9965 - val_MinusLogProbMetric: 16.9965 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 475/1000
2023-09-28 05:12:02.642 
Epoch 475/1000 
	 loss: 16.7919, MinusLogProbMetric: 16.7919, val_loss: 16.9748, val_MinusLogProbMetric: 16.9748

Epoch 475: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.7919 - MinusLogProbMetric: 16.7919 - val_loss: 16.9748 - val_MinusLogProbMetric: 16.9748 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 476/1000
2023-09-28 05:13:22.591 
Epoch 476/1000 
	 loss: 16.7980, MinusLogProbMetric: 16.7980, val_loss: 16.9604, val_MinusLogProbMetric: 16.9604

Epoch 476: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.7980 - MinusLogProbMetric: 16.7980 - val_loss: 16.9604 - val_MinusLogProbMetric: 16.9604 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 477/1000
2023-09-28 05:14:42.278 
Epoch 477/1000 
	 loss: 16.7885, MinusLogProbMetric: 16.7885, val_loss: 16.9735, val_MinusLogProbMetric: 16.9735

Epoch 477: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.7885 - MinusLogProbMetric: 16.7885 - val_loss: 16.9735 - val_MinusLogProbMetric: 16.9735 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 478/1000
2023-09-28 05:16:01.977 
Epoch 478/1000 
	 loss: 16.7905, MinusLogProbMetric: 16.7905, val_loss: 17.0631, val_MinusLogProbMetric: 17.0631

Epoch 478: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.7905 - MinusLogProbMetric: 16.7905 - val_loss: 17.0631 - val_MinusLogProbMetric: 17.0631 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 479/1000
2023-09-28 05:17:21.890 
Epoch 479/1000 
	 loss: 16.7949, MinusLogProbMetric: 16.7949, val_loss: 17.1635, val_MinusLogProbMetric: 17.1635

Epoch 479: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.7949 - MinusLogProbMetric: 16.7949 - val_loss: 17.1635 - val_MinusLogProbMetric: 17.1635 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 480/1000
2023-09-28 05:18:41.534 
Epoch 480/1000 
	 loss: 16.8377, MinusLogProbMetric: 16.8377, val_loss: 16.9728, val_MinusLogProbMetric: 16.9728

Epoch 480: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.8377 - MinusLogProbMetric: 16.8377 - val_loss: 16.9728 - val_MinusLogProbMetric: 16.9728 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 481/1000
2023-09-28 05:20:01.692 
Epoch 481/1000 
	 loss: 16.7775, MinusLogProbMetric: 16.7775, val_loss: 16.9556, val_MinusLogProbMetric: 16.9556

Epoch 481: val_loss did not improve from 16.95332
196/196 - 80s - loss: 16.7775 - MinusLogProbMetric: 16.7775 - val_loss: 16.9556 - val_MinusLogProbMetric: 16.9556 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 482/1000
2023-09-28 05:21:21.317 
Epoch 482/1000 
	 loss: 16.8033, MinusLogProbMetric: 16.8033, val_loss: 16.9397, val_MinusLogProbMetric: 16.9397

Epoch 482: val_loss improved from 16.95332 to 16.93966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.8033 - MinusLogProbMetric: 16.8033 - val_loss: 16.9397 - val_MinusLogProbMetric: 16.9397 - lr: 8.3333e-05 - 81s/epoch - 414ms/step
Epoch 483/1000
2023-09-28 05:22:42.653 
Epoch 483/1000 
	 loss: 17.0202, MinusLogProbMetric: 17.0202, val_loss: 19.0286, val_MinusLogProbMetric: 19.0286

Epoch 483: val_loss did not improve from 16.93966
196/196 - 80s - loss: 17.0202 - MinusLogProbMetric: 17.0202 - val_loss: 19.0286 - val_MinusLogProbMetric: 19.0286 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 484/1000
2023-09-28 05:24:02.731 
Epoch 484/1000 
	 loss: 16.9866, MinusLogProbMetric: 16.9866, val_loss: 16.9948, val_MinusLogProbMetric: 16.9948

Epoch 484: val_loss did not improve from 16.93966
196/196 - 80s - loss: 16.9866 - MinusLogProbMetric: 16.9866 - val_loss: 16.9948 - val_MinusLogProbMetric: 16.9948 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 485/1000
2023-09-28 05:25:22.277 
Epoch 485/1000 
	 loss: 16.7999, MinusLogProbMetric: 16.7999, val_loss: 16.9679, val_MinusLogProbMetric: 16.9679

Epoch 485: val_loss did not improve from 16.93966
196/196 - 80s - loss: 16.7999 - MinusLogProbMetric: 16.7999 - val_loss: 16.9679 - val_MinusLogProbMetric: 16.9679 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 486/1000
2023-09-28 05:26:42.485 
Epoch 486/1000 
	 loss: 16.7908, MinusLogProbMetric: 16.7908, val_loss: 16.9306, val_MinusLogProbMetric: 16.9306

Epoch 486: val_loss improved from 16.93966 to 16.93057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 16.7908 - MinusLogProbMetric: 16.7908 - val_loss: 16.9306 - val_MinusLogProbMetric: 16.9306 - lr: 8.3333e-05 - 82s/epoch - 417ms/step
Epoch 487/1000
2023-09-28 05:28:04.186 
Epoch 487/1000 
	 loss: 16.8000, MinusLogProbMetric: 16.8000, val_loss: 16.9830, val_MinusLogProbMetric: 16.9830

Epoch 487: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8000 - MinusLogProbMetric: 16.8000 - val_loss: 16.9830 - val_MinusLogProbMetric: 16.9830 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 488/1000
2023-09-28 05:29:24.376 
Epoch 488/1000 
	 loss: 16.7914, MinusLogProbMetric: 16.7914, val_loss: 17.0022, val_MinusLogProbMetric: 17.0022

Epoch 488: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.7914 - MinusLogProbMetric: 16.7914 - val_loss: 17.0022 - val_MinusLogProbMetric: 17.0022 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 489/1000
2023-09-28 05:30:44.393 
Epoch 489/1000 
	 loss: 16.8070, MinusLogProbMetric: 16.8070, val_loss: 16.9905, val_MinusLogProbMetric: 16.9905

Epoch 489: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8070 - MinusLogProbMetric: 16.8070 - val_loss: 16.9905 - val_MinusLogProbMetric: 16.9905 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 490/1000
2023-09-28 05:32:04.689 
Epoch 490/1000 
	 loss: 16.8005, MinusLogProbMetric: 16.8005, val_loss: 16.9497, val_MinusLogProbMetric: 16.9497

Epoch 490: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8005 - MinusLogProbMetric: 16.8005 - val_loss: 16.9497 - val_MinusLogProbMetric: 16.9497 - lr: 8.3333e-05 - 80s/epoch - 410ms/step
Epoch 491/1000
2023-09-28 05:33:24.511 
Epoch 491/1000 
	 loss: 16.7886, MinusLogProbMetric: 16.7886, val_loss: 16.9759, val_MinusLogProbMetric: 16.9759

Epoch 491: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.7886 - MinusLogProbMetric: 16.7886 - val_loss: 16.9759 - val_MinusLogProbMetric: 16.9759 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 492/1000
2023-09-28 05:34:44.314 
Epoch 492/1000 
	 loss: 16.7829, MinusLogProbMetric: 16.7829, val_loss: 17.0114, val_MinusLogProbMetric: 17.0114

Epoch 492: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.7829 - MinusLogProbMetric: 16.7829 - val_loss: 17.0114 - val_MinusLogProbMetric: 17.0114 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 493/1000
2023-09-28 05:36:04.202 
Epoch 493/1000 
	 loss: 16.7886, MinusLogProbMetric: 16.7886, val_loss: 16.9993, val_MinusLogProbMetric: 16.9993

Epoch 493: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.7886 - MinusLogProbMetric: 16.7886 - val_loss: 16.9993 - val_MinusLogProbMetric: 16.9993 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 494/1000
2023-09-28 05:37:24.068 
Epoch 494/1000 
	 loss: 16.8261, MinusLogProbMetric: 16.8261, val_loss: 16.9512, val_MinusLogProbMetric: 16.9512

Epoch 494: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8261 - MinusLogProbMetric: 16.8261 - val_loss: 16.9512 - val_MinusLogProbMetric: 16.9512 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 495/1000
2023-09-28 05:38:43.679 
Epoch 495/1000 
	 loss: 16.7857, MinusLogProbMetric: 16.7857, val_loss: 16.9964, val_MinusLogProbMetric: 16.9964

Epoch 495: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.7857 - MinusLogProbMetric: 16.7857 - val_loss: 16.9964 - val_MinusLogProbMetric: 16.9964 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 496/1000
2023-09-28 05:40:03.753 
Epoch 496/1000 
	 loss: 16.8159, MinusLogProbMetric: 16.8159, val_loss: 16.9813, val_MinusLogProbMetric: 16.9813

Epoch 496: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8159 - MinusLogProbMetric: 16.8159 - val_loss: 16.9813 - val_MinusLogProbMetric: 16.9813 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 497/1000
2023-09-28 05:41:23.811 
Epoch 497/1000 
	 loss: 16.8078, MinusLogProbMetric: 16.8078, val_loss: 17.2827, val_MinusLogProbMetric: 17.2827

Epoch 497: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8078 - MinusLogProbMetric: 16.8078 - val_loss: 17.2827 - val_MinusLogProbMetric: 17.2827 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 498/1000
2023-09-28 05:42:43.967 
Epoch 498/1000 
	 loss: 16.8108, MinusLogProbMetric: 16.8108, val_loss: 16.9494, val_MinusLogProbMetric: 16.9494

Epoch 498: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8108 - MinusLogProbMetric: 16.8108 - val_loss: 16.9494 - val_MinusLogProbMetric: 16.9494 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 499/1000
2023-09-28 05:44:03.951 
Epoch 499/1000 
	 loss: 16.7880, MinusLogProbMetric: 16.7880, val_loss: 16.9789, val_MinusLogProbMetric: 16.9789

Epoch 499: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.7880 - MinusLogProbMetric: 16.7880 - val_loss: 16.9789 - val_MinusLogProbMetric: 16.9789 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 500/1000
2023-09-28 05:45:23.645 
Epoch 500/1000 
	 loss: 16.7918, MinusLogProbMetric: 16.7918, val_loss: 16.9844, val_MinusLogProbMetric: 16.9844

Epoch 500: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.7918 - MinusLogProbMetric: 16.7918 - val_loss: 16.9844 - val_MinusLogProbMetric: 16.9844 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 501/1000
2023-09-28 05:46:43.286 
Epoch 501/1000 
	 loss: 16.8301, MinusLogProbMetric: 16.8301, val_loss: 17.0733, val_MinusLogProbMetric: 17.0733

Epoch 501: val_loss did not improve from 16.93057
196/196 - 80s - loss: 16.8301 - MinusLogProbMetric: 16.8301 - val_loss: 17.0733 - val_MinusLogProbMetric: 17.0733 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 502/1000
2023-09-28 05:48:02.675 
Epoch 502/1000 
	 loss: 16.7862, MinusLogProbMetric: 16.7862, val_loss: 16.9385, val_MinusLogProbMetric: 16.9385

Epoch 502: val_loss did not improve from 16.93057
196/196 - 79s - loss: 16.7862 - MinusLogProbMetric: 16.7862 - val_loss: 16.9385 - val_MinusLogProbMetric: 16.9385 - lr: 8.3333e-05 - 79s/epoch - 405ms/step
Epoch 503/1000
2023-09-28 05:49:22.507 
Epoch 503/1000 
	 loss: 16.8351, MinusLogProbMetric: 16.8351, val_loss: 16.9161, val_MinusLogProbMetric: 16.9161

Epoch 503: val_loss improved from 16.93057 to 16.91606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.8351 - MinusLogProbMetric: 16.8351 - val_loss: 16.9161 - val_MinusLogProbMetric: 16.9161 - lr: 8.3333e-05 - 81s/epoch - 414ms/step
Epoch 504/1000
2023-09-28 05:50:44.602 
Epoch 504/1000 
	 loss: 16.7875, MinusLogProbMetric: 16.7875, val_loss: 16.9438, val_MinusLogProbMetric: 16.9438

Epoch 504: val_loss did not improve from 16.91606
196/196 - 81s - loss: 16.7875 - MinusLogProbMetric: 16.7875 - val_loss: 16.9438 - val_MinusLogProbMetric: 16.9438 - lr: 8.3333e-05 - 81s/epoch - 412ms/step
Epoch 505/1000
2023-09-28 05:52:03.580 
Epoch 505/1000 
	 loss: 16.7976, MinusLogProbMetric: 16.7976, val_loss: 17.0559, val_MinusLogProbMetric: 17.0559

Epoch 505: val_loss did not improve from 16.91606
196/196 - 79s - loss: 16.7976 - MinusLogProbMetric: 16.7976 - val_loss: 17.0559 - val_MinusLogProbMetric: 17.0559 - lr: 8.3333e-05 - 79s/epoch - 403ms/step
Epoch 506/1000
2023-09-28 05:53:22.981 
Epoch 506/1000 
	 loss: 16.8543, MinusLogProbMetric: 16.8543, val_loss: 17.0305, val_MinusLogProbMetric: 17.0305

Epoch 506: val_loss did not improve from 16.91606
196/196 - 79s - loss: 16.8543 - MinusLogProbMetric: 16.8543 - val_loss: 17.0305 - val_MinusLogProbMetric: 17.0305 - lr: 8.3333e-05 - 79s/epoch - 405ms/step
Epoch 507/1000
2023-09-28 05:54:42.756 
Epoch 507/1000 
	 loss: 16.8124, MinusLogProbMetric: 16.8124, val_loss: 17.2837, val_MinusLogProbMetric: 17.2837

Epoch 507: val_loss did not improve from 16.91606
196/196 - 80s - loss: 16.8124 - MinusLogProbMetric: 16.8124 - val_loss: 17.2837 - val_MinusLogProbMetric: 17.2837 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 508/1000
2023-09-28 05:56:02.507 
Epoch 508/1000 
	 loss: 16.8042, MinusLogProbMetric: 16.8042, val_loss: 16.9573, val_MinusLogProbMetric: 16.9573

Epoch 508: val_loss did not improve from 16.91606
196/196 - 80s - loss: 16.8042 - MinusLogProbMetric: 16.8042 - val_loss: 16.9573 - val_MinusLogProbMetric: 16.9573 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 509/1000
2023-09-28 05:57:22.152 
Epoch 509/1000 
	 loss: 16.7983, MinusLogProbMetric: 16.7983, val_loss: 16.9933, val_MinusLogProbMetric: 16.9933

Epoch 509: val_loss did not improve from 16.91606
196/196 - 80s - loss: 16.7983 - MinusLogProbMetric: 16.7983 - val_loss: 16.9933 - val_MinusLogProbMetric: 16.9933 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 510/1000
2023-09-28 05:58:41.362 
Epoch 510/1000 
	 loss: 16.7975, MinusLogProbMetric: 16.7975, val_loss: 16.9382, val_MinusLogProbMetric: 16.9382

Epoch 510: val_loss did not improve from 16.91606
196/196 - 79s - loss: 16.7975 - MinusLogProbMetric: 16.7975 - val_loss: 16.9382 - val_MinusLogProbMetric: 16.9382 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 511/1000
2023-09-28 06:00:01.827 
Epoch 511/1000 
	 loss: 16.8039, MinusLogProbMetric: 16.8039, val_loss: 17.0014, val_MinusLogProbMetric: 17.0014

Epoch 511: val_loss did not improve from 16.91606
196/196 - 80s - loss: 16.8039 - MinusLogProbMetric: 16.8039 - val_loss: 17.0014 - val_MinusLogProbMetric: 17.0014 - lr: 8.3333e-05 - 80s/epoch - 411ms/step
Epoch 512/1000
2023-09-28 06:01:22.098 
Epoch 512/1000 
	 loss: 16.7979, MinusLogProbMetric: 16.7979, val_loss: 17.0649, val_MinusLogProbMetric: 17.0649

Epoch 512: val_loss did not improve from 16.91606
196/196 - 80s - loss: 16.7979 - MinusLogProbMetric: 16.7979 - val_loss: 17.0649 - val_MinusLogProbMetric: 17.0649 - lr: 8.3333e-05 - 80s/epoch - 410ms/step
Epoch 513/1000
2023-09-28 06:02:42.181 
Epoch 513/1000 
	 loss: 16.7856, MinusLogProbMetric: 16.7856, val_loss: 17.1522, val_MinusLogProbMetric: 17.1522

Epoch 513: val_loss did not improve from 16.91606
196/196 - 80s - loss: 16.7856 - MinusLogProbMetric: 16.7856 - val_loss: 17.1522 - val_MinusLogProbMetric: 17.1522 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 514/1000
2023-09-28 06:04:02.055 
Epoch 514/1000 
	 loss: 16.8138, MinusLogProbMetric: 16.8138, val_loss: 16.8991, val_MinusLogProbMetric: 16.8991

Epoch 514: val_loss improved from 16.91606 to 16.89913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.8138 - MinusLogProbMetric: 16.8138 - val_loss: 16.8991 - val_MinusLogProbMetric: 16.8991 - lr: 8.3333e-05 - 81s/epoch - 414ms/step
Epoch 515/1000
2023-09-28 06:05:23.170 
Epoch 515/1000 
	 loss: 16.7953, MinusLogProbMetric: 16.7953, val_loss: 16.9277, val_MinusLogProbMetric: 16.9277

Epoch 515: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.7953 - MinusLogProbMetric: 16.7953 - val_loss: 16.9277 - val_MinusLogProbMetric: 16.9277 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 516/1000
2023-09-28 06:06:43.455 
Epoch 516/1000 
	 loss: 16.7926, MinusLogProbMetric: 16.7926, val_loss: 16.9918, val_MinusLogProbMetric: 16.9918

Epoch 516: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.7926 - MinusLogProbMetric: 16.7926 - val_loss: 16.9918 - val_MinusLogProbMetric: 16.9918 - lr: 8.3333e-05 - 80s/epoch - 410ms/step
Epoch 517/1000
2023-09-28 06:08:03.517 
Epoch 517/1000 
	 loss: 16.8086, MinusLogProbMetric: 16.8086, val_loss: 16.9548, val_MinusLogProbMetric: 16.9548

Epoch 517: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.8086 - MinusLogProbMetric: 16.8086 - val_loss: 16.9548 - val_MinusLogProbMetric: 16.9548 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 518/1000
2023-09-28 06:09:22.918 
Epoch 518/1000 
	 loss: 16.7956, MinusLogProbMetric: 16.7956, val_loss: 17.0510, val_MinusLogProbMetric: 17.0510

Epoch 518: val_loss did not improve from 16.89913
196/196 - 79s - loss: 16.7956 - MinusLogProbMetric: 16.7956 - val_loss: 17.0510 - val_MinusLogProbMetric: 17.0510 - lr: 8.3333e-05 - 79s/epoch - 405ms/step
Epoch 519/1000
2023-09-28 06:10:43.767 
Epoch 519/1000 
	 loss: 16.7907, MinusLogProbMetric: 16.7907, val_loss: 16.9852, val_MinusLogProbMetric: 16.9852

Epoch 519: val_loss did not improve from 16.89913
196/196 - 81s - loss: 16.7907 - MinusLogProbMetric: 16.7907 - val_loss: 16.9852 - val_MinusLogProbMetric: 16.9852 - lr: 8.3333e-05 - 81s/epoch - 412ms/step
Epoch 520/1000
2023-09-28 06:12:03.854 
Epoch 520/1000 
	 loss: 16.7941, MinusLogProbMetric: 16.7941, val_loss: 17.0031, val_MinusLogProbMetric: 17.0031

Epoch 520: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.7941 - MinusLogProbMetric: 16.7941 - val_loss: 17.0031 - val_MinusLogProbMetric: 17.0031 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 521/1000
2023-09-28 06:13:24.439 
Epoch 521/1000 
	 loss: 17.6187, MinusLogProbMetric: 17.6187, val_loss: 17.3235, val_MinusLogProbMetric: 17.3235

Epoch 521: val_loss did not improve from 16.89913
196/196 - 81s - loss: 17.6187 - MinusLogProbMetric: 17.6187 - val_loss: 17.3235 - val_MinusLogProbMetric: 17.3235 - lr: 8.3333e-05 - 81s/epoch - 411ms/step
Epoch 522/1000
2023-09-28 06:14:44.040 
Epoch 522/1000 
	 loss: 17.1147, MinusLogProbMetric: 17.1147, val_loss: 17.3196, val_MinusLogProbMetric: 17.3196

Epoch 522: val_loss did not improve from 16.89913
196/196 - 80s - loss: 17.1147 - MinusLogProbMetric: 17.1147 - val_loss: 17.3196 - val_MinusLogProbMetric: 17.3196 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 523/1000
2023-09-28 06:16:03.918 
Epoch 523/1000 
	 loss: 17.0699, MinusLogProbMetric: 17.0699, val_loss: 17.2551, val_MinusLogProbMetric: 17.2551

Epoch 523: val_loss did not improve from 16.89913
196/196 - 80s - loss: 17.0699 - MinusLogProbMetric: 17.0699 - val_loss: 17.2551 - val_MinusLogProbMetric: 17.2551 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 524/1000
2023-09-28 06:17:23.036 
Epoch 524/1000 
	 loss: 17.0698, MinusLogProbMetric: 17.0698, val_loss: 17.2600, val_MinusLogProbMetric: 17.2600

Epoch 524: val_loss did not improve from 16.89913
196/196 - 79s - loss: 17.0698 - MinusLogProbMetric: 17.0698 - val_loss: 17.2600 - val_MinusLogProbMetric: 17.2600 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 525/1000
2023-09-28 06:18:41.829 
Epoch 525/1000 
	 loss: 17.0458, MinusLogProbMetric: 17.0458, val_loss: 17.1149, val_MinusLogProbMetric: 17.1149

Epoch 525: val_loss did not improve from 16.89913
196/196 - 79s - loss: 17.0458 - MinusLogProbMetric: 17.0458 - val_loss: 17.1149 - val_MinusLogProbMetric: 17.1149 - lr: 8.3333e-05 - 79s/epoch - 402ms/step
Epoch 526/1000
2023-09-28 06:20:01.530 
Epoch 526/1000 
	 loss: 16.8786, MinusLogProbMetric: 16.8786, val_loss: 16.9965, val_MinusLogProbMetric: 16.9965

Epoch 526: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.8786 - MinusLogProbMetric: 16.8786 - val_loss: 16.9965 - val_MinusLogProbMetric: 16.9965 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 527/1000
2023-09-28 06:21:21.584 
Epoch 527/1000 
	 loss: 16.8891, MinusLogProbMetric: 16.8891, val_loss: 17.2026, val_MinusLogProbMetric: 17.2026

Epoch 527: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.8891 - MinusLogProbMetric: 16.8891 - val_loss: 17.2026 - val_MinusLogProbMetric: 17.2026 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 528/1000
2023-09-28 06:22:40.829 
Epoch 528/1000 
	 loss: 16.8189, MinusLogProbMetric: 16.8189, val_loss: 16.9514, val_MinusLogProbMetric: 16.9514

Epoch 528: val_loss did not improve from 16.89913
196/196 - 79s - loss: 16.8189 - MinusLogProbMetric: 16.8189 - val_loss: 16.9514 - val_MinusLogProbMetric: 16.9514 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 529/1000
2023-09-28 06:24:00.360 
Epoch 529/1000 
	 loss: 16.7809, MinusLogProbMetric: 16.7809, val_loss: 16.9383, val_MinusLogProbMetric: 16.9383

Epoch 529: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.7809 - MinusLogProbMetric: 16.7809 - val_loss: 16.9383 - val_MinusLogProbMetric: 16.9383 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 530/1000
2023-09-28 06:25:19.958 
Epoch 530/1000 
	 loss: 16.7817, MinusLogProbMetric: 16.7817, val_loss: 16.9426, val_MinusLogProbMetric: 16.9426

Epoch 530: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.7817 - MinusLogProbMetric: 16.7817 - val_loss: 16.9426 - val_MinusLogProbMetric: 16.9426 - lr: 8.3333e-05 - 80s/epoch - 406ms/step
Epoch 531/1000
2023-09-28 06:26:36.549 
Epoch 531/1000 
	 loss: 16.7924, MinusLogProbMetric: 16.7924, val_loss: 17.0031, val_MinusLogProbMetric: 17.0031

Epoch 531: val_loss did not improve from 16.89913
196/196 - 77s - loss: 16.7924 - MinusLogProbMetric: 16.7924 - val_loss: 17.0031 - val_MinusLogProbMetric: 17.0031 - lr: 8.3333e-05 - 77s/epoch - 391ms/step
Epoch 532/1000
2023-09-28 06:27:47.250 
Epoch 532/1000 
	 loss: 16.7774, MinusLogProbMetric: 16.7774, val_loss: 16.9705, val_MinusLogProbMetric: 16.9705

Epoch 532: val_loss did not improve from 16.89913
196/196 - 71s - loss: 16.7774 - MinusLogProbMetric: 16.7774 - val_loss: 16.9705 - val_MinusLogProbMetric: 16.9705 - lr: 8.3333e-05 - 71s/epoch - 361ms/step
Epoch 533/1000
2023-09-28 06:28:55.101 
Epoch 533/1000 
	 loss: 16.7976, MinusLogProbMetric: 16.7976, val_loss: 16.9337, val_MinusLogProbMetric: 16.9337

Epoch 533: val_loss did not improve from 16.89913
196/196 - 68s - loss: 16.7976 - MinusLogProbMetric: 16.7976 - val_loss: 16.9337 - val_MinusLogProbMetric: 16.9337 - lr: 8.3333e-05 - 68s/epoch - 346ms/step
Epoch 534/1000
2023-09-28 06:30:11.992 
Epoch 534/1000 
	 loss: 16.7765, MinusLogProbMetric: 16.7765, val_loss: 17.0422, val_MinusLogProbMetric: 17.0422

Epoch 534: val_loss did not improve from 16.89913
196/196 - 77s - loss: 16.7765 - MinusLogProbMetric: 16.7765 - val_loss: 17.0422 - val_MinusLogProbMetric: 17.0422 - lr: 8.3333e-05 - 77s/epoch - 392ms/step
Epoch 535/1000
2023-09-28 06:31:22.341 
Epoch 535/1000 
	 loss: 16.7878, MinusLogProbMetric: 16.7878, val_loss: 17.0309, val_MinusLogProbMetric: 17.0309

Epoch 535: val_loss did not improve from 16.89913
196/196 - 70s - loss: 16.7878 - MinusLogProbMetric: 16.7878 - val_loss: 17.0309 - val_MinusLogProbMetric: 17.0309 - lr: 8.3333e-05 - 70s/epoch - 359ms/step
Epoch 536/1000
2023-09-28 06:32:32.287 
Epoch 536/1000 
	 loss: 16.7898, MinusLogProbMetric: 16.7898, val_loss: 16.9674, val_MinusLogProbMetric: 16.9674

Epoch 536: val_loss did not improve from 16.89913
196/196 - 70s - loss: 16.7898 - MinusLogProbMetric: 16.7898 - val_loss: 16.9674 - val_MinusLogProbMetric: 16.9674 - lr: 8.3333e-05 - 70s/epoch - 357ms/step
Epoch 537/1000
2023-09-28 06:33:49.623 
Epoch 537/1000 
	 loss: 16.7896, MinusLogProbMetric: 16.7896, val_loss: 16.9515, val_MinusLogProbMetric: 16.9515

Epoch 537: val_loss did not improve from 16.89913
196/196 - 77s - loss: 16.7896 - MinusLogProbMetric: 16.7896 - val_loss: 16.9515 - val_MinusLogProbMetric: 16.9515 - lr: 8.3333e-05 - 77s/epoch - 395ms/step
Epoch 538/1000
2023-09-28 06:34:58.272 
Epoch 538/1000 
	 loss: 16.7840, MinusLogProbMetric: 16.7840, val_loss: 16.9904, val_MinusLogProbMetric: 16.9904

Epoch 538: val_loss did not improve from 16.89913
196/196 - 69s - loss: 16.7840 - MinusLogProbMetric: 16.7840 - val_loss: 16.9904 - val_MinusLogProbMetric: 16.9904 - lr: 8.3333e-05 - 69s/epoch - 350ms/step
Epoch 539/1000
2023-09-28 06:36:11.821 
Epoch 539/1000 
	 loss: 16.7634, MinusLogProbMetric: 16.7634, val_loss: 16.9153, val_MinusLogProbMetric: 16.9153

Epoch 539: val_loss did not improve from 16.89913
196/196 - 74s - loss: 16.7634 - MinusLogProbMetric: 16.7634 - val_loss: 16.9153 - val_MinusLogProbMetric: 16.9153 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 540/1000
2023-09-28 06:37:24.925 
Epoch 540/1000 
	 loss: 16.7855, MinusLogProbMetric: 16.7855, val_loss: 16.9977, val_MinusLogProbMetric: 16.9977

Epoch 540: val_loss did not improve from 16.89913
196/196 - 73s - loss: 16.7855 - MinusLogProbMetric: 16.7855 - val_loss: 16.9977 - val_MinusLogProbMetric: 16.9977 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 541/1000
2023-09-28 06:38:32.646 
Epoch 541/1000 
	 loss: 16.8239, MinusLogProbMetric: 16.8239, val_loss: 17.0335, val_MinusLogProbMetric: 17.0335

Epoch 541: val_loss did not improve from 16.89913
196/196 - 68s - loss: 16.8239 - MinusLogProbMetric: 16.8239 - val_loss: 17.0335 - val_MinusLogProbMetric: 17.0335 - lr: 8.3333e-05 - 68s/epoch - 345ms/step
Epoch 542/1000
2023-09-28 06:39:50.577 
Epoch 542/1000 
	 loss: 16.7967, MinusLogProbMetric: 16.7967, val_loss: 16.9675, val_MinusLogProbMetric: 16.9675

Epoch 542: val_loss did not improve from 16.89913
196/196 - 78s - loss: 16.7967 - MinusLogProbMetric: 16.7967 - val_loss: 16.9675 - val_MinusLogProbMetric: 16.9675 - lr: 8.3333e-05 - 78s/epoch - 398ms/step
Epoch 543/1000
2023-09-28 06:40:58.013 
Epoch 543/1000 
	 loss: 16.8418, MinusLogProbMetric: 16.8418, val_loss: 16.9209, val_MinusLogProbMetric: 16.9209

Epoch 543: val_loss did not improve from 16.89913
196/196 - 67s - loss: 16.8418 - MinusLogProbMetric: 16.8418 - val_loss: 16.9209 - val_MinusLogProbMetric: 16.9209 - lr: 8.3333e-05 - 67s/epoch - 344ms/step
Epoch 544/1000
2023-09-28 06:42:10.731 
Epoch 544/1000 
	 loss: 16.7851, MinusLogProbMetric: 16.7851, val_loss: 16.9770, val_MinusLogProbMetric: 16.9770

Epoch 544: val_loss did not improve from 16.89913
196/196 - 73s - loss: 16.7851 - MinusLogProbMetric: 16.7851 - val_loss: 16.9770 - val_MinusLogProbMetric: 16.9770 - lr: 8.3333e-05 - 73s/epoch - 371ms/step
Epoch 545/1000
2023-09-28 06:43:23.349 
Epoch 545/1000 
	 loss: 16.7784, MinusLogProbMetric: 16.7784, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 545: val_loss did not improve from 16.89913
196/196 - 73s - loss: 16.7784 - MinusLogProbMetric: 16.7784 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 8.3333e-05 - 73s/epoch - 370ms/step
Epoch 546/1000
2023-09-28 06:44:31.654 
Epoch 546/1000 
	 loss: 16.7925, MinusLogProbMetric: 16.7925, val_loss: 17.0814, val_MinusLogProbMetric: 17.0814

Epoch 546: val_loss did not improve from 16.89913
196/196 - 68s - loss: 16.7925 - MinusLogProbMetric: 16.7925 - val_loss: 17.0814 - val_MinusLogProbMetric: 17.0814 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 547/1000
2023-09-28 06:45:47.201 
Epoch 547/1000 
	 loss: 16.8026, MinusLogProbMetric: 16.8026, val_loss: 16.9808, val_MinusLogProbMetric: 16.9808

Epoch 547: val_loss did not improve from 16.89913
196/196 - 76s - loss: 16.8026 - MinusLogProbMetric: 16.8026 - val_loss: 16.9808 - val_MinusLogProbMetric: 16.9808 - lr: 8.3333e-05 - 76s/epoch - 385ms/step
Epoch 548/1000
2023-09-28 06:46:59.165 
Epoch 548/1000 
	 loss: 16.7916, MinusLogProbMetric: 16.7916, val_loss: 16.9247, val_MinusLogProbMetric: 16.9247

Epoch 548: val_loss did not improve from 16.89913
196/196 - 72s - loss: 16.7916 - MinusLogProbMetric: 16.7916 - val_loss: 16.9247 - val_MinusLogProbMetric: 16.9247 - lr: 8.3333e-05 - 72s/epoch - 367ms/step
Epoch 549/1000
2023-09-28 06:48:04.902 
Epoch 549/1000 
	 loss: 16.7828, MinusLogProbMetric: 16.7828, val_loss: 16.9839, val_MinusLogProbMetric: 16.9839

Epoch 549: val_loss did not improve from 16.89913
196/196 - 66s - loss: 16.7828 - MinusLogProbMetric: 16.7828 - val_loss: 16.9839 - val_MinusLogProbMetric: 16.9839 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 550/1000
2023-09-28 06:49:20.691 
Epoch 550/1000 
	 loss: 16.8006, MinusLogProbMetric: 16.8006, val_loss: 16.9456, val_MinusLogProbMetric: 16.9456

Epoch 550: val_loss did not improve from 16.89913
196/196 - 76s - loss: 16.8006 - MinusLogProbMetric: 16.8006 - val_loss: 16.9456 - val_MinusLogProbMetric: 16.9456 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 551/1000
2023-09-28 06:50:26.544 
Epoch 551/1000 
	 loss: 16.7880, MinusLogProbMetric: 16.7880, val_loss: 17.0868, val_MinusLogProbMetric: 17.0868

Epoch 551: val_loss did not improve from 16.89913
196/196 - 66s - loss: 16.7880 - MinusLogProbMetric: 16.7880 - val_loss: 17.0868 - val_MinusLogProbMetric: 17.0868 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 552/1000
2023-09-28 06:51:35.384 
Epoch 552/1000 
	 loss: 16.7728, MinusLogProbMetric: 16.7728, val_loss: 17.0171, val_MinusLogProbMetric: 17.0171

Epoch 552: val_loss did not improve from 16.89913
196/196 - 69s - loss: 16.7728 - MinusLogProbMetric: 16.7728 - val_loss: 17.0171 - val_MinusLogProbMetric: 17.0171 - lr: 8.3333e-05 - 69s/epoch - 351ms/step
Epoch 553/1000
2023-09-28 06:52:52.490 
Epoch 553/1000 
	 loss: 16.7810, MinusLogProbMetric: 16.7810, val_loss: 16.9933, val_MinusLogProbMetric: 16.9933

Epoch 553: val_loss did not improve from 16.89913
196/196 - 77s - loss: 16.7810 - MinusLogProbMetric: 16.7810 - val_loss: 16.9933 - val_MinusLogProbMetric: 16.9933 - lr: 8.3333e-05 - 77s/epoch - 393ms/step
Epoch 554/1000
2023-09-28 06:53:58.774 
Epoch 554/1000 
	 loss: 16.7708, MinusLogProbMetric: 16.7708, val_loss: 17.0118, val_MinusLogProbMetric: 17.0118

Epoch 554: val_loss did not improve from 16.89913
196/196 - 66s - loss: 16.7708 - MinusLogProbMetric: 16.7708 - val_loss: 17.0118 - val_MinusLogProbMetric: 17.0118 - lr: 8.3333e-05 - 66s/epoch - 338ms/step
Epoch 555/1000
2023-09-28 06:55:10.449 
Epoch 555/1000 
	 loss: 16.7852, MinusLogProbMetric: 16.7852, val_loss: 16.9736, val_MinusLogProbMetric: 16.9736

Epoch 555: val_loss did not improve from 16.89913
196/196 - 72s - loss: 16.7852 - MinusLogProbMetric: 16.7852 - val_loss: 16.9736 - val_MinusLogProbMetric: 16.9736 - lr: 8.3333e-05 - 72s/epoch - 366ms/step
Epoch 556/1000
2023-09-28 06:56:25.284 
Epoch 556/1000 
	 loss: 16.7790, MinusLogProbMetric: 16.7790, val_loss: 16.9009, val_MinusLogProbMetric: 16.9009

Epoch 556: val_loss did not improve from 16.89913
196/196 - 75s - loss: 16.7790 - MinusLogProbMetric: 16.7790 - val_loss: 16.9009 - val_MinusLogProbMetric: 16.9009 - lr: 8.3333e-05 - 75s/epoch - 382ms/step
Epoch 557/1000
2023-09-28 06:57:33.267 
Epoch 557/1000 
	 loss: 16.8788, MinusLogProbMetric: 16.8788, val_loss: 17.1662, val_MinusLogProbMetric: 17.1662

Epoch 557: val_loss did not improve from 16.89913
196/196 - 68s - loss: 16.8788 - MinusLogProbMetric: 16.8788 - val_loss: 17.1662 - val_MinusLogProbMetric: 17.1662 - lr: 8.3333e-05 - 68s/epoch - 347ms/step
Epoch 558/1000
2023-09-28 06:58:49.062 
Epoch 558/1000 
	 loss: 16.7930, MinusLogProbMetric: 16.7930, val_loss: 16.9218, val_MinusLogProbMetric: 16.9218

Epoch 558: val_loss did not improve from 16.89913
196/196 - 76s - loss: 16.7930 - MinusLogProbMetric: 16.7930 - val_loss: 16.9218 - val_MinusLogProbMetric: 16.9218 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 559/1000
2023-09-28 07:00:00.061 
Epoch 559/1000 
	 loss: 16.8026, MinusLogProbMetric: 16.8026, val_loss: 16.9421, val_MinusLogProbMetric: 16.9421

Epoch 559: val_loss did not improve from 16.89913
196/196 - 71s - loss: 16.8026 - MinusLogProbMetric: 16.8026 - val_loss: 16.9421 - val_MinusLogProbMetric: 16.9421 - lr: 8.3333e-05 - 71s/epoch - 362ms/step
Epoch 560/1000
2023-09-28 07:01:08.177 
Epoch 560/1000 
	 loss: 16.7895, MinusLogProbMetric: 16.7895, val_loss: 16.9548, val_MinusLogProbMetric: 16.9548

Epoch 560: val_loss did not improve from 16.89913
196/196 - 68s - loss: 16.7895 - MinusLogProbMetric: 16.7895 - val_loss: 16.9548 - val_MinusLogProbMetric: 16.9548 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 561/1000
2023-09-28 07:02:27.965 
Epoch 561/1000 
	 loss: 16.7599, MinusLogProbMetric: 16.7599, val_loss: 16.9457, val_MinusLogProbMetric: 16.9457

Epoch 561: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.7599 - MinusLogProbMetric: 16.7599 - val_loss: 16.9457 - val_MinusLogProbMetric: 16.9457 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 562/1000
2023-09-28 07:03:41.324 
Epoch 562/1000 
	 loss: 16.7933, MinusLogProbMetric: 16.7933, val_loss: 17.0158, val_MinusLogProbMetric: 17.0158

Epoch 562: val_loss did not improve from 16.89913
196/196 - 73s - loss: 16.7933 - MinusLogProbMetric: 16.7933 - val_loss: 17.0158 - val_MinusLogProbMetric: 17.0158 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 563/1000
2023-09-28 07:05:00.444 
Epoch 563/1000 
	 loss: 16.7865, MinusLogProbMetric: 16.7865, val_loss: 16.9988, val_MinusLogProbMetric: 16.9988

Epoch 563: val_loss did not improve from 16.89913
196/196 - 79s - loss: 16.7865 - MinusLogProbMetric: 16.7865 - val_loss: 16.9988 - val_MinusLogProbMetric: 16.9988 - lr: 8.3333e-05 - 79s/epoch - 404ms/step
Epoch 564/1000
2023-09-28 07:06:19.757 
Epoch 564/1000 
	 loss: 16.7794, MinusLogProbMetric: 16.7794, val_loss: 16.9585, val_MinusLogProbMetric: 16.9585

Epoch 564: val_loss did not improve from 16.89913
196/196 - 79s - loss: 16.7794 - MinusLogProbMetric: 16.7794 - val_loss: 16.9585 - val_MinusLogProbMetric: 16.9585 - lr: 8.3333e-05 - 79s/epoch - 405ms/step
Epoch 565/1000
2023-09-28 07:07:40.098 
Epoch 565/1000 
	 loss: 16.7964, MinusLogProbMetric: 16.7964, val_loss: 16.9063, val_MinusLogProbMetric: 16.9063

Epoch 565: val_loss did not improve from 16.89913
196/196 - 80s - loss: 16.7964 - MinusLogProbMetric: 16.7964 - val_loss: 16.9063 - val_MinusLogProbMetric: 16.9063 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 566/1000
2023-09-28 07:08:58.934 
Epoch 566/1000 
	 loss: 16.7165, MinusLogProbMetric: 16.7165, val_loss: 16.8930, val_MinusLogProbMetric: 16.8930

Epoch 566: val_loss improved from 16.89913 to 16.89300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 80s - loss: 16.7165 - MinusLogProbMetric: 16.7165 - val_loss: 16.8930 - val_MinusLogProbMetric: 16.8930 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 567/1000
2023-09-28 07:10:13.242 
Epoch 567/1000 
	 loss: 16.7055, MinusLogProbMetric: 16.7055, val_loss: 16.9043, val_MinusLogProbMetric: 16.9043

Epoch 567: val_loss did not improve from 16.89300
196/196 - 73s - loss: 16.7055 - MinusLogProbMetric: 16.7055 - val_loss: 16.9043 - val_MinusLogProbMetric: 16.9043 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 568/1000
2023-09-28 07:11:32.742 
Epoch 568/1000 
	 loss: 16.7187, MinusLogProbMetric: 16.7187, val_loss: 16.8681, val_MinusLogProbMetric: 16.8681

Epoch 568: val_loss improved from 16.89300 to 16.86812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.7187 - MinusLogProbMetric: 16.7187 - val_loss: 16.8681 - val_MinusLogProbMetric: 16.8681 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 569/1000
2023-09-28 07:12:54.046 
Epoch 569/1000 
	 loss: 16.7168, MinusLogProbMetric: 16.7168, val_loss: 16.9332, val_MinusLogProbMetric: 16.9332

Epoch 569: val_loss did not improve from 16.86812
196/196 - 80s - loss: 16.7168 - MinusLogProbMetric: 16.7168 - val_loss: 16.9332 - val_MinusLogProbMetric: 16.9332 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 570/1000
2023-09-28 07:14:14.148 
Epoch 570/1000 
	 loss: 16.7102, MinusLogProbMetric: 16.7102, val_loss: 16.8929, val_MinusLogProbMetric: 16.8929

Epoch 570: val_loss did not improve from 16.86812
196/196 - 80s - loss: 16.7102 - MinusLogProbMetric: 16.7102 - val_loss: 16.8929 - val_MinusLogProbMetric: 16.8929 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 571/1000
2023-09-28 07:15:34.428 
Epoch 571/1000 
	 loss: 16.7140, MinusLogProbMetric: 16.7140, val_loss: 16.8928, val_MinusLogProbMetric: 16.8928

Epoch 571: val_loss did not improve from 16.86812
196/196 - 80s - loss: 16.7140 - MinusLogProbMetric: 16.7140 - val_loss: 16.8928 - val_MinusLogProbMetric: 16.8928 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 572/1000
2023-09-28 07:16:54.665 
Epoch 572/1000 
	 loss: 16.7067, MinusLogProbMetric: 16.7067, val_loss: 16.8873, val_MinusLogProbMetric: 16.8873

Epoch 572: val_loss did not improve from 16.86812
196/196 - 80s - loss: 16.7067 - MinusLogProbMetric: 16.7067 - val_loss: 16.8873 - val_MinusLogProbMetric: 16.8873 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 573/1000
2023-09-28 07:18:14.387 
Epoch 573/1000 
	 loss: 16.7041, MinusLogProbMetric: 16.7041, val_loss: 16.8730, val_MinusLogProbMetric: 16.8730

Epoch 573: val_loss did not improve from 16.86812
196/196 - 80s - loss: 16.7041 - MinusLogProbMetric: 16.7041 - val_loss: 16.8730 - val_MinusLogProbMetric: 16.8730 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 574/1000
2023-09-28 07:19:34.585 
Epoch 574/1000 
	 loss: 16.7083, MinusLogProbMetric: 16.7083, val_loss: 16.8649, val_MinusLogProbMetric: 16.8649

Epoch 574: val_loss improved from 16.86812 to 16.86494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.7083 - MinusLogProbMetric: 16.7083 - val_loss: 16.8649 - val_MinusLogProbMetric: 16.8649 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 575/1000
2023-09-28 07:20:56.153 
Epoch 575/1000 
	 loss: 16.7011, MinusLogProbMetric: 16.7011, val_loss: 16.8997, val_MinusLogProbMetric: 16.8997

Epoch 575: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7011 - MinusLogProbMetric: 16.7011 - val_loss: 16.8997 - val_MinusLogProbMetric: 16.8997 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 576/1000
2023-09-28 07:22:15.975 
Epoch 576/1000 
	 loss: 16.7118, MinusLogProbMetric: 16.7118, val_loss: 16.8961, val_MinusLogProbMetric: 16.8961

Epoch 576: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7118 - MinusLogProbMetric: 16.7118 - val_loss: 16.8961 - val_MinusLogProbMetric: 16.8961 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 577/1000
2023-09-28 07:23:36.396 
Epoch 577/1000 
	 loss: 16.7064, MinusLogProbMetric: 16.7064, val_loss: 16.8886, val_MinusLogProbMetric: 16.8886

Epoch 577: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7064 - MinusLogProbMetric: 16.7064 - val_loss: 16.8886 - val_MinusLogProbMetric: 16.8886 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 578/1000
2023-09-28 07:24:57.093 
Epoch 578/1000 
	 loss: 16.7017, MinusLogProbMetric: 16.7017, val_loss: 16.9030, val_MinusLogProbMetric: 16.9030

Epoch 578: val_loss did not improve from 16.86494
196/196 - 81s - loss: 16.7017 - MinusLogProbMetric: 16.7017 - val_loss: 16.9030 - val_MinusLogProbMetric: 16.9030 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 579/1000
2023-09-28 07:26:16.957 
Epoch 579/1000 
	 loss: 16.6983, MinusLogProbMetric: 16.6983, val_loss: 16.8814, val_MinusLogProbMetric: 16.8814

Epoch 579: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.6983 - MinusLogProbMetric: 16.6983 - val_loss: 16.8814 - val_MinusLogProbMetric: 16.8814 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 580/1000
2023-09-28 07:27:36.429 
Epoch 580/1000 
	 loss: 16.7027, MinusLogProbMetric: 16.7027, val_loss: 16.8917, val_MinusLogProbMetric: 16.8917

Epoch 580: val_loss did not improve from 16.86494
196/196 - 79s - loss: 16.7027 - MinusLogProbMetric: 16.7027 - val_loss: 16.8917 - val_MinusLogProbMetric: 16.8917 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 581/1000
2023-09-28 07:28:56.222 
Epoch 581/1000 
	 loss: 16.7047, MinusLogProbMetric: 16.7047, val_loss: 16.9060, val_MinusLogProbMetric: 16.9060

Epoch 581: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7047 - MinusLogProbMetric: 16.7047 - val_loss: 16.9060 - val_MinusLogProbMetric: 16.9060 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 582/1000
2023-09-28 07:30:16.247 
Epoch 582/1000 
	 loss: 16.7102, MinusLogProbMetric: 16.7102, val_loss: 16.9208, val_MinusLogProbMetric: 16.9208

Epoch 582: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7102 - MinusLogProbMetric: 16.7102 - val_loss: 16.9208 - val_MinusLogProbMetric: 16.9208 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 583/1000
2023-09-28 07:31:36.032 
Epoch 583/1000 
	 loss: 16.7015, MinusLogProbMetric: 16.7015, val_loss: 16.8713, val_MinusLogProbMetric: 16.8713

Epoch 583: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7015 - MinusLogProbMetric: 16.7015 - val_loss: 16.8713 - val_MinusLogProbMetric: 16.8713 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 584/1000
2023-09-28 07:32:56.703 
Epoch 584/1000 
	 loss: 16.7134, MinusLogProbMetric: 16.7134, val_loss: 17.0037, val_MinusLogProbMetric: 17.0037

Epoch 584: val_loss did not improve from 16.86494
196/196 - 81s - loss: 16.7134 - MinusLogProbMetric: 16.7134 - val_loss: 17.0037 - val_MinusLogProbMetric: 17.0037 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 585/1000
2023-09-28 07:34:15.752 
Epoch 585/1000 
	 loss: 16.7331, MinusLogProbMetric: 16.7331, val_loss: 16.8882, val_MinusLogProbMetric: 16.8882

Epoch 585: val_loss did not improve from 16.86494
196/196 - 79s - loss: 16.7331 - MinusLogProbMetric: 16.7331 - val_loss: 16.8882 - val_MinusLogProbMetric: 16.8882 - lr: 4.1667e-05 - 79s/epoch - 403ms/step
Epoch 586/1000
2023-09-28 07:35:35.660 
Epoch 586/1000 
	 loss: 16.7066, MinusLogProbMetric: 16.7066, val_loss: 16.9028, val_MinusLogProbMetric: 16.9028

Epoch 586: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7066 - MinusLogProbMetric: 16.7066 - val_loss: 16.9028 - val_MinusLogProbMetric: 16.9028 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 587/1000
2023-09-28 07:36:55.553 
Epoch 587/1000 
	 loss: 16.8813, MinusLogProbMetric: 16.8813, val_loss: 16.9573, val_MinusLogProbMetric: 16.9573

Epoch 587: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.8813 - MinusLogProbMetric: 16.8813 - val_loss: 16.9573 - val_MinusLogProbMetric: 16.9573 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 588/1000
2023-09-28 07:38:15.590 
Epoch 588/1000 
	 loss: 16.7163, MinusLogProbMetric: 16.7163, val_loss: 16.9016, val_MinusLogProbMetric: 16.9016

Epoch 588: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7163 - MinusLogProbMetric: 16.7163 - val_loss: 16.9016 - val_MinusLogProbMetric: 16.9016 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 589/1000
2023-09-28 07:39:35.427 
Epoch 589/1000 
	 loss: 16.7144, MinusLogProbMetric: 16.7144, val_loss: 16.8712, val_MinusLogProbMetric: 16.8712

Epoch 589: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7144 - MinusLogProbMetric: 16.7144 - val_loss: 16.8712 - val_MinusLogProbMetric: 16.8712 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 590/1000
2023-09-28 07:40:55.293 
Epoch 590/1000 
	 loss: 16.7110, MinusLogProbMetric: 16.7110, val_loss: 16.9250, val_MinusLogProbMetric: 16.9250

Epoch 590: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7110 - MinusLogProbMetric: 16.7110 - val_loss: 16.9250 - val_MinusLogProbMetric: 16.9250 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 591/1000
2023-09-28 07:42:14.933 
Epoch 591/1000 
	 loss: 16.7057, MinusLogProbMetric: 16.7057, val_loss: 16.9063, val_MinusLogProbMetric: 16.9063

Epoch 591: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7057 - MinusLogProbMetric: 16.7057 - val_loss: 16.9063 - val_MinusLogProbMetric: 16.9063 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 592/1000
2023-09-28 07:43:35.031 
Epoch 592/1000 
	 loss: 16.7085, MinusLogProbMetric: 16.7085, val_loss: 16.8870, val_MinusLogProbMetric: 16.8870

Epoch 592: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7085 - MinusLogProbMetric: 16.7085 - val_loss: 16.8870 - val_MinusLogProbMetric: 16.8870 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 593/1000
2023-09-28 07:44:55.164 
Epoch 593/1000 
	 loss: 16.7053, MinusLogProbMetric: 16.7053, val_loss: 16.9152, val_MinusLogProbMetric: 16.9152

Epoch 593: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7053 - MinusLogProbMetric: 16.7053 - val_loss: 16.9152 - val_MinusLogProbMetric: 16.9152 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 594/1000
2023-09-28 07:46:14.810 
Epoch 594/1000 
	 loss: 16.7104, MinusLogProbMetric: 16.7104, val_loss: 16.9657, val_MinusLogProbMetric: 16.9657

Epoch 594: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7104 - MinusLogProbMetric: 16.7104 - val_loss: 16.9657 - val_MinusLogProbMetric: 16.9657 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 595/1000
2023-09-28 07:47:34.730 
Epoch 595/1000 
	 loss: 16.7119, MinusLogProbMetric: 16.7119, val_loss: 16.8972, val_MinusLogProbMetric: 16.8972

Epoch 595: val_loss did not improve from 16.86494
196/196 - 80s - loss: 16.7119 - MinusLogProbMetric: 16.7119 - val_loss: 16.8972 - val_MinusLogProbMetric: 16.8972 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 596/1000
2023-09-28 07:48:55.305 
Epoch 596/1000 
	 loss: 16.7071, MinusLogProbMetric: 16.7071, val_loss: 16.8868, val_MinusLogProbMetric: 16.8868

Epoch 596: val_loss did not improve from 16.86494
196/196 - 81s - loss: 16.7071 - MinusLogProbMetric: 16.7071 - val_loss: 16.8868 - val_MinusLogProbMetric: 16.8868 - lr: 4.1667e-05 - 81s/epoch - 411ms/step
Epoch 597/1000
2023-09-28 07:50:15.414 
Epoch 597/1000 
	 loss: 16.7074, MinusLogProbMetric: 16.7074, val_loss: 16.8636, val_MinusLogProbMetric: 16.8636

Epoch 597: val_loss improved from 16.86494 to 16.86358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.7074 - MinusLogProbMetric: 16.7074 - val_loss: 16.8636 - val_MinusLogProbMetric: 16.8636 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 598/1000
2023-09-28 07:51:36.611 
Epoch 598/1000 
	 loss: 16.7129, MinusLogProbMetric: 16.7129, val_loss: 16.8638, val_MinusLogProbMetric: 16.8638

Epoch 598: val_loss did not improve from 16.86358
196/196 - 80s - loss: 16.7129 - MinusLogProbMetric: 16.7129 - val_loss: 16.8638 - val_MinusLogProbMetric: 16.8638 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 599/1000
2023-09-28 07:52:56.805 
Epoch 599/1000 
	 loss: 16.7041, MinusLogProbMetric: 16.7041, val_loss: 16.8687, val_MinusLogProbMetric: 16.8687

Epoch 599: val_loss did not improve from 16.86358
196/196 - 80s - loss: 16.7041 - MinusLogProbMetric: 16.7041 - val_loss: 16.8687 - val_MinusLogProbMetric: 16.8687 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 600/1000
2023-09-28 07:54:15.985 
Epoch 600/1000 
	 loss: 16.7297, MinusLogProbMetric: 16.7297, val_loss: 16.9043, val_MinusLogProbMetric: 16.9043

Epoch 600: val_loss did not improve from 16.86358
196/196 - 79s - loss: 16.7297 - MinusLogProbMetric: 16.7297 - val_loss: 16.9043 - val_MinusLogProbMetric: 16.9043 - lr: 4.1667e-05 - 79s/epoch - 404ms/step
Epoch 601/1000
2023-09-28 07:55:35.778 
Epoch 601/1000 
	 loss: 16.7041, MinusLogProbMetric: 16.7041, val_loss: 16.9371, val_MinusLogProbMetric: 16.9371

Epoch 601: val_loss did not improve from 16.86358
196/196 - 80s - loss: 16.7041 - MinusLogProbMetric: 16.7041 - val_loss: 16.9371 - val_MinusLogProbMetric: 16.9371 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 602/1000
2023-09-28 07:56:55.609 
Epoch 602/1000 
	 loss: 16.7174, MinusLogProbMetric: 16.7174, val_loss: 16.8820, val_MinusLogProbMetric: 16.8820

Epoch 602: val_loss did not improve from 16.86358
196/196 - 80s - loss: 16.7174 - MinusLogProbMetric: 16.7174 - val_loss: 16.8820 - val_MinusLogProbMetric: 16.8820 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 603/1000
2023-09-28 07:58:15.231 
Epoch 603/1000 
	 loss: 16.7074, MinusLogProbMetric: 16.7074, val_loss: 16.8886, val_MinusLogProbMetric: 16.8886

Epoch 603: val_loss did not improve from 16.86358
196/196 - 80s - loss: 16.7074 - MinusLogProbMetric: 16.7074 - val_loss: 16.8886 - val_MinusLogProbMetric: 16.8886 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 604/1000
2023-09-28 07:59:34.893 
Epoch 604/1000 
	 loss: 16.7058, MinusLogProbMetric: 16.7058, val_loss: 16.9015, val_MinusLogProbMetric: 16.9015

Epoch 604: val_loss did not improve from 16.86358
196/196 - 80s - loss: 16.7058 - MinusLogProbMetric: 16.7058 - val_loss: 16.9015 - val_MinusLogProbMetric: 16.9015 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 605/1000
2023-09-28 08:00:54.715 
Epoch 605/1000 
	 loss: 16.7112, MinusLogProbMetric: 16.7112, val_loss: 16.8689, val_MinusLogProbMetric: 16.8689

Epoch 605: val_loss did not improve from 16.86358
196/196 - 80s - loss: 16.7112 - MinusLogProbMetric: 16.7112 - val_loss: 16.8689 - val_MinusLogProbMetric: 16.8689 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 606/1000
2023-09-28 08:02:14.057 
Epoch 606/1000 
	 loss: 16.6976, MinusLogProbMetric: 16.6976, val_loss: 16.8814, val_MinusLogProbMetric: 16.8814

Epoch 606: val_loss did not improve from 16.86358
196/196 - 79s - loss: 16.6976 - MinusLogProbMetric: 16.6976 - val_loss: 16.8814 - val_MinusLogProbMetric: 16.8814 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 607/1000
2023-09-28 08:03:34.079 
Epoch 607/1000 
	 loss: 16.7029, MinusLogProbMetric: 16.7029, val_loss: 16.8592, val_MinusLogProbMetric: 16.8592

Epoch 607: val_loss improved from 16.86358 to 16.85924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.7029 - MinusLogProbMetric: 16.7029 - val_loss: 16.8592 - val_MinusLogProbMetric: 16.8592 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 608/1000
2023-09-28 08:04:55.799 
Epoch 608/1000 
	 loss: 16.6969, MinusLogProbMetric: 16.6969, val_loss: 16.9088, val_MinusLogProbMetric: 16.9088

Epoch 608: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.6969 - MinusLogProbMetric: 16.6969 - val_loss: 16.9088 - val_MinusLogProbMetric: 16.9088 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 609/1000
2023-09-28 08:06:15.404 
Epoch 609/1000 
	 loss: 16.7020, MinusLogProbMetric: 16.7020, val_loss: 16.9012, val_MinusLogProbMetric: 16.9012

Epoch 609: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7020 - MinusLogProbMetric: 16.7020 - val_loss: 16.9012 - val_MinusLogProbMetric: 16.9012 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 610/1000
2023-09-28 08:07:34.911 
Epoch 610/1000 
	 loss: 16.7255, MinusLogProbMetric: 16.7255, val_loss: 16.9588, val_MinusLogProbMetric: 16.9588

Epoch 610: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7255 - MinusLogProbMetric: 16.7255 - val_loss: 16.9588 - val_MinusLogProbMetric: 16.9588 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 611/1000
2023-09-28 08:08:54.829 
Epoch 611/1000 
	 loss: 16.7184, MinusLogProbMetric: 16.7184, val_loss: 16.8657, val_MinusLogProbMetric: 16.8657

Epoch 611: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7184 - MinusLogProbMetric: 16.7184 - val_loss: 16.8657 - val_MinusLogProbMetric: 16.8657 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 612/1000
2023-09-28 08:10:14.703 
Epoch 612/1000 
	 loss: 16.7064, MinusLogProbMetric: 16.7064, val_loss: 16.8919, val_MinusLogProbMetric: 16.8919

Epoch 612: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7064 - MinusLogProbMetric: 16.7064 - val_loss: 16.8919 - val_MinusLogProbMetric: 16.8919 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 613/1000
2023-09-28 08:11:34.788 
Epoch 613/1000 
	 loss: 16.7147, MinusLogProbMetric: 16.7147, val_loss: 16.9040, val_MinusLogProbMetric: 16.9040

Epoch 613: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7147 - MinusLogProbMetric: 16.7147 - val_loss: 16.9040 - val_MinusLogProbMetric: 16.9040 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 614/1000
2023-09-28 08:12:54.867 
Epoch 614/1000 
	 loss: 16.7132, MinusLogProbMetric: 16.7132, val_loss: 16.9097, val_MinusLogProbMetric: 16.9097

Epoch 614: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7132 - MinusLogProbMetric: 16.7132 - val_loss: 16.9097 - val_MinusLogProbMetric: 16.9097 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 615/1000
2023-09-28 08:14:14.887 
Epoch 615/1000 
	 loss: 16.7107, MinusLogProbMetric: 16.7107, val_loss: 16.8640, val_MinusLogProbMetric: 16.8640

Epoch 615: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7107 - MinusLogProbMetric: 16.7107 - val_loss: 16.8640 - val_MinusLogProbMetric: 16.8640 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 616/1000
2023-09-28 08:15:34.844 
Epoch 616/1000 
	 loss: 16.7059, MinusLogProbMetric: 16.7059, val_loss: 16.8746, val_MinusLogProbMetric: 16.8746

Epoch 616: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7059 - MinusLogProbMetric: 16.7059 - val_loss: 16.8746 - val_MinusLogProbMetric: 16.8746 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 617/1000
2023-09-28 08:16:54.226 
Epoch 617/1000 
	 loss: 16.7135, MinusLogProbMetric: 16.7135, val_loss: 16.9135, val_MinusLogProbMetric: 16.9135

Epoch 617: val_loss did not improve from 16.85924
196/196 - 79s - loss: 16.7135 - MinusLogProbMetric: 16.7135 - val_loss: 16.9135 - val_MinusLogProbMetric: 16.9135 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 618/1000
2023-09-28 08:18:14.330 
Epoch 618/1000 
	 loss: 16.7089, MinusLogProbMetric: 16.7089, val_loss: 16.8625, val_MinusLogProbMetric: 16.8625

Epoch 618: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7089 - MinusLogProbMetric: 16.7089 - val_loss: 16.8625 - val_MinusLogProbMetric: 16.8625 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 619/1000
2023-09-28 08:19:33.905 
Epoch 619/1000 
	 loss: 16.7126, MinusLogProbMetric: 16.7126, val_loss: 16.8767, val_MinusLogProbMetric: 16.8767

Epoch 619: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7126 - MinusLogProbMetric: 16.7126 - val_loss: 16.8767 - val_MinusLogProbMetric: 16.8767 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 620/1000
2023-09-28 08:20:53.040 
Epoch 620/1000 
	 loss: 16.7053, MinusLogProbMetric: 16.7053, val_loss: 16.8941, val_MinusLogProbMetric: 16.8941

Epoch 620: val_loss did not improve from 16.85924
196/196 - 79s - loss: 16.7053 - MinusLogProbMetric: 16.7053 - val_loss: 16.8941 - val_MinusLogProbMetric: 16.8941 - lr: 4.1667e-05 - 79s/epoch - 404ms/step
Epoch 621/1000
2023-09-28 08:22:13.922 
Epoch 621/1000 
	 loss: 16.7036, MinusLogProbMetric: 16.7036, val_loss: 16.8770, val_MinusLogProbMetric: 16.8770

Epoch 621: val_loss did not improve from 16.85924
196/196 - 81s - loss: 16.7036 - MinusLogProbMetric: 16.7036 - val_loss: 16.8770 - val_MinusLogProbMetric: 16.8770 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 622/1000
2023-09-28 08:23:34.377 
Epoch 622/1000 
	 loss: 16.7022, MinusLogProbMetric: 16.7022, val_loss: 16.8829, val_MinusLogProbMetric: 16.8829

Epoch 622: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7022 - MinusLogProbMetric: 16.7022 - val_loss: 16.8829 - val_MinusLogProbMetric: 16.8829 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 623/1000
2023-09-28 08:24:54.319 
Epoch 623/1000 
	 loss: 16.7159, MinusLogProbMetric: 16.7159, val_loss: 16.8693, val_MinusLogProbMetric: 16.8693

Epoch 623: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7159 - MinusLogProbMetric: 16.7159 - val_loss: 16.8693 - val_MinusLogProbMetric: 16.8693 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 624/1000
2023-09-28 08:26:14.304 
Epoch 624/1000 
	 loss: 16.7058, MinusLogProbMetric: 16.7058, val_loss: 16.8866, val_MinusLogProbMetric: 16.8866

Epoch 624: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7058 - MinusLogProbMetric: 16.7058 - val_loss: 16.8866 - val_MinusLogProbMetric: 16.8866 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 625/1000
2023-09-28 08:27:35.090 
Epoch 625/1000 
	 loss: 16.7056, MinusLogProbMetric: 16.7056, val_loss: 16.8710, val_MinusLogProbMetric: 16.8710

Epoch 625: val_loss did not improve from 16.85924
196/196 - 81s - loss: 16.7056 - MinusLogProbMetric: 16.7056 - val_loss: 16.8710 - val_MinusLogProbMetric: 16.8710 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 626/1000
2023-09-28 08:28:54.987 
Epoch 626/1000 
	 loss: 16.8004, MinusLogProbMetric: 16.8004, val_loss: 16.9397, val_MinusLogProbMetric: 16.9397

Epoch 626: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.8004 - MinusLogProbMetric: 16.8004 - val_loss: 16.9397 - val_MinusLogProbMetric: 16.9397 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 627/1000
2023-09-28 08:30:15.169 
Epoch 627/1000 
	 loss: 16.7054, MinusLogProbMetric: 16.7054, val_loss: 16.8784, val_MinusLogProbMetric: 16.8784

Epoch 627: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7054 - MinusLogProbMetric: 16.7054 - val_loss: 16.8784 - val_MinusLogProbMetric: 16.8784 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 628/1000
2023-09-28 08:31:35.499 
Epoch 628/1000 
	 loss: 16.7072, MinusLogProbMetric: 16.7072, val_loss: 16.8901, val_MinusLogProbMetric: 16.8901

Epoch 628: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7072 - MinusLogProbMetric: 16.7072 - val_loss: 16.8901 - val_MinusLogProbMetric: 16.8901 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 629/1000
2023-09-28 08:32:54.655 
Epoch 629/1000 
	 loss: 16.6982, MinusLogProbMetric: 16.6982, val_loss: 16.9198, val_MinusLogProbMetric: 16.9198

Epoch 629: val_loss did not improve from 16.85924
196/196 - 79s - loss: 16.6982 - MinusLogProbMetric: 16.6982 - val_loss: 16.9198 - val_MinusLogProbMetric: 16.9198 - lr: 4.1667e-05 - 79s/epoch - 404ms/step
Epoch 630/1000
2023-09-28 08:34:14.591 
Epoch 630/1000 
	 loss: 16.7971, MinusLogProbMetric: 16.7971, val_loss: 16.9199, val_MinusLogProbMetric: 16.9199

Epoch 630: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7971 - MinusLogProbMetric: 16.7971 - val_loss: 16.9199 - val_MinusLogProbMetric: 16.9199 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 631/1000
2023-09-28 08:35:34.475 
Epoch 631/1000 
	 loss: 16.6978, MinusLogProbMetric: 16.6978, val_loss: 16.8822, val_MinusLogProbMetric: 16.8822

Epoch 631: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.6978 - MinusLogProbMetric: 16.6978 - val_loss: 16.8822 - val_MinusLogProbMetric: 16.8822 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 632/1000
2023-09-28 08:36:54.300 
Epoch 632/1000 
	 loss: 16.7010, MinusLogProbMetric: 16.7010, val_loss: 16.8730, val_MinusLogProbMetric: 16.8730

Epoch 632: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7010 - MinusLogProbMetric: 16.7010 - val_loss: 16.8730 - val_MinusLogProbMetric: 16.8730 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 633/1000
2023-09-28 08:38:14.704 
Epoch 633/1000 
	 loss: 16.6928, MinusLogProbMetric: 16.6928, val_loss: 16.8687, val_MinusLogProbMetric: 16.8687

Epoch 633: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.6928 - MinusLogProbMetric: 16.6928 - val_loss: 16.8687 - val_MinusLogProbMetric: 16.8687 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 634/1000
2023-09-28 08:39:34.103 
Epoch 634/1000 
	 loss: 16.6983, MinusLogProbMetric: 16.6983, val_loss: 16.8903, val_MinusLogProbMetric: 16.8903

Epoch 634: val_loss did not improve from 16.85924
196/196 - 79s - loss: 16.6983 - MinusLogProbMetric: 16.6983 - val_loss: 16.8903 - val_MinusLogProbMetric: 16.8903 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 635/1000
2023-09-28 08:40:53.986 
Epoch 635/1000 
	 loss: 16.6975, MinusLogProbMetric: 16.6975, val_loss: 16.8762, val_MinusLogProbMetric: 16.8762

Epoch 635: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.6975 - MinusLogProbMetric: 16.6975 - val_loss: 16.8762 - val_MinusLogProbMetric: 16.8762 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 636/1000
2023-09-28 08:42:14.368 
Epoch 636/1000 
	 loss: 16.7079, MinusLogProbMetric: 16.7079, val_loss: 16.8909, val_MinusLogProbMetric: 16.8909

Epoch 636: val_loss did not improve from 16.85924
196/196 - 80s - loss: 16.7079 - MinusLogProbMetric: 16.7079 - val_loss: 16.8909 - val_MinusLogProbMetric: 16.8909 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 637/1000
2023-09-28 08:43:33.759 
Epoch 637/1000 
	 loss: 16.6989, MinusLogProbMetric: 16.6989, val_loss: 16.8888, val_MinusLogProbMetric: 16.8888

Epoch 637: val_loss did not improve from 16.85924
196/196 - 79s - loss: 16.6989 - MinusLogProbMetric: 16.6989 - val_loss: 16.8888 - val_MinusLogProbMetric: 16.8888 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 638/1000
2023-09-28 08:44:52.506 
Epoch 638/1000 
	 loss: 16.7078, MinusLogProbMetric: 16.7078, val_loss: 16.8645, val_MinusLogProbMetric: 16.8645

Epoch 638: val_loss did not improve from 16.85924
196/196 - 79s - loss: 16.7078 - MinusLogProbMetric: 16.7078 - val_loss: 16.8645 - val_MinusLogProbMetric: 16.8645 - lr: 4.1667e-05 - 79s/epoch - 402ms/step
Epoch 639/1000
2023-09-28 08:46:10.993 
Epoch 639/1000 
	 loss: 16.6948, MinusLogProbMetric: 16.6948, val_loss: 16.8734, val_MinusLogProbMetric: 16.8734

Epoch 639: val_loss did not improve from 16.85924
196/196 - 78s - loss: 16.6948 - MinusLogProbMetric: 16.6948 - val_loss: 16.8734 - val_MinusLogProbMetric: 16.8734 - lr: 4.1667e-05 - 78s/epoch - 400ms/step
Epoch 640/1000
2023-09-28 08:47:29.410 
Epoch 640/1000 
	 loss: 16.7018, MinusLogProbMetric: 16.7018, val_loss: 16.8639, val_MinusLogProbMetric: 16.8639

Epoch 640: val_loss did not improve from 16.85924
196/196 - 78s - loss: 16.7018 - MinusLogProbMetric: 16.7018 - val_loss: 16.8639 - val_MinusLogProbMetric: 16.8639 - lr: 4.1667e-05 - 78s/epoch - 400ms/step
Epoch 641/1000
2023-09-28 08:48:43.500 
Epoch 641/1000 
	 loss: 16.7286, MinusLogProbMetric: 16.7286, val_loss: 16.8749, val_MinusLogProbMetric: 16.8749

Epoch 641: val_loss did not improve from 16.85924
196/196 - 74s - loss: 16.7286 - MinusLogProbMetric: 16.7286 - val_loss: 16.8749 - val_MinusLogProbMetric: 16.8749 - lr: 4.1667e-05 - 74s/epoch - 378ms/step
Epoch 642/1000
2023-09-28 08:49:56.181 
Epoch 642/1000 
	 loss: 16.7131, MinusLogProbMetric: 16.7131, val_loss: 16.8621, val_MinusLogProbMetric: 16.8621

Epoch 642: val_loss did not improve from 16.85924
196/196 - 73s - loss: 16.7131 - MinusLogProbMetric: 16.7131 - val_loss: 16.8621 - val_MinusLogProbMetric: 16.8621 - lr: 4.1667e-05 - 73s/epoch - 371ms/step
Epoch 643/1000
2023-09-28 08:51:19.084 
Epoch 643/1000 
	 loss: 16.7168, MinusLogProbMetric: 16.7168, val_loss: 16.8772, val_MinusLogProbMetric: 16.8772

Epoch 643: val_loss did not improve from 16.85924
196/196 - 83s - loss: 16.7168 - MinusLogProbMetric: 16.7168 - val_loss: 16.8772 - val_MinusLogProbMetric: 16.8772 - lr: 4.1667e-05 - 83s/epoch - 423ms/step
Epoch 644/1000
2023-09-28 08:52:40.147 
Epoch 644/1000 
	 loss: 16.6921, MinusLogProbMetric: 16.6921, val_loss: 16.8722, val_MinusLogProbMetric: 16.8722

Epoch 644: val_loss did not improve from 16.85924
196/196 - 81s - loss: 16.6921 - MinusLogProbMetric: 16.6921 - val_loss: 16.8722 - val_MinusLogProbMetric: 16.8722 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 645/1000
2023-09-28 08:54:00.813 
Epoch 645/1000 
	 loss: 16.7097, MinusLogProbMetric: 16.7097, val_loss: 16.8959, val_MinusLogProbMetric: 16.8959

Epoch 645: val_loss did not improve from 16.85924
196/196 - 81s - loss: 16.7097 - MinusLogProbMetric: 16.7097 - val_loss: 16.8959 - val_MinusLogProbMetric: 16.8959 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 646/1000
2023-09-28 08:55:21.433 
Epoch 646/1000 
	 loss: 16.7088, MinusLogProbMetric: 16.7088, val_loss: 16.8899, val_MinusLogProbMetric: 16.8899

Epoch 646: val_loss did not improve from 16.85924
196/196 - 81s - loss: 16.7088 - MinusLogProbMetric: 16.7088 - val_loss: 16.8899 - val_MinusLogProbMetric: 16.8899 - lr: 4.1667e-05 - 81s/epoch - 411ms/step
Epoch 647/1000
2023-09-28 08:56:43.255 
Epoch 647/1000 
	 loss: 16.6922, MinusLogProbMetric: 16.6922, val_loss: 16.8958, val_MinusLogProbMetric: 16.8958

Epoch 647: val_loss did not improve from 16.85924
196/196 - 82s - loss: 16.6922 - MinusLogProbMetric: 16.6922 - val_loss: 16.8958 - val_MinusLogProbMetric: 16.8958 - lr: 4.1667e-05 - 82s/epoch - 417ms/step
Epoch 648/1000
2023-09-28 08:58:04.069 
Epoch 648/1000 
	 loss: 16.7020, MinusLogProbMetric: 16.7020, val_loss: 16.9070, val_MinusLogProbMetric: 16.9070

Epoch 648: val_loss did not improve from 16.85924
196/196 - 81s - loss: 16.7020 - MinusLogProbMetric: 16.7020 - val_loss: 16.9070 - val_MinusLogProbMetric: 16.9070 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 649/1000
2023-09-28 08:59:24.706 
Epoch 649/1000 
	 loss: 16.7031, MinusLogProbMetric: 16.7031, val_loss: 16.9288, val_MinusLogProbMetric: 16.9288

Epoch 649: val_loss did not improve from 16.85924
196/196 - 81s - loss: 16.7031 - MinusLogProbMetric: 16.7031 - val_loss: 16.9288 - val_MinusLogProbMetric: 16.9288 - lr: 4.1667e-05 - 81s/epoch - 411ms/step
Epoch 650/1000
2023-09-28 09:00:46.476 
Epoch 650/1000 
	 loss: 16.7112, MinusLogProbMetric: 16.7112, val_loss: 16.9185, val_MinusLogProbMetric: 16.9185

Epoch 650: val_loss did not improve from 16.85924
196/196 - 82s - loss: 16.7112 - MinusLogProbMetric: 16.7112 - val_loss: 16.9185 - val_MinusLogProbMetric: 16.9185 - lr: 4.1667e-05 - 82s/epoch - 417ms/step
Epoch 651/1000
2023-09-28 09:02:07.952 
Epoch 651/1000 
	 loss: 16.7184, MinusLogProbMetric: 16.7184, val_loss: 16.8568, val_MinusLogProbMetric: 16.8568

Epoch 651: val_loss improved from 16.85924 to 16.85685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.7184 - MinusLogProbMetric: 16.7184 - val_loss: 16.8568 - val_MinusLogProbMetric: 16.8568 - lr: 4.1667e-05 - 83s/epoch - 423ms/step
Epoch 652/1000
2023-09-28 09:03:30.191 
Epoch 652/1000 
	 loss: 16.6972, MinusLogProbMetric: 16.6972, val_loss: 16.8535, val_MinusLogProbMetric: 16.8535

Epoch 652: val_loss improved from 16.85685 to 16.85349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 16.6972 - MinusLogProbMetric: 16.6972 - val_loss: 16.8535 - val_MinusLogProbMetric: 16.8535 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 653/1000
2023-09-28 09:04:52.608 
Epoch 653/1000 
	 loss: 16.7041, MinusLogProbMetric: 16.7041, val_loss: 16.8703, val_MinusLogProbMetric: 16.8703

Epoch 653: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.7041 - MinusLogProbMetric: 16.7041 - val_loss: 16.8703 - val_MinusLogProbMetric: 16.8703 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 654/1000
2023-09-28 09:06:13.713 
Epoch 654/1000 
	 loss: 16.7101, MinusLogProbMetric: 16.7101, val_loss: 16.9104, val_MinusLogProbMetric: 16.9104

Epoch 654: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.7101 - MinusLogProbMetric: 16.7101 - val_loss: 16.9104 - val_MinusLogProbMetric: 16.9104 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 655/1000
2023-09-28 09:07:34.416 
Epoch 655/1000 
	 loss: 16.7043, MinusLogProbMetric: 16.7043, val_loss: 17.0393, val_MinusLogProbMetric: 17.0393

Epoch 655: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.7043 - MinusLogProbMetric: 16.7043 - val_loss: 17.0393 - val_MinusLogProbMetric: 17.0393 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 656/1000
2023-09-28 09:08:55.511 
Epoch 656/1000 
	 loss: 16.6980, MinusLogProbMetric: 16.6980, val_loss: 16.8638, val_MinusLogProbMetric: 16.8638

Epoch 656: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.6980 - MinusLogProbMetric: 16.6980 - val_loss: 16.8638 - val_MinusLogProbMetric: 16.8638 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 657/1000
2023-09-28 09:10:17.225 
Epoch 657/1000 
	 loss: 16.6943, MinusLogProbMetric: 16.6943, val_loss: 16.9108, val_MinusLogProbMetric: 16.9108

Epoch 657: val_loss did not improve from 16.85349
196/196 - 82s - loss: 16.6943 - MinusLogProbMetric: 16.6943 - val_loss: 16.9108 - val_MinusLogProbMetric: 16.9108 - lr: 4.1667e-05 - 82s/epoch - 417ms/step
Epoch 658/1000
2023-09-28 09:11:38.728 
Epoch 658/1000 
	 loss: 16.7047, MinusLogProbMetric: 16.7047, val_loss: 16.8640, val_MinusLogProbMetric: 16.8640

Epoch 658: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.7047 - MinusLogProbMetric: 16.7047 - val_loss: 16.8640 - val_MinusLogProbMetric: 16.8640 - lr: 4.1667e-05 - 81s/epoch - 416ms/step
Epoch 659/1000
2023-09-28 09:12:59.465 
Epoch 659/1000 
	 loss: 16.7038, MinusLogProbMetric: 16.7038, val_loss: 16.8735, val_MinusLogProbMetric: 16.8735

Epoch 659: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.7038 - MinusLogProbMetric: 16.7038 - val_loss: 16.8735 - val_MinusLogProbMetric: 16.8735 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 660/1000
2023-09-28 09:14:20.312 
Epoch 660/1000 
	 loss: 16.6919, MinusLogProbMetric: 16.6919, val_loss: 16.8871, val_MinusLogProbMetric: 16.8871

Epoch 660: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.6919 - MinusLogProbMetric: 16.6919 - val_loss: 16.8871 - val_MinusLogProbMetric: 16.8871 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 661/1000
2023-09-28 09:15:41.787 
Epoch 661/1000 
	 loss: 16.6941, MinusLogProbMetric: 16.6941, val_loss: 16.8690, val_MinusLogProbMetric: 16.8690

Epoch 661: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.6941 - MinusLogProbMetric: 16.6941 - val_loss: 16.8690 - val_MinusLogProbMetric: 16.8690 - lr: 4.1667e-05 - 81s/epoch - 416ms/step
Epoch 662/1000
2023-09-28 09:17:03.252 
Epoch 662/1000 
	 loss: 16.7493, MinusLogProbMetric: 16.7493, val_loss: 17.0651, val_MinusLogProbMetric: 17.0651

Epoch 662: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.7493 - MinusLogProbMetric: 16.7493 - val_loss: 17.0651 - val_MinusLogProbMetric: 17.0651 - lr: 4.1667e-05 - 81s/epoch - 416ms/step
Epoch 663/1000
2023-09-28 09:18:24.560 
Epoch 663/1000 
	 loss: 16.7114, MinusLogProbMetric: 16.7114, val_loss: 16.8967, val_MinusLogProbMetric: 16.8967

Epoch 663: val_loss did not improve from 16.85349
196/196 - 81s - loss: 16.7114 - MinusLogProbMetric: 16.7114 - val_loss: 16.8967 - val_MinusLogProbMetric: 16.8967 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 664/1000
2023-09-28 09:19:45.664 
Epoch 664/1000 
	 loss: 16.6999, MinusLogProbMetric: 16.6999, val_loss: 16.8503, val_MinusLogProbMetric: 16.8503

Epoch 664: val_loss improved from 16.85349 to 16.85028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.6999 - MinusLogProbMetric: 16.6999 - val_loss: 16.8503 - val_MinusLogProbMetric: 16.8503 - lr: 4.1667e-05 - 83s/epoch - 421ms/step
Epoch 665/1000
2023-09-28 09:21:08.127 
Epoch 665/1000 
	 loss: 16.7053, MinusLogProbMetric: 16.7053, val_loss: 16.8821, val_MinusLogProbMetric: 16.8821

Epoch 665: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7053 - MinusLogProbMetric: 16.7053 - val_loss: 16.8821 - val_MinusLogProbMetric: 16.8821 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 666/1000
2023-09-28 09:22:29.099 
Epoch 666/1000 
	 loss: 16.6959, MinusLogProbMetric: 16.6959, val_loss: 16.8665, val_MinusLogProbMetric: 16.8665

Epoch 666: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.6959 - MinusLogProbMetric: 16.6959 - val_loss: 16.8665 - val_MinusLogProbMetric: 16.8665 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 667/1000
2023-09-28 09:23:50.327 
Epoch 667/1000 
	 loss: 16.7198, MinusLogProbMetric: 16.7198, val_loss: 16.9100, val_MinusLogProbMetric: 16.9100

Epoch 667: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7198 - MinusLogProbMetric: 16.7198 - val_loss: 16.9100 - val_MinusLogProbMetric: 16.9100 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 668/1000
2023-09-28 09:25:11.341 
Epoch 668/1000 
	 loss: 16.7045, MinusLogProbMetric: 16.7045, val_loss: 16.8626, val_MinusLogProbMetric: 16.8626

Epoch 668: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7045 - MinusLogProbMetric: 16.7045 - val_loss: 16.8626 - val_MinusLogProbMetric: 16.8626 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 669/1000
2023-09-28 09:26:32.902 
Epoch 669/1000 
	 loss: 16.7003, MinusLogProbMetric: 16.7003, val_loss: 16.9155, val_MinusLogProbMetric: 16.9155

Epoch 669: val_loss did not improve from 16.85028
196/196 - 82s - loss: 16.7003 - MinusLogProbMetric: 16.7003 - val_loss: 16.9155 - val_MinusLogProbMetric: 16.9155 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 670/1000
2023-09-28 09:27:53.698 
Epoch 670/1000 
	 loss: 16.7086, MinusLogProbMetric: 16.7086, val_loss: 16.9058, val_MinusLogProbMetric: 16.9058

Epoch 670: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7086 - MinusLogProbMetric: 16.7086 - val_loss: 16.9058 - val_MinusLogProbMetric: 16.9058 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 671/1000
2023-09-28 09:29:14.649 
Epoch 671/1000 
	 loss: 16.7032, MinusLogProbMetric: 16.7032, val_loss: 16.8748, val_MinusLogProbMetric: 16.8748

Epoch 671: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7032 - MinusLogProbMetric: 16.7032 - val_loss: 16.8748 - val_MinusLogProbMetric: 16.8748 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 672/1000
2023-09-28 09:30:35.353 
Epoch 672/1000 
	 loss: 16.6968, MinusLogProbMetric: 16.6968, val_loss: 16.8792, val_MinusLogProbMetric: 16.8792

Epoch 672: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.6968 - MinusLogProbMetric: 16.6968 - val_loss: 16.8792 - val_MinusLogProbMetric: 16.8792 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 673/1000
2023-09-28 09:31:56.350 
Epoch 673/1000 
	 loss: 16.6913, MinusLogProbMetric: 16.6913, val_loss: 16.9131, val_MinusLogProbMetric: 16.9131

Epoch 673: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.6913 - MinusLogProbMetric: 16.6913 - val_loss: 16.9131 - val_MinusLogProbMetric: 16.9131 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 674/1000
2023-09-28 09:33:16.752 
Epoch 674/1000 
	 loss: 16.7086, MinusLogProbMetric: 16.7086, val_loss: 16.9223, val_MinusLogProbMetric: 16.9223

Epoch 674: val_loss did not improve from 16.85028
196/196 - 80s - loss: 16.7086 - MinusLogProbMetric: 16.7086 - val_loss: 16.9223 - val_MinusLogProbMetric: 16.9223 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 675/1000
2023-09-28 09:34:36.602 
Epoch 675/1000 
	 loss: 16.7004, MinusLogProbMetric: 16.7004, val_loss: 17.0005, val_MinusLogProbMetric: 17.0005

Epoch 675: val_loss did not improve from 16.85028
196/196 - 80s - loss: 16.7004 - MinusLogProbMetric: 16.7004 - val_loss: 17.0005 - val_MinusLogProbMetric: 17.0005 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 676/1000
2023-09-28 09:35:57.618 
Epoch 676/1000 
	 loss: 16.6970, MinusLogProbMetric: 16.6970, val_loss: 16.8774, val_MinusLogProbMetric: 16.8774

Epoch 676: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.6970 - MinusLogProbMetric: 16.6970 - val_loss: 16.8774 - val_MinusLogProbMetric: 16.8774 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 677/1000
2023-09-28 09:37:17.936 
Epoch 677/1000 
	 loss: 16.6924, MinusLogProbMetric: 16.6924, val_loss: 16.8908, val_MinusLogProbMetric: 16.8908

Epoch 677: val_loss did not improve from 16.85028
196/196 - 80s - loss: 16.6924 - MinusLogProbMetric: 16.6924 - val_loss: 16.8908 - val_MinusLogProbMetric: 16.8908 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 678/1000
2023-09-28 09:38:38.347 
Epoch 678/1000 
	 loss: 16.7051, MinusLogProbMetric: 16.7051, val_loss: 16.8504, val_MinusLogProbMetric: 16.8504

Epoch 678: val_loss did not improve from 16.85028
196/196 - 80s - loss: 16.7051 - MinusLogProbMetric: 16.7051 - val_loss: 16.8504 - val_MinusLogProbMetric: 16.8504 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 679/1000
2023-09-28 09:39:58.736 
Epoch 679/1000 
	 loss: 16.6955, MinusLogProbMetric: 16.6955, val_loss: 16.9187, val_MinusLogProbMetric: 16.9187

Epoch 679: val_loss did not improve from 16.85028
196/196 - 80s - loss: 16.6955 - MinusLogProbMetric: 16.6955 - val_loss: 16.9187 - val_MinusLogProbMetric: 16.9187 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 680/1000
2023-09-28 09:41:19.688 
Epoch 680/1000 
	 loss: 16.6980, MinusLogProbMetric: 16.6980, val_loss: 16.9645, val_MinusLogProbMetric: 16.9645

Epoch 680: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.6980 - MinusLogProbMetric: 16.6980 - val_loss: 16.9645 - val_MinusLogProbMetric: 16.9645 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 681/1000
2023-09-28 09:42:40.415 
Epoch 681/1000 
	 loss: 16.7038, MinusLogProbMetric: 16.7038, val_loss: 16.8727, val_MinusLogProbMetric: 16.8727

Epoch 681: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7038 - MinusLogProbMetric: 16.7038 - val_loss: 16.8727 - val_MinusLogProbMetric: 16.8727 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 682/1000
2023-09-28 09:44:00.709 
Epoch 682/1000 
	 loss: 16.6997, MinusLogProbMetric: 16.6997, val_loss: 16.8680, val_MinusLogProbMetric: 16.8680

Epoch 682: val_loss did not improve from 16.85028
196/196 - 80s - loss: 16.6997 - MinusLogProbMetric: 16.6997 - val_loss: 16.8680 - val_MinusLogProbMetric: 16.8680 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 683/1000
2023-09-28 09:45:21.632 
Epoch 683/1000 
	 loss: 16.7010, MinusLogProbMetric: 16.7010, val_loss: 16.8597, val_MinusLogProbMetric: 16.8597

Epoch 683: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7010 - MinusLogProbMetric: 16.7010 - val_loss: 16.8597 - val_MinusLogProbMetric: 16.8597 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 684/1000
2023-09-28 09:46:42.144 
Epoch 684/1000 
	 loss: 16.7038, MinusLogProbMetric: 16.7038, val_loss: 16.8753, val_MinusLogProbMetric: 16.8753

Epoch 684: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7038 - MinusLogProbMetric: 16.7038 - val_loss: 16.8753 - val_MinusLogProbMetric: 16.8753 - lr: 4.1667e-05 - 81s/epoch - 411ms/step
Epoch 685/1000
2023-09-28 09:48:03.063 
Epoch 685/1000 
	 loss: 16.6943, MinusLogProbMetric: 16.6943, val_loss: 16.8676, val_MinusLogProbMetric: 16.8676

Epoch 685: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.6943 - MinusLogProbMetric: 16.6943 - val_loss: 16.8676 - val_MinusLogProbMetric: 16.8676 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 686/1000
2023-09-28 09:49:23.735 
Epoch 686/1000 
	 loss: 16.7939, MinusLogProbMetric: 16.7939, val_loss: 16.8868, val_MinusLogProbMetric: 16.8868

Epoch 686: val_loss did not improve from 16.85028
196/196 - 81s - loss: 16.7939 - MinusLogProbMetric: 16.7939 - val_loss: 16.8868 - val_MinusLogProbMetric: 16.8868 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 687/1000
2023-09-28 09:50:43.132 
Epoch 687/1000 
	 loss: 16.6992, MinusLogProbMetric: 16.6992, val_loss: 16.9343, val_MinusLogProbMetric: 16.9343

Epoch 687: val_loss did not improve from 16.85028
196/196 - 79s - loss: 16.6992 - MinusLogProbMetric: 16.6992 - val_loss: 16.9343 - val_MinusLogProbMetric: 16.9343 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 688/1000
2023-09-28 09:52:04.599 
Epoch 688/1000 
	 loss: 16.6922, MinusLogProbMetric: 16.6922, val_loss: 16.8464, val_MinusLogProbMetric: 16.8464

Epoch 688: val_loss improved from 16.85028 to 16.84639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.6922 - MinusLogProbMetric: 16.6922 - val_loss: 16.8464 - val_MinusLogProbMetric: 16.8464 - lr: 4.1667e-05 - 83s/epoch - 426ms/step
Epoch 689/1000
2023-09-28 09:53:28.145 
Epoch 689/1000 
	 loss: 16.6940, MinusLogProbMetric: 16.6940, val_loss: 16.8671, val_MinusLogProbMetric: 16.8671

Epoch 689: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6940 - MinusLogProbMetric: 16.6940 - val_loss: 16.8671 - val_MinusLogProbMetric: 16.8671 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 690/1000
2023-09-28 09:54:50.168 
Epoch 690/1000 
	 loss: 16.6936, MinusLogProbMetric: 16.6936, val_loss: 16.8749, val_MinusLogProbMetric: 16.8749

Epoch 690: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6936 - MinusLogProbMetric: 16.6936 - val_loss: 16.8749 - val_MinusLogProbMetric: 16.8749 - lr: 4.1667e-05 - 82s/epoch - 418ms/step
Epoch 691/1000
2023-09-28 09:56:12.630 
Epoch 691/1000 
	 loss: 16.6848, MinusLogProbMetric: 16.6848, val_loss: 16.8790, val_MinusLogProbMetric: 16.8790

Epoch 691: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6848 - MinusLogProbMetric: 16.6848 - val_loss: 16.8790 - val_MinusLogProbMetric: 16.8790 - lr: 4.1667e-05 - 82s/epoch - 421ms/step
Epoch 692/1000
2023-09-28 09:57:35.357 
Epoch 692/1000 
	 loss: 16.6878, MinusLogProbMetric: 16.6878, val_loss: 16.8613, val_MinusLogProbMetric: 16.8613

Epoch 692: val_loss did not improve from 16.84639
196/196 - 83s - loss: 16.6878 - MinusLogProbMetric: 16.6878 - val_loss: 16.8613 - val_MinusLogProbMetric: 16.8613 - lr: 4.1667e-05 - 83s/epoch - 422ms/step
Epoch 693/1000
2023-09-28 09:58:57.691 
Epoch 693/1000 
	 loss: 16.6967, MinusLogProbMetric: 16.6967, val_loss: 16.8561, val_MinusLogProbMetric: 16.8561

Epoch 693: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6967 - MinusLogProbMetric: 16.6967 - val_loss: 16.8561 - val_MinusLogProbMetric: 16.8561 - lr: 4.1667e-05 - 82s/epoch - 420ms/step
Epoch 694/1000
2023-09-28 10:00:19.830 
Epoch 694/1000 
	 loss: 16.7655, MinusLogProbMetric: 16.7655, val_loss: 16.8861, val_MinusLogProbMetric: 16.8861

Epoch 694: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.7655 - MinusLogProbMetric: 16.7655 - val_loss: 16.8861 - val_MinusLogProbMetric: 16.8861 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 695/1000
2023-09-28 10:01:42.612 
Epoch 695/1000 
	 loss: 16.7000, MinusLogProbMetric: 16.7000, val_loss: 16.8626, val_MinusLogProbMetric: 16.8626

Epoch 695: val_loss did not improve from 16.84639
196/196 - 83s - loss: 16.7000 - MinusLogProbMetric: 16.7000 - val_loss: 16.8626 - val_MinusLogProbMetric: 16.8626 - lr: 4.1667e-05 - 83s/epoch - 422ms/step
Epoch 696/1000
2023-09-28 10:03:04.725 
Epoch 696/1000 
	 loss: 16.7096, MinusLogProbMetric: 16.7096, val_loss: 16.8664, val_MinusLogProbMetric: 16.8664

Epoch 696: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.7096 - MinusLogProbMetric: 16.7096 - val_loss: 16.8664 - val_MinusLogProbMetric: 16.8664 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 697/1000
2023-09-28 10:04:27.327 
Epoch 697/1000 
	 loss: 16.7003, MinusLogProbMetric: 16.7003, val_loss: 16.8788, val_MinusLogProbMetric: 16.8788

Epoch 697: val_loss did not improve from 16.84639
196/196 - 83s - loss: 16.7003 - MinusLogProbMetric: 16.7003 - val_loss: 16.8788 - val_MinusLogProbMetric: 16.8788 - lr: 4.1667e-05 - 83s/epoch - 421ms/step
Epoch 698/1000
2023-09-28 10:05:49.680 
Epoch 698/1000 
	 loss: 16.6866, MinusLogProbMetric: 16.6866, val_loss: 16.8908, val_MinusLogProbMetric: 16.8908

Epoch 698: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6866 - MinusLogProbMetric: 16.6866 - val_loss: 16.8908 - val_MinusLogProbMetric: 16.8908 - lr: 4.1667e-05 - 82s/epoch - 420ms/step
Epoch 699/1000
2023-09-28 10:07:11.794 
Epoch 699/1000 
	 loss: 16.6923, MinusLogProbMetric: 16.6923, val_loss: 16.8714, val_MinusLogProbMetric: 16.8714

Epoch 699: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6923 - MinusLogProbMetric: 16.6923 - val_loss: 16.8714 - val_MinusLogProbMetric: 16.8714 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 700/1000
2023-09-28 10:08:34.005 
Epoch 700/1000 
	 loss: 16.6975, MinusLogProbMetric: 16.6975, val_loss: 16.9451, val_MinusLogProbMetric: 16.9451

Epoch 700: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6975 - MinusLogProbMetric: 16.6975 - val_loss: 16.9451 - val_MinusLogProbMetric: 16.9451 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 701/1000
2023-09-28 10:09:55.675 
Epoch 701/1000 
	 loss: 16.6897, MinusLogProbMetric: 16.6897, val_loss: 16.8558, val_MinusLogProbMetric: 16.8558

Epoch 701: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6897 - MinusLogProbMetric: 16.6897 - val_loss: 16.8558 - val_MinusLogProbMetric: 16.8558 - lr: 4.1667e-05 - 82s/epoch - 417ms/step
Epoch 702/1000
2023-09-28 10:11:16.898 
Epoch 702/1000 
	 loss: 16.6881, MinusLogProbMetric: 16.6881, val_loss: 16.8933, val_MinusLogProbMetric: 16.8933

Epoch 702: val_loss did not improve from 16.84639
196/196 - 81s - loss: 16.6881 - MinusLogProbMetric: 16.6881 - val_loss: 16.8933 - val_MinusLogProbMetric: 16.8933 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 703/1000
2023-09-28 10:12:37.754 
Epoch 703/1000 
	 loss: 16.6929, MinusLogProbMetric: 16.6929, val_loss: 16.9092, val_MinusLogProbMetric: 16.9092

Epoch 703: val_loss did not improve from 16.84639
196/196 - 81s - loss: 16.6929 - MinusLogProbMetric: 16.6929 - val_loss: 16.9092 - val_MinusLogProbMetric: 16.9092 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 704/1000
2023-09-28 10:13:57.628 
Epoch 704/1000 
	 loss: 16.6965, MinusLogProbMetric: 16.6965, val_loss: 16.9029, val_MinusLogProbMetric: 16.9029

Epoch 704: val_loss did not improve from 16.84639
196/196 - 80s - loss: 16.6965 - MinusLogProbMetric: 16.6965 - val_loss: 16.9029 - val_MinusLogProbMetric: 16.9029 - lr: 4.1667e-05 - 80s/epoch - 408ms/step
Epoch 705/1000
2023-09-28 10:15:19.157 
Epoch 705/1000 
	 loss: 16.6975, MinusLogProbMetric: 16.6975, val_loss: 16.8779, val_MinusLogProbMetric: 16.8779

Epoch 705: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6975 - MinusLogProbMetric: 16.6975 - val_loss: 16.8779 - val_MinusLogProbMetric: 16.8779 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 706/1000
2023-09-28 10:16:40.552 
Epoch 706/1000 
	 loss: 16.7095, MinusLogProbMetric: 16.7095, val_loss: 16.9887, val_MinusLogProbMetric: 16.9887

Epoch 706: val_loss did not improve from 16.84639
196/196 - 81s - loss: 16.7095 - MinusLogProbMetric: 16.7095 - val_loss: 16.9887 - val_MinusLogProbMetric: 16.9887 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 707/1000
2023-09-28 10:18:01.594 
Epoch 707/1000 
	 loss: 16.6902, MinusLogProbMetric: 16.6902, val_loss: 16.8503, val_MinusLogProbMetric: 16.8503

Epoch 707: val_loss did not improve from 16.84639
196/196 - 81s - loss: 16.6902 - MinusLogProbMetric: 16.6902 - val_loss: 16.8503 - val_MinusLogProbMetric: 16.8503 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 708/1000
2023-09-28 10:19:21.753 
Epoch 708/1000 
	 loss: 16.7020, MinusLogProbMetric: 16.7020, val_loss: 16.8712, val_MinusLogProbMetric: 16.8712

Epoch 708: val_loss did not improve from 16.84639
196/196 - 80s - loss: 16.7020 - MinusLogProbMetric: 16.7020 - val_loss: 16.8712 - val_MinusLogProbMetric: 16.8712 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 709/1000
2023-09-28 10:20:41.017 
Epoch 709/1000 
	 loss: 16.6844, MinusLogProbMetric: 16.6844, val_loss: 16.8643, val_MinusLogProbMetric: 16.8643

Epoch 709: val_loss did not improve from 16.84639
196/196 - 79s - loss: 16.6844 - MinusLogProbMetric: 16.6844 - val_loss: 16.8643 - val_MinusLogProbMetric: 16.8643 - lr: 4.1667e-05 - 79s/epoch - 404ms/step
Epoch 710/1000
2023-09-28 10:22:01.208 
Epoch 710/1000 
	 loss: 16.6942, MinusLogProbMetric: 16.6942, val_loss: 16.9111, val_MinusLogProbMetric: 16.9111

Epoch 710: val_loss did not improve from 16.84639
196/196 - 80s - loss: 16.6942 - MinusLogProbMetric: 16.6942 - val_loss: 16.9111 - val_MinusLogProbMetric: 16.9111 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 711/1000
2023-09-28 10:23:20.929 
Epoch 711/1000 
	 loss: 16.7066, MinusLogProbMetric: 16.7066, val_loss: 16.9390, val_MinusLogProbMetric: 16.9390

Epoch 711: val_loss did not improve from 16.84639
196/196 - 80s - loss: 16.7066 - MinusLogProbMetric: 16.7066 - val_loss: 16.9390 - val_MinusLogProbMetric: 16.9390 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 712/1000
2023-09-28 10:24:41.225 
Epoch 712/1000 
	 loss: 16.7003, MinusLogProbMetric: 16.7003, val_loss: 16.8942, val_MinusLogProbMetric: 16.8942

Epoch 712: val_loss did not improve from 16.84639
196/196 - 80s - loss: 16.7003 - MinusLogProbMetric: 16.7003 - val_loss: 16.8942 - val_MinusLogProbMetric: 16.8942 - lr: 4.1667e-05 - 80s/epoch - 410ms/step
Epoch 713/1000
2023-09-28 10:25:59.800 
Epoch 713/1000 
	 loss: 16.6903, MinusLogProbMetric: 16.6903, val_loss: 16.8930, val_MinusLogProbMetric: 16.8930

Epoch 713: val_loss did not improve from 16.84639
196/196 - 79s - loss: 16.6903 - MinusLogProbMetric: 16.6903 - val_loss: 16.8930 - val_MinusLogProbMetric: 16.8930 - lr: 4.1667e-05 - 79s/epoch - 401ms/step
Epoch 714/1000
2023-09-28 10:27:21.752 
Epoch 714/1000 
	 loss: 16.6973, MinusLogProbMetric: 16.6973, val_loss: 16.9445, val_MinusLogProbMetric: 16.9445

Epoch 714: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6973 - MinusLogProbMetric: 16.6973 - val_loss: 16.9445 - val_MinusLogProbMetric: 16.9445 - lr: 4.1667e-05 - 82s/epoch - 418ms/step
Epoch 715/1000
2023-09-28 10:28:43.260 
Epoch 715/1000 
	 loss: 16.6963, MinusLogProbMetric: 16.6963, val_loss: 16.8894, val_MinusLogProbMetric: 16.8894

Epoch 715: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6963 - MinusLogProbMetric: 16.6963 - val_loss: 16.8894 - val_MinusLogProbMetric: 16.8894 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 716/1000
2023-09-28 10:30:04.853 
Epoch 716/1000 
	 loss: 16.6995, MinusLogProbMetric: 16.6995, val_loss: 16.8756, val_MinusLogProbMetric: 16.8756

Epoch 716: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.6995 - MinusLogProbMetric: 16.6995 - val_loss: 16.8756 - val_MinusLogProbMetric: 16.8756 - lr: 4.1667e-05 - 82s/epoch - 416ms/step
Epoch 717/1000
2023-09-28 10:31:27.251 
Epoch 717/1000 
	 loss: 16.7029, MinusLogProbMetric: 16.7029, val_loss: 17.1148, val_MinusLogProbMetric: 17.1148

Epoch 717: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.7029 - MinusLogProbMetric: 16.7029 - val_loss: 17.1148 - val_MinusLogProbMetric: 17.1148 - lr: 4.1667e-05 - 82s/epoch - 420ms/step
Epoch 718/1000
2023-09-28 10:32:49.489 
Epoch 718/1000 
	 loss: 16.7202, MinusLogProbMetric: 16.7202, val_loss: 16.8489, val_MinusLogProbMetric: 16.8489

Epoch 718: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.7202 - MinusLogProbMetric: 16.7202 - val_loss: 16.8489 - val_MinusLogProbMetric: 16.8489 - lr: 4.1667e-05 - 82s/epoch - 420ms/step
Epoch 719/1000
2023-09-28 10:34:11.729 
Epoch 719/1000 
	 loss: 16.7114, MinusLogProbMetric: 16.7114, val_loss: 16.8822, val_MinusLogProbMetric: 16.8822

Epoch 719: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.7114 - MinusLogProbMetric: 16.7114 - val_loss: 16.8822 - val_MinusLogProbMetric: 16.8822 - lr: 4.1667e-05 - 82s/epoch - 420ms/step
Epoch 720/1000
2023-09-28 10:35:33.480 
Epoch 720/1000 
	 loss: 16.7058, MinusLogProbMetric: 16.7058, val_loss: 16.9625, val_MinusLogProbMetric: 16.9625

Epoch 720: val_loss did not improve from 16.84639
196/196 - 82s - loss: 16.7058 - MinusLogProbMetric: 16.7058 - val_loss: 16.9625 - val_MinusLogProbMetric: 16.9625 - lr: 4.1667e-05 - 82s/epoch - 417ms/step
Epoch 721/1000
2023-09-28 10:36:52.949 
Epoch 721/1000 
	 loss: 16.6930, MinusLogProbMetric: 16.6930, val_loss: 16.8779, val_MinusLogProbMetric: 16.8779

Epoch 721: val_loss did not improve from 16.84639
196/196 - 79s - loss: 16.6930 - MinusLogProbMetric: 16.6930 - val_loss: 16.8779 - val_MinusLogProbMetric: 16.8779 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 722/1000
2023-09-28 10:38:09.652 
Epoch 722/1000 
	 loss: 16.6883, MinusLogProbMetric: 16.6883, val_loss: 16.8800, val_MinusLogProbMetric: 16.8800

Epoch 722: val_loss did not improve from 16.84639
196/196 - 77s - loss: 16.6883 - MinusLogProbMetric: 16.6883 - val_loss: 16.8800 - val_MinusLogProbMetric: 16.8800 - lr: 4.1667e-05 - 77s/epoch - 391ms/step
Epoch 723/1000
2023-09-28 10:39:28.992 
Epoch 723/1000 
	 loss: 16.7064, MinusLogProbMetric: 16.7064, val_loss: 16.8888, val_MinusLogProbMetric: 16.8888

Epoch 723: val_loss did not improve from 16.84639
196/196 - 79s - loss: 16.7064 - MinusLogProbMetric: 16.7064 - val_loss: 16.8888 - val_MinusLogProbMetric: 16.8888 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 724/1000
2023-09-28 10:40:50.079 
Epoch 724/1000 
	 loss: 16.6918, MinusLogProbMetric: 16.6918, val_loss: 16.8663, val_MinusLogProbMetric: 16.8663

Epoch 724: val_loss did not improve from 16.84639
196/196 - 81s - loss: 16.6918 - MinusLogProbMetric: 16.6918 - val_loss: 16.8663 - val_MinusLogProbMetric: 16.8663 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 725/1000
2023-09-28 10:42:08.407 
Epoch 725/1000 
	 loss: 16.6950, MinusLogProbMetric: 16.6950, val_loss: 16.8685, val_MinusLogProbMetric: 16.8685

Epoch 725: val_loss did not improve from 16.84639
196/196 - 78s - loss: 16.6950 - MinusLogProbMetric: 16.6950 - val_loss: 16.8685 - val_MinusLogProbMetric: 16.8685 - lr: 4.1667e-05 - 78s/epoch - 400ms/step
Epoch 726/1000
2023-09-28 10:43:28.872 
Epoch 726/1000 
	 loss: 16.7033, MinusLogProbMetric: 16.7033, val_loss: 16.9594, val_MinusLogProbMetric: 16.9594

Epoch 726: val_loss did not improve from 16.84639
196/196 - 80s - loss: 16.7033 - MinusLogProbMetric: 16.7033 - val_loss: 16.9594 - val_MinusLogProbMetric: 16.9594 - lr: 4.1667e-05 - 80s/epoch - 411ms/step
Epoch 727/1000
2023-09-28 10:44:45.208 
Epoch 727/1000 
	 loss: 16.7048, MinusLogProbMetric: 16.7048, val_loss: 16.9089, val_MinusLogProbMetric: 16.9089

Epoch 727: val_loss did not improve from 16.84639
196/196 - 76s - loss: 16.7048 - MinusLogProbMetric: 16.7048 - val_loss: 16.9089 - val_MinusLogProbMetric: 16.9089 - lr: 4.1667e-05 - 76s/epoch - 389ms/step
Epoch 728/1000
2023-09-28 10:45:58.460 
Epoch 728/1000 
	 loss: 16.6966, MinusLogProbMetric: 16.6966, val_loss: 16.9118, val_MinusLogProbMetric: 16.9118

Epoch 728: val_loss did not improve from 16.84639
196/196 - 73s - loss: 16.6966 - MinusLogProbMetric: 16.6966 - val_loss: 16.9118 - val_MinusLogProbMetric: 16.9118 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 729/1000
2023-09-28 10:47:08.179 
Epoch 729/1000 
	 loss: 16.6880, MinusLogProbMetric: 16.6880, val_loss: 16.9173, val_MinusLogProbMetric: 16.9173

Epoch 729: val_loss did not improve from 16.84639
196/196 - 70s - loss: 16.6880 - MinusLogProbMetric: 16.6880 - val_loss: 16.9173 - val_MinusLogProbMetric: 16.9173 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 730/1000
2023-09-28 10:48:22.431 
Epoch 730/1000 
	 loss: 16.6861, MinusLogProbMetric: 16.6861, val_loss: 16.9743, val_MinusLogProbMetric: 16.9743

Epoch 730: val_loss did not improve from 16.84639
196/196 - 74s - loss: 16.6861 - MinusLogProbMetric: 16.6861 - val_loss: 16.9743 - val_MinusLogProbMetric: 16.9743 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 731/1000
2023-09-28 10:49:33.237 
Epoch 731/1000 
	 loss: 16.7065, MinusLogProbMetric: 16.7065, val_loss: 16.9359, val_MinusLogProbMetric: 16.9359

Epoch 731: val_loss did not improve from 16.84639
196/196 - 71s - loss: 16.7065 - MinusLogProbMetric: 16.7065 - val_loss: 16.9359 - val_MinusLogProbMetric: 16.9359 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 732/1000
2023-09-28 10:50:45.706 
Epoch 732/1000 
	 loss: 16.6896, MinusLogProbMetric: 16.6896, val_loss: 16.9346, val_MinusLogProbMetric: 16.9346

Epoch 732: val_loss did not improve from 16.84639
196/196 - 72s - loss: 16.6896 - MinusLogProbMetric: 16.6896 - val_loss: 16.9346 - val_MinusLogProbMetric: 16.9346 - lr: 4.1667e-05 - 72s/epoch - 370ms/step
Epoch 733/1000
2023-09-28 10:51:59.612 
Epoch 733/1000 
	 loss: 16.6937, MinusLogProbMetric: 16.6937, val_loss: 16.8735, val_MinusLogProbMetric: 16.8735

Epoch 733: val_loss did not improve from 16.84639
196/196 - 74s - loss: 16.6937 - MinusLogProbMetric: 16.6937 - val_loss: 16.8735 - val_MinusLogProbMetric: 16.8735 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 734/1000
2023-09-28 10:53:13.532 
Epoch 734/1000 
	 loss: 16.6922, MinusLogProbMetric: 16.6922, val_loss: 16.8459, val_MinusLogProbMetric: 16.8459

Epoch 734: val_loss improved from 16.84639 to 16.84586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 75s - loss: 16.6922 - MinusLogProbMetric: 16.6922 - val_loss: 16.8459 - val_MinusLogProbMetric: 16.8459 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 735/1000
2023-09-28 10:54:29.394 
Epoch 735/1000 
	 loss: 16.6811, MinusLogProbMetric: 16.6811, val_loss: 16.8596, val_MinusLogProbMetric: 16.8596

Epoch 735: val_loss did not improve from 16.84586
196/196 - 75s - loss: 16.6811 - MinusLogProbMetric: 16.6811 - val_loss: 16.8596 - val_MinusLogProbMetric: 16.8596 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 736/1000
2023-09-28 10:55:38.727 
Epoch 736/1000 
	 loss: 16.6954, MinusLogProbMetric: 16.6954, val_loss: 16.8667, val_MinusLogProbMetric: 16.8667

Epoch 736: val_loss did not improve from 16.84586
196/196 - 69s - loss: 16.6954 - MinusLogProbMetric: 16.6954 - val_loss: 16.8667 - val_MinusLogProbMetric: 16.8667 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 737/1000
2023-09-28 10:56:49.279 
Epoch 737/1000 
	 loss: 16.6926, MinusLogProbMetric: 16.6926, val_loss: 16.8704, val_MinusLogProbMetric: 16.8704

Epoch 737: val_loss did not improve from 16.84586
196/196 - 71s - loss: 16.6926 - MinusLogProbMetric: 16.6926 - val_loss: 16.8704 - val_MinusLogProbMetric: 16.8704 - lr: 4.1667e-05 - 71s/epoch - 360ms/step
Epoch 738/1000
2023-09-28 10:57:59.681 
Epoch 738/1000 
	 loss: 16.6919, MinusLogProbMetric: 16.6919, val_loss: 16.8860, val_MinusLogProbMetric: 16.8860

Epoch 738: val_loss did not improve from 16.84586
196/196 - 70s - loss: 16.6919 - MinusLogProbMetric: 16.6919 - val_loss: 16.8860 - val_MinusLogProbMetric: 16.8860 - lr: 4.1667e-05 - 70s/epoch - 359ms/step
Epoch 739/1000
2023-09-28 10:59:09.904 
Epoch 739/1000 
	 loss: 16.7864, MinusLogProbMetric: 16.7864, val_loss: 16.8666, val_MinusLogProbMetric: 16.8666

Epoch 739: val_loss did not improve from 16.84586
196/196 - 70s - loss: 16.7864 - MinusLogProbMetric: 16.7864 - val_loss: 16.8666 - val_MinusLogProbMetric: 16.8666 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 740/1000
2023-09-28 11:00:20.733 
Epoch 740/1000 
	 loss: 16.7082, MinusLogProbMetric: 16.7082, val_loss: 16.9110, val_MinusLogProbMetric: 16.9110

Epoch 740: val_loss did not improve from 16.84586
196/196 - 71s - loss: 16.7082 - MinusLogProbMetric: 16.7082 - val_loss: 16.9110 - val_MinusLogProbMetric: 16.9110 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 741/1000
2023-09-28 11:01:30.425 
Epoch 741/1000 
	 loss: 16.6881, MinusLogProbMetric: 16.6881, val_loss: 16.8765, val_MinusLogProbMetric: 16.8765

Epoch 741: val_loss did not improve from 16.84586
196/196 - 70s - loss: 16.6881 - MinusLogProbMetric: 16.6881 - val_loss: 16.8765 - val_MinusLogProbMetric: 16.8765 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 742/1000
2023-09-28 11:02:44.246 
Epoch 742/1000 
	 loss: 16.6871, MinusLogProbMetric: 16.6871, val_loss: 16.9472, val_MinusLogProbMetric: 16.9472

Epoch 742: val_loss did not improve from 16.84586
196/196 - 74s - loss: 16.6871 - MinusLogProbMetric: 16.6871 - val_loss: 16.9472 - val_MinusLogProbMetric: 16.9472 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 743/1000
2023-09-28 11:03:55.254 
Epoch 743/1000 
	 loss: 16.6806, MinusLogProbMetric: 16.6806, val_loss: 16.8594, val_MinusLogProbMetric: 16.8594

Epoch 743: val_loss did not improve from 16.84586
196/196 - 71s - loss: 16.6806 - MinusLogProbMetric: 16.6806 - val_loss: 16.8594 - val_MinusLogProbMetric: 16.8594 - lr: 4.1667e-05 - 71s/epoch - 362ms/step
Epoch 744/1000
2023-09-28 11:05:08.300 
Epoch 744/1000 
	 loss: 16.6879, MinusLogProbMetric: 16.6879, val_loss: 16.9184, val_MinusLogProbMetric: 16.9184

Epoch 744: val_loss did not improve from 16.84586
196/196 - 73s - loss: 16.6879 - MinusLogProbMetric: 16.6879 - val_loss: 16.9184 - val_MinusLogProbMetric: 16.9184 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 745/1000
2023-09-28 11:06:21.125 
Epoch 745/1000 
	 loss: 16.6868, MinusLogProbMetric: 16.6868, val_loss: 16.8718, val_MinusLogProbMetric: 16.8718

Epoch 745: val_loss did not improve from 16.84586
196/196 - 73s - loss: 16.6868 - MinusLogProbMetric: 16.6868 - val_loss: 16.8718 - val_MinusLogProbMetric: 16.8718 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 746/1000
2023-09-28 11:07:37.155 
Epoch 746/1000 
	 loss: 16.6980, MinusLogProbMetric: 16.6980, val_loss: 16.8808, val_MinusLogProbMetric: 16.8808

Epoch 746: val_loss did not improve from 16.84586
196/196 - 76s - loss: 16.6980 - MinusLogProbMetric: 16.6980 - val_loss: 16.8808 - val_MinusLogProbMetric: 16.8808 - lr: 4.1667e-05 - 76s/epoch - 388ms/step
Epoch 747/1000
2023-09-28 11:08:46.821 
Epoch 747/1000 
	 loss: 16.6885, MinusLogProbMetric: 16.6885, val_loss: 16.8526, val_MinusLogProbMetric: 16.8526

Epoch 747: val_loss did not improve from 16.84586
196/196 - 70s - loss: 16.6885 - MinusLogProbMetric: 16.6885 - val_loss: 16.8526 - val_MinusLogProbMetric: 16.8526 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 748/1000
2023-09-28 11:09:57.503 
Epoch 748/1000 
	 loss: 16.7060, MinusLogProbMetric: 16.7060, val_loss: 16.8815, val_MinusLogProbMetric: 16.8815

Epoch 748: val_loss did not improve from 16.84586
196/196 - 71s - loss: 16.7060 - MinusLogProbMetric: 16.7060 - val_loss: 16.8815 - val_MinusLogProbMetric: 16.8815 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 749/1000
2023-09-28 11:11:06.922 
Epoch 749/1000 
	 loss: 16.6931, MinusLogProbMetric: 16.6931, val_loss: 16.8953, val_MinusLogProbMetric: 16.8953

Epoch 749: val_loss did not improve from 16.84586
196/196 - 69s - loss: 16.6931 - MinusLogProbMetric: 16.6931 - val_loss: 16.8953 - val_MinusLogProbMetric: 16.8953 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 750/1000
2023-09-28 11:12:17.353 
Epoch 750/1000 
	 loss: 16.7005, MinusLogProbMetric: 16.7005, val_loss: 16.8590, val_MinusLogProbMetric: 16.8590

Epoch 750: val_loss did not improve from 16.84586
196/196 - 70s - loss: 16.7005 - MinusLogProbMetric: 16.7005 - val_loss: 16.8590 - val_MinusLogProbMetric: 16.8590 - lr: 4.1667e-05 - 70s/epoch - 359ms/step
Epoch 751/1000
2023-09-28 11:13:35.203 
Epoch 751/1000 
	 loss: 16.6892, MinusLogProbMetric: 16.6892, val_loss: 16.8992, val_MinusLogProbMetric: 16.8992

Epoch 751: val_loss did not improve from 16.84586
196/196 - 78s - loss: 16.6892 - MinusLogProbMetric: 16.6892 - val_loss: 16.8992 - val_MinusLogProbMetric: 16.8992 - lr: 4.1667e-05 - 78s/epoch - 397ms/step
Epoch 752/1000
2023-09-28 11:14:56.019 
Epoch 752/1000 
	 loss: 16.6788, MinusLogProbMetric: 16.6788, val_loss: 16.8818, val_MinusLogProbMetric: 16.8818

Epoch 752: val_loss did not improve from 16.84586
196/196 - 81s - loss: 16.6788 - MinusLogProbMetric: 16.6788 - val_loss: 16.8818 - val_MinusLogProbMetric: 16.8818 - lr: 4.1667e-05 - 81s/epoch - 412ms/step
Epoch 753/1000
2023-09-28 11:16:13.304 
Epoch 753/1000 
	 loss: 16.6904, MinusLogProbMetric: 16.6904, val_loss: 16.8398, val_MinusLogProbMetric: 16.8398

Epoch 753: val_loss improved from 16.84586 to 16.83982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 78s - loss: 16.6904 - MinusLogProbMetric: 16.6904 - val_loss: 16.8398 - val_MinusLogProbMetric: 16.8398 - lr: 4.1667e-05 - 78s/epoch - 400ms/step
Epoch 754/1000
2023-09-28 11:17:32.569 
Epoch 754/1000 
	 loss: 16.6983, MinusLogProbMetric: 16.6983, val_loss: 16.8744, val_MinusLogProbMetric: 16.8744

Epoch 754: val_loss did not improve from 16.83982
196/196 - 78s - loss: 16.6983 - MinusLogProbMetric: 16.6983 - val_loss: 16.8744 - val_MinusLogProbMetric: 16.8744 - lr: 4.1667e-05 - 78s/epoch - 399ms/step
Epoch 755/1000
2023-09-28 11:18:51.619 
Epoch 755/1000 
	 loss: 16.6800, MinusLogProbMetric: 16.6800, val_loss: 16.8608, val_MinusLogProbMetric: 16.8608

Epoch 755: val_loss did not improve from 16.83982
196/196 - 79s - loss: 16.6800 - MinusLogProbMetric: 16.6800 - val_loss: 16.8608 - val_MinusLogProbMetric: 16.8608 - lr: 4.1667e-05 - 79s/epoch - 403ms/step
Epoch 756/1000
2023-09-28 11:20:08.536 
Epoch 756/1000 
	 loss: 16.6824, MinusLogProbMetric: 16.6824, val_loss: 16.8689, val_MinusLogProbMetric: 16.8689

Epoch 756: val_loss did not improve from 16.83982
196/196 - 77s - loss: 16.6824 - MinusLogProbMetric: 16.6824 - val_loss: 16.8689 - val_MinusLogProbMetric: 16.8689 - lr: 4.1667e-05 - 77s/epoch - 392ms/step
Epoch 757/1000
2023-09-28 11:21:24.815 
Epoch 757/1000 
	 loss: 16.6946, MinusLogProbMetric: 16.6946, val_loss: 16.8809, val_MinusLogProbMetric: 16.8809

Epoch 757: val_loss did not improve from 16.83982
196/196 - 76s - loss: 16.6946 - MinusLogProbMetric: 16.6946 - val_loss: 16.8809 - val_MinusLogProbMetric: 16.8809 - lr: 4.1667e-05 - 76s/epoch - 389ms/step
Epoch 758/1000
2023-09-28 11:22:43.092 
Epoch 758/1000 
	 loss: 16.9552, MinusLogProbMetric: 16.9552, val_loss: 16.9976, val_MinusLogProbMetric: 16.9976

Epoch 758: val_loss did not improve from 16.83982
196/196 - 78s - loss: 16.9552 - MinusLogProbMetric: 16.9552 - val_loss: 16.9976 - val_MinusLogProbMetric: 16.9976 - lr: 4.1667e-05 - 78s/epoch - 399ms/step
Epoch 759/1000
2023-09-28 11:24:00.243 
Epoch 759/1000 
	 loss: 16.7329, MinusLogProbMetric: 16.7329, val_loss: 16.8771, val_MinusLogProbMetric: 16.8771

Epoch 759: val_loss did not improve from 16.83982
196/196 - 77s - loss: 16.7329 - MinusLogProbMetric: 16.7329 - val_loss: 16.8771 - val_MinusLogProbMetric: 16.8771 - lr: 4.1667e-05 - 77s/epoch - 394ms/step
Epoch 760/1000
2023-09-28 11:25:22.206 
Epoch 760/1000 
	 loss: 16.6862, MinusLogProbMetric: 16.6862, val_loss: 16.8575, val_MinusLogProbMetric: 16.8575

Epoch 760: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6862 - MinusLogProbMetric: 16.6862 - val_loss: 16.8575 - val_MinusLogProbMetric: 16.8575 - lr: 4.1667e-05 - 82s/epoch - 418ms/step
Epoch 761/1000
2023-09-28 11:26:41.916 
Epoch 761/1000 
	 loss: 16.6857, MinusLogProbMetric: 16.6857, val_loss: 16.9199, val_MinusLogProbMetric: 16.9199

Epoch 761: val_loss did not improve from 16.83982
196/196 - 80s - loss: 16.6857 - MinusLogProbMetric: 16.6857 - val_loss: 16.9199 - val_MinusLogProbMetric: 16.9199 - lr: 4.1667e-05 - 80s/epoch - 407ms/step
Epoch 762/1000
2023-09-28 11:28:04.100 
Epoch 762/1000 
	 loss: 16.6845, MinusLogProbMetric: 16.6845, val_loss: 16.8432, val_MinusLogProbMetric: 16.8432

Epoch 762: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6845 - MinusLogProbMetric: 16.6845 - val_loss: 16.8432 - val_MinusLogProbMetric: 16.8432 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 763/1000
2023-09-28 11:29:25.966 
Epoch 763/1000 
	 loss: 16.6882, MinusLogProbMetric: 16.6882, val_loss: 16.8992, val_MinusLogProbMetric: 16.8992

Epoch 763: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6882 - MinusLogProbMetric: 16.6882 - val_loss: 16.8992 - val_MinusLogProbMetric: 16.8992 - lr: 4.1667e-05 - 82s/epoch - 418ms/step
Epoch 764/1000
2023-09-28 11:30:48.229 
Epoch 764/1000 
	 loss: 16.6868, MinusLogProbMetric: 16.6868, val_loss: 16.8842, val_MinusLogProbMetric: 16.8842

Epoch 764: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6868 - MinusLogProbMetric: 16.6868 - val_loss: 16.8842 - val_MinusLogProbMetric: 16.8842 - lr: 4.1667e-05 - 82s/epoch - 420ms/step
Epoch 765/1000
2023-09-28 11:32:09.398 
Epoch 765/1000 
	 loss: 16.7272, MinusLogProbMetric: 16.7272, val_loss: 16.8819, val_MinusLogProbMetric: 16.8819

Epoch 765: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.7272 - MinusLogProbMetric: 16.7272 - val_loss: 16.8819 - val_MinusLogProbMetric: 16.8819 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 766/1000
2023-09-28 11:33:30.599 
Epoch 766/1000 
	 loss: 16.6821, MinusLogProbMetric: 16.6821, val_loss: 16.8722, val_MinusLogProbMetric: 16.8722

Epoch 766: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6821 - MinusLogProbMetric: 16.6821 - val_loss: 16.8722 - val_MinusLogProbMetric: 16.8722 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 767/1000
2023-09-28 11:34:52.493 
Epoch 767/1000 
	 loss: 16.6763, MinusLogProbMetric: 16.6763, val_loss: 16.8536, val_MinusLogProbMetric: 16.8536

Epoch 767: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6763 - MinusLogProbMetric: 16.6763 - val_loss: 16.8536 - val_MinusLogProbMetric: 16.8536 - lr: 4.1667e-05 - 82s/epoch - 418ms/step
Epoch 768/1000
2023-09-28 11:36:13.047 
Epoch 768/1000 
	 loss: 16.7649, MinusLogProbMetric: 16.7649, val_loss: 16.8506, val_MinusLogProbMetric: 16.8506

Epoch 768: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.7649 - MinusLogProbMetric: 16.7649 - val_loss: 16.8506 - val_MinusLogProbMetric: 16.8506 - lr: 4.1667e-05 - 81s/epoch - 411ms/step
Epoch 769/1000
2023-09-28 11:37:32.022 
Epoch 769/1000 
	 loss: 16.6879, MinusLogProbMetric: 16.6879, val_loss: 16.8701, val_MinusLogProbMetric: 16.8701

Epoch 769: val_loss did not improve from 16.83982
196/196 - 79s - loss: 16.6879 - MinusLogProbMetric: 16.6879 - val_loss: 16.8701 - val_MinusLogProbMetric: 16.8701 - lr: 4.1667e-05 - 79s/epoch - 403ms/step
Epoch 770/1000
2023-09-28 11:38:51.077 
Epoch 770/1000 
	 loss: 16.6859, MinusLogProbMetric: 16.6859, val_loss: 16.8652, val_MinusLogProbMetric: 16.8652

Epoch 770: val_loss did not improve from 16.83982
196/196 - 79s - loss: 16.6859 - MinusLogProbMetric: 16.6859 - val_loss: 16.8652 - val_MinusLogProbMetric: 16.8652 - lr: 4.1667e-05 - 79s/epoch - 403ms/step
Epoch 771/1000
2023-09-28 11:40:09.804 
Epoch 771/1000 
	 loss: 16.6889, MinusLogProbMetric: 16.6889, val_loss: 16.8585, val_MinusLogProbMetric: 16.8585

Epoch 771: val_loss did not improve from 16.83982
196/196 - 79s - loss: 16.6889 - MinusLogProbMetric: 16.6889 - val_loss: 16.8585 - val_MinusLogProbMetric: 16.8585 - lr: 4.1667e-05 - 79s/epoch - 402ms/step
Epoch 772/1000
2023-09-28 11:41:31.231 
Epoch 772/1000 
	 loss: 16.6863, MinusLogProbMetric: 16.6863, val_loss: 16.8943, val_MinusLogProbMetric: 16.8943

Epoch 772: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6863 - MinusLogProbMetric: 16.6863 - val_loss: 16.8943 - val_MinusLogProbMetric: 16.8943 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 773/1000
2023-09-28 11:42:52.484 
Epoch 773/1000 
	 loss: 16.6833, MinusLogProbMetric: 16.6833, val_loss: 16.8902, val_MinusLogProbMetric: 16.8902

Epoch 773: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6833 - MinusLogProbMetric: 16.6833 - val_loss: 16.8902 - val_MinusLogProbMetric: 16.8902 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 774/1000
2023-09-28 11:44:14.249 
Epoch 774/1000 
	 loss: 16.6802, MinusLogProbMetric: 16.6802, val_loss: 16.9833, val_MinusLogProbMetric: 16.9833

Epoch 774: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6802 - MinusLogProbMetric: 16.6802 - val_loss: 16.9833 - val_MinusLogProbMetric: 16.9833 - lr: 4.1667e-05 - 82s/epoch - 417ms/step
Epoch 775/1000
2023-09-28 11:45:35.726 
Epoch 775/1000 
	 loss: 16.6978, MinusLogProbMetric: 16.6978, val_loss: 16.8615, val_MinusLogProbMetric: 16.8615

Epoch 775: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6978 - MinusLogProbMetric: 16.6978 - val_loss: 16.8615 - val_MinusLogProbMetric: 16.8615 - lr: 4.1667e-05 - 81s/epoch - 416ms/step
Epoch 776/1000
2023-09-28 11:46:57.142 
Epoch 776/1000 
	 loss: 16.6869, MinusLogProbMetric: 16.6869, val_loss: 16.8776, val_MinusLogProbMetric: 16.8776

Epoch 776: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6869 - MinusLogProbMetric: 16.6869 - val_loss: 16.8776 - val_MinusLogProbMetric: 16.8776 - lr: 4.1667e-05 - 81s/epoch - 415ms/step
Epoch 777/1000
2023-09-28 11:48:08.874 
Epoch 777/1000 
	 loss: 16.6806, MinusLogProbMetric: 16.6806, val_loss: 16.9141, val_MinusLogProbMetric: 16.9141

Epoch 777: val_loss did not improve from 16.83982
196/196 - 72s - loss: 16.6806 - MinusLogProbMetric: 16.6806 - val_loss: 16.9141 - val_MinusLogProbMetric: 16.9141 - lr: 4.1667e-05 - 72s/epoch - 366ms/step
Epoch 778/1000
2023-09-28 11:49:19.192 
Epoch 778/1000 
	 loss: 16.6731, MinusLogProbMetric: 16.6731, val_loss: 16.8721, val_MinusLogProbMetric: 16.8721

Epoch 778: val_loss did not improve from 16.83982
196/196 - 70s - loss: 16.6731 - MinusLogProbMetric: 16.6731 - val_loss: 16.8721 - val_MinusLogProbMetric: 16.8721 - lr: 4.1667e-05 - 70s/epoch - 359ms/step
Epoch 779/1000
2023-09-28 11:50:33.643 
Epoch 779/1000 
	 loss: 16.6894, MinusLogProbMetric: 16.6894, val_loss: 17.0777, val_MinusLogProbMetric: 17.0777

Epoch 779: val_loss did not improve from 16.83982
196/196 - 74s - loss: 16.6894 - MinusLogProbMetric: 16.6894 - val_loss: 17.0777 - val_MinusLogProbMetric: 17.0777 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 780/1000
2023-09-28 11:51:46.550 
Epoch 780/1000 
	 loss: 16.6915, MinusLogProbMetric: 16.6915, val_loss: 16.8693, val_MinusLogProbMetric: 16.8693

Epoch 780: val_loss did not improve from 16.83982
196/196 - 73s - loss: 16.6915 - MinusLogProbMetric: 16.6915 - val_loss: 16.8693 - val_MinusLogProbMetric: 16.8693 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 781/1000
2023-09-28 11:52:57.487 
Epoch 781/1000 
	 loss: 16.6921, MinusLogProbMetric: 16.6921, val_loss: 16.8469, val_MinusLogProbMetric: 16.8469

Epoch 781: val_loss did not improve from 16.83982
196/196 - 71s - loss: 16.6921 - MinusLogProbMetric: 16.6921 - val_loss: 16.8469 - val_MinusLogProbMetric: 16.8469 - lr: 4.1667e-05 - 71s/epoch - 362ms/step
Epoch 782/1000
2023-09-28 11:54:11.710 
Epoch 782/1000 
	 loss: 16.6842, MinusLogProbMetric: 16.6842, val_loss: 16.8785, val_MinusLogProbMetric: 16.8785

Epoch 782: val_loss did not improve from 16.83982
196/196 - 74s - loss: 16.6842 - MinusLogProbMetric: 16.6842 - val_loss: 16.8785 - val_MinusLogProbMetric: 16.8785 - lr: 4.1667e-05 - 74s/epoch - 379ms/step
Epoch 783/1000
2023-09-28 11:55:26.127 
Epoch 783/1000 
	 loss: 16.6902, MinusLogProbMetric: 16.6902, val_loss: 16.8563, val_MinusLogProbMetric: 16.8563

Epoch 783: val_loss did not improve from 16.83982
196/196 - 74s - loss: 16.6902 - MinusLogProbMetric: 16.6902 - val_loss: 16.8563 - val_MinusLogProbMetric: 16.8563 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 784/1000
2023-09-28 11:56:38.820 
Epoch 784/1000 
	 loss: 32.2682, MinusLogProbMetric: 32.2682, val_loss: 20.1979, val_MinusLogProbMetric: 20.1979

Epoch 784: val_loss did not improve from 16.83982
196/196 - 73s - loss: 32.2682 - MinusLogProbMetric: 32.2682 - val_loss: 20.1979 - val_MinusLogProbMetric: 20.1979 - lr: 4.1667e-05 - 73s/epoch - 371ms/step
Epoch 785/1000
2023-09-28 11:57:57.032 
Epoch 785/1000 
	 loss: 19.1037, MinusLogProbMetric: 19.1037, val_loss: 18.7187, val_MinusLogProbMetric: 18.7187

Epoch 785: val_loss did not improve from 16.83982
196/196 - 78s - loss: 19.1037 - MinusLogProbMetric: 19.1037 - val_loss: 18.7187 - val_MinusLogProbMetric: 18.7187 - lr: 4.1667e-05 - 78s/epoch - 399ms/step
Epoch 786/1000
2023-09-28 11:59:18.191 
Epoch 786/1000 
	 loss: 18.3952, MinusLogProbMetric: 18.3952, val_loss: 18.3525, val_MinusLogProbMetric: 18.3525

Epoch 786: val_loss did not improve from 16.83982
196/196 - 81s - loss: 18.3952 - MinusLogProbMetric: 18.3952 - val_loss: 18.3525 - val_MinusLogProbMetric: 18.3525 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 787/1000
2023-09-28 12:00:39.351 
Epoch 787/1000 
	 loss: 17.7272, MinusLogProbMetric: 17.7272, val_loss: 17.5005, val_MinusLogProbMetric: 17.5005

Epoch 787: val_loss did not improve from 16.83982
196/196 - 81s - loss: 17.7272 - MinusLogProbMetric: 17.7272 - val_loss: 17.5005 - val_MinusLogProbMetric: 17.5005 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 788/1000
2023-09-28 12:01:58.818 
Epoch 788/1000 
	 loss: 17.1781, MinusLogProbMetric: 17.1781, val_loss: 17.0805, val_MinusLogProbMetric: 17.0805

Epoch 788: val_loss did not improve from 16.83982
196/196 - 79s - loss: 17.1781 - MinusLogProbMetric: 17.1781 - val_loss: 17.0805 - val_MinusLogProbMetric: 17.0805 - lr: 4.1667e-05 - 79s/epoch - 405ms/step
Epoch 789/1000
2023-09-28 12:03:15.719 
Epoch 789/1000 
	 loss: 16.8780, MinusLogProbMetric: 16.8780, val_loss: 17.0044, val_MinusLogProbMetric: 17.0044

Epoch 789: val_loss did not improve from 16.83982
196/196 - 77s - loss: 16.8780 - MinusLogProbMetric: 16.8780 - val_loss: 17.0044 - val_MinusLogProbMetric: 17.0044 - lr: 4.1667e-05 - 77s/epoch - 392ms/step
Epoch 790/1000
2023-09-28 12:04:35.872 
Epoch 790/1000 
	 loss: 16.8362, MinusLogProbMetric: 16.8362, val_loss: 16.9782, val_MinusLogProbMetric: 16.9782

Epoch 790: val_loss did not improve from 16.83982
196/196 - 80s - loss: 16.8362 - MinusLogProbMetric: 16.8362 - val_loss: 16.9782 - val_MinusLogProbMetric: 16.9782 - lr: 4.1667e-05 - 80s/epoch - 409ms/step
Epoch 791/1000
2023-09-28 12:05:50.349 
Epoch 791/1000 
	 loss: 16.8084, MinusLogProbMetric: 16.8084, val_loss: 16.9625, val_MinusLogProbMetric: 16.9625

Epoch 791: val_loss did not improve from 16.83982
196/196 - 74s - loss: 16.8084 - MinusLogProbMetric: 16.8084 - val_loss: 16.9625 - val_MinusLogProbMetric: 16.9625 - lr: 4.1667e-05 - 74s/epoch - 380ms/step
Epoch 792/1000
2023-09-28 12:06:58.551 
Epoch 792/1000 
	 loss: 16.7771, MinusLogProbMetric: 16.7771, val_loss: 16.9196, val_MinusLogProbMetric: 16.9196

Epoch 792: val_loss did not improve from 16.83982
196/196 - 68s - loss: 16.7771 - MinusLogProbMetric: 16.7771 - val_loss: 16.9196 - val_MinusLogProbMetric: 16.9196 - lr: 4.1667e-05 - 68s/epoch - 348ms/step
Epoch 793/1000
2023-09-28 12:08:02.405 
Epoch 793/1000 
	 loss: 16.7615, MinusLogProbMetric: 16.7615, val_loss: 16.9562, val_MinusLogProbMetric: 16.9562

Epoch 793: val_loss did not improve from 16.83982
196/196 - 64s - loss: 16.7615 - MinusLogProbMetric: 16.7615 - val_loss: 16.9562 - val_MinusLogProbMetric: 16.9562 - lr: 4.1667e-05 - 64s/epoch - 326ms/step
Epoch 794/1000
2023-09-28 12:09:08.548 
Epoch 794/1000 
	 loss: 16.7622, MinusLogProbMetric: 16.7622, val_loss: 16.9036, val_MinusLogProbMetric: 16.9036

Epoch 794: val_loss did not improve from 16.83982
196/196 - 66s - loss: 16.7622 - MinusLogProbMetric: 16.7622 - val_loss: 16.9036 - val_MinusLogProbMetric: 16.9036 - lr: 4.1667e-05 - 66s/epoch - 337ms/step
Epoch 795/1000
2023-09-28 12:10:16.421 
Epoch 795/1000 
	 loss: 16.7428, MinusLogProbMetric: 16.7428, val_loss: 16.9511, val_MinusLogProbMetric: 16.9511

Epoch 795: val_loss did not improve from 16.83982
196/196 - 68s - loss: 16.7428 - MinusLogProbMetric: 16.7428 - val_loss: 16.9511 - val_MinusLogProbMetric: 16.9511 - lr: 4.1667e-05 - 68s/epoch - 346ms/step
Epoch 796/1000
2023-09-28 12:11:21.534 
Epoch 796/1000 
	 loss: 16.7249, MinusLogProbMetric: 16.7249, val_loss: 16.8923, val_MinusLogProbMetric: 16.8923

Epoch 796: val_loss did not improve from 16.83982
196/196 - 65s - loss: 16.7249 - MinusLogProbMetric: 16.7249 - val_loss: 16.8923 - val_MinusLogProbMetric: 16.8923 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 797/1000
2023-09-28 12:12:30.899 
Epoch 797/1000 
	 loss: 16.7178, MinusLogProbMetric: 16.7178, val_loss: 16.8851, val_MinusLogProbMetric: 16.8851

Epoch 797: val_loss did not improve from 16.83982
196/196 - 69s - loss: 16.7178 - MinusLogProbMetric: 16.7178 - val_loss: 16.8851 - val_MinusLogProbMetric: 16.8851 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 798/1000
2023-09-28 12:13:37.762 
Epoch 798/1000 
	 loss: 16.7371, MinusLogProbMetric: 16.7371, val_loss: 17.0218, val_MinusLogProbMetric: 17.0218

Epoch 798: val_loss did not improve from 16.83982
196/196 - 67s - loss: 16.7371 - MinusLogProbMetric: 16.7371 - val_loss: 17.0218 - val_MinusLogProbMetric: 17.0218 - lr: 4.1667e-05 - 67s/epoch - 341ms/step
Epoch 799/1000
2023-09-28 12:14:46.337 
Epoch 799/1000 
	 loss: 16.7354, MinusLogProbMetric: 16.7354, val_loss: 16.9350, val_MinusLogProbMetric: 16.9350

Epoch 799: val_loss did not improve from 16.83982
196/196 - 69s - loss: 16.7354 - MinusLogProbMetric: 16.7354 - val_loss: 16.9350 - val_MinusLogProbMetric: 16.9350 - lr: 4.1667e-05 - 69s/epoch - 350ms/step
Epoch 800/1000
2023-09-28 12:15:54.276 
Epoch 800/1000 
	 loss: 16.7382, MinusLogProbMetric: 16.7382, val_loss: 16.8766, val_MinusLogProbMetric: 16.8766

Epoch 800: val_loss did not improve from 16.83982
196/196 - 68s - loss: 16.7382 - MinusLogProbMetric: 16.7382 - val_loss: 16.8766 - val_MinusLogProbMetric: 16.8766 - lr: 4.1667e-05 - 68s/epoch - 347ms/step
Epoch 801/1000
2023-09-28 12:16:59.974 
Epoch 801/1000 
	 loss: 16.7081, MinusLogProbMetric: 16.7081, val_loss: 16.9173, val_MinusLogProbMetric: 16.9173

Epoch 801: val_loss did not improve from 16.83982
196/196 - 66s - loss: 16.7081 - MinusLogProbMetric: 16.7081 - val_loss: 16.9173 - val_MinusLogProbMetric: 16.9173 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 802/1000
2023-09-28 12:18:08.059 
Epoch 802/1000 
	 loss: 16.7154, MinusLogProbMetric: 16.7154, val_loss: 16.8762, val_MinusLogProbMetric: 16.8762

Epoch 802: val_loss did not improve from 16.83982
196/196 - 68s - loss: 16.7154 - MinusLogProbMetric: 16.7154 - val_loss: 16.8762 - val_MinusLogProbMetric: 16.8762 - lr: 4.1667e-05 - 68s/epoch - 347ms/step
Epoch 803/1000
2023-09-28 12:19:14.491 
Epoch 803/1000 
	 loss: 16.7145, MinusLogProbMetric: 16.7145, val_loss: 16.9876, val_MinusLogProbMetric: 16.9876

Epoch 803: val_loss did not improve from 16.83982
196/196 - 66s - loss: 16.7145 - MinusLogProbMetric: 16.7145 - val_loss: 16.9876 - val_MinusLogProbMetric: 16.9876 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 804/1000
2023-09-28 12:20:24.148 
Epoch 804/1000 
	 loss: 16.6754, MinusLogProbMetric: 16.6754, val_loss: 16.8461, val_MinusLogProbMetric: 16.8461

Epoch 804: val_loss did not improve from 16.83982
196/196 - 70s - loss: 16.6754 - MinusLogProbMetric: 16.6754 - val_loss: 16.8461 - val_MinusLogProbMetric: 16.8461 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 805/1000
2023-09-28 12:21:30.322 
Epoch 805/1000 
	 loss: 16.6727, MinusLogProbMetric: 16.6727, val_loss: 16.8812, val_MinusLogProbMetric: 16.8812

Epoch 805: val_loss did not improve from 16.83982
196/196 - 66s - loss: 16.6727 - MinusLogProbMetric: 16.6727 - val_loss: 16.8812 - val_MinusLogProbMetric: 16.8812 - lr: 2.0833e-05 - 66s/epoch - 338ms/step
Epoch 806/1000
2023-09-28 12:22:46.974 
Epoch 806/1000 
	 loss: 16.6788, MinusLogProbMetric: 16.6788, val_loss: 16.8531, val_MinusLogProbMetric: 16.8531

Epoch 806: val_loss did not improve from 16.83982
196/196 - 77s - loss: 16.6788 - MinusLogProbMetric: 16.6788 - val_loss: 16.8531 - val_MinusLogProbMetric: 16.8531 - lr: 2.0833e-05 - 77s/epoch - 391ms/step
Epoch 807/1000
2023-09-28 12:24:06.898 
Epoch 807/1000 
	 loss: 16.6736, MinusLogProbMetric: 16.6736, val_loss: 16.8582, val_MinusLogProbMetric: 16.8582

Epoch 807: val_loss did not improve from 16.83982
196/196 - 80s - loss: 16.6736 - MinusLogProbMetric: 16.6736 - val_loss: 16.8582 - val_MinusLogProbMetric: 16.8582 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 808/1000
2023-09-28 12:25:22.382 
Epoch 808/1000 
	 loss: 16.6755, MinusLogProbMetric: 16.6755, val_loss: 16.8436, val_MinusLogProbMetric: 16.8436

Epoch 808: val_loss did not improve from 16.83982
196/196 - 75s - loss: 16.6755 - MinusLogProbMetric: 16.6755 - val_loss: 16.8436 - val_MinusLogProbMetric: 16.8436 - lr: 2.0833e-05 - 75s/epoch - 385ms/step
Epoch 809/1000
2023-09-28 12:26:37.477 
Epoch 809/1000 
	 loss: 16.6737, MinusLogProbMetric: 16.6737, val_loss: 16.8412, val_MinusLogProbMetric: 16.8412

Epoch 809: val_loss did not improve from 16.83982
196/196 - 75s - loss: 16.6737 - MinusLogProbMetric: 16.6737 - val_loss: 16.8412 - val_MinusLogProbMetric: 16.8412 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 810/1000
2023-09-28 12:27:54.330 
Epoch 810/1000 
	 loss: 16.6728, MinusLogProbMetric: 16.6728, val_loss: 16.8454, val_MinusLogProbMetric: 16.8454

Epoch 810: val_loss did not improve from 16.83982
196/196 - 77s - loss: 16.6728 - MinusLogProbMetric: 16.6728 - val_loss: 16.8454 - val_MinusLogProbMetric: 16.8454 - lr: 2.0833e-05 - 77s/epoch - 392ms/step
Epoch 811/1000
2023-09-28 12:29:09.249 
Epoch 811/1000 
	 loss: 16.6709, MinusLogProbMetric: 16.6709, val_loss: 16.8460, val_MinusLogProbMetric: 16.8460

Epoch 811: val_loss did not improve from 16.83982
196/196 - 75s - loss: 16.6709 - MinusLogProbMetric: 16.6709 - val_loss: 16.8460 - val_MinusLogProbMetric: 16.8460 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 812/1000
2023-09-28 12:30:30.595 
Epoch 812/1000 
	 loss: 16.6711, MinusLogProbMetric: 16.6711, val_loss: 16.8464, val_MinusLogProbMetric: 16.8464

Epoch 812: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6711 - MinusLogProbMetric: 16.6711 - val_loss: 16.8464 - val_MinusLogProbMetric: 16.8464 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 813/1000
2023-09-28 12:31:50.446 
Epoch 813/1000 
	 loss: 16.6665, MinusLogProbMetric: 16.6665, val_loss: 16.8693, val_MinusLogProbMetric: 16.8693

Epoch 813: val_loss did not improve from 16.83982
196/196 - 80s - loss: 16.6665 - MinusLogProbMetric: 16.6665 - val_loss: 16.8693 - val_MinusLogProbMetric: 16.8693 - lr: 2.0833e-05 - 80s/epoch - 407ms/step
Epoch 814/1000
2023-09-28 12:33:11.731 
Epoch 814/1000 
	 loss: 16.6659, MinusLogProbMetric: 16.6659, val_loss: 16.8425, val_MinusLogProbMetric: 16.8425

Epoch 814: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6659 - MinusLogProbMetric: 16.6659 - val_loss: 16.8425 - val_MinusLogProbMetric: 16.8425 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 815/1000
2023-09-28 12:34:33.908 
Epoch 815/1000 
	 loss: 16.6870, MinusLogProbMetric: 16.6870, val_loss: 16.8730, val_MinusLogProbMetric: 16.8730

Epoch 815: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6870 - MinusLogProbMetric: 16.6870 - val_loss: 16.8730 - val_MinusLogProbMetric: 16.8730 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 816/1000
2023-09-28 12:35:54.003 
Epoch 816/1000 
	 loss: 16.7230, MinusLogProbMetric: 16.7230, val_loss: 16.9274, val_MinusLogProbMetric: 16.9274

Epoch 816: val_loss did not improve from 16.83982
196/196 - 80s - loss: 16.7230 - MinusLogProbMetric: 16.7230 - val_loss: 16.9274 - val_MinusLogProbMetric: 16.9274 - lr: 2.0833e-05 - 80s/epoch - 409ms/step
Epoch 817/1000
2023-09-28 12:37:15.419 
Epoch 817/1000 
	 loss: 16.6775, MinusLogProbMetric: 16.6775, val_loss: 16.8458, val_MinusLogProbMetric: 16.8458

Epoch 817: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6775 - MinusLogProbMetric: 16.6775 - val_loss: 16.8458 - val_MinusLogProbMetric: 16.8458 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 818/1000
2023-09-28 12:38:37.267 
Epoch 818/1000 
	 loss: 16.6679, MinusLogProbMetric: 16.6679, val_loss: 16.8577, val_MinusLogProbMetric: 16.8577

Epoch 818: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6679 - MinusLogProbMetric: 16.6679 - val_loss: 16.8577 - val_MinusLogProbMetric: 16.8577 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 819/1000
2023-09-28 12:39:59.374 
Epoch 819/1000 
	 loss: 16.6672, MinusLogProbMetric: 16.6672, val_loss: 16.8591, val_MinusLogProbMetric: 16.8591

Epoch 819: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6672 - MinusLogProbMetric: 16.6672 - val_loss: 16.8591 - val_MinusLogProbMetric: 16.8591 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 820/1000
2023-09-28 12:41:21.678 
Epoch 820/1000 
	 loss: 16.6602, MinusLogProbMetric: 16.6602, val_loss: 16.8433, val_MinusLogProbMetric: 16.8433

Epoch 820: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6602 - MinusLogProbMetric: 16.6602 - val_loss: 16.8433 - val_MinusLogProbMetric: 16.8433 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 821/1000
2023-09-28 12:42:43.029 
Epoch 821/1000 
	 loss: 16.6616, MinusLogProbMetric: 16.6616, val_loss: 16.8430, val_MinusLogProbMetric: 16.8430

Epoch 821: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6616 - MinusLogProbMetric: 16.6616 - val_loss: 16.8430 - val_MinusLogProbMetric: 16.8430 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 822/1000
2023-09-28 12:44:04.988 
Epoch 822/1000 
	 loss: 16.6628, MinusLogProbMetric: 16.6628, val_loss: 16.8435, val_MinusLogProbMetric: 16.8435

Epoch 822: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6628 - MinusLogProbMetric: 16.6628 - val_loss: 16.8435 - val_MinusLogProbMetric: 16.8435 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 823/1000
2023-09-28 12:45:27.325 
Epoch 823/1000 
	 loss: 16.6630, MinusLogProbMetric: 16.6630, val_loss: 16.8441, val_MinusLogProbMetric: 16.8441

Epoch 823: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6630 - MinusLogProbMetric: 16.6630 - val_loss: 16.8441 - val_MinusLogProbMetric: 16.8441 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 824/1000
2023-09-28 12:46:49.159 
Epoch 824/1000 
	 loss: 16.6627, MinusLogProbMetric: 16.6627, val_loss: 16.8657, val_MinusLogProbMetric: 16.8657

Epoch 824: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6627 - MinusLogProbMetric: 16.6627 - val_loss: 16.8657 - val_MinusLogProbMetric: 16.8657 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 825/1000
2023-09-28 12:48:11.351 
Epoch 825/1000 
	 loss: 16.6650, MinusLogProbMetric: 16.6650, val_loss: 16.8854, val_MinusLogProbMetric: 16.8854

Epoch 825: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6650 - MinusLogProbMetric: 16.6650 - val_loss: 16.8854 - val_MinusLogProbMetric: 16.8854 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 826/1000
2023-09-28 12:49:31.472 
Epoch 826/1000 
	 loss: 16.6596, MinusLogProbMetric: 16.6596, val_loss: 16.8489, val_MinusLogProbMetric: 16.8489

Epoch 826: val_loss did not improve from 16.83982
196/196 - 80s - loss: 16.6596 - MinusLogProbMetric: 16.6596 - val_loss: 16.8489 - val_MinusLogProbMetric: 16.8489 - lr: 2.0833e-05 - 80s/epoch - 409ms/step
Epoch 827/1000
2023-09-28 12:50:52.985 
Epoch 827/1000 
	 loss: 16.6619, MinusLogProbMetric: 16.6619, val_loss: 16.8496, val_MinusLogProbMetric: 16.8496

Epoch 827: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6619 - MinusLogProbMetric: 16.6619 - val_loss: 16.8496 - val_MinusLogProbMetric: 16.8496 - lr: 2.0833e-05 - 82s/epoch - 416ms/step
Epoch 828/1000
2023-09-28 12:52:14.416 
Epoch 828/1000 
	 loss: 16.6603, MinusLogProbMetric: 16.6603, val_loss: 16.8453, val_MinusLogProbMetric: 16.8453

Epoch 828: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6603 - MinusLogProbMetric: 16.6603 - val_loss: 16.8453 - val_MinusLogProbMetric: 16.8453 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 829/1000
2023-09-28 12:53:35.778 
Epoch 829/1000 
	 loss: 16.6624, MinusLogProbMetric: 16.6624, val_loss: 16.8403, val_MinusLogProbMetric: 16.8403

Epoch 829: val_loss did not improve from 16.83982
196/196 - 81s - loss: 16.6624 - MinusLogProbMetric: 16.6624 - val_loss: 16.8403 - val_MinusLogProbMetric: 16.8403 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 830/1000
2023-09-28 12:54:57.863 
Epoch 830/1000 
	 loss: 16.6622, MinusLogProbMetric: 16.6622, val_loss: 16.8443, val_MinusLogProbMetric: 16.8443

Epoch 830: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6622 - MinusLogProbMetric: 16.6622 - val_loss: 16.8443 - val_MinusLogProbMetric: 16.8443 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 831/1000
2023-09-28 12:56:19.814 
Epoch 831/1000 
	 loss: 16.6567, MinusLogProbMetric: 16.6567, val_loss: 16.8493, val_MinusLogProbMetric: 16.8493

Epoch 831: val_loss did not improve from 16.83982
196/196 - 82s - loss: 16.6567 - MinusLogProbMetric: 16.6567 - val_loss: 16.8493 - val_MinusLogProbMetric: 16.8493 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 832/1000
2023-09-28 12:57:41.747 
Epoch 832/1000 
	 loss: 16.6573, MinusLogProbMetric: 16.6573, val_loss: 16.8369, val_MinusLogProbMetric: 16.8369

Epoch 832: val_loss improved from 16.83982 to 16.83689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.6573 - MinusLogProbMetric: 16.6573 - val_loss: 16.8369 - val_MinusLogProbMetric: 16.8369 - lr: 2.0833e-05 - 83s/epoch - 424ms/step
Epoch 833/1000
2023-09-28 12:59:04.759 
Epoch 833/1000 
	 loss: 16.6615, MinusLogProbMetric: 16.6615, val_loss: 16.8371, val_MinusLogProbMetric: 16.8371

Epoch 833: val_loss did not improve from 16.83689
196/196 - 82s - loss: 16.6615 - MinusLogProbMetric: 16.6615 - val_loss: 16.8371 - val_MinusLogProbMetric: 16.8371 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 834/1000
2023-09-28 13:00:26.623 
Epoch 834/1000 
	 loss: 16.6589, MinusLogProbMetric: 16.6589, val_loss: 16.8314, val_MinusLogProbMetric: 16.8314

Epoch 834: val_loss improved from 16.83689 to 16.83139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 84s - loss: 16.6589 - MinusLogProbMetric: 16.6589 - val_loss: 16.8314 - val_MinusLogProbMetric: 16.8314 - lr: 2.0833e-05 - 84s/epoch - 426ms/step
Epoch 835/1000
2023-09-28 13:01:48.388 
Epoch 835/1000 
	 loss: 16.6618, MinusLogProbMetric: 16.6618, val_loss: 16.8327, val_MinusLogProbMetric: 16.8327

Epoch 835: val_loss did not improve from 16.83139
196/196 - 80s - loss: 16.6618 - MinusLogProbMetric: 16.6618 - val_loss: 16.8327 - val_MinusLogProbMetric: 16.8327 - lr: 2.0833e-05 - 80s/epoch - 409ms/step
Epoch 836/1000
2023-09-28 13:03:10.317 
Epoch 836/1000 
	 loss: 16.6582, MinusLogProbMetric: 16.6582, val_loss: 16.8340, val_MinusLogProbMetric: 16.8340

Epoch 836: val_loss did not improve from 16.83139
196/196 - 82s - loss: 16.6582 - MinusLogProbMetric: 16.6582 - val_loss: 16.8340 - val_MinusLogProbMetric: 16.8340 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 837/1000
2023-09-28 13:04:31.947 
Epoch 837/1000 
	 loss: 16.6536, MinusLogProbMetric: 16.6536, val_loss: 16.8374, val_MinusLogProbMetric: 16.8374

Epoch 837: val_loss did not improve from 16.83139
196/196 - 82s - loss: 16.6536 - MinusLogProbMetric: 16.6536 - val_loss: 16.8374 - val_MinusLogProbMetric: 16.8374 - lr: 2.0833e-05 - 82s/epoch - 416ms/step
Epoch 838/1000
2023-09-28 13:05:53.117 
Epoch 838/1000 
	 loss: 16.6602, MinusLogProbMetric: 16.6602, val_loss: 16.8326, val_MinusLogProbMetric: 16.8326

Epoch 838: val_loss did not improve from 16.83139
196/196 - 81s - loss: 16.6602 - MinusLogProbMetric: 16.6602 - val_loss: 16.8326 - val_MinusLogProbMetric: 16.8326 - lr: 2.0833e-05 - 81s/epoch - 414ms/step
Epoch 839/1000
2023-09-28 13:07:14.103 
Epoch 839/1000 
	 loss: 16.6558, MinusLogProbMetric: 16.6558, val_loss: 16.8379, val_MinusLogProbMetric: 16.8379

Epoch 839: val_loss did not improve from 16.83139
196/196 - 81s - loss: 16.6558 - MinusLogProbMetric: 16.6558 - val_loss: 16.8379 - val_MinusLogProbMetric: 16.8379 - lr: 2.0833e-05 - 81s/epoch - 413ms/step
Epoch 840/1000
2023-09-28 13:08:36.511 
Epoch 840/1000 
	 loss: 16.7290, MinusLogProbMetric: 16.7290, val_loss: 16.8448, val_MinusLogProbMetric: 16.8448

Epoch 840: val_loss did not improve from 16.83139
196/196 - 82s - loss: 16.7290 - MinusLogProbMetric: 16.7290 - val_loss: 16.8448 - val_MinusLogProbMetric: 16.8448 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 841/1000
2023-09-28 13:09:58.928 
Epoch 841/1000 
	 loss: 16.6593, MinusLogProbMetric: 16.6593, val_loss: 16.8349, val_MinusLogProbMetric: 16.8349

Epoch 841: val_loss did not improve from 16.83139
196/196 - 82s - loss: 16.6593 - MinusLogProbMetric: 16.6593 - val_loss: 16.8349 - val_MinusLogProbMetric: 16.8349 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 842/1000
2023-09-28 13:11:19.194 
Epoch 842/1000 
	 loss: 16.6577, MinusLogProbMetric: 16.6577, val_loss: 16.8306, val_MinusLogProbMetric: 16.8306

Epoch 842: val_loss improved from 16.83139 to 16.83061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.6577 - MinusLogProbMetric: 16.6577 - val_loss: 16.8306 - val_MinusLogProbMetric: 16.8306 - lr: 2.0833e-05 - 81s/epoch - 416ms/step
Epoch 843/1000
2023-09-28 13:12:40.578 
Epoch 843/1000 
	 loss: 16.6655, MinusLogProbMetric: 16.6655, val_loss: 16.8582, val_MinusLogProbMetric: 16.8582

Epoch 843: val_loss did not improve from 16.83061
196/196 - 80s - loss: 16.6655 - MinusLogProbMetric: 16.6655 - val_loss: 16.8582 - val_MinusLogProbMetric: 16.8582 - lr: 2.0833e-05 - 80s/epoch - 409ms/step
Epoch 844/1000
2023-09-28 13:14:00.890 
Epoch 844/1000 
	 loss: 16.6583, MinusLogProbMetric: 16.6583, val_loss: 16.8494, val_MinusLogProbMetric: 16.8494

Epoch 844: val_loss did not improve from 16.83061
196/196 - 80s - loss: 16.6583 - MinusLogProbMetric: 16.6583 - val_loss: 16.8494 - val_MinusLogProbMetric: 16.8494 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 845/1000
2023-09-28 13:15:20.284 
Epoch 845/1000 
	 loss: 16.6560, MinusLogProbMetric: 16.6560, val_loss: 16.8393, val_MinusLogProbMetric: 16.8393

Epoch 845: val_loss did not improve from 16.83061
196/196 - 79s - loss: 16.6560 - MinusLogProbMetric: 16.6560 - val_loss: 16.8393 - val_MinusLogProbMetric: 16.8393 - lr: 2.0833e-05 - 79s/epoch - 405ms/step
Epoch 846/1000
2023-09-28 13:16:40.076 
Epoch 846/1000 
	 loss: 16.6549, MinusLogProbMetric: 16.6549, val_loss: 16.8335, val_MinusLogProbMetric: 16.8335

Epoch 846: val_loss did not improve from 16.83061
196/196 - 80s - loss: 16.6549 - MinusLogProbMetric: 16.6549 - val_loss: 16.8335 - val_MinusLogProbMetric: 16.8335 - lr: 2.0833e-05 - 80s/epoch - 407ms/step
Epoch 847/1000
2023-09-28 13:17:59.928 
Epoch 847/1000 
	 loss: 16.6544, MinusLogProbMetric: 16.6544, val_loss: 16.8304, val_MinusLogProbMetric: 16.8304

Epoch 847: val_loss improved from 16.83061 to 16.83038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 81s - loss: 16.6544 - MinusLogProbMetric: 16.6544 - val_loss: 16.8304 - val_MinusLogProbMetric: 16.8304 - lr: 2.0833e-05 - 81s/epoch - 414ms/step
Epoch 848/1000
2023-09-28 13:19:20.049 
Epoch 848/1000 
	 loss: 16.6528, MinusLogProbMetric: 16.6528, val_loss: 16.8417, val_MinusLogProbMetric: 16.8417

Epoch 848: val_loss did not improve from 16.83038
196/196 - 79s - loss: 16.6528 - MinusLogProbMetric: 16.6528 - val_loss: 16.8417 - val_MinusLogProbMetric: 16.8417 - lr: 2.0833e-05 - 79s/epoch - 402ms/step
Epoch 849/1000
2023-09-28 13:20:39.949 
Epoch 849/1000 
	 loss: 16.6627, MinusLogProbMetric: 16.6627, val_loss: 16.8560, val_MinusLogProbMetric: 16.8560

Epoch 849: val_loss did not improve from 16.83038
196/196 - 80s - loss: 16.6627 - MinusLogProbMetric: 16.6627 - val_loss: 16.8560 - val_MinusLogProbMetric: 16.8560 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 850/1000
2023-09-28 13:21:57.671 
Epoch 850/1000 
	 loss: 16.6500, MinusLogProbMetric: 16.6500, val_loss: 16.8434, val_MinusLogProbMetric: 16.8434

Epoch 850: val_loss did not improve from 16.83038
196/196 - 78s - loss: 16.6500 - MinusLogProbMetric: 16.6500 - val_loss: 16.8434 - val_MinusLogProbMetric: 16.8434 - lr: 2.0833e-05 - 78s/epoch - 397ms/step
Epoch 851/1000
2023-09-28 13:23:18.101 
Epoch 851/1000 
	 loss: 16.6550, MinusLogProbMetric: 16.6550, val_loss: 16.8332, val_MinusLogProbMetric: 16.8332

Epoch 851: val_loss did not improve from 16.83038
196/196 - 80s - loss: 16.6550 - MinusLogProbMetric: 16.6550 - val_loss: 16.8332 - val_MinusLogProbMetric: 16.8332 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 852/1000
2023-09-28 13:24:34.446 
Epoch 852/1000 
	 loss: 16.6535, MinusLogProbMetric: 16.6535, val_loss: 16.8415, val_MinusLogProbMetric: 16.8415

Epoch 852: val_loss did not improve from 16.83038
196/196 - 76s - loss: 16.6535 - MinusLogProbMetric: 16.6535 - val_loss: 16.8415 - val_MinusLogProbMetric: 16.8415 - lr: 2.0833e-05 - 76s/epoch - 389ms/step
Epoch 853/1000
2023-09-28 13:25:50.918 
Epoch 853/1000 
	 loss: 16.6532, MinusLogProbMetric: 16.6532, val_loss: 16.8246, val_MinusLogProbMetric: 16.8246

Epoch 853: val_loss improved from 16.83038 to 16.82462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 78s - loss: 16.6532 - MinusLogProbMetric: 16.6532 - val_loss: 16.8246 - val_MinusLogProbMetric: 16.8246 - lr: 2.0833e-05 - 78s/epoch - 396ms/step
Epoch 854/1000
2023-09-28 13:27:12.361 
Epoch 854/1000 
	 loss: 16.6539, MinusLogProbMetric: 16.6539, val_loss: 16.8358, val_MinusLogProbMetric: 16.8358

Epoch 854: val_loss did not improve from 16.82462
196/196 - 80s - loss: 16.6539 - MinusLogProbMetric: 16.6539 - val_loss: 16.8358 - val_MinusLogProbMetric: 16.8358 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 855/1000
2023-09-28 13:28:32.646 
Epoch 855/1000 
	 loss: 16.6478, MinusLogProbMetric: 16.6478, val_loss: 16.8358, val_MinusLogProbMetric: 16.8358

Epoch 855: val_loss did not improve from 16.82462
196/196 - 80s - loss: 16.6478 - MinusLogProbMetric: 16.6478 - val_loss: 16.8358 - val_MinusLogProbMetric: 16.8358 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 856/1000
2023-09-28 13:29:51.534 
Epoch 856/1000 
	 loss: 16.6510, MinusLogProbMetric: 16.6510, val_loss: 16.8413, val_MinusLogProbMetric: 16.8413

Epoch 856: val_loss did not improve from 16.82462
196/196 - 79s - loss: 16.6510 - MinusLogProbMetric: 16.6510 - val_loss: 16.8413 - val_MinusLogProbMetric: 16.8413 - lr: 2.0833e-05 - 79s/epoch - 402ms/step
Epoch 857/1000
2023-09-28 13:31:10.216 
Epoch 857/1000 
	 loss: 16.6500, MinusLogProbMetric: 16.6500, val_loss: 16.8508, val_MinusLogProbMetric: 16.8508

Epoch 857: val_loss did not improve from 16.82462
196/196 - 79s - loss: 16.6500 - MinusLogProbMetric: 16.6500 - val_loss: 16.8508 - val_MinusLogProbMetric: 16.8508 - lr: 2.0833e-05 - 79s/epoch - 401ms/step
Epoch 858/1000
2023-09-28 13:32:30.662 
Epoch 858/1000 
	 loss: 16.6501, MinusLogProbMetric: 16.6501, val_loss: 16.8349, val_MinusLogProbMetric: 16.8349

Epoch 858: val_loss did not improve from 16.82462
196/196 - 80s - loss: 16.6501 - MinusLogProbMetric: 16.6501 - val_loss: 16.8349 - val_MinusLogProbMetric: 16.8349 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 859/1000
2023-09-28 13:33:52.879 
Epoch 859/1000 
	 loss: 16.6458, MinusLogProbMetric: 16.6458, val_loss: 16.8930, val_MinusLogProbMetric: 16.8930

Epoch 859: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6458 - MinusLogProbMetric: 16.6458 - val_loss: 16.8930 - val_MinusLogProbMetric: 16.8930 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 860/1000
2023-09-28 13:35:14.135 
Epoch 860/1000 
	 loss: 16.6552, MinusLogProbMetric: 16.6552, val_loss: 16.8318, val_MinusLogProbMetric: 16.8318

Epoch 860: val_loss did not improve from 16.82462
196/196 - 81s - loss: 16.6552 - MinusLogProbMetric: 16.6552 - val_loss: 16.8318 - val_MinusLogProbMetric: 16.8318 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 861/1000
2023-09-28 13:36:34.923 
Epoch 861/1000 
	 loss: 16.6650, MinusLogProbMetric: 16.6650, val_loss: 16.8332, val_MinusLogProbMetric: 16.8332

Epoch 861: val_loss did not improve from 16.82462
196/196 - 81s - loss: 16.6650 - MinusLogProbMetric: 16.6650 - val_loss: 16.8332 - val_MinusLogProbMetric: 16.8332 - lr: 2.0833e-05 - 81s/epoch - 412ms/step
Epoch 862/1000
2023-09-28 13:37:56.373 
Epoch 862/1000 
	 loss: 16.6564, MinusLogProbMetric: 16.6564, val_loss: 16.8427, val_MinusLogProbMetric: 16.8427

Epoch 862: val_loss did not improve from 16.82462
196/196 - 81s - loss: 16.6564 - MinusLogProbMetric: 16.6564 - val_loss: 16.8427 - val_MinusLogProbMetric: 16.8427 - lr: 2.0833e-05 - 81s/epoch - 416ms/step
Epoch 863/1000
2023-09-28 13:39:19.327 
Epoch 863/1000 
	 loss: 16.6492, MinusLogProbMetric: 16.6492, val_loss: 16.8629, val_MinusLogProbMetric: 16.8629

Epoch 863: val_loss did not improve from 16.82462
196/196 - 83s - loss: 16.6492 - MinusLogProbMetric: 16.6492 - val_loss: 16.8629 - val_MinusLogProbMetric: 16.8629 - lr: 2.0833e-05 - 83s/epoch - 423ms/step
Epoch 864/1000
2023-09-28 13:40:41.440 
Epoch 864/1000 
	 loss: 16.6516, MinusLogProbMetric: 16.6516, val_loss: 16.8297, val_MinusLogProbMetric: 16.8297

Epoch 864: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6516 - MinusLogProbMetric: 16.6516 - val_loss: 16.8297 - val_MinusLogProbMetric: 16.8297 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 865/1000
2023-09-28 13:42:02.554 
Epoch 865/1000 
	 loss: 16.6497, MinusLogProbMetric: 16.6497, val_loss: 16.8698, val_MinusLogProbMetric: 16.8698

Epoch 865: val_loss did not improve from 16.82462
196/196 - 81s - loss: 16.6497 - MinusLogProbMetric: 16.6497 - val_loss: 16.8698 - val_MinusLogProbMetric: 16.8698 - lr: 2.0833e-05 - 81s/epoch - 414ms/step
Epoch 866/1000
2023-09-28 13:43:24.749 
Epoch 866/1000 
	 loss: 16.6528, MinusLogProbMetric: 16.6528, val_loss: 16.8457, val_MinusLogProbMetric: 16.8457

Epoch 866: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6528 - MinusLogProbMetric: 16.6528 - val_loss: 16.8457 - val_MinusLogProbMetric: 16.8457 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 867/1000
2023-09-28 13:44:44.620 
Epoch 867/1000 
	 loss: 16.6527, MinusLogProbMetric: 16.6527, val_loss: 16.8377, val_MinusLogProbMetric: 16.8377

Epoch 867: val_loss did not improve from 16.82462
196/196 - 80s - loss: 16.6527 - MinusLogProbMetric: 16.6527 - val_loss: 16.8377 - val_MinusLogProbMetric: 16.8377 - lr: 2.0833e-05 - 80s/epoch - 407ms/step
Epoch 868/1000
2023-09-28 13:46:05.665 
Epoch 868/1000 
	 loss: 16.6488, MinusLogProbMetric: 16.6488, val_loss: 16.9010, val_MinusLogProbMetric: 16.9010

Epoch 868: val_loss did not improve from 16.82462
196/196 - 81s - loss: 16.6488 - MinusLogProbMetric: 16.6488 - val_loss: 16.9010 - val_MinusLogProbMetric: 16.9010 - lr: 2.0833e-05 - 81s/epoch - 413ms/step
Epoch 869/1000
2023-09-28 13:47:25.319 
Epoch 869/1000 
	 loss: 16.6526, MinusLogProbMetric: 16.6526, val_loss: 16.8669, val_MinusLogProbMetric: 16.8669

Epoch 869: val_loss did not improve from 16.82462
196/196 - 80s - loss: 16.6526 - MinusLogProbMetric: 16.6526 - val_loss: 16.8669 - val_MinusLogProbMetric: 16.8669 - lr: 2.0833e-05 - 80s/epoch - 406ms/step
Epoch 870/1000
2023-09-28 13:48:45.234 
Epoch 870/1000 
	 loss: 16.6537, MinusLogProbMetric: 16.6537, val_loss: 16.8360, val_MinusLogProbMetric: 16.8360

Epoch 870: val_loss did not improve from 16.82462
196/196 - 80s - loss: 16.6537 - MinusLogProbMetric: 16.6537 - val_loss: 16.8360 - val_MinusLogProbMetric: 16.8360 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 871/1000
2023-09-28 13:50:07.549 
Epoch 871/1000 
	 loss: 16.6509, MinusLogProbMetric: 16.6509, val_loss: 16.8572, val_MinusLogProbMetric: 16.8572

Epoch 871: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6509 - MinusLogProbMetric: 16.6509 - val_loss: 16.8572 - val_MinusLogProbMetric: 16.8572 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 872/1000
2023-09-28 13:51:29.554 
Epoch 872/1000 
	 loss: 16.6685, MinusLogProbMetric: 16.6685, val_loss: 16.8335, val_MinusLogProbMetric: 16.8335

Epoch 872: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6685 - MinusLogProbMetric: 16.6685 - val_loss: 16.8335 - val_MinusLogProbMetric: 16.8335 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 873/1000
2023-09-28 13:52:51.426 
Epoch 873/1000 
	 loss: 16.6541, MinusLogProbMetric: 16.6541, val_loss: 16.8315, val_MinusLogProbMetric: 16.8315

Epoch 873: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6541 - MinusLogProbMetric: 16.6541 - val_loss: 16.8315 - val_MinusLogProbMetric: 16.8315 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 874/1000
2023-09-28 13:54:13.413 
Epoch 874/1000 
	 loss: 16.6502, MinusLogProbMetric: 16.6502, val_loss: 16.8356, val_MinusLogProbMetric: 16.8356

Epoch 874: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6502 - MinusLogProbMetric: 16.6502 - val_loss: 16.8356 - val_MinusLogProbMetric: 16.8356 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 875/1000
2023-09-28 13:55:34.757 
Epoch 875/1000 
	 loss: 16.6476, MinusLogProbMetric: 16.6476, val_loss: 16.8266, val_MinusLogProbMetric: 16.8266

Epoch 875: val_loss did not improve from 16.82462
196/196 - 81s - loss: 16.6476 - MinusLogProbMetric: 16.6476 - val_loss: 16.8266 - val_MinusLogProbMetric: 16.8266 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 876/1000
2023-09-28 13:56:56.542 
Epoch 876/1000 
	 loss: 16.6523, MinusLogProbMetric: 16.6523, val_loss: 16.8349, val_MinusLogProbMetric: 16.8349

Epoch 876: val_loss did not improve from 16.82462
196/196 - 82s - loss: 16.6523 - MinusLogProbMetric: 16.6523 - val_loss: 16.8349 - val_MinusLogProbMetric: 16.8349 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 877/1000
2023-09-28 13:58:17.938 
Epoch 877/1000 
	 loss: 16.6731, MinusLogProbMetric: 16.6731, val_loss: 16.8222, val_MinusLogProbMetric: 16.8222

Epoch 877: val_loss improved from 16.82462 to 16.82216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.6731 - MinusLogProbMetric: 16.6731 - val_loss: 16.8222 - val_MinusLogProbMetric: 16.8222 - lr: 2.0833e-05 - 83s/epoch - 423ms/step
Epoch 878/1000
2023-09-28 13:59:41.640 
Epoch 878/1000 
	 loss: 16.6474, MinusLogProbMetric: 16.6474, val_loss: 16.8431, val_MinusLogProbMetric: 16.8431

Epoch 878: val_loss did not improve from 16.82216
196/196 - 82s - loss: 16.6474 - MinusLogProbMetric: 16.6474 - val_loss: 16.8431 - val_MinusLogProbMetric: 16.8431 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 879/1000
2023-09-28 14:01:01.579 
Epoch 879/1000 
	 loss: 16.6504, MinusLogProbMetric: 16.6504, val_loss: 16.8273, val_MinusLogProbMetric: 16.8273

Epoch 879: val_loss did not improve from 16.82216
196/196 - 80s - loss: 16.6504 - MinusLogProbMetric: 16.6504 - val_loss: 16.8273 - val_MinusLogProbMetric: 16.8273 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 880/1000
2023-09-28 14:02:23.658 
Epoch 880/1000 
	 loss: 16.6492, MinusLogProbMetric: 16.6492, val_loss: 16.8355, val_MinusLogProbMetric: 16.8355

Epoch 880: val_loss did not improve from 16.82216
196/196 - 82s - loss: 16.6492 - MinusLogProbMetric: 16.6492 - val_loss: 16.8355 - val_MinusLogProbMetric: 16.8355 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 881/1000
2023-09-28 14:03:45.413 
Epoch 881/1000 
	 loss: 16.6505, MinusLogProbMetric: 16.6505, val_loss: 16.8409, val_MinusLogProbMetric: 16.8409

Epoch 881: val_loss did not improve from 16.82216
196/196 - 82s - loss: 16.6505 - MinusLogProbMetric: 16.6505 - val_loss: 16.8409 - val_MinusLogProbMetric: 16.8409 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 882/1000
2023-09-28 14:05:07.873 
Epoch 882/1000 
	 loss: 16.6466, MinusLogProbMetric: 16.6466, val_loss: 16.8708, val_MinusLogProbMetric: 16.8708

Epoch 882: val_loss did not improve from 16.82216
196/196 - 82s - loss: 16.6466 - MinusLogProbMetric: 16.6466 - val_loss: 16.8708 - val_MinusLogProbMetric: 16.8708 - lr: 2.0833e-05 - 82s/epoch - 421ms/step
Epoch 883/1000
2023-09-28 14:06:27.807 
Epoch 883/1000 
	 loss: 16.6477, MinusLogProbMetric: 16.6477, val_loss: 16.8373, val_MinusLogProbMetric: 16.8373

Epoch 883: val_loss did not improve from 16.82216
196/196 - 80s - loss: 16.6477 - MinusLogProbMetric: 16.6477 - val_loss: 16.8373 - val_MinusLogProbMetric: 16.8373 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 884/1000
2023-09-28 14:07:48.519 
Epoch 884/1000 
	 loss: 16.6525, MinusLogProbMetric: 16.6525, val_loss: 16.8968, val_MinusLogProbMetric: 16.8968

Epoch 884: val_loss did not improve from 16.82216
196/196 - 81s - loss: 16.6525 - MinusLogProbMetric: 16.6525 - val_loss: 16.8968 - val_MinusLogProbMetric: 16.8968 - lr: 2.0833e-05 - 81s/epoch - 412ms/step
Epoch 885/1000
2023-09-28 14:09:09.375 
Epoch 885/1000 
	 loss: 16.6663, MinusLogProbMetric: 16.6663, val_loss: 16.8817, val_MinusLogProbMetric: 16.8817

Epoch 885: val_loss did not improve from 16.82216
196/196 - 81s - loss: 16.6663 - MinusLogProbMetric: 16.6663 - val_loss: 16.8817 - val_MinusLogProbMetric: 16.8817 - lr: 2.0833e-05 - 81s/epoch - 413ms/step
Epoch 886/1000
2023-09-28 14:10:31.458 
Epoch 886/1000 
	 loss: 16.6453, MinusLogProbMetric: 16.6453, val_loss: 16.8246, val_MinusLogProbMetric: 16.8246

Epoch 886: val_loss did not improve from 16.82216
196/196 - 82s - loss: 16.6453 - MinusLogProbMetric: 16.6453 - val_loss: 16.8246 - val_MinusLogProbMetric: 16.8246 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 887/1000
2023-09-28 14:11:52.417 
Epoch 887/1000 
	 loss: 16.6492, MinusLogProbMetric: 16.6492, val_loss: 16.8198, val_MinusLogProbMetric: 16.8198

Epoch 887: val_loss improved from 16.82216 to 16.81982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.6492 - MinusLogProbMetric: 16.6492 - val_loss: 16.8198 - val_MinusLogProbMetric: 16.8198 - lr: 2.0833e-05 - 83s/epoch - 421ms/step
Epoch 888/1000
2023-09-28 14:13:12.505 
Epoch 888/1000 
	 loss: 16.6525, MinusLogProbMetric: 16.6525, val_loss: 16.8371, val_MinusLogProbMetric: 16.8371

Epoch 888: val_loss did not improve from 16.81982
196/196 - 78s - loss: 16.6525 - MinusLogProbMetric: 16.6525 - val_loss: 16.8371 - val_MinusLogProbMetric: 16.8371 - lr: 2.0833e-05 - 78s/epoch - 400ms/step
Epoch 889/1000
2023-09-28 14:14:32.010 
Epoch 889/1000 
	 loss: 16.6490, MinusLogProbMetric: 16.6490, val_loss: 16.8246, val_MinusLogProbMetric: 16.8246

Epoch 889: val_loss did not improve from 16.81982
196/196 - 80s - loss: 16.6490 - MinusLogProbMetric: 16.6490 - val_loss: 16.8246 - val_MinusLogProbMetric: 16.8246 - lr: 2.0833e-05 - 80s/epoch - 406ms/step
Epoch 890/1000
2023-09-28 14:15:51.604 
Epoch 890/1000 
	 loss: 16.6523, MinusLogProbMetric: 16.6523, val_loss: 16.8375, val_MinusLogProbMetric: 16.8375

Epoch 890: val_loss did not improve from 16.81982
196/196 - 80s - loss: 16.6523 - MinusLogProbMetric: 16.6523 - val_loss: 16.8375 - val_MinusLogProbMetric: 16.8375 - lr: 2.0833e-05 - 80s/epoch - 406ms/step
Epoch 891/1000
2023-09-28 14:17:08.629 
Epoch 891/1000 
	 loss: 16.6447, MinusLogProbMetric: 16.6447, val_loss: 16.8455, val_MinusLogProbMetric: 16.8455

Epoch 891: val_loss did not improve from 16.81982
196/196 - 77s - loss: 16.6447 - MinusLogProbMetric: 16.6447 - val_loss: 16.8455 - val_MinusLogProbMetric: 16.8455 - lr: 2.0833e-05 - 77s/epoch - 393ms/step
Epoch 892/1000
2023-09-28 14:18:29.498 
Epoch 892/1000 
	 loss: 16.6542, MinusLogProbMetric: 16.6542, val_loss: 16.8851, val_MinusLogProbMetric: 16.8851

Epoch 892: val_loss did not improve from 16.81982
196/196 - 81s - loss: 16.6542 - MinusLogProbMetric: 16.6542 - val_loss: 16.8851 - val_MinusLogProbMetric: 16.8851 - lr: 2.0833e-05 - 81s/epoch - 413ms/step
Epoch 893/1000
2023-09-28 14:19:51.188 
Epoch 893/1000 
	 loss: 16.6574, MinusLogProbMetric: 16.6574, val_loss: 16.8288, val_MinusLogProbMetric: 16.8288

Epoch 893: val_loss did not improve from 16.81982
196/196 - 82s - loss: 16.6574 - MinusLogProbMetric: 16.6574 - val_loss: 16.8288 - val_MinusLogProbMetric: 16.8288 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 894/1000
2023-09-28 14:21:10.549 
Epoch 894/1000 
	 loss: 16.6428, MinusLogProbMetric: 16.6428, val_loss: 16.8298, val_MinusLogProbMetric: 16.8298

Epoch 894: val_loss did not improve from 16.81982
196/196 - 79s - loss: 16.6428 - MinusLogProbMetric: 16.6428 - val_loss: 16.8298 - val_MinusLogProbMetric: 16.8298 - lr: 2.0833e-05 - 79s/epoch - 405ms/step
Epoch 895/1000
2023-09-28 14:22:25.399 
Epoch 895/1000 
	 loss: 16.6432, MinusLogProbMetric: 16.6432, val_loss: 16.8421, val_MinusLogProbMetric: 16.8421

Epoch 895: val_loss did not improve from 16.81982
196/196 - 75s - loss: 16.6432 - MinusLogProbMetric: 16.6432 - val_loss: 16.8421 - val_MinusLogProbMetric: 16.8421 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 896/1000
2023-09-28 14:23:42.706 
Epoch 896/1000 
	 loss: 16.6507, MinusLogProbMetric: 16.6507, val_loss: 16.8636, val_MinusLogProbMetric: 16.8636

Epoch 896: val_loss did not improve from 16.81982
196/196 - 77s - loss: 16.6507 - MinusLogProbMetric: 16.6507 - val_loss: 16.8636 - val_MinusLogProbMetric: 16.8636 - lr: 2.0833e-05 - 77s/epoch - 394ms/step
Epoch 897/1000
2023-09-28 14:24:58.945 
Epoch 897/1000 
	 loss: 16.6461, MinusLogProbMetric: 16.6461, val_loss: 16.8299, val_MinusLogProbMetric: 16.8299

Epoch 897: val_loss did not improve from 16.81982
196/196 - 76s - loss: 16.6461 - MinusLogProbMetric: 16.6461 - val_loss: 16.8299 - val_MinusLogProbMetric: 16.8299 - lr: 2.0833e-05 - 76s/epoch - 389ms/step
Epoch 898/1000
2023-09-28 14:26:07.237 
Epoch 898/1000 
	 loss: 16.6458, MinusLogProbMetric: 16.6458, val_loss: 16.8370, val_MinusLogProbMetric: 16.8370

Epoch 898: val_loss did not improve from 16.81982
196/196 - 68s - loss: 16.6458 - MinusLogProbMetric: 16.6458 - val_loss: 16.8370 - val_MinusLogProbMetric: 16.8370 - lr: 2.0833e-05 - 68s/epoch - 348ms/step
Epoch 899/1000
2023-09-28 14:27:21.881 
Epoch 899/1000 
	 loss: 16.6458, MinusLogProbMetric: 16.6458, val_loss: 16.8682, val_MinusLogProbMetric: 16.8682

Epoch 899: val_loss did not improve from 16.81982
196/196 - 75s - loss: 16.6458 - MinusLogProbMetric: 16.6458 - val_loss: 16.8682 - val_MinusLogProbMetric: 16.8682 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 900/1000
2023-09-28 14:28:38.031 
Epoch 900/1000 
	 loss: 16.8717, MinusLogProbMetric: 16.8717, val_loss: 16.8389, val_MinusLogProbMetric: 16.8389

Epoch 900: val_loss did not improve from 16.81982
196/196 - 76s - loss: 16.8717 - MinusLogProbMetric: 16.8717 - val_loss: 16.8389 - val_MinusLogProbMetric: 16.8389 - lr: 2.0833e-05 - 76s/epoch - 389ms/step
Epoch 901/1000
2023-09-28 14:29:46.846 
Epoch 901/1000 
	 loss: 16.6528, MinusLogProbMetric: 16.6528, val_loss: 16.8464, val_MinusLogProbMetric: 16.8464

Epoch 901: val_loss did not improve from 16.81982
196/196 - 69s - loss: 16.6528 - MinusLogProbMetric: 16.6528 - val_loss: 16.8464 - val_MinusLogProbMetric: 16.8464 - lr: 2.0833e-05 - 69s/epoch - 351ms/step
Epoch 902/1000
2023-09-28 14:30:52.523 
Epoch 902/1000 
	 loss: 16.6486, MinusLogProbMetric: 16.6486, val_loss: 16.8369, val_MinusLogProbMetric: 16.8369

Epoch 902: val_loss did not improve from 16.81982
196/196 - 66s - loss: 16.6486 - MinusLogProbMetric: 16.6486 - val_loss: 16.8369 - val_MinusLogProbMetric: 16.8369 - lr: 2.0833e-05 - 66s/epoch - 335ms/step
Epoch 903/1000
2023-09-28 14:31:57.643 
Epoch 903/1000 
	 loss: 16.6501, MinusLogProbMetric: 16.6501, val_loss: 16.8248, val_MinusLogProbMetric: 16.8248

Epoch 903: val_loss did not improve from 16.81982
196/196 - 65s - loss: 16.6501 - MinusLogProbMetric: 16.6501 - val_loss: 16.8248 - val_MinusLogProbMetric: 16.8248 - lr: 2.0833e-05 - 65s/epoch - 332ms/step
Epoch 904/1000
2023-09-28 14:33:02.124 
Epoch 904/1000 
	 loss: 16.6568, MinusLogProbMetric: 16.6568, val_loss: 16.8448, val_MinusLogProbMetric: 16.8448

Epoch 904: val_loss did not improve from 16.81982
196/196 - 64s - loss: 16.6568 - MinusLogProbMetric: 16.6568 - val_loss: 16.8448 - val_MinusLogProbMetric: 16.8448 - lr: 2.0833e-05 - 64s/epoch - 329ms/step
Epoch 905/1000
2023-09-28 14:34:13.138 
Epoch 905/1000 
	 loss: 16.6603, MinusLogProbMetric: 16.6603, val_loss: 16.8502, val_MinusLogProbMetric: 16.8502

Epoch 905: val_loss did not improve from 16.81982
196/196 - 71s - loss: 16.6603 - MinusLogProbMetric: 16.6603 - val_loss: 16.8502 - val_MinusLogProbMetric: 16.8502 - lr: 2.0833e-05 - 71s/epoch - 362ms/step
Epoch 906/1000
2023-09-28 14:35:17.306 
Epoch 906/1000 
	 loss: 16.6632, MinusLogProbMetric: 16.6632, val_loss: 16.9028, val_MinusLogProbMetric: 16.9028

Epoch 906: val_loss did not improve from 16.81982
196/196 - 64s - loss: 16.6632 - MinusLogProbMetric: 16.6632 - val_loss: 16.9028 - val_MinusLogProbMetric: 16.9028 - lr: 2.0833e-05 - 64s/epoch - 327ms/step
Epoch 907/1000
2023-09-28 14:36:23.810 
Epoch 907/1000 
	 loss: 16.6502, MinusLogProbMetric: 16.6502, val_loss: 16.8433, val_MinusLogProbMetric: 16.8433

Epoch 907: val_loss did not improve from 16.81982
196/196 - 67s - loss: 16.6502 - MinusLogProbMetric: 16.6502 - val_loss: 16.8433 - val_MinusLogProbMetric: 16.8433 - lr: 2.0833e-05 - 67s/epoch - 339ms/step
Epoch 908/1000
2023-09-28 14:37:28.885 
Epoch 908/1000 
	 loss: 16.6587, MinusLogProbMetric: 16.6587, val_loss: 16.8321, val_MinusLogProbMetric: 16.8321

Epoch 908: val_loss did not improve from 16.81982
196/196 - 65s - loss: 16.6587 - MinusLogProbMetric: 16.6587 - val_loss: 16.8321 - val_MinusLogProbMetric: 16.8321 - lr: 2.0833e-05 - 65s/epoch - 332ms/step
Epoch 909/1000
2023-09-28 14:38:34.411 
Epoch 909/1000 
	 loss: 16.6400, MinusLogProbMetric: 16.6400, val_loss: 16.8284, val_MinusLogProbMetric: 16.8284

Epoch 909: val_loss did not improve from 16.81982
196/196 - 66s - loss: 16.6400 - MinusLogProbMetric: 16.6400 - val_loss: 16.8284 - val_MinusLogProbMetric: 16.8284 - lr: 2.0833e-05 - 66s/epoch - 334ms/step
Epoch 910/1000
2023-09-28 14:39:45.985 
Epoch 910/1000 
	 loss: 16.6425, MinusLogProbMetric: 16.6425, val_loss: 16.8299, val_MinusLogProbMetric: 16.8299

Epoch 910: val_loss did not improve from 16.81982
196/196 - 72s - loss: 16.6425 - MinusLogProbMetric: 16.6425 - val_loss: 16.8299 - val_MinusLogProbMetric: 16.8299 - lr: 2.0833e-05 - 72s/epoch - 365ms/step
Epoch 911/1000
2023-09-28 14:40:59.467 
Epoch 911/1000 
	 loss: 16.6438, MinusLogProbMetric: 16.6438, val_loss: 16.8604, val_MinusLogProbMetric: 16.8604

Epoch 911: val_loss did not improve from 16.81982
196/196 - 73s - loss: 16.6438 - MinusLogProbMetric: 16.6438 - val_loss: 16.8604 - val_MinusLogProbMetric: 16.8604 - lr: 2.0833e-05 - 73s/epoch - 375ms/step
Epoch 912/1000
2023-09-28 14:42:11.937 
Epoch 912/1000 
	 loss: 16.6423, MinusLogProbMetric: 16.6423, val_loss: 16.9233, val_MinusLogProbMetric: 16.9233

Epoch 912: val_loss did not improve from 16.81982
196/196 - 72s - loss: 16.6423 - MinusLogProbMetric: 16.6423 - val_loss: 16.9233 - val_MinusLogProbMetric: 16.9233 - lr: 2.0833e-05 - 72s/epoch - 370ms/step
Epoch 913/1000
2023-09-28 14:43:28.676 
Epoch 913/1000 
	 loss: 16.6465, MinusLogProbMetric: 16.6465, val_loss: 16.8499, val_MinusLogProbMetric: 16.8499

Epoch 913: val_loss did not improve from 16.81982
196/196 - 77s - loss: 16.6465 - MinusLogProbMetric: 16.6465 - val_loss: 16.8499 - val_MinusLogProbMetric: 16.8499 - lr: 2.0833e-05 - 77s/epoch - 391ms/step
Epoch 914/1000
2023-09-28 14:44:39.726 
Epoch 914/1000 
	 loss: 16.6478, MinusLogProbMetric: 16.6478, val_loss: 16.8270, val_MinusLogProbMetric: 16.8270

Epoch 914: val_loss did not improve from 16.81982
196/196 - 71s - loss: 16.6478 - MinusLogProbMetric: 16.6478 - val_loss: 16.8270 - val_MinusLogProbMetric: 16.8270 - lr: 2.0833e-05 - 71s/epoch - 363ms/step
Epoch 915/1000
2023-09-28 14:45:57.950 
Epoch 915/1000 
	 loss: 16.6521, MinusLogProbMetric: 16.6521, val_loss: 16.8269, val_MinusLogProbMetric: 16.8269

Epoch 915: val_loss did not improve from 16.81982
196/196 - 78s - loss: 16.6521 - MinusLogProbMetric: 16.6521 - val_loss: 16.8269 - val_MinusLogProbMetric: 16.8269 - lr: 2.0833e-05 - 78s/epoch - 399ms/step
Epoch 916/1000
2023-09-28 14:47:14.697 
Epoch 916/1000 
	 loss: 16.6492, MinusLogProbMetric: 16.6492, val_loss: 16.8401, val_MinusLogProbMetric: 16.8401

Epoch 916: val_loss did not improve from 16.81982
196/196 - 77s - loss: 16.6492 - MinusLogProbMetric: 16.6492 - val_loss: 16.8401 - val_MinusLogProbMetric: 16.8401 - lr: 2.0833e-05 - 77s/epoch - 392ms/step
Epoch 917/1000
2023-09-28 14:48:31.751 
Epoch 917/1000 
	 loss: 16.6502, MinusLogProbMetric: 16.6502, val_loss: 16.8341, val_MinusLogProbMetric: 16.8341

Epoch 917: val_loss did not improve from 16.81982
196/196 - 77s - loss: 16.6502 - MinusLogProbMetric: 16.6502 - val_loss: 16.8341 - val_MinusLogProbMetric: 16.8341 - lr: 2.0833e-05 - 77s/epoch - 393ms/step
Epoch 918/1000
2023-09-28 14:49:46.454 
Epoch 918/1000 
	 loss: 16.6555, MinusLogProbMetric: 16.6555, val_loss: 17.0050, val_MinusLogProbMetric: 17.0050

Epoch 918: val_loss did not improve from 16.81982
196/196 - 75s - loss: 16.6555 - MinusLogProbMetric: 16.6555 - val_loss: 17.0050 - val_MinusLogProbMetric: 17.0050 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 919/1000
2023-09-28 14:51:08.100 
Epoch 919/1000 
	 loss: 16.6511, MinusLogProbMetric: 16.6511, val_loss: 16.8382, val_MinusLogProbMetric: 16.8382

Epoch 919: val_loss did not improve from 16.81982
196/196 - 82s - loss: 16.6511 - MinusLogProbMetric: 16.6511 - val_loss: 16.8382 - val_MinusLogProbMetric: 16.8382 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 920/1000
2023-09-28 14:52:29.960 
Epoch 920/1000 
	 loss: 16.6414, MinusLogProbMetric: 16.6414, val_loss: 16.8304, val_MinusLogProbMetric: 16.8304

Epoch 920: val_loss did not improve from 16.81982
196/196 - 82s - loss: 16.6414 - MinusLogProbMetric: 16.6414 - val_loss: 16.8304 - val_MinusLogProbMetric: 16.8304 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 921/1000
2023-09-28 14:53:51.904 
Epoch 921/1000 
	 loss: 16.6426, MinusLogProbMetric: 16.6426, val_loss: 16.8312, val_MinusLogProbMetric: 16.8312

Epoch 921: val_loss did not improve from 16.81982
196/196 - 82s - loss: 16.6426 - MinusLogProbMetric: 16.6426 - val_loss: 16.8312 - val_MinusLogProbMetric: 16.8312 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 922/1000
2023-09-28 14:55:14.246 
Epoch 922/1000 
	 loss: 16.6420, MinusLogProbMetric: 16.6420, val_loss: 16.8356, val_MinusLogProbMetric: 16.8356

Epoch 922: val_loss did not improve from 16.81982
196/196 - 82s - loss: 16.6420 - MinusLogProbMetric: 16.6420 - val_loss: 16.8356 - val_MinusLogProbMetric: 16.8356 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 923/1000
2023-09-28 14:56:36.130 
Epoch 923/1000 
	 loss: 16.6483, MinusLogProbMetric: 16.6483, val_loss: 16.8180, val_MinusLogProbMetric: 16.8180

Epoch 923: val_loss improved from 16.81982 to 16.81804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.6483 - MinusLogProbMetric: 16.6483 - val_loss: 16.8180 - val_MinusLogProbMetric: 16.8180 - lr: 2.0833e-05 - 83s/epoch - 424ms/step
Epoch 924/1000
2023-09-28 14:58:00.160 
Epoch 924/1000 
	 loss: 16.6394, MinusLogProbMetric: 16.6394, val_loss: 16.8347, val_MinusLogProbMetric: 16.8347

Epoch 924: val_loss did not improve from 16.81804
196/196 - 83s - loss: 16.6394 - MinusLogProbMetric: 16.6394 - val_loss: 16.8347 - val_MinusLogProbMetric: 16.8347 - lr: 2.0833e-05 - 83s/epoch - 422ms/step
Epoch 925/1000
2023-09-28 14:59:23.262 
Epoch 925/1000 
	 loss: 16.6432, MinusLogProbMetric: 16.6432, val_loss: 16.8337, val_MinusLogProbMetric: 16.8337

Epoch 925: val_loss did not improve from 16.81804
196/196 - 83s - loss: 16.6432 - MinusLogProbMetric: 16.6432 - val_loss: 16.8337 - val_MinusLogProbMetric: 16.8337 - lr: 2.0833e-05 - 83s/epoch - 424ms/step
Epoch 926/1000
2023-09-28 15:00:44.998 
Epoch 926/1000 
	 loss: 16.6455, MinusLogProbMetric: 16.6455, val_loss: 16.8268, val_MinusLogProbMetric: 16.8268

Epoch 926: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6455 - MinusLogProbMetric: 16.6455 - val_loss: 16.8268 - val_MinusLogProbMetric: 16.8268 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 927/1000
2023-09-28 15:02:07.844 
Epoch 927/1000 
	 loss: 16.6526, MinusLogProbMetric: 16.6526, val_loss: 16.8336, val_MinusLogProbMetric: 16.8336

Epoch 927: val_loss did not improve from 16.81804
196/196 - 83s - loss: 16.6526 - MinusLogProbMetric: 16.6526 - val_loss: 16.8336 - val_MinusLogProbMetric: 16.8336 - lr: 2.0833e-05 - 83s/epoch - 423ms/step
Epoch 928/1000
2023-09-28 15:03:30.353 
Epoch 928/1000 
	 loss: 16.6483, MinusLogProbMetric: 16.6483, val_loss: 16.8367, val_MinusLogProbMetric: 16.8367

Epoch 928: val_loss did not improve from 16.81804
196/196 - 83s - loss: 16.6483 - MinusLogProbMetric: 16.6483 - val_loss: 16.8367 - val_MinusLogProbMetric: 16.8367 - lr: 2.0833e-05 - 83s/epoch - 421ms/step
Epoch 929/1000
2023-09-28 15:04:52.442 
Epoch 929/1000 
	 loss: 16.6453, MinusLogProbMetric: 16.6453, val_loss: 16.8295, val_MinusLogProbMetric: 16.8295

Epoch 929: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6453 - MinusLogProbMetric: 16.6453 - val_loss: 16.8295 - val_MinusLogProbMetric: 16.8295 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 930/1000
2023-09-28 15:06:14.187 
Epoch 930/1000 
	 loss: 16.6398, MinusLogProbMetric: 16.6398, val_loss: 16.8272, val_MinusLogProbMetric: 16.8272

Epoch 930: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6398 - MinusLogProbMetric: 16.6398 - val_loss: 16.8272 - val_MinusLogProbMetric: 16.8272 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 931/1000
2023-09-28 15:07:35.732 
Epoch 931/1000 
	 loss: 16.6413, MinusLogProbMetric: 16.6413, val_loss: 16.8343, val_MinusLogProbMetric: 16.8343

Epoch 931: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6413 - MinusLogProbMetric: 16.6413 - val_loss: 16.8343 - val_MinusLogProbMetric: 16.8343 - lr: 2.0833e-05 - 82s/epoch - 416ms/step
Epoch 932/1000
2023-09-28 15:08:57.244 
Epoch 932/1000 
	 loss: 16.6487, MinusLogProbMetric: 16.6487, val_loss: 16.8452, val_MinusLogProbMetric: 16.8452

Epoch 932: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6487 - MinusLogProbMetric: 16.6487 - val_loss: 16.8452 - val_MinusLogProbMetric: 16.8452 - lr: 2.0833e-05 - 82s/epoch - 416ms/step
Epoch 933/1000
2023-09-28 15:10:18.988 
Epoch 933/1000 
	 loss: 16.6479, MinusLogProbMetric: 16.6479, val_loss: 16.8428, val_MinusLogProbMetric: 16.8428

Epoch 933: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6479 - MinusLogProbMetric: 16.6479 - val_loss: 16.8428 - val_MinusLogProbMetric: 16.8428 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 934/1000
2023-09-28 15:11:41.455 
Epoch 934/1000 
	 loss: 16.6394, MinusLogProbMetric: 16.6394, val_loss: 16.8252, val_MinusLogProbMetric: 16.8252

Epoch 934: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6394 - MinusLogProbMetric: 16.6394 - val_loss: 16.8252 - val_MinusLogProbMetric: 16.8252 - lr: 2.0833e-05 - 82s/epoch - 421ms/step
Epoch 935/1000
2023-09-28 15:13:03.592 
Epoch 935/1000 
	 loss: 16.6416, MinusLogProbMetric: 16.6416, val_loss: 16.8289, val_MinusLogProbMetric: 16.8289

Epoch 935: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6416 - MinusLogProbMetric: 16.6416 - val_loss: 16.8289 - val_MinusLogProbMetric: 16.8289 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 936/1000
2023-09-28 15:14:26.000 
Epoch 936/1000 
	 loss: 16.6450, MinusLogProbMetric: 16.6450, val_loss: 16.8355, val_MinusLogProbMetric: 16.8355

Epoch 936: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6450 - MinusLogProbMetric: 16.6450 - val_loss: 16.8355 - val_MinusLogProbMetric: 16.8355 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 937/1000
2023-09-28 15:15:48.228 
Epoch 937/1000 
	 loss: 16.6482, MinusLogProbMetric: 16.6482, val_loss: 16.8243, val_MinusLogProbMetric: 16.8243

Epoch 937: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6482 - MinusLogProbMetric: 16.6482 - val_loss: 16.8243 - val_MinusLogProbMetric: 16.8243 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 938/1000
2023-09-28 15:17:10.524 
Epoch 938/1000 
	 loss: 16.6959, MinusLogProbMetric: 16.6959, val_loss: 16.8274, val_MinusLogProbMetric: 16.8274

Epoch 938: val_loss did not improve from 16.81804
196/196 - 82s - loss: 16.6959 - MinusLogProbMetric: 16.6959 - val_loss: 16.8274 - val_MinusLogProbMetric: 16.8274 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 939/1000
2023-09-28 15:18:33.121 
Epoch 939/1000 
	 loss: 16.6440, MinusLogProbMetric: 16.6440, val_loss: 16.8152, val_MinusLogProbMetric: 16.8152

Epoch 939: val_loss improved from 16.81804 to 16.81521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 84s - loss: 16.6440 - MinusLogProbMetric: 16.6440 - val_loss: 16.8152 - val_MinusLogProbMetric: 16.8152 - lr: 2.0833e-05 - 84s/epoch - 428ms/step
Epoch 940/1000
2023-09-28 15:19:57.073 
Epoch 940/1000 
	 loss: 16.6385, MinusLogProbMetric: 16.6385, val_loss: 16.8268, val_MinusLogProbMetric: 16.8268

Epoch 940: val_loss did not improve from 16.81521
196/196 - 83s - loss: 16.6385 - MinusLogProbMetric: 16.6385 - val_loss: 16.8268 - val_MinusLogProbMetric: 16.8268 - lr: 2.0833e-05 - 83s/epoch - 422ms/step
Epoch 941/1000
2023-09-28 15:21:19.073 
Epoch 941/1000 
	 loss: 16.6417, MinusLogProbMetric: 16.6417, val_loss: 16.8222, val_MinusLogProbMetric: 16.8222

Epoch 941: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6417 - MinusLogProbMetric: 16.6417 - val_loss: 16.8222 - val_MinusLogProbMetric: 16.8222 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 942/1000
2023-09-28 15:22:40.801 
Epoch 942/1000 
	 loss: 16.6386, MinusLogProbMetric: 16.6386, val_loss: 16.8300, val_MinusLogProbMetric: 16.8300

Epoch 942: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6386 - MinusLogProbMetric: 16.6386 - val_loss: 16.8300 - val_MinusLogProbMetric: 16.8300 - lr: 2.0833e-05 - 82s/epoch - 417ms/step
Epoch 943/1000
2023-09-28 15:24:03.428 
Epoch 943/1000 
	 loss: 16.6455, MinusLogProbMetric: 16.6455, val_loss: 16.8160, val_MinusLogProbMetric: 16.8160

Epoch 943: val_loss did not improve from 16.81521
196/196 - 83s - loss: 16.6455 - MinusLogProbMetric: 16.6455 - val_loss: 16.8160 - val_MinusLogProbMetric: 16.8160 - lr: 2.0833e-05 - 83s/epoch - 422ms/step
Epoch 944/1000
2023-09-28 15:25:19.473 
Epoch 944/1000 
	 loss: 16.6508, MinusLogProbMetric: 16.6508, val_loss: 16.8268, val_MinusLogProbMetric: 16.8268

Epoch 944: val_loss did not improve from 16.81521
196/196 - 76s - loss: 16.6508 - MinusLogProbMetric: 16.6508 - val_loss: 16.8268 - val_MinusLogProbMetric: 16.8268 - lr: 2.0833e-05 - 76s/epoch - 388ms/step
Epoch 945/1000
2023-09-28 15:26:38.011 
Epoch 945/1000 
	 loss: 16.6484, MinusLogProbMetric: 16.6484, val_loss: 16.8605, val_MinusLogProbMetric: 16.8605

Epoch 945: val_loss did not improve from 16.81521
196/196 - 79s - loss: 16.6484 - MinusLogProbMetric: 16.6484 - val_loss: 16.8605 - val_MinusLogProbMetric: 16.8605 - lr: 2.0833e-05 - 79s/epoch - 401ms/step
Epoch 946/1000
2023-09-28 15:27:58.321 
Epoch 946/1000 
	 loss: 16.6421, MinusLogProbMetric: 16.6421, val_loss: 16.8319, val_MinusLogProbMetric: 16.8319

Epoch 946: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6421 - MinusLogProbMetric: 16.6421 - val_loss: 16.8319 - val_MinusLogProbMetric: 16.8319 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 947/1000
2023-09-28 15:29:15.388 
Epoch 947/1000 
	 loss: 16.6403, MinusLogProbMetric: 16.6403, val_loss: 16.8192, val_MinusLogProbMetric: 16.8192

Epoch 947: val_loss did not improve from 16.81521
196/196 - 77s - loss: 16.6403 - MinusLogProbMetric: 16.6403 - val_loss: 16.8192 - val_MinusLogProbMetric: 16.8192 - lr: 2.0833e-05 - 77s/epoch - 393ms/step
Epoch 948/1000
2023-09-28 15:30:35.912 
Epoch 948/1000 
	 loss: 16.6427, MinusLogProbMetric: 16.6427, val_loss: 16.8500, val_MinusLogProbMetric: 16.8500

Epoch 948: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6427 - MinusLogProbMetric: 16.6427 - val_loss: 16.8500 - val_MinusLogProbMetric: 16.8500 - lr: 2.0833e-05 - 81s/epoch - 411ms/step
Epoch 949/1000
2023-09-28 15:31:53.305 
Epoch 949/1000 
	 loss: 16.6431, MinusLogProbMetric: 16.6431, val_loss: 16.8450, val_MinusLogProbMetric: 16.8450

Epoch 949: val_loss did not improve from 16.81521
196/196 - 77s - loss: 16.6431 - MinusLogProbMetric: 16.6431 - val_loss: 16.8450 - val_MinusLogProbMetric: 16.8450 - lr: 2.0833e-05 - 77s/epoch - 395ms/step
Epoch 950/1000
2023-09-28 15:33:11.934 
Epoch 950/1000 
	 loss: 16.6431, MinusLogProbMetric: 16.6431, val_loss: 16.8587, val_MinusLogProbMetric: 16.8587

Epoch 950: val_loss did not improve from 16.81521
196/196 - 79s - loss: 16.6431 - MinusLogProbMetric: 16.6431 - val_loss: 16.8587 - val_MinusLogProbMetric: 16.8587 - lr: 2.0833e-05 - 79s/epoch - 401ms/step
Epoch 951/1000
2023-09-28 15:34:30.611 
Epoch 951/1000 
	 loss: 16.6416, MinusLogProbMetric: 16.6416, val_loss: 16.8233, val_MinusLogProbMetric: 16.8233

Epoch 951: val_loss did not improve from 16.81521
196/196 - 79s - loss: 16.6416 - MinusLogProbMetric: 16.6416 - val_loss: 16.8233 - val_MinusLogProbMetric: 16.8233 - lr: 2.0833e-05 - 79s/epoch - 401ms/step
Epoch 952/1000
2023-09-28 15:35:50.171 
Epoch 952/1000 
	 loss: 16.6418, MinusLogProbMetric: 16.6418, val_loss: 16.8530, val_MinusLogProbMetric: 16.8530

Epoch 952: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6418 - MinusLogProbMetric: 16.6418 - val_loss: 16.8530 - val_MinusLogProbMetric: 16.8530 - lr: 2.0833e-05 - 80s/epoch - 406ms/step
Epoch 953/1000
2023-09-28 15:37:09.317 
Epoch 953/1000 
	 loss: 16.6420, MinusLogProbMetric: 16.6420, val_loss: 16.8317, val_MinusLogProbMetric: 16.8317

Epoch 953: val_loss did not improve from 16.81521
196/196 - 79s - loss: 16.6420 - MinusLogProbMetric: 16.6420 - val_loss: 16.8317 - val_MinusLogProbMetric: 16.8317 - lr: 2.0833e-05 - 79s/epoch - 404ms/step
Epoch 954/1000
2023-09-28 15:38:26.291 
Epoch 954/1000 
	 loss: 16.6452, MinusLogProbMetric: 16.6452, val_loss: 16.8333, val_MinusLogProbMetric: 16.8333

Epoch 954: val_loss did not improve from 16.81521
196/196 - 77s - loss: 16.6452 - MinusLogProbMetric: 16.6452 - val_loss: 16.8333 - val_MinusLogProbMetric: 16.8333 - lr: 2.0833e-05 - 77s/epoch - 393ms/step
Epoch 955/1000
2023-09-28 15:39:44.736 
Epoch 955/1000 
	 loss: 16.6520, MinusLogProbMetric: 16.6520, val_loss: 16.8699, val_MinusLogProbMetric: 16.8699

Epoch 955: val_loss did not improve from 16.81521
196/196 - 78s - loss: 16.6520 - MinusLogProbMetric: 16.6520 - val_loss: 16.8699 - val_MinusLogProbMetric: 16.8699 - lr: 2.0833e-05 - 78s/epoch - 400ms/step
Epoch 956/1000
2023-09-28 15:41:00.464 
Epoch 956/1000 
	 loss: 16.6430, MinusLogProbMetric: 16.6430, val_loss: 16.8974, val_MinusLogProbMetric: 16.8974

Epoch 956: val_loss did not improve from 16.81521
196/196 - 76s - loss: 16.6430 - MinusLogProbMetric: 16.6430 - val_loss: 16.8974 - val_MinusLogProbMetric: 16.8974 - lr: 2.0833e-05 - 76s/epoch - 386ms/step
Epoch 957/1000
2023-09-28 15:42:20.048 
Epoch 957/1000 
	 loss: 16.6423, MinusLogProbMetric: 16.6423, val_loss: 16.8448, val_MinusLogProbMetric: 16.8448

Epoch 957: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6423 - MinusLogProbMetric: 16.6423 - val_loss: 16.8448 - val_MinusLogProbMetric: 16.8448 - lr: 2.0833e-05 - 80s/epoch - 406ms/step
Epoch 958/1000
2023-09-28 15:43:40.886 
Epoch 958/1000 
	 loss: 16.6599, MinusLogProbMetric: 16.6599, val_loss: 16.8282, val_MinusLogProbMetric: 16.8282

Epoch 958: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6599 - MinusLogProbMetric: 16.6599 - val_loss: 16.8282 - val_MinusLogProbMetric: 16.8282 - lr: 2.0833e-05 - 81s/epoch - 412ms/step
Epoch 959/1000
2023-09-28 15:45:00.896 
Epoch 959/1000 
	 loss: 16.6625, MinusLogProbMetric: 16.6625, val_loss: 16.8863, val_MinusLogProbMetric: 16.8863

Epoch 959: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6625 - MinusLogProbMetric: 16.6625 - val_loss: 16.8863 - val_MinusLogProbMetric: 16.8863 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 960/1000
2023-09-28 15:46:22.515 
Epoch 960/1000 
	 loss: 16.6433, MinusLogProbMetric: 16.6433, val_loss: 16.8279, val_MinusLogProbMetric: 16.8279

Epoch 960: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6433 - MinusLogProbMetric: 16.6433 - val_loss: 16.8279 - val_MinusLogProbMetric: 16.8279 - lr: 2.0833e-05 - 82s/epoch - 416ms/step
Epoch 961/1000
2023-09-28 15:47:43.939 
Epoch 961/1000 
	 loss: 16.6379, MinusLogProbMetric: 16.6379, val_loss: 16.8242, val_MinusLogProbMetric: 16.8242

Epoch 961: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6379 - MinusLogProbMetric: 16.6379 - val_loss: 16.8242 - val_MinusLogProbMetric: 16.8242 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 962/1000
2023-09-28 15:49:06.310 
Epoch 962/1000 
	 loss: 16.6458, MinusLogProbMetric: 16.6458, val_loss: 16.8431, val_MinusLogProbMetric: 16.8431

Epoch 962: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6458 - MinusLogProbMetric: 16.6458 - val_loss: 16.8431 - val_MinusLogProbMetric: 16.8431 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 963/1000
2023-09-28 15:50:28.186 
Epoch 963/1000 
	 loss: 16.6415, MinusLogProbMetric: 16.6415, val_loss: 16.8379, val_MinusLogProbMetric: 16.8379

Epoch 963: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6415 - MinusLogProbMetric: 16.6415 - val_loss: 16.8379 - val_MinusLogProbMetric: 16.8379 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 964/1000
2023-09-28 15:51:46.976 
Epoch 964/1000 
	 loss: 16.6453, MinusLogProbMetric: 16.6453, val_loss: 16.8362, val_MinusLogProbMetric: 16.8362

Epoch 964: val_loss did not improve from 16.81521
196/196 - 79s - loss: 16.6453 - MinusLogProbMetric: 16.6453 - val_loss: 16.8362 - val_MinusLogProbMetric: 16.8362 - lr: 2.0833e-05 - 79s/epoch - 402ms/step
Epoch 965/1000
2023-09-28 15:53:09.674 
Epoch 965/1000 
	 loss: 16.7049, MinusLogProbMetric: 16.7049, val_loss: 16.8180, val_MinusLogProbMetric: 16.8180

Epoch 965: val_loss did not improve from 16.81521
196/196 - 83s - loss: 16.7049 - MinusLogProbMetric: 16.7049 - val_loss: 16.8180 - val_MinusLogProbMetric: 16.8180 - lr: 2.0833e-05 - 83s/epoch - 422ms/step
Epoch 966/1000
2023-09-28 15:54:32.582 
Epoch 966/1000 
	 loss: 16.6408, MinusLogProbMetric: 16.6408, val_loss: 16.8365, val_MinusLogProbMetric: 16.8365

Epoch 966: val_loss did not improve from 16.81521
196/196 - 83s - loss: 16.6408 - MinusLogProbMetric: 16.6408 - val_loss: 16.8365 - val_MinusLogProbMetric: 16.8365 - lr: 2.0833e-05 - 83s/epoch - 423ms/step
Epoch 967/1000
2023-09-28 15:55:54.096 
Epoch 967/1000 
	 loss: 16.6422, MinusLogProbMetric: 16.6422, val_loss: 16.8234, val_MinusLogProbMetric: 16.8234

Epoch 967: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6422 - MinusLogProbMetric: 16.6422 - val_loss: 16.8234 - val_MinusLogProbMetric: 16.8234 - lr: 2.0833e-05 - 82s/epoch - 416ms/step
Epoch 968/1000
2023-09-28 15:57:14.212 
Epoch 968/1000 
	 loss: 16.6346, MinusLogProbMetric: 16.6346, val_loss: 16.8284, val_MinusLogProbMetric: 16.8284

Epoch 968: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6346 - MinusLogProbMetric: 16.6346 - val_loss: 16.8284 - val_MinusLogProbMetric: 16.8284 - lr: 2.0833e-05 - 80s/epoch - 409ms/step
Epoch 969/1000
2023-09-28 15:58:34.207 
Epoch 969/1000 
	 loss: 16.6360, MinusLogProbMetric: 16.6360, val_loss: 16.8250, val_MinusLogProbMetric: 16.8250

Epoch 969: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6360 - MinusLogProbMetric: 16.6360 - val_loss: 16.8250 - val_MinusLogProbMetric: 16.8250 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 970/1000
2023-09-28 15:59:54.766 
Epoch 970/1000 
	 loss: 16.6375, MinusLogProbMetric: 16.6375, val_loss: 16.8302, val_MinusLogProbMetric: 16.8302

Epoch 970: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6375 - MinusLogProbMetric: 16.6375 - val_loss: 16.8302 - val_MinusLogProbMetric: 16.8302 - lr: 2.0833e-05 - 81s/epoch - 411ms/step
Epoch 971/1000
2023-09-28 16:01:15.126 
Epoch 971/1000 
	 loss: 16.6379, MinusLogProbMetric: 16.6379, val_loss: 16.8183, val_MinusLogProbMetric: 16.8183

Epoch 971: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6379 - MinusLogProbMetric: 16.6379 - val_loss: 16.8183 - val_MinusLogProbMetric: 16.8183 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 972/1000
2023-09-28 16:02:35.704 
Epoch 972/1000 
	 loss: 16.6621, MinusLogProbMetric: 16.6621, val_loss: 16.8389, val_MinusLogProbMetric: 16.8389

Epoch 972: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6621 - MinusLogProbMetric: 16.6621 - val_loss: 16.8389 - val_MinusLogProbMetric: 16.8389 - lr: 2.0833e-05 - 81s/epoch - 411ms/step
Epoch 973/1000
2023-09-28 16:03:58.174 
Epoch 973/1000 
	 loss: 16.6432, MinusLogProbMetric: 16.6432, val_loss: 16.9089, val_MinusLogProbMetric: 16.9089

Epoch 973: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6432 - MinusLogProbMetric: 16.6432 - val_loss: 16.9089 - val_MinusLogProbMetric: 16.9089 - lr: 2.0833e-05 - 82s/epoch - 421ms/step
Epoch 974/1000
2023-09-28 16:05:17.898 
Epoch 974/1000 
	 loss: 16.6447, MinusLogProbMetric: 16.6447, val_loss: 16.8217, val_MinusLogProbMetric: 16.8217

Epoch 974: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6447 - MinusLogProbMetric: 16.6447 - val_loss: 16.8217 - val_MinusLogProbMetric: 16.8217 - lr: 2.0833e-05 - 80s/epoch - 407ms/step
Epoch 975/1000
2023-09-28 16:06:39.212 
Epoch 975/1000 
	 loss: 16.6388, MinusLogProbMetric: 16.6388, val_loss: 16.8325, val_MinusLogProbMetric: 16.8325

Epoch 975: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6388 - MinusLogProbMetric: 16.6388 - val_loss: 16.8325 - val_MinusLogProbMetric: 16.8325 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 976/1000
2023-09-28 16:07:59.722 
Epoch 976/1000 
	 loss: 16.6429, MinusLogProbMetric: 16.6429, val_loss: 16.8624, val_MinusLogProbMetric: 16.8624

Epoch 976: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6429 - MinusLogProbMetric: 16.6429 - val_loss: 16.8624 - val_MinusLogProbMetric: 16.8624 - lr: 2.0833e-05 - 81s/epoch - 411ms/step
Epoch 977/1000
2023-09-28 16:09:21.768 
Epoch 977/1000 
	 loss: 16.6387, MinusLogProbMetric: 16.6387, val_loss: 16.8205, val_MinusLogProbMetric: 16.8205

Epoch 977: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6387 - MinusLogProbMetric: 16.6387 - val_loss: 16.8205 - val_MinusLogProbMetric: 16.8205 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 978/1000
2023-09-28 16:10:44.244 
Epoch 978/1000 
	 loss: 16.6406, MinusLogProbMetric: 16.6406, val_loss: 16.8161, val_MinusLogProbMetric: 16.8161

Epoch 978: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6406 - MinusLogProbMetric: 16.6406 - val_loss: 16.8161 - val_MinusLogProbMetric: 16.8161 - lr: 2.0833e-05 - 82s/epoch - 421ms/step
Epoch 979/1000
2023-09-28 16:12:04.699 
Epoch 979/1000 
	 loss: 16.6416, MinusLogProbMetric: 16.6416, val_loss: 16.8348, val_MinusLogProbMetric: 16.8348

Epoch 979: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6416 - MinusLogProbMetric: 16.6416 - val_loss: 16.8348 - val_MinusLogProbMetric: 16.8348 - lr: 2.0833e-05 - 80s/epoch - 410ms/step
Epoch 980/1000
2023-09-28 16:13:26.051 
Epoch 980/1000 
	 loss: 16.6383, MinusLogProbMetric: 16.6383, val_loss: 16.8717, val_MinusLogProbMetric: 16.8717

Epoch 980: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6383 - MinusLogProbMetric: 16.6383 - val_loss: 16.8717 - val_MinusLogProbMetric: 16.8717 - lr: 2.0833e-05 - 81s/epoch - 415ms/step
Epoch 981/1000
2023-09-28 16:14:46.043 
Epoch 981/1000 
	 loss: 16.6452, MinusLogProbMetric: 16.6452, val_loss: 16.8292, val_MinusLogProbMetric: 16.8292

Epoch 981: val_loss did not improve from 16.81521
196/196 - 80s - loss: 16.6452 - MinusLogProbMetric: 16.6452 - val_loss: 16.8292 - val_MinusLogProbMetric: 16.8292 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 982/1000
2023-09-28 16:16:08.285 
Epoch 982/1000 
	 loss: 16.7256, MinusLogProbMetric: 16.7256, val_loss: 16.8243, val_MinusLogProbMetric: 16.8243

Epoch 982: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.7256 - MinusLogProbMetric: 16.7256 - val_loss: 16.8243 - val_MinusLogProbMetric: 16.8243 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 983/1000
2023-09-28 16:17:30.665 
Epoch 983/1000 
	 loss: 16.6421, MinusLogProbMetric: 16.6421, val_loss: 16.8224, val_MinusLogProbMetric: 16.8224

Epoch 983: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6421 - MinusLogProbMetric: 16.6421 - val_loss: 16.8224 - val_MinusLogProbMetric: 16.8224 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 984/1000
2023-09-28 16:18:52.540 
Epoch 984/1000 
	 loss: 16.6369, MinusLogProbMetric: 16.6369, val_loss: 16.8287, val_MinusLogProbMetric: 16.8287

Epoch 984: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6369 - MinusLogProbMetric: 16.6369 - val_loss: 16.8287 - val_MinusLogProbMetric: 16.8287 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 985/1000
2023-09-28 16:20:15.095 
Epoch 985/1000 
	 loss: 16.6404, MinusLogProbMetric: 16.6404, val_loss: 16.8456, val_MinusLogProbMetric: 16.8456

Epoch 985: val_loss did not improve from 16.81521
196/196 - 83s - loss: 16.6404 - MinusLogProbMetric: 16.6404 - val_loss: 16.8456 - val_MinusLogProbMetric: 16.8456 - lr: 2.0833e-05 - 83s/epoch - 421ms/step
Epoch 986/1000
2023-09-28 16:21:38.185 
Epoch 986/1000 
	 loss: 16.6434, MinusLogProbMetric: 16.6434, val_loss: 16.8316, val_MinusLogProbMetric: 16.8316

Epoch 986: val_loss did not improve from 16.81521
196/196 - 83s - loss: 16.6434 - MinusLogProbMetric: 16.6434 - val_loss: 16.8316 - val_MinusLogProbMetric: 16.8316 - lr: 2.0833e-05 - 83s/epoch - 424ms/step
Epoch 987/1000
2023-09-28 16:23:00.466 
Epoch 987/1000 
	 loss: 16.6456, MinusLogProbMetric: 16.6456, val_loss: 16.8417, val_MinusLogProbMetric: 16.8417

Epoch 987: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6456 - MinusLogProbMetric: 16.6456 - val_loss: 16.8417 - val_MinusLogProbMetric: 16.8417 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 988/1000
2023-09-28 16:24:20.994 
Epoch 988/1000 
	 loss: 16.6430, MinusLogProbMetric: 16.6430, val_loss: 16.8575, val_MinusLogProbMetric: 16.8575

Epoch 988: val_loss did not improve from 16.81521
196/196 - 81s - loss: 16.6430 - MinusLogProbMetric: 16.6430 - val_loss: 16.8575 - val_MinusLogProbMetric: 16.8575 - lr: 2.0833e-05 - 81s/epoch - 411ms/step
Epoch 989/1000
2023-09-28 16:25:43.213 
Epoch 989/1000 
	 loss: 16.6402, MinusLogProbMetric: 16.6402, val_loss: 16.8483, val_MinusLogProbMetric: 16.8483

Epoch 989: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6402 - MinusLogProbMetric: 16.6402 - val_loss: 16.8483 - val_MinusLogProbMetric: 16.8483 - lr: 2.0833e-05 - 82s/epoch - 419ms/step
Epoch 990/1000
2023-09-28 16:27:05.214 
Epoch 990/1000 
	 loss: 16.6275, MinusLogProbMetric: 16.6275, val_loss: 16.8405, val_MinusLogProbMetric: 16.8405

Epoch 990: val_loss did not improve from 16.81521
196/196 - 82s - loss: 16.6275 - MinusLogProbMetric: 16.6275 - val_loss: 16.8405 - val_MinusLogProbMetric: 16.8405 - lr: 1.0417e-05 - 82s/epoch - 418ms/step
Epoch 991/1000
2023-09-28 16:28:26.882 
Epoch 991/1000 
	 loss: 16.6237, MinusLogProbMetric: 16.6237, val_loss: 16.8089, val_MinusLogProbMetric: 16.8089

Epoch 991: val_loss improved from 16.81521 to 16.80889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 83s - loss: 16.6237 - MinusLogProbMetric: 16.6237 - val_loss: 16.8089 - val_MinusLogProbMetric: 16.8089 - lr: 1.0417e-05 - 83s/epoch - 424ms/step
Epoch 992/1000
2023-09-28 16:29:49.541 
Epoch 992/1000 
	 loss: 16.6273, MinusLogProbMetric: 16.6273, val_loss: 16.8205, val_MinusLogProbMetric: 16.8205

Epoch 992: val_loss did not improve from 16.80889
196/196 - 81s - loss: 16.6273 - MinusLogProbMetric: 16.6273 - val_loss: 16.8205 - val_MinusLogProbMetric: 16.8205 - lr: 1.0417e-05 - 81s/epoch - 415ms/step
Epoch 993/1000
2023-09-28 16:31:10.846 
Epoch 993/1000 
	 loss: 16.6229, MinusLogProbMetric: 16.6229, val_loss: 16.8346, val_MinusLogProbMetric: 16.8346

Epoch 993: val_loss did not improve from 16.80889
196/196 - 81s - loss: 16.6229 - MinusLogProbMetric: 16.6229 - val_loss: 16.8346 - val_MinusLogProbMetric: 16.8346 - lr: 1.0417e-05 - 81s/epoch - 415ms/step
Epoch 994/1000
2023-09-28 16:32:31.697 
Epoch 994/1000 
	 loss: 16.6241, MinusLogProbMetric: 16.6241, val_loss: 16.8076, val_MinusLogProbMetric: 16.8076

Epoch 994: val_loss improved from 16.80889 to 16.80763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_312/weights/best_weights.h5
196/196 - 82s - loss: 16.6241 - MinusLogProbMetric: 16.6241 - val_loss: 16.8076 - val_MinusLogProbMetric: 16.8076 - lr: 1.0417e-05 - 82s/epoch - 419ms/step
Epoch 995/1000
2023-09-28 16:33:55.463 
Epoch 995/1000 
	 loss: 16.6242, MinusLogProbMetric: 16.6242, val_loss: 16.8188, val_MinusLogProbMetric: 16.8188

Epoch 995: val_loss did not improve from 16.80763
196/196 - 83s - loss: 16.6242 - MinusLogProbMetric: 16.6242 - val_loss: 16.8188 - val_MinusLogProbMetric: 16.8188 - lr: 1.0417e-05 - 83s/epoch - 421ms/step
Epoch 996/1000
2023-09-28 16:35:13.290 
Epoch 996/1000 
	 loss: 16.6248, MinusLogProbMetric: 16.6248, val_loss: 16.8167, val_MinusLogProbMetric: 16.8167

Epoch 996: val_loss did not improve from 16.80763
196/196 - 78s - loss: 16.6248 - MinusLogProbMetric: 16.6248 - val_loss: 16.8167 - val_MinusLogProbMetric: 16.8167 - lr: 1.0417e-05 - 78s/epoch - 397ms/step
Epoch 997/1000
2023-09-28 16:36:27.914 
Epoch 997/1000 
	 loss: 16.6234, MinusLogProbMetric: 16.6234, val_loss: 16.8119, val_MinusLogProbMetric: 16.8119

Epoch 997: val_loss did not improve from 16.80763
196/196 - 75s - loss: 16.6234 - MinusLogProbMetric: 16.6234 - val_loss: 16.8119 - val_MinusLogProbMetric: 16.8119 - lr: 1.0417e-05 - 75s/epoch - 381ms/step
Epoch 998/1000
2023-09-28 16:37:41.690 
Epoch 998/1000 
	 loss: 16.6901, MinusLogProbMetric: 16.6901, val_loss: 16.8371, val_MinusLogProbMetric: 16.8371

Epoch 998: val_loss did not improve from 16.80763
196/196 - 74s - loss: 16.6901 - MinusLogProbMetric: 16.6901 - val_loss: 16.8371 - val_MinusLogProbMetric: 16.8371 - lr: 1.0417e-05 - 74s/epoch - 376ms/step
Epoch 999/1000
2023-09-28 16:39:02.211 
Epoch 999/1000 
	 loss: 16.6256, MinusLogProbMetric: 16.6256, val_loss: 16.8198, val_MinusLogProbMetric: 16.8198

Epoch 999: val_loss did not improve from 16.80763
196/196 - 81s - loss: 16.6256 - MinusLogProbMetric: 16.6256 - val_loss: 16.8198 - val_MinusLogProbMetric: 16.8198 - lr: 1.0417e-05 - 81s/epoch - 411ms/step
Epoch 1000/1000
2023-09-28 16:40:25.266 
Epoch 1000/1000 
	 loss: 16.6217, MinusLogProbMetric: 16.6217, val_loss: 16.8157, val_MinusLogProbMetric: 16.8157

Epoch 1000: val_loss did not improve from 16.80763
196/196 - 83s - loss: 16.6217 - MinusLogProbMetric: 16.6217 - val_loss: 16.8157 - val_MinusLogProbMetric: 16.8157 - lr: 1.0417e-05 - 83s/epoch - 424ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 29.319731037016027 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 18.995675571030006 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 12.824036495992914 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 13.718715470051393 seconds.
Training succeeded with seed 926.
Model trained in 79372.29 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 77.20 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 77.41 s.
===========
Run 312/720 done in 79653.90 s.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

===========
Generating train data for run 324.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_324/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_324/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_324/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_324
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_77"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_78 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  2139360   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7fc295e7ba30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc32437b0d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc32437b0d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc23847cbe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc22a81f190>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc22a81f370>, <keras.callbacks.ModelCheckpoint object at 0x7fc22a81ee30>, <keras.callbacks.EarlyStopping object at 0x7fc22a81cfa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc22a81feb0>, <keras.callbacks.TerminateOnNaN object at 0x7fc22a81ed40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_324/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 324/720 with hyperparameters:
timestamp = 2023-09-28 16:41:48.303365
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 16:43:41.463 
Epoch 1/1000 
	 loss: 619.0815, MinusLogProbMetric: 619.0815, val_loss: 202.2841, val_MinusLogProbMetric: 202.2841

Epoch 1: val_loss improved from inf to 202.28415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 114s - loss: 619.0815 - MinusLogProbMetric: 619.0815 - val_loss: 202.2841 - val_MinusLogProbMetric: 202.2841 - lr: 0.0010 - 114s/epoch - 581ms/step
Epoch 2/1000
2023-09-28 16:44:25.427 
Epoch 2/1000 
	 loss: 130.1672, MinusLogProbMetric: 130.1672, val_loss: 94.1318, val_MinusLogProbMetric: 94.1318

Epoch 2: val_loss improved from 202.28415 to 94.13175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 130.1672 - MinusLogProbMetric: 130.1672 - val_loss: 94.1318 - val_MinusLogProbMetric: 94.1318 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 3/1000
2023-09-28 16:45:08.924 
Epoch 3/1000 
	 loss: 83.4662, MinusLogProbMetric: 83.4662, val_loss: 70.1205, val_MinusLogProbMetric: 70.1205

Epoch 3: val_loss improved from 94.13175 to 70.12048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 83.4662 - MinusLogProbMetric: 83.4662 - val_loss: 70.1205 - val_MinusLogProbMetric: 70.1205 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 4/1000
2023-09-28 16:45:51.398 
Epoch 4/1000 
	 loss: 66.8303, MinusLogProbMetric: 66.8303, val_loss: 62.9139, val_MinusLogProbMetric: 62.9139

Epoch 4: val_loss improved from 70.12048 to 62.91391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 66.8303 - MinusLogProbMetric: 66.8303 - val_loss: 62.9139 - val_MinusLogProbMetric: 62.9139 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 5/1000
2023-09-28 16:46:35.209 
Epoch 5/1000 
	 loss: 58.3351, MinusLogProbMetric: 58.3351, val_loss: 58.8221, val_MinusLogProbMetric: 58.8221

Epoch 5: val_loss improved from 62.91391 to 58.82215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 58.3351 - MinusLogProbMetric: 58.3351 - val_loss: 58.8221 - val_MinusLogProbMetric: 58.8221 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 6/1000
2023-09-28 16:47:17.001 
Epoch 6/1000 
	 loss: 54.8933, MinusLogProbMetric: 54.8933, val_loss: 51.8100, val_MinusLogProbMetric: 51.8100

Epoch 6: val_loss improved from 58.82215 to 51.81002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 54.8933 - MinusLogProbMetric: 54.8933 - val_loss: 51.8100 - val_MinusLogProbMetric: 51.8100 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 7/1000
2023-09-28 16:47:58.244 
Epoch 7/1000 
	 loss: 50.0956, MinusLogProbMetric: 50.0956, val_loss: 48.0802, val_MinusLogProbMetric: 48.0802

Epoch 7: val_loss improved from 51.81002 to 48.08025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 41s - loss: 50.0956 - MinusLogProbMetric: 50.0956 - val_loss: 48.0802 - val_MinusLogProbMetric: 48.0802 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 8/1000
2023-09-28 16:48:41.393 
Epoch 8/1000 
	 loss: 47.8999, MinusLogProbMetric: 47.8999, val_loss: 44.7069, val_MinusLogProbMetric: 44.7069

Epoch 8: val_loss improved from 48.08025 to 44.70691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 47.8999 - MinusLogProbMetric: 47.8999 - val_loss: 44.7069 - val_MinusLogProbMetric: 44.7069 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 9/1000
2023-09-28 16:49:24.715 
Epoch 9/1000 
	 loss: 73.3552, MinusLogProbMetric: 73.3552, val_loss: 53.8745, val_MinusLogProbMetric: 53.8745

Epoch 9: val_loss did not improve from 44.70691
196/196 - 43s - loss: 73.3552 - MinusLogProbMetric: 73.3552 - val_loss: 53.8745 - val_MinusLogProbMetric: 53.8745 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 10/1000
2023-09-28 16:50:07.551 
Epoch 10/1000 
	 loss: 50.9815, MinusLogProbMetric: 50.9815, val_loss: 47.1249, val_MinusLogProbMetric: 47.1249

Epoch 10: val_loss did not improve from 44.70691
196/196 - 43s - loss: 50.9815 - MinusLogProbMetric: 50.9815 - val_loss: 47.1249 - val_MinusLogProbMetric: 47.1249 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 11/1000
2023-09-28 16:50:50.355 
Epoch 11/1000 
	 loss: 47.0204, MinusLogProbMetric: 47.0204, val_loss: 43.9742, val_MinusLogProbMetric: 43.9742

Epoch 11: val_loss improved from 44.70691 to 43.97424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 47.0204 - MinusLogProbMetric: 47.0204 - val_loss: 43.9742 - val_MinusLogProbMetric: 43.9742 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 12/1000
2023-09-28 16:51:34.070 
Epoch 12/1000 
	 loss: 45.5315, MinusLogProbMetric: 45.5315, val_loss: 42.5282, val_MinusLogProbMetric: 42.5282

Epoch 12: val_loss improved from 43.97424 to 42.52818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 45.5315 - MinusLogProbMetric: 45.5315 - val_loss: 42.5282 - val_MinusLogProbMetric: 42.5282 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 13/1000
2023-09-28 16:52:16.728 
Epoch 13/1000 
	 loss: 42.0923, MinusLogProbMetric: 42.0923, val_loss: 44.2998, val_MinusLogProbMetric: 44.2998

Epoch 13: val_loss did not improve from 42.52818
196/196 - 42s - loss: 42.0923 - MinusLogProbMetric: 42.0923 - val_loss: 44.2998 - val_MinusLogProbMetric: 44.2998 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 14/1000
2023-09-28 16:52:59.378 
Epoch 14/1000 
	 loss: 41.3551, MinusLogProbMetric: 41.3551, val_loss: 39.0479, val_MinusLogProbMetric: 39.0479

Epoch 14: val_loss improved from 42.52818 to 39.04792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 41.3551 - MinusLogProbMetric: 41.3551 - val_loss: 39.0479 - val_MinusLogProbMetric: 39.0479 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 15/1000
2023-09-28 16:53:43.162 
Epoch 15/1000 
	 loss: 41.2614, MinusLogProbMetric: 41.2614, val_loss: 39.9746, val_MinusLogProbMetric: 39.9746

Epoch 15: val_loss did not improve from 39.04792
196/196 - 43s - loss: 41.2614 - MinusLogProbMetric: 41.2614 - val_loss: 39.9746 - val_MinusLogProbMetric: 39.9746 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 16/1000
2023-09-28 16:54:26.253 
Epoch 16/1000 
	 loss: 39.4379, MinusLogProbMetric: 39.4379, val_loss: 56.0280, val_MinusLogProbMetric: 56.0280

Epoch 16: val_loss did not improve from 39.04792
196/196 - 43s - loss: 39.4379 - MinusLogProbMetric: 39.4379 - val_loss: 56.0280 - val_MinusLogProbMetric: 56.0280 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 17/1000
2023-09-28 16:55:08.436 
Epoch 17/1000 
	 loss: 40.0530, MinusLogProbMetric: 40.0530, val_loss: 38.2720, val_MinusLogProbMetric: 38.2720

Epoch 17: val_loss improved from 39.04792 to 38.27202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 40.0530 - MinusLogProbMetric: 40.0530 - val_loss: 38.2720 - val_MinusLogProbMetric: 38.2720 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 18/1000
2023-09-28 16:55:52.339 
Epoch 18/1000 
	 loss: 38.0928, MinusLogProbMetric: 38.0928, val_loss: 40.2116, val_MinusLogProbMetric: 40.2116

Epoch 18: val_loss did not improve from 38.27202
196/196 - 43s - loss: 38.0928 - MinusLogProbMetric: 38.0928 - val_loss: 40.2116 - val_MinusLogProbMetric: 40.2116 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 19/1000
2023-09-28 16:56:34.955 
Epoch 19/1000 
	 loss: 37.7817, MinusLogProbMetric: 37.7817, val_loss: 37.5423, val_MinusLogProbMetric: 37.5423

Epoch 19: val_loss improved from 38.27202 to 37.54230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 37.7817 - MinusLogProbMetric: 37.7817 - val_loss: 37.5423 - val_MinusLogProbMetric: 37.5423 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 20/1000
2023-09-28 16:57:19.181 
Epoch 20/1000 
	 loss: 37.5914, MinusLogProbMetric: 37.5914, val_loss: 38.0172, val_MinusLogProbMetric: 38.0172

Epoch 20: val_loss did not improve from 37.54230
196/196 - 43s - loss: 37.5914 - MinusLogProbMetric: 37.5914 - val_loss: 38.0172 - val_MinusLogProbMetric: 38.0172 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 21/1000
2023-09-28 16:58:00.718 
Epoch 21/1000 
	 loss: 37.3941, MinusLogProbMetric: 37.3941, val_loss: 37.0719, val_MinusLogProbMetric: 37.0719

Epoch 21: val_loss improved from 37.54230 to 37.07190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 37.3941 - MinusLogProbMetric: 37.3941 - val_loss: 37.0719 - val_MinusLogProbMetric: 37.0719 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 22/1000
2023-09-28 16:58:43.135 
Epoch 22/1000 
	 loss: 36.6864, MinusLogProbMetric: 36.6864, val_loss: 37.1343, val_MinusLogProbMetric: 37.1343

Epoch 22: val_loss did not improve from 37.07190
196/196 - 42s - loss: 36.6864 - MinusLogProbMetric: 36.6864 - val_loss: 37.1343 - val_MinusLogProbMetric: 37.1343 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 23/1000
2023-09-28 16:59:25.172 
Epoch 23/1000 
	 loss: 36.4811, MinusLogProbMetric: 36.4811, val_loss: 37.0290, val_MinusLogProbMetric: 37.0290

Epoch 23: val_loss improved from 37.07190 to 37.02900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 36.4811 - MinusLogProbMetric: 36.4811 - val_loss: 37.0290 - val_MinusLogProbMetric: 37.0290 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 24/1000
2023-09-28 17:00:06.741 
Epoch 24/1000 
	 loss: 36.4501, MinusLogProbMetric: 36.4501, val_loss: 39.3699, val_MinusLogProbMetric: 39.3699

Epoch 24: val_loss did not improve from 37.02900
196/196 - 41s - loss: 36.4501 - MinusLogProbMetric: 36.4501 - val_loss: 39.3699 - val_MinusLogProbMetric: 39.3699 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 25/1000
2023-09-28 17:00:48.914 
Epoch 25/1000 
	 loss: 35.8588, MinusLogProbMetric: 35.8588, val_loss: 37.7537, val_MinusLogProbMetric: 37.7537

Epoch 25: val_loss did not improve from 37.02900
196/196 - 42s - loss: 35.8588 - MinusLogProbMetric: 35.8588 - val_loss: 37.7537 - val_MinusLogProbMetric: 37.7537 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 26/1000
2023-09-28 17:01:31.044 
Epoch 26/1000 
	 loss: 35.7153, MinusLogProbMetric: 35.7153, val_loss: 35.3722, val_MinusLogProbMetric: 35.3722

Epoch 26: val_loss improved from 37.02900 to 35.37225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 35.7153 - MinusLogProbMetric: 35.7153 - val_loss: 35.3722 - val_MinusLogProbMetric: 35.3722 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 27/1000
2023-09-28 17:02:13.448 
Epoch 27/1000 
	 loss: 35.5694, MinusLogProbMetric: 35.5694, val_loss: 35.2009, val_MinusLogProbMetric: 35.2009

Epoch 27: val_loss improved from 35.37225 to 35.20093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 35.5694 - MinusLogProbMetric: 35.5694 - val_loss: 35.2009 - val_MinusLogProbMetric: 35.2009 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 28/1000
2023-09-28 17:02:55.794 
Epoch 28/1000 
	 loss: 35.2157, MinusLogProbMetric: 35.2157, val_loss: 34.7873, val_MinusLogProbMetric: 34.7873

Epoch 28: val_loss improved from 35.20093 to 34.78732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 35.2157 - MinusLogProbMetric: 35.2157 - val_loss: 34.7873 - val_MinusLogProbMetric: 34.7873 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 29/1000
2023-09-28 17:03:37.640 
Epoch 29/1000 
	 loss: 35.3678, MinusLogProbMetric: 35.3678, val_loss: 38.0789, val_MinusLogProbMetric: 38.0789

Epoch 29: val_loss did not improve from 34.78732
196/196 - 41s - loss: 35.3678 - MinusLogProbMetric: 35.3678 - val_loss: 38.0789 - val_MinusLogProbMetric: 38.0789 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 30/1000
2023-09-28 17:04:18.548 
Epoch 30/1000 
	 loss: 35.1874, MinusLogProbMetric: 35.1874, val_loss: 34.5840, val_MinusLogProbMetric: 34.5840

Epoch 30: val_loss improved from 34.78732 to 34.58404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 35.1874 - MinusLogProbMetric: 35.1874 - val_loss: 34.5840 - val_MinusLogProbMetric: 34.5840 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 31/1000
2023-09-28 17:05:01.370 
Epoch 31/1000 
	 loss: 34.6439, MinusLogProbMetric: 34.6439, val_loss: 34.1805, val_MinusLogProbMetric: 34.1805

Epoch 31: val_loss improved from 34.58404 to 34.18052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 34.6439 - MinusLogProbMetric: 34.6439 - val_loss: 34.1805 - val_MinusLogProbMetric: 34.1805 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 32/1000
2023-09-28 17:05:44.267 
Epoch 32/1000 
	 loss: 34.8115, MinusLogProbMetric: 34.8115, val_loss: 35.0881, val_MinusLogProbMetric: 35.0881

Epoch 32: val_loss did not improve from 34.18052
196/196 - 42s - loss: 34.8115 - MinusLogProbMetric: 34.8115 - val_loss: 35.0881 - val_MinusLogProbMetric: 35.0881 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 33/1000
2023-09-28 17:06:25.252 
Epoch 33/1000 
	 loss: 34.5267, MinusLogProbMetric: 34.5267, val_loss: 35.2198, val_MinusLogProbMetric: 35.2198

Epoch 33: val_loss did not improve from 34.18052
196/196 - 41s - loss: 34.5267 - MinusLogProbMetric: 34.5267 - val_loss: 35.2198 - val_MinusLogProbMetric: 35.2198 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 34/1000
2023-09-28 17:07:06.576 
Epoch 34/1000 
	 loss: 34.2359, MinusLogProbMetric: 34.2359, val_loss: 36.8071, val_MinusLogProbMetric: 36.8071

Epoch 34: val_loss did not improve from 34.18052
196/196 - 41s - loss: 34.2359 - MinusLogProbMetric: 34.2359 - val_loss: 36.8071 - val_MinusLogProbMetric: 36.8071 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 35/1000
2023-09-28 17:07:47.704 
Epoch 35/1000 
	 loss: 34.1408, MinusLogProbMetric: 34.1408, val_loss: 34.0991, val_MinusLogProbMetric: 34.0991

Epoch 35: val_loss improved from 34.18052 to 34.09908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 34.1408 - MinusLogProbMetric: 34.1408 - val_loss: 34.0991 - val_MinusLogProbMetric: 34.0991 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 36/1000
2023-09-28 17:08:30.431 
Epoch 36/1000 
	 loss: 34.1435, MinusLogProbMetric: 34.1435, val_loss: 33.8627, val_MinusLogProbMetric: 33.8627

Epoch 36: val_loss improved from 34.09908 to 33.86274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 34.1435 - MinusLogProbMetric: 34.1435 - val_loss: 33.8627 - val_MinusLogProbMetric: 33.8627 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 37/1000
2023-09-28 17:09:13.045 
Epoch 37/1000 
	 loss: 34.0337, MinusLogProbMetric: 34.0337, val_loss: 35.0101, val_MinusLogProbMetric: 35.0101

Epoch 37: val_loss did not improve from 33.86274
196/196 - 42s - loss: 34.0337 - MinusLogProbMetric: 34.0337 - val_loss: 35.0101 - val_MinusLogProbMetric: 35.0101 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 38/1000
2023-09-28 17:09:54.887 
Epoch 38/1000 
	 loss: 33.8983, MinusLogProbMetric: 33.8983, val_loss: 35.5747, val_MinusLogProbMetric: 35.5747

Epoch 38: val_loss did not improve from 33.86274
196/196 - 42s - loss: 33.8983 - MinusLogProbMetric: 33.8983 - val_loss: 35.5747 - val_MinusLogProbMetric: 35.5747 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 39/1000
2023-09-28 17:10:36.670 
Epoch 39/1000 
	 loss: 33.7771, MinusLogProbMetric: 33.7771, val_loss: 34.0053, val_MinusLogProbMetric: 34.0053

Epoch 39: val_loss did not improve from 33.86274
196/196 - 42s - loss: 33.7771 - MinusLogProbMetric: 33.7771 - val_loss: 34.0053 - val_MinusLogProbMetric: 34.0053 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 40/1000
2023-09-28 17:11:19.004 
Epoch 40/1000 
	 loss: 33.5591, MinusLogProbMetric: 33.5591, val_loss: 33.7813, val_MinusLogProbMetric: 33.7813

Epoch 40: val_loss improved from 33.86274 to 33.78128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 33.5591 - MinusLogProbMetric: 33.5591 - val_loss: 33.7813 - val_MinusLogProbMetric: 33.7813 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 41/1000
2023-09-28 17:12:01.857 
Epoch 41/1000 
	 loss: 33.5044, MinusLogProbMetric: 33.5044, val_loss: 34.6613, val_MinusLogProbMetric: 34.6613

Epoch 41: val_loss did not improve from 33.78128
196/196 - 42s - loss: 33.5044 - MinusLogProbMetric: 33.5044 - val_loss: 34.6613 - val_MinusLogProbMetric: 34.6613 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 42/1000
2023-09-28 17:12:45.138 
Epoch 42/1000 
	 loss: 33.6844, MinusLogProbMetric: 33.6844, val_loss: 33.9985, val_MinusLogProbMetric: 33.9985

Epoch 42: val_loss did not improve from 33.78128
196/196 - 43s - loss: 33.6844 - MinusLogProbMetric: 33.6844 - val_loss: 33.9985 - val_MinusLogProbMetric: 33.9985 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 43/1000
2023-09-28 17:13:27.446 
Epoch 43/1000 
	 loss: 33.3211, MinusLogProbMetric: 33.3211, val_loss: 33.0528, val_MinusLogProbMetric: 33.0528

Epoch 43: val_loss improved from 33.78128 to 33.05278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 33.3211 - MinusLogProbMetric: 33.3211 - val_loss: 33.0528 - val_MinusLogProbMetric: 33.0528 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 44/1000
2023-09-28 17:14:11.310 
Epoch 44/1000 
	 loss: 33.4993, MinusLogProbMetric: 33.4993, val_loss: 33.7399, val_MinusLogProbMetric: 33.7399

Epoch 44: val_loss did not improve from 33.05278
196/196 - 43s - loss: 33.4993 - MinusLogProbMetric: 33.4993 - val_loss: 33.7399 - val_MinusLogProbMetric: 33.7399 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 45/1000
2023-09-28 17:14:53.134 
Epoch 45/1000 
	 loss: 33.1739, MinusLogProbMetric: 33.1739, val_loss: 33.0780, val_MinusLogProbMetric: 33.0780

Epoch 45: val_loss did not improve from 33.05278
196/196 - 42s - loss: 33.1739 - MinusLogProbMetric: 33.1739 - val_loss: 33.0780 - val_MinusLogProbMetric: 33.0780 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 46/1000
2023-09-28 17:15:34.823 
Epoch 46/1000 
	 loss: 33.2067, MinusLogProbMetric: 33.2067, val_loss: 32.7374, val_MinusLogProbMetric: 32.7374

Epoch 46: val_loss improved from 33.05278 to 32.73738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 33.2067 - MinusLogProbMetric: 33.2067 - val_loss: 32.7374 - val_MinusLogProbMetric: 32.7374 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 47/1000
2023-09-28 17:16:15.518 
Epoch 47/1000 
	 loss: 33.3075, MinusLogProbMetric: 33.3075, val_loss: 33.1166, val_MinusLogProbMetric: 33.1166

Epoch 47: val_loss did not improve from 32.73738
196/196 - 40s - loss: 33.3075 - MinusLogProbMetric: 33.3075 - val_loss: 33.1166 - val_MinusLogProbMetric: 33.1166 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 48/1000
2023-09-28 17:16:56.864 
Epoch 48/1000 
	 loss: 33.1713, MinusLogProbMetric: 33.1713, val_loss: 33.8736, val_MinusLogProbMetric: 33.8736

Epoch 48: val_loss did not improve from 32.73738
196/196 - 41s - loss: 33.1713 - MinusLogProbMetric: 33.1713 - val_loss: 33.8736 - val_MinusLogProbMetric: 33.8736 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 49/1000
2023-09-28 17:17:37.580 
Epoch 49/1000 
	 loss: 32.9365, MinusLogProbMetric: 32.9365, val_loss: 33.9190, val_MinusLogProbMetric: 33.9190

Epoch 49: val_loss did not improve from 32.73738
196/196 - 41s - loss: 32.9365 - MinusLogProbMetric: 32.9365 - val_loss: 33.9190 - val_MinusLogProbMetric: 33.9190 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 50/1000
2023-09-28 17:18:18.114 
Epoch 50/1000 
	 loss: 33.0957, MinusLogProbMetric: 33.0957, val_loss: 33.4366, val_MinusLogProbMetric: 33.4366

Epoch 50: val_loss did not improve from 32.73738
196/196 - 41s - loss: 33.0957 - MinusLogProbMetric: 33.0957 - val_loss: 33.4366 - val_MinusLogProbMetric: 33.4366 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 51/1000
2023-09-28 17:18:59.647 
Epoch 51/1000 
	 loss: 32.8986, MinusLogProbMetric: 32.8986, val_loss: 32.1392, val_MinusLogProbMetric: 32.1392

Epoch 51: val_loss improved from 32.73738 to 32.13916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 32.8986 - MinusLogProbMetric: 32.8986 - val_loss: 32.1392 - val_MinusLogProbMetric: 32.1392 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 52/1000
2023-09-28 17:19:41.488 
Epoch 52/1000 
	 loss: 32.7366, MinusLogProbMetric: 32.7366, val_loss: 32.3593, val_MinusLogProbMetric: 32.3593

Epoch 52: val_loss did not improve from 32.13916
196/196 - 41s - loss: 32.7366 - MinusLogProbMetric: 32.7366 - val_loss: 32.3593 - val_MinusLogProbMetric: 32.3593 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 53/1000
2023-09-28 17:20:20.723 
Epoch 53/1000 
	 loss: 32.8425, MinusLogProbMetric: 32.8425, val_loss: 34.7977, val_MinusLogProbMetric: 34.7977

Epoch 53: val_loss did not improve from 32.13916
196/196 - 39s - loss: 32.8425 - MinusLogProbMetric: 32.8425 - val_loss: 34.7977 - val_MinusLogProbMetric: 34.7977 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 54/1000
2023-09-28 17:21:02.985 
Epoch 54/1000 
	 loss: 32.6134, MinusLogProbMetric: 32.6134, val_loss: 33.5402, val_MinusLogProbMetric: 33.5402

Epoch 54: val_loss did not improve from 32.13916
196/196 - 42s - loss: 32.6134 - MinusLogProbMetric: 32.6134 - val_loss: 33.5402 - val_MinusLogProbMetric: 33.5402 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 55/1000
2023-09-28 17:21:44.133 
Epoch 55/1000 
	 loss: 32.6077, MinusLogProbMetric: 32.6077, val_loss: 33.4096, val_MinusLogProbMetric: 33.4096

Epoch 55: val_loss did not improve from 32.13916
196/196 - 41s - loss: 32.6077 - MinusLogProbMetric: 32.6077 - val_loss: 33.4096 - val_MinusLogProbMetric: 33.4096 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 56/1000
2023-09-28 17:22:22.939 
Epoch 56/1000 
	 loss: 32.4485, MinusLogProbMetric: 32.4485, val_loss: 33.6848, val_MinusLogProbMetric: 33.6848

Epoch 56: val_loss did not improve from 32.13916
196/196 - 39s - loss: 32.4485 - MinusLogProbMetric: 32.4485 - val_loss: 33.6848 - val_MinusLogProbMetric: 33.6848 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 57/1000
2023-09-28 17:23:03.411 
Epoch 57/1000 
	 loss: 32.7355, MinusLogProbMetric: 32.7355, val_loss: 32.0322, val_MinusLogProbMetric: 32.0322

Epoch 57: val_loss improved from 32.13916 to 32.03222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 41s - loss: 32.7355 - MinusLogProbMetric: 32.7355 - val_loss: 32.0322 - val_MinusLogProbMetric: 32.0322 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 58/1000
2023-09-28 17:23:43.741 
Epoch 58/1000 
	 loss: 32.4067, MinusLogProbMetric: 32.4067, val_loss: 32.8286, val_MinusLogProbMetric: 32.8286

Epoch 58: val_loss did not improve from 32.03222
196/196 - 40s - loss: 32.4067 - MinusLogProbMetric: 32.4067 - val_loss: 32.8286 - val_MinusLogProbMetric: 32.8286 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 59/1000
2023-09-28 17:24:24.370 
Epoch 59/1000 
	 loss: 32.3472, MinusLogProbMetric: 32.3472, val_loss: 33.1105, val_MinusLogProbMetric: 33.1105

Epoch 59: val_loss did not improve from 32.03222
196/196 - 41s - loss: 32.3472 - MinusLogProbMetric: 32.3472 - val_loss: 33.1105 - val_MinusLogProbMetric: 33.1105 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 60/1000
2023-09-28 17:25:04.536 
Epoch 60/1000 
	 loss: 32.3538, MinusLogProbMetric: 32.3538, val_loss: 32.4803, val_MinusLogProbMetric: 32.4803

Epoch 60: val_loss did not improve from 32.03222
196/196 - 40s - loss: 32.3538 - MinusLogProbMetric: 32.3538 - val_loss: 32.4803 - val_MinusLogProbMetric: 32.4803 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 61/1000
2023-09-28 17:25:44.266 
Epoch 61/1000 
	 loss: 32.1485, MinusLogProbMetric: 32.1485, val_loss: 33.9641, val_MinusLogProbMetric: 33.9641

Epoch 61: val_loss did not improve from 32.03222
196/196 - 40s - loss: 32.1485 - MinusLogProbMetric: 32.1485 - val_loss: 33.9641 - val_MinusLogProbMetric: 33.9641 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 62/1000
2023-09-28 17:26:25.586 
Epoch 62/1000 
	 loss: 32.3034, MinusLogProbMetric: 32.3034, val_loss: 32.7118, val_MinusLogProbMetric: 32.7118

Epoch 62: val_loss did not improve from 32.03222
196/196 - 41s - loss: 32.3034 - MinusLogProbMetric: 32.3034 - val_loss: 32.7118 - val_MinusLogProbMetric: 32.7118 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 63/1000
2023-09-28 17:27:06.293 
Epoch 63/1000 
	 loss: 32.2759, MinusLogProbMetric: 32.2759, val_loss: 32.6350, val_MinusLogProbMetric: 32.6350

Epoch 63: val_loss did not improve from 32.03222
196/196 - 41s - loss: 32.2759 - MinusLogProbMetric: 32.2759 - val_loss: 32.6350 - val_MinusLogProbMetric: 32.6350 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 64/1000
2023-09-28 17:27:44.315 
Epoch 64/1000 
	 loss: 32.0509, MinusLogProbMetric: 32.0509, val_loss: 31.5160, val_MinusLogProbMetric: 31.5160

Epoch 64: val_loss improved from 32.03222 to 31.51600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 39s - loss: 32.0509 - MinusLogProbMetric: 32.0509 - val_loss: 31.5160 - val_MinusLogProbMetric: 31.5160 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 65/1000
2023-09-28 17:28:25.050 
Epoch 65/1000 
	 loss: 32.1099, MinusLogProbMetric: 32.1099, val_loss: 32.0600, val_MinusLogProbMetric: 32.0600

Epoch 65: val_loss did not improve from 31.51600
196/196 - 40s - loss: 32.1099 - MinusLogProbMetric: 32.1099 - val_loss: 32.0600 - val_MinusLogProbMetric: 32.0600 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 66/1000
2023-09-28 17:29:04.249 
Epoch 66/1000 
	 loss: 32.2519, MinusLogProbMetric: 32.2519, val_loss: 32.9804, val_MinusLogProbMetric: 32.9804

Epoch 66: val_loss did not improve from 31.51600
196/196 - 39s - loss: 32.2519 - MinusLogProbMetric: 32.2519 - val_loss: 32.9804 - val_MinusLogProbMetric: 32.9804 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 67/1000
2023-09-28 17:29:44.208 
Epoch 67/1000 
	 loss: 32.0332, MinusLogProbMetric: 32.0332, val_loss: 32.6961, val_MinusLogProbMetric: 32.6961

Epoch 67: val_loss did not improve from 31.51600
196/196 - 40s - loss: 32.0332 - MinusLogProbMetric: 32.0332 - val_loss: 32.6961 - val_MinusLogProbMetric: 32.6961 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 68/1000
2023-09-28 17:30:25.718 
Epoch 68/1000 
	 loss: 32.0897, MinusLogProbMetric: 32.0897, val_loss: 33.4825, val_MinusLogProbMetric: 33.4825

Epoch 68: val_loss did not improve from 31.51600
196/196 - 42s - loss: 32.0897 - MinusLogProbMetric: 32.0897 - val_loss: 33.4825 - val_MinusLogProbMetric: 33.4825 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 69/1000
2023-09-28 17:31:06.069 
Epoch 69/1000 
	 loss: 31.9288, MinusLogProbMetric: 31.9288, val_loss: 32.6847, val_MinusLogProbMetric: 32.6847

Epoch 69: val_loss did not improve from 31.51600
196/196 - 40s - loss: 31.9288 - MinusLogProbMetric: 31.9288 - val_loss: 32.6847 - val_MinusLogProbMetric: 32.6847 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 70/1000
2023-09-28 17:31:43.862 
Epoch 70/1000 
	 loss: 31.9419, MinusLogProbMetric: 31.9419, val_loss: 33.6001, val_MinusLogProbMetric: 33.6001

Epoch 70: val_loss did not improve from 31.51600
196/196 - 38s - loss: 31.9419 - MinusLogProbMetric: 31.9419 - val_loss: 33.6001 - val_MinusLogProbMetric: 33.6001 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 71/1000
2023-09-28 17:32:25.303 
Epoch 71/1000 
	 loss: 31.9419, MinusLogProbMetric: 31.9419, val_loss: 31.7897, val_MinusLogProbMetric: 31.7897

Epoch 71: val_loss did not improve from 31.51600
196/196 - 41s - loss: 31.9419 - MinusLogProbMetric: 31.9419 - val_loss: 31.7897 - val_MinusLogProbMetric: 31.7897 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 72/1000
2023-09-28 17:33:05.252 
Epoch 72/1000 
	 loss: 31.9173, MinusLogProbMetric: 31.9173, val_loss: 31.5094, val_MinusLogProbMetric: 31.5094

Epoch 72: val_loss improved from 31.51600 to 31.50938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 41s - loss: 31.9173 - MinusLogProbMetric: 31.9173 - val_loss: 31.5094 - val_MinusLogProbMetric: 31.5094 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 73/1000
2023-09-28 17:33:49.084 
Epoch 73/1000 
	 loss: 31.7262, MinusLogProbMetric: 31.7262, val_loss: 31.6112, val_MinusLogProbMetric: 31.6112

Epoch 73: val_loss did not improve from 31.50938
196/196 - 43s - loss: 31.7262 - MinusLogProbMetric: 31.7262 - val_loss: 31.6112 - val_MinusLogProbMetric: 31.6112 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 74/1000
2023-09-28 17:34:30.727 
Epoch 74/1000 
	 loss: 31.7028, MinusLogProbMetric: 31.7028, val_loss: 33.1051, val_MinusLogProbMetric: 33.1051

Epoch 74: val_loss did not improve from 31.50938
196/196 - 42s - loss: 31.7028 - MinusLogProbMetric: 31.7028 - val_loss: 33.1051 - val_MinusLogProbMetric: 33.1051 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 75/1000
2023-09-28 17:35:13.778 
Epoch 75/1000 
	 loss: 31.8490, MinusLogProbMetric: 31.8490, val_loss: 34.1232, val_MinusLogProbMetric: 34.1232

Epoch 75: val_loss did not improve from 31.50938
196/196 - 43s - loss: 31.8490 - MinusLogProbMetric: 31.8490 - val_loss: 34.1232 - val_MinusLogProbMetric: 34.1232 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 76/1000
2023-09-28 17:35:56.560 
Epoch 76/1000 
	 loss: 31.7936, MinusLogProbMetric: 31.7936, val_loss: 31.4340, val_MinusLogProbMetric: 31.4340

Epoch 76: val_loss improved from 31.50938 to 31.43402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 31.7936 - MinusLogProbMetric: 31.7936 - val_loss: 31.4340 - val_MinusLogProbMetric: 31.4340 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 77/1000
2023-09-28 17:36:40.449 
Epoch 77/1000 
	 loss: 31.3717, MinusLogProbMetric: 31.3717, val_loss: 32.0755, val_MinusLogProbMetric: 32.0755

Epoch 77: val_loss did not improve from 31.43402
196/196 - 43s - loss: 31.3717 - MinusLogProbMetric: 31.3717 - val_loss: 32.0755 - val_MinusLogProbMetric: 32.0755 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 78/1000
2023-09-28 17:37:23.286 
Epoch 78/1000 
	 loss: 31.6487, MinusLogProbMetric: 31.6487, val_loss: 31.4559, val_MinusLogProbMetric: 31.4559

Epoch 78: val_loss did not improve from 31.43402
196/196 - 43s - loss: 31.6487 - MinusLogProbMetric: 31.6487 - val_loss: 31.4559 - val_MinusLogProbMetric: 31.4559 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 79/1000
2023-09-28 17:38:05.716 
Epoch 79/1000 
	 loss: 31.4706, MinusLogProbMetric: 31.4706, val_loss: 31.9279, val_MinusLogProbMetric: 31.9279

Epoch 79: val_loss did not improve from 31.43402
196/196 - 42s - loss: 31.4706 - MinusLogProbMetric: 31.4706 - val_loss: 31.9279 - val_MinusLogProbMetric: 31.9279 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 80/1000
2023-09-28 17:38:48.481 
Epoch 80/1000 
	 loss: 31.8315, MinusLogProbMetric: 31.8315, val_loss: 32.8310, val_MinusLogProbMetric: 32.8310

Epoch 80: val_loss did not improve from 31.43402
196/196 - 43s - loss: 31.8315 - MinusLogProbMetric: 31.8315 - val_loss: 32.8310 - val_MinusLogProbMetric: 32.8310 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 81/1000
2023-09-28 17:39:31.447 
Epoch 81/1000 
	 loss: 31.4597, MinusLogProbMetric: 31.4597, val_loss: 31.1504, val_MinusLogProbMetric: 31.1504

Epoch 81: val_loss improved from 31.43402 to 31.15035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 31.4597 - MinusLogProbMetric: 31.4597 - val_loss: 31.1504 - val_MinusLogProbMetric: 31.1504 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 82/1000
2023-09-28 17:40:14.930 
Epoch 82/1000 
	 loss: 31.3995, MinusLogProbMetric: 31.3995, val_loss: 31.2643, val_MinusLogProbMetric: 31.2643

Epoch 82: val_loss did not improve from 31.15035
196/196 - 43s - loss: 31.3995 - MinusLogProbMetric: 31.3995 - val_loss: 31.2643 - val_MinusLogProbMetric: 31.2643 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 83/1000
2023-09-28 17:40:58.020 
Epoch 83/1000 
	 loss: 31.5576, MinusLogProbMetric: 31.5576, val_loss: 31.3921, val_MinusLogProbMetric: 31.3921

Epoch 83: val_loss did not improve from 31.15035
196/196 - 43s - loss: 31.5576 - MinusLogProbMetric: 31.5576 - val_loss: 31.3921 - val_MinusLogProbMetric: 31.3921 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 84/1000
2023-09-28 17:41:40.692 
Epoch 84/1000 
	 loss: 31.3426, MinusLogProbMetric: 31.3426, val_loss: 32.1775, val_MinusLogProbMetric: 32.1775

Epoch 84: val_loss did not improve from 31.15035
196/196 - 43s - loss: 31.3426 - MinusLogProbMetric: 31.3426 - val_loss: 32.1775 - val_MinusLogProbMetric: 32.1775 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 85/1000
2023-09-28 17:42:23.495 
Epoch 85/1000 
	 loss: 31.3704, MinusLogProbMetric: 31.3704, val_loss: 31.2936, val_MinusLogProbMetric: 31.2936

Epoch 85: val_loss did not improve from 31.15035
196/196 - 43s - loss: 31.3704 - MinusLogProbMetric: 31.3704 - val_loss: 31.2936 - val_MinusLogProbMetric: 31.2936 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 86/1000
2023-09-28 17:43:05.318 
Epoch 86/1000 
	 loss: 31.1993, MinusLogProbMetric: 31.1993, val_loss: 30.9169, val_MinusLogProbMetric: 30.9169

Epoch 86: val_loss improved from 31.15035 to 30.91691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 31.1993 - MinusLogProbMetric: 31.1993 - val_loss: 30.9169 - val_MinusLogProbMetric: 30.9169 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 87/1000
2023-09-28 17:43:48.807 
Epoch 87/1000 
	 loss: 31.2810, MinusLogProbMetric: 31.2810, val_loss: 31.2480, val_MinusLogProbMetric: 31.2480

Epoch 87: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.2810 - MinusLogProbMetric: 31.2810 - val_loss: 31.2480 - val_MinusLogProbMetric: 31.2480 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 88/1000
2023-09-28 17:44:31.842 
Epoch 88/1000 
	 loss: 31.2942, MinusLogProbMetric: 31.2942, val_loss: 31.0158, val_MinusLogProbMetric: 31.0158

Epoch 88: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.2942 - MinusLogProbMetric: 31.2942 - val_loss: 31.0158 - val_MinusLogProbMetric: 31.0158 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 89/1000
2023-09-28 17:45:14.676 
Epoch 89/1000 
	 loss: 31.2269, MinusLogProbMetric: 31.2269, val_loss: 32.7924, val_MinusLogProbMetric: 32.7924

Epoch 89: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.2269 - MinusLogProbMetric: 31.2269 - val_loss: 32.7924 - val_MinusLogProbMetric: 32.7924 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 90/1000
2023-09-28 17:45:57.747 
Epoch 90/1000 
	 loss: 31.4292, MinusLogProbMetric: 31.4292, val_loss: 31.8110, val_MinusLogProbMetric: 31.8110

Epoch 90: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.4292 - MinusLogProbMetric: 31.4292 - val_loss: 31.8110 - val_MinusLogProbMetric: 31.8110 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 91/1000
2023-09-28 17:46:40.715 
Epoch 91/1000 
	 loss: 31.1719, MinusLogProbMetric: 31.1719, val_loss: 31.8935, val_MinusLogProbMetric: 31.8935

Epoch 91: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.1719 - MinusLogProbMetric: 31.1719 - val_loss: 31.8935 - val_MinusLogProbMetric: 31.8935 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 92/1000
2023-09-28 17:47:23.631 
Epoch 92/1000 
	 loss: 31.3151, MinusLogProbMetric: 31.3151, val_loss: 33.2926, val_MinusLogProbMetric: 33.2926

Epoch 92: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.3151 - MinusLogProbMetric: 31.3151 - val_loss: 33.2926 - val_MinusLogProbMetric: 33.2926 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 93/1000
2023-09-28 17:48:06.575 
Epoch 93/1000 
	 loss: 31.2725, MinusLogProbMetric: 31.2725, val_loss: 33.4720, val_MinusLogProbMetric: 33.4720

Epoch 93: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.2725 - MinusLogProbMetric: 31.2725 - val_loss: 33.4720 - val_MinusLogProbMetric: 33.4720 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 94/1000
2023-09-28 17:48:49.747 
Epoch 94/1000 
	 loss: 31.0179, MinusLogProbMetric: 31.0179, val_loss: 31.7628, val_MinusLogProbMetric: 31.7628

Epoch 94: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.0179 - MinusLogProbMetric: 31.0179 - val_loss: 31.7628 - val_MinusLogProbMetric: 31.7628 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 95/1000
2023-09-28 17:49:32.626 
Epoch 95/1000 
	 loss: 31.2328, MinusLogProbMetric: 31.2328, val_loss: 32.4753, val_MinusLogProbMetric: 32.4753

Epoch 95: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.2328 - MinusLogProbMetric: 31.2328 - val_loss: 32.4753 - val_MinusLogProbMetric: 32.4753 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 96/1000
2023-09-28 17:50:15.659 
Epoch 96/1000 
	 loss: 31.1386, MinusLogProbMetric: 31.1386, val_loss: 31.1148, val_MinusLogProbMetric: 31.1148

Epoch 96: val_loss did not improve from 30.91691
196/196 - 43s - loss: 31.1386 - MinusLogProbMetric: 31.1386 - val_loss: 31.1148 - val_MinusLogProbMetric: 31.1148 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 97/1000
2023-09-28 17:50:58.570 
Epoch 97/1000 
	 loss: 31.3055, MinusLogProbMetric: 31.3055, val_loss: 30.7688, val_MinusLogProbMetric: 30.7688

Epoch 97: val_loss improved from 30.91691 to 30.76885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 31.3055 - MinusLogProbMetric: 31.3055 - val_loss: 30.7688 - val_MinusLogProbMetric: 30.7688 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 98/1000
2023-09-28 17:51:42.575 
Epoch 98/1000 
	 loss: 30.9064, MinusLogProbMetric: 30.9064, val_loss: 31.2989, val_MinusLogProbMetric: 31.2989

Epoch 98: val_loss did not improve from 30.76885
196/196 - 43s - loss: 30.9064 - MinusLogProbMetric: 30.9064 - val_loss: 31.2989 - val_MinusLogProbMetric: 31.2989 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 99/1000
2023-09-28 17:52:25.484 
Epoch 99/1000 
	 loss: 31.0870, MinusLogProbMetric: 31.0870, val_loss: 30.5424, val_MinusLogProbMetric: 30.5424

Epoch 99: val_loss improved from 30.76885 to 30.54235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 31.0870 - MinusLogProbMetric: 31.0870 - val_loss: 30.5424 - val_MinusLogProbMetric: 30.5424 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 100/1000
2023-09-28 17:53:08.937 
Epoch 100/1000 
	 loss: 31.0125, MinusLogProbMetric: 31.0125, val_loss: 31.1846, val_MinusLogProbMetric: 31.1846

Epoch 100: val_loss did not improve from 30.54235
196/196 - 43s - loss: 31.0125 - MinusLogProbMetric: 31.0125 - val_loss: 31.1846 - val_MinusLogProbMetric: 31.1846 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 101/1000
2023-09-28 17:53:51.198 
Epoch 101/1000 
	 loss: 30.7681, MinusLogProbMetric: 30.7681, val_loss: 30.8038, val_MinusLogProbMetric: 30.8038

Epoch 101: val_loss did not improve from 30.54235
196/196 - 42s - loss: 30.7681 - MinusLogProbMetric: 30.7681 - val_loss: 30.8038 - val_MinusLogProbMetric: 30.8038 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 102/1000
2023-09-28 17:54:33.876 
Epoch 102/1000 
	 loss: 31.2475, MinusLogProbMetric: 31.2475, val_loss: 32.5677, val_MinusLogProbMetric: 32.5677

Epoch 102: val_loss did not improve from 30.54235
196/196 - 43s - loss: 31.2475 - MinusLogProbMetric: 31.2475 - val_loss: 32.5677 - val_MinusLogProbMetric: 32.5677 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 103/1000
2023-09-28 17:55:16.203 
Epoch 103/1000 
	 loss: 30.9691, MinusLogProbMetric: 30.9691, val_loss: 30.9517, val_MinusLogProbMetric: 30.9517

Epoch 103: val_loss did not improve from 30.54235
196/196 - 42s - loss: 30.9691 - MinusLogProbMetric: 30.9691 - val_loss: 30.9517 - val_MinusLogProbMetric: 30.9517 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 104/1000
2023-09-28 17:55:59.237 
Epoch 104/1000 
	 loss: 30.9649, MinusLogProbMetric: 30.9649, val_loss: 31.5413, val_MinusLogProbMetric: 31.5413

Epoch 104: val_loss did not improve from 30.54235
196/196 - 43s - loss: 30.9649 - MinusLogProbMetric: 30.9649 - val_loss: 31.5413 - val_MinusLogProbMetric: 31.5413 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 105/1000
2023-09-28 17:56:42.099 
Epoch 105/1000 
	 loss: 30.8343, MinusLogProbMetric: 30.8343, val_loss: 30.8446, val_MinusLogProbMetric: 30.8446

Epoch 105: val_loss did not improve from 30.54235
196/196 - 43s - loss: 30.8343 - MinusLogProbMetric: 30.8343 - val_loss: 30.8446 - val_MinusLogProbMetric: 30.8446 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 106/1000
2023-09-28 17:57:24.578 
Epoch 106/1000 
	 loss: 30.8562, MinusLogProbMetric: 30.8562, val_loss: 30.5747, val_MinusLogProbMetric: 30.5747

Epoch 106: val_loss did not improve from 30.54235
196/196 - 42s - loss: 30.8562 - MinusLogProbMetric: 30.8562 - val_loss: 30.5747 - val_MinusLogProbMetric: 30.5747 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 107/1000
2023-09-28 17:58:07.398 
Epoch 107/1000 
	 loss: 30.8336, MinusLogProbMetric: 30.8336, val_loss: 30.7785, val_MinusLogProbMetric: 30.7785

Epoch 107: val_loss did not improve from 30.54235
196/196 - 43s - loss: 30.8336 - MinusLogProbMetric: 30.8336 - val_loss: 30.7785 - val_MinusLogProbMetric: 30.7785 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 108/1000
2023-09-28 17:58:47.688 
Epoch 108/1000 
	 loss: 30.9345, MinusLogProbMetric: 30.9345, val_loss: 31.4226, val_MinusLogProbMetric: 31.4226

Epoch 108: val_loss did not improve from 30.54235
196/196 - 40s - loss: 30.9345 - MinusLogProbMetric: 30.9345 - val_loss: 31.4226 - val_MinusLogProbMetric: 31.4226 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 109/1000
2023-09-28 17:59:23.954 
Epoch 109/1000 
	 loss: 30.9125, MinusLogProbMetric: 30.9125, val_loss: 31.9860, val_MinusLogProbMetric: 31.9860

Epoch 109: val_loss did not improve from 30.54235
196/196 - 36s - loss: 30.9125 - MinusLogProbMetric: 30.9125 - val_loss: 31.9860 - val_MinusLogProbMetric: 31.9860 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 110/1000
2023-09-28 18:00:03.108 
Epoch 110/1000 
	 loss: 31.1214, MinusLogProbMetric: 31.1214, val_loss: 31.2456, val_MinusLogProbMetric: 31.2456

Epoch 110: val_loss did not improve from 30.54235
196/196 - 39s - loss: 31.1214 - MinusLogProbMetric: 31.1214 - val_loss: 31.2456 - val_MinusLogProbMetric: 31.2456 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 111/1000
2023-09-28 18:00:41.467 
Epoch 111/1000 
	 loss: 30.7783, MinusLogProbMetric: 30.7783, val_loss: 32.0352, val_MinusLogProbMetric: 32.0352

Epoch 111: val_loss did not improve from 30.54235
196/196 - 38s - loss: 30.7783 - MinusLogProbMetric: 30.7783 - val_loss: 32.0352 - val_MinusLogProbMetric: 32.0352 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 112/1000
2023-09-28 18:01:20.992 
Epoch 112/1000 
	 loss: 30.9403, MinusLogProbMetric: 30.9403, val_loss: 31.5845, val_MinusLogProbMetric: 31.5845

Epoch 112: val_loss did not improve from 30.54235
196/196 - 40s - loss: 30.9403 - MinusLogProbMetric: 30.9403 - val_loss: 31.5845 - val_MinusLogProbMetric: 31.5845 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 113/1000
2023-09-28 18:01:58.346 
Epoch 113/1000 
	 loss: 30.6583, MinusLogProbMetric: 30.6583, val_loss: 32.2532, val_MinusLogProbMetric: 32.2532

Epoch 113: val_loss did not improve from 30.54235
196/196 - 37s - loss: 30.6583 - MinusLogProbMetric: 30.6583 - val_loss: 32.2532 - val_MinusLogProbMetric: 32.2532 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 114/1000
2023-09-28 18:02:34.488 
Epoch 114/1000 
	 loss: 30.8437, MinusLogProbMetric: 30.8437, val_loss: 31.0425, val_MinusLogProbMetric: 31.0425

Epoch 114: val_loss did not improve from 30.54235
196/196 - 36s - loss: 30.8437 - MinusLogProbMetric: 30.8437 - val_loss: 31.0425 - val_MinusLogProbMetric: 31.0425 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 115/1000
2023-09-28 18:03:11.440 
Epoch 115/1000 
	 loss: 30.6258, MinusLogProbMetric: 30.6258, val_loss: 31.2373, val_MinusLogProbMetric: 31.2373

Epoch 115: val_loss did not improve from 30.54235
196/196 - 37s - loss: 30.6258 - MinusLogProbMetric: 30.6258 - val_loss: 31.2373 - val_MinusLogProbMetric: 31.2373 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 116/1000
2023-09-28 18:03:47.201 
Epoch 116/1000 
	 loss: 30.7634, MinusLogProbMetric: 30.7634, val_loss: 33.6618, val_MinusLogProbMetric: 33.6618

Epoch 116: val_loss did not improve from 30.54235
196/196 - 36s - loss: 30.7634 - MinusLogProbMetric: 30.7634 - val_loss: 33.6618 - val_MinusLogProbMetric: 33.6618 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 117/1000
2023-09-28 18:04:22.839 
Epoch 117/1000 
	 loss: 30.8819, MinusLogProbMetric: 30.8819, val_loss: 32.3703, val_MinusLogProbMetric: 32.3703

Epoch 117: val_loss did not improve from 30.54235
196/196 - 36s - loss: 30.8819 - MinusLogProbMetric: 30.8819 - val_loss: 32.3703 - val_MinusLogProbMetric: 32.3703 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 118/1000
2023-09-28 18:05:00.291 
Epoch 118/1000 
	 loss: 30.5795, MinusLogProbMetric: 30.5795, val_loss: 32.0190, val_MinusLogProbMetric: 32.0190

Epoch 118: val_loss did not improve from 30.54235
196/196 - 37s - loss: 30.5795 - MinusLogProbMetric: 30.5795 - val_loss: 32.0190 - val_MinusLogProbMetric: 32.0190 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 119/1000
2023-09-28 18:05:38.700 
Epoch 119/1000 
	 loss: 30.6920, MinusLogProbMetric: 30.6920, val_loss: 31.7486, val_MinusLogProbMetric: 31.7486

Epoch 119: val_loss did not improve from 30.54235
196/196 - 38s - loss: 30.6920 - MinusLogProbMetric: 30.6920 - val_loss: 31.7486 - val_MinusLogProbMetric: 31.7486 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 120/1000
2023-09-28 18:06:17.626 
Epoch 120/1000 
	 loss: 30.6429, MinusLogProbMetric: 30.6429, val_loss: 31.0638, val_MinusLogProbMetric: 31.0638

Epoch 120: val_loss did not improve from 30.54235
196/196 - 39s - loss: 30.6429 - MinusLogProbMetric: 30.6429 - val_loss: 31.0638 - val_MinusLogProbMetric: 31.0638 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 121/1000
2023-09-28 18:06:53.256 
Epoch 121/1000 
	 loss: 30.4893, MinusLogProbMetric: 30.4893, val_loss: 31.2689, val_MinusLogProbMetric: 31.2689

Epoch 121: val_loss did not improve from 30.54235
196/196 - 36s - loss: 30.4893 - MinusLogProbMetric: 30.4893 - val_loss: 31.2689 - val_MinusLogProbMetric: 31.2689 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 122/1000
2023-09-28 18:07:29.489 
Epoch 122/1000 
	 loss: 30.5822, MinusLogProbMetric: 30.5822, val_loss: 32.3552, val_MinusLogProbMetric: 32.3552

Epoch 122: val_loss did not improve from 30.54235
196/196 - 36s - loss: 30.5822 - MinusLogProbMetric: 30.5822 - val_loss: 32.3552 - val_MinusLogProbMetric: 32.3552 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 123/1000
2023-09-28 18:08:04.813 
Epoch 123/1000 
	 loss: 30.7231, MinusLogProbMetric: 30.7231, val_loss: 31.4694, val_MinusLogProbMetric: 31.4694

Epoch 123: val_loss did not improve from 30.54235
196/196 - 35s - loss: 30.7231 - MinusLogProbMetric: 30.7231 - val_loss: 31.4694 - val_MinusLogProbMetric: 31.4694 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 124/1000
2023-09-28 18:08:39.801 
Epoch 124/1000 
	 loss: 30.6656, MinusLogProbMetric: 30.6656, val_loss: 30.7847, val_MinusLogProbMetric: 30.7847

Epoch 124: val_loss did not improve from 30.54235
196/196 - 35s - loss: 30.6656 - MinusLogProbMetric: 30.6656 - val_loss: 30.7847 - val_MinusLogProbMetric: 30.7847 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 125/1000
2023-09-28 18:09:14.759 
Epoch 125/1000 
	 loss: 30.5415, MinusLogProbMetric: 30.5415, val_loss: 30.8302, val_MinusLogProbMetric: 30.8302

Epoch 125: val_loss did not improve from 30.54235
196/196 - 35s - loss: 30.5415 - MinusLogProbMetric: 30.5415 - val_loss: 30.8302 - val_MinusLogProbMetric: 30.8302 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 126/1000
2023-09-28 18:09:49.094 
Epoch 126/1000 
	 loss: 30.5857, MinusLogProbMetric: 30.5857, val_loss: 31.7065, val_MinusLogProbMetric: 31.7065

Epoch 126: val_loss did not improve from 30.54235
196/196 - 34s - loss: 30.5857 - MinusLogProbMetric: 30.5857 - val_loss: 31.7065 - val_MinusLogProbMetric: 31.7065 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 127/1000
2023-09-28 18:10:28.322 
Epoch 127/1000 
	 loss: 30.5226, MinusLogProbMetric: 30.5226, val_loss: 30.7034, val_MinusLogProbMetric: 30.7034

Epoch 127: val_loss did not improve from 30.54235
196/196 - 39s - loss: 30.5226 - MinusLogProbMetric: 30.5226 - val_loss: 30.7034 - val_MinusLogProbMetric: 30.7034 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 128/1000
2023-09-28 18:11:06.560 
Epoch 128/1000 
	 loss: 30.5160, MinusLogProbMetric: 30.5160, val_loss: 31.4172, val_MinusLogProbMetric: 31.4172

Epoch 128: val_loss did not improve from 30.54235
196/196 - 38s - loss: 30.5160 - MinusLogProbMetric: 30.5160 - val_loss: 31.4172 - val_MinusLogProbMetric: 31.4172 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 129/1000
2023-09-28 18:11:45.057 
Epoch 129/1000 
	 loss: 30.4489, MinusLogProbMetric: 30.4489, val_loss: 30.3232, val_MinusLogProbMetric: 30.3232

Epoch 129: val_loss improved from 30.54235 to 30.32317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 39s - loss: 30.4489 - MinusLogProbMetric: 30.4489 - val_loss: 30.3232 - val_MinusLogProbMetric: 30.3232 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 130/1000
2023-09-28 18:12:20.157 
Epoch 130/1000 
	 loss: 30.4742, MinusLogProbMetric: 30.4742, val_loss: 30.3792, val_MinusLogProbMetric: 30.3792

Epoch 130: val_loss did not improve from 30.32317
196/196 - 34s - loss: 30.4742 - MinusLogProbMetric: 30.4742 - val_loss: 30.3792 - val_MinusLogProbMetric: 30.3792 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 131/1000
2023-09-28 18:13:00.268 
Epoch 131/1000 
	 loss: 30.3263, MinusLogProbMetric: 30.3263, val_loss: 31.5757, val_MinusLogProbMetric: 31.5757

Epoch 131: val_loss did not improve from 30.32317
196/196 - 40s - loss: 30.3263 - MinusLogProbMetric: 30.3263 - val_loss: 31.5757 - val_MinusLogProbMetric: 31.5757 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 132/1000
2023-09-28 18:13:39.337 
Epoch 132/1000 
	 loss: 30.4987, MinusLogProbMetric: 30.4987, val_loss: 31.1270, val_MinusLogProbMetric: 31.1270

Epoch 132: val_loss did not improve from 30.32317
196/196 - 39s - loss: 30.4987 - MinusLogProbMetric: 30.4987 - val_loss: 31.1270 - val_MinusLogProbMetric: 31.1270 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 133/1000
2023-09-28 18:14:17.537 
Epoch 133/1000 
	 loss: 30.4495, MinusLogProbMetric: 30.4495, val_loss: 30.8291, val_MinusLogProbMetric: 30.8291

Epoch 133: val_loss did not improve from 30.32317
196/196 - 38s - loss: 30.4495 - MinusLogProbMetric: 30.4495 - val_loss: 30.8291 - val_MinusLogProbMetric: 30.8291 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 134/1000
2023-09-28 18:14:54.918 
Epoch 134/1000 
	 loss: 30.3572, MinusLogProbMetric: 30.3572, val_loss: 34.1320, val_MinusLogProbMetric: 34.1320

Epoch 134: val_loss did not improve from 30.32317
196/196 - 37s - loss: 30.3572 - MinusLogProbMetric: 30.3572 - val_loss: 34.1320 - val_MinusLogProbMetric: 34.1320 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 135/1000
2023-09-28 18:15:36.599 
Epoch 135/1000 
	 loss: 30.4852, MinusLogProbMetric: 30.4852, val_loss: 31.8341, val_MinusLogProbMetric: 31.8341

Epoch 135: val_loss did not improve from 30.32317
196/196 - 42s - loss: 30.4852 - MinusLogProbMetric: 30.4852 - val_loss: 31.8341 - val_MinusLogProbMetric: 31.8341 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 136/1000
2023-09-28 18:16:14.894 
Epoch 136/1000 
	 loss: 30.5071, MinusLogProbMetric: 30.5071, val_loss: 30.8004, val_MinusLogProbMetric: 30.8004

Epoch 136: val_loss did not improve from 30.32317
196/196 - 38s - loss: 30.5071 - MinusLogProbMetric: 30.5071 - val_loss: 30.8004 - val_MinusLogProbMetric: 30.8004 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 137/1000
2023-09-28 18:16:51.854 
Epoch 137/1000 
	 loss: 30.3759, MinusLogProbMetric: 30.3759, val_loss: 30.2333, val_MinusLogProbMetric: 30.2333

Epoch 137: val_loss improved from 30.32317 to 30.23335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 38s - loss: 30.3759 - MinusLogProbMetric: 30.3759 - val_loss: 30.2333 - val_MinusLogProbMetric: 30.2333 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 138/1000
2023-09-28 18:17:31.422 
Epoch 138/1000 
	 loss: 30.4577, MinusLogProbMetric: 30.4577, val_loss: 31.5799, val_MinusLogProbMetric: 31.5799

Epoch 138: val_loss did not improve from 30.23335
196/196 - 39s - loss: 30.4577 - MinusLogProbMetric: 30.4577 - val_loss: 31.5799 - val_MinusLogProbMetric: 31.5799 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 139/1000
2023-09-28 18:18:12.547 
Epoch 139/1000 
	 loss: 30.5140, MinusLogProbMetric: 30.5140, val_loss: 31.9235, val_MinusLogProbMetric: 31.9235

Epoch 139: val_loss did not improve from 30.23335
196/196 - 41s - loss: 30.5140 - MinusLogProbMetric: 30.5140 - val_loss: 31.9235 - val_MinusLogProbMetric: 31.9235 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 140/1000
2023-09-28 18:18:50.643 
Epoch 140/1000 
	 loss: 30.4015, MinusLogProbMetric: 30.4015, val_loss: 30.9516, val_MinusLogProbMetric: 30.9516

Epoch 140: val_loss did not improve from 30.23335
196/196 - 38s - loss: 30.4015 - MinusLogProbMetric: 30.4015 - val_loss: 30.9516 - val_MinusLogProbMetric: 30.9516 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 141/1000
2023-09-28 18:19:27.544 
Epoch 141/1000 
	 loss: 30.3355, MinusLogProbMetric: 30.3355, val_loss: 32.0780, val_MinusLogProbMetric: 32.0780

Epoch 141: val_loss did not improve from 30.23335
196/196 - 37s - loss: 30.3355 - MinusLogProbMetric: 30.3355 - val_loss: 32.0780 - val_MinusLogProbMetric: 32.0780 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 142/1000
2023-09-28 18:20:06.407 
Epoch 142/1000 
	 loss: 30.3587, MinusLogProbMetric: 30.3587, val_loss: 31.2041, val_MinusLogProbMetric: 31.2041

Epoch 142: val_loss did not improve from 30.23335
196/196 - 39s - loss: 30.3587 - MinusLogProbMetric: 30.3587 - val_loss: 31.2041 - val_MinusLogProbMetric: 31.2041 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 143/1000
2023-09-28 18:20:41.217 
Epoch 143/1000 
	 loss: 30.2690, MinusLogProbMetric: 30.2690, val_loss: 30.2176, val_MinusLogProbMetric: 30.2176

Epoch 143: val_loss improved from 30.23335 to 30.21760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 35s - loss: 30.2690 - MinusLogProbMetric: 30.2690 - val_loss: 30.2176 - val_MinusLogProbMetric: 30.2176 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 144/1000
2023-09-28 18:21:17.126 
Epoch 144/1000 
	 loss: 30.1450, MinusLogProbMetric: 30.1450, val_loss: 30.7093, val_MinusLogProbMetric: 30.7093

Epoch 144: val_loss did not improve from 30.21760
196/196 - 35s - loss: 30.1450 - MinusLogProbMetric: 30.1450 - val_loss: 30.7093 - val_MinusLogProbMetric: 30.7093 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 145/1000
2023-09-28 18:21:52.321 
Epoch 145/1000 
	 loss: 30.4383, MinusLogProbMetric: 30.4383, val_loss: 30.7817, val_MinusLogProbMetric: 30.7817

Epoch 145: val_loss did not improve from 30.21760
196/196 - 35s - loss: 30.4383 - MinusLogProbMetric: 30.4383 - val_loss: 30.7817 - val_MinusLogProbMetric: 30.7817 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 146/1000
2023-09-28 18:22:27.972 
Epoch 146/1000 
	 loss: 30.2156, MinusLogProbMetric: 30.2156, val_loss: 31.6932, val_MinusLogProbMetric: 31.6932

Epoch 146: val_loss did not improve from 30.21760
196/196 - 36s - loss: 30.2156 - MinusLogProbMetric: 30.2156 - val_loss: 31.6932 - val_MinusLogProbMetric: 31.6932 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 147/1000
2023-09-28 18:23:02.931 
Epoch 147/1000 
	 loss: 30.3679, MinusLogProbMetric: 30.3679, val_loss: 30.9298, val_MinusLogProbMetric: 30.9298

Epoch 147: val_loss did not improve from 30.21760
196/196 - 35s - loss: 30.3679 - MinusLogProbMetric: 30.3679 - val_loss: 30.9298 - val_MinusLogProbMetric: 30.9298 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 148/1000
2023-09-28 18:23:37.873 
Epoch 148/1000 
	 loss: 30.2064, MinusLogProbMetric: 30.2064, val_loss: 32.4413, val_MinusLogProbMetric: 32.4413

Epoch 148: val_loss did not improve from 30.21760
196/196 - 35s - loss: 30.2064 - MinusLogProbMetric: 30.2064 - val_loss: 32.4413 - val_MinusLogProbMetric: 32.4413 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 149/1000
2023-09-28 18:24:13.393 
Epoch 149/1000 
	 loss: 30.1801, MinusLogProbMetric: 30.1801, val_loss: 30.0220, val_MinusLogProbMetric: 30.0220

Epoch 149: val_loss improved from 30.21760 to 30.02205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 36s - loss: 30.1801 - MinusLogProbMetric: 30.1801 - val_loss: 30.0220 - val_MinusLogProbMetric: 30.0220 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 150/1000
2023-09-28 18:24:49.437 
Epoch 150/1000 
	 loss: 30.1579, MinusLogProbMetric: 30.1579, val_loss: 30.3965, val_MinusLogProbMetric: 30.3965

Epoch 150: val_loss did not improve from 30.02205
196/196 - 35s - loss: 30.1579 - MinusLogProbMetric: 30.1579 - val_loss: 30.3965 - val_MinusLogProbMetric: 30.3965 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 151/1000
2023-09-28 18:25:24.884 
Epoch 151/1000 
	 loss: 30.0777, MinusLogProbMetric: 30.0777, val_loss: 30.6048, val_MinusLogProbMetric: 30.6048

Epoch 151: val_loss did not improve from 30.02205
196/196 - 35s - loss: 30.0777 - MinusLogProbMetric: 30.0777 - val_loss: 30.6048 - val_MinusLogProbMetric: 30.6048 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 152/1000
2023-09-28 18:25:59.851 
Epoch 152/1000 
	 loss: 30.2983, MinusLogProbMetric: 30.2983, val_loss: 30.3517, val_MinusLogProbMetric: 30.3517

Epoch 152: val_loss did not improve from 30.02205
196/196 - 35s - loss: 30.2983 - MinusLogProbMetric: 30.2983 - val_loss: 30.3517 - val_MinusLogProbMetric: 30.3517 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 153/1000
2023-09-28 18:26:36.750 
Epoch 153/1000 
	 loss: 29.9966, MinusLogProbMetric: 29.9966, val_loss: 31.2198, val_MinusLogProbMetric: 31.2198

Epoch 153: val_loss did not improve from 30.02205
196/196 - 37s - loss: 29.9966 - MinusLogProbMetric: 29.9966 - val_loss: 31.2198 - val_MinusLogProbMetric: 31.2198 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 154/1000
2023-09-28 18:27:13.504 
Epoch 154/1000 
	 loss: 30.0394, MinusLogProbMetric: 30.0394, val_loss: 30.6649, val_MinusLogProbMetric: 30.6649

Epoch 154: val_loss did not improve from 30.02205
196/196 - 37s - loss: 30.0394 - MinusLogProbMetric: 30.0394 - val_loss: 30.6649 - val_MinusLogProbMetric: 30.6649 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 155/1000
2023-09-28 18:27:50.559 
Epoch 155/1000 
	 loss: 30.0456, MinusLogProbMetric: 30.0456, val_loss: 31.3534, val_MinusLogProbMetric: 31.3534

Epoch 155: val_loss did not improve from 30.02205
196/196 - 37s - loss: 30.0456 - MinusLogProbMetric: 30.0456 - val_loss: 31.3534 - val_MinusLogProbMetric: 31.3534 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 156/1000
2023-09-28 18:28:27.535 
Epoch 156/1000 
	 loss: 30.0142, MinusLogProbMetric: 30.0142, val_loss: 30.4098, val_MinusLogProbMetric: 30.4098

Epoch 156: val_loss did not improve from 30.02205
196/196 - 37s - loss: 30.0142 - MinusLogProbMetric: 30.0142 - val_loss: 30.4098 - val_MinusLogProbMetric: 30.4098 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 157/1000
2023-09-28 18:29:06.800 
Epoch 157/1000 
	 loss: 29.9281, MinusLogProbMetric: 29.9281, val_loss: 30.8257, val_MinusLogProbMetric: 30.8257

Epoch 157: val_loss did not improve from 30.02205
196/196 - 39s - loss: 29.9281 - MinusLogProbMetric: 29.9281 - val_loss: 30.8257 - val_MinusLogProbMetric: 30.8257 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 158/1000
2023-09-28 18:29:44.049 
Epoch 158/1000 
	 loss: 29.9669, MinusLogProbMetric: 29.9669, val_loss: 29.7256, val_MinusLogProbMetric: 29.7256

Epoch 158: val_loss improved from 30.02205 to 29.72564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 38s - loss: 29.9669 - MinusLogProbMetric: 29.9669 - val_loss: 29.7256 - val_MinusLogProbMetric: 29.7256 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 159/1000
2023-09-28 18:30:20.390 
Epoch 159/1000 
	 loss: 29.9461, MinusLogProbMetric: 29.9461, val_loss: 30.9698, val_MinusLogProbMetric: 30.9698

Epoch 159: val_loss did not improve from 29.72564
196/196 - 36s - loss: 29.9461 - MinusLogProbMetric: 29.9461 - val_loss: 30.9698 - val_MinusLogProbMetric: 30.9698 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 160/1000
2023-09-28 18:30:56.354 
Epoch 160/1000 
	 loss: 30.0626, MinusLogProbMetric: 30.0626, val_loss: 30.6386, val_MinusLogProbMetric: 30.6386

Epoch 160: val_loss did not improve from 29.72564
196/196 - 36s - loss: 30.0626 - MinusLogProbMetric: 30.0626 - val_loss: 30.6386 - val_MinusLogProbMetric: 30.6386 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 161/1000
2023-09-28 18:31:36.353 
Epoch 161/1000 
	 loss: 29.9467, MinusLogProbMetric: 29.9467, val_loss: 30.1687, val_MinusLogProbMetric: 30.1687

Epoch 161: val_loss did not improve from 29.72564
196/196 - 40s - loss: 29.9467 - MinusLogProbMetric: 29.9467 - val_loss: 30.1687 - val_MinusLogProbMetric: 30.1687 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 162/1000
2023-09-28 18:32:13.134 
Epoch 162/1000 
	 loss: 30.1566, MinusLogProbMetric: 30.1566, val_loss: 30.6112, val_MinusLogProbMetric: 30.6112

Epoch 162: val_loss did not improve from 29.72564
196/196 - 37s - loss: 30.1566 - MinusLogProbMetric: 30.1566 - val_loss: 30.6112 - val_MinusLogProbMetric: 30.6112 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 163/1000
2023-09-28 18:32:48.772 
Epoch 163/1000 
	 loss: 29.9536, MinusLogProbMetric: 29.9536, val_loss: 30.6839, val_MinusLogProbMetric: 30.6839

Epoch 163: val_loss did not improve from 29.72564
196/196 - 36s - loss: 29.9536 - MinusLogProbMetric: 29.9536 - val_loss: 30.6839 - val_MinusLogProbMetric: 30.6839 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 164/1000
2023-09-28 18:33:27.109 
Epoch 164/1000 
	 loss: 29.9553, MinusLogProbMetric: 29.9553, val_loss: 30.9726, val_MinusLogProbMetric: 30.9726

Epoch 164: val_loss did not improve from 29.72564
196/196 - 38s - loss: 29.9553 - MinusLogProbMetric: 29.9553 - val_loss: 30.9726 - val_MinusLogProbMetric: 30.9726 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 165/1000
2023-09-28 18:34:09.102 
Epoch 165/1000 
	 loss: 30.0857, MinusLogProbMetric: 30.0857, val_loss: 31.1693, val_MinusLogProbMetric: 31.1693

Epoch 165: val_loss did not improve from 29.72564
196/196 - 42s - loss: 30.0857 - MinusLogProbMetric: 30.0857 - val_loss: 31.1693 - val_MinusLogProbMetric: 31.1693 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 166/1000
2023-09-28 18:34:50.186 
Epoch 166/1000 
	 loss: 29.9437, MinusLogProbMetric: 29.9437, val_loss: 29.8967, val_MinusLogProbMetric: 29.8967

Epoch 166: val_loss did not improve from 29.72564
196/196 - 41s - loss: 29.9437 - MinusLogProbMetric: 29.9437 - val_loss: 29.8967 - val_MinusLogProbMetric: 29.8967 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 167/1000
2023-09-28 18:35:32.288 
Epoch 167/1000 
	 loss: 30.0025, MinusLogProbMetric: 30.0025, val_loss: 30.2783, val_MinusLogProbMetric: 30.2783

Epoch 167: val_loss did not improve from 29.72564
196/196 - 42s - loss: 30.0025 - MinusLogProbMetric: 30.0025 - val_loss: 30.2783 - val_MinusLogProbMetric: 30.2783 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 168/1000
2023-09-28 18:36:14.313 
Epoch 168/1000 
	 loss: 29.9569, MinusLogProbMetric: 29.9569, val_loss: 33.2368, val_MinusLogProbMetric: 33.2368

Epoch 168: val_loss did not improve from 29.72564
196/196 - 42s - loss: 29.9569 - MinusLogProbMetric: 29.9569 - val_loss: 33.2368 - val_MinusLogProbMetric: 33.2368 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 169/1000
2023-09-28 18:36:52.795 
Epoch 169/1000 
	 loss: 30.0928, MinusLogProbMetric: 30.0928, val_loss: 30.3777, val_MinusLogProbMetric: 30.3777

Epoch 169: val_loss did not improve from 29.72564
196/196 - 38s - loss: 30.0928 - MinusLogProbMetric: 30.0928 - val_loss: 30.3777 - val_MinusLogProbMetric: 30.3777 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 170/1000
2023-09-28 18:37:34.909 
Epoch 170/1000 
	 loss: 29.9958, MinusLogProbMetric: 29.9958, val_loss: 30.8279, val_MinusLogProbMetric: 30.8279

Epoch 170: val_loss did not improve from 29.72564
196/196 - 42s - loss: 29.9958 - MinusLogProbMetric: 29.9958 - val_loss: 30.8279 - val_MinusLogProbMetric: 30.8279 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 171/1000
2023-09-28 18:38:17.254 
Epoch 171/1000 
	 loss: 29.9333, MinusLogProbMetric: 29.9333, val_loss: 30.5508, val_MinusLogProbMetric: 30.5508

Epoch 171: val_loss did not improve from 29.72564
196/196 - 42s - loss: 29.9333 - MinusLogProbMetric: 29.9333 - val_loss: 30.5508 - val_MinusLogProbMetric: 30.5508 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 172/1000
2023-09-28 18:39:00.230 
Epoch 172/1000 
	 loss: 29.9029, MinusLogProbMetric: 29.9029, val_loss: 31.3361, val_MinusLogProbMetric: 31.3361

Epoch 172: val_loss did not improve from 29.72564
196/196 - 43s - loss: 29.9029 - MinusLogProbMetric: 29.9029 - val_loss: 31.3361 - val_MinusLogProbMetric: 31.3361 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 173/1000
2023-09-28 18:39:39.864 
Epoch 173/1000 
	 loss: 29.9340, MinusLogProbMetric: 29.9340, val_loss: 31.1290, val_MinusLogProbMetric: 31.1290

Epoch 173: val_loss did not improve from 29.72564
196/196 - 40s - loss: 29.9340 - MinusLogProbMetric: 29.9340 - val_loss: 31.1290 - val_MinusLogProbMetric: 31.1290 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 174/1000
2023-09-28 18:40:21.663 
Epoch 174/1000 
	 loss: 29.8703, MinusLogProbMetric: 29.8703, val_loss: 30.0815, val_MinusLogProbMetric: 30.0815

Epoch 174: val_loss did not improve from 29.72564
196/196 - 42s - loss: 29.8703 - MinusLogProbMetric: 29.8703 - val_loss: 30.0815 - val_MinusLogProbMetric: 30.0815 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 175/1000
2023-09-28 18:41:04.951 
Epoch 175/1000 
	 loss: 29.6891, MinusLogProbMetric: 29.6891, val_loss: 29.9567, val_MinusLogProbMetric: 29.9567

Epoch 175: val_loss did not improve from 29.72564
196/196 - 43s - loss: 29.6891 - MinusLogProbMetric: 29.6891 - val_loss: 29.9567 - val_MinusLogProbMetric: 29.9567 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 176/1000
2023-09-28 18:41:46.573 
Epoch 176/1000 
	 loss: 29.9849, MinusLogProbMetric: 29.9849, val_loss: 30.4164, val_MinusLogProbMetric: 30.4164

Epoch 176: val_loss did not improve from 29.72564
196/196 - 42s - loss: 29.9849 - MinusLogProbMetric: 29.9849 - val_loss: 30.4164 - val_MinusLogProbMetric: 30.4164 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 177/1000
2023-09-28 18:42:28.293 
Epoch 177/1000 
	 loss: 29.8480, MinusLogProbMetric: 29.8480, val_loss: 31.5574, val_MinusLogProbMetric: 31.5574

Epoch 177: val_loss did not improve from 29.72564
196/196 - 42s - loss: 29.8480 - MinusLogProbMetric: 29.8480 - val_loss: 31.5574 - val_MinusLogProbMetric: 31.5574 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 178/1000
2023-09-28 18:43:10.782 
Epoch 178/1000 
	 loss: 29.8584, MinusLogProbMetric: 29.8584, val_loss: 30.5969, val_MinusLogProbMetric: 30.5969

Epoch 178: val_loss did not improve from 29.72564
196/196 - 42s - loss: 29.8584 - MinusLogProbMetric: 29.8584 - val_loss: 30.5969 - val_MinusLogProbMetric: 30.5969 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 179/1000
2023-09-28 18:43:52.413 
Epoch 179/1000 
	 loss: 29.7425, MinusLogProbMetric: 29.7425, val_loss: 29.6952, val_MinusLogProbMetric: 29.6952

Epoch 179: val_loss improved from 29.72564 to 29.69519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 29.7425 - MinusLogProbMetric: 29.7425 - val_loss: 29.6952 - val_MinusLogProbMetric: 29.6952 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 180/1000
2023-09-28 18:44:35.693 
Epoch 180/1000 
	 loss: 29.8604, MinusLogProbMetric: 29.8604, val_loss: 31.0149, val_MinusLogProbMetric: 31.0149

Epoch 180: val_loss did not improve from 29.69519
196/196 - 43s - loss: 29.8604 - MinusLogProbMetric: 29.8604 - val_loss: 31.0149 - val_MinusLogProbMetric: 31.0149 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 181/1000
2023-09-28 18:45:18.981 
Epoch 181/1000 
	 loss: 29.8298, MinusLogProbMetric: 29.8298, val_loss: 29.6107, val_MinusLogProbMetric: 29.6107

Epoch 181: val_loss improved from 29.69519 to 29.61068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 29.8298 - MinusLogProbMetric: 29.8298 - val_loss: 29.6107 - val_MinusLogProbMetric: 29.6107 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 182/1000
2023-09-28 18:46:02.477 
Epoch 182/1000 
	 loss: 29.8788, MinusLogProbMetric: 29.8788, val_loss: 29.7118, val_MinusLogProbMetric: 29.7118

Epoch 182: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.8788 - MinusLogProbMetric: 29.8788 - val_loss: 29.7118 - val_MinusLogProbMetric: 29.7118 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 183/1000
2023-09-28 18:46:45.272 
Epoch 183/1000 
	 loss: 29.8159, MinusLogProbMetric: 29.8159, val_loss: 29.7610, val_MinusLogProbMetric: 29.7610

Epoch 183: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.8159 - MinusLogProbMetric: 29.8159 - val_loss: 29.7610 - val_MinusLogProbMetric: 29.7610 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 184/1000
2023-09-28 18:47:28.735 
Epoch 184/1000 
	 loss: 29.7791, MinusLogProbMetric: 29.7791, val_loss: 29.9040, val_MinusLogProbMetric: 29.9040

Epoch 184: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.7791 - MinusLogProbMetric: 29.7791 - val_loss: 29.9040 - val_MinusLogProbMetric: 29.9040 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 185/1000
2023-09-28 18:48:11.767 
Epoch 185/1000 
	 loss: 29.6842, MinusLogProbMetric: 29.6842, val_loss: 30.0523, val_MinusLogProbMetric: 30.0523

Epoch 185: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.6842 - MinusLogProbMetric: 29.6842 - val_loss: 30.0523 - val_MinusLogProbMetric: 30.0523 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 186/1000
2023-09-28 18:48:54.588 
Epoch 186/1000 
	 loss: 29.7571, MinusLogProbMetric: 29.7571, val_loss: 29.9432, val_MinusLogProbMetric: 29.9432

Epoch 186: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.7571 - MinusLogProbMetric: 29.7571 - val_loss: 29.9432 - val_MinusLogProbMetric: 29.9432 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 187/1000
2023-09-28 18:49:37.315 
Epoch 187/1000 
	 loss: 29.7910, MinusLogProbMetric: 29.7910, val_loss: 30.0087, val_MinusLogProbMetric: 30.0087

Epoch 187: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.7910 - MinusLogProbMetric: 29.7910 - val_loss: 30.0087 - val_MinusLogProbMetric: 30.0087 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 188/1000
2023-09-28 18:50:20.217 
Epoch 188/1000 
	 loss: 29.7641, MinusLogProbMetric: 29.7641, val_loss: 30.1393, val_MinusLogProbMetric: 30.1393

Epoch 188: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.7641 - MinusLogProbMetric: 29.7641 - val_loss: 30.1393 - val_MinusLogProbMetric: 30.1393 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 189/1000
2023-09-28 18:51:03.339 
Epoch 189/1000 
	 loss: 29.6604, MinusLogProbMetric: 29.6604, val_loss: 30.9198, val_MinusLogProbMetric: 30.9198

Epoch 189: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.6604 - MinusLogProbMetric: 29.6604 - val_loss: 30.9198 - val_MinusLogProbMetric: 30.9198 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 190/1000
2023-09-28 18:51:46.278 
Epoch 190/1000 
	 loss: 29.7805, MinusLogProbMetric: 29.7805, val_loss: 30.1627, val_MinusLogProbMetric: 30.1627

Epoch 190: val_loss did not improve from 29.61068
196/196 - 43s - loss: 29.7805 - MinusLogProbMetric: 29.7805 - val_loss: 30.1627 - val_MinusLogProbMetric: 30.1627 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 191/1000
2023-09-28 18:52:27.586 
Epoch 191/1000 
	 loss: 29.6514, MinusLogProbMetric: 29.6514, val_loss: 30.5662, val_MinusLogProbMetric: 30.5662

Epoch 191: val_loss did not improve from 29.61068
196/196 - 41s - loss: 29.6514 - MinusLogProbMetric: 29.6514 - val_loss: 30.5662 - val_MinusLogProbMetric: 30.5662 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 192/1000
2023-09-28 18:53:07.434 
Epoch 192/1000 
	 loss: 29.7820, MinusLogProbMetric: 29.7820, val_loss: 30.1045, val_MinusLogProbMetric: 30.1045

Epoch 192: val_loss did not improve from 29.61068
196/196 - 40s - loss: 29.7820 - MinusLogProbMetric: 29.7820 - val_loss: 30.1045 - val_MinusLogProbMetric: 30.1045 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 193/1000
2023-09-28 18:53:46.888 
Epoch 193/1000 
	 loss: 29.6296, MinusLogProbMetric: 29.6296, val_loss: 30.7683, val_MinusLogProbMetric: 30.7683

Epoch 193: val_loss did not improve from 29.61068
196/196 - 39s - loss: 29.6296 - MinusLogProbMetric: 29.6296 - val_loss: 30.7683 - val_MinusLogProbMetric: 30.7683 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 194/1000
2023-09-28 18:54:26.585 
Epoch 194/1000 
	 loss: 29.6982, MinusLogProbMetric: 29.6982, val_loss: 29.5974, val_MinusLogProbMetric: 29.5974

Epoch 194: val_loss improved from 29.61068 to 29.59743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 40s - loss: 29.6982 - MinusLogProbMetric: 29.6982 - val_loss: 29.5974 - val_MinusLogProbMetric: 29.5974 - lr: 0.0010 - 40s/epoch - 207ms/step
Epoch 195/1000
2023-09-28 18:55:06.387 
Epoch 195/1000 
	 loss: 29.5378, MinusLogProbMetric: 29.5378, val_loss: 30.0828, val_MinusLogProbMetric: 30.0828

Epoch 195: val_loss did not improve from 29.59743
196/196 - 39s - loss: 29.5378 - MinusLogProbMetric: 29.5378 - val_loss: 30.0828 - val_MinusLogProbMetric: 30.0828 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 196/1000
2023-09-28 18:55:46.380 
Epoch 196/1000 
	 loss: 29.6351, MinusLogProbMetric: 29.6351, val_loss: 30.9004, val_MinusLogProbMetric: 30.9004

Epoch 196: val_loss did not improve from 29.59743
196/196 - 40s - loss: 29.6351 - MinusLogProbMetric: 29.6351 - val_loss: 30.9004 - val_MinusLogProbMetric: 30.9004 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 197/1000
2023-09-28 18:56:25.619 
Epoch 197/1000 
	 loss: 29.7358, MinusLogProbMetric: 29.7358, val_loss: 29.8638, val_MinusLogProbMetric: 29.8638

Epoch 197: val_loss did not improve from 29.59743
196/196 - 39s - loss: 29.7358 - MinusLogProbMetric: 29.7358 - val_loss: 29.8638 - val_MinusLogProbMetric: 29.8638 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 198/1000
2023-09-28 18:57:06.988 
Epoch 198/1000 
	 loss: 29.6268, MinusLogProbMetric: 29.6268, val_loss: 29.6182, val_MinusLogProbMetric: 29.6182

Epoch 198: val_loss did not improve from 29.59743
196/196 - 41s - loss: 29.6268 - MinusLogProbMetric: 29.6268 - val_loss: 29.6182 - val_MinusLogProbMetric: 29.6182 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 199/1000
2023-09-28 18:57:48.901 
Epoch 199/1000 
	 loss: 29.5594, MinusLogProbMetric: 29.5594, val_loss: 30.0983, val_MinusLogProbMetric: 30.0983

Epoch 199: val_loss did not improve from 29.59743
196/196 - 42s - loss: 29.5594 - MinusLogProbMetric: 29.5594 - val_loss: 30.0983 - val_MinusLogProbMetric: 30.0983 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 200/1000
2023-09-28 18:58:30.383 
Epoch 200/1000 
	 loss: 29.4378, MinusLogProbMetric: 29.4378, val_loss: 30.1686, val_MinusLogProbMetric: 30.1686

Epoch 200: val_loss did not improve from 29.59743
196/196 - 41s - loss: 29.4378 - MinusLogProbMetric: 29.4378 - val_loss: 30.1686 - val_MinusLogProbMetric: 30.1686 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 201/1000
2023-09-28 18:59:12.762 
Epoch 201/1000 
	 loss: 29.6741, MinusLogProbMetric: 29.6741, val_loss: 30.7260, val_MinusLogProbMetric: 30.7260

Epoch 201: val_loss did not improve from 29.59743
196/196 - 42s - loss: 29.6741 - MinusLogProbMetric: 29.6741 - val_loss: 30.7260 - val_MinusLogProbMetric: 30.7260 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 202/1000
2023-09-28 18:59:53.656 
Epoch 202/1000 
	 loss: 29.6464, MinusLogProbMetric: 29.6464, val_loss: 30.4980, val_MinusLogProbMetric: 30.4980

Epoch 202: val_loss did not improve from 29.59743
196/196 - 41s - loss: 29.6464 - MinusLogProbMetric: 29.6464 - val_loss: 30.4980 - val_MinusLogProbMetric: 30.4980 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 203/1000
2023-09-28 19:00:36.538 
Epoch 203/1000 
	 loss: 29.5983, MinusLogProbMetric: 29.5983, val_loss: 30.0358, val_MinusLogProbMetric: 30.0358

Epoch 203: val_loss did not improve from 29.59743
196/196 - 43s - loss: 29.5983 - MinusLogProbMetric: 29.5983 - val_loss: 30.0358 - val_MinusLogProbMetric: 30.0358 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 204/1000
2023-09-28 19:01:18.778 
Epoch 204/1000 
	 loss: 29.6147, MinusLogProbMetric: 29.6147, val_loss: 29.4423, val_MinusLogProbMetric: 29.4423

Epoch 204: val_loss improved from 29.59743 to 29.44233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 29.6147 - MinusLogProbMetric: 29.6147 - val_loss: 29.4423 - val_MinusLogProbMetric: 29.4423 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 205/1000
2023-09-28 19:02:00.840 
Epoch 205/1000 
	 loss: 29.6217, MinusLogProbMetric: 29.6217, val_loss: 29.9737, val_MinusLogProbMetric: 29.9737

Epoch 205: val_loss did not improve from 29.44233
196/196 - 42s - loss: 29.6217 - MinusLogProbMetric: 29.6217 - val_loss: 29.9737 - val_MinusLogProbMetric: 29.9737 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 206/1000
2023-09-28 19:02:43.367 
Epoch 206/1000 
	 loss: 29.5722, MinusLogProbMetric: 29.5722, val_loss: 30.0702, val_MinusLogProbMetric: 30.0702

Epoch 206: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.5722 - MinusLogProbMetric: 29.5722 - val_loss: 30.0702 - val_MinusLogProbMetric: 30.0702 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 207/1000
2023-09-28 19:03:26.430 
Epoch 207/1000 
	 loss: 29.5836, MinusLogProbMetric: 29.5836, val_loss: 29.9138, val_MinusLogProbMetric: 29.9138

Epoch 207: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.5836 - MinusLogProbMetric: 29.5836 - val_loss: 29.9138 - val_MinusLogProbMetric: 29.9138 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 208/1000
2023-09-28 19:04:09.655 
Epoch 208/1000 
	 loss: 29.7597, MinusLogProbMetric: 29.7597, val_loss: 30.4794, val_MinusLogProbMetric: 30.4794

Epoch 208: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.7597 - MinusLogProbMetric: 29.7597 - val_loss: 30.4794 - val_MinusLogProbMetric: 30.4794 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 209/1000
2023-09-28 19:04:52.387 
Epoch 209/1000 
	 loss: 29.4521, MinusLogProbMetric: 29.4521, val_loss: 29.9516, val_MinusLogProbMetric: 29.9516

Epoch 209: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4521 - MinusLogProbMetric: 29.4521 - val_loss: 29.9516 - val_MinusLogProbMetric: 29.9516 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 210/1000
2023-09-28 19:05:35.239 
Epoch 210/1000 
	 loss: 29.5452, MinusLogProbMetric: 29.5452, val_loss: 31.2065, val_MinusLogProbMetric: 31.2065

Epoch 210: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.5452 - MinusLogProbMetric: 29.5452 - val_loss: 31.2065 - val_MinusLogProbMetric: 31.2065 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 211/1000
2023-09-28 19:06:17.534 
Epoch 211/1000 
	 loss: 29.4741, MinusLogProbMetric: 29.4741, val_loss: 29.7068, val_MinusLogProbMetric: 29.7068

Epoch 211: val_loss did not improve from 29.44233
196/196 - 42s - loss: 29.4741 - MinusLogProbMetric: 29.4741 - val_loss: 29.7068 - val_MinusLogProbMetric: 29.7068 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 212/1000
2023-09-28 19:07:00.728 
Epoch 212/1000 
	 loss: 29.6753, MinusLogProbMetric: 29.6753, val_loss: 29.8506, val_MinusLogProbMetric: 29.8506

Epoch 212: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.6753 - MinusLogProbMetric: 29.6753 - val_loss: 29.8506 - val_MinusLogProbMetric: 29.8506 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 213/1000
2023-09-28 19:07:43.423 
Epoch 213/1000 
	 loss: 29.4971, MinusLogProbMetric: 29.4971, val_loss: 29.8475, val_MinusLogProbMetric: 29.8475

Epoch 213: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4971 - MinusLogProbMetric: 29.4971 - val_loss: 29.8475 - val_MinusLogProbMetric: 29.8475 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 214/1000
2023-09-28 19:08:26.424 
Epoch 214/1000 
	 loss: 29.5553, MinusLogProbMetric: 29.5553, val_loss: 31.2963, val_MinusLogProbMetric: 31.2963

Epoch 214: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.5553 - MinusLogProbMetric: 29.5553 - val_loss: 31.2963 - val_MinusLogProbMetric: 31.2963 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 215/1000
2023-09-28 19:09:09.048 
Epoch 215/1000 
	 loss: 29.5216, MinusLogProbMetric: 29.5216, val_loss: 29.9244, val_MinusLogProbMetric: 29.9244

Epoch 215: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.5216 - MinusLogProbMetric: 29.5216 - val_loss: 29.9244 - val_MinusLogProbMetric: 29.9244 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 216/1000
2023-09-28 19:09:51.556 
Epoch 216/1000 
	 loss: 29.3888, MinusLogProbMetric: 29.3888, val_loss: 30.2436, val_MinusLogProbMetric: 30.2436

Epoch 216: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.3888 - MinusLogProbMetric: 29.3888 - val_loss: 30.2436 - val_MinusLogProbMetric: 30.2436 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 217/1000
2023-09-28 19:10:33.891 
Epoch 217/1000 
	 loss: 29.5314, MinusLogProbMetric: 29.5314, val_loss: 29.8862, val_MinusLogProbMetric: 29.8862

Epoch 217: val_loss did not improve from 29.44233
196/196 - 42s - loss: 29.5314 - MinusLogProbMetric: 29.5314 - val_loss: 29.8862 - val_MinusLogProbMetric: 29.8862 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 218/1000
2023-09-28 19:11:16.791 
Epoch 218/1000 
	 loss: 29.4250, MinusLogProbMetric: 29.4250, val_loss: 29.6894, val_MinusLogProbMetric: 29.6894

Epoch 218: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4250 - MinusLogProbMetric: 29.4250 - val_loss: 29.6894 - val_MinusLogProbMetric: 29.6894 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 219/1000
2023-09-28 19:11:59.458 
Epoch 219/1000 
	 loss: 29.4839, MinusLogProbMetric: 29.4839, val_loss: 30.0638, val_MinusLogProbMetric: 30.0638

Epoch 219: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4839 - MinusLogProbMetric: 29.4839 - val_loss: 30.0638 - val_MinusLogProbMetric: 30.0638 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 220/1000
2023-09-28 19:12:42.430 
Epoch 220/1000 
	 loss: 29.3896, MinusLogProbMetric: 29.3896, val_loss: 29.8708, val_MinusLogProbMetric: 29.8708

Epoch 220: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.3896 - MinusLogProbMetric: 29.3896 - val_loss: 29.8708 - val_MinusLogProbMetric: 29.8708 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 221/1000
2023-09-28 19:13:25.292 
Epoch 221/1000 
	 loss: 29.4562, MinusLogProbMetric: 29.4562, val_loss: 29.7760, val_MinusLogProbMetric: 29.7760

Epoch 221: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4562 - MinusLogProbMetric: 29.4562 - val_loss: 29.7760 - val_MinusLogProbMetric: 29.7760 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 222/1000
2023-09-28 19:14:08.552 
Epoch 222/1000 
	 loss: 29.3324, MinusLogProbMetric: 29.3324, val_loss: 30.0972, val_MinusLogProbMetric: 30.0972

Epoch 222: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.3324 - MinusLogProbMetric: 29.3324 - val_loss: 30.0972 - val_MinusLogProbMetric: 30.0972 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 223/1000
2023-09-28 19:14:51.321 
Epoch 223/1000 
	 loss: 29.7078, MinusLogProbMetric: 29.7078, val_loss: 29.7178, val_MinusLogProbMetric: 29.7178

Epoch 223: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.7078 - MinusLogProbMetric: 29.7078 - val_loss: 29.7178 - val_MinusLogProbMetric: 29.7178 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 224/1000
2023-09-28 19:15:34.451 
Epoch 224/1000 
	 loss: 29.3820, MinusLogProbMetric: 29.3820, val_loss: 30.6019, val_MinusLogProbMetric: 30.6019

Epoch 224: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.3820 - MinusLogProbMetric: 29.3820 - val_loss: 30.6019 - val_MinusLogProbMetric: 30.6019 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 225/1000
2023-09-28 19:16:17.177 
Epoch 225/1000 
	 loss: 29.5127, MinusLogProbMetric: 29.5127, val_loss: 30.0210, val_MinusLogProbMetric: 30.0210

Epoch 225: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.5127 - MinusLogProbMetric: 29.5127 - val_loss: 30.0210 - val_MinusLogProbMetric: 30.0210 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 226/1000
2023-09-28 19:17:00.207 
Epoch 226/1000 
	 loss: 29.3474, MinusLogProbMetric: 29.3474, val_loss: 29.7851, val_MinusLogProbMetric: 29.7851

Epoch 226: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.3474 - MinusLogProbMetric: 29.3474 - val_loss: 29.7851 - val_MinusLogProbMetric: 29.7851 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 227/1000
2023-09-28 19:17:41.542 
Epoch 227/1000 
	 loss: 29.6611, MinusLogProbMetric: 29.6611, val_loss: 29.8551, val_MinusLogProbMetric: 29.8551

Epoch 227: val_loss did not improve from 29.44233
196/196 - 41s - loss: 29.6611 - MinusLogProbMetric: 29.6611 - val_loss: 29.8551 - val_MinusLogProbMetric: 29.8551 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 228/1000
2023-09-28 19:18:24.333 
Epoch 228/1000 
	 loss: 29.4373, MinusLogProbMetric: 29.4373, val_loss: 30.4315, val_MinusLogProbMetric: 30.4315

Epoch 228: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4373 - MinusLogProbMetric: 29.4373 - val_loss: 30.4315 - val_MinusLogProbMetric: 30.4315 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 229/1000
2023-09-28 19:19:05.804 
Epoch 229/1000 
	 loss: 29.5490, MinusLogProbMetric: 29.5490, val_loss: 29.6095, val_MinusLogProbMetric: 29.6095

Epoch 229: val_loss did not improve from 29.44233
196/196 - 41s - loss: 29.5490 - MinusLogProbMetric: 29.5490 - val_loss: 29.6095 - val_MinusLogProbMetric: 29.6095 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 230/1000
2023-09-28 19:19:48.606 
Epoch 230/1000 
	 loss: 29.4632, MinusLogProbMetric: 29.4632, val_loss: 30.1231, val_MinusLogProbMetric: 30.1231

Epoch 230: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4632 - MinusLogProbMetric: 29.4632 - val_loss: 30.1231 - val_MinusLogProbMetric: 30.1231 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 231/1000
2023-09-28 19:20:31.632 
Epoch 231/1000 
	 loss: 29.4096, MinusLogProbMetric: 29.4096, val_loss: 30.2147, val_MinusLogProbMetric: 30.2147

Epoch 231: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4096 - MinusLogProbMetric: 29.4096 - val_loss: 30.2147 - val_MinusLogProbMetric: 30.2147 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 232/1000
2023-09-28 19:21:15.049 
Epoch 232/1000 
	 loss: 29.4045, MinusLogProbMetric: 29.4045, val_loss: 30.6425, val_MinusLogProbMetric: 30.6425

Epoch 232: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4045 - MinusLogProbMetric: 29.4045 - val_loss: 30.6425 - val_MinusLogProbMetric: 30.6425 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 233/1000
2023-09-28 19:21:57.761 
Epoch 233/1000 
	 loss: 29.4881, MinusLogProbMetric: 29.4881, val_loss: 30.5578, val_MinusLogProbMetric: 30.5578

Epoch 233: val_loss did not improve from 29.44233
196/196 - 43s - loss: 29.4881 - MinusLogProbMetric: 29.4881 - val_loss: 30.5578 - val_MinusLogProbMetric: 30.5578 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 234/1000
2023-09-28 19:22:40.708 
Epoch 234/1000 
	 loss: 29.3632, MinusLogProbMetric: 29.3632, val_loss: 29.3675, val_MinusLogProbMetric: 29.3675

Epoch 234: val_loss improved from 29.44233 to 29.36753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 29.3632 - MinusLogProbMetric: 29.3632 - val_loss: 29.3675 - val_MinusLogProbMetric: 29.3675 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 235/1000
2023-09-28 19:23:22.614 
Epoch 235/1000 
	 loss: 29.4355, MinusLogProbMetric: 29.4355, val_loss: 29.9382, val_MinusLogProbMetric: 29.9382

Epoch 235: val_loss did not improve from 29.36753
196/196 - 41s - loss: 29.4355 - MinusLogProbMetric: 29.4355 - val_loss: 29.9382 - val_MinusLogProbMetric: 29.9382 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 236/1000
2023-09-28 19:24:05.064 
Epoch 236/1000 
	 loss: 29.3941, MinusLogProbMetric: 29.3941, val_loss: 30.4577, val_MinusLogProbMetric: 30.4577

Epoch 236: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.3941 - MinusLogProbMetric: 29.3941 - val_loss: 30.4577 - val_MinusLogProbMetric: 30.4577 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 237/1000
2023-09-28 19:24:47.822 
Epoch 237/1000 
	 loss: 29.4961, MinusLogProbMetric: 29.4961, val_loss: 30.0122, val_MinusLogProbMetric: 30.0122

Epoch 237: val_loss did not improve from 29.36753
196/196 - 43s - loss: 29.4961 - MinusLogProbMetric: 29.4961 - val_loss: 30.0122 - val_MinusLogProbMetric: 30.0122 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 238/1000
2023-09-28 19:25:29.719 
Epoch 238/1000 
	 loss: 29.4019, MinusLogProbMetric: 29.4019, val_loss: 29.6909, val_MinusLogProbMetric: 29.6909

Epoch 238: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.4019 - MinusLogProbMetric: 29.4019 - val_loss: 29.6909 - val_MinusLogProbMetric: 29.6909 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 239/1000
2023-09-28 19:26:12.135 
Epoch 239/1000 
	 loss: 29.4723, MinusLogProbMetric: 29.4723, val_loss: 29.4693, val_MinusLogProbMetric: 29.4693

Epoch 239: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.4723 - MinusLogProbMetric: 29.4723 - val_loss: 29.4693 - val_MinusLogProbMetric: 29.4693 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 240/1000
2023-09-28 19:26:54.179 
Epoch 240/1000 
	 loss: 29.4196, MinusLogProbMetric: 29.4196, val_loss: 30.0643, val_MinusLogProbMetric: 30.0643

Epoch 240: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.4196 - MinusLogProbMetric: 29.4196 - val_loss: 30.0643 - val_MinusLogProbMetric: 30.0643 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 241/1000
2023-09-28 19:27:37.027 
Epoch 241/1000 
	 loss: 29.4534, MinusLogProbMetric: 29.4534, val_loss: 30.0452, val_MinusLogProbMetric: 30.0452

Epoch 241: val_loss did not improve from 29.36753
196/196 - 43s - loss: 29.4534 - MinusLogProbMetric: 29.4534 - val_loss: 30.0452 - val_MinusLogProbMetric: 30.0452 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 242/1000
2023-09-28 19:28:19.828 
Epoch 242/1000 
	 loss: 29.4405, MinusLogProbMetric: 29.4405, val_loss: 29.6956, val_MinusLogProbMetric: 29.6956

Epoch 242: val_loss did not improve from 29.36753
196/196 - 43s - loss: 29.4405 - MinusLogProbMetric: 29.4405 - val_loss: 29.6956 - val_MinusLogProbMetric: 29.6956 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 243/1000
2023-09-28 19:29:01.026 
Epoch 243/1000 
	 loss: 29.3213, MinusLogProbMetric: 29.3213, val_loss: 29.7268, val_MinusLogProbMetric: 29.7268

Epoch 243: val_loss did not improve from 29.36753
196/196 - 41s - loss: 29.3213 - MinusLogProbMetric: 29.3213 - val_loss: 29.7268 - val_MinusLogProbMetric: 29.7268 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 244/1000
2023-09-28 19:29:43.524 
Epoch 244/1000 
	 loss: 29.2183, MinusLogProbMetric: 29.2183, val_loss: 29.9226, val_MinusLogProbMetric: 29.9226

Epoch 244: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.2183 - MinusLogProbMetric: 29.2183 - val_loss: 29.9226 - val_MinusLogProbMetric: 29.9226 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 245/1000
2023-09-28 19:30:26.652 
Epoch 245/1000 
	 loss: 29.4742, MinusLogProbMetric: 29.4742, val_loss: 30.0916, val_MinusLogProbMetric: 30.0916

Epoch 245: val_loss did not improve from 29.36753
196/196 - 43s - loss: 29.4742 - MinusLogProbMetric: 29.4742 - val_loss: 30.0916 - val_MinusLogProbMetric: 30.0916 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 246/1000
2023-09-28 19:31:07.709 
Epoch 246/1000 
	 loss: 29.3931, MinusLogProbMetric: 29.3931, val_loss: 29.5438, val_MinusLogProbMetric: 29.5438

Epoch 246: val_loss did not improve from 29.36753
196/196 - 41s - loss: 29.3931 - MinusLogProbMetric: 29.3931 - val_loss: 29.5438 - val_MinusLogProbMetric: 29.5438 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 247/1000
2023-09-28 19:31:47.620 
Epoch 247/1000 
	 loss: 29.3616, MinusLogProbMetric: 29.3616, val_loss: 30.7723, val_MinusLogProbMetric: 30.7723

Epoch 247: val_loss did not improve from 29.36753
196/196 - 40s - loss: 29.3616 - MinusLogProbMetric: 29.3616 - val_loss: 30.7723 - val_MinusLogProbMetric: 30.7723 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 248/1000
2023-09-28 19:32:30.219 
Epoch 248/1000 
	 loss: 29.3981, MinusLogProbMetric: 29.3981, val_loss: 30.7397, val_MinusLogProbMetric: 30.7397

Epoch 248: val_loss did not improve from 29.36753
196/196 - 43s - loss: 29.3981 - MinusLogProbMetric: 29.3981 - val_loss: 30.7397 - val_MinusLogProbMetric: 30.7397 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 249/1000
2023-09-28 19:33:12.320 
Epoch 249/1000 
	 loss: 29.2824, MinusLogProbMetric: 29.2824, val_loss: 29.7738, val_MinusLogProbMetric: 29.7738

Epoch 249: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.2824 - MinusLogProbMetric: 29.2824 - val_loss: 29.7738 - val_MinusLogProbMetric: 29.7738 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 250/1000
2023-09-28 19:33:54.500 
Epoch 250/1000 
	 loss: 29.4137, MinusLogProbMetric: 29.4137, val_loss: 30.0365, val_MinusLogProbMetric: 30.0365

Epoch 250: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.4137 - MinusLogProbMetric: 29.4137 - val_loss: 30.0365 - val_MinusLogProbMetric: 30.0365 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 251/1000
2023-09-28 19:34:31.588 
Epoch 251/1000 
	 loss: 29.3384, MinusLogProbMetric: 29.3384, val_loss: 30.1905, val_MinusLogProbMetric: 30.1905

Epoch 251: val_loss did not improve from 29.36753
196/196 - 37s - loss: 29.3384 - MinusLogProbMetric: 29.3384 - val_loss: 30.1905 - val_MinusLogProbMetric: 30.1905 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 252/1000
2023-09-28 19:35:13.013 
Epoch 252/1000 
	 loss: 29.3121, MinusLogProbMetric: 29.3121, val_loss: 30.5661, val_MinusLogProbMetric: 30.5661

Epoch 252: val_loss did not improve from 29.36753
196/196 - 41s - loss: 29.3121 - MinusLogProbMetric: 29.3121 - val_loss: 30.5661 - val_MinusLogProbMetric: 30.5661 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 253/1000
2023-09-28 19:35:55.073 
Epoch 253/1000 
	 loss: 29.3632, MinusLogProbMetric: 29.3632, val_loss: 29.8769, val_MinusLogProbMetric: 29.8769

Epoch 253: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.3632 - MinusLogProbMetric: 29.3632 - val_loss: 29.8769 - val_MinusLogProbMetric: 29.8769 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 254/1000
2023-09-28 19:36:34.938 
Epoch 254/1000 
	 loss: 29.3072, MinusLogProbMetric: 29.3072, val_loss: 30.0594, val_MinusLogProbMetric: 30.0594

Epoch 254: val_loss did not improve from 29.36753
196/196 - 40s - loss: 29.3072 - MinusLogProbMetric: 29.3072 - val_loss: 30.0594 - val_MinusLogProbMetric: 30.0594 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 255/1000
2023-09-28 19:37:17.017 
Epoch 255/1000 
	 loss: 29.4014, MinusLogProbMetric: 29.4014, val_loss: 29.8775, val_MinusLogProbMetric: 29.8775

Epoch 255: val_loss did not improve from 29.36753
196/196 - 42s - loss: 29.4014 - MinusLogProbMetric: 29.4014 - val_loss: 29.8775 - val_MinusLogProbMetric: 29.8775 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 256/1000
2023-09-28 19:37:56.742 
Epoch 256/1000 
	 loss: 29.1261, MinusLogProbMetric: 29.1261, val_loss: 29.4759, val_MinusLogProbMetric: 29.4759

Epoch 256: val_loss did not improve from 29.36753
196/196 - 40s - loss: 29.1261 - MinusLogProbMetric: 29.1261 - val_loss: 29.4759 - val_MinusLogProbMetric: 29.4759 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 257/1000
2023-09-28 19:38:33.878 
Epoch 257/1000 
	 loss: 29.3828, MinusLogProbMetric: 29.3828, val_loss: 29.5461, val_MinusLogProbMetric: 29.5461

Epoch 257: val_loss did not improve from 29.36753
196/196 - 37s - loss: 29.3828 - MinusLogProbMetric: 29.3828 - val_loss: 29.5461 - val_MinusLogProbMetric: 29.5461 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 258/1000
2023-09-28 19:39:08.846 
Epoch 258/1000 
	 loss: 29.3241, MinusLogProbMetric: 29.3241, val_loss: 29.9744, val_MinusLogProbMetric: 29.9744

Epoch 258: val_loss did not improve from 29.36753
196/196 - 35s - loss: 29.3241 - MinusLogProbMetric: 29.3241 - val_loss: 29.9744 - val_MinusLogProbMetric: 29.9744 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 259/1000
2023-09-28 19:39:45.070 
Epoch 259/1000 
	 loss: 29.3168, MinusLogProbMetric: 29.3168, val_loss: 30.0986, val_MinusLogProbMetric: 30.0986

Epoch 259: val_loss did not improve from 29.36753
196/196 - 36s - loss: 29.3168 - MinusLogProbMetric: 29.3168 - val_loss: 30.0986 - val_MinusLogProbMetric: 30.0986 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 260/1000
2023-09-28 19:40:19.797 
Epoch 260/1000 
	 loss: 29.2828, MinusLogProbMetric: 29.2828, val_loss: 29.9199, val_MinusLogProbMetric: 29.9199

Epoch 260: val_loss did not improve from 29.36753
196/196 - 35s - loss: 29.2828 - MinusLogProbMetric: 29.2828 - val_loss: 29.9199 - val_MinusLogProbMetric: 29.9199 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 261/1000
2023-09-28 19:40:54.294 
Epoch 261/1000 
	 loss: 29.2072, MinusLogProbMetric: 29.2072, val_loss: 30.8398, val_MinusLogProbMetric: 30.8398

Epoch 261: val_loss did not improve from 29.36753
196/196 - 34s - loss: 29.2072 - MinusLogProbMetric: 29.2072 - val_loss: 30.8398 - val_MinusLogProbMetric: 30.8398 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 262/1000
2023-09-28 19:41:29.002 
Epoch 262/1000 
	 loss: 29.2733, MinusLogProbMetric: 29.2733, val_loss: 29.5287, val_MinusLogProbMetric: 29.5287

Epoch 262: val_loss did not improve from 29.36753
196/196 - 35s - loss: 29.2733 - MinusLogProbMetric: 29.2733 - val_loss: 29.5287 - val_MinusLogProbMetric: 29.5287 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 263/1000
2023-09-28 19:42:06.376 
Epoch 263/1000 
	 loss: 29.1482, MinusLogProbMetric: 29.1482, val_loss: 29.6456, val_MinusLogProbMetric: 29.6456

Epoch 263: val_loss did not improve from 29.36753
196/196 - 37s - loss: 29.1482 - MinusLogProbMetric: 29.1482 - val_loss: 29.6456 - val_MinusLogProbMetric: 29.6456 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 264/1000
2023-09-28 19:42:44.553 
Epoch 264/1000 
	 loss: 29.3396, MinusLogProbMetric: 29.3396, val_loss: 30.3832, val_MinusLogProbMetric: 30.3832

Epoch 264: val_loss did not improve from 29.36753
196/196 - 38s - loss: 29.3396 - MinusLogProbMetric: 29.3396 - val_loss: 30.3832 - val_MinusLogProbMetric: 30.3832 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 265/1000
2023-09-28 19:43:20.636 
Epoch 265/1000 
	 loss: 29.2346, MinusLogProbMetric: 29.2346, val_loss: 29.1846, val_MinusLogProbMetric: 29.1846

Epoch 265: val_loss improved from 29.36753 to 29.18457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 37s - loss: 29.2346 - MinusLogProbMetric: 29.2346 - val_loss: 29.1846 - val_MinusLogProbMetric: 29.1846 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 266/1000
2023-09-28 19:43:57.015 
Epoch 266/1000 
	 loss: 29.3101, MinusLogProbMetric: 29.3101, val_loss: 29.7729, val_MinusLogProbMetric: 29.7729

Epoch 266: val_loss did not improve from 29.18457
196/196 - 36s - loss: 29.3101 - MinusLogProbMetric: 29.3101 - val_loss: 29.7729 - val_MinusLogProbMetric: 29.7729 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 267/1000
2023-09-28 19:44:33.639 
Epoch 267/1000 
	 loss: 29.4173, MinusLogProbMetric: 29.4173, val_loss: 30.4334, val_MinusLogProbMetric: 30.4334

Epoch 267: val_loss did not improve from 29.18457
196/196 - 37s - loss: 29.4173 - MinusLogProbMetric: 29.4173 - val_loss: 30.4334 - val_MinusLogProbMetric: 30.4334 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 268/1000
2023-09-28 19:45:08.759 
Epoch 268/1000 
	 loss: 29.1501, MinusLogProbMetric: 29.1501, val_loss: 30.1762, val_MinusLogProbMetric: 30.1762

Epoch 268: val_loss did not improve from 29.18457
196/196 - 35s - loss: 29.1501 - MinusLogProbMetric: 29.1501 - val_loss: 30.1762 - val_MinusLogProbMetric: 30.1762 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 269/1000
2023-09-28 19:45:50.879 
Epoch 269/1000 
	 loss: 29.2467, MinusLogProbMetric: 29.2467, val_loss: 30.2899, val_MinusLogProbMetric: 30.2899

Epoch 269: val_loss did not improve from 29.18457
196/196 - 42s - loss: 29.2467 - MinusLogProbMetric: 29.2467 - val_loss: 30.2899 - val_MinusLogProbMetric: 30.2899 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 270/1000
2023-09-28 19:46:28.784 
Epoch 270/1000 
	 loss: 29.1811, MinusLogProbMetric: 29.1811, val_loss: 29.4087, val_MinusLogProbMetric: 29.4087

Epoch 270: val_loss did not improve from 29.18457
196/196 - 38s - loss: 29.1811 - MinusLogProbMetric: 29.1811 - val_loss: 29.4087 - val_MinusLogProbMetric: 29.4087 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 271/1000
2023-09-28 19:47:10.411 
Epoch 271/1000 
	 loss: 29.3303, MinusLogProbMetric: 29.3303, val_loss: 29.6749, val_MinusLogProbMetric: 29.6749

Epoch 271: val_loss did not improve from 29.18457
196/196 - 42s - loss: 29.3303 - MinusLogProbMetric: 29.3303 - val_loss: 29.6749 - val_MinusLogProbMetric: 29.6749 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 272/1000
2023-09-28 19:47:46.387 
Epoch 272/1000 
	 loss: 29.1791, MinusLogProbMetric: 29.1791, val_loss: 29.8636, val_MinusLogProbMetric: 29.8636

Epoch 272: val_loss did not improve from 29.18457
196/196 - 36s - loss: 29.1791 - MinusLogProbMetric: 29.1791 - val_loss: 29.8636 - val_MinusLogProbMetric: 29.8636 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 273/1000
2023-09-28 19:48:25.309 
Epoch 273/1000 
	 loss: 29.1635, MinusLogProbMetric: 29.1635, val_loss: 29.5856, val_MinusLogProbMetric: 29.5856

Epoch 273: val_loss did not improve from 29.18457
196/196 - 39s - loss: 29.1635 - MinusLogProbMetric: 29.1635 - val_loss: 29.5856 - val_MinusLogProbMetric: 29.5856 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 274/1000
2023-09-28 19:49:01.851 
Epoch 274/1000 
	 loss: 29.1257, MinusLogProbMetric: 29.1257, val_loss: 29.9759, val_MinusLogProbMetric: 29.9759

Epoch 274: val_loss did not improve from 29.18457
196/196 - 37s - loss: 29.1257 - MinusLogProbMetric: 29.1257 - val_loss: 29.9759 - val_MinusLogProbMetric: 29.9759 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 275/1000
2023-09-28 19:49:39.816 
Epoch 275/1000 
	 loss: 29.2032, MinusLogProbMetric: 29.2032, val_loss: 29.1777, val_MinusLogProbMetric: 29.1777

Epoch 275: val_loss improved from 29.18457 to 29.17769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 39s - loss: 29.2032 - MinusLogProbMetric: 29.2032 - val_loss: 29.1777 - val_MinusLogProbMetric: 29.1777 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 276/1000
2023-09-28 19:50:17.417 
Epoch 276/1000 
	 loss: 29.1858, MinusLogProbMetric: 29.1858, val_loss: 29.9954, val_MinusLogProbMetric: 29.9954

Epoch 276: val_loss did not improve from 29.17769
196/196 - 37s - loss: 29.1858 - MinusLogProbMetric: 29.1858 - val_loss: 29.9954 - val_MinusLogProbMetric: 29.9954 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 277/1000
2023-09-28 19:50:56.294 
Epoch 277/1000 
	 loss: 29.0763, MinusLogProbMetric: 29.0763, val_loss: 29.3234, val_MinusLogProbMetric: 29.3234

Epoch 277: val_loss did not improve from 29.17769
196/196 - 39s - loss: 29.0763 - MinusLogProbMetric: 29.0763 - val_loss: 29.3234 - val_MinusLogProbMetric: 29.3234 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 278/1000
2023-09-28 19:51:36.486 
Epoch 278/1000 
	 loss: 29.1581, MinusLogProbMetric: 29.1581, val_loss: 29.8000, val_MinusLogProbMetric: 29.8000

Epoch 278: val_loss did not improve from 29.17769
196/196 - 40s - loss: 29.1581 - MinusLogProbMetric: 29.1581 - val_loss: 29.8000 - val_MinusLogProbMetric: 29.8000 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 279/1000
2023-09-28 19:52:14.541 
Epoch 279/1000 
	 loss: 29.1513, MinusLogProbMetric: 29.1513, val_loss: 30.8453, val_MinusLogProbMetric: 30.8453

Epoch 279: val_loss did not improve from 29.17769
196/196 - 38s - loss: 29.1513 - MinusLogProbMetric: 29.1513 - val_loss: 30.8453 - val_MinusLogProbMetric: 30.8453 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 280/1000
2023-09-28 19:52:53.547 
Epoch 280/1000 
	 loss: 29.3110, MinusLogProbMetric: 29.3110, val_loss: 29.9547, val_MinusLogProbMetric: 29.9547

Epoch 280: val_loss did not improve from 29.17769
196/196 - 39s - loss: 29.3110 - MinusLogProbMetric: 29.3110 - val_loss: 29.9547 - val_MinusLogProbMetric: 29.9547 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 281/1000
2023-09-28 19:53:33.582 
Epoch 281/1000 
	 loss: 29.1293, MinusLogProbMetric: 29.1293, val_loss: 29.3561, val_MinusLogProbMetric: 29.3561

Epoch 281: val_loss did not improve from 29.17769
196/196 - 40s - loss: 29.1293 - MinusLogProbMetric: 29.1293 - val_loss: 29.3561 - val_MinusLogProbMetric: 29.3561 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 282/1000
2023-09-28 19:54:13.562 
Epoch 282/1000 
	 loss: 29.0604, MinusLogProbMetric: 29.0604, val_loss: 29.8733, val_MinusLogProbMetric: 29.8733

Epoch 282: val_loss did not improve from 29.17769
196/196 - 40s - loss: 29.0604 - MinusLogProbMetric: 29.0604 - val_loss: 29.8733 - val_MinusLogProbMetric: 29.8733 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 283/1000
2023-09-28 19:54:50.802 
Epoch 283/1000 
	 loss: 29.1872, MinusLogProbMetric: 29.1872, val_loss: 29.6000, val_MinusLogProbMetric: 29.6000

Epoch 283: val_loss did not improve from 29.17769
196/196 - 37s - loss: 29.1872 - MinusLogProbMetric: 29.1872 - val_loss: 29.6000 - val_MinusLogProbMetric: 29.6000 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 284/1000
2023-09-28 19:55:30.272 
Epoch 284/1000 
	 loss: 29.3097, MinusLogProbMetric: 29.3097, val_loss: 29.5773, val_MinusLogProbMetric: 29.5773

Epoch 284: val_loss did not improve from 29.17769
196/196 - 39s - loss: 29.3097 - MinusLogProbMetric: 29.3097 - val_loss: 29.5773 - val_MinusLogProbMetric: 29.5773 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 285/1000
2023-09-28 19:56:12.314 
Epoch 285/1000 
	 loss: 29.1289, MinusLogProbMetric: 29.1289, val_loss: 29.3144, val_MinusLogProbMetric: 29.3144

Epoch 285: val_loss did not improve from 29.17769
196/196 - 42s - loss: 29.1289 - MinusLogProbMetric: 29.1289 - val_loss: 29.3144 - val_MinusLogProbMetric: 29.3144 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 286/1000
2023-09-28 19:56:53.063 
Epoch 286/1000 
	 loss: 28.9900, MinusLogProbMetric: 28.9900, val_loss: 29.4584, val_MinusLogProbMetric: 29.4584

Epoch 286: val_loss did not improve from 29.17769
196/196 - 41s - loss: 28.9900 - MinusLogProbMetric: 28.9900 - val_loss: 29.4584 - val_MinusLogProbMetric: 29.4584 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 287/1000
2023-09-28 19:57:32.499 
Epoch 287/1000 
	 loss: 29.0823, MinusLogProbMetric: 29.0823, val_loss: 30.0365, val_MinusLogProbMetric: 30.0365

Epoch 287: val_loss did not improve from 29.17769
196/196 - 39s - loss: 29.0823 - MinusLogProbMetric: 29.0823 - val_loss: 30.0365 - val_MinusLogProbMetric: 30.0365 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 288/1000
2023-09-28 19:58:11.798 
Epoch 288/1000 
	 loss: 29.0788, MinusLogProbMetric: 29.0788, val_loss: 29.1597, val_MinusLogProbMetric: 29.1597

Epoch 288: val_loss improved from 29.17769 to 29.15968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 40s - loss: 29.0788 - MinusLogProbMetric: 29.0788 - val_loss: 29.1597 - val_MinusLogProbMetric: 29.1597 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 289/1000
2023-09-28 19:58:53.221 
Epoch 289/1000 
	 loss: 29.0685, MinusLogProbMetric: 29.0685, val_loss: 29.1644, val_MinusLogProbMetric: 29.1644

Epoch 289: val_loss did not improve from 29.15968
196/196 - 41s - loss: 29.0685 - MinusLogProbMetric: 29.0685 - val_loss: 29.1644 - val_MinusLogProbMetric: 29.1644 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 290/1000
2023-09-28 19:59:32.320 
Epoch 290/1000 
	 loss: 29.0225, MinusLogProbMetric: 29.0225, val_loss: 29.7941, val_MinusLogProbMetric: 29.7941

Epoch 290: val_loss did not improve from 29.15968
196/196 - 39s - loss: 29.0225 - MinusLogProbMetric: 29.0225 - val_loss: 29.7941 - val_MinusLogProbMetric: 29.7941 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 291/1000
2023-09-28 20:00:11.595 
Epoch 291/1000 
	 loss: 29.0542, MinusLogProbMetric: 29.0542, val_loss: 29.7566, val_MinusLogProbMetric: 29.7566

Epoch 291: val_loss did not improve from 29.15968
196/196 - 39s - loss: 29.0542 - MinusLogProbMetric: 29.0542 - val_loss: 29.7566 - val_MinusLogProbMetric: 29.7566 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 292/1000
2023-09-28 20:00:52.454 
Epoch 292/1000 
	 loss: 29.0361, MinusLogProbMetric: 29.0361, val_loss: 30.2367, val_MinusLogProbMetric: 30.2367

Epoch 292: val_loss did not improve from 29.15968
196/196 - 41s - loss: 29.0361 - MinusLogProbMetric: 29.0361 - val_loss: 30.2367 - val_MinusLogProbMetric: 30.2367 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 293/1000
2023-09-28 20:01:32.214 
Epoch 293/1000 
	 loss: 29.1106, MinusLogProbMetric: 29.1106, val_loss: 29.4305, val_MinusLogProbMetric: 29.4305

Epoch 293: val_loss did not improve from 29.15968
196/196 - 40s - loss: 29.1106 - MinusLogProbMetric: 29.1106 - val_loss: 29.4305 - val_MinusLogProbMetric: 29.4305 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 294/1000
2023-09-28 20:02:11.200 
Epoch 294/1000 
	 loss: 28.9251, MinusLogProbMetric: 28.9251, val_loss: 30.2668, val_MinusLogProbMetric: 30.2668

Epoch 294: val_loss did not improve from 29.15968
196/196 - 39s - loss: 28.9251 - MinusLogProbMetric: 28.9251 - val_loss: 30.2668 - val_MinusLogProbMetric: 30.2668 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 295/1000
2023-09-28 20:02:50.135 
Epoch 295/1000 
	 loss: 29.0158, MinusLogProbMetric: 29.0158, val_loss: 29.4336, val_MinusLogProbMetric: 29.4336

Epoch 295: val_loss did not improve from 29.15968
196/196 - 39s - loss: 29.0158 - MinusLogProbMetric: 29.0158 - val_loss: 29.4336 - val_MinusLogProbMetric: 29.4336 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 296/1000
2023-09-28 20:03:30.520 
Epoch 296/1000 
	 loss: 29.0132, MinusLogProbMetric: 29.0132, val_loss: 30.1787, val_MinusLogProbMetric: 30.1787

Epoch 296: val_loss did not improve from 29.15968
196/196 - 40s - loss: 29.0132 - MinusLogProbMetric: 29.0132 - val_loss: 30.1787 - val_MinusLogProbMetric: 30.1787 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 297/1000
2023-09-28 20:04:08.754 
Epoch 297/1000 
	 loss: 29.1754, MinusLogProbMetric: 29.1754, val_loss: 30.0498, val_MinusLogProbMetric: 30.0498

Epoch 297: val_loss did not improve from 29.15968
196/196 - 38s - loss: 29.1754 - MinusLogProbMetric: 29.1754 - val_loss: 30.0498 - val_MinusLogProbMetric: 30.0498 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 298/1000
2023-09-28 20:04:47.144 
Epoch 298/1000 
	 loss: 29.0704, MinusLogProbMetric: 29.0704, val_loss: 30.3250, val_MinusLogProbMetric: 30.3250

Epoch 298: val_loss did not improve from 29.15968
196/196 - 38s - loss: 29.0704 - MinusLogProbMetric: 29.0704 - val_loss: 30.3250 - val_MinusLogProbMetric: 30.3250 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 299/1000
2023-09-28 20:05:27.442 
Epoch 299/1000 
	 loss: 29.1769, MinusLogProbMetric: 29.1769, val_loss: 30.3679, val_MinusLogProbMetric: 30.3679

Epoch 299: val_loss did not improve from 29.15968
196/196 - 40s - loss: 29.1769 - MinusLogProbMetric: 29.1769 - val_loss: 30.3679 - val_MinusLogProbMetric: 30.3679 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 300/1000
2023-09-28 20:06:06.029 
Epoch 300/1000 
	 loss: 29.1273, MinusLogProbMetric: 29.1273, val_loss: 29.7127, val_MinusLogProbMetric: 29.7127

Epoch 300: val_loss did not improve from 29.15968
196/196 - 39s - loss: 29.1273 - MinusLogProbMetric: 29.1273 - val_loss: 29.7127 - val_MinusLogProbMetric: 29.7127 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 301/1000
2023-09-28 20:06:45.931 
Epoch 301/1000 
	 loss: 29.0314, MinusLogProbMetric: 29.0314, val_loss: 29.8127, val_MinusLogProbMetric: 29.8127

Epoch 301: val_loss did not improve from 29.15968
196/196 - 40s - loss: 29.0314 - MinusLogProbMetric: 29.0314 - val_loss: 29.8127 - val_MinusLogProbMetric: 29.8127 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 302/1000
2023-09-28 20:07:27.849 
Epoch 302/1000 
	 loss: 29.1022, MinusLogProbMetric: 29.1022, val_loss: 29.6913, val_MinusLogProbMetric: 29.6913

Epoch 302: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.1022 - MinusLogProbMetric: 29.1022 - val_loss: 29.6913 - val_MinusLogProbMetric: 29.6913 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 303/1000
2023-09-28 20:08:10.337 
Epoch 303/1000 
	 loss: 29.1252, MinusLogProbMetric: 29.1252, val_loss: 30.0755, val_MinusLogProbMetric: 30.0755

Epoch 303: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.1252 - MinusLogProbMetric: 29.1252 - val_loss: 30.0755 - val_MinusLogProbMetric: 30.0755 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 304/1000
2023-09-28 20:08:52.477 
Epoch 304/1000 
	 loss: 29.0754, MinusLogProbMetric: 29.0754, val_loss: 29.3531, val_MinusLogProbMetric: 29.3531

Epoch 304: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0754 - MinusLogProbMetric: 29.0754 - val_loss: 29.3531 - val_MinusLogProbMetric: 29.3531 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 305/1000
2023-09-28 20:09:34.584 
Epoch 305/1000 
	 loss: 29.0098, MinusLogProbMetric: 29.0098, val_loss: 29.7539, val_MinusLogProbMetric: 29.7539

Epoch 305: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0098 - MinusLogProbMetric: 29.0098 - val_loss: 29.7539 - val_MinusLogProbMetric: 29.7539 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 306/1000
2023-09-28 20:10:16.294 
Epoch 306/1000 
	 loss: 29.0457, MinusLogProbMetric: 29.0457, val_loss: 29.6262, val_MinusLogProbMetric: 29.6262

Epoch 306: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0457 - MinusLogProbMetric: 29.0457 - val_loss: 29.6262 - val_MinusLogProbMetric: 29.6262 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 307/1000
2023-09-28 20:10:58.141 
Epoch 307/1000 
	 loss: 29.0810, MinusLogProbMetric: 29.0810, val_loss: 29.9386, val_MinusLogProbMetric: 29.9386

Epoch 307: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0810 - MinusLogProbMetric: 29.0810 - val_loss: 29.9386 - val_MinusLogProbMetric: 29.9386 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 308/1000
2023-09-28 20:11:38.707 
Epoch 308/1000 
	 loss: 28.8769, MinusLogProbMetric: 28.8769, val_loss: 29.8141, val_MinusLogProbMetric: 29.8141

Epoch 308: val_loss did not improve from 29.15968
196/196 - 41s - loss: 28.8769 - MinusLogProbMetric: 28.8769 - val_loss: 29.8141 - val_MinusLogProbMetric: 29.8141 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 309/1000
2023-09-28 20:12:20.929 
Epoch 309/1000 
	 loss: 29.0812, MinusLogProbMetric: 29.0812, val_loss: 30.6354, val_MinusLogProbMetric: 30.6354

Epoch 309: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0812 - MinusLogProbMetric: 29.0812 - val_loss: 30.6354 - val_MinusLogProbMetric: 30.6354 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 310/1000
2023-09-28 20:13:03.243 
Epoch 310/1000 
	 loss: 28.9186, MinusLogProbMetric: 28.9186, val_loss: 29.3297, val_MinusLogProbMetric: 29.3297

Epoch 310: val_loss did not improve from 29.15968
196/196 - 42s - loss: 28.9186 - MinusLogProbMetric: 28.9186 - val_loss: 29.3297 - val_MinusLogProbMetric: 29.3297 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 311/1000
2023-09-28 20:13:44.814 
Epoch 311/1000 
	 loss: 29.0510, MinusLogProbMetric: 29.0510, val_loss: 29.7363, val_MinusLogProbMetric: 29.7363

Epoch 311: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0510 - MinusLogProbMetric: 29.0510 - val_loss: 29.7363 - val_MinusLogProbMetric: 29.7363 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 312/1000
2023-09-28 20:14:26.790 
Epoch 312/1000 
	 loss: 29.0932, MinusLogProbMetric: 29.0932, val_loss: 30.1080, val_MinusLogProbMetric: 30.1080

Epoch 312: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0932 - MinusLogProbMetric: 29.0932 - val_loss: 30.1080 - val_MinusLogProbMetric: 30.1080 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 313/1000
2023-09-28 20:15:07.968 
Epoch 313/1000 
	 loss: 28.9941, MinusLogProbMetric: 28.9941, val_loss: 29.7147, val_MinusLogProbMetric: 29.7147

Epoch 313: val_loss did not improve from 29.15968
196/196 - 41s - loss: 28.9941 - MinusLogProbMetric: 28.9941 - val_loss: 29.7147 - val_MinusLogProbMetric: 29.7147 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 314/1000
2023-09-28 20:15:49.206 
Epoch 314/1000 
	 loss: 29.1260, MinusLogProbMetric: 29.1260, val_loss: 29.7995, val_MinusLogProbMetric: 29.7995

Epoch 314: val_loss did not improve from 29.15968
196/196 - 41s - loss: 29.1260 - MinusLogProbMetric: 29.1260 - val_loss: 29.7995 - val_MinusLogProbMetric: 29.7995 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 315/1000
2023-09-28 20:16:32.273 
Epoch 315/1000 
	 loss: 29.0140, MinusLogProbMetric: 29.0140, val_loss: 29.5629, val_MinusLogProbMetric: 29.5629

Epoch 315: val_loss did not improve from 29.15968
196/196 - 43s - loss: 29.0140 - MinusLogProbMetric: 29.0140 - val_loss: 29.5629 - val_MinusLogProbMetric: 29.5629 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 316/1000
2023-09-28 20:17:14.706 
Epoch 316/1000 
	 loss: 29.0156, MinusLogProbMetric: 29.0156, val_loss: 30.6030, val_MinusLogProbMetric: 30.6030

Epoch 316: val_loss did not improve from 29.15968
196/196 - 42s - loss: 29.0156 - MinusLogProbMetric: 29.0156 - val_loss: 30.6030 - val_MinusLogProbMetric: 30.6030 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 317/1000
2023-09-28 20:17:57.771 
Epoch 317/1000 
	 loss: 28.9609, MinusLogProbMetric: 28.9609, val_loss: 29.5190, val_MinusLogProbMetric: 29.5190

Epoch 317: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.9609 - MinusLogProbMetric: 28.9609 - val_loss: 29.5190 - val_MinusLogProbMetric: 29.5190 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 318/1000
2023-09-28 20:18:39.799 
Epoch 318/1000 
	 loss: 28.9452, MinusLogProbMetric: 28.9452, val_loss: 30.1210, val_MinusLogProbMetric: 30.1210

Epoch 318: val_loss did not improve from 29.15968
196/196 - 42s - loss: 28.9452 - MinusLogProbMetric: 28.9452 - val_loss: 30.1210 - val_MinusLogProbMetric: 30.1210 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 319/1000
2023-09-28 20:19:22.939 
Epoch 319/1000 
	 loss: 29.0763, MinusLogProbMetric: 29.0763, val_loss: 30.7672, val_MinusLogProbMetric: 30.7672

Epoch 319: val_loss did not improve from 29.15968
196/196 - 43s - loss: 29.0763 - MinusLogProbMetric: 29.0763 - val_loss: 30.7672 - val_MinusLogProbMetric: 30.7672 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 320/1000
2023-09-28 20:20:05.729 
Epoch 320/1000 
	 loss: 29.0567, MinusLogProbMetric: 29.0567, val_loss: 30.8489, val_MinusLogProbMetric: 30.8489

Epoch 320: val_loss did not improve from 29.15968
196/196 - 43s - loss: 29.0567 - MinusLogProbMetric: 29.0567 - val_loss: 30.8489 - val_MinusLogProbMetric: 30.8489 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 321/1000
2023-09-28 20:20:49.364 
Epoch 321/1000 
	 loss: 28.9881, MinusLogProbMetric: 28.9881, val_loss: 29.1756, val_MinusLogProbMetric: 29.1756

Epoch 321: val_loss did not improve from 29.15968
196/196 - 44s - loss: 28.9881 - MinusLogProbMetric: 28.9881 - val_loss: 29.1756 - val_MinusLogProbMetric: 29.1756 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 322/1000
2023-09-28 20:21:30.149 
Epoch 322/1000 
	 loss: 28.9445, MinusLogProbMetric: 28.9445, val_loss: 29.3618, val_MinusLogProbMetric: 29.3618

Epoch 322: val_loss did not improve from 29.15968
196/196 - 41s - loss: 28.9445 - MinusLogProbMetric: 28.9445 - val_loss: 29.3618 - val_MinusLogProbMetric: 29.3618 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 323/1000
2023-09-28 20:22:13.389 
Epoch 323/1000 
	 loss: 28.9328, MinusLogProbMetric: 28.9328, val_loss: 29.7516, val_MinusLogProbMetric: 29.7516

Epoch 323: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.9328 - MinusLogProbMetric: 28.9328 - val_loss: 29.7516 - val_MinusLogProbMetric: 29.7516 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 324/1000
2023-09-28 20:22:56.413 
Epoch 324/1000 
	 loss: 29.0950, MinusLogProbMetric: 29.0950, val_loss: 29.5485, val_MinusLogProbMetric: 29.5485

Epoch 324: val_loss did not improve from 29.15968
196/196 - 43s - loss: 29.0950 - MinusLogProbMetric: 29.0950 - val_loss: 29.5485 - val_MinusLogProbMetric: 29.5485 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 325/1000
2023-09-28 20:23:40.091 
Epoch 325/1000 
	 loss: 28.7823, MinusLogProbMetric: 28.7823, val_loss: 30.0030, val_MinusLogProbMetric: 30.0030

Epoch 325: val_loss did not improve from 29.15968
196/196 - 44s - loss: 28.7823 - MinusLogProbMetric: 28.7823 - val_loss: 30.0030 - val_MinusLogProbMetric: 30.0030 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 326/1000
2023-09-28 20:24:19.649 
Epoch 326/1000 
	 loss: 29.0236, MinusLogProbMetric: 29.0236, val_loss: 29.6532, val_MinusLogProbMetric: 29.6532

Epoch 326: val_loss did not improve from 29.15968
196/196 - 40s - loss: 29.0236 - MinusLogProbMetric: 29.0236 - val_loss: 29.6532 - val_MinusLogProbMetric: 29.6532 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 327/1000
2023-09-28 20:25:01.747 
Epoch 327/1000 
	 loss: 28.9974, MinusLogProbMetric: 28.9974, val_loss: 29.5409, val_MinusLogProbMetric: 29.5409

Epoch 327: val_loss did not improve from 29.15968
196/196 - 42s - loss: 28.9974 - MinusLogProbMetric: 28.9974 - val_loss: 29.5409 - val_MinusLogProbMetric: 29.5409 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 328/1000
2023-09-28 20:25:44.828 
Epoch 328/1000 
	 loss: 28.9686, MinusLogProbMetric: 28.9686, val_loss: 29.3877, val_MinusLogProbMetric: 29.3877

Epoch 328: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.9686 - MinusLogProbMetric: 28.9686 - val_loss: 29.3877 - val_MinusLogProbMetric: 29.3877 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 329/1000
2023-09-28 20:26:27.948 
Epoch 329/1000 
	 loss: 28.9066, MinusLogProbMetric: 28.9066, val_loss: 29.3816, val_MinusLogProbMetric: 29.3816

Epoch 329: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.9066 - MinusLogProbMetric: 28.9066 - val_loss: 29.3816 - val_MinusLogProbMetric: 29.3816 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 330/1000
2023-09-28 20:27:10.959 
Epoch 330/1000 
	 loss: 28.8748, MinusLogProbMetric: 28.8748, val_loss: 29.7047, val_MinusLogProbMetric: 29.7047

Epoch 330: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.8748 - MinusLogProbMetric: 28.8748 - val_loss: 29.7047 - val_MinusLogProbMetric: 29.7047 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 331/1000
2023-09-28 20:27:53.997 
Epoch 331/1000 
	 loss: 28.8964, MinusLogProbMetric: 28.8964, val_loss: 29.9925, val_MinusLogProbMetric: 29.9925

Epoch 331: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.8964 - MinusLogProbMetric: 28.8964 - val_loss: 29.9925 - val_MinusLogProbMetric: 29.9925 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 332/1000
2023-09-28 20:28:36.868 
Epoch 332/1000 
	 loss: 28.8780, MinusLogProbMetric: 28.8780, val_loss: 29.4352, val_MinusLogProbMetric: 29.4352

Epoch 332: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.8780 - MinusLogProbMetric: 28.8780 - val_loss: 29.4352 - val_MinusLogProbMetric: 29.4352 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 333/1000
2023-09-28 20:29:19.488 
Epoch 333/1000 
	 loss: 29.0584, MinusLogProbMetric: 29.0584, val_loss: 29.9094, val_MinusLogProbMetric: 29.9094

Epoch 333: val_loss did not improve from 29.15968
196/196 - 43s - loss: 29.0584 - MinusLogProbMetric: 29.0584 - val_loss: 29.9094 - val_MinusLogProbMetric: 29.9094 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 334/1000
2023-09-28 20:30:02.619 
Epoch 334/1000 
	 loss: 28.9190, MinusLogProbMetric: 28.9190, val_loss: 30.1570, val_MinusLogProbMetric: 30.1570

Epoch 334: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.9190 - MinusLogProbMetric: 28.9190 - val_loss: 30.1570 - val_MinusLogProbMetric: 30.1570 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 335/1000
2023-09-28 20:30:45.539 
Epoch 335/1000 
	 loss: 28.9037, MinusLogProbMetric: 28.9037, val_loss: 29.4431, val_MinusLogProbMetric: 29.4431

Epoch 335: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.9037 - MinusLogProbMetric: 28.9037 - val_loss: 29.4431 - val_MinusLogProbMetric: 29.4431 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 336/1000
2023-09-28 20:31:28.600 
Epoch 336/1000 
	 loss: 28.8566, MinusLogProbMetric: 28.8566, val_loss: 29.2281, val_MinusLogProbMetric: 29.2281

Epoch 336: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.8566 - MinusLogProbMetric: 28.8566 - val_loss: 29.2281 - val_MinusLogProbMetric: 29.2281 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 337/1000
2023-09-28 20:32:11.544 
Epoch 337/1000 
	 loss: 28.9313, MinusLogProbMetric: 28.9313, val_loss: 29.6825, val_MinusLogProbMetric: 29.6825

Epoch 337: val_loss did not improve from 29.15968
196/196 - 43s - loss: 28.9313 - MinusLogProbMetric: 28.9313 - val_loss: 29.6825 - val_MinusLogProbMetric: 29.6825 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 338/1000
2023-09-28 20:32:54.281 
Epoch 338/1000 
	 loss: 29.1037, MinusLogProbMetric: 29.1037, val_loss: 29.5673, val_MinusLogProbMetric: 29.5673

Epoch 338: val_loss did not improve from 29.15968
196/196 - 43s - loss: 29.1037 - MinusLogProbMetric: 29.1037 - val_loss: 29.5673 - val_MinusLogProbMetric: 29.5673 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 339/1000
2023-09-28 20:33:37.614 
Epoch 339/1000 
	 loss: 28.0745, MinusLogProbMetric: 28.0745, val_loss: 28.8074, val_MinusLogProbMetric: 28.8074

Epoch 339: val_loss improved from 29.15968 to 28.80738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 28.0745 - MinusLogProbMetric: 28.0745 - val_loss: 28.8074 - val_MinusLogProbMetric: 28.8074 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 340/1000
2023-09-28 20:34:21.210 
Epoch 340/1000 
	 loss: 28.0199, MinusLogProbMetric: 28.0199, val_loss: 28.6735, val_MinusLogProbMetric: 28.6735

Epoch 340: val_loss improved from 28.80738 to 28.67347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 28.0199 - MinusLogProbMetric: 28.0199 - val_loss: 28.6735 - val_MinusLogProbMetric: 28.6735 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 341/1000
2023-09-28 20:35:05.242 
Epoch 341/1000 
	 loss: 27.9987, MinusLogProbMetric: 27.9987, val_loss: 28.8379, val_MinusLogProbMetric: 28.8379

Epoch 341: val_loss did not improve from 28.67347
196/196 - 43s - loss: 27.9987 - MinusLogProbMetric: 27.9987 - val_loss: 28.8379 - val_MinusLogProbMetric: 28.8379 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 342/1000
2023-09-28 20:35:48.320 
Epoch 342/1000 
	 loss: 28.0125, MinusLogProbMetric: 28.0125, val_loss: 28.6999, val_MinusLogProbMetric: 28.6999

Epoch 342: val_loss did not improve from 28.67347
196/196 - 43s - loss: 28.0125 - MinusLogProbMetric: 28.0125 - val_loss: 28.6999 - val_MinusLogProbMetric: 28.6999 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 343/1000
2023-09-28 20:36:31.385 
Epoch 343/1000 
	 loss: 28.1442, MinusLogProbMetric: 28.1442, val_loss: 28.8495, val_MinusLogProbMetric: 28.8495

Epoch 343: val_loss did not improve from 28.67347
196/196 - 43s - loss: 28.1442 - MinusLogProbMetric: 28.1442 - val_loss: 28.8495 - val_MinusLogProbMetric: 28.8495 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 344/1000
2023-09-28 20:37:13.988 
Epoch 344/1000 
	 loss: 28.0148, MinusLogProbMetric: 28.0148, val_loss: 28.7136, val_MinusLogProbMetric: 28.7136

Epoch 344: val_loss did not improve from 28.67347
196/196 - 43s - loss: 28.0148 - MinusLogProbMetric: 28.0148 - val_loss: 28.7136 - val_MinusLogProbMetric: 28.7136 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 345/1000
2023-09-28 20:37:56.584 
Epoch 345/1000 
	 loss: 28.0132, MinusLogProbMetric: 28.0132, val_loss: 28.7996, val_MinusLogProbMetric: 28.7996

Epoch 345: val_loss did not improve from 28.67347
196/196 - 43s - loss: 28.0132 - MinusLogProbMetric: 28.0132 - val_loss: 28.7996 - val_MinusLogProbMetric: 28.7996 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 346/1000
2023-09-28 20:38:39.768 
Epoch 346/1000 
	 loss: 28.0063, MinusLogProbMetric: 28.0063, val_loss: 28.6898, val_MinusLogProbMetric: 28.6898

Epoch 346: val_loss did not improve from 28.67347
196/196 - 43s - loss: 28.0063 - MinusLogProbMetric: 28.0063 - val_loss: 28.6898 - val_MinusLogProbMetric: 28.6898 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 347/1000
2023-09-28 20:39:22.581 
Epoch 347/1000 
	 loss: 28.0253, MinusLogProbMetric: 28.0253, val_loss: 28.6641, val_MinusLogProbMetric: 28.6641

Epoch 347: val_loss improved from 28.67347 to 28.66412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 28.0253 - MinusLogProbMetric: 28.0253 - val_loss: 28.6641 - val_MinusLogProbMetric: 28.6641 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 348/1000
2023-09-28 20:40:06.729 
Epoch 348/1000 
	 loss: 27.9837, MinusLogProbMetric: 27.9837, val_loss: 28.7777, val_MinusLogProbMetric: 28.7777

Epoch 348: val_loss did not improve from 28.66412
196/196 - 43s - loss: 27.9837 - MinusLogProbMetric: 27.9837 - val_loss: 28.7777 - val_MinusLogProbMetric: 28.7777 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 349/1000
2023-09-28 20:40:49.580 
Epoch 349/1000 
	 loss: 28.1204, MinusLogProbMetric: 28.1204, val_loss: 28.7307, val_MinusLogProbMetric: 28.7307

Epoch 349: val_loss did not improve from 28.66412
196/196 - 43s - loss: 28.1204 - MinusLogProbMetric: 28.1204 - val_loss: 28.7307 - val_MinusLogProbMetric: 28.7307 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 350/1000
2023-09-28 20:41:32.571 
Epoch 350/1000 
	 loss: 28.0047, MinusLogProbMetric: 28.0047, val_loss: 28.6887, val_MinusLogProbMetric: 28.6887

Epoch 350: val_loss did not improve from 28.66412
196/196 - 43s - loss: 28.0047 - MinusLogProbMetric: 28.0047 - val_loss: 28.6887 - val_MinusLogProbMetric: 28.6887 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 351/1000
2023-09-28 20:42:14.175 
Epoch 351/1000 
	 loss: 28.0020, MinusLogProbMetric: 28.0020, val_loss: 28.6614, val_MinusLogProbMetric: 28.6614

Epoch 351: val_loss improved from 28.66412 to 28.66137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 28.0020 - MinusLogProbMetric: 28.0020 - val_loss: 28.6614 - val_MinusLogProbMetric: 28.6614 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 352/1000
2023-09-28 20:42:58.059 
Epoch 352/1000 
	 loss: 28.2205, MinusLogProbMetric: 28.2205, val_loss: 28.7786, val_MinusLogProbMetric: 28.7786

Epoch 352: val_loss did not improve from 28.66137
196/196 - 43s - loss: 28.2205 - MinusLogProbMetric: 28.2205 - val_loss: 28.7786 - val_MinusLogProbMetric: 28.7786 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 353/1000
2023-09-28 20:43:41.009 
Epoch 353/1000 
	 loss: 28.0687, MinusLogProbMetric: 28.0687, val_loss: 28.9935, val_MinusLogProbMetric: 28.9935

Epoch 353: val_loss did not improve from 28.66137
196/196 - 43s - loss: 28.0687 - MinusLogProbMetric: 28.0687 - val_loss: 28.9935 - val_MinusLogProbMetric: 28.9935 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 354/1000
2023-09-28 20:44:21.642 
Epoch 354/1000 
	 loss: 28.1105, MinusLogProbMetric: 28.1105, val_loss: 28.9955, val_MinusLogProbMetric: 28.9955

Epoch 354: val_loss did not improve from 28.66137
196/196 - 41s - loss: 28.1105 - MinusLogProbMetric: 28.1105 - val_loss: 28.9955 - val_MinusLogProbMetric: 28.9955 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 355/1000
2023-09-28 20:45:04.820 
Epoch 355/1000 
	 loss: 28.0572, MinusLogProbMetric: 28.0572, val_loss: 28.9548, val_MinusLogProbMetric: 28.9548

Epoch 355: val_loss did not improve from 28.66137
196/196 - 43s - loss: 28.0572 - MinusLogProbMetric: 28.0572 - val_loss: 28.9548 - val_MinusLogProbMetric: 28.9548 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 356/1000
2023-09-28 20:45:48.077 
Epoch 356/1000 
	 loss: 28.0563, MinusLogProbMetric: 28.0563, val_loss: 28.8477, val_MinusLogProbMetric: 28.8477

Epoch 356: val_loss did not improve from 28.66137
196/196 - 43s - loss: 28.0563 - MinusLogProbMetric: 28.0563 - val_loss: 28.8477 - val_MinusLogProbMetric: 28.8477 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 357/1000
2023-09-28 20:46:29.736 
Epoch 357/1000 
	 loss: 28.2451, MinusLogProbMetric: 28.2451, val_loss: 28.8682, val_MinusLogProbMetric: 28.8682

Epoch 357: val_loss did not improve from 28.66137
196/196 - 42s - loss: 28.2451 - MinusLogProbMetric: 28.2451 - val_loss: 28.8682 - val_MinusLogProbMetric: 28.8682 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 358/1000
2023-09-28 20:47:12.790 
Epoch 358/1000 
	 loss: 28.0270, MinusLogProbMetric: 28.0270, val_loss: 28.7422, val_MinusLogProbMetric: 28.7422

Epoch 358: val_loss did not improve from 28.66137
196/196 - 43s - loss: 28.0270 - MinusLogProbMetric: 28.0270 - val_loss: 28.7422 - val_MinusLogProbMetric: 28.7422 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 359/1000
2023-09-28 20:47:54.915 
Epoch 359/1000 
	 loss: 28.0453, MinusLogProbMetric: 28.0453, val_loss: 28.6329, val_MinusLogProbMetric: 28.6329

Epoch 359: val_loss improved from 28.66137 to 28.63290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 28.0453 - MinusLogProbMetric: 28.0453 - val_loss: 28.6329 - val_MinusLogProbMetric: 28.6329 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 360/1000
2023-09-28 20:48:38.707 
Epoch 360/1000 
	 loss: 28.0850, MinusLogProbMetric: 28.0850, val_loss: 28.8684, val_MinusLogProbMetric: 28.8684

Epoch 360: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.0850 - MinusLogProbMetric: 28.0850 - val_loss: 28.8684 - val_MinusLogProbMetric: 28.8684 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 361/1000
2023-09-28 20:49:21.486 
Epoch 361/1000 
	 loss: 28.2151, MinusLogProbMetric: 28.2151, val_loss: 28.8317, val_MinusLogProbMetric: 28.8317

Epoch 361: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.2151 - MinusLogProbMetric: 28.2151 - val_loss: 28.8317 - val_MinusLogProbMetric: 28.8317 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 362/1000
2023-09-28 20:50:03.831 
Epoch 362/1000 
	 loss: 28.0684, MinusLogProbMetric: 28.0684, val_loss: 28.9512, val_MinusLogProbMetric: 28.9512

Epoch 362: val_loss did not improve from 28.63290
196/196 - 42s - loss: 28.0684 - MinusLogProbMetric: 28.0684 - val_loss: 28.9512 - val_MinusLogProbMetric: 28.9512 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 363/1000
2023-09-28 20:50:46.608 
Epoch 363/1000 
	 loss: 28.0805, MinusLogProbMetric: 28.0805, val_loss: 28.9101, val_MinusLogProbMetric: 28.9101

Epoch 363: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.0805 - MinusLogProbMetric: 28.0805 - val_loss: 28.9101 - val_MinusLogProbMetric: 28.9101 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 364/1000
2023-09-28 20:51:28.918 
Epoch 364/1000 
	 loss: 28.0590, MinusLogProbMetric: 28.0590, val_loss: 28.9757, val_MinusLogProbMetric: 28.9757

Epoch 364: val_loss did not improve from 28.63290
196/196 - 42s - loss: 28.0590 - MinusLogProbMetric: 28.0590 - val_loss: 28.9757 - val_MinusLogProbMetric: 28.9757 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 365/1000
2023-09-28 20:52:11.951 
Epoch 365/1000 
	 loss: 28.1366, MinusLogProbMetric: 28.1366, val_loss: 28.7722, val_MinusLogProbMetric: 28.7722

Epoch 365: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.1366 - MinusLogProbMetric: 28.1366 - val_loss: 28.7722 - val_MinusLogProbMetric: 28.7722 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 366/1000
2023-09-28 20:52:55.273 
Epoch 366/1000 
	 loss: 28.0940, MinusLogProbMetric: 28.0940, val_loss: 28.8528, val_MinusLogProbMetric: 28.8528

Epoch 366: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.0940 - MinusLogProbMetric: 28.0940 - val_loss: 28.8528 - val_MinusLogProbMetric: 28.8528 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 367/1000
2023-09-28 20:53:38.730 
Epoch 367/1000 
	 loss: 28.0966, MinusLogProbMetric: 28.0966, val_loss: 28.8665, val_MinusLogProbMetric: 28.8665

Epoch 367: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.0966 - MinusLogProbMetric: 28.0966 - val_loss: 28.8665 - val_MinusLogProbMetric: 28.8665 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 368/1000
2023-09-28 20:54:22.160 
Epoch 368/1000 
	 loss: 28.0828, MinusLogProbMetric: 28.0828, val_loss: 28.8504, val_MinusLogProbMetric: 28.8504

Epoch 368: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.0828 - MinusLogProbMetric: 28.0828 - val_loss: 28.8504 - val_MinusLogProbMetric: 28.8504 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 369/1000
2023-09-28 20:55:04.601 
Epoch 369/1000 
	 loss: 28.1548, MinusLogProbMetric: 28.1548, val_loss: 29.1435, val_MinusLogProbMetric: 29.1435

Epoch 369: val_loss did not improve from 28.63290
196/196 - 42s - loss: 28.1548 - MinusLogProbMetric: 28.1548 - val_loss: 29.1435 - val_MinusLogProbMetric: 29.1435 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 370/1000
2023-09-28 20:55:47.595 
Epoch 370/1000 
	 loss: 28.0962, MinusLogProbMetric: 28.0962, val_loss: 29.1129, val_MinusLogProbMetric: 29.1129

Epoch 370: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.0962 - MinusLogProbMetric: 28.0962 - val_loss: 29.1129 - val_MinusLogProbMetric: 29.1129 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 371/1000
2023-09-28 20:56:27.989 
Epoch 371/1000 
	 loss: 28.0369, MinusLogProbMetric: 28.0369, val_loss: 28.7217, val_MinusLogProbMetric: 28.7217

Epoch 371: val_loss did not improve from 28.63290
196/196 - 40s - loss: 28.0369 - MinusLogProbMetric: 28.0369 - val_loss: 28.7217 - val_MinusLogProbMetric: 28.7217 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 372/1000
2023-09-28 20:57:09.061 
Epoch 372/1000 
	 loss: 28.0586, MinusLogProbMetric: 28.0586, val_loss: 28.7481, val_MinusLogProbMetric: 28.7481

Epoch 372: val_loss did not improve from 28.63290
196/196 - 41s - loss: 28.0586 - MinusLogProbMetric: 28.0586 - val_loss: 28.7481 - val_MinusLogProbMetric: 28.7481 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 373/1000
2023-09-28 20:57:52.435 
Epoch 373/1000 
	 loss: 28.1764, MinusLogProbMetric: 28.1764, val_loss: 28.7788, val_MinusLogProbMetric: 28.7788

Epoch 373: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.1764 - MinusLogProbMetric: 28.1764 - val_loss: 28.7788 - val_MinusLogProbMetric: 28.7788 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 374/1000
2023-09-28 20:58:31.386 
Epoch 374/1000 
	 loss: 28.2136, MinusLogProbMetric: 28.2136, val_loss: 28.8531, val_MinusLogProbMetric: 28.8531

Epoch 374: val_loss did not improve from 28.63290
196/196 - 39s - loss: 28.2136 - MinusLogProbMetric: 28.2136 - val_loss: 28.8531 - val_MinusLogProbMetric: 28.8531 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 375/1000
2023-09-28 20:59:14.048 
Epoch 375/1000 
	 loss: 27.9847, MinusLogProbMetric: 27.9847, val_loss: 28.8753, val_MinusLogProbMetric: 28.8753

Epoch 375: val_loss did not improve from 28.63290
196/196 - 43s - loss: 27.9847 - MinusLogProbMetric: 27.9847 - val_loss: 28.8753 - val_MinusLogProbMetric: 28.8753 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 376/1000
2023-09-28 20:59:57.292 
Epoch 376/1000 
	 loss: 28.0965, MinusLogProbMetric: 28.0965, val_loss: 28.8092, val_MinusLogProbMetric: 28.8092

Epoch 376: val_loss did not improve from 28.63290
196/196 - 43s - loss: 28.0965 - MinusLogProbMetric: 28.0965 - val_loss: 28.8092 - val_MinusLogProbMetric: 28.8092 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 377/1000
2023-09-28 21:00:38.896 
Epoch 377/1000 
	 loss: 28.0100, MinusLogProbMetric: 28.0100, val_loss: 28.8720, val_MinusLogProbMetric: 28.8720

Epoch 377: val_loss did not improve from 28.63290
196/196 - 42s - loss: 28.0100 - MinusLogProbMetric: 28.0100 - val_loss: 28.8720 - val_MinusLogProbMetric: 28.8720 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 378/1000
2023-09-28 21:01:17.951 
Epoch 378/1000 
	 loss: 27.9963, MinusLogProbMetric: 27.9963, val_loss: 28.7900, val_MinusLogProbMetric: 28.7900

Epoch 378: val_loss did not improve from 28.63290
196/196 - 39s - loss: 27.9963 - MinusLogProbMetric: 27.9963 - val_loss: 28.7900 - val_MinusLogProbMetric: 28.7900 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 379/1000
2023-09-28 21:01:55.855 
Epoch 379/1000 
	 loss: 28.0085, MinusLogProbMetric: 28.0085, val_loss: 28.9970, val_MinusLogProbMetric: 28.9970

Epoch 379: val_loss did not improve from 28.63290
196/196 - 38s - loss: 28.0085 - MinusLogProbMetric: 28.0085 - val_loss: 28.9970 - val_MinusLogProbMetric: 28.9970 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 380/1000
2023-09-28 21:02:36.068 
Epoch 380/1000 
	 loss: 28.1054, MinusLogProbMetric: 28.1054, val_loss: 28.7971, val_MinusLogProbMetric: 28.7971

Epoch 380: val_loss did not improve from 28.63290
196/196 - 40s - loss: 28.1054 - MinusLogProbMetric: 28.1054 - val_loss: 28.7971 - val_MinusLogProbMetric: 28.7971 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 381/1000
2023-09-28 21:03:14.539 
Epoch 381/1000 
	 loss: 28.1294, MinusLogProbMetric: 28.1294, val_loss: 28.7088, val_MinusLogProbMetric: 28.7088

Epoch 381: val_loss did not improve from 28.63290
196/196 - 38s - loss: 28.1294 - MinusLogProbMetric: 28.1294 - val_loss: 28.7088 - val_MinusLogProbMetric: 28.7088 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 382/1000
2023-09-28 21:03:50.405 
Epoch 382/1000 
	 loss: 28.0653, MinusLogProbMetric: 28.0653, val_loss: 28.9203, val_MinusLogProbMetric: 28.9203

Epoch 382: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0653 - MinusLogProbMetric: 28.0653 - val_loss: 28.9203 - val_MinusLogProbMetric: 28.9203 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 383/1000
2023-09-28 21:04:26.428 
Epoch 383/1000 
	 loss: 28.0637, MinusLogProbMetric: 28.0637, val_loss: 28.7962, val_MinusLogProbMetric: 28.7962

Epoch 383: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0637 - MinusLogProbMetric: 28.0637 - val_loss: 28.7962 - val_MinusLogProbMetric: 28.7962 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 384/1000
2023-09-28 21:05:05.678 
Epoch 384/1000 
	 loss: 28.1461, MinusLogProbMetric: 28.1461, val_loss: 29.0368, val_MinusLogProbMetric: 29.0368

Epoch 384: val_loss did not improve from 28.63290
196/196 - 39s - loss: 28.1461 - MinusLogProbMetric: 28.1461 - val_loss: 29.0368 - val_MinusLogProbMetric: 29.0368 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 385/1000
2023-09-28 21:05:45.666 
Epoch 385/1000 
	 loss: 28.0886, MinusLogProbMetric: 28.0886, val_loss: 28.6372, val_MinusLogProbMetric: 28.6372

Epoch 385: val_loss did not improve from 28.63290
196/196 - 40s - loss: 28.0886 - MinusLogProbMetric: 28.0886 - val_loss: 28.6372 - val_MinusLogProbMetric: 28.6372 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 386/1000
2023-09-28 21:06:23.533 
Epoch 386/1000 
	 loss: 28.1149, MinusLogProbMetric: 28.1149, val_loss: 28.9570, val_MinusLogProbMetric: 28.9570

Epoch 386: val_loss did not improve from 28.63290
196/196 - 38s - loss: 28.1149 - MinusLogProbMetric: 28.1149 - val_loss: 28.9570 - val_MinusLogProbMetric: 28.9570 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 387/1000
2023-09-28 21:06:59.880 
Epoch 387/1000 
	 loss: 28.0065, MinusLogProbMetric: 28.0065, val_loss: 29.1487, val_MinusLogProbMetric: 29.1487

Epoch 387: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0065 - MinusLogProbMetric: 28.0065 - val_loss: 29.1487 - val_MinusLogProbMetric: 29.1487 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 388/1000
2023-09-28 21:07:35.983 
Epoch 388/1000 
	 loss: 28.0172, MinusLogProbMetric: 28.0172, val_loss: 28.8727, val_MinusLogProbMetric: 28.8727

Epoch 388: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0172 - MinusLogProbMetric: 28.0172 - val_loss: 28.8727 - val_MinusLogProbMetric: 28.8727 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 389/1000
2023-09-28 21:08:14.351 
Epoch 389/1000 
	 loss: 28.0106, MinusLogProbMetric: 28.0106, val_loss: 28.6955, val_MinusLogProbMetric: 28.6955

Epoch 389: val_loss did not improve from 28.63290
196/196 - 38s - loss: 28.0106 - MinusLogProbMetric: 28.0106 - val_loss: 28.6955 - val_MinusLogProbMetric: 28.6955 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 390/1000
2023-09-28 21:08:54.914 
Epoch 390/1000 
	 loss: 28.0394, MinusLogProbMetric: 28.0394, val_loss: 28.7167, val_MinusLogProbMetric: 28.7167

Epoch 390: val_loss did not improve from 28.63290
196/196 - 41s - loss: 28.0394 - MinusLogProbMetric: 28.0394 - val_loss: 28.7167 - val_MinusLogProbMetric: 28.7167 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 391/1000
2023-09-28 21:09:32.552 
Epoch 391/1000 
	 loss: 27.9686, MinusLogProbMetric: 27.9686, val_loss: 29.2500, val_MinusLogProbMetric: 29.2500

Epoch 391: val_loss did not improve from 28.63290
196/196 - 38s - loss: 27.9686 - MinusLogProbMetric: 27.9686 - val_loss: 29.2500 - val_MinusLogProbMetric: 29.2500 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 392/1000
2023-09-28 21:10:09.322 
Epoch 392/1000 
	 loss: 28.0039, MinusLogProbMetric: 28.0039, val_loss: 28.7258, val_MinusLogProbMetric: 28.7258

Epoch 392: val_loss did not improve from 28.63290
196/196 - 37s - loss: 28.0039 - MinusLogProbMetric: 28.0039 - val_loss: 28.7258 - val_MinusLogProbMetric: 28.7258 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 393/1000
2023-09-28 21:10:45.448 
Epoch 393/1000 
	 loss: 28.0814, MinusLogProbMetric: 28.0814, val_loss: 28.7028, val_MinusLogProbMetric: 28.7028

Epoch 393: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0814 - MinusLogProbMetric: 28.0814 - val_loss: 28.7028 - val_MinusLogProbMetric: 28.7028 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 394/1000
2023-09-28 21:11:25.328 
Epoch 394/1000 
	 loss: 28.0993, MinusLogProbMetric: 28.0993, val_loss: 28.7715, val_MinusLogProbMetric: 28.7715

Epoch 394: val_loss did not improve from 28.63290
196/196 - 40s - loss: 28.0993 - MinusLogProbMetric: 28.0993 - val_loss: 28.7715 - val_MinusLogProbMetric: 28.7715 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 395/1000
2023-09-28 21:12:05.168 
Epoch 395/1000 
	 loss: 28.0399, MinusLogProbMetric: 28.0399, val_loss: 28.7628, val_MinusLogProbMetric: 28.7628

Epoch 395: val_loss did not improve from 28.63290
196/196 - 40s - loss: 28.0399 - MinusLogProbMetric: 28.0399 - val_loss: 28.7628 - val_MinusLogProbMetric: 28.7628 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 396/1000
2023-09-28 21:12:42.741 
Epoch 396/1000 
	 loss: 28.0823, MinusLogProbMetric: 28.0823, val_loss: 29.0020, val_MinusLogProbMetric: 29.0020

Epoch 396: val_loss did not improve from 28.63290
196/196 - 38s - loss: 28.0823 - MinusLogProbMetric: 28.0823 - val_loss: 29.0020 - val_MinusLogProbMetric: 29.0020 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 397/1000
2023-09-28 21:13:18.749 
Epoch 397/1000 
	 loss: 28.0152, MinusLogProbMetric: 28.0152, val_loss: 28.8735, val_MinusLogProbMetric: 28.8735

Epoch 397: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0152 - MinusLogProbMetric: 28.0152 - val_loss: 28.8735 - val_MinusLogProbMetric: 28.8735 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 398/1000
2023-09-28 21:13:55.175 
Epoch 398/1000 
	 loss: 28.0584, MinusLogProbMetric: 28.0584, val_loss: 28.9961, val_MinusLogProbMetric: 28.9961

Epoch 398: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0584 - MinusLogProbMetric: 28.0584 - val_loss: 28.9961 - val_MinusLogProbMetric: 28.9961 - lr: 5.0000e-04 - 36s/epoch - 186ms/step
Epoch 399/1000
2023-09-28 21:14:33.390 
Epoch 399/1000 
	 loss: 28.0103, MinusLogProbMetric: 28.0103, val_loss: 28.9412, val_MinusLogProbMetric: 28.9412

Epoch 399: val_loss did not improve from 28.63290
196/196 - 38s - loss: 28.0103 - MinusLogProbMetric: 28.0103 - val_loss: 28.9412 - val_MinusLogProbMetric: 28.9412 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 400/1000
2023-09-28 21:15:13.119 
Epoch 400/1000 
	 loss: 28.0059, MinusLogProbMetric: 28.0059, val_loss: 28.8646, val_MinusLogProbMetric: 28.8646

Epoch 400: val_loss did not improve from 28.63290
196/196 - 40s - loss: 28.0059 - MinusLogProbMetric: 28.0059 - val_loss: 28.8646 - val_MinusLogProbMetric: 28.8646 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 401/1000
2023-09-28 21:15:50.572 
Epoch 401/1000 
	 loss: 28.2040, MinusLogProbMetric: 28.2040, val_loss: 29.0237, val_MinusLogProbMetric: 29.0237

Epoch 401: val_loss did not improve from 28.63290
196/196 - 37s - loss: 28.2040 - MinusLogProbMetric: 28.2040 - val_loss: 29.0237 - val_MinusLogProbMetric: 29.0237 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 402/1000
2023-09-28 21:16:27.401 
Epoch 402/1000 
	 loss: 27.9812, MinusLogProbMetric: 27.9812, val_loss: 29.0587, val_MinusLogProbMetric: 29.0587

Epoch 402: val_loss did not improve from 28.63290
196/196 - 37s - loss: 27.9812 - MinusLogProbMetric: 27.9812 - val_loss: 29.0587 - val_MinusLogProbMetric: 29.0587 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 403/1000
2023-09-28 21:17:03.605 
Epoch 403/1000 
	 loss: 28.0528, MinusLogProbMetric: 28.0528, val_loss: 28.7455, val_MinusLogProbMetric: 28.7455

Epoch 403: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.0528 - MinusLogProbMetric: 28.0528 - val_loss: 28.7455 - val_MinusLogProbMetric: 28.7455 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 404/1000
2023-09-28 21:17:43.843 
Epoch 404/1000 
	 loss: 27.9136, MinusLogProbMetric: 27.9136, val_loss: 28.7301, val_MinusLogProbMetric: 28.7301

Epoch 404: val_loss did not improve from 28.63290
196/196 - 40s - loss: 27.9136 - MinusLogProbMetric: 27.9136 - val_loss: 28.7301 - val_MinusLogProbMetric: 28.7301 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 405/1000
2023-09-28 21:18:23.059 
Epoch 405/1000 
	 loss: 27.9861, MinusLogProbMetric: 27.9861, val_loss: 28.8974, val_MinusLogProbMetric: 28.8974

Epoch 405: val_loss did not improve from 28.63290
196/196 - 39s - loss: 27.9861 - MinusLogProbMetric: 27.9861 - val_loss: 28.8974 - val_MinusLogProbMetric: 28.8974 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 406/1000
2023-09-28 21:18:58.895 
Epoch 406/1000 
	 loss: 27.9697, MinusLogProbMetric: 27.9697, val_loss: 28.7937, val_MinusLogProbMetric: 28.7937

Epoch 406: val_loss did not improve from 28.63290
196/196 - 36s - loss: 27.9697 - MinusLogProbMetric: 27.9697 - val_loss: 28.7937 - val_MinusLogProbMetric: 28.7937 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 407/1000
2023-09-28 21:19:34.683 
Epoch 407/1000 
	 loss: 27.9908, MinusLogProbMetric: 27.9908, val_loss: 28.7002, val_MinusLogProbMetric: 28.7002

Epoch 407: val_loss did not improve from 28.63290
196/196 - 36s - loss: 27.9908 - MinusLogProbMetric: 27.9908 - val_loss: 28.7002 - val_MinusLogProbMetric: 28.7002 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 408/1000
2023-09-28 21:20:10.584 
Epoch 408/1000 
	 loss: 28.2583, MinusLogProbMetric: 28.2583, val_loss: 28.8742, val_MinusLogProbMetric: 28.8742

Epoch 408: val_loss did not improve from 28.63290
196/196 - 36s - loss: 28.2583 - MinusLogProbMetric: 28.2583 - val_loss: 28.8742 - val_MinusLogProbMetric: 28.8742 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 409/1000
2023-09-28 21:20:49.094 
Epoch 409/1000 
	 loss: 28.0762, MinusLogProbMetric: 28.0762, val_loss: 28.8239, val_MinusLogProbMetric: 28.8239

Epoch 409: val_loss did not improve from 28.63290
196/196 - 39s - loss: 28.0762 - MinusLogProbMetric: 28.0762 - val_loss: 28.8239 - val_MinusLogProbMetric: 28.8239 - lr: 5.0000e-04 - 39s/epoch - 196ms/step
Epoch 410/1000
2023-09-28 21:21:29.400 
Epoch 410/1000 
	 loss: 27.7180, MinusLogProbMetric: 27.7180, val_loss: 28.6043, val_MinusLogProbMetric: 28.6043

Epoch 410: val_loss improved from 28.63290 to 28.60433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 41s - loss: 27.7180 - MinusLogProbMetric: 27.7180 - val_loss: 28.6043 - val_MinusLogProbMetric: 28.6043 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 411/1000
2023-09-28 21:22:06.897 
Epoch 411/1000 
	 loss: 27.7116, MinusLogProbMetric: 27.7116, val_loss: 28.5237, val_MinusLogProbMetric: 28.5237

Epoch 411: val_loss improved from 28.60433 to 28.52371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 37s - loss: 27.7116 - MinusLogProbMetric: 27.7116 - val_loss: 28.5237 - val_MinusLogProbMetric: 28.5237 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 412/1000
2023-09-28 21:22:43.288 
Epoch 412/1000 
	 loss: 27.7129, MinusLogProbMetric: 27.7129, val_loss: 28.6623, val_MinusLogProbMetric: 28.6623

Epoch 412: val_loss did not improve from 28.52371
196/196 - 36s - loss: 27.7129 - MinusLogProbMetric: 27.7129 - val_loss: 28.6623 - val_MinusLogProbMetric: 28.6623 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 413/1000
2023-09-28 21:23:19.240 
Epoch 413/1000 
	 loss: 27.7089, MinusLogProbMetric: 27.7089, val_loss: 28.6326, val_MinusLogProbMetric: 28.6326

Epoch 413: val_loss did not improve from 28.52371
196/196 - 36s - loss: 27.7089 - MinusLogProbMetric: 27.7089 - val_loss: 28.6326 - val_MinusLogProbMetric: 28.6326 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 414/1000
2023-09-28 21:23:57.046 
Epoch 414/1000 
	 loss: 27.6858, MinusLogProbMetric: 27.6858, val_loss: 28.6476, val_MinusLogProbMetric: 28.6476

Epoch 414: val_loss did not improve from 28.52371
196/196 - 38s - loss: 27.6858 - MinusLogProbMetric: 27.6858 - val_loss: 28.6476 - val_MinusLogProbMetric: 28.6476 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 415/1000
2023-09-28 21:24:36.954 
Epoch 415/1000 
	 loss: 27.7273, MinusLogProbMetric: 27.7273, val_loss: 28.5349, val_MinusLogProbMetric: 28.5349

Epoch 415: val_loss did not improve from 28.52371
196/196 - 40s - loss: 27.7273 - MinusLogProbMetric: 27.7273 - val_loss: 28.5349 - val_MinusLogProbMetric: 28.5349 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 416/1000
2023-09-28 21:25:13.015 
Epoch 416/1000 
	 loss: 27.7064, MinusLogProbMetric: 27.7064, val_loss: 28.5076, val_MinusLogProbMetric: 28.5076

Epoch 416: val_loss improved from 28.52371 to 28.50756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 37s - loss: 27.7064 - MinusLogProbMetric: 27.7064 - val_loss: 28.5076 - val_MinusLogProbMetric: 28.5076 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 417/1000
2023-09-28 21:25:49.778 
Epoch 417/1000 
	 loss: 27.7255, MinusLogProbMetric: 27.7255, val_loss: 28.5635, val_MinusLogProbMetric: 28.5635

Epoch 417: val_loss did not improve from 28.50756
196/196 - 36s - loss: 27.7255 - MinusLogProbMetric: 27.7255 - val_loss: 28.5635 - val_MinusLogProbMetric: 28.5635 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 418/1000
2023-09-28 21:26:25.757 
Epoch 418/1000 
	 loss: 27.7033, MinusLogProbMetric: 27.7033, val_loss: 28.5205, val_MinusLogProbMetric: 28.5205

Epoch 418: val_loss did not improve from 28.50756
196/196 - 36s - loss: 27.7033 - MinusLogProbMetric: 27.7033 - val_loss: 28.5205 - val_MinusLogProbMetric: 28.5205 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 419/1000
2023-09-28 21:27:06.239 
Epoch 419/1000 
	 loss: 27.6993, MinusLogProbMetric: 27.6993, val_loss: 28.5755, val_MinusLogProbMetric: 28.5755

Epoch 419: val_loss did not improve from 28.50756
196/196 - 40s - loss: 27.6993 - MinusLogProbMetric: 27.6993 - val_loss: 28.5755 - val_MinusLogProbMetric: 28.5755 - lr: 2.5000e-04 - 40s/epoch - 207ms/step
Epoch 420/1000
2023-09-28 21:27:46.113 
Epoch 420/1000 
	 loss: 27.7026, MinusLogProbMetric: 27.7026, val_loss: 28.5179, val_MinusLogProbMetric: 28.5179

Epoch 420: val_loss did not improve from 28.50756
196/196 - 40s - loss: 27.7026 - MinusLogProbMetric: 27.7026 - val_loss: 28.5179 - val_MinusLogProbMetric: 28.5179 - lr: 2.5000e-04 - 40s/epoch - 203ms/step
Epoch 421/1000
2023-09-28 21:28:22.674 
Epoch 421/1000 
	 loss: 27.6819, MinusLogProbMetric: 27.6819, val_loss: 28.6040, val_MinusLogProbMetric: 28.6040

Epoch 421: val_loss did not improve from 28.50756
196/196 - 37s - loss: 27.6819 - MinusLogProbMetric: 27.6819 - val_loss: 28.6040 - val_MinusLogProbMetric: 28.6040 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 422/1000
2023-09-28 21:28:58.737 
Epoch 422/1000 
	 loss: 27.7035, MinusLogProbMetric: 27.7035, val_loss: 28.5258, val_MinusLogProbMetric: 28.5258

Epoch 422: val_loss did not improve from 28.50756
196/196 - 36s - loss: 27.7035 - MinusLogProbMetric: 27.7035 - val_loss: 28.5258 - val_MinusLogProbMetric: 28.5258 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 423/1000
2023-09-28 21:29:34.501 
Epoch 423/1000 
	 loss: 27.6839, MinusLogProbMetric: 27.6839, val_loss: 28.5787, val_MinusLogProbMetric: 28.5787

Epoch 423: val_loss did not improve from 28.50756
196/196 - 36s - loss: 27.6839 - MinusLogProbMetric: 27.6839 - val_loss: 28.5787 - val_MinusLogProbMetric: 28.5787 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 424/1000
2023-09-28 21:30:09.801 
Epoch 424/1000 
	 loss: 27.6976, MinusLogProbMetric: 27.6976, val_loss: 28.6052, val_MinusLogProbMetric: 28.6052

Epoch 424: val_loss did not improve from 28.50756
196/196 - 35s - loss: 27.6976 - MinusLogProbMetric: 27.6976 - val_loss: 28.6052 - val_MinusLogProbMetric: 28.6052 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 425/1000
2023-09-28 21:30:47.356 
Epoch 425/1000 
	 loss: 27.6943, MinusLogProbMetric: 27.6943, val_loss: 28.5004, val_MinusLogProbMetric: 28.5004

Epoch 425: val_loss improved from 28.50756 to 28.50037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 38s - loss: 27.6943 - MinusLogProbMetric: 27.6943 - val_loss: 28.5004 - val_MinusLogProbMetric: 28.5004 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 426/1000
2023-09-28 21:31:22.881 
Epoch 426/1000 
	 loss: 27.7010, MinusLogProbMetric: 27.7010, val_loss: 28.5297, val_MinusLogProbMetric: 28.5297

Epoch 426: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.7010 - MinusLogProbMetric: 27.7010 - val_loss: 28.5297 - val_MinusLogProbMetric: 28.5297 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 427/1000
2023-09-28 21:31:57.576 
Epoch 427/1000 
	 loss: 27.6928, MinusLogProbMetric: 27.6928, val_loss: 28.8476, val_MinusLogProbMetric: 28.8476

Epoch 427: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.6928 - MinusLogProbMetric: 27.6928 - val_loss: 28.8476 - val_MinusLogProbMetric: 28.8476 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 428/1000
2023-09-28 21:32:32.955 
Epoch 428/1000 
	 loss: 27.6930, MinusLogProbMetric: 27.6930, val_loss: 28.6432, val_MinusLogProbMetric: 28.6432

Epoch 428: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.6930 - MinusLogProbMetric: 27.6930 - val_loss: 28.6432 - val_MinusLogProbMetric: 28.6432 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 429/1000
2023-09-28 21:33:07.502 
Epoch 429/1000 
	 loss: 27.6926, MinusLogProbMetric: 27.6926, val_loss: 28.5123, val_MinusLogProbMetric: 28.5123

Epoch 429: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.6926 - MinusLogProbMetric: 27.6926 - val_loss: 28.5123 - val_MinusLogProbMetric: 28.5123 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 430/1000
2023-09-28 21:33:41.830 
Epoch 430/1000 
	 loss: 27.6757, MinusLogProbMetric: 27.6757, val_loss: 28.6459, val_MinusLogProbMetric: 28.6459

Epoch 430: val_loss did not improve from 28.50037
196/196 - 34s - loss: 27.6757 - MinusLogProbMetric: 27.6757 - val_loss: 28.6459 - val_MinusLogProbMetric: 28.6459 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 431/1000
2023-09-28 21:34:17.324 
Epoch 431/1000 
	 loss: 27.7045, MinusLogProbMetric: 27.7045, val_loss: 28.5778, val_MinusLogProbMetric: 28.5778

Epoch 431: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.7045 - MinusLogProbMetric: 27.7045 - val_loss: 28.5778 - val_MinusLogProbMetric: 28.5778 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 432/1000
2023-09-28 21:34:51.976 
Epoch 432/1000 
	 loss: 27.6862, MinusLogProbMetric: 27.6862, val_loss: 28.6857, val_MinusLogProbMetric: 28.6857

Epoch 432: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.6862 - MinusLogProbMetric: 27.6862 - val_loss: 28.6857 - val_MinusLogProbMetric: 28.6857 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 433/1000
2023-09-28 21:35:26.530 
Epoch 433/1000 
	 loss: 27.7069, MinusLogProbMetric: 27.7069, val_loss: 28.6591, val_MinusLogProbMetric: 28.6591

Epoch 433: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.7069 - MinusLogProbMetric: 27.7069 - val_loss: 28.6591 - val_MinusLogProbMetric: 28.6591 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 434/1000
2023-09-28 21:36:01.954 
Epoch 434/1000 
	 loss: 27.7374, MinusLogProbMetric: 27.7374, val_loss: 28.5339, val_MinusLogProbMetric: 28.5339

Epoch 434: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.7374 - MinusLogProbMetric: 27.7374 - val_loss: 28.5339 - val_MinusLogProbMetric: 28.5339 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 435/1000
2023-09-28 21:36:36.168 
Epoch 435/1000 
	 loss: 27.6696, MinusLogProbMetric: 27.6696, val_loss: 28.5312, val_MinusLogProbMetric: 28.5312

Epoch 435: val_loss did not improve from 28.50037
196/196 - 34s - loss: 27.6696 - MinusLogProbMetric: 27.6696 - val_loss: 28.5312 - val_MinusLogProbMetric: 28.5312 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 436/1000
2023-09-28 21:37:12.408 
Epoch 436/1000 
	 loss: 27.7260, MinusLogProbMetric: 27.7260, val_loss: 28.5570, val_MinusLogProbMetric: 28.5570

Epoch 436: val_loss did not improve from 28.50037
196/196 - 36s - loss: 27.7260 - MinusLogProbMetric: 27.7260 - val_loss: 28.5570 - val_MinusLogProbMetric: 28.5570 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 437/1000
2023-09-28 21:37:47.188 
Epoch 437/1000 
	 loss: 27.6827, MinusLogProbMetric: 27.6827, val_loss: 28.5180, val_MinusLogProbMetric: 28.5180

Epoch 437: val_loss did not improve from 28.50037
196/196 - 35s - loss: 27.6827 - MinusLogProbMetric: 27.6827 - val_loss: 28.5180 - val_MinusLogProbMetric: 28.5180 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 438/1000
2023-09-28 21:38:22.024 
Epoch 438/1000 
	 loss: 27.7201, MinusLogProbMetric: 27.7201, val_loss: 28.4928, val_MinusLogProbMetric: 28.4928

Epoch 438: val_loss improved from 28.50037 to 28.49279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 35s - loss: 27.7201 - MinusLogProbMetric: 27.7201 - val_loss: 28.4928 - val_MinusLogProbMetric: 28.4928 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 439/1000
2023-09-28 21:38:57.605 
Epoch 439/1000 
	 loss: 27.7087, MinusLogProbMetric: 27.7087, val_loss: 28.5077, val_MinusLogProbMetric: 28.5077

Epoch 439: val_loss did not improve from 28.49279
196/196 - 35s - loss: 27.7087 - MinusLogProbMetric: 27.7087 - val_loss: 28.5077 - val_MinusLogProbMetric: 28.5077 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 440/1000
2023-09-28 21:39:34.033 
Epoch 440/1000 
	 loss: 27.6864, MinusLogProbMetric: 27.6864, val_loss: 28.4592, val_MinusLogProbMetric: 28.4592

Epoch 440: val_loss improved from 28.49279 to 28.45922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 37s - loss: 27.6864 - MinusLogProbMetric: 27.6864 - val_loss: 28.4592 - val_MinusLogProbMetric: 28.4592 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 441/1000
2023-09-28 21:40:09.494 
Epoch 441/1000 
	 loss: 27.6591, MinusLogProbMetric: 27.6591, val_loss: 28.6407, val_MinusLogProbMetric: 28.6407

Epoch 441: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.6591 - MinusLogProbMetric: 27.6591 - val_loss: 28.6407 - val_MinusLogProbMetric: 28.6407 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 442/1000
2023-09-28 21:40:46.123 
Epoch 442/1000 
	 loss: 27.6898, MinusLogProbMetric: 27.6898, val_loss: 28.6657, val_MinusLogProbMetric: 28.6657

Epoch 442: val_loss did not improve from 28.45922
196/196 - 37s - loss: 27.6898 - MinusLogProbMetric: 27.6898 - val_loss: 28.6657 - val_MinusLogProbMetric: 28.6657 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 443/1000
2023-09-28 21:41:22.236 
Epoch 443/1000 
	 loss: 27.7203, MinusLogProbMetric: 27.7203, val_loss: 28.7885, val_MinusLogProbMetric: 28.7885

Epoch 443: val_loss did not improve from 28.45922
196/196 - 36s - loss: 27.7203 - MinusLogProbMetric: 27.7203 - val_loss: 28.7885 - val_MinusLogProbMetric: 28.7885 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 444/1000
2023-09-28 21:41:56.835 
Epoch 444/1000 
	 loss: 27.7063, MinusLogProbMetric: 27.7063, val_loss: 28.5010, val_MinusLogProbMetric: 28.5010

Epoch 444: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.7063 - MinusLogProbMetric: 27.7063 - val_loss: 28.5010 - val_MinusLogProbMetric: 28.5010 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 445/1000
2023-09-28 21:42:30.633 
Epoch 445/1000 
	 loss: 27.6824, MinusLogProbMetric: 27.6824, val_loss: 28.5767, val_MinusLogProbMetric: 28.5767

Epoch 445: val_loss did not improve from 28.45922
196/196 - 34s - loss: 27.6824 - MinusLogProbMetric: 27.6824 - val_loss: 28.5767 - val_MinusLogProbMetric: 28.5767 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 446/1000
2023-09-28 21:43:05.533 
Epoch 446/1000 
	 loss: 27.6862, MinusLogProbMetric: 27.6862, val_loss: 28.5945, val_MinusLogProbMetric: 28.5945

Epoch 446: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.6862 - MinusLogProbMetric: 27.6862 - val_loss: 28.5945 - val_MinusLogProbMetric: 28.5945 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 447/1000
2023-09-28 21:43:40.474 
Epoch 447/1000 
	 loss: 27.6796, MinusLogProbMetric: 27.6796, val_loss: 28.5129, val_MinusLogProbMetric: 28.5129

Epoch 447: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.6796 - MinusLogProbMetric: 27.6796 - val_loss: 28.5129 - val_MinusLogProbMetric: 28.5129 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 448/1000
2023-09-28 21:44:15.255 
Epoch 448/1000 
	 loss: 27.7342, MinusLogProbMetric: 27.7342, val_loss: 28.5771, val_MinusLogProbMetric: 28.5771

Epoch 448: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.7342 - MinusLogProbMetric: 27.7342 - val_loss: 28.5771 - val_MinusLogProbMetric: 28.5771 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 449/1000
2023-09-28 21:44:50.518 
Epoch 449/1000 
	 loss: 27.7237, MinusLogProbMetric: 27.7237, val_loss: 28.4993, val_MinusLogProbMetric: 28.4993

Epoch 449: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.7237 - MinusLogProbMetric: 27.7237 - val_loss: 28.4993 - val_MinusLogProbMetric: 28.4993 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 450/1000
2023-09-28 21:45:24.916 
Epoch 450/1000 
	 loss: 27.7090, MinusLogProbMetric: 27.7090, val_loss: 28.5154, val_MinusLogProbMetric: 28.5154

Epoch 450: val_loss did not improve from 28.45922
196/196 - 34s - loss: 27.7090 - MinusLogProbMetric: 27.7090 - val_loss: 28.5154 - val_MinusLogProbMetric: 28.5154 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 451/1000
2023-09-28 21:46:02.348 
Epoch 451/1000 
	 loss: 27.7072, MinusLogProbMetric: 27.7072, val_loss: 28.6516, val_MinusLogProbMetric: 28.6516

Epoch 451: val_loss did not improve from 28.45922
196/196 - 37s - loss: 27.7072 - MinusLogProbMetric: 27.7072 - val_loss: 28.6516 - val_MinusLogProbMetric: 28.6516 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 452/1000
2023-09-28 21:46:42.820 
Epoch 452/1000 
	 loss: 27.7345, MinusLogProbMetric: 27.7345, val_loss: 28.7598, val_MinusLogProbMetric: 28.7598

Epoch 452: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.7345 - MinusLogProbMetric: 27.7345 - val_loss: 28.7598 - val_MinusLogProbMetric: 28.7598 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 453/1000
2023-09-28 21:47:19.257 
Epoch 453/1000 
	 loss: 27.7463, MinusLogProbMetric: 27.7463, val_loss: 28.8291, val_MinusLogProbMetric: 28.8291

Epoch 453: val_loss did not improve from 28.45922
196/196 - 36s - loss: 27.7463 - MinusLogProbMetric: 27.7463 - val_loss: 28.8291 - val_MinusLogProbMetric: 28.8291 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 454/1000
2023-09-28 21:47:54.519 
Epoch 454/1000 
	 loss: 27.7388, MinusLogProbMetric: 27.7388, val_loss: 28.5273, val_MinusLogProbMetric: 28.5273

Epoch 454: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.7388 - MinusLogProbMetric: 27.7388 - val_loss: 28.5273 - val_MinusLogProbMetric: 28.5273 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 455/1000
2023-09-28 21:48:29.106 
Epoch 455/1000 
	 loss: 27.6794, MinusLogProbMetric: 27.6794, val_loss: 28.6479, val_MinusLogProbMetric: 28.6479

Epoch 455: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.6794 - MinusLogProbMetric: 27.6794 - val_loss: 28.6479 - val_MinusLogProbMetric: 28.6479 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 456/1000
2023-09-28 21:49:04.943 
Epoch 456/1000 
	 loss: 27.6830, MinusLogProbMetric: 27.6830, val_loss: 28.5213, val_MinusLogProbMetric: 28.5213

Epoch 456: val_loss did not improve from 28.45922
196/196 - 36s - loss: 27.6830 - MinusLogProbMetric: 27.6830 - val_loss: 28.5213 - val_MinusLogProbMetric: 28.5213 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 457/1000
2023-09-28 21:49:43.023 
Epoch 457/1000 
	 loss: 27.6704, MinusLogProbMetric: 27.6704, val_loss: 28.5009, val_MinusLogProbMetric: 28.5009

Epoch 457: val_loss did not improve from 28.45922
196/196 - 38s - loss: 27.6704 - MinusLogProbMetric: 27.6704 - val_loss: 28.5009 - val_MinusLogProbMetric: 28.5009 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 458/1000
2023-09-28 21:50:22.208 
Epoch 458/1000 
	 loss: 27.7056, MinusLogProbMetric: 27.7056, val_loss: 28.5350, val_MinusLogProbMetric: 28.5350

Epoch 458: val_loss did not improve from 28.45922
196/196 - 39s - loss: 27.7056 - MinusLogProbMetric: 27.7056 - val_loss: 28.5350 - val_MinusLogProbMetric: 28.5350 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 459/1000
2023-09-28 21:51:00.367 
Epoch 459/1000 
	 loss: 27.7185, MinusLogProbMetric: 27.7185, val_loss: 28.8069, val_MinusLogProbMetric: 28.8069

Epoch 459: val_loss did not improve from 28.45922
196/196 - 38s - loss: 27.7185 - MinusLogProbMetric: 27.7185 - val_loss: 28.8069 - val_MinusLogProbMetric: 28.8069 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 460/1000
2023-09-28 21:51:38.657 
Epoch 460/1000 
	 loss: 27.6949, MinusLogProbMetric: 27.6949, val_loss: 28.5428, val_MinusLogProbMetric: 28.5428

Epoch 460: val_loss did not improve from 28.45922
196/196 - 38s - loss: 27.6949 - MinusLogProbMetric: 27.6949 - val_loss: 28.5428 - val_MinusLogProbMetric: 28.5428 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 461/1000
2023-09-28 21:52:19.472 
Epoch 461/1000 
	 loss: 27.6657, MinusLogProbMetric: 27.6657, val_loss: 28.6010, val_MinusLogProbMetric: 28.6010

Epoch 461: val_loss did not improve from 28.45922
196/196 - 41s - loss: 27.6657 - MinusLogProbMetric: 27.6657 - val_loss: 28.6010 - val_MinusLogProbMetric: 28.6010 - lr: 2.5000e-04 - 41s/epoch - 208ms/step
Epoch 462/1000
2023-09-28 21:52:55.214 
Epoch 462/1000 
	 loss: 27.7094, MinusLogProbMetric: 27.7094, val_loss: 28.5427, val_MinusLogProbMetric: 28.5427

Epoch 462: val_loss did not improve from 28.45922
196/196 - 36s - loss: 27.7094 - MinusLogProbMetric: 27.7094 - val_loss: 28.5427 - val_MinusLogProbMetric: 28.5427 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 463/1000
2023-09-28 21:53:29.921 
Epoch 463/1000 
	 loss: 27.7306, MinusLogProbMetric: 27.7306, val_loss: 28.5431, val_MinusLogProbMetric: 28.5431

Epoch 463: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.7306 - MinusLogProbMetric: 27.7306 - val_loss: 28.5431 - val_MinusLogProbMetric: 28.5431 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 464/1000
2023-09-28 21:54:04.783 
Epoch 464/1000 
	 loss: 27.6747, MinusLogProbMetric: 27.6747, val_loss: 28.5666, val_MinusLogProbMetric: 28.5666

Epoch 464: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.6747 - MinusLogProbMetric: 27.6747 - val_loss: 28.5666 - val_MinusLogProbMetric: 28.5666 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 465/1000
2023-09-28 21:54:39.509 
Epoch 465/1000 
	 loss: 27.6806, MinusLogProbMetric: 27.6806, val_loss: 28.4981, val_MinusLogProbMetric: 28.4981

Epoch 465: val_loss did not improve from 28.45922
196/196 - 35s - loss: 27.6806 - MinusLogProbMetric: 27.6806 - val_loss: 28.4981 - val_MinusLogProbMetric: 28.4981 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 466/1000
2023-09-28 21:55:16.204 
Epoch 466/1000 
	 loss: 27.6822, MinusLogProbMetric: 27.6822, val_loss: 28.5375, val_MinusLogProbMetric: 28.5375

Epoch 466: val_loss did not improve from 28.45922
196/196 - 37s - loss: 27.6822 - MinusLogProbMetric: 27.6822 - val_loss: 28.5375 - val_MinusLogProbMetric: 28.5375 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 467/1000
2023-09-28 21:55:56.635 
Epoch 467/1000 
	 loss: 27.6842, MinusLogProbMetric: 27.6842, val_loss: 28.5357, val_MinusLogProbMetric: 28.5357

Epoch 467: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.6842 - MinusLogProbMetric: 27.6842 - val_loss: 28.5357 - val_MinusLogProbMetric: 28.5357 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 468/1000
2023-09-28 21:56:34.619 
Epoch 468/1000 
	 loss: 27.7018, MinusLogProbMetric: 27.7018, val_loss: 28.7036, val_MinusLogProbMetric: 28.7036

Epoch 468: val_loss did not improve from 28.45922
196/196 - 38s - loss: 27.7018 - MinusLogProbMetric: 27.7018 - val_loss: 28.7036 - val_MinusLogProbMetric: 28.7036 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 469/1000
2023-09-28 21:57:13.200 
Epoch 469/1000 
	 loss: 27.7027, MinusLogProbMetric: 27.7027, val_loss: 28.5165, val_MinusLogProbMetric: 28.5165

Epoch 469: val_loss did not improve from 28.45922
196/196 - 39s - loss: 27.7027 - MinusLogProbMetric: 27.7027 - val_loss: 28.5165 - val_MinusLogProbMetric: 28.5165 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 470/1000
2023-09-28 21:57:53.525 
Epoch 470/1000 
	 loss: 27.6786, MinusLogProbMetric: 27.6786, val_loss: 28.5690, val_MinusLogProbMetric: 28.5690

Epoch 470: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.6786 - MinusLogProbMetric: 27.6786 - val_loss: 28.5690 - val_MinusLogProbMetric: 28.5690 - lr: 2.5000e-04 - 40s/epoch - 206ms/step
Epoch 471/1000
2023-09-28 21:58:35.851 
Epoch 471/1000 
	 loss: 27.6758, MinusLogProbMetric: 27.6758, val_loss: 28.6211, val_MinusLogProbMetric: 28.6211

Epoch 471: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.6758 - MinusLogProbMetric: 27.6758 - val_loss: 28.6211 - val_MinusLogProbMetric: 28.6211 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 472/1000
2023-09-28 21:59:12.661 
Epoch 472/1000 
	 loss: 27.7246, MinusLogProbMetric: 27.7246, val_loss: 28.5807, val_MinusLogProbMetric: 28.5807

Epoch 472: val_loss did not improve from 28.45922
196/196 - 37s - loss: 27.7246 - MinusLogProbMetric: 27.7246 - val_loss: 28.5807 - val_MinusLogProbMetric: 28.5807 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 473/1000
2023-09-28 21:59:52.913 
Epoch 473/1000 
	 loss: 27.6830, MinusLogProbMetric: 27.6830, val_loss: 28.6199, val_MinusLogProbMetric: 28.6199

Epoch 473: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.6830 - MinusLogProbMetric: 27.6830 - val_loss: 28.6199 - val_MinusLogProbMetric: 28.6199 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 474/1000
2023-09-28 22:00:31.730 
Epoch 474/1000 
	 loss: 27.7013, MinusLogProbMetric: 27.7013, val_loss: 28.5221, val_MinusLogProbMetric: 28.5221

Epoch 474: val_loss did not improve from 28.45922
196/196 - 39s - loss: 27.7013 - MinusLogProbMetric: 27.7013 - val_loss: 28.5221 - val_MinusLogProbMetric: 28.5221 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 475/1000
2023-09-28 22:01:11.248 
Epoch 475/1000 
	 loss: 27.6735, MinusLogProbMetric: 27.6735, val_loss: 28.5367, val_MinusLogProbMetric: 28.5367

Epoch 475: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.6735 - MinusLogProbMetric: 27.6735 - val_loss: 28.5367 - val_MinusLogProbMetric: 28.5367 - lr: 2.5000e-04 - 40s/epoch - 202ms/step
Epoch 476/1000
2023-09-28 22:01:49.537 
Epoch 476/1000 
	 loss: 27.6789, MinusLogProbMetric: 27.6789, val_loss: 28.7832, val_MinusLogProbMetric: 28.7832

Epoch 476: val_loss did not improve from 28.45922
196/196 - 38s - loss: 27.6789 - MinusLogProbMetric: 27.6789 - val_loss: 28.7832 - val_MinusLogProbMetric: 28.7832 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 477/1000
2023-09-28 22:02:29.775 
Epoch 477/1000 
	 loss: 27.7113, MinusLogProbMetric: 27.7113, val_loss: 28.4914, val_MinusLogProbMetric: 28.4914

Epoch 477: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.7113 - MinusLogProbMetric: 27.7113 - val_loss: 28.4914 - val_MinusLogProbMetric: 28.4914 - lr: 2.5000e-04 - 40s/epoch - 205ms/step
Epoch 478/1000
2023-09-28 22:03:11.285 
Epoch 478/1000 
	 loss: 27.6857, MinusLogProbMetric: 27.6857, val_loss: 28.6075, val_MinusLogProbMetric: 28.6075

Epoch 478: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.6857 - MinusLogProbMetric: 27.6857 - val_loss: 28.6075 - val_MinusLogProbMetric: 28.6075 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 479/1000
2023-09-28 22:03:51.220 
Epoch 479/1000 
	 loss: 27.7105, MinusLogProbMetric: 27.7105, val_loss: 28.5449, val_MinusLogProbMetric: 28.5449

Epoch 479: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.7105 - MinusLogProbMetric: 27.7105 - val_loss: 28.5449 - val_MinusLogProbMetric: 28.5449 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 480/1000
2023-09-28 22:04:32.718 
Epoch 480/1000 
	 loss: 27.6916, MinusLogProbMetric: 27.6916, val_loss: 28.7651, val_MinusLogProbMetric: 28.7651

Epoch 480: val_loss did not improve from 28.45922
196/196 - 41s - loss: 27.6916 - MinusLogProbMetric: 27.6916 - val_loss: 28.7651 - val_MinusLogProbMetric: 28.7651 - lr: 2.5000e-04 - 41s/epoch - 212ms/step
Epoch 481/1000
2023-09-28 22:05:14.955 
Epoch 481/1000 
	 loss: 27.7075, MinusLogProbMetric: 27.7075, val_loss: 28.8575, val_MinusLogProbMetric: 28.8575

Epoch 481: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.7075 - MinusLogProbMetric: 27.7075 - val_loss: 28.8575 - val_MinusLogProbMetric: 28.8575 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 482/1000
2023-09-28 22:05:56.935 
Epoch 482/1000 
	 loss: 27.6868, MinusLogProbMetric: 27.6868, val_loss: 28.5766, val_MinusLogProbMetric: 28.5766

Epoch 482: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.6868 - MinusLogProbMetric: 27.6868 - val_loss: 28.5766 - val_MinusLogProbMetric: 28.5766 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 483/1000
2023-09-28 22:06:39.174 
Epoch 483/1000 
	 loss: 27.6676, MinusLogProbMetric: 27.6676, val_loss: 28.7953, val_MinusLogProbMetric: 28.7953

Epoch 483: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.6676 - MinusLogProbMetric: 27.6676 - val_loss: 28.7953 - val_MinusLogProbMetric: 28.7953 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 484/1000
2023-09-28 22:07:21.369 
Epoch 484/1000 
	 loss: 27.6921, MinusLogProbMetric: 27.6921, val_loss: 28.5911, val_MinusLogProbMetric: 28.5911

Epoch 484: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.6921 - MinusLogProbMetric: 27.6921 - val_loss: 28.5911 - val_MinusLogProbMetric: 28.5911 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 485/1000
2023-09-28 22:08:03.537 
Epoch 485/1000 
	 loss: 27.6501, MinusLogProbMetric: 27.6501, val_loss: 28.6827, val_MinusLogProbMetric: 28.6827

Epoch 485: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.6501 - MinusLogProbMetric: 27.6501 - val_loss: 28.6827 - val_MinusLogProbMetric: 28.6827 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 486/1000
2023-09-28 22:08:45.696 
Epoch 486/1000 
	 loss: 27.7226, MinusLogProbMetric: 27.7226, val_loss: 28.6322, val_MinusLogProbMetric: 28.6322

Epoch 486: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.7226 - MinusLogProbMetric: 27.7226 - val_loss: 28.6322 - val_MinusLogProbMetric: 28.6322 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 487/1000
2023-09-28 22:09:26.927 
Epoch 487/1000 
	 loss: 27.6968, MinusLogProbMetric: 27.6968, val_loss: 28.6579, val_MinusLogProbMetric: 28.6579

Epoch 487: val_loss did not improve from 28.45922
196/196 - 41s - loss: 27.6968 - MinusLogProbMetric: 27.6968 - val_loss: 28.6579 - val_MinusLogProbMetric: 28.6579 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 488/1000
2023-09-28 22:10:08.964 
Epoch 488/1000 
	 loss: 27.6517, MinusLogProbMetric: 27.6517, val_loss: 28.4949, val_MinusLogProbMetric: 28.4949

Epoch 488: val_loss did not improve from 28.45922
196/196 - 42s - loss: 27.6517 - MinusLogProbMetric: 27.6517 - val_loss: 28.4949 - val_MinusLogProbMetric: 28.4949 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 489/1000
2023-09-28 22:10:49.014 
Epoch 489/1000 
	 loss: 27.6586, MinusLogProbMetric: 27.6586, val_loss: 28.6146, val_MinusLogProbMetric: 28.6146

Epoch 489: val_loss did not improve from 28.45922
196/196 - 40s - loss: 27.6586 - MinusLogProbMetric: 27.6586 - val_loss: 28.6146 - val_MinusLogProbMetric: 28.6146 - lr: 2.5000e-04 - 40s/epoch - 204ms/step
Epoch 490/1000
2023-09-28 22:11:23.081 
Epoch 490/1000 
	 loss: 27.6794, MinusLogProbMetric: 27.6794, val_loss: 28.4972, val_MinusLogProbMetric: 28.4972

Epoch 490: val_loss did not improve from 28.45922
196/196 - 34s - loss: 27.6794 - MinusLogProbMetric: 27.6794 - val_loss: 28.4972 - val_MinusLogProbMetric: 28.4972 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 491/1000
2023-09-28 22:11:57.167 
Epoch 491/1000 
	 loss: 27.5668, MinusLogProbMetric: 27.5668, val_loss: 28.4542, val_MinusLogProbMetric: 28.4542

Epoch 491: val_loss improved from 28.45922 to 28.45419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 35s - loss: 27.5668 - MinusLogProbMetric: 27.5668 - val_loss: 28.4542 - val_MinusLogProbMetric: 28.4542 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 492/1000
2023-09-28 22:12:32.829 
Epoch 492/1000 
	 loss: 27.5510, MinusLogProbMetric: 27.5510, val_loss: 28.4554, val_MinusLogProbMetric: 28.4554

Epoch 492: val_loss did not improve from 28.45419
196/196 - 35s - loss: 27.5510 - MinusLogProbMetric: 27.5510 - val_loss: 28.4554 - val_MinusLogProbMetric: 28.4554 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 493/1000
2023-09-28 22:13:08.158 
Epoch 493/1000 
	 loss: 27.5778, MinusLogProbMetric: 27.5778, val_loss: 28.4644, val_MinusLogProbMetric: 28.4644

Epoch 493: val_loss did not improve from 28.45419
196/196 - 35s - loss: 27.5778 - MinusLogProbMetric: 27.5778 - val_loss: 28.4644 - val_MinusLogProbMetric: 28.4644 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 494/1000
2023-09-28 22:13:50.242 
Epoch 494/1000 
	 loss: 27.5542, MinusLogProbMetric: 27.5542, val_loss: 28.4825, val_MinusLogProbMetric: 28.4825

Epoch 494: val_loss did not improve from 28.45419
196/196 - 42s - loss: 27.5542 - MinusLogProbMetric: 27.5542 - val_loss: 28.4825 - val_MinusLogProbMetric: 28.4825 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 495/1000
2023-09-28 22:14:32.546 
Epoch 495/1000 
	 loss: 27.5545, MinusLogProbMetric: 27.5545, val_loss: 28.4604, val_MinusLogProbMetric: 28.4604

Epoch 495: val_loss did not improve from 28.45419
196/196 - 42s - loss: 27.5545 - MinusLogProbMetric: 27.5545 - val_loss: 28.4604 - val_MinusLogProbMetric: 28.4604 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 496/1000
2023-09-28 22:15:11.934 
Epoch 496/1000 
	 loss: 27.5494, MinusLogProbMetric: 27.5494, val_loss: 28.4299, val_MinusLogProbMetric: 28.4299

Epoch 496: val_loss improved from 28.45419 to 28.42988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 40s - loss: 27.5494 - MinusLogProbMetric: 27.5494 - val_loss: 28.4299 - val_MinusLogProbMetric: 28.4299 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 497/1000
2023-09-28 22:15:54.955 
Epoch 497/1000 
	 loss: 27.5521, MinusLogProbMetric: 27.5521, val_loss: 28.4502, val_MinusLogProbMetric: 28.4502

Epoch 497: val_loss did not improve from 28.42988
196/196 - 42s - loss: 27.5521 - MinusLogProbMetric: 27.5521 - val_loss: 28.4502 - val_MinusLogProbMetric: 28.4502 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 498/1000
2023-09-28 22:16:37.265 
Epoch 498/1000 
	 loss: 27.5561, MinusLogProbMetric: 27.5561, val_loss: 28.5484, val_MinusLogProbMetric: 28.5484

Epoch 498: val_loss did not improve from 28.42988
196/196 - 42s - loss: 27.5561 - MinusLogProbMetric: 27.5561 - val_loss: 28.5484 - val_MinusLogProbMetric: 28.5484 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 499/1000
2023-09-28 22:17:19.151 
Epoch 499/1000 
	 loss: 27.5551, MinusLogProbMetric: 27.5551, val_loss: 28.4246, val_MinusLogProbMetric: 28.4246

Epoch 499: val_loss improved from 28.42988 to 28.42464, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 27.5551 - MinusLogProbMetric: 27.5551 - val_loss: 28.4246 - val_MinusLogProbMetric: 28.4246 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 500/1000
2023-09-28 22:18:02.290 
Epoch 500/1000 
	 loss: 27.5544, MinusLogProbMetric: 27.5544, val_loss: 28.5442, val_MinusLogProbMetric: 28.5442

Epoch 500: val_loss did not improve from 28.42464
196/196 - 43s - loss: 27.5544 - MinusLogProbMetric: 27.5544 - val_loss: 28.5442 - val_MinusLogProbMetric: 28.5442 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 501/1000
2023-09-28 22:18:41.895 
Epoch 501/1000 
	 loss: 27.5537, MinusLogProbMetric: 27.5537, val_loss: 28.4382, val_MinusLogProbMetric: 28.4382

Epoch 501: val_loss did not improve from 28.42464
196/196 - 40s - loss: 27.5537 - MinusLogProbMetric: 27.5537 - val_loss: 28.4382 - val_MinusLogProbMetric: 28.4382 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 502/1000
2023-09-28 22:19:23.754 
Epoch 502/1000 
	 loss: 27.5500, MinusLogProbMetric: 27.5500, val_loss: 28.4656, val_MinusLogProbMetric: 28.4656

Epoch 502: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5500 - MinusLogProbMetric: 27.5500 - val_loss: 28.4656 - val_MinusLogProbMetric: 28.4656 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 503/1000
2023-09-28 22:20:06.240 
Epoch 503/1000 
	 loss: 27.5523, MinusLogProbMetric: 27.5523, val_loss: 28.4652, val_MinusLogProbMetric: 28.4652

Epoch 503: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5523 - MinusLogProbMetric: 27.5523 - val_loss: 28.4652 - val_MinusLogProbMetric: 28.4652 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 504/1000
2023-09-28 22:20:48.370 
Epoch 504/1000 
	 loss: 27.5448, MinusLogProbMetric: 27.5448, val_loss: 28.4425, val_MinusLogProbMetric: 28.4425

Epoch 504: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5448 - MinusLogProbMetric: 27.5448 - val_loss: 28.4425 - val_MinusLogProbMetric: 28.4425 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 505/1000
2023-09-28 22:21:30.384 
Epoch 505/1000 
	 loss: 27.5495, MinusLogProbMetric: 27.5495, val_loss: 28.4706, val_MinusLogProbMetric: 28.4706

Epoch 505: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5495 - MinusLogProbMetric: 27.5495 - val_loss: 28.4706 - val_MinusLogProbMetric: 28.4706 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 506/1000
2023-09-28 22:22:12.692 
Epoch 506/1000 
	 loss: 27.5454, MinusLogProbMetric: 27.5454, val_loss: 28.4504, val_MinusLogProbMetric: 28.4504

Epoch 506: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5454 - MinusLogProbMetric: 27.5454 - val_loss: 28.4504 - val_MinusLogProbMetric: 28.4504 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 507/1000
2023-09-28 22:22:54.663 
Epoch 507/1000 
	 loss: 27.5502, MinusLogProbMetric: 27.5502, val_loss: 28.4918, val_MinusLogProbMetric: 28.4918

Epoch 507: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5502 - MinusLogProbMetric: 27.5502 - val_loss: 28.4918 - val_MinusLogProbMetric: 28.4918 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 508/1000
2023-09-28 22:23:37.163 
Epoch 508/1000 
	 loss: 27.5583, MinusLogProbMetric: 27.5583, val_loss: 28.5598, val_MinusLogProbMetric: 28.5598

Epoch 508: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5583 - MinusLogProbMetric: 27.5583 - val_loss: 28.5598 - val_MinusLogProbMetric: 28.5598 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 509/1000
2023-09-28 22:24:18.963 
Epoch 509/1000 
	 loss: 27.5596, MinusLogProbMetric: 27.5596, val_loss: 28.4490, val_MinusLogProbMetric: 28.4490

Epoch 509: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5596 - MinusLogProbMetric: 27.5596 - val_loss: 28.4490 - val_MinusLogProbMetric: 28.4490 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 510/1000
2023-09-28 22:25:01.296 
Epoch 510/1000 
	 loss: 27.5508, MinusLogProbMetric: 27.5508, val_loss: 28.4553, val_MinusLogProbMetric: 28.4553

Epoch 510: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5508 - MinusLogProbMetric: 27.5508 - val_loss: 28.4553 - val_MinusLogProbMetric: 28.4553 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 511/1000
2023-09-28 22:25:43.512 
Epoch 511/1000 
	 loss: 27.5479, MinusLogProbMetric: 27.5479, val_loss: 28.4396, val_MinusLogProbMetric: 28.4396

Epoch 511: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5479 - MinusLogProbMetric: 27.5479 - val_loss: 28.4396 - val_MinusLogProbMetric: 28.4396 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 512/1000
2023-09-28 22:26:25.405 
Epoch 512/1000 
	 loss: 27.5533, MinusLogProbMetric: 27.5533, val_loss: 28.5820, val_MinusLogProbMetric: 28.5820

Epoch 512: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5533 - MinusLogProbMetric: 27.5533 - val_loss: 28.5820 - val_MinusLogProbMetric: 28.5820 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 513/1000
2023-09-28 22:27:07.326 
Epoch 513/1000 
	 loss: 27.5530, MinusLogProbMetric: 27.5530, val_loss: 28.4989, val_MinusLogProbMetric: 28.4989

Epoch 513: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5530 - MinusLogProbMetric: 27.5530 - val_loss: 28.4989 - val_MinusLogProbMetric: 28.4989 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 514/1000
2023-09-28 22:27:49.332 
Epoch 514/1000 
	 loss: 27.5672, MinusLogProbMetric: 27.5672, val_loss: 28.5004, val_MinusLogProbMetric: 28.5004

Epoch 514: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5672 - MinusLogProbMetric: 27.5672 - val_loss: 28.5004 - val_MinusLogProbMetric: 28.5004 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 515/1000
2023-09-28 22:28:31.530 
Epoch 515/1000 
	 loss: 27.5599, MinusLogProbMetric: 27.5599, val_loss: 28.4331, val_MinusLogProbMetric: 28.4331

Epoch 515: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5599 - MinusLogProbMetric: 27.5599 - val_loss: 28.4331 - val_MinusLogProbMetric: 28.4331 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 516/1000
2023-09-28 22:29:13.629 
Epoch 516/1000 
	 loss: 27.5502, MinusLogProbMetric: 27.5502, val_loss: 28.5506, val_MinusLogProbMetric: 28.5506

Epoch 516: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5502 - MinusLogProbMetric: 27.5502 - val_loss: 28.5506 - val_MinusLogProbMetric: 28.5506 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 517/1000
2023-09-28 22:29:56.247 
Epoch 517/1000 
	 loss: 27.5475, MinusLogProbMetric: 27.5475, val_loss: 28.4656, val_MinusLogProbMetric: 28.4656

Epoch 517: val_loss did not improve from 28.42464
196/196 - 43s - loss: 27.5475 - MinusLogProbMetric: 27.5475 - val_loss: 28.4656 - val_MinusLogProbMetric: 28.4656 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 518/1000
2023-09-28 22:30:38.995 
Epoch 518/1000 
	 loss: 27.5468, MinusLogProbMetric: 27.5468, val_loss: 28.4401, val_MinusLogProbMetric: 28.4401

Epoch 518: val_loss did not improve from 28.42464
196/196 - 43s - loss: 27.5468 - MinusLogProbMetric: 27.5468 - val_loss: 28.4401 - val_MinusLogProbMetric: 28.4401 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 519/1000
2023-09-28 22:31:20.987 
Epoch 519/1000 
	 loss: 27.5456, MinusLogProbMetric: 27.5456, val_loss: 28.5016, val_MinusLogProbMetric: 28.5016

Epoch 519: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5456 - MinusLogProbMetric: 27.5456 - val_loss: 28.5016 - val_MinusLogProbMetric: 28.5016 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 520/1000
2023-09-28 22:32:01.214 
Epoch 520/1000 
	 loss: 27.5416, MinusLogProbMetric: 27.5416, val_loss: 28.4278, val_MinusLogProbMetric: 28.4278

Epoch 520: val_loss did not improve from 28.42464
196/196 - 40s - loss: 27.5416 - MinusLogProbMetric: 27.5416 - val_loss: 28.4278 - val_MinusLogProbMetric: 28.4278 - lr: 1.2500e-04 - 40s/epoch - 205ms/step
Epoch 521/1000
2023-09-28 22:32:43.157 
Epoch 521/1000 
	 loss: 27.5424, MinusLogProbMetric: 27.5424, val_loss: 28.4899, val_MinusLogProbMetric: 28.4899

Epoch 521: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5424 - MinusLogProbMetric: 27.5424 - val_loss: 28.4899 - val_MinusLogProbMetric: 28.4899 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 522/1000
2023-09-28 22:33:23.769 
Epoch 522/1000 
	 loss: 27.5440, MinusLogProbMetric: 27.5440, val_loss: 28.5324, val_MinusLogProbMetric: 28.5324

Epoch 522: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5440 - MinusLogProbMetric: 27.5440 - val_loss: 28.5324 - val_MinusLogProbMetric: 28.5324 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 523/1000
2023-09-28 22:34:06.056 
Epoch 523/1000 
	 loss: 27.5475, MinusLogProbMetric: 27.5475, val_loss: 28.6206, val_MinusLogProbMetric: 28.6206

Epoch 523: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5475 - MinusLogProbMetric: 27.5475 - val_loss: 28.6206 - val_MinusLogProbMetric: 28.6206 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 524/1000
2023-09-28 22:34:45.126 
Epoch 524/1000 
	 loss: 27.5521, MinusLogProbMetric: 27.5521, val_loss: 28.4896, val_MinusLogProbMetric: 28.4896

Epoch 524: val_loss did not improve from 28.42464
196/196 - 39s - loss: 27.5521 - MinusLogProbMetric: 27.5521 - val_loss: 28.4896 - val_MinusLogProbMetric: 28.4896 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 525/1000
2023-09-28 22:35:26.942 
Epoch 525/1000 
	 loss: 27.5670, MinusLogProbMetric: 27.5670, val_loss: 28.4531, val_MinusLogProbMetric: 28.4531

Epoch 525: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5670 - MinusLogProbMetric: 27.5670 - val_loss: 28.4531 - val_MinusLogProbMetric: 28.4531 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 526/1000
2023-09-28 22:36:08.940 
Epoch 526/1000 
	 loss: 27.5326, MinusLogProbMetric: 27.5326, val_loss: 28.4668, val_MinusLogProbMetric: 28.4668

Epoch 526: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5326 - MinusLogProbMetric: 27.5326 - val_loss: 28.4668 - val_MinusLogProbMetric: 28.4668 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 527/1000
2023-09-28 22:36:49.993 
Epoch 527/1000 
	 loss: 27.5380, MinusLogProbMetric: 27.5380, val_loss: 28.4565, val_MinusLogProbMetric: 28.4565

Epoch 527: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5380 - MinusLogProbMetric: 27.5380 - val_loss: 28.4565 - val_MinusLogProbMetric: 28.4565 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 528/1000
2023-09-28 22:37:27.900 
Epoch 528/1000 
	 loss: 27.5542, MinusLogProbMetric: 27.5542, val_loss: 28.4557, val_MinusLogProbMetric: 28.4557

Epoch 528: val_loss did not improve from 28.42464
196/196 - 38s - loss: 27.5542 - MinusLogProbMetric: 27.5542 - val_loss: 28.4557 - val_MinusLogProbMetric: 28.4557 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 529/1000
2023-09-28 22:38:07.455 
Epoch 529/1000 
	 loss: 27.5397, MinusLogProbMetric: 27.5397, val_loss: 28.4625, val_MinusLogProbMetric: 28.4625

Epoch 529: val_loss did not improve from 28.42464
196/196 - 40s - loss: 27.5397 - MinusLogProbMetric: 27.5397 - val_loss: 28.4625 - val_MinusLogProbMetric: 28.4625 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 530/1000
2023-09-28 22:38:49.417 
Epoch 530/1000 
	 loss: 27.5380, MinusLogProbMetric: 27.5380, val_loss: 28.4890, val_MinusLogProbMetric: 28.4890

Epoch 530: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5380 - MinusLogProbMetric: 27.5380 - val_loss: 28.4890 - val_MinusLogProbMetric: 28.4890 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 531/1000
2023-09-28 22:39:28.316 
Epoch 531/1000 
	 loss: 27.5382, MinusLogProbMetric: 27.5382, val_loss: 28.5206, val_MinusLogProbMetric: 28.5206

Epoch 531: val_loss did not improve from 28.42464
196/196 - 39s - loss: 27.5382 - MinusLogProbMetric: 27.5382 - val_loss: 28.5206 - val_MinusLogProbMetric: 28.5206 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 532/1000
2023-09-28 22:40:09.315 
Epoch 532/1000 
	 loss: 27.5400, MinusLogProbMetric: 27.5400, val_loss: 28.4586, val_MinusLogProbMetric: 28.4586

Epoch 532: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5400 - MinusLogProbMetric: 27.5400 - val_loss: 28.4586 - val_MinusLogProbMetric: 28.4586 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 533/1000
2023-09-28 22:40:49.694 
Epoch 533/1000 
	 loss: 27.5485, MinusLogProbMetric: 27.5485, val_loss: 28.5177, val_MinusLogProbMetric: 28.5177

Epoch 533: val_loss did not improve from 28.42464
196/196 - 40s - loss: 27.5485 - MinusLogProbMetric: 27.5485 - val_loss: 28.5177 - val_MinusLogProbMetric: 28.5177 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 534/1000
2023-09-28 22:41:30.584 
Epoch 534/1000 
	 loss: 27.5528, MinusLogProbMetric: 27.5528, val_loss: 28.4640, val_MinusLogProbMetric: 28.4640

Epoch 534: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5528 - MinusLogProbMetric: 27.5528 - val_loss: 28.4640 - val_MinusLogProbMetric: 28.4640 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 535/1000
2023-09-28 22:42:11.618 
Epoch 535/1000 
	 loss: 27.5479, MinusLogProbMetric: 27.5479, val_loss: 28.4764, val_MinusLogProbMetric: 28.4764

Epoch 535: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5479 - MinusLogProbMetric: 27.5479 - val_loss: 28.4764 - val_MinusLogProbMetric: 28.4764 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 536/1000
2023-09-28 22:42:52.455 
Epoch 536/1000 
	 loss: 27.5657, MinusLogProbMetric: 27.5657, val_loss: 28.4974, val_MinusLogProbMetric: 28.4974

Epoch 536: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5657 - MinusLogProbMetric: 27.5657 - val_loss: 28.4974 - val_MinusLogProbMetric: 28.4974 - lr: 1.2500e-04 - 41s/epoch - 208ms/step
Epoch 537/1000
2023-09-28 22:43:34.556 
Epoch 537/1000 
	 loss: 27.5344, MinusLogProbMetric: 27.5344, val_loss: 28.4427, val_MinusLogProbMetric: 28.4427

Epoch 537: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5344 - MinusLogProbMetric: 27.5344 - val_loss: 28.4427 - val_MinusLogProbMetric: 28.4427 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 538/1000
2023-09-28 22:44:16.587 
Epoch 538/1000 
	 loss: 27.5413, MinusLogProbMetric: 27.5413, val_loss: 28.5250, val_MinusLogProbMetric: 28.5250

Epoch 538: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5413 - MinusLogProbMetric: 27.5413 - val_loss: 28.5250 - val_MinusLogProbMetric: 28.5250 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 539/1000
2023-09-28 22:44:58.460 
Epoch 539/1000 
	 loss: 27.5372, MinusLogProbMetric: 27.5372, val_loss: 28.4449, val_MinusLogProbMetric: 28.4449

Epoch 539: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5372 - MinusLogProbMetric: 27.5372 - val_loss: 28.4449 - val_MinusLogProbMetric: 28.4449 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 540/1000
2023-09-28 22:45:40.856 
Epoch 540/1000 
	 loss: 27.5547, MinusLogProbMetric: 27.5547, val_loss: 28.7018, val_MinusLogProbMetric: 28.7018

Epoch 540: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5547 - MinusLogProbMetric: 27.5547 - val_loss: 28.7018 - val_MinusLogProbMetric: 28.7018 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 541/1000
2023-09-28 22:46:23.033 
Epoch 541/1000 
	 loss: 27.5375, MinusLogProbMetric: 27.5375, val_loss: 28.4653, val_MinusLogProbMetric: 28.4653

Epoch 541: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5375 - MinusLogProbMetric: 27.5375 - val_loss: 28.4653 - val_MinusLogProbMetric: 28.4653 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 542/1000
2023-09-28 22:47:04.860 
Epoch 542/1000 
	 loss: 27.5421, MinusLogProbMetric: 27.5421, val_loss: 28.4540, val_MinusLogProbMetric: 28.4540

Epoch 542: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5421 - MinusLogProbMetric: 27.5421 - val_loss: 28.4540 - val_MinusLogProbMetric: 28.4540 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 543/1000
2023-09-28 22:47:45.439 
Epoch 543/1000 
	 loss: 27.5355, MinusLogProbMetric: 27.5355, val_loss: 28.4628, val_MinusLogProbMetric: 28.4628

Epoch 543: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5355 - MinusLogProbMetric: 27.5355 - val_loss: 28.4628 - val_MinusLogProbMetric: 28.4628 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 544/1000
2023-09-28 22:48:24.305 
Epoch 544/1000 
	 loss: 27.5343, MinusLogProbMetric: 27.5343, val_loss: 28.4473, val_MinusLogProbMetric: 28.4473

Epoch 544: val_loss did not improve from 28.42464
196/196 - 39s - loss: 27.5343 - MinusLogProbMetric: 27.5343 - val_loss: 28.4473 - val_MinusLogProbMetric: 28.4473 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 545/1000
2023-09-28 22:49:06.661 
Epoch 545/1000 
	 loss: 27.5677, MinusLogProbMetric: 27.5677, val_loss: 28.4553, val_MinusLogProbMetric: 28.4553

Epoch 545: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5677 - MinusLogProbMetric: 27.5677 - val_loss: 28.4553 - val_MinusLogProbMetric: 28.4553 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 546/1000
2023-09-28 22:49:48.395 
Epoch 546/1000 
	 loss: 27.5387, MinusLogProbMetric: 27.5387, val_loss: 28.4772, val_MinusLogProbMetric: 28.4772

Epoch 546: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5387 - MinusLogProbMetric: 27.5387 - val_loss: 28.4772 - val_MinusLogProbMetric: 28.4772 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 547/1000
2023-09-28 22:50:29.787 
Epoch 547/1000 
	 loss: 27.5589, MinusLogProbMetric: 27.5589, val_loss: 28.4492, val_MinusLogProbMetric: 28.4492

Epoch 547: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5589 - MinusLogProbMetric: 27.5589 - val_loss: 28.4492 - val_MinusLogProbMetric: 28.4492 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 548/1000
2023-09-28 22:51:11.565 
Epoch 548/1000 
	 loss: 27.5362, MinusLogProbMetric: 27.5362, val_loss: 28.4630, val_MinusLogProbMetric: 28.4630

Epoch 548: val_loss did not improve from 28.42464
196/196 - 42s - loss: 27.5362 - MinusLogProbMetric: 27.5362 - val_loss: 28.4630 - val_MinusLogProbMetric: 28.4630 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 549/1000
2023-09-28 22:51:52.602 
Epoch 549/1000 
	 loss: 27.5276, MinusLogProbMetric: 27.5276, val_loss: 28.4434, val_MinusLogProbMetric: 28.4434

Epoch 549: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.5276 - MinusLogProbMetric: 27.5276 - val_loss: 28.4434 - val_MinusLogProbMetric: 28.4434 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 550/1000
2023-09-28 22:52:33.966 
Epoch 550/1000 
	 loss: 27.4899, MinusLogProbMetric: 27.4899, val_loss: 28.4432, val_MinusLogProbMetric: 28.4432

Epoch 550: val_loss did not improve from 28.42464
196/196 - 41s - loss: 27.4899 - MinusLogProbMetric: 27.4899 - val_loss: 28.4432 - val_MinusLogProbMetric: 28.4432 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 551/1000
2023-09-28 22:53:15.642 
Epoch 551/1000 
	 loss: 27.4838, MinusLogProbMetric: 27.4838, val_loss: 28.4206, val_MinusLogProbMetric: 28.4206

Epoch 551: val_loss improved from 28.42464 to 28.42059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 42s - loss: 27.4838 - MinusLogProbMetric: 27.4838 - val_loss: 28.4206 - val_MinusLogProbMetric: 28.4206 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 552/1000
2023-09-28 22:53:58.162 
Epoch 552/1000 
	 loss: 27.4860, MinusLogProbMetric: 27.4860, val_loss: 28.4227, val_MinusLogProbMetric: 28.4227

Epoch 552: val_loss did not improve from 28.42059
196/196 - 42s - loss: 27.4860 - MinusLogProbMetric: 27.4860 - val_loss: 28.4227 - val_MinusLogProbMetric: 28.4227 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 553/1000
2023-09-28 22:54:37.721 
Epoch 553/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.4567, val_MinusLogProbMetric: 28.4567

Epoch 553: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.4567 - val_MinusLogProbMetric: 28.4567 - lr: 6.2500e-05 - 40s/epoch - 202ms/step
Epoch 554/1000
2023-09-28 22:55:18.156 
Epoch 554/1000 
	 loss: 27.4848, MinusLogProbMetric: 27.4848, val_loss: 28.4263, val_MinusLogProbMetric: 28.4263

Epoch 554: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4848 - MinusLogProbMetric: 27.4848 - val_loss: 28.4263 - val_MinusLogProbMetric: 28.4263 - lr: 6.2500e-05 - 40s/epoch - 206ms/step
Epoch 555/1000
2023-09-28 22:55:57.645 
Epoch 555/1000 
	 loss: 27.4854, MinusLogProbMetric: 27.4854, val_loss: 28.4421, val_MinusLogProbMetric: 28.4421

Epoch 555: val_loss did not improve from 28.42059
196/196 - 39s - loss: 27.4854 - MinusLogProbMetric: 27.4854 - val_loss: 28.4421 - val_MinusLogProbMetric: 28.4421 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 556/1000
2023-09-28 22:56:39.289 
Epoch 556/1000 
	 loss: 27.4897, MinusLogProbMetric: 27.4897, val_loss: 28.4328, val_MinusLogProbMetric: 28.4328

Epoch 556: val_loss did not improve from 28.42059
196/196 - 42s - loss: 27.4897 - MinusLogProbMetric: 27.4897 - val_loss: 28.4328 - val_MinusLogProbMetric: 28.4328 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 557/1000
2023-09-28 22:57:19.089 
Epoch 557/1000 
	 loss: 27.4804, MinusLogProbMetric: 27.4804, val_loss: 28.4307, val_MinusLogProbMetric: 28.4307

Epoch 557: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4804 - MinusLogProbMetric: 27.4804 - val_loss: 28.4307 - val_MinusLogProbMetric: 28.4307 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 558/1000
2023-09-28 22:57:59.297 
Epoch 558/1000 
	 loss: 27.4867, MinusLogProbMetric: 27.4867, val_loss: 28.4304, val_MinusLogProbMetric: 28.4304

Epoch 558: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4867 - MinusLogProbMetric: 27.4867 - val_loss: 28.4304 - val_MinusLogProbMetric: 28.4304 - lr: 6.2500e-05 - 40s/epoch - 205ms/step
Epoch 559/1000
2023-09-28 22:58:36.509 
Epoch 559/1000 
	 loss: 27.4862, MinusLogProbMetric: 27.4862, val_loss: 28.4448, val_MinusLogProbMetric: 28.4448

Epoch 559: val_loss did not improve from 28.42059
196/196 - 37s - loss: 27.4862 - MinusLogProbMetric: 27.4862 - val_loss: 28.4448 - val_MinusLogProbMetric: 28.4448 - lr: 6.2500e-05 - 37s/epoch - 190ms/step
Epoch 560/1000
2023-09-28 22:59:17.445 
Epoch 560/1000 
	 loss: 27.4827, MinusLogProbMetric: 27.4827, val_loss: 28.4635, val_MinusLogProbMetric: 28.4635

Epoch 560: val_loss did not improve from 28.42059
196/196 - 41s - loss: 27.4827 - MinusLogProbMetric: 27.4827 - val_loss: 28.4635 - val_MinusLogProbMetric: 28.4635 - lr: 6.2500e-05 - 41s/epoch - 209ms/step
Epoch 561/1000
2023-09-28 22:59:56.032 
Epoch 561/1000 
	 loss: 27.4820, MinusLogProbMetric: 27.4820, val_loss: 28.4240, val_MinusLogProbMetric: 28.4240

Epoch 561: val_loss did not improve from 28.42059
196/196 - 39s - loss: 27.4820 - MinusLogProbMetric: 27.4820 - val_loss: 28.4240 - val_MinusLogProbMetric: 28.4240 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 562/1000
2023-09-28 23:00:35.575 
Epoch 562/1000 
	 loss: 27.4877, MinusLogProbMetric: 27.4877, val_loss: 28.4283, val_MinusLogProbMetric: 28.4283

Epoch 562: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4877 - MinusLogProbMetric: 27.4877 - val_loss: 28.4283 - val_MinusLogProbMetric: 28.4283 - lr: 6.2500e-05 - 40s/epoch - 202ms/step
Epoch 563/1000
2023-09-28 23:01:11.991 
Epoch 563/1000 
	 loss: 27.4886, MinusLogProbMetric: 27.4886, val_loss: 28.4224, val_MinusLogProbMetric: 28.4224

Epoch 563: val_loss did not improve from 28.42059
196/196 - 36s - loss: 27.4886 - MinusLogProbMetric: 27.4886 - val_loss: 28.4224 - val_MinusLogProbMetric: 28.4224 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 564/1000
2023-09-28 23:01:48.426 
Epoch 564/1000 
	 loss: 27.4799, MinusLogProbMetric: 27.4799, val_loss: 28.4749, val_MinusLogProbMetric: 28.4749

Epoch 564: val_loss did not improve from 28.42059
196/196 - 36s - loss: 27.4799 - MinusLogProbMetric: 27.4799 - val_loss: 28.4749 - val_MinusLogProbMetric: 28.4749 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 565/1000
2023-09-28 23:02:30.190 
Epoch 565/1000 
	 loss: 27.4898, MinusLogProbMetric: 27.4898, val_loss: 28.4595, val_MinusLogProbMetric: 28.4595

Epoch 565: val_loss did not improve from 28.42059
196/196 - 42s - loss: 27.4898 - MinusLogProbMetric: 27.4898 - val_loss: 28.4595 - val_MinusLogProbMetric: 28.4595 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 566/1000
2023-09-28 23:03:12.206 
Epoch 566/1000 
	 loss: 27.4844, MinusLogProbMetric: 27.4844, val_loss: 28.4527, val_MinusLogProbMetric: 28.4527

Epoch 566: val_loss did not improve from 28.42059
196/196 - 42s - loss: 27.4844 - MinusLogProbMetric: 27.4844 - val_loss: 28.4527 - val_MinusLogProbMetric: 28.4527 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 567/1000
2023-09-28 23:03:51.753 
Epoch 567/1000 
	 loss: 27.4894, MinusLogProbMetric: 27.4894, val_loss: 28.5034, val_MinusLogProbMetric: 28.5034

Epoch 567: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4894 - MinusLogProbMetric: 27.4894 - val_loss: 28.5034 - val_MinusLogProbMetric: 28.5034 - lr: 6.2500e-05 - 40s/epoch - 202ms/step
Epoch 568/1000
2023-09-28 23:04:31.722 
Epoch 568/1000 
	 loss: 27.4830, MinusLogProbMetric: 27.4830, val_loss: 28.4426, val_MinusLogProbMetric: 28.4426

Epoch 568: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4830 - MinusLogProbMetric: 27.4830 - val_loss: 28.4426 - val_MinusLogProbMetric: 28.4426 - lr: 6.2500e-05 - 40s/epoch - 204ms/step
Epoch 569/1000
2023-09-28 23:05:10.915 
Epoch 569/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 28.4587, val_MinusLogProbMetric: 28.4587

Epoch 569: val_loss did not improve from 28.42059
196/196 - 39s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 28.4587 - val_MinusLogProbMetric: 28.4587 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 570/1000
2023-09-28 23:05:50.629 
Epoch 570/1000 
	 loss: 27.4808, MinusLogProbMetric: 27.4808, val_loss: 28.4682, val_MinusLogProbMetric: 28.4682

Epoch 570: val_loss did not improve from 28.42059
196/196 - 40s - loss: 27.4808 - MinusLogProbMetric: 27.4808 - val_loss: 28.4682 - val_MinusLogProbMetric: 28.4682 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 571/1000
2023-09-28 23:06:29.482 
Epoch 571/1000 
	 loss: 27.4831, MinusLogProbMetric: 27.4831, val_loss: 28.4334, val_MinusLogProbMetric: 28.4334

Epoch 571: val_loss did not improve from 28.42059
196/196 - 39s - loss: 27.4831 - MinusLogProbMetric: 27.4831 - val_loss: 28.4334 - val_MinusLogProbMetric: 28.4334 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 572/1000
2023-09-28 23:07:07.470 
Epoch 572/1000 
	 loss: 27.4948, MinusLogProbMetric: 27.4948, val_loss: 28.4179, val_MinusLogProbMetric: 28.4179

Epoch 572: val_loss improved from 28.42059 to 28.41794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 39s - loss: 27.4948 - MinusLogProbMetric: 27.4948 - val_loss: 28.4179 - val_MinusLogProbMetric: 28.4179 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 573/1000
2023-09-28 23:07:46.570 
Epoch 573/1000 
	 loss: 27.4804, MinusLogProbMetric: 27.4804, val_loss: 28.4192, val_MinusLogProbMetric: 28.4192

Epoch 573: val_loss did not improve from 28.41794
196/196 - 38s - loss: 27.4804 - MinusLogProbMetric: 27.4804 - val_loss: 28.4192 - val_MinusLogProbMetric: 28.4192 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 574/1000
2023-09-28 23:08:26.013 
Epoch 574/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.4347, val_MinusLogProbMetric: 28.4347

Epoch 574: val_loss did not improve from 28.41794
196/196 - 39s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.4347 - val_MinusLogProbMetric: 28.4347 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 575/1000
2023-09-28 23:09:07.385 
Epoch 575/1000 
	 loss: 27.4888, MinusLogProbMetric: 27.4888, val_loss: 28.4467, val_MinusLogProbMetric: 28.4467

Epoch 575: val_loss did not improve from 28.41794
196/196 - 41s - loss: 27.4888 - MinusLogProbMetric: 27.4888 - val_loss: 28.4467 - val_MinusLogProbMetric: 28.4467 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 576/1000
2023-09-28 23:09:42.255 
Epoch 576/1000 
	 loss: 27.4833, MinusLogProbMetric: 27.4833, val_loss: 28.4265, val_MinusLogProbMetric: 28.4265

Epoch 576: val_loss did not improve from 28.41794
196/196 - 35s - loss: 27.4833 - MinusLogProbMetric: 27.4833 - val_loss: 28.4265 - val_MinusLogProbMetric: 28.4265 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 577/1000
2023-09-28 23:10:20.833 
Epoch 577/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 28.4386, val_MinusLogProbMetric: 28.4386

Epoch 577: val_loss did not improve from 28.41794
196/196 - 39s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 28.4386 - val_MinusLogProbMetric: 28.4386 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 578/1000
2023-09-28 23:11:01.559 
Epoch 578/1000 
	 loss: 27.4873, MinusLogProbMetric: 27.4873, val_loss: 28.4448, val_MinusLogProbMetric: 28.4448

Epoch 578: val_loss did not improve from 28.41794
196/196 - 41s - loss: 27.4873 - MinusLogProbMetric: 27.4873 - val_loss: 28.4448 - val_MinusLogProbMetric: 28.4448 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 579/1000
2023-09-28 23:11:43.081 
Epoch 579/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.4774, val_MinusLogProbMetric: 28.4774

Epoch 579: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.4774 - val_MinusLogProbMetric: 28.4774 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 580/1000
2023-09-28 23:12:25.248 
Epoch 580/1000 
	 loss: 27.4846, MinusLogProbMetric: 27.4846, val_loss: 28.4546, val_MinusLogProbMetric: 28.4546

Epoch 580: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4846 - MinusLogProbMetric: 27.4846 - val_loss: 28.4546 - val_MinusLogProbMetric: 28.4546 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 581/1000
2023-09-28 23:13:05.088 
Epoch 581/1000 
	 loss: 27.4786, MinusLogProbMetric: 27.4786, val_loss: 28.4353, val_MinusLogProbMetric: 28.4353

Epoch 581: val_loss did not improve from 28.41794
196/196 - 40s - loss: 27.4786 - MinusLogProbMetric: 27.4786 - val_loss: 28.4353 - val_MinusLogProbMetric: 28.4353 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 582/1000
2023-09-28 23:13:46.735 
Epoch 582/1000 
	 loss: 27.4748, MinusLogProbMetric: 27.4748, val_loss: 28.4299, val_MinusLogProbMetric: 28.4299

Epoch 582: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4748 - MinusLogProbMetric: 27.4748 - val_loss: 28.4299 - val_MinusLogProbMetric: 28.4299 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 583/1000
2023-09-28 23:14:28.843 
Epoch 583/1000 
	 loss: 27.4840, MinusLogProbMetric: 27.4840, val_loss: 28.4380, val_MinusLogProbMetric: 28.4380

Epoch 583: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4840 - MinusLogProbMetric: 27.4840 - val_loss: 28.4380 - val_MinusLogProbMetric: 28.4380 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 584/1000
2023-09-28 23:15:06.646 
Epoch 584/1000 
	 loss: 27.4752, MinusLogProbMetric: 27.4752, val_loss: 28.4347, val_MinusLogProbMetric: 28.4347

Epoch 584: val_loss did not improve from 28.41794
196/196 - 38s - loss: 27.4752 - MinusLogProbMetric: 27.4752 - val_loss: 28.4347 - val_MinusLogProbMetric: 28.4347 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 585/1000
2023-09-28 23:15:43.341 
Epoch 585/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.4280, val_MinusLogProbMetric: 28.4280

Epoch 585: val_loss did not improve from 28.41794
196/196 - 37s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.4280 - val_MinusLogProbMetric: 28.4280 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 586/1000
2023-09-28 23:16:24.144 
Epoch 586/1000 
	 loss: 27.4788, MinusLogProbMetric: 27.4788, val_loss: 28.4395, val_MinusLogProbMetric: 28.4395

Epoch 586: val_loss did not improve from 28.41794
196/196 - 41s - loss: 27.4788 - MinusLogProbMetric: 27.4788 - val_loss: 28.4395 - val_MinusLogProbMetric: 28.4395 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 587/1000
2023-09-28 23:17:02.615 
Epoch 587/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 28.4579, val_MinusLogProbMetric: 28.4579

Epoch 587: val_loss did not improve from 28.41794
196/196 - 38s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 28.4579 - val_MinusLogProbMetric: 28.4579 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 588/1000
2023-09-28 23:17:40.258 
Epoch 588/1000 
	 loss: 27.4900, MinusLogProbMetric: 27.4900, val_loss: 28.4831, val_MinusLogProbMetric: 28.4831

Epoch 588: val_loss did not improve from 28.41794
196/196 - 38s - loss: 27.4900 - MinusLogProbMetric: 27.4900 - val_loss: 28.4831 - val_MinusLogProbMetric: 28.4831 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 589/1000
2023-09-28 23:18:21.542 
Epoch 589/1000 
	 loss: 27.4878, MinusLogProbMetric: 27.4878, val_loss: 28.4383, val_MinusLogProbMetric: 28.4383

Epoch 589: val_loss did not improve from 28.41794
196/196 - 41s - loss: 27.4878 - MinusLogProbMetric: 27.4878 - val_loss: 28.4383 - val_MinusLogProbMetric: 28.4383 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 590/1000
2023-09-28 23:19:02.993 
Epoch 590/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.4315, val_MinusLogProbMetric: 28.4315

Epoch 590: val_loss did not improve from 28.41794
196/196 - 41s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.4315 - val_MinusLogProbMetric: 28.4315 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 591/1000
2023-09-28 23:19:44.715 
Epoch 591/1000 
	 loss: 27.4764, MinusLogProbMetric: 27.4764, val_loss: 28.4437, val_MinusLogProbMetric: 28.4437

Epoch 591: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4764 - MinusLogProbMetric: 27.4764 - val_loss: 28.4437 - val_MinusLogProbMetric: 28.4437 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 592/1000
2023-09-28 23:20:24.487 
Epoch 592/1000 
	 loss: 27.4779, MinusLogProbMetric: 27.4779, val_loss: 28.4443, val_MinusLogProbMetric: 28.4443

Epoch 592: val_loss did not improve from 28.41794
196/196 - 40s - loss: 27.4779 - MinusLogProbMetric: 27.4779 - val_loss: 28.4443 - val_MinusLogProbMetric: 28.4443 - lr: 6.2500e-05 - 40s/epoch - 203ms/step
Epoch 593/1000
2023-09-28 23:21:03.384 
Epoch 593/1000 
	 loss: 27.4775, MinusLogProbMetric: 27.4775, val_loss: 28.4364, val_MinusLogProbMetric: 28.4364

Epoch 593: val_loss did not improve from 28.41794
196/196 - 39s - loss: 27.4775 - MinusLogProbMetric: 27.4775 - val_loss: 28.4364 - val_MinusLogProbMetric: 28.4364 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 594/1000
2023-09-28 23:21:41.023 
Epoch 594/1000 
	 loss: 27.4759, MinusLogProbMetric: 27.4759, val_loss: 28.4397, val_MinusLogProbMetric: 28.4397

Epoch 594: val_loss did not improve from 28.41794
196/196 - 38s - loss: 27.4759 - MinusLogProbMetric: 27.4759 - val_loss: 28.4397 - val_MinusLogProbMetric: 28.4397 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 595/1000
2023-09-28 23:22:19.064 
Epoch 595/1000 
	 loss: 27.4777, MinusLogProbMetric: 27.4777, val_loss: 28.4523, val_MinusLogProbMetric: 28.4523

Epoch 595: val_loss did not improve from 28.41794
196/196 - 38s - loss: 27.4777 - MinusLogProbMetric: 27.4777 - val_loss: 28.4523 - val_MinusLogProbMetric: 28.4523 - lr: 6.2500e-05 - 38s/epoch - 194ms/step
Epoch 596/1000
2023-09-28 23:22:56.139 
Epoch 596/1000 
	 loss: 27.4812, MinusLogProbMetric: 27.4812, val_loss: 28.4488, val_MinusLogProbMetric: 28.4488

Epoch 596: val_loss did not improve from 28.41794
196/196 - 37s - loss: 27.4812 - MinusLogProbMetric: 27.4812 - val_loss: 28.4488 - val_MinusLogProbMetric: 28.4488 - lr: 6.2500e-05 - 37s/epoch - 189ms/step
Epoch 597/1000
2023-09-28 23:23:36.094 
Epoch 597/1000 
	 loss: 27.4815, MinusLogProbMetric: 27.4815, val_loss: 28.4442, val_MinusLogProbMetric: 28.4442

Epoch 597: val_loss did not improve from 28.41794
196/196 - 40s - loss: 27.4815 - MinusLogProbMetric: 27.4815 - val_loss: 28.4442 - val_MinusLogProbMetric: 28.4442 - lr: 6.2500e-05 - 40s/epoch - 204ms/step
Epoch 598/1000
2023-09-28 23:24:16.133 
Epoch 598/1000 
	 loss: 27.4738, MinusLogProbMetric: 27.4738, val_loss: 28.4523, val_MinusLogProbMetric: 28.4523

Epoch 598: val_loss did not improve from 28.41794
196/196 - 40s - loss: 27.4738 - MinusLogProbMetric: 27.4738 - val_loss: 28.4523 - val_MinusLogProbMetric: 28.4523 - lr: 6.2500e-05 - 40s/epoch - 204ms/step
Epoch 599/1000
2023-09-28 23:24:57.755 
Epoch 599/1000 
	 loss: 27.4863, MinusLogProbMetric: 27.4863, val_loss: 28.4751, val_MinusLogProbMetric: 28.4751

Epoch 599: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4863 - MinusLogProbMetric: 27.4863 - val_loss: 28.4751 - val_MinusLogProbMetric: 28.4751 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 600/1000
2023-09-28 23:25:36.501 
Epoch 600/1000 
	 loss: 27.4845, MinusLogProbMetric: 27.4845, val_loss: 28.4350, val_MinusLogProbMetric: 28.4350

Epoch 600: val_loss did not improve from 28.41794
196/196 - 39s - loss: 27.4845 - MinusLogProbMetric: 27.4845 - val_loss: 28.4350 - val_MinusLogProbMetric: 28.4350 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 601/1000
2023-09-28 23:26:15.431 
Epoch 601/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.4715, val_MinusLogProbMetric: 28.4715

Epoch 601: val_loss did not improve from 28.41794
196/196 - 39s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.4715 - val_MinusLogProbMetric: 28.4715 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 602/1000
2023-09-28 23:26:54.349 
Epoch 602/1000 
	 loss: 27.4719, MinusLogProbMetric: 27.4719, val_loss: 28.4402, val_MinusLogProbMetric: 28.4402

Epoch 602: val_loss did not improve from 28.41794
196/196 - 39s - loss: 27.4719 - MinusLogProbMetric: 27.4719 - val_loss: 28.4402 - val_MinusLogProbMetric: 28.4402 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 603/1000
2023-09-28 23:27:33.098 
Epoch 603/1000 
	 loss: 27.4847, MinusLogProbMetric: 27.4847, val_loss: 28.4976, val_MinusLogProbMetric: 28.4976

Epoch 603: val_loss did not improve from 28.41794
196/196 - 39s - loss: 27.4847 - MinusLogProbMetric: 27.4847 - val_loss: 28.4976 - val_MinusLogProbMetric: 28.4976 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 604/1000
2023-09-28 23:28:11.014 
Epoch 604/1000 
	 loss: 27.4767, MinusLogProbMetric: 27.4767, val_loss: 28.4410, val_MinusLogProbMetric: 28.4410

Epoch 604: val_loss did not improve from 28.41794
196/196 - 38s - loss: 27.4767 - MinusLogProbMetric: 27.4767 - val_loss: 28.4410 - val_MinusLogProbMetric: 28.4410 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 605/1000
2023-09-28 23:28:47.674 
Epoch 605/1000 
	 loss: 27.4925, MinusLogProbMetric: 27.4925, val_loss: 28.4296, val_MinusLogProbMetric: 28.4296

Epoch 605: val_loss did not improve from 28.41794
196/196 - 37s - loss: 27.4925 - MinusLogProbMetric: 27.4925 - val_loss: 28.4296 - val_MinusLogProbMetric: 28.4296 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 606/1000
2023-09-28 23:29:30.966 
Epoch 606/1000 
	 loss: 27.4738, MinusLogProbMetric: 27.4738, val_loss: 28.5425, val_MinusLogProbMetric: 28.5425

Epoch 606: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4738 - MinusLogProbMetric: 27.4738 - val_loss: 28.5425 - val_MinusLogProbMetric: 28.5425 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 607/1000
2023-09-28 23:30:13.494 
Epoch 607/1000 
	 loss: 27.4902, MinusLogProbMetric: 27.4902, val_loss: 28.4331, val_MinusLogProbMetric: 28.4331

Epoch 607: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4902 - MinusLogProbMetric: 27.4902 - val_loss: 28.4331 - val_MinusLogProbMetric: 28.4331 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 608/1000
2023-09-28 23:30:56.050 
Epoch 608/1000 
	 loss: 27.4756, MinusLogProbMetric: 27.4756, val_loss: 28.4446, val_MinusLogProbMetric: 28.4446

Epoch 608: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4756 - MinusLogProbMetric: 27.4756 - val_loss: 28.4446 - val_MinusLogProbMetric: 28.4446 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 609/1000
2023-09-28 23:31:37.605 
Epoch 609/1000 
	 loss: 27.4723, MinusLogProbMetric: 27.4723, val_loss: 28.4381, val_MinusLogProbMetric: 28.4381

Epoch 609: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4723 - MinusLogProbMetric: 27.4723 - val_loss: 28.4381 - val_MinusLogProbMetric: 28.4381 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 610/1000
2023-09-28 23:32:17.232 
Epoch 610/1000 
	 loss: 27.4681, MinusLogProbMetric: 27.4681, val_loss: 28.4275, val_MinusLogProbMetric: 28.4275

Epoch 610: val_loss did not improve from 28.41794
196/196 - 40s - loss: 27.4681 - MinusLogProbMetric: 27.4681 - val_loss: 28.4275 - val_MinusLogProbMetric: 28.4275 - lr: 6.2500e-05 - 40s/epoch - 202ms/step
Epoch 611/1000
2023-09-28 23:33:00.185 
Epoch 611/1000 
	 loss: 27.4804, MinusLogProbMetric: 27.4804, val_loss: 28.4586, val_MinusLogProbMetric: 28.4586

Epoch 611: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4804 - MinusLogProbMetric: 27.4804 - val_loss: 28.4586 - val_MinusLogProbMetric: 28.4586 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 612/1000
2023-09-28 23:33:43.039 
Epoch 612/1000 
	 loss: 27.4733, MinusLogProbMetric: 27.4733, val_loss: 28.4422, val_MinusLogProbMetric: 28.4422

Epoch 612: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4733 - MinusLogProbMetric: 27.4733 - val_loss: 28.4422 - val_MinusLogProbMetric: 28.4422 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 613/1000
2023-09-28 23:34:24.335 
Epoch 613/1000 
	 loss: 27.4674, MinusLogProbMetric: 27.4674, val_loss: 28.4612, val_MinusLogProbMetric: 28.4612

Epoch 613: val_loss did not improve from 28.41794
196/196 - 41s - loss: 27.4674 - MinusLogProbMetric: 27.4674 - val_loss: 28.4612 - val_MinusLogProbMetric: 28.4612 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 614/1000
2023-09-28 23:35:07.043 
Epoch 614/1000 
	 loss: 27.4780, MinusLogProbMetric: 27.4780, val_loss: 28.4312, val_MinusLogProbMetric: 28.4312

Epoch 614: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4780 - MinusLogProbMetric: 27.4780 - val_loss: 28.4312 - val_MinusLogProbMetric: 28.4312 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 615/1000
2023-09-28 23:35:49.762 
Epoch 615/1000 
	 loss: 27.4697, MinusLogProbMetric: 27.4697, val_loss: 28.4519, val_MinusLogProbMetric: 28.4519

Epoch 615: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4697 - MinusLogProbMetric: 27.4697 - val_loss: 28.4519 - val_MinusLogProbMetric: 28.4519 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 616/1000
2023-09-28 23:36:32.659 
Epoch 616/1000 
	 loss: 27.4722, MinusLogProbMetric: 27.4722, val_loss: 28.4824, val_MinusLogProbMetric: 28.4824

Epoch 616: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4722 - MinusLogProbMetric: 27.4722 - val_loss: 28.4824 - val_MinusLogProbMetric: 28.4824 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 617/1000
2023-09-28 23:37:13.817 
Epoch 617/1000 
	 loss: 27.4740, MinusLogProbMetric: 27.4740, val_loss: 28.4637, val_MinusLogProbMetric: 28.4637

Epoch 617: val_loss did not improve from 28.41794
196/196 - 41s - loss: 27.4740 - MinusLogProbMetric: 27.4740 - val_loss: 28.4637 - val_MinusLogProbMetric: 28.4637 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 618/1000
2023-09-28 23:37:55.631 
Epoch 618/1000 
	 loss: 27.4833, MinusLogProbMetric: 27.4833, val_loss: 28.4345, val_MinusLogProbMetric: 28.4345

Epoch 618: val_loss did not improve from 28.41794
196/196 - 42s - loss: 27.4833 - MinusLogProbMetric: 27.4833 - val_loss: 28.4345 - val_MinusLogProbMetric: 28.4345 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 619/1000
2023-09-28 23:38:38.472 
Epoch 619/1000 
	 loss: 27.4753, MinusLogProbMetric: 27.4753, val_loss: 28.4262, val_MinusLogProbMetric: 28.4262

Epoch 619: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4753 - MinusLogProbMetric: 27.4753 - val_loss: 28.4262 - val_MinusLogProbMetric: 28.4262 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 620/1000
2023-09-28 23:39:21.259 
Epoch 620/1000 
	 loss: 27.4736, MinusLogProbMetric: 27.4736, val_loss: 28.4611, val_MinusLogProbMetric: 28.4611

Epoch 620: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4736 - MinusLogProbMetric: 27.4736 - val_loss: 28.4611 - val_MinusLogProbMetric: 28.4611 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 621/1000
2023-09-28 23:40:04.515 
Epoch 621/1000 
	 loss: 27.4860, MinusLogProbMetric: 27.4860, val_loss: 28.4458, val_MinusLogProbMetric: 28.4458

Epoch 621: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4860 - MinusLogProbMetric: 27.4860 - val_loss: 28.4458 - val_MinusLogProbMetric: 28.4458 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 622/1000
2023-09-28 23:40:47.214 
Epoch 622/1000 
	 loss: 27.4738, MinusLogProbMetric: 27.4738, val_loss: 28.4279, val_MinusLogProbMetric: 28.4279

Epoch 622: val_loss did not improve from 28.41794
196/196 - 43s - loss: 27.4738 - MinusLogProbMetric: 27.4738 - val_loss: 28.4279 - val_MinusLogProbMetric: 28.4279 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 623/1000
2023-09-28 23:41:30.417 
Epoch 623/1000 
	 loss: 27.4519, MinusLogProbMetric: 27.4519, val_loss: 28.4143, val_MinusLogProbMetric: 28.4143

Epoch 623: val_loss improved from 28.41794 to 28.41434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 44s - loss: 27.4519 - MinusLogProbMetric: 27.4519 - val_loss: 28.4143 - val_MinusLogProbMetric: 28.4143 - lr: 3.1250e-05 - 44s/epoch - 224ms/step
Epoch 624/1000
2023-09-28 23:42:13.994 
Epoch 624/1000 
	 loss: 27.4519, MinusLogProbMetric: 27.4519, val_loss: 28.4236, val_MinusLogProbMetric: 28.4236

Epoch 624: val_loss did not improve from 28.41434
196/196 - 43s - loss: 27.4519 - MinusLogProbMetric: 27.4519 - val_loss: 28.4236 - val_MinusLogProbMetric: 28.4236 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 625/1000
2023-09-28 23:42:56.735 
Epoch 625/1000 
	 loss: 27.4442, MinusLogProbMetric: 27.4442, val_loss: 28.4215, val_MinusLogProbMetric: 28.4215

Epoch 625: val_loss did not improve from 28.41434
196/196 - 43s - loss: 27.4442 - MinusLogProbMetric: 27.4442 - val_loss: 28.4215 - val_MinusLogProbMetric: 28.4215 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 626/1000
2023-09-28 23:43:38.899 
Epoch 626/1000 
	 loss: 27.4488, MinusLogProbMetric: 27.4488, val_loss: 28.4260, val_MinusLogProbMetric: 28.4260

Epoch 626: val_loss did not improve from 28.41434
196/196 - 42s - loss: 27.4488 - MinusLogProbMetric: 27.4488 - val_loss: 28.4260 - val_MinusLogProbMetric: 28.4260 - lr: 3.1250e-05 - 42s/epoch - 215ms/step
Epoch 627/1000
2023-09-28 23:44:20.326 
Epoch 627/1000 
	 loss: 27.4501, MinusLogProbMetric: 27.4501, val_loss: 28.4462, val_MinusLogProbMetric: 28.4462

Epoch 627: val_loss did not improve from 28.41434
196/196 - 41s - loss: 27.4501 - MinusLogProbMetric: 27.4501 - val_loss: 28.4462 - val_MinusLogProbMetric: 28.4462 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 628/1000
2023-09-28 23:45:03.758 
Epoch 628/1000 
	 loss: 27.4468, MinusLogProbMetric: 27.4468, val_loss: 28.4331, val_MinusLogProbMetric: 28.4331

Epoch 628: val_loss did not improve from 28.41434
196/196 - 43s - loss: 27.4468 - MinusLogProbMetric: 27.4468 - val_loss: 28.4331 - val_MinusLogProbMetric: 28.4331 - lr: 3.1250e-05 - 43s/epoch - 222ms/step
Epoch 629/1000
2023-09-28 23:45:46.861 
Epoch 629/1000 
	 loss: 27.4529, MinusLogProbMetric: 27.4529, val_loss: 28.4213, val_MinusLogProbMetric: 28.4213

Epoch 629: val_loss did not improve from 28.41434
196/196 - 43s - loss: 27.4529 - MinusLogProbMetric: 27.4529 - val_loss: 28.4213 - val_MinusLogProbMetric: 28.4213 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 630/1000
2023-09-28 23:46:30.004 
Epoch 630/1000 
	 loss: 27.4487, MinusLogProbMetric: 27.4487, val_loss: 28.4242, val_MinusLogProbMetric: 28.4242

Epoch 630: val_loss did not improve from 28.41434
196/196 - 43s - loss: 27.4487 - MinusLogProbMetric: 27.4487 - val_loss: 28.4242 - val_MinusLogProbMetric: 28.4242 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 631/1000
2023-09-28 23:47:12.774 
Epoch 631/1000 
	 loss: 27.4521, MinusLogProbMetric: 27.4521, val_loss: 28.4198, val_MinusLogProbMetric: 28.4198

Epoch 631: val_loss did not improve from 28.41434
196/196 - 43s - loss: 27.4521 - MinusLogProbMetric: 27.4521 - val_loss: 28.4198 - val_MinusLogProbMetric: 28.4198 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 632/1000
2023-09-28 23:47:54.481 
Epoch 632/1000 
	 loss: 27.4469, MinusLogProbMetric: 27.4469, val_loss: 28.4356, val_MinusLogProbMetric: 28.4356

Epoch 632: val_loss did not improve from 28.41434
196/196 - 42s - loss: 27.4469 - MinusLogProbMetric: 27.4469 - val_loss: 28.4356 - val_MinusLogProbMetric: 28.4356 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 633/1000
2023-09-28 23:48:37.679 
Epoch 633/1000 
	 loss: 27.4462, MinusLogProbMetric: 27.4462, val_loss: 28.4226, val_MinusLogProbMetric: 28.4226

Epoch 633: val_loss did not improve from 28.41434
196/196 - 43s - loss: 27.4462 - MinusLogProbMetric: 27.4462 - val_loss: 28.4226 - val_MinusLogProbMetric: 28.4226 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 634/1000
2023-09-28 23:49:19.716 
Epoch 634/1000 
	 loss: 27.4497, MinusLogProbMetric: 27.4497, val_loss: 28.4302, val_MinusLogProbMetric: 28.4302

Epoch 634: val_loss did not improve from 28.41434
196/196 - 42s - loss: 27.4497 - MinusLogProbMetric: 27.4497 - val_loss: 28.4302 - val_MinusLogProbMetric: 28.4302 - lr: 3.1250e-05 - 42s/epoch - 214ms/step
Epoch 635/1000
2023-09-28 23:50:01.981 
Epoch 635/1000 
	 loss: 27.4476, MinusLogProbMetric: 27.4476, val_loss: 28.4138, val_MinusLogProbMetric: 28.4138

Epoch 635: val_loss improved from 28.41434 to 28.41375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_324/weights/best_weights.h5
196/196 - 43s - loss: 27.4476 - MinusLogProbMetric: 27.4476 - val_loss: 28.4138 - val_MinusLogProbMetric: 28.4138 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 636/1000
2023-09-28 23:50:46.241 
Epoch 636/1000 
	 loss: 27.4451, MinusLogProbMetric: 27.4451, val_loss: 28.4194, val_MinusLogProbMetric: 28.4194

Epoch 636: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4451 - MinusLogProbMetric: 27.4451 - val_loss: 28.4194 - val_MinusLogProbMetric: 28.4194 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 637/1000
2023-09-28 23:51:27.668 
Epoch 637/1000 
	 loss: 27.4460, MinusLogProbMetric: 27.4460, val_loss: 28.4245, val_MinusLogProbMetric: 28.4245

Epoch 637: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4460 - MinusLogProbMetric: 27.4460 - val_loss: 28.4245 - val_MinusLogProbMetric: 28.4245 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 638/1000
2023-09-28 23:52:11.070 
Epoch 638/1000 
	 loss: 27.4504, MinusLogProbMetric: 27.4504, val_loss: 28.4353, val_MinusLogProbMetric: 28.4353

Epoch 638: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4504 - MinusLogProbMetric: 27.4504 - val_loss: 28.4353 - val_MinusLogProbMetric: 28.4353 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 639/1000
2023-09-28 23:52:52.848 
Epoch 639/1000 
	 loss: 27.4462, MinusLogProbMetric: 27.4462, val_loss: 28.4209, val_MinusLogProbMetric: 28.4209

Epoch 639: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4462 - MinusLogProbMetric: 27.4462 - val_loss: 28.4209 - val_MinusLogProbMetric: 28.4209 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 640/1000
2023-09-28 23:53:35.697 
Epoch 640/1000 
	 loss: 27.4469, MinusLogProbMetric: 27.4469, val_loss: 28.4191, val_MinusLogProbMetric: 28.4191

Epoch 640: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4469 - MinusLogProbMetric: 27.4469 - val_loss: 28.4191 - val_MinusLogProbMetric: 28.4191 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 641/1000
2023-09-28 23:54:18.807 
Epoch 641/1000 
	 loss: 27.4449, MinusLogProbMetric: 27.4449, val_loss: 28.4246, val_MinusLogProbMetric: 28.4246

Epoch 641: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4449 - MinusLogProbMetric: 27.4449 - val_loss: 28.4246 - val_MinusLogProbMetric: 28.4246 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 642/1000
2023-09-28 23:55:00.156 
Epoch 642/1000 
	 loss: 27.4474, MinusLogProbMetric: 27.4474, val_loss: 28.4223, val_MinusLogProbMetric: 28.4223

Epoch 642: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4474 - MinusLogProbMetric: 27.4474 - val_loss: 28.4223 - val_MinusLogProbMetric: 28.4223 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 643/1000
2023-09-28 23:55:42.804 
Epoch 643/1000 
	 loss: 27.4433, MinusLogProbMetric: 27.4433, val_loss: 28.4244, val_MinusLogProbMetric: 28.4244

Epoch 643: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4433 - MinusLogProbMetric: 27.4433 - val_loss: 28.4244 - val_MinusLogProbMetric: 28.4244 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 644/1000
2023-09-28 23:56:25.684 
Epoch 644/1000 
	 loss: 27.4430, MinusLogProbMetric: 27.4430, val_loss: 28.4296, val_MinusLogProbMetric: 28.4296

Epoch 644: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4430 - MinusLogProbMetric: 27.4430 - val_loss: 28.4296 - val_MinusLogProbMetric: 28.4296 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 645/1000
2023-09-28 23:57:07.891 
Epoch 645/1000 
	 loss: 27.4461, MinusLogProbMetric: 27.4461, val_loss: 28.4184, val_MinusLogProbMetric: 28.4184

Epoch 645: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4461 - MinusLogProbMetric: 27.4461 - val_loss: 28.4184 - val_MinusLogProbMetric: 28.4184 - lr: 3.1250e-05 - 42s/epoch - 215ms/step
Epoch 646/1000
2023-09-28 23:57:51.007 
Epoch 646/1000 
	 loss: 27.4395, MinusLogProbMetric: 27.4395, val_loss: 28.4256, val_MinusLogProbMetric: 28.4256

Epoch 646: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4395 - MinusLogProbMetric: 27.4395 - val_loss: 28.4256 - val_MinusLogProbMetric: 28.4256 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 647/1000
2023-09-28 23:58:33.706 
Epoch 647/1000 
	 loss: 27.4445, MinusLogProbMetric: 27.4445, val_loss: 28.4192, val_MinusLogProbMetric: 28.4192

Epoch 647: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4445 - MinusLogProbMetric: 27.4445 - val_loss: 28.4192 - val_MinusLogProbMetric: 28.4192 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 648/1000
2023-09-28 23:59:16.292 
Epoch 648/1000 
	 loss: 27.4439, MinusLogProbMetric: 27.4439, val_loss: 28.4218, val_MinusLogProbMetric: 28.4218

Epoch 648: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4439 - MinusLogProbMetric: 27.4439 - val_loss: 28.4218 - val_MinusLogProbMetric: 28.4218 - lr: 3.1250e-05 - 43s/epoch - 217ms/step
Epoch 649/1000
2023-09-28 23:59:58.587 
Epoch 649/1000 
	 loss: 27.4495, MinusLogProbMetric: 27.4495, val_loss: 28.4277, val_MinusLogProbMetric: 28.4277

Epoch 649: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4495 - MinusLogProbMetric: 27.4495 - val_loss: 28.4277 - val_MinusLogProbMetric: 28.4277 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 650/1000
2023-09-29 00:00:41.419 
Epoch 650/1000 
	 loss: 27.4456, MinusLogProbMetric: 27.4456, val_loss: 28.4253, val_MinusLogProbMetric: 28.4253

Epoch 650: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4456 - MinusLogProbMetric: 27.4456 - val_loss: 28.4253 - val_MinusLogProbMetric: 28.4253 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 651/1000
2023-09-29 00:01:24.290 
Epoch 651/1000 
	 loss: 27.4447, MinusLogProbMetric: 27.4447, val_loss: 28.4280, val_MinusLogProbMetric: 28.4280

Epoch 651: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4447 - MinusLogProbMetric: 27.4447 - val_loss: 28.4280 - val_MinusLogProbMetric: 28.4280 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 652/1000
2023-09-29 00:02:07.301 
Epoch 652/1000 
	 loss: 27.4440, MinusLogProbMetric: 27.4440, val_loss: 28.4408, val_MinusLogProbMetric: 28.4408

Epoch 652: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4440 - MinusLogProbMetric: 27.4440 - val_loss: 28.4408 - val_MinusLogProbMetric: 28.4408 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 653/1000
2023-09-29 00:02:50.410 
Epoch 653/1000 
	 loss: 27.4424, MinusLogProbMetric: 27.4424, val_loss: 28.4284, val_MinusLogProbMetric: 28.4284

Epoch 653: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4424 - MinusLogProbMetric: 27.4424 - val_loss: 28.4284 - val_MinusLogProbMetric: 28.4284 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 654/1000
2023-09-29 00:03:33.336 
Epoch 654/1000 
	 loss: 27.4491, MinusLogProbMetric: 27.4491, val_loss: 28.4297, val_MinusLogProbMetric: 28.4297

Epoch 654: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4491 - MinusLogProbMetric: 27.4491 - val_loss: 28.4297 - val_MinusLogProbMetric: 28.4297 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 655/1000
2023-09-29 00:04:16.468 
Epoch 655/1000 
	 loss: 27.4435, MinusLogProbMetric: 27.4435, val_loss: 28.4327, val_MinusLogProbMetric: 28.4327

Epoch 655: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4435 - MinusLogProbMetric: 27.4435 - val_loss: 28.4327 - val_MinusLogProbMetric: 28.4327 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 656/1000
2023-09-29 00:04:59.621 
Epoch 656/1000 
	 loss: 27.4461, MinusLogProbMetric: 27.4461, val_loss: 28.4198, val_MinusLogProbMetric: 28.4198

Epoch 656: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4461 - MinusLogProbMetric: 27.4461 - val_loss: 28.4198 - val_MinusLogProbMetric: 28.4198 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 657/1000
2023-09-29 00:05:41.342 
Epoch 657/1000 
	 loss: 27.4446, MinusLogProbMetric: 27.4446, val_loss: 28.4428, val_MinusLogProbMetric: 28.4428

Epoch 657: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4446 - MinusLogProbMetric: 27.4446 - val_loss: 28.4428 - val_MinusLogProbMetric: 28.4428 - lr: 3.1250e-05 - 42s/epoch - 213ms/step
Epoch 658/1000
2023-09-29 00:06:24.522 
Epoch 658/1000 
	 loss: 27.4539, MinusLogProbMetric: 27.4539, val_loss: 28.4211, val_MinusLogProbMetric: 28.4211

Epoch 658: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4539 - MinusLogProbMetric: 27.4539 - val_loss: 28.4211 - val_MinusLogProbMetric: 28.4211 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 659/1000
2023-09-29 00:07:07.914 
Epoch 659/1000 
	 loss: 27.4444, MinusLogProbMetric: 27.4444, val_loss: 28.4265, val_MinusLogProbMetric: 28.4265

Epoch 659: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4444 - MinusLogProbMetric: 27.4444 - val_loss: 28.4265 - val_MinusLogProbMetric: 28.4265 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 660/1000
2023-09-29 00:07:48.862 
Epoch 660/1000 
	 loss: 27.4461, MinusLogProbMetric: 27.4461, val_loss: 28.4204, val_MinusLogProbMetric: 28.4204

Epoch 660: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4461 - MinusLogProbMetric: 27.4461 - val_loss: 28.4204 - val_MinusLogProbMetric: 28.4204 - lr: 3.1250e-05 - 41s/epoch - 209ms/step
Epoch 661/1000
2023-09-29 00:08:31.056 
Epoch 661/1000 
	 loss: 27.4503, MinusLogProbMetric: 27.4503, val_loss: 28.4296, val_MinusLogProbMetric: 28.4296

Epoch 661: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4503 - MinusLogProbMetric: 27.4503 - val_loss: 28.4296 - val_MinusLogProbMetric: 28.4296 - lr: 3.1250e-05 - 42s/epoch - 215ms/step
Epoch 662/1000
2023-09-29 00:09:14.300 
Epoch 662/1000 
	 loss: 27.4413, MinusLogProbMetric: 27.4413, val_loss: 28.4216, val_MinusLogProbMetric: 28.4216

Epoch 662: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4413 - MinusLogProbMetric: 27.4413 - val_loss: 28.4216 - val_MinusLogProbMetric: 28.4216 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 663/1000
2023-09-29 00:09:56.636 
Epoch 663/1000 
	 loss: 27.4428, MinusLogProbMetric: 27.4428, val_loss: 28.4209, val_MinusLogProbMetric: 28.4209

Epoch 663: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4428 - MinusLogProbMetric: 27.4428 - val_loss: 28.4209 - val_MinusLogProbMetric: 28.4209 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 664/1000
2023-09-29 00:10:39.887 
Epoch 664/1000 
	 loss: 27.4411, MinusLogProbMetric: 27.4411, val_loss: 28.4261, val_MinusLogProbMetric: 28.4261

Epoch 664: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4411 - MinusLogProbMetric: 27.4411 - val_loss: 28.4261 - val_MinusLogProbMetric: 28.4261 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 665/1000
2023-09-29 00:11:22.555 
Epoch 665/1000 
	 loss: 27.4438, MinusLogProbMetric: 27.4438, val_loss: 28.4222, val_MinusLogProbMetric: 28.4222

Epoch 665: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4438 - MinusLogProbMetric: 27.4438 - val_loss: 28.4222 - val_MinusLogProbMetric: 28.4222 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 666/1000
2023-09-29 00:12:05.616 
Epoch 666/1000 
	 loss: 27.4440, MinusLogProbMetric: 27.4440, val_loss: 28.4220, val_MinusLogProbMetric: 28.4220

Epoch 666: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4440 - MinusLogProbMetric: 27.4440 - val_loss: 28.4220 - val_MinusLogProbMetric: 28.4220 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 667/1000
2023-09-29 00:12:48.634 
Epoch 667/1000 
	 loss: 27.4440, MinusLogProbMetric: 27.4440, val_loss: 28.4513, val_MinusLogProbMetric: 28.4513

Epoch 667: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4440 - MinusLogProbMetric: 27.4440 - val_loss: 28.4513 - val_MinusLogProbMetric: 28.4513 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 668/1000
2023-09-29 00:13:31.398 
Epoch 668/1000 
	 loss: 27.4436, MinusLogProbMetric: 27.4436, val_loss: 28.4498, val_MinusLogProbMetric: 28.4498

Epoch 668: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4436 - MinusLogProbMetric: 27.4436 - val_loss: 28.4498 - val_MinusLogProbMetric: 28.4498 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 669/1000
2023-09-29 00:14:14.342 
Epoch 669/1000 
	 loss: 27.4433, MinusLogProbMetric: 27.4433, val_loss: 28.4359, val_MinusLogProbMetric: 28.4359

Epoch 669: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4433 - MinusLogProbMetric: 27.4433 - val_loss: 28.4359 - val_MinusLogProbMetric: 28.4359 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 670/1000
2023-09-29 00:14:57.197 
Epoch 670/1000 
	 loss: 27.4449, MinusLogProbMetric: 27.4449, val_loss: 28.4283, val_MinusLogProbMetric: 28.4283

Epoch 670: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4449 - MinusLogProbMetric: 27.4449 - val_loss: 28.4283 - val_MinusLogProbMetric: 28.4283 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 671/1000
2023-09-29 00:15:39.389 
Epoch 671/1000 
	 loss: 27.4438, MinusLogProbMetric: 27.4438, val_loss: 28.4233, val_MinusLogProbMetric: 28.4233

Epoch 671: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4438 - MinusLogProbMetric: 27.4438 - val_loss: 28.4233 - val_MinusLogProbMetric: 28.4233 - lr: 3.1250e-05 - 42s/epoch - 215ms/step
Epoch 672/1000
2023-09-29 00:16:21.729 
Epoch 672/1000 
	 loss: 27.4437, MinusLogProbMetric: 27.4437, val_loss: 28.4237, val_MinusLogProbMetric: 28.4237

Epoch 672: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4437 - MinusLogProbMetric: 27.4437 - val_loss: 28.4237 - val_MinusLogProbMetric: 28.4237 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 673/1000
2023-09-29 00:17:03.986 
Epoch 673/1000 
	 loss: 27.4438, MinusLogProbMetric: 27.4438, val_loss: 28.4307, val_MinusLogProbMetric: 28.4307

Epoch 673: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4438 - MinusLogProbMetric: 27.4438 - val_loss: 28.4307 - val_MinusLogProbMetric: 28.4307 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 674/1000
2023-09-29 00:17:46.834 
Epoch 674/1000 
	 loss: 27.4433, MinusLogProbMetric: 27.4433, val_loss: 28.4198, val_MinusLogProbMetric: 28.4198

Epoch 674: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4433 - MinusLogProbMetric: 27.4433 - val_loss: 28.4198 - val_MinusLogProbMetric: 28.4198 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 675/1000
2023-09-29 00:18:26.288 
Epoch 675/1000 
	 loss: 27.4438, MinusLogProbMetric: 27.4438, val_loss: 28.4429, val_MinusLogProbMetric: 28.4429

Epoch 675: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4438 - MinusLogProbMetric: 27.4438 - val_loss: 28.4429 - val_MinusLogProbMetric: 28.4429 - lr: 3.1250e-05 - 39s/epoch - 201ms/step
Epoch 676/1000
2023-09-29 00:19:09.011 
Epoch 676/1000 
	 loss: 27.4446, MinusLogProbMetric: 27.4446, val_loss: 28.4291, val_MinusLogProbMetric: 28.4291

Epoch 676: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4446 - MinusLogProbMetric: 27.4446 - val_loss: 28.4291 - val_MinusLogProbMetric: 28.4291 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 677/1000
2023-09-29 00:19:49.861 
Epoch 677/1000 
	 loss: 27.4437, MinusLogProbMetric: 27.4437, val_loss: 28.4334, val_MinusLogProbMetric: 28.4334

Epoch 677: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4437 - MinusLogProbMetric: 27.4437 - val_loss: 28.4334 - val_MinusLogProbMetric: 28.4334 - lr: 3.1250e-05 - 41s/epoch - 208ms/step
Epoch 678/1000
2023-09-29 00:20:31.240 
Epoch 678/1000 
	 loss: 27.4378, MinusLogProbMetric: 27.4378, val_loss: 28.4257, val_MinusLogProbMetric: 28.4257

Epoch 678: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4378 - MinusLogProbMetric: 27.4378 - val_loss: 28.4257 - val_MinusLogProbMetric: 28.4257 - lr: 3.1250e-05 - 41s/epoch - 211ms/step
Epoch 679/1000
2023-09-29 00:21:12.412 
Epoch 679/1000 
	 loss: 27.4432, MinusLogProbMetric: 27.4432, val_loss: 28.4771, val_MinusLogProbMetric: 28.4771

Epoch 679: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4432 - MinusLogProbMetric: 27.4432 - val_loss: 28.4771 - val_MinusLogProbMetric: 28.4771 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 680/1000
2023-09-29 00:21:53.009 
Epoch 680/1000 
	 loss: 27.4386, MinusLogProbMetric: 27.4386, val_loss: 28.4340, val_MinusLogProbMetric: 28.4340

Epoch 680: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4386 - MinusLogProbMetric: 27.4386 - val_loss: 28.4340 - val_MinusLogProbMetric: 28.4340 - lr: 3.1250e-05 - 41s/epoch - 207ms/step
Epoch 681/1000
2023-09-29 00:22:34.512 
Epoch 681/1000 
	 loss: 27.4441, MinusLogProbMetric: 27.4441, val_loss: 28.4346, val_MinusLogProbMetric: 28.4346

Epoch 681: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4441 - MinusLogProbMetric: 27.4441 - val_loss: 28.4346 - val_MinusLogProbMetric: 28.4346 - lr: 3.1250e-05 - 41s/epoch - 212ms/step
Epoch 682/1000
2023-09-29 00:23:12.877 
Epoch 682/1000 
	 loss: 27.4427, MinusLogProbMetric: 27.4427, val_loss: 28.4220, val_MinusLogProbMetric: 28.4220

Epoch 682: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4427 - MinusLogProbMetric: 27.4427 - val_loss: 28.4220 - val_MinusLogProbMetric: 28.4220 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 683/1000
2023-09-29 00:23:51.256 
Epoch 683/1000 
	 loss: 27.4443, MinusLogProbMetric: 27.4443, val_loss: 28.4291, val_MinusLogProbMetric: 28.4291

Epoch 683: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4443 - MinusLogProbMetric: 27.4443 - val_loss: 28.4291 - val_MinusLogProbMetric: 28.4291 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 684/1000
2023-09-29 00:24:32.430 
Epoch 684/1000 
	 loss: 27.4452, MinusLogProbMetric: 27.4452, val_loss: 28.4421, val_MinusLogProbMetric: 28.4421

Epoch 684: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4452 - MinusLogProbMetric: 27.4452 - val_loss: 28.4421 - val_MinusLogProbMetric: 28.4421 - lr: 3.1250e-05 - 41s/epoch - 210ms/step
Epoch 685/1000
2023-09-29 00:25:10.846 
Epoch 685/1000 
	 loss: 27.4436, MinusLogProbMetric: 27.4436, val_loss: 28.4482, val_MinusLogProbMetric: 28.4482

Epoch 685: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4436 - MinusLogProbMetric: 27.4436 - val_loss: 28.4482 - val_MinusLogProbMetric: 28.4482 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 686/1000
2023-09-29 00:25:50.028 
Epoch 686/1000 
	 loss: 27.4319, MinusLogProbMetric: 27.4319, val_loss: 28.4252, val_MinusLogProbMetric: 28.4252

Epoch 686: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4319 - MinusLogProbMetric: 27.4319 - val_loss: 28.4252 - val_MinusLogProbMetric: 28.4252 - lr: 1.5625e-05 - 39s/epoch - 200ms/step
Epoch 687/1000
2023-09-29 00:26:31.356 
Epoch 687/1000 
	 loss: 27.4291, MinusLogProbMetric: 27.4291, val_loss: 28.4195, val_MinusLogProbMetric: 28.4195

Epoch 687: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4291 - MinusLogProbMetric: 27.4291 - val_loss: 28.4195 - val_MinusLogProbMetric: 28.4195 - lr: 1.5625e-05 - 41s/epoch - 211ms/step
Epoch 688/1000
2023-09-29 00:27:10.718 
Epoch 688/1000 
	 loss: 27.4284, MinusLogProbMetric: 27.4284, val_loss: 28.4353, val_MinusLogProbMetric: 28.4353

Epoch 688: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4284 - MinusLogProbMetric: 27.4284 - val_loss: 28.4353 - val_MinusLogProbMetric: 28.4353 - lr: 1.5625e-05 - 39s/epoch - 201ms/step
Epoch 689/1000
2023-09-29 00:27:50.383 
Epoch 689/1000 
	 loss: 27.4326, MinusLogProbMetric: 27.4326, val_loss: 28.4234, val_MinusLogProbMetric: 28.4234

Epoch 689: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4326 - MinusLogProbMetric: 27.4326 - val_loss: 28.4234 - val_MinusLogProbMetric: 28.4234 - lr: 1.5625e-05 - 40s/epoch - 202ms/step
Epoch 690/1000
2023-09-29 00:28:28.870 
Epoch 690/1000 
	 loss: 27.4300, MinusLogProbMetric: 27.4300, val_loss: 28.4215, val_MinusLogProbMetric: 28.4215

Epoch 690: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4300 - MinusLogProbMetric: 27.4300 - val_loss: 28.4215 - val_MinusLogProbMetric: 28.4215 - lr: 1.5625e-05 - 38s/epoch - 196ms/step
Epoch 691/1000
2023-09-29 00:29:06.738 
Epoch 691/1000 
	 loss: 27.4286, MinusLogProbMetric: 27.4286, val_loss: 28.4357, val_MinusLogProbMetric: 28.4357

Epoch 691: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4286 - MinusLogProbMetric: 27.4286 - val_loss: 28.4357 - val_MinusLogProbMetric: 28.4357 - lr: 1.5625e-05 - 38s/epoch - 193ms/step
Epoch 692/1000
2023-09-29 00:29:46.415 
Epoch 692/1000 
	 loss: 27.4278, MinusLogProbMetric: 27.4278, val_loss: 28.4251, val_MinusLogProbMetric: 28.4251

Epoch 692: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4278 - MinusLogProbMetric: 27.4278 - val_loss: 28.4251 - val_MinusLogProbMetric: 28.4251 - lr: 1.5625e-05 - 40s/epoch - 202ms/step
Epoch 693/1000
2023-09-29 00:30:26.151 
Epoch 693/1000 
	 loss: 27.4315, MinusLogProbMetric: 27.4315, val_loss: 28.4243, val_MinusLogProbMetric: 28.4243

Epoch 693: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4315 - MinusLogProbMetric: 27.4315 - val_loss: 28.4243 - val_MinusLogProbMetric: 28.4243 - lr: 1.5625e-05 - 40s/epoch - 203ms/step
Epoch 694/1000
2023-09-29 00:31:05.537 
Epoch 694/1000 
	 loss: 27.4286, MinusLogProbMetric: 27.4286, val_loss: 28.4139, val_MinusLogProbMetric: 28.4139

Epoch 694: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4286 - MinusLogProbMetric: 27.4286 - val_loss: 28.4139 - val_MinusLogProbMetric: 28.4139 - lr: 1.5625e-05 - 39s/epoch - 201ms/step
Epoch 695/1000
2023-09-29 00:31:46.097 
Epoch 695/1000 
	 loss: 27.4274, MinusLogProbMetric: 27.4274, val_loss: 28.4247, val_MinusLogProbMetric: 28.4247

Epoch 695: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4274 - MinusLogProbMetric: 27.4274 - val_loss: 28.4247 - val_MinusLogProbMetric: 28.4247 - lr: 1.5625e-05 - 41s/epoch - 207ms/step
Epoch 696/1000
2023-09-29 00:32:28.313 
Epoch 696/1000 
	 loss: 27.4291, MinusLogProbMetric: 27.4291, val_loss: 28.4150, val_MinusLogProbMetric: 28.4150

Epoch 696: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4291 - MinusLogProbMetric: 27.4291 - val_loss: 28.4150 - val_MinusLogProbMetric: 28.4150 - lr: 1.5625e-05 - 42s/epoch - 215ms/step
Epoch 697/1000
2023-09-29 00:33:05.193 
Epoch 697/1000 
	 loss: 27.4289, MinusLogProbMetric: 27.4289, val_loss: 28.4176, val_MinusLogProbMetric: 28.4176

Epoch 697: val_loss did not improve from 28.41375
196/196 - 37s - loss: 27.4289 - MinusLogProbMetric: 27.4289 - val_loss: 28.4176 - val_MinusLogProbMetric: 28.4176 - lr: 1.5625e-05 - 37s/epoch - 188ms/step
Epoch 698/1000
2023-09-29 00:33:43.740 
Epoch 698/1000 
	 loss: 27.4300, MinusLogProbMetric: 27.4300, val_loss: 28.4201, val_MinusLogProbMetric: 28.4201

Epoch 698: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4300 - MinusLogProbMetric: 27.4300 - val_loss: 28.4201 - val_MinusLogProbMetric: 28.4201 - lr: 1.5625e-05 - 39s/epoch - 197ms/step
Epoch 699/1000
2023-09-29 00:34:21.589 
Epoch 699/1000 
	 loss: 27.4295, MinusLogProbMetric: 27.4295, val_loss: 28.4177, val_MinusLogProbMetric: 28.4177

Epoch 699: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4295 - MinusLogProbMetric: 27.4295 - val_loss: 28.4177 - val_MinusLogProbMetric: 28.4177 - lr: 1.5625e-05 - 38s/epoch - 193ms/step
Epoch 700/1000
2023-09-29 00:35:02.819 
Epoch 700/1000 
	 loss: 27.4275, MinusLogProbMetric: 27.4275, val_loss: 28.4159, val_MinusLogProbMetric: 28.4159

Epoch 700: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4275 - MinusLogProbMetric: 27.4275 - val_loss: 28.4159 - val_MinusLogProbMetric: 28.4159 - lr: 1.5625e-05 - 41s/epoch - 210ms/step
Epoch 701/1000
2023-09-29 00:35:45.274 
Epoch 701/1000 
	 loss: 27.4296, MinusLogProbMetric: 27.4296, val_loss: 28.4182, val_MinusLogProbMetric: 28.4182

Epoch 701: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4296 - MinusLogProbMetric: 27.4296 - val_loss: 28.4182 - val_MinusLogProbMetric: 28.4182 - lr: 1.5625e-05 - 42s/epoch - 217ms/step
Epoch 702/1000
2023-09-29 00:36:26.263 
Epoch 702/1000 
	 loss: 27.4294, MinusLogProbMetric: 27.4294, val_loss: 28.4256, val_MinusLogProbMetric: 28.4256

Epoch 702: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4294 - MinusLogProbMetric: 27.4294 - val_loss: 28.4256 - val_MinusLogProbMetric: 28.4256 - lr: 1.5625e-05 - 41s/epoch - 209ms/step
Epoch 703/1000
2023-09-29 00:37:07.596 
Epoch 703/1000 
	 loss: 27.4281, MinusLogProbMetric: 27.4281, val_loss: 28.4186, val_MinusLogProbMetric: 28.4186

Epoch 703: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4281 - MinusLogProbMetric: 27.4281 - val_loss: 28.4186 - val_MinusLogProbMetric: 28.4186 - lr: 1.5625e-05 - 41s/epoch - 211ms/step
Epoch 704/1000
2023-09-29 00:37:48.074 
Epoch 704/1000 
	 loss: 27.4301, MinusLogProbMetric: 27.4301, val_loss: 28.4230, val_MinusLogProbMetric: 28.4230

Epoch 704: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4301 - MinusLogProbMetric: 27.4301 - val_loss: 28.4230 - val_MinusLogProbMetric: 28.4230 - lr: 1.5625e-05 - 40s/epoch - 207ms/step
Epoch 705/1000
2023-09-29 00:38:24.282 
Epoch 705/1000 
	 loss: 27.4267, MinusLogProbMetric: 27.4267, val_loss: 28.4228, val_MinusLogProbMetric: 28.4228

Epoch 705: val_loss did not improve from 28.41375
196/196 - 36s - loss: 27.4267 - MinusLogProbMetric: 27.4267 - val_loss: 28.4228 - val_MinusLogProbMetric: 28.4228 - lr: 1.5625e-05 - 36s/epoch - 185ms/step
Epoch 706/1000
2023-09-29 00:39:02.540 
Epoch 706/1000 
	 loss: 27.4288, MinusLogProbMetric: 27.4288, val_loss: 28.4266, val_MinusLogProbMetric: 28.4266

Epoch 706: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4288 - MinusLogProbMetric: 27.4288 - val_loss: 28.4266 - val_MinusLogProbMetric: 28.4266 - lr: 1.5625e-05 - 38s/epoch - 195ms/step
Epoch 707/1000
2023-09-29 00:39:43.752 
Epoch 707/1000 
	 loss: 27.4279, MinusLogProbMetric: 27.4279, val_loss: 28.4142, val_MinusLogProbMetric: 28.4142

Epoch 707: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4279 - MinusLogProbMetric: 27.4279 - val_loss: 28.4142 - val_MinusLogProbMetric: 28.4142 - lr: 1.5625e-05 - 41s/epoch - 210ms/step
Epoch 708/1000
2023-09-29 00:40:26.036 
Epoch 708/1000 
	 loss: 27.4293, MinusLogProbMetric: 27.4293, val_loss: 28.4197, val_MinusLogProbMetric: 28.4197

Epoch 708: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4293 - MinusLogProbMetric: 27.4293 - val_loss: 28.4197 - val_MinusLogProbMetric: 28.4197 - lr: 1.5625e-05 - 42s/epoch - 216ms/step
Epoch 709/1000
2023-09-29 00:41:06.235 
Epoch 709/1000 
	 loss: 27.4273, MinusLogProbMetric: 27.4273, val_loss: 28.4265, val_MinusLogProbMetric: 28.4265

Epoch 709: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4273 - MinusLogProbMetric: 27.4273 - val_loss: 28.4265 - val_MinusLogProbMetric: 28.4265 - lr: 1.5625e-05 - 40s/epoch - 205ms/step
Epoch 710/1000
2023-09-29 00:41:43.745 
Epoch 710/1000 
	 loss: 27.4275, MinusLogProbMetric: 27.4275, val_loss: 28.4328, val_MinusLogProbMetric: 28.4328

Epoch 710: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4275 - MinusLogProbMetric: 27.4275 - val_loss: 28.4328 - val_MinusLogProbMetric: 28.4328 - lr: 1.5625e-05 - 38s/epoch - 191ms/step
Epoch 711/1000
2023-09-29 00:42:25.388 
Epoch 711/1000 
	 loss: 27.4266, MinusLogProbMetric: 27.4266, val_loss: 28.4192, val_MinusLogProbMetric: 28.4192

Epoch 711: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4266 - MinusLogProbMetric: 27.4266 - val_loss: 28.4192 - val_MinusLogProbMetric: 28.4192 - lr: 1.5625e-05 - 42s/epoch - 212ms/step
Epoch 712/1000
2023-09-29 00:43:04.152 
Epoch 712/1000 
	 loss: 27.4278, MinusLogProbMetric: 27.4278, val_loss: 28.4223, val_MinusLogProbMetric: 28.4223

Epoch 712: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4278 - MinusLogProbMetric: 27.4278 - val_loss: 28.4223 - val_MinusLogProbMetric: 28.4223 - lr: 1.5625e-05 - 39s/epoch - 198ms/step
Epoch 713/1000
2023-09-29 00:43:43.995 
Epoch 713/1000 
	 loss: 27.4273, MinusLogProbMetric: 27.4273, val_loss: 28.4192, val_MinusLogProbMetric: 28.4192

Epoch 713: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4273 - MinusLogProbMetric: 27.4273 - val_loss: 28.4192 - val_MinusLogProbMetric: 28.4192 - lr: 1.5625e-05 - 40s/epoch - 203ms/step
Epoch 714/1000
2023-09-29 00:44:23.411 
Epoch 714/1000 
	 loss: 27.4273, MinusLogProbMetric: 27.4273, val_loss: 28.4321, val_MinusLogProbMetric: 28.4321

Epoch 714: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4273 - MinusLogProbMetric: 27.4273 - val_loss: 28.4321 - val_MinusLogProbMetric: 28.4321 - lr: 1.5625e-05 - 39s/epoch - 201ms/step
Epoch 715/1000
2023-09-29 00:45:01.248 
Epoch 715/1000 
	 loss: 27.4289, MinusLogProbMetric: 27.4289, val_loss: 28.4249, val_MinusLogProbMetric: 28.4249

Epoch 715: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4289 - MinusLogProbMetric: 27.4289 - val_loss: 28.4249 - val_MinusLogProbMetric: 28.4249 - lr: 1.5625e-05 - 38s/epoch - 193ms/step
Epoch 716/1000
2023-09-29 00:45:44.017 
Epoch 716/1000 
	 loss: 27.4293, MinusLogProbMetric: 27.4293, val_loss: 28.4254, val_MinusLogProbMetric: 28.4254

Epoch 716: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4293 - MinusLogProbMetric: 27.4293 - val_loss: 28.4254 - val_MinusLogProbMetric: 28.4254 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 717/1000
2023-09-29 00:46:26.375 
Epoch 717/1000 
	 loss: 27.4260, MinusLogProbMetric: 27.4260, val_loss: 28.4254, val_MinusLogProbMetric: 28.4254

Epoch 717: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4260 - MinusLogProbMetric: 27.4260 - val_loss: 28.4254 - val_MinusLogProbMetric: 28.4254 - lr: 1.5625e-05 - 42s/epoch - 216ms/step
Epoch 718/1000
2023-09-29 00:47:09.231 
Epoch 718/1000 
	 loss: 27.4278, MinusLogProbMetric: 27.4278, val_loss: 28.4233, val_MinusLogProbMetric: 28.4233

Epoch 718: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4278 - MinusLogProbMetric: 27.4278 - val_loss: 28.4233 - val_MinusLogProbMetric: 28.4233 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 719/1000
2023-09-29 00:47:52.281 
Epoch 719/1000 
	 loss: 27.4261, MinusLogProbMetric: 27.4261, val_loss: 28.4174, val_MinusLogProbMetric: 28.4174

Epoch 719: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4261 - MinusLogProbMetric: 27.4261 - val_loss: 28.4174 - val_MinusLogProbMetric: 28.4174 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 720/1000
2023-09-29 00:48:35.543 
Epoch 720/1000 
	 loss: 27.4249, MinusLogProbMetric: 27.4249, val_loss: 28.4270, val_MinusLogProbMetric: 28.4270

Epoch 720: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4249 - MinusLogProbMetric: 27.4249 - val_loss: 28.4270 - val_MinusLogProbMetric: 28.4270 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 721/1000
2023-09-29 00:49:18.729 
Epoch 721/1000 
	 loss: 27.4260, MinusLogProbMetric: 27.4260, val_loss: 28.4267, val_MinusLogProbMetric: 28.4267

Epoch 721: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4260 - MinusLogProbMetric: 27.4260 - val_loss: 28.4267 - val_MinusLogProbMetric: 28.4267 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 722/1000
2023-09-29 00:50:01.723 
Epoch 722/1000 
	 loss: 27.4258, MinusLogProbMetric: 27.4258, val_loss: 28.4252, val_MinusLogProbMetric: 28.4252

Epoch 722: val_loss did not improve from 28.41375
196/196 - 43s - loss: 27.4258 - MinusLogProbMetric: 27.4258 - val_loss: 28.4252 - val_MinusLogProbMetric: 28.4252 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 723/1000
2023-09-29 00:50:41.744 
Epoch 723/1000 
	 loss: 27.4250, MinusLogProbMetric: 27.4250, val_loss: 28.4177, val_MinusLogProbMetric: 28.4177

Epoch 723: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4250 - MinusLogProbMetric: 27.4250 - val_loss: 28.4177 - val_MinusLogProbMetric: 28.4177 - lr: 1.5625e-05 - 40s/epoch - 204ms/step
Epoch 724/1000
2023-09-29 00:51:22.044 
Epoch 724/1000 
	 loss: 27.4244, MinusLogProbMetric: 27.4244, val_loss: 28.4171, val_MinusLogProbMetric: 28.4171

Epoch 724: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4244 - MinusLogProbMetric: 27.4244 - val_loss: 28.4171 - val_MinusLogProbMetric: 28.4171 - lr: 1.5625e-05 - 40s/epoch - 206ms/step
Epoch 725/1000
2023-09-29 00:52:00.731 
Epoch 725/1000 
	 loss: 27.4255, MinusLogProbMetric: 27.4255, val_loss: 28.4225, val_MinusLogProbMetric: 28.4225

Epoch 725: val_loss did not improve from 28.41375
196/196 - 39s - loss: 27.4255 - MinusLogProbMetric: 27.4255 - val_loss: 28.4225 - val_MinusLogProbMetric: 28.4225 - lr: 1.5625e-05 - 39s/epoch - 197ms/step
Epoch 726/1000
2023-09-29 00:52:42.064 
Epoch 726/1000 
	 loss: 27.4264, MinusLogProbMetric: 27.4264, val_loss: 28.4221, val_MinusLogProbMetric: 28.4221

Epoch 726: val_loss did not improve from 28.41375
196/196 - 41s - loss: 27.4264 - MinusLogProbMetric: 27.4264 - val_loss: 28.4221 - val_MinusLogProbMetric: 28.4221 - lr: 1.5625e-05 - 41s/epoch - 211ms/step
Epoch 727/1000
2023-09-29 00:53:21.769 
Epoch 727/1000 
	 loss: 27.4246, MinusLogProbMetric: 27.4246, val_loss: 28.4196, val_MinusLogProbMetric: 28.4196

Epoch 727: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4246 - MinusLogProbMetric: 27.4246 - val_loss: 28.4196 - val_MinusLogProbMetric: 28.4196 - lr: 1.5625e-05 - 40s/epoch - 203ms/step
Epoch 728/1000
2023-09-29 00:53:59.730 
Epoch 728/1000 
	 loss: 27.4253, MinusLogProbMetric: 27.4253, val_loss: 28.4269, val_MinusLogProbMetric: 28.4269

Epoch 728: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4253 - MinusLogProbMetric: 27.4253 - val_loss: 28.4269 - val_MinusLogProbMetric: 28.4269 - lr: 1.5625e-05 - 38s/epoch - 194ms/step
Epoch 729/1000
2023-09-29 00:54:40.144 
Epoch 729/1000 
	 loss: 27.4257, MinusLogProbMetric: 27.4257, val_loss: 28.4183, val_MinusLogProbMetric: 28.4183

Epoch 729: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4257 - MinusLogProbMetric: 27.4257 - val_loss: 28.4183 - val_MinusLogProbMetric: 28.4183 - lr: 1.5625e-05 - 40s/epoch - 206ms/step
Epoch 730/1000
2023-09-29 00:55:18.523 
Epoch 730/1000 
	 loss: 27.4234, MinusLogProbMetric: 27.4234, val_loss: 28.4257, val_MinusLogProbMetric: 28.4257

Epoch 730: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4234 - MinusLogProbMetric: 27.4234 - val_loss: 28.4257 - val_MinusLogProbMetric: 28.4257 - lr: 1.5625e-05 - 38s/epoch - 196ms/step
Epoch 731/1000
2023-09-29 00:55:58.035 
Epoch 731/1000 
	 loss: 27.4251, MinusLogProbMetric: 27.4251, val_loss: 28.4147, val_MinusLogProbMetric: 28.4147

Epoch 731: val_loss did not improve from 28.41375
196/196 - 40s - loss: 27.4251 - MinusLogProbMetric: 27.4251 - val_loss: 28.4147 - val_MinusLogProbMetric: 28.4147 - lr: 1.5625e-05 - 40s/epoch - 202ms/step
Epoch 732/1000
2023-09-29 00:56:36.188 
Epoch 732/1000 
	 loss: 27.4253, MinusLogProbMetric: 27.4253, val_loss: 28.4315, val_MinusLogProbMetric: 28.4315

Epoch 732: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4253 - MinusLogProbMetric: 27.4253 - val_loss: 28.4315 - val_MinusLogProbMetric: 28.4315 - lr: 1.5625e-05 - 38s/epoch - 195ms/step
Epoch 733/1000
2023-09-29 00:57:17.917 
Epoch 733/1000 
	 loss: 27.4256, MinusLogProbMetric: 27.4256, val_loss: 28.4201, val_MinusLogProbMetric: 28.4201

Epoch 733: val_loss did not improve from 28.41375
196/196 - 42s - loss: 27.4256 - MinusLogProbMetric: 27.4256 - val_loss: 28.4201 - val_MinusLogProbMetric: 28.4201 - lr: 1.5625e-05 - 42s/epoch - 213ms/step
Epoch 734/1000
2023-09-29 00:57:55.847 
Epoch 734/1000 
	 loss: 27.4244, MinusLogProbMetric: 27.4244, val_loss: 28.4188, val_MinusLogProbMetric: 28.4188

Epoch 734: val_loss did not improve from 28.41375
196/196 - 38s - loss: 27.4244 - MinusLogProbMetric: 27.4244 - val_loss: 28.4188 - val_MinusLogProbMetric: 28.4188 - lr: 1.5625e-05 - 38s/epoch - 194ms/step
Epoch 735/1000
2023-09-29 00:58:34.559 
Epoch 735/1000 
	 loss: 27.4250, MinusLogProbMetric: 27.4250, val_loss: 28.4167, val_MinusLogProbMetric: 28.4167

Epoch 735: val_loss did not improve from 28.41375
Restoring model weights from the end of the best epoch: 635.
196/196 - 39s - loss: 27.4250 - MinusLogProbMetric: 27.4250 - val_loss: 28.4167 - val_MinusLogProbMetric: 28.4167 - lr: 1.5625e-05 - 39s/epoch - 200ms/step
Epoch 735: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7fc08416ab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 18.632588178967126 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7fc0841685e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 14.03809018805623 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7fc084169fc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 8.046390398987569 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7fc08416b1c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 8.781257131020539 seconds.
Training succeeded with seed 0.
Model trained in 29806.88 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 50.77 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 51.01 s.
===========
Run 324/720 done in 29863.24 s.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

===========
Generating train data for run 330.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_330/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_330/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_330/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_330
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_83"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_84 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1645920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7fc1ef2eae60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc1ef095840>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc1ef095840>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc57dae1270>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc42c1ffbe0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc42c1ffa90>, <keras.callbacks.ModelCheckpoint object at 0x7fc42c1ffac0>, <keras.callbacks.EarlyStopping object at 0x7fc42c1ff6a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc42c1ff7c0>, <keras.callbacks.TerminateOnNaN object at 0x7fc42c1ff160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_330/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 330/720 with hyperparameters:
timestamp = 2023-09-29 00:59:30.315990
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 01:00:57.066 
Epoch 1/1000 
	 loss: 388.6265, MinusLogProbMetric: 388.6265, val_loss: 78.8783, val_MinusLogProbMetric: 78.8783

Epoch 1: val_loss improved from inf to 78.87826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 87s - loss: 388.6265 - MinusLogProbMetric: 388.6265 - val_loss: 78.8783 - val_MinusLogProbMetric: 78.8783 - lr: 0.0010 - 87s/epoch - 444ms/step
Epoch 2/1000
2023-09-29 01:01:30.889 
Epoch 2/1000 
	 loss: 63.8024, MinusLogProbMetric: 63.8024, val_loss: 54.8750, val_MinusLogProbMetric: 54.8750

Epoch 2: val_loss improved from 78.87826 to 54.87497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 63.8024 - MinusLogProbMetric: 63.8024 - val_loss: 54.8750 - val_MinusLogProbMetric: 54.8750 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 3/1000
2023-09-29 01:02:03.011 
Epoch 3/1000 
	 loss: 52.1065, MinusLogProbMetric: 52.1065, val_loss: 50.3908, val_MinusLogProbMetric: 50.3908

Epoch 3: val_loss improved from 54.87497 to 50.39078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 32s - loss: 52.1065 - MinusLogProbMetric: 52.1065 - val_loss: 50.3908 - val_MinusLogProbMetric: 50.3908 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 4/1000
2023-09-29 01:02:37.057 
Epoch 4/1000 
	 loss: 46.4556, MinusLogProbMetric: 46.4556, val_loss: 45.8755, val_MinusLogProbMetric: 45.8755

Epoch 4: val_loss improved from 50.39078 to 45.87547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 46.4556 - MinusLogProbMetric: 46.4556 - val_loss: 45.8755 - val_MinusLogProbMetric: 45.8755 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 5/1000
2023-09-29 01:03:11.213 
Epoch 5/1000 
	 loss: 43.7089, MinusLogProbMetric: 43.7089, val_loss: 43.7875, val_MinusLogProbMetric: 43.7875

Epoch 5: val_loss improved from 45.87547 to 43.78748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 43.7089 - MinusLogProbMetric: 43.7089 - val_loss: 43.7875 - val_MinusLogProbMetric: 43.7875 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 6/1000
2023-09-29 01:03:44.259 
Epoch 6/1000 
	 loss: 41.5444, MinusLogProbMetric: 41.5444, val_loss: 41.0728, val_MinusLogProbMetric: 41.0728

Epoch 6: val_loss improved from 43.78748 to 41.07278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 41.5444 - MinusLogProbMetric: 41.5444 - val_loss: 41.0728 - val_MinusLogProbMetric: 41.0728 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 7/1000
2023-09-29 01:04:17.231 
Epoch 7/1000 
	 loss: 40.6277, MinusLogProbMetric: 40.6277, val_loss: 39.6882, val_MinusLogProbMetric: 39.6882

Epoch 7: val_loss improved from 41.07278 to 39.68818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 40.6277 - MinusLogProbMetric: 40.6277 - val_loss: 39.6882 - val_MinusLogProbMetric: 39.6882 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 8/1000
2023-09-29 01:04:51.947 
Epoch 8/1000 
	 loss: 38.9080, MinusLogProbMetric: 38.9080, val_loss: 38.0033, val_MinusLogProbMetric: 38.0033

Epoch 8: val_loss improved from 39.68818 to 38.00330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 35s - loss: 38.9080 - MinusLogProbMetric: 38.9080 - val_loss: 38.0033 - val_MinusLogProbMetric: 38.0033 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 9/1000
2023-09-29 01:05:27.091 
Epoch 9/1000 
	 loss: 38.3393, MinusLogProbMetric: 38.3393, val_loss: 37.9977, val_MinusLogProbMetric: 37.9977

Epoch 9: val_loss improved from 38.00330 to 37.99771, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 35s - loss: 38.3393 - MinusLogProbMetric: 38.3393 - val_loss: 37.9977 - val_MinusLogProbMetric: 37.9977 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 10/1000
2023-09-29 01:06:01.772 
Epoch 10/1000 
	 loss: 37.9939, MinusLogProbMetric: 37.9939, val_loss: 38.6820, val_MinusLogProbMetric: 38.6820

Epoch 10: val_loss did not improve from 37.99771
196/196 - 34s - loss: 37.9939 - MinusLogProbMetric: 37.9939 - val_loss: 38.6820 - val_MinusLogProbMetric: 38.6820 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 11/1000
2023-09-29 01:06:34.622 
Epoch 11/1000 
	 loss: 36.8572, MinusLogProbMetric: 36.8572, val_loss: 37.2891, val_MinusLogProbMetric: 37.2891

Epoch 11: val_loss improved from 37.99771 to 37.28906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 36.8572 - MinusLogProbMetric: 36.8572 - val_loss: 37.2891 - val_MinusLogProbMetric: 37.2891 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 12/1000
2023-09-29 01:07:07.858 
Epoch 12/1000 
	 loss: 36.6933, MinusLogProbMetric: 36.6933, val_loss: 36.3077, val_MinusLogProbMetric: 36.3077

Epoch 12: val_loss improved from 37.28906 to 36.30773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 36.6933 - MinusLogProbMetric: 36.6933 - val_loss: 36.3077 - val_MinusLogProbMetric: 36.3077 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 13/1000
2023-09-29 01:07:41.941 
Epoch 13/1000 
	 loss: 36.3178, MinusLogProbMetric: 36.3178, val_loss: 37.5264, val_MinusLogProbMetric: 37.5264

Epoch 13: val_loss did not improve from 36.30773
196/196 - 34s - loss: 36.3178 - MinusLogProbMetric: 36.3178 - val_loss: 37.5264 - val_MinusLogProbMetric: 37.5264 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 14/1000
2023-09-29 01:08:12.923 
Epoch 14/1000 
	 loss: 36.1092, MinusLogProbMetric: 36.1092, val_loss: 35.4186, val_MinusLogProbMetric: 35.4186

Epoch 14: val_loss improved from 36.30773 to 35.41864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 32s - loss: 36.1092 - MinusLogProbMetric: 36.1092 - val_loss: 35.4186 - val_MinusLogProbMetric: 35.4186 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 15/1000
2023-09-29 01:08:45.606 
Epoch 15/1000 
	 loss: 35.4563, MinusLogProbMetric: 35.4563, val_loss: 36.1190, val_MinusLogProbMetric: 36.1190

Epoch 15: val_loss did not improve from 35.41864
196/196 - 32s - loss: 35.4563 - MinusLogProbMetric: 35.4563 - val_loss: 36.1190 - val_MinusLogProbMetric: 36.1190 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 16/1000
2023-09-29 01:09:13.684 
Epoch 16/1000 
	 loss: 35.0865, MinusLogProbMetric: 35.0865, val_loss: 34.8587, val_MinusLogProbMetric: 34.8587

Epoch 16: val_loss improved from 35.41864 to 34.85870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 29s - loss: 35.0865 - MinusLogProbMetric: 35.0865 - val_loss: 34.8587 - val_MinusLogProbMetric: 34.8587 - lr: 0.0010 - 29s/epoch - 145ms/step
Epoch 17/1000
2023-09-29 01:09:41.737 
Epoch 17/1000 
	 loss: 34.9929, MinusLogProbMetric: 34.9929, val_loss: 34.2733, val_MinusLogProbMetric: 34.2733

Epoch 17: val_loss improved from 34.85870 to 34.27332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 28s - loss: 34.9929 - MinusLogProbMetric: 34.9929 - val_loss: 34.2733 - val_MinusLogProbMetric: 34.2733 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 18/1000
2023-09-29 01:10:11.275 
Epoch 18/1000 
	 loss: 34.7733, MinusLogProbMetric: 34.7733, val_loss: 34.1357, val_MinusLogProbMetric: 34.1357

Epoch 18: val_loss improved from 34.27332 to 34.13571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 30s - loss: 34.7733 - MinusLogProbMetric: 34.7733 - val_loss: 34.1357 - val_MinusLogProbMetric: 34.1357 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 19/1000
2023-09-29 01:10:40.856 
Epoch 19/1000 
	 loss: 34.6494, MinusLogProbMetric: 34.6494, val_loss: 35.9906, val_MinusLogProbMetric: 35.9906

Epoch 19: val_loss did not improve from 34.13571
196/196 - 29s - loss: 34.6494 - MinusLogProbMetric: 34.6494 - val_loss: 35.9906 - val_MinusLogProbMetric: 35.9906 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 20/1000
2023-09-29 01:11:10.071 
Epoch 20/1000 
	 loss: 34.3187, MinusLogProbMetric: 34.3187, val_loss: 34.7269, val_MinusLogProbMetric: 34.7269

Epoch 20: val_loss did not improve from 34.13571
196/196 - 29s - loss: 34.3187 - MinusLogProbMetric: 34.3187 - val_loss: 34.7269 - val_MinusLogProbMetric: 34.7269 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 21/1000
2023-09-29 01:11:38.310 
Epoch 21/1000 
	 loss: 34.1150, MinusLogProbMetric: 34.1150, val_loss: 35.0410, val_MinusLogProbMetric: 35.0410

Epoch 21: val_loss did not improve from 34.13571
196/196 - 28s - loss: 34.1150 - MinusLogProbMetric: 34.1150 - val_loss: 35.0410 - val_MinusLogProbMetric: 35.0410 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 22/1000
2023-09-29 01:12:06.103 
Epoch 22/1000 
	 loss: 33.8992, MinusLogProbMetric: 33.8992, val_loss: 33.2094, val_MinusLogProbMetric: 33.2094

Epoch 22: val_loss improved from 34.13571 to 33.20940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 28s - loss: 33.8992 - MinusLogProbMetric: 33.8992 - val_loss: 33.2094 - val_MinusLogProbMetric: 33.2094 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 23/1000
2023-09-29 01:12:34.615 
Epoch 23/1000 
	 loss: 33.7759, MinusLogProbMetric: 33.7759, val_loss: 33.8357, val_MinusLogProbMetric: 33.8357

Epoch 23: val_loss did not improve from 33.20940
196/196 - 28s - loss: 33.7759 - MinusLogProbMetric: 33.7759 - val_loss: 33.8357 - val_MinusLogProbMetric: 33.8357 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 24/1000
2023-09-29 01:13:02.882 
Epoch 24/1000 
	 loss: 33.6879, MinusLogProbMetric: 33.6879, val_loss: 33.4209, val_MinusLogProbMetric: 33.4209

Epoch 24: val_loss did not improve from 33.20940
196/196 - 28s - loss: 33.6879 - MinusLogProbMetric: 33.6879 - val_loss: 33.4209 - val_MinusLogProbMetric: 33.4209 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 25/1000
2023-09-29 01:13:31.119 
Epoch 25/1000 
	 loss: 33.4838, MinusLogProbMetric: 33.4838, val_loss: 33.2111, val_MinusLogProbMetric: 33.2111

Epoch 25: val_loss did not improve from 33.20940
196/196 - 28s - loss: 33.4838 - MinusLogProbMetric: 33.4838 - val_loss: 33.2111 - val_MinusLogProbMetric: 33.2111 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 26/1000
2023-09-29 01:14:01.239 
Epoch 26/1000 
	 loss: 33.3625, MinusLogProbMetric: 33.3625, val_loss: 32.3278, val_MinusLogProbMetric: 32.3278

Epoch 26: val_loss improved from 33.20940 to 32.32784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 31s - loss: 33.3625 - MinusLogProbMetric: 33.3625 - val_loss: 32.3278 - val_MinusLogProbMetric: 32.3278 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 27/1000
2023-09-29 01:14:31.328 
Epoch 27/1000 
	 loss: 33.3455, MinusLogProbMetric: 33.3455, val_loss: 33.4772, val_MinusLogProbMetric: 33.4772

Epoch 27: val_loss did not improve from 32.32784
196/196 - 30s - loss: 33.3455 - MinusLogProbMetric: 33.3455 - val_loss: 33.4772 - val_MinusLogProbMetric: 33.4772 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 28/1000
2023-09-29 01:15:00.771 
Epoch 28/1000 
	 loss: 33.0968, MinusLogProbMetric: 33.0968, val_loss: 34.5561, val_MinusLogProbMetric: 34.5561

Epoch 28: val_loss did not improve from 32.32784
196/196 - 29s - loss: 33.0968 - MinusLogProbMetric: 33.0968 - val_loss: 34.5561 - val_MinusLogProbMetric: 34.5561 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 29/1000
2023-09-29 01:15:28.673 
Epoch 29/1000 
	 loss: 33.2150, MinusLogProbMetric: 33.2150, val_loss: 32.1940, val_MinusLogProbMetric: 32.1940

Epoch 29: val_loss improved from 32.32784 to 32.19405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 28s - loss: 33.2150 - MinusLogProbMetric: 33.2150 - val_loss: 32.1940 - val_MinusLogProbMetric: 32.1940 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 30/1000
2023-09-29 01:15:56.858 
Epoch 30/1000 
	 loss: 32.7217, MinusLogProbMetric: 32.7217, val_loss: 32.6737, val_MinusLogProbMetric: 32.6737

Epoch 30: val_loss did not improve from 32.19405
196/196 - 28s - loss: 32.7217 - MinusLogProbMetric: 32.7217 - val_loss: 32.6737 - val_MinusLogProbMetric: 32.6737 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 31/1000
2023-09-29 01:16:25.113 
Epoch 31/1000 
	 loss: 32.8135, MinusLogProbMetric: 32.8135, val_loss: 32.0458, val_MinusLogProbMetric: 32.0458

Epoch 31: val_loss improved from 32.19405 to 32.04584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 29s - loss: 32.8135 - MinusLogProbMetric: 32.8135 - val_loss: 32.0458 - val_MinusLogProbMetric: 32.0458 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 32/1000
2023-09-29 01:16:55.923 
Epoch 32/1000 
	 loss: 32.6381, MinusLogProbMetric: 32.6381, val_loss: 32.4449, val_MinusLogProbMetric: 32.4449

Epoch 32: val_loss did not improve from 32.04584
196/196 - 30s - loss: 32.6381 - MinusLogProbMetric: 32.6381 - val_loss: 32.4449 - val_MinusLogProbMetric: 32.4449 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 33/1000
2023-09-29 01:17:24.715 
Epoch 33/1000 
	 loss: 32.5834, MinusLogProbMetric: 32.5834, val_loss: 32.6704, val_MinusLogProbMetric: 32.6704

Epoch 33: val_loss did not improve from 32.04584
196/196 - 29s - loss: 32.5834 - MinusLogProbMetric: 32.5834 - val_loss: 32.6704 - val_MinusLogProbMetric: 32.6704 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 34/1000
2023-09-29 01:17:53.632 
Epoch 34/1000 
	 loss: 32.4838, MinusLogProbMetric: 32.4838, val_loss: 33.1554, val_MinusLogProbMetric: 33.1554

Epoch 34: val_loss did not improve from 32.04584
196/196 - 29s - loss: 32.4838 - MinusLogProbMetric: 32.4838 - val_loss: 33.1554 - val_MinusLogProbMetric: 33.1554 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 35/1000
2023-09-29 01:18:21.809 
Epoch 35/1000 
	 loss: 32.4555, MinusLogProbMetric: 32.4555, val_loss: 31.9559, val_MinusLogProbMetric: 31.9559

Epoch 35: val_loss improved from 32.04584 to 31.95593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 29s - loss: 32.4555 - MinusLogProbMetric: 32.4555 - val_loss: 31.9559 - val_MinusLogProbMetric: 31.9559 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 36/1000
2023-09-29 01:18:52.241 
Epoch 36/1000 
	 loss: 32.1622, MinusLogProbMetric: 32.1622, val_loss: 33.5037, val_MinusLogProbMetric: 33.5037

Epoch 36: val_loss did not improve from 31.95593
196/196 - 30s - loss: 32.1622 - MinusLogProbMetric: 32.1622 - val_loss: 33.5037 - val_MinusLogProbMetric: 33.5037 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 37/1000
2023-09-29 01:19:21.952 
Epoch 37/1000 
	 loss: 32.1564, MinusLogProbMetric: 32.1564, val_loss: 31.5745, val_MinusLogProbMetric: 31.5745

Epoch 37: val_loss improved from 31.95593 to 31.57455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 30s - loss: 32.1564 - MinusLogProbMetric: 32.1564 - val_loss: 31.5745 - val_MinusLogProbMetric: 31.5745 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 38/1000
2023-09-29 01:19:51.162 
Epoch 38/1000 
	 loss: 32.1636, MinusLogProbMetric: 32.1636, val_loss: 33.1976, val_MinusLogProbMetric: 33.1976

Epoch 38: val_loss did not improve from 31.57455
196/196 - 29s - loss: 32.1636 - MinusLogProbMetric: 32.1636 - val_loss: 33.1976 - val_MinusLogProbMetric: 33.1976 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 39/1000
2023-09-29 01:20:18.989 
Epoch 39/1000 
	 loss: 32.3090, MinusLogProbMetric: 32.3090, val_loss: 32.4110, val_MinusLogProbMetric: 32.4110

Epoch 39: val_loss did not improve from 31.57455
196/196 - 28s - loss: 32.3090 - MinusLogProbMetric: 32.3090 - val_loss: 32.4110 - val_MinusLogProbMetric: 32.4110 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 40/1000
2023-09-29 01:20:47.073 
Epoch 40/1000 
	 loss: 31.8706, MinusLogProbMetric: 31.8706, val_loss: 32.2258, val_MinusLogProbMetric: 32.2258

Epoch 40: val_loss did not improve from 31.57455
196/196 - 28s - loss: 31.8706 - MinusLogProbMetric: 31.8706 - val_loss: 32.2258 - val_MinusLogProbMetric: 32.2258 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 41/1000
2023-09-29 01:21:15.336 
Epoch 41/1000 
	 loss: 32.0234, MinusLogProbMetric: 32.0234, val_loss: 31.5941, val_MinusLogProbMetric: 31.5941

Epoch 41: val_loss did not improve from 31.57455
196/196 - 28s - loss: 32.0234 - MinusLogProbMetric: 32.0234 - val_loss: 31.5941 - val_MinusLogProbMetric: 31.5941 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 42/1000
2023-09-29 01:21:43.341 
Epoch 42/1000 
	 loss: 31.8031, MinusLogProbMetric: 31.8031, val_loss: 32.1388, val_MinusLogProbMetric: 32.1388

Epoch 42: val_loss did not improve from 31.57455
196/196 - 28s - loss: 31.8031 - MinusLogProbMetric: 31.8031 - val_loss: 32.1388 - val_MinusLogProbMetric: 32.1388 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 43/1000
2023-09-29 01:22:12.485 
Epoch 43/1000 
	 loss: 31.9661, MinusLogProbMetric: 31.9661, val_loss: 31.6970, val_MinusLogProbMetric: 31.6970

Epoch 43: val_loss did not improve from 31.57455
196/196 - 29s - loss: 31.9661 - MinusLogProbMetric: 31.9661 - val_loss: 31.6970 - val_MinusLogProbMetric: 31.6970 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 44/1000
2023-09-29 01:22:40.837 
Epoch 44/1000 
	 loss: 31.6875, MinusLogProbMetric: 31.6875, val_loss: 31.9569, val_MinusLogProbMetric: 31.9569

Epoch 44: val_loss did not improve from 31.57455
196/196 - 28s - loss: 31.6875 - MinusLogProbMetric: 31.6875 - val_loss: 31.9569 - val_MinusLogProbMetric: 31.9569 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 45/1000
2023-09-29 01:23:13.908 
Epoch 45/1000 
	 loss: 31.5783, MinusLogProbMetric: 31.5783, val_loss: 31.8503, val_MinusLogProbMetric: 31.8503

Epoch 45: val_loss did not improve from 31.57455
196/196 - 33s - loss: 31.5783 - MinusLogProbMetric: 31.5783 - val_loss: 31.8503 - val_MinusLogProbMetric: 31.8503 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 46/1000
2023-09-29 01:23:42.830 
Epoch 46/1000 
	 loss: 31.4278, MinusLogProbMetric: 31.4278, val_loss: 31.7378, val_MinusLogProbMetric: 31.7378

Epoch 46: val_loss did not improve from 31.57455
196/196 - 29s - loss: 31.4278 - MinusLogProbMetric: 31.4278 - val_loss: 31.7378 - val_MinusLogProbMetric: 31.7378 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 47/1000
2023-09-29 01:24:11.362 
Epoch 47/1000 
	 loss: 31.3885, MinusLogProbMetric: 31.3885, val_loss: 32.0631, val_MinusLogProbMetric: 32.0631

Epoch 47: val_loss did not improve from 31.57455
196/196 - 29s - loss: 31.3885 - MinusLogProbMetric: 31.3885 - val_loss: 32.0631 - val_MinusLogProbMetric: 32.0631 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 48/1000
2023-09-29 01:24:44.331 
Epoch 48/1000 
	 loss: 31.5444, MinusLogProbMetric: 31.5444, val_loss: 31.4735, val_MinusLogProbMetric: 31.4735

Epoch 48: val_loss improved from 31.57455 to 31.47346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 31.5444 - MinusLogProbMetric: 31.5444 - val_loss: 31.4735 - val_MinusLogProbMetric: 31.4735 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 49/1000
2023-09-29 01:25:13.153 
Epoch 49/1000 
	 loss: 31.6032, MinusLogProbMetric: 31.6032, val_loss: 31.5337, val_MinusLogProbMetric: 31.5337

Epoch 49: val_loss did not improve from 31.47346
196/196 - 28s - loss: 31.6032 - MinusLogProbMetric: 31.6032 - val_loss: 31.5337 - val_MinusLogProbMetric: 31.5337 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 50/1000
2023-09-29 01:25:42.452 
Epoch 50/1000 
	 loss: 31.5165, MinusLogProbMetric: 31.5165, val_loss: 30.8463, val_MinusLogProbMetric: 30.8463

Epoch 50: val_loss improved from 31.47346 to 30.84628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 30s - loss: 31.5165 - MinusLogProbMetric: 31.5165 - val_loss: 30.8463 - val_MinusLogProbMetric: 30.8463 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 51/1000
2023-09-29 01:26:11.321 
Epoch 51/1000 
	 loss: 31.5137, MinusLogProbMetric: 31.5137, val_loss: 31.7716, val_MinusLogProbMetric: 31.7716

Epoch 51: val_loss did not improve from 30.84628
196/196 - 28s - loss: 31.5137 - MinusLogProbMetric: 31.5137 - val_loss: 31.7716 - val_MinusLogProbMetric: 31.7716 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 52/1000
2023-09-29 01:26:38.951 
Epoch 52/1000 
	 loss: 31.3276, MinusLogProbMetric: 31.3276, val_loss: 31.2132, val_MinusLogProbMetric: 31.2132

Epoch 52: val_loss did not improve from 30.84628
196/196 - 28s - loss: 31.3276 - MinusLogProbMetric: 31.3276 - val_loss: 31.2132 - val_MinusLogProbMetric: 31.2132 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 53/1000
2023-09-29 01:27:08.176 
Epoch 53/1000 
	 loss: 31.4555, MinusLogProbMetric: 31.4555, val_loss: 32.1697, val_MinusLogProbMetric: 32.1697

Epoch 53: val_loss did not improve from 30.84628
196/196 - 29s - loss: 31.4555 - MinusLogProbMetric: 31.4555 - val_loss: 32.1697 - val_MinusLogProbMetric: 32.1697 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 54/1000
2023-09-29 01:27:38.030 
Epoch 54/1000 
	 loss: 31.3524, MinusLogProbMetric: 31.3524, val_loss: 31.5861, val_MinusLogProbMetric: 31.5861

Epoch 54: val_loss did not improve from 30.84628
196/196 - 30s - loss: 31.3524 - MinusLogProbMetric: 31.3524 - val_loss: 31.5861 - val_MinusLogProbMetric: 31.5861 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 55/1000
2023-09-29 01:28:05.107 
Epoch 55/1000 
	 loss: 31.4468, MinusLogProbMetric: 31.4468, val_loss: 32.2131, val_MinusLogProbMetric: 32.2131

Epoch 55: val_loss did not improve from 30.84628
196/196 - 27s - loss: 31.4468 - MinusLogProbMetric: 31.4468 - val_loss: 32.2131 - val_MinusLogProbMetric: 32.2131 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 56/1000
2023-09-29 01:28:33.834 
Epoch 56/1000 
	 loss: 31.1940, MinusLogProbMetric: 31.1940, val_loss: 31.9534, val_MinusLogProbMetric: 31.9534

Epoch 56: val_loss did not improve from 30.84628
196/196 - 29s - loss: 31.1940 - MinusLogProbMetric: 31.1940 - val_loss: 31.9534 - val_MinusLogProbMetric: 31.9534 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 57/1000
2023-09-29 01:29:01.785 
Epoch 57/1000 
	 loss: 31.2916, MinusLogProbMetric: 31.2916, val_loss: 33.7849, val_MinusLogProbMetric: 33.7849

Epoch 57: val_loss did not improve from 30.84628
196/196 - 28s - loss: 31.2916 - MinusLogProbMetric: 31.2916 - val_loss: 33.7849 - val_MinusLogProbMetric: 33.7849 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 58/1000
2023-09-29 01:29:32.228 
Epoch 58/1000 
	 loss: 31.2972, MinusLogProbMetric: 31.2972, val_loss: 32.1046, val_MinusLogProbMetric: 32.1046

Epoch 58: val_loss did not improve from 30.84628
196/196 - 30s - loss: 31.2972 - MinusLogProbMetric: 31.2972 - val_loss: 32.1046 - val_MinusLogProbMetric: 32.1046 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 59/1000
2023-09-29 01:29:59.451 
Epoch 59/1000 
	 loss: 31.3246, MinusLogProbMetric: 31.3246, val_loss: 31.4886, val_MinusLogProbMetric: 31.4886

Epoch 59: val_loss did not improve from 30.84628
196/196 - 27s - loss: 31.3246 - MinusLogProbMetric: 31.3246 - val_loss: 31.4886 - val_MinusLogProbMetric: 31.4886 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 60/1000
2023-09-29 01:30:26.757 
Epoch 60/1000 
	 loss: 31.1310, MinusLogProbMetric: 31.1310, val_loss: 30.8994, val_MinusLogProbMetric: 30.8994

Epoch 60: val_loss did not improve from 30.84628
196/196 - 27s - loss: 31.1310 - MinusLogProbMetric: 31.1310 - val_loss: 30.8994 - val_MinusLogProbMetric: 30.8994 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 61/1000
2023-09-29 01:30:56.171 
Epoch 61/1000 
	 loss: 30.9964, MinusLogProbMetric: 30.9964, val_loss: 32.3137, val_MinusLogProbMetric: 32.3137

Epoch 61: val_loss did not improve from 30.84628
196/196 - 29s - loss: 30.9964 - MinusLogProbMetric: 30.9964 - val_loss: 32.3137 - val_MinusLogProbMetric: 32.3137 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 62/1000
2023-09-29 01:31:26.249 
Epoch 62/1000 
	 loss: 31.2767, MinusLogProbMetric: 31.2767, val_loss: 31.4619, val_MinusLogProbMetric: 31.4619

Epoch 62: val_loss did not improve from 30.84628
196/196 - 30s - loss: 31.2767 - MinusLogProbMetric: 31.2767 - val_loss: 31.4619 - val_MinusLogProbMetric: 31.4619 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 63/1000
2023-09-29 01:31:55.702 
Epoch 63/1000 
	 loss: 30.7996, MinusLogProbMetric: 30.7996, val_loss: 30.9027, val_MinusLogProbMetric: 30.9027

Epoch 63: val_loss did not improve from 30.84628
196/196 - 29s - loss: 30.7996 - MinusLogProbMetric: 30.7996 - val_loss: 30.9027 - val_MinusLogProbMetric: 30.9027 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 64/1000
2023-09-29 01:32:27.131 
Epoch 64/1000 
	 loss: 30.9415, MinusLogProbMetric: 30.9415, val_loss: 32.1695, val_MinusLogProbMetric: 32.1695

Epoch 64: val_loss did not improve from 30.84628
196/196 - 31s - loss: 30.9415 - MinusLogProbMetric: 30.9415 - val_loss: 32.1695 - val_MinusLogProbMetric: 32.1695 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 65/1000
2023-09-29 01:32:54.702 
Epoch 65/1000 
	 loss: 31.1173, MinusLogProbMetric: 31.1173, val_loss: 30.9104, val_MinusLogProbMetric: 30.9104

Epoch 65: val_loss did not improve from 30.84628
196/196 - 28s - loss: 31.1173 - MinusLogProbMetric: 31.1173 - val_loss: 30.9104 - val_MinusLogProbMetric: 30.9104 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 66/1000
2023-09-29 01:33:23.491 
Epoch 66/1000 
	 loss: 30.8341, MinusLogProbMetric: 30.8341, val_loss: 31.6248, val_MinusLogProbMetric: 31.6248

Epoch 66: val_loss did not improve from 30.84628
196/196 - 29s - loss: 30.8341 - MinusLogProbMetric: 30.8341 - val_loss: 31.6248 - val_MinusLogProbMetric: 31.6248 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 67/1000
2023-09-29 01:33:52.133 
Epoch 67/1000 
	 loss: 30.8208, MinusLogProbMetric: 30.8208, val_loss: 31.0995, val_MinusLogProbMetric: 31.0995

Epoch 67: val_loss did not improve from 30.84628
196/196 - 29s - loss: 30.8208 - MinusLogProbMetric: 30.8208 - val_loss: 31.0995 - val_MinusLogProbMetric: 31.0995 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 68/1000
2023-09-29 01:34:20.637 
Epoch 68/1000 
	 loss: 30.7518, MinusLogProbMetric: 30.7518, val_loss: 30.7099, val_MinusLogProbMetric: 30.7099

Epoch 68: val_loss improved from 30.84628 to 30.70987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 29s - loss: 30.7518 - MinusLogProbMetric: 30.7518 - val_loss: 30.7099 - val_MinusLogProbMetric: 30.7099 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 69/1000
2023-09-29 01:34:49.919 
Epoch 69/1000 
	 loss: 30.7907, MinusLogProbMetric: 30.7907, val_loss: 31.1790, val_MinusLogProbMetric: 31.1790

Epoch 69: val_loss did not improve from 30.70987
196/196 - 29s - loss: 30.7907 - MinusLogProbMetric: 30.7907 - val_loss: 31.1790 - val_MinusLogProbMetric: 31.1790 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 70/1000
2023-09-29 01:35:20.229 
Epoch 70/1000 
	 loss: 30.6695, MinusLogProbMetric: 30.6695, val_loss: 31.0061, val_MinusLogProbMetric: 31.0061

Epoch 70: val_loss did not improve from 30.70987
196/196 - 30s - loss: 30.6695 - MinusLogProbMetric: 30.6695 - val_loss: 31.0061 - val_MinusLogProbMetric: 31.0061 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 71/1000
2023-09-29 01:35:48.291 
Epoch 71/1000 
	 loss: 30.6309, MinusLogProbMetric: 30.6309, val_loss: 30.5711, val_MinusLogProbMetric: 30.5711

Epoch 71: val_loss improved from 30.70987 to 30.57113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 28s - loss: 30.6309 - MinusLogProbMetric: 30.6309 - val_loss: 30.5711 - val_MinusLogProbMetric: 30.5711 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 72/1000
2023-09-29 01:36:17.206 
Epoch 72/1000 
	 loss: 30.7573, MinusLogProbMetric: 30.7573, val_loss: 31.4394, val_MinusLogProbMetric: 31.4394

Epoch 72: val_loss did not improve from 30.57113
196/196 - 28s - loss: 30.7573 - MinusLogProbMetric: 30.7573 - val_loss: 31.4394 - val_MinusLogProbMetric: 31.4394 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 73/1000
2023-09-29 01:36:48.499 
Epoch 73/1000 
	 loss: 30.7395, MinusLogProbMetric: 30.7395, val_loss: 30.5530, val_MinusLogProbMetric: 30.5530

Epoch 73: val_loss improved from 30.57113 to 30.55299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 32s - loss: 30.7395 - MinusLogProbMetric: 30.7395 - val_loss: 30.5530 - val_MinusLogProbMetric: 30.5530 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 74/1000
2023-09-29 01:37:15.957 
Epoch 74/1000 
	 loss: 30.6393, MinusLogProbMetric: 30.6393, val_loss: 31.9440, val_MinusLogProbMetric: 31.9440

Epoch 74: val_loss did not improve from 30.55299
196/196 - 27s - loss: 30.6393 - MinusLogProbMetric: 30.6393 - val_loss: 31.9440 - val_MinusLogProbMetric: 31.9440 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 75/1000
2023-09-29 01:37:45.215 
Epoch 75/1000 
	 loss: 30.5012, MinusLogProbMetric: 30.5012, val_loss: 31.0633, val_MinusLogProbMetric: 31.0633

Epoch 75: val_loss did not improve from 30.55299
196/196 - 29s - loss: 30.5012 - MinusLogProbMetric: 30.5012 - val_loss: 31.0633 - val_MinusLogProbMetric: 31.0633 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 76/1000
2023-09-29 01:38:14.646 
Epoch 76/1000 
	 loss: 30.5979, MinusLogProbMetric: 30.5979, val_loss: 30.2707, val_MinusLogProbMetric: 30.2707

Epoch 76: val_loss improved from 30.55299 to 30.27071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 30s - loss: 30.5979 - MinusLogProbMetric: 30.5979 - val_loss: 30.2707 - val_MinusLogProbMetric: 30.2707 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 77/1000
2023-09-29 01:38:42.968 
Epoch 77/1000 
	 loss: 30.5480, MinusLogProbMetric: 30.5480, val_loss: 30.6355, val_MinusLogProbMetric: 30.6355

Epoch 77: val_loss did not improve from 30.27071
196/196 - 28s - loss: 30.5480 - MinusLogProbMetric: 30.5480 - val_loss: 30.6355 - val_MinusLogProbMetric: 30.6355 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 78/1000
2023-09-29 01:39:12.685 
Epoch 78/1000 
	 loss: 30.4627, MinusLogProbMetric: 30.4627, val_loss: 31.6672, val_MinusLogProbMetric: 31.6672

Epoch 78: val_loss did not improve from 30.27071
196/196 - 30s - loss: 30.4627 - MinusLogProbMetric: 30.4627 - val_loss: 31.6672 - val_MinusLogProbMetric: 31.6672 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 79/1000
2023-09-29 01:39:41.836 
Epoch 79/1000 
	 loss: 30.6186, MinusLogProbMetric: 30.6186, val_loss: 30.3333, val_MinusLogProbMetric: 30.3333

Epoch 79: val_loss did not improve from 30.27071
196/196 - 29s - loss: 30.6186 - MinusLogProbMetric: 30.6186 - val_loss: 30.3333 - val_MinusLogProbMetric: 30.3333 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 80/1000
2023-09-29 01:40:09.773 
Epoch 80/1000 
	 loss: 30.1797, MinusLogProbMetric: 30.1797, val_loss: 30.6574, val_MinusLogProbMetric: 30.6574

Epoch 80: val_loss did not improve from 30.27071
196/196 - 28s - loss: 30.1797 - MinusLogProbMetric: 30.1797 - val_loss: 30.6574 - val_MinusLogProbMetric: 30.6574 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 81/1000
2023-09-29 01:40:39.436 
Epoch 81/1000 
	 loss: 30.3939, MinusLogProbMetric: 30.3939, val_loss: 30.2168, val_MinusLogProbMetric: 30.2168

Epoch 81: val_loss improved from 30.27071 to 30.21678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 30s - loss: 30.3939 - MinusLogProbMetric: 30.3939 - val_loss: 30.2168 - val_MinusLogProbMetric: 30.2168 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 82/1000
2023-09-29 01:41:12.763 
Epoch 82/1000 
	 loss: 30.3683, MinusLogProbMetric: 30.3683, val_loss: 30.3555, val_MinusLogProbMetric: 30.3555

Epoch 82: val_loss did not improve from 30.21678
196/196 - 33s - loss: 30.3683 - MinusLogProbMetric: 30.3683 - val_loss: 30.3555 - val_MinusLogProbMetric: 30.3555 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 83/1000
2023-09-29 01:41:41.928 
Epoch 83/1000 
	 loss: 30.2829, MinusLogProbMetric: 30.2829, val_loss: 30.2298, val_MinusLogProbMetric: 30.2298

Epoch 83: val_loss did not improve from 30.21678
196/196 - 29s - loss: 30.2829 - MinusLogProbMetric: 30.2829 - val_loss: 30.2298 - val_MinusLogProbMetric: 30.2298 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 84/1000
2023-09-29 01:42:12.626 
Epoch 84/1000 
	 loss: 30.5166, MinusLogProbMetric: 30.5166, val_loss: 30.9595, val_MinusLogProbMetric: 30.9595

Epoch 84: val_loss did not improve from 30.21678
196/196 - 31s - loss: 30.5166 - MinusLogProbMetric: 30.5166 - val_loss: 30.9595 - val_MinusLogProbMetric: 30.9595 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 85/1000
2023-09-29 01:42:41.655 
Epoch 85/1000 
	 loss: 30.2075, MinusLogProbMetric: 30.2075, val_loss: 31.1702, val_MinusLogProbMetric: 31.1702

Epoch 85: val_loss did not improve from 30.21678
196/196 - 29s - loss: 30.2075 - MinusLogProbMetric: 30.2075 - val_loss: 31.1702 - val_MinusLogProbMetric: 31.1702 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 86/1000
2023-09-29 01:43:10.861 
Epoch 86/1000 
	 loss: 30.4317, MinusLogProbMetric: 30.4317, val_loss: 31.0889, val_MinusLogProbMetric: 31.0889

Epoch 86: val_loss did not improve from 30.21678
196/196 - 29s - loss: 30.4317 - MinusLogProbMetric: 30.4317 - val_loss: 31.0889 - val_MinusLogProbMetric: 31.0889 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 87/1000
2023-09-29 01:43:38.391 
Epoch 87/1000 
	 loss: 30.5503, MinusLogProbMetric: 30.5503, val_loss: 31.4277, val_MinusLogProbMetric: 31.4277

Epoch 87: val_loss did not improve from 30.21678
196/196 - 28s - loss: 30.5503 - MinusLogProbMetric: 30.5503 - val_loss: 31.4277 - val_MinusLogProbMetric: 31.4277 - lr: 0.0010 - 28s/epoch - 140ms/step
Epoch 88/1000
2023-09-29 01:44:09.602 
Epoch 88/1000 
	 loss: 30.2435, MinusLogProbMetric: 30.2435, val_loss: 30.2312, val_MinusLogProbMetric: 30.2312

Epoch 88: val_loss did not improve from 30.21678
196/196 - 31s - loss: 30.2435 - MinusLogProbMetric: 30.2435 - val_loss: 30.2312 - val_MinusLogProbMetric: 30.2312 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 89/1000
2023-09-29 01:44:38.788 
Epoch 89/1000 
	 loss: 30.2175, MinusLogProbMetric: 30.2175, val_loss: 31.2746, val_MinusLogProbMetric: 31.2746

Epoch 89: val_loss did not improve from 30.21678
196/196 - 29s - loss: 30.2175 - MinusLogProbMetric: 30.2175 - val_loss: 31.2746 - val_MinusLogProbMetric: 31.2746 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 90/1000
2023-09-29 01:45:07.638 
Epoch 90/1000 
	 loss: 30.3122, MinusLogProbMetric: 30.3122, val_loss: 30.3020, val_MinusLogProbMetric: 30.3020

Epoch 90: val_loss did not improve from 30.21678
196/196 - 29s - loss: 30.3122 - MinusLogProbMetric: 30.3122 - val_loss: 30.3020 - val_MinusLogProbMetric: 30.3020 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 91/1000
2023-09-29 01:45:37.351 
Epoch 91/1000 
	 loss: 30.4422, MinusLogProbMetric: 30.4422, val_loss: 32.5607, val_MinusLogProbMetric: 32.5607

Epoch 91: val_loss did not improve from 30.21678
196/196 - 30s - loss: 30.4422 - MinusLogProbMetric: 30.4422 - val_loss: 32.5607 - val_MinusLogProbMetric: 32.5607 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 92/1000
2023-09-29 01:46:06.883 
Epoch 92/1000 
	 loss: 30.4110, MinusLogProbMetric: 30.4110, val_loss: 31.2861, val_MinusLogProbMetric: 31.2861

Epoch 92: val_loss did not improve from 30.21678
196/196 - 30s - loss: 30.4110 - MinusLogProbMetric: 30.4110 - val_loss: 31.2861 - val_MinusLogProbMetric: 31.2861 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 93/1000
2023-09-29 01:46:36.114 
Epoch 93/1000 
	 loss: 30.2668, MinusLogProbMetric: 30.2668, val_loss: 31.0523, val_MinusLogProbMetric: 31.0523

Epoch 93: val_loss did not improve from 30.21678
196/196 - 29s - loss: 30.2668 - MinusLogProbMetric: 30.2668 - val_loss: 31.0523 - val_MinusLogProbMetric: 31.0523 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 94/1000
2023-09-29 01:47:04.665 
Epoch 94/1000 
	 loss: 30.1531, MinusLogProbMetric: 30.1531, val_loss: 30.6856, val_MinusLogProbMetric: 30.6856

Epoch 94: val_loss did not improve from 30.21678
196/196 - 29s - loss: 30.1531 - MinusLogProbMetric: 30.1531 - val_loss: 30.6856 - val_MinusLogProbMetric: 30.6856 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 95/1000
2023-09-29 01:47:36.334 
Epoch 95/1000 
	 loss: 30.1794, MinusLogProbMetric: 30.1794, val_loss: 29.5308, val_MinusLogProbMetric: 29.5308

Epoch 95: val_loss improved from 30.21678 to 29.53083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 32s - loss: 30.1794 - MinusLogProbMetric: 30.1794 - val_loss: 29.5308 - val_MinusLogProbMetric: 29.5308 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 96/1000
2023-09-29 01:48:05.922 
Epoch 96/1000 
	 loss: 30.1959, MinusLogProbMetric: 30.1959, val_loss: 30.2005, val_MinusLogProbMetric: 30.2005

Epoch 96: val_loss did not improve from 29.53083
196/196 - 29s - loss: 30.1959 - MinusLogProbMetric: 30.1959 - val_loss: 30.2005 - val_MinusLogProbMetric: 30.2005 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 97/1000
2023-09-29 01:48:36.057 
Epoch 97/1000 
	 loss: 29.9825, MinusLogProbMetric: 29.9825, val_loss: 30.5749, val_MinusLogProbMetric: 30.5749

Epoch 97: val_loss did not improve from 29.53083
196/196 - 30s - loss: 29.9825 - MinusLogProbMetric: 29.9825 - val_loss: 30.5749 - val_MinusLogProbMetric: 30.5749 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 98/1000
2023-09-29 01:49:08.572 
Epoch 98/1000 
	 loss: 30.0625, MinusLogProbMetric: 30.0625, val_loss: 30.3885, val_MinusLogProbMetric: 30.3885

Epoch 98: val_loss did not improve from 29.53083
196/196 - 33s - loss: 30.0625 - MinusLogProbMetric: 30.0625 - val_loss: 30.3885 - val_MinusLogProbMetric: 30.3885 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 99/1000
2023-09-29 01:49:35.593 
Epoch 99/1000 
	 loss: 30.1126, MinusLogProbMetric: 30.1126, val_loss: 30.1047, val_MinusLogProbMetric: 30.1047

Epoch 99: val_loss did not improve from 29.53083
196/196 - 27s - loss: 30.1126 - MinusLogProbMetric: 30.1126 - val_loss: 30.1047 - val_MinusLogProbMetric: 30.1047 - lr: 0.0010 - 27s/epoch - 138ms/step
Epoch 100/1000
2023-09-29 01:50:03.851 
Epoch 100/1000 
	 loss: 29.9957, MinusLogProbMetric: 29.9957, val_loss: 30.3542, val_MinusLogProbMetric: 30.3542

Epoch 100: val_loss did not improve from 29.53083
196/196 - 28s - loss: 29.9957 - MinusLogProbMetric: 29.9957 - val_loss: 30.3542 - val_MinusLogProbMetric: 30.3542 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 101/1000
2023-09-29 01:50:35.567 
Epoch 101/1000 
	 loss: 29.9703, MinusLogProbMetric: 29.9703, val_loss: 30.4672, val_MinusLogProbMetric: 30.4672

Epoch 101: val_loss did not improve from 29.53083
196/196 - 32s - loss: 29.9703 - MinusLogProbMetric: 29.9703 - val_loss: 30.4672 - val_MinusLogProbMetric: 30.4672 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 102/1000
2023-09-29 01:51:03.811 
Epoch 102/1000 
	 loss: 30.0883, MinusLogProbMetric: 30.0883, val_loss: 30.1078, val_MinusLogProbMetric: 30.1078

Epoch 102: val_loss did not improve from 29.53083
196/196 - 28s - loss: 30.0883 - MinusLogProbMetric: 30.0883 - val_loss: 30.1078 - val_MinusLogProbMetric: 30.1078 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 103/1000
2023-09-29 01:51:31.700 
Epoch 103/1000 
	 loss: 30.0497, MinusLogProbMetric: 30.0497, val_loss: 30.5853, val_MinusLogProbMetric: 30.5853

Epoch 103: val_loss did not improve from 29.53083
196/196 - 28s - loss: 30.0497 - MinusLogProbMetric: 30.0497 - val_loss: 30.5853 - val_MinusLogProbMetric: 30.5853 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 104/1000
2023-09-29 01:52:01.362 
Epoch 104/1000 
	 loss: 30.0722, MinusLogProbMetric: 30.0722, val_loss: 30.2319, val_MinusLogProbMetric: 30.2319

Epoch 104: val_loss did not improve from 29.53083
196/196 - 30s - loss: 30.0722 - MinusLogProbMetric: 30.0722 - val_loss: 30.2319 - val_MinusLogProbMetric: 30.2319 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 105/1000
2023-09-29 01:52:30.116 
Epoch 105/1000 
	 loss: 29.8817, MinusLogProbMetric: 29.8817, val_loss: 30.9390, val_MinusLogProbMetric: 30.9390

Epoch 105: val_loss did not improve from 29.53083
196/196 - 29s - loss: 29.8817 - MinusLogProbMetric: 29.8817 - val_loss: 30.9390 - val_MinusLogProbMetric: 30.9390 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 106/1000
2023-09-29 01:52:58.982 
Epoch 106/1000 
	 loss: 29.9563, MinusLogProbMetric: 29.9563, val_loss: 30.5170, val_MinusLogProbMetric: 30.5170

Epoch 106: val_loss did not improve from 29.53083
196/196 - 29s - loss: 29.9563 - MinusLogProbMetric: 29.9563 - val_loss: 30.5170 - val_MinusLogProbMetric: 30.5170 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 107/1000
2023-09-29 01:53:28.105 
Epoch 107/1000 
	 loss: 30.1746, MinusLogProbMetric: 30.1746, val_loss: 30.0209, val_MinusLogProbMetric: 30.0209

Epoch 107: val_loss did not improve from 29.53083
196/196 - 29s - loss: 30.1746 - MinusLogProbMetric: 30.1746 - val_loss: 30.0209 - val_MinusLogProbMetric: 30.0209 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 108/1000
2023-09-29 01:53:56.028 
Epoch 108/1000 
	 loss: 30.0320, MinusLogProbMetric: 30.0320, val_loss: 30.0803, val_MinusLogProbMetric: 30.0803

Epoch 108: val_loss did not improve from 29.53083
196/196 - 28s - loss: 30.0320 - MinusLogProbMetric: 30.0320 - val_loss: 30.0803 - val_MinusLogProbMetric: 30.0803 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 109/1000
2023-09-29 01:54:24.202 
Epoch 109/1000 
	 loss: 29.9914, MinusLogProbMetric: 29.9914, val_loss: 29.5253, val_MinusLogProbMetric: 29.5253

Epoch 109: val_loss improved from 29.53083 to 29.52530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 29s - loss: 29.9914 - MinusLogProbMetric: 29.9914 - val_loss: 29.5253 - val_MinusLogProbMetric: 29.5253 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 110/1000
2023-09-29 01:54:53.575 
Epoch 110/1000 
	 loss: 29.8157, MinusLogProbMetric: 29.8157, val_loss: 29.6711, val_MinusLogProbMetric: 29.6711

Epoch 110: val_loss did not improve from 29.52530
196/196 - 29s - loss: 29.8157 - MinusLogProbMetric: 29.8157 - val_loss: 29.6711 - val_MinusLogProbMetric: 29.6711 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 111/1000
2023-09-29 01:55:22.435 
Epoch 111/1000 
	 loss: 29.8270, MinusLogProbMetric: 29.8270, val_loss: 31.1477, val_MinusLogProbMetric: 31.1477

Epoch 111: val_loss did not improve from 29.52530
196/196 - 29s - loss: 29.8270 - MinusLogProbMetric: 29.8270 - val_loss: 31.1477 - val_MinusLogProbMetric: 31.1477 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 112/1000
2023-09-29 01:55:49.645 
Epoch 112/1000 
	 loss: 29.8113, MinusLogProbMetric: 29.8113, val_loss: 29.7087, val_MinusLogProbMetric: 29.7087

Epoch 112: val_loss did not improve from 29.52530
196/196 - 27s - loss: 29.8113 - MinusLogProbMetric: 29.8113 - val_loss: 29.7087 - val_MinusLogProbMetric: 29.7087 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 113/1000
2023-09-29 01:56:18.544 
Epoch 113/1000 
	 loss: 30.1393, MinusLogProbMetric: 30.1393, val_loss: 31.3423, val_MinusLogProbMetric: 31.3423

Epoch 113: val_loss did not improve from 29.52530
196/196 - 29s - loss: 30.1393 - MinusLogProbMetric: 30.1393 - val_loss: 31.3423 - val_MinusLogProbMetric: 31.3423 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 114/1000
2023-09-29 01:56:46.707 
Epoch 114/1000 
	 loss: 29.7720, MinusLogProbMetric: 29.7720, val_loss: 30.2883, val_MinusLogProbMetric: 30.2883

Epoch 114: val_loss did not improve from 29.52530
196/196 - 28s - loss: 29.7720 - MinusLogProbMetric: 29.7720 - val_loss: 30.2883 - val_MinusLogProbMetric: 30.2883 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 115/1000
2023-09-29 01:57:14.765 
Epoch 115/1000 
	 loss: 29.7900, MinusLogProbMetric: 29.7900, val_loss: 31.0568, val_MinusLogProbMetric: 31.0568

Epoch 115: val_loss did not improve from 29.52530
196/196 - 28s - loss: 29.7900 - MinusLogProbMetric: 29.7900 - val_loss: 31.0568 - val_MinusLogProbMetric: 31.0568 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 116/1000
2023-09-29 01:57:42.232 
Epoch 116/1000 
	 loss: 30.1050, MinusLogProbMetric: 30.1050, val_loss: 29.8398, val_MinusLogProbMetric: 29.8398

Epoch 116: val_loss did not improve from 29.52530
196/196 - 27s - loss: 30.1050 - MinusLogProbMetric: 30.1050 - val_loss: 29.8398 - val_MinusLogProbMetric: 29.8398 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 117/1000
2023-09-29 01:58:10.011 
Epoch 117/1000 
	 loss: 29.7886, MinusLogProbMetric: 29.7886, val_loss: 30.5565, val_MinusLogProbMetric: 30.5565

Epoch 117: val_loss did not improve from 29.52530
196/196 - 28s - loss: 29.7886 - MinusLogProbMetric: 29.7886 - val_loss: 30.5565 - val_MinusLogProbMetric: 30.5565 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 118/1000
2023-09-29 01:58:38.209 
Epoch 118/1000 
	 loss: 29.8405, MinusLogProbMetric: 29.8405, val_loss: 30.2830, val_MinusLogProbMetric: 30.2830

Epoch 118: val_loss did not improve from 29.52530
196/196 - 28s - loss: 29.8405 - MinusLogProbMetric: 29.8405 - val_loss: 30.2830 - val_MinusLogProbMetric: 30.2830 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 119/1000
2023-09-29 01:59:08.443 
Epoch 119/1000 
	 loss: 29.9661, MinusLogProbMetric: 29.9661, val_loss: 29.9361, val_MinusLogProbMetric: 29.9361

Epoch 119: val_loss did not improve from 29.52530
196/196 - 30s - loss: 29.9661 - MinusLogProbMetric: 29.9661 - val_loss: 29.9361 - val_MinusLogProbMetric: 29.9361 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 120/1000
2023-09-29 01:59:39.809 
Epoch 120/1000 
	 loss: 29.7970, MinusLogProbMetric: 29.7970, val_loss: 30.6769, val_MinusLogProbMetric: 30.6769

Epoch 120: val_loss did not improve from 29.52530
196/196 - 31s - loss: 29.7970 - MinusLogProbMetric: 29.7970 - val_loss: 30.6769 - val_MinusLogProbMetric: 30.6769 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 121/1000
2023-09-29 02:00:07.420 
Epoch 121/1000 
	 loss: 29.9364, MinusLogProbMetric: 29.9364, val_loss: 29.3801, val_MinusLogProbMetric: 29.3801

Epoch 121: val_loss improved from 29.52530 to 29.38014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 28s - loss: 29.9364 - MinusLogProbMetric: 29.9364 - val_loss: 29.3801 - val_MinusLogProbMetric: 29.3801 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 122/1000
2023-09-29 02:00:36.599 
Epoch 122/1000 
	 loss: 29.7578, MinusLogProbMetric: 29.7578, val_loss: 30.7620, val_MinusLogProbMetric: 30.7620

Epoch 122: val_loss did not improve from 29.38014
196/196 - 29s - loss: 29.7578 - MinusLogProbMetric: 29.7578 - val_loss: 30.7620 - val_MinusLogProbMetric: 30.7620 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 123/1000
2023-09-29 02:01:06.881 
Epoch 123/1000 
	 loss: 29.7706, MinusLogProbMetric: 29.7706, val_loss: 29.5739, val_MinusLogProbMetric: 29.5739

Epoch 123: val_loss did not improve from 29.38014
196/196 - 30s - loss: 29.7706 - MinusLogProbMetric: 29.7706 - val_loss: 29.5739 - val_MinusLogProbMetric: 29.5739 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 124/1000
2023-09-29 02:01:38.558 
Epoch 124/1000 
	 loss: 29.7675, MinusLogProbMetric: 29.7675, val_loss: 32.8484, val_MinusLogProbMetric: 32.8484

Epoch 124: val_loss did not improve from 29.38014
196/196 - 32s - loss: 29.7675 - MinusLogProbMetric: 29.7675 - val_loss: 32.8484 - val_MinusLogProbMetric: 32.8484 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 125/1000
2023-09-29 02:02:06.110 
Epoch 125/1000 
	 loss: 29.8569, MinusLogProbMetric: 29.8569, val_loss: 30.1441, val_MinusLogProbMetric: 30.1441

Epoch 125: val_loss did not improve from 29.38014
196/196 - 28s - loss: 29.8569 - MinusLogProbMetric: 29.8569 - val_loss: 30.1441 - val_MinusLogProbMetric: 30.1441 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 126/1000
2023-09-29 02:02:35.702 
Epoch 126/1000 
	 loss: 29.4976, MinusLogProbMetric: 29.4976, val_loss: 29.8056, val_MinusLogProbMetric: 29.8056

Epoch 126: val_loss did not improve from 29.38014
196/196 - 30s - loss: 29.4976 - MinusLogProbMetric: 29.4976 - val_loss: 29.8056 - val_MinusLogProbMetric: 29.8056 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 127/1000
2023-09-29 02:03:05.091 
Epoch 127/1000 
	 loss: 29.7312, MinusLogProbMetric: 29.7312, val_loss: 29.3125, val_MinusLogProbMetric: 29.3125

Epoch 127: val_loss improved from 29.38014 to 29.31252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 30s - loss: 29.7312 - MinusLogProbMetric: 29.7312 - val_loss: 29.3125 - val_MinusLogProbMetric: 29.3125 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 128/1000
2023-09-29 02:03:33.983 
Epoch 128/1000 
	 loss: 29.7228, MinusLogProbMetric: 29.7228, val_loss: 29.7955, val_MinusLogProbMetric: 29.7955

Epoch 128: val_loss did not improve from 29.31252
196/196 - 28s - loss: 29.7228 - MinusLogProbMetric: 29.7228 - val_loss: 29.7955 - val_MinusLogProbMetric: 29.7955 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 129/1000
2023-09-29 02:04:01.425 
Epoch 129/1000 
	 loss: 29.7292, MinusLogProbMetric: 29.7292, val_loss: 30.3209, val_MinusLogProbMetric: 30.3209

Epoch 129: val_loss did not improve from 29.31252
196/196 - 27s - loss: 29.7292 - MinusLogProbMetric: 29.7292 - val_loss: 30.3209 - val_MinusLogProbMetric: 30.3209 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 130/1000
2023-09-29 02:04:28.745 
Epoch 130/1000 
	 loss: 29.5271, MinusLogProbMetric: 29.5271, val_loss: 29.9237, val_MinusLogProbMetric: 29.9237

Epoch 130: val_loss did not improve from 29.31252
196/196 - 27s - loss: 29.5271 - MinusLogProbMetric: 29.5271 - val_loss: 29.9237 - val_MinusLogProbMetric: 29.9237 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 131/1000
2023-09-29 02:04:57.410 
Epoch 131/1000 
	 loss: 29.6876, MinusLogProbMetric: 29.6876, val_loss: 31.4682, val_MinusLogProbMetric: 31.4682

Epoch 131: val_loss did not improve from 29.31252
196/196 - 29s - loss: 29.6876 - MinusLogProbMetric: 29.6876 - val_loss: 31.4682 - val_MinusLogProbMetric: 31.4682 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 132/1000
2023-09-29 02:05:26.821 
Epoch 132/1000 
	 loss: 29.5924, MinusLogProbMetric: 29.5924, val_loss: 29.7070, val_MinusLogProbMetric: 29.7070

Epoch 132: val_loss did not improve from 29.31252
196/196 - 29s - loss: 29.5924 - MinusLogProbMetric: 29.5924 - val_loss: 29.7070 - val_MinusLogProbMetric: 29.7070 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 133/1000
2023-09-29 02:05:55.298 
Epoch 133/1000 
	 loss: 29.7547, MinusLogProbMetric: 29.7547, val_loss: 31.0322, val_MinusLogProbMetric: 31.0322

Epoch 133: val_loss did not improve from 29.31252
196/196 - 28s - loss: 29.7547 - MinusLogProbMetric: 29.7547 - val_loss: 31.0322 - val_MinusLogProbMetric: 31.0322 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 134/1000
2023-09-29 02:06:23.655 
Epoch 134/1000 
	 loss: 29.6616, MinusLogProbMetric: 29.6616, val_loss: 29.4334, val_MinusLogProbMetric: 29.4334

Epoch 134: val_loss did not improve from 29.31252
196/196 - 28s - loss: 29.6616 - MinusLogProbMetric: 29.6616 - val_loss: 29.4334 - val_MinusLogProbMetric: 29.4334 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 135/1000
2023-09-29 02:06:51.606 
Epoch 135/1000 
	 loss: 29.6608, MinusLogProbMetric: 29.6608, val_loss: 30.2094, val_MinusLogProbMetric: 30.2094

Epoch 135: val_loss did not improve from 29.31252
196/196 - 28s - loss: 29.6608 - MinusLogProbMetric: 29.6608 - val_loss: 30.2094 - val_MinusLogProbMetric: 30.2094 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 136/1000
2023-09-29 02:07:20.348 
Epoch 136/1000 
	 loss: 29.6622, MinusLogProbMetric: 29.6622, val_loss: 29.8307, val_MinusLogProbMetric: 29.8307

Epoch 136: val_loss did not improve from 29.31252
196/196 - 29s - loss: 29.6622 - MinusLogProbMetric: 29.6622 - val_loss: 29.8307 - val_MinusLogProbMetric: 29.8307 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 137/1000
2023-09-29 02:07:48.221 
Epoch 137/1000 
	 loss: 29.5585, MinusLogProbMetric: 29.5585, val_loss: 30.0082, val_MinusLogProbMetric: 30.0082

Epoch 137: val_loss did not improve from 29.31252
196/196 - 28s - loss: 29.5585 - MinusLogProbMetric: 29.5585 - val_loss: 30.0082 - val_MinusLogProbMetric: 30.0082 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 138/1000
2023-09-29 02:08:18.044 
Epoch 138/1000 
	 loss: 29.5870, MinusLogProbMetric: 29.5870, val_loss: 29.5741, val_MinusLogProbMetric: 29.5741

Epoch 138: val_loss did not improve from 29.31252
196/196 - 30s - loss: 29.5870 - MinusLogProbMetric: 29.5870 - val_loss: 29.5741 - val_MinusLogProbMetric: 29.5741 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 139/1000
2023-09-29 02:08:47.140 
Epoch 139/1000 
	 loss: 29.5967, MinusLogProbMetric: 29.5967, val_loss: 30.7360, val_MinusLogProbMetric: 30.7360

Epoch 139: val_loss did not improve from 29.31252
196/196 - 29s - loss: 29.5967 - MinusLogProbMetric: 29.5967 - val_loss: 30.7360 - val_MinusLogProbMetric: 30.7360 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 140/1000
2023-09-29 02:09:15.853 
Epoch 140/1000 
	 loss: 29.5896, MinusLogProbMetric: 29.5896, val_loss: 29.7157, val_MinusLogProbMetric: 29.7157

Epoch 140: val_loss did not improve from 29.31252
196/196 - 29s - loss: 29.5896 - MinusLogProbMetric: 29.5896 - val_loss: 29.7157 - val_MinusLogProbMetric: 29.7157 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 141/1000
2023-09-29 02:09:43.584 
Epoch 141/1000 
	 loss: 29.4931, MinusLogProbMetric: 29.4931, val_loss: 29.6570, val_MinusLogProbMetric: 29.6570

Epoch 141: val_loss did not improve from 29.31252
196/196 - 28s - loss: 29.4931 - MinusLogProbMetric: 29.4931 - val_loss: 29.6570 - val_MinusLogProbMetric: 29.6570 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 142/1000
2023-09-29 02:10:13.183 
Epoch 142/1000 
	 loss: 29.5443, MinusLogProbMetric: 29.5443, val_loss: 30.0288, val_MinusLogProbMetric: 30.0288

Epoch 142: val_loss did not improve from 29.31252
196/196 - 30s - loss: 29.5443 - MinusLogProbMetric: 29.5443 - val_loss: 30.0288 - val_MinusLogProbMetric: 30.0288 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 143/1000
2023-09-29 02:10:47.696 
Epoch 143/1000 
	 loss: 29.5880, MinusLogProbMetric: 29.5880, val_loss: 29.4691, val_MinusLogProbMetric: 29.4691

Epoch 143: val_loss did not improve from 29.31252
196/196 - 35s - loss: 29.5880 - MinusLogProbMetric: 29.5880 - val_loss: 29.4691 - val_MinusLogProbMetric: 29.4691 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 144/1000
2023-09-29 02:11:22.292 
Epoch 144/1000 
	 loss: 29.3330, MinusLogProbMetric: 29.3330, val_loss: 29.3573, val_MinusLogProbMetric: 29.3573

Epoch 144: val_loss did not improve from 29.31252
196/196 - 35s - loss: 29.3330 - MinusLogProbMetric: 29.3330 - val_loss: 29.3573 - val_MinusLogProbMetric: 29.3573 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 145/1000
2023-09-29 02:11:58.070 
Epoch 145/1000 
	 loss: 29.4064, MinusLogProbMetric: 29.4064, val_loss: 30.4874, val_MinusLogProbMetric: 30.4874

Epoch 145: val_loss did not improve from 29.31252
196/196 - 36s - loss: 29.4064 - MinusLogProbMetric: 29.4064 - val_loss: 30.4874 - val_MinusLogProbMetric: 30.4874 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 146/1000
2023-09-29 02:12:32.366 
Epoch 146/1000 
	 loss: 29.5637, MinusLogProbMetric: 29.5637, val_loss: 31.4725, val_MinusLogProbMetric: 31.4725

Epoch 146: val_loss did not improve from 29.31252
196/196 - 34s - loss: 29.5637 - MinusLogProbMetric: 29.5637 - val_loss: 31.4725 - val_MinusLogProbMetric: 31.4725 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 147/1000
2023-09-29 02:13:06.491 
Epoch 147/1000 
	 loss: 29.5387, MinusLogProbMetric: 29.5387, val_loss: 30.3381, val_MinusLogProbMetric: 30.3381

Epoch 147: val_loss did not improve from 29.31252
196/196 - 34s - loss: 29.5387 - MinusLogProbMetric: 29.5387 - val_loss: 30.3381 - val_MinusLogProbMetric: 30.3381 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 148/1000
2023-09-29 02:13:40.634 
Epoch 148/1000 
	 loss: 29.5564, MinusLogProbMetric: 29.5564, val_loss: 31.6017, val_MinusLogProbMetric: 31.6017

Epoch 148: val_loss did not improve from 29.31252
196/196 - 34s - loss: 29.5564 - MinusLogProbMetric: 29.5564 - val_loss: 31.6017 - val_MinusLogProbMetric: 31.6017 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 149/1000
2023-09-29 02:14:12.701 
Epoch 149/1000 
	 loss: 29.4810, MinusLogProbMetric: 29.4810, val_loss: 29.1469, val_MinusLogProbMetric: 29.1469

Epoch 149: val_loss improved from 29.31252 to 29.14686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 29.4810 - MinusLogProbMetric: 29.4810 - val_loss: 29.1469 - val_MinusLogProbMetric: 29.1469 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 150/1000
2023-09-29 02:14:45.432 
Epoch 150/1000 
	 loss: 29.2859, MinusLogProbMetric: 29.2859, val_loss: 29.5869, val_MinusLogProbMetric: 29.5869

Epoch 150: val_loss did not improve from 29.14686
196/196 - 32s - loss: 29.2859 - MinusLogProbMetric: 29.2859 - val_loss: 29.5869 - val_MinusLogProbMetric: 29.5869 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 151/1000
2023-09-29 02:15:17.241 
Epoch 151/1000 
	 loss: 29.2766, MinusLogProbMetric: 29.2766, val_loss: 31.1921, val_MinusLogProbMetric: 31.1921

Epoch 151: val_loss did not improve from 29.14686
196/196 - 32s - loss: 29.2766 - MinusLogProbMetric: 29.2766 - val_loss: 31.1921 - val_MinusLogProbMetric: 31.1921 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 152/1000
2023-09-29 02:15:49.846 
Epoch 152/1000 
	 loss: 29.4803, MinusLogProbMetric: 29.4803, val_loss: 30.2115, val_MinusLogProbMetric: 30.2115

Epoch 152: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.4803 - MinusLogProbMetric: 29.4803 - val_loss: 30.2115 - val_MinusLogProbMetric: 30.2115 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 153/1000
2023-09-29 02:16:22.409 
Epoch 153/1000 
	 loss: 29.4820, MinusLogProbMetric: 29.4820, val_loss: 31.8499, val_MinusLogProbMetric: 31.8499

Epoch 153: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.4820 - MinusLogProbMetric: 29.4820 - val_loss: 31.8499 - val_MinusLogProbMetric: 31.8499 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 154/1000
2023-09-29 02:16:54.424 
Epoch 154/1000 
	 loss: 29.4681, MinusLogProbMetric: 29.4681, val_loss: 29.9662, val_MinusLogProbMetric: 29.9662

Epoch 154: val_loss did not improve from 29.14686
196/196 - 32s - loss: 29.4681 - MinusLogProbMetric: 29.4681 - val_loss: 29.9662 - val_MinusLogProbMetric: 29.9662 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 155/1000
2023-09-29 02:17:28.535 
Epoch 155/1000 
	 loss: 29.3916, MinusLogProbMetric: 29.3916, val_loss: 29.6330, val_MinusLogProbMetric: 29.6330

Epoch 155: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.3916 - MinusLogProbMetric: 29.3916 - val_loss: 29.6330 - val_MinusLogProbMetric: 29.6330 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 156/1000
2023-09-29 02:18:01.853 
Epoch 156/1000 
	 loss: 29.4791, MinusLogProbMetric: 29.4791, val_loss: 30.9245, val_MinusLogProbMetric: 30.9245

Epoch 156: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.4791 - MinusLogProbMetric: 29.4791 - val_loss: 30.9245 - val_MinusLogProbMetric: 30.9245 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 157/1000
2023-09-29 02:18:36.264 
Epoch 157/1000 
	 loss: 29.3455, MinusLogProbMetric: 29.3455, val_loss: 31.0512, val_MinusLogProbMetric: 31.0512

Epoch 157: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.3455 - MinusLogProbMetric: 29.3455 - val_loss: 31.0512 - val_MinusLogProbMetric: 31.0512 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 158/1000
2023-09-29 02:19:09.739 
Epoch 158/1000 
	 loss: 29.4289, MinusLogProbMetric: 29.4289, val_loss: 31.0019, val_MinusLogProbMetric: 31.0019

Epoch 158: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.4289 - MinusLogProbMetric: 29.4289 - val_loss: 31.0019 - val_MinusLogProbMetric: 31.0019 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 159/1000
2023-09-29 02:19:42.683 
Epoch 159/1000 
	 loss: 29.4476, MinusLogProbMetric: 29.4476, val_loss: 29.3221, val_MinusLogProbMetric: 29.3221

Epoch 159: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.4476 - MinusLogProbMetric: 29.4476 - val_loss: 29.3221 - val_MinusLogProbMetric: 29.3221 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 160/1000
2023-09-29 02:20:15.017 
Epoch 160/1000 
	 loss: 29.2874, MinusLogProbMetric: 29.2874, val_loss: 29.9835, val_MinusLogProbMetric: 29.9835

Epoch 160: val_loss did not improve from 29.14686
196/196 - 32s - loss: 29.2874 - MinusLogProbMetric: 29.2874 - val_loss: 29.9835 - val_MinusLogProbMetric: 29.9835 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 161/1000
2023-09-29 02:20:48.783 
Epoch 161/1000 
	 loss: 29.4733, MinusLogProbMetric: 29.4733, val_loss: 30.4174, val_MinusLogProbMetric: 30.4174

Epoch 161: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.4733 - MinusLogProbMetric: 29.4733 - val_loss: 30.4174 - val_MinusLogProbMetric: 30.4174 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 162/1000
2023-09-29 02:21:22.355 
Epoch 162/1000 
	 loss: 29.2594, MinusLogProbMetric: 29.2594, val_loss: 29.7099, val_MinusLogProbMetric: 29.7099

Epoch 162: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.2594 - MinusLogProbMetric: 29.2594 - val_loss: 29.7099 - val_MinusLogProbMetric: 29.7099 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 163/1000
2023-09-29 02:21:54.902 
Epoch 163/1000 
	 loss: 29.3180, MinusLogProbMetric: 29.3180, val_loss: 29.5359, val_MinusLogProbMetric: 29.5359

Epoch 163: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.3180 - MinusLogProbMetric: 29.3180 - val_loss: 29.5359 - val_MinusLogProbMetric: 29.5359 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 164/1000
2023-09-29 02:22:25.350 
Epoch 164/1000 
	 loss: 29.4197, MinusLogProbMetric: 29.4197, val_loss: 30.0957, val_MinusLogProbMetric: 30.0957

Epoch 164: val_loss did not improve from 29.14686
196/196 - 30s - loss: 29.4197 - MinusLogProbMetric: 29.4197 - val_loss: 30.0957 - val_MinusLogProbMetric: 30.0957 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 165/1000
2023-09-29 02:22:58.310 
Epoch 165/1000 
	 loss: 29.3309, MinusLogProbMetric: 29.3309, val_loss: 29.3205, val_MinusLogProbMetric: 29.3205

Epoch 165: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.3309 - MinusLogProbMetric: 29.3309 - val_loss: 29.3205 - val_MinusLogProbMetric: 29.3205 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 166/1000
2023-09-29 02:23:32.649 
Epoch 166/1000 
	 loss: 29.2151, MinusLogProbMetric: 29.2151, val_loss: 29.5757, val_MinusLogProbMetric: 29.5757

Epoch 166: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.2151 - MinusLogProbMetric: 29.2151 - val_loss: 29.5757 - val_MinusLogProbMetric: 29.5757 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 167/1000
2023-09-29 02:24:05.568 
Epoch 167/1000 
	 loss: 29.4113, MinusLogProbMetric: 29.4113, val_loss: 31.2176, val_MinusLogProbMetric: 31.2176

Epoch 167: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.4113 - MinusLogProbMetric: 29.4113 - val_loss: 31.2176 - val_MinusLogProbMetric: 31.2176 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 168/1000
2023-09-29 02:24:39.632 
Epoch 168/1000 
	 loss: 29.5175, MinusLogProbMetric: 29.5175, val_loss: 30.1297, val_MinusLogProbMetric: 30.1297

Epoch 168: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.5175 - MinusLogProbMetric: 29.5175 - val_loss: 30.1297 - val_MinusLogProbMetric: 30.1297 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 169/1000
2023-09-29 02:25:10.758 
Epoch 169/1000 
	 loss: 29.2261, MinusLogProbMetric: 29.2261, val_loss: 29.4039, val_MinusLogProbMetric: 29.4039

Epoch 169: val_loss did not improve from 29.14686
196/196 - 31s - loss: 29.2261 - MinusLogProbMetric: 29.2261 - val_loss: 29.4039 - val_MinusLogProbMetric: 29.4039 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 170/1000
2023-09-29 02:25:43.352 
Epoch 170/1000 
	 loss: 29.2108, MinusLogProbMetric: 29.2108, val_loss: 30.4738, val_MinusLogProbMetric: 30.4738

Epoch 170: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.2108 - MinusLogProbMetric: 29.2108 - val_loss: 30.4738 - val_MinusLogProbMetric: 30.4738 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 171/1000
2023-09-29 02:26:13.645 
Epoch 171/1000 
	 loss: 29.2248, MinusLogProbMetric: 29.2248, val_loss: 29.7754, val_MinusLogProbMetric: 29.7754

Epoch 171: val_loss did not improve from 29.14686
196/196 - 30s - loss: 29.2248 - MinusLogProbMetric: 29.2248 - val_loss: 29.7754 - val_MinusLogProbMetric: 29.7754 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 172/1000
2023-09-29 02:26:48.456 
Epoch 172/1000 
	 loss: 29.2080, MinusLogProbMetric: 29.2080, val_loss: 29.3977, val_MinusLogProbMetric: 29.3977

Epoch 172: val_loss did not improve from 29.14686
196/196 - 35s - loss: 29.2080 - MinusLogProbMetric: 29.2080 - val_loss: 29.3977 - val_MinusLogProbMetric: 29.3977 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 173/1000
2023-09-29 02:27:23.354 
Epoch 173/1000 
	 loss: 29.2796, MinusLogProbMetric: 29.2796, val_loss: 32.2467, val_MinusLogProbMetric: 32.2467

Epoch 173: val_loss did not improve from 29.14686
196/196 - 35s - loss: 29.2796 - MinusLogProbMetric: 29.2796 - val_loss: 32.2467 - val_MinusLogProbMetric: 32.2467 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 174/1000
2023-09-29 02:27:58.627 
Epoch 174/1000 
	 loss: 29.2041, MinusLogProbMetric: 29.2041, val_loss: 29.6136, val_MinusLogProbMetric: 29.6136

Epoch 174: val_loss did not improve from 29.14686
196/196 - 35s - loss: 29.2041 - MinusLogProbMetric: 29.2041 - val_loss: 29.6136 - val_MinusLogProbMetric: 29.6136 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 175/1000
2023-09-29 02:28:30.946 
Epoch 175/1000 
	 loss: 29.2727, MinusLogProbMetric: 29.2727, val_loss: 29.5023, val_MinusLogProbMetric: 29.5023

Epoch 175: val_loss did not improve from 29.14686
196/196 - 32s - loss: 29.2727 - MinusLogProbMetric: 29.2727 - val_loss: 29.5023 - val_MinusLogProbMetric: 29.5023 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 176/1000
2023-09-29 02:29:04.016 
Epoch 176/1000 
	 loss: 29.2565, MinusLogProbMetric: 29.2565, val_loss: 30.7282, val_MinusLogProbMetric: 30.7282

Epoch 176: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.2565 - MinusLogProbMetric: 29.2565 - val_loss: 30.7282 - val_MinusLogProbMetric: 30.7282 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 177/1000
2023-09-29 02:29:39.515 
Epoch 177/1000 
	 loss: 29.1837, MinusLogProbMetric: 29.1837, val_loss: 30.0084, val_MinusLogProbMetric: 30.0084

Epoch 177: val_loss did not improve from 29.14686
196/196 - 35s - loss: 29.1837 - MinusLogProbMetric: 29.1837 - val_loss: 30.0084 - val_MinusLogProbMetric: 30.0084 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 178/1000
2023-09-29 02:30:15.013 
Epoch 178/1000 
	 loss: 29.1194, MinusLogProbMetric: 29.1194, val_loss: 30.1418, val_MinusLogProbMetric: 30.1418

Epoch 178: val_loss did not improve from 29.14686
196/196 - 35s - loss: 29.1194 - MinusLogProbMetric: 29.1194 - val_loss: 30.1418 - val_MinusLogProbMetric: 30.1418 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 179/1000
2023-09-29 02:30:47.152 
Epoch 179/1000 
	 loss: 29.1496, MinusLogProbMetric: 29.1496, val_loss: 31.4071, val_MinusLogProbMetric: 31.4071

Epoch 179: val_loss did not improve from 29.14686
196/196 - 32s - loss: 29.1496 - MinusLogProbMetric: 29.1496 - val_loss: 31.4071 - val_MinusLogProbMetric: 31.4071 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 180/1000
2023-09-29 02:31:20.383 
Epoch 180/1000 
	 loss: 29.2755, MinusLogProbMetric: 29.2755, val_loss: 29.2026, val_MinusLogProbMetric: 29.2026

Epoch 180: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.2755 - MinusLogProbMetric: 29.2755 - val_loss: 29.2026 - val_MinusLogProbMetric: 29.2026 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 181/1000
2023-09-29 02:31:53.160 
Epoch 181/1000 
	 loss: 29.1709, MinusLogProbMetric: 29.1709, val_loss: 29.6163, val_MinusLogProbMetric: 29.6163

Epoch 181: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.1709 - MinusLogProbMetric: 29.1709 - val_loss: 29.6163 - val_MinusLogProbMetric: 29.6163 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 182/1000
2023-09-29 02:32:23.898 
Epoch 182/1000 
	 loss: 29.0794, MinusLogProbMetric: 29.0794, val_loss: 29.2991, val_MinusLogProbMetric: 29.2991

Epoch 182: val_loss did not improve from 29.14686
196/196 - 31s - loss: 29.0794 - MinusLogProbMetric: 29.0794 - val_loss: 29.2991 - val_MinusLogProbMetric: 29.2991 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 183/1000
2023-09-29 02:32:57.609 
Epoch 183/1000 
	 loss: 29.1728, MinusLogProbMetric: 29.1728, val_loss: 29.2936, val_MinusLogProbMetric: 29.2936

Epoch 183: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.1728 - MinusLogProbMetric: 29.1728 - val_loss: 29.2936 - val_MinusLogProbMetric: 29.2936 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 184/1000
2023-09-29 02:33:31.772 
Epoch 184/1000 
	 loss: 29.1728, MinusLogProbMetric: 29.1728, val_loss: 29.6144, val_MinusLogProbMetric: 29.6144

Epoch 184: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.1728 - MinusLogProbMetric: 29.1728 - val_loss: 29.6144 - val_MinusLogProbMetric: 29.6144 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 185/1000
2023-09-29 02:34:04.319 
Epoch 185/1000 
	 loss: 29.1630, MinusLogProbMetric: 29.1630, val_loss: 29.1896, val_MinusLogProbMetric: 29.1896

Epoch 185: val_loss did not improve from 29.14686
196/196 - 33s - loss: 29.1630 - MinusLogProbMetric: 29.1630 - val_loss: 29.1896 - val_MinusLogProbMetric: 29.1896 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 186/1000
2023-09-29 02:34:36.752 
Epoch 186/1000 
	 loss: 29.1827, MinusLogProbMetric: 29.1827, val_loss: 29.6209, val_MinusLogProbMetric: 29.6209

Epoch 186: val_loss did not improve from 29.14686
196/196 - 32s - loss: 29.1827 - MinusLogProbMetric: 29.1827 - val_loss: 29.6209 - val_MinusLogProbMetric: 29.6209 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 187/1000
2023-09-29 02:35:08.099 
Epoch 187/1000 
	 loss: 29.1239, MinusLogProbMetric: 29.1239, val_loss: 30.2015, val_MinusLogProbMetric: 30.2015

Epoch 187: val_loss did not improve from 29.14686
196/196 - 31s - loss: 29.1239 - MinusLogProbMetric: 29.1239 - val_loss: 30.2015 - val_MinusLogProbMetric: 30.2015 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 188/1000
2023-09-29 02:35:42.503 
Epoch 188/1000 
	 loss: 29.0821, MinusLogProbMetric: 29.0821, val_loss: 29.6262, val_MinusLogProbMetric: 29.6262

Epoch 188: val_loss did not improve from 29.14686
196/196 - 34s - loss: 29.0821 - MinusLogProbMetric: 29.0821 - val_loss: 29.6262 - val_MinusLogProbMetric: 29.6262 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 189/1000
2023-09-29 02:36:17.467 
Epoch 189/1000 
	 loss: 29.0663, MinusLogProbMetric: 29.0663, val_loss: 29.0718, val_MinusLogProbMetric: 29.0718

Epoch 189: val_loss improved from 29.14686 to 29.07182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 36s - loss: 29.0663 - MinusLogProbMetric: 29.0663 - val_loss: 29.0718 - val_MinusLogProbMetric: 29.0718 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 190/1000
2023-09-29 02:36:53.728 
Epoch 190/1000 
	 loss: 29.0719, MinusLogProbMetric: 29.0719, val_loss: 29.9309, val_MinusLogProbMetric: 29.9309

Epoch 190: val_loss did not improve from 29.07182
196/196 - 35s - loss: 29.0719 - MinusLogProbMetric: 29.0719 - val_loss: 29.9309 - val_MinusLogProbMetric: 29.9309 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 191/1000
2023-09-29 02:37:28.571 
Epoch 191/1000 
	 loss: 29.0457, MinusLogProbMetric: 29.0457, val_loss: 29.9759, val_MinusLogProbMetric: 29.9759

Epoch 191: val_loss did not improve from 29.07182
196/196 - 35s - loss: 29.0457 - MinusLogProbMetric: 29.0457 - val_loss: 29.9759 - val_MinusLogProbMetric: 29.9759 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 192/1000
2023-09-29 02:38:00.852 
Epoch 192/1000 
	 loss: 29.0406, MinusLogProbMetric: 29.0406, val_loss: 29.5071, val_MinusLogProbMetric: 29.5071

Epoch 192: val_loss did not improve from 29.07182
196/196 - 32s - loss: 29.0406 - MinusLogProbMetric: 29.0406 - val_loss: 29.5071 - val_MinusLogProbMetric: 29.5071 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 193/1000
2023-09-29 02:38:35.458 
Epoch 193/1000 
	 loss: 29.0967, MinusLogProbMetric: 29.0967, val_loss: 29.2937, val_MinusLogProbMetric: 29.2937

Epoch 193: val_loss did not improve from 29.07182
196/196 - 35s - loss: 29.0967 - MinusLogProbMetric: 29.0967 - val_loss: 29.2937 - val_MinusLogProbMetric: 29.2937 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 194/1000
2023-09-29 02:39:09.608 
Epoch 194/1000 
	 loss: 29.0680, MinusLogProbMetric: 29.0680, val_loss: 29.2713, val_MinusLogProbMetric: 29.2713

Epoch 194: val_loss did not improve from 29.07182
196/196 - 34s - loss: 29.0680 - MinusLogProbMetric: 29.0680 - val_loss: 29.2713 - val_MinusLogProbMetric: 29.2713 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 195/1000
2023-09-29 02:39:45.174 
Epoch 195/1000 
	 loss: 28.9825, MinusLogProbMetric: 28.9825, val_loss: 29.2500, val_MinusLogProbMetric: 29.2500

Epoch 195: val_loss did not improve from 29.07182
196/196 - 36s - loss: 28.9825 - MinusLogProbMetric: 28.9825 - val_loss: 29.2500 - val_MinusLogProbMetric: 29.2500 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 196/1000
2023-09-29 02:40:17.771 
Epoch 196/1000 
	 loss: 28.9874, MinusLogProbMetric: 28.9874, val_loss: 29.3396, val_MinusLogProbMetric: 29.3396

Epoch 196: val_loss did not improve from 29.07182
196/196 - 33s - loss: 28.9874 - MinusLogProbMetric: 28.9874 - val_loss: 29.3396 - val_MinusLogProbMetric: 29.3396 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 197/1000
2023-09-29 02:40:51.621 
Epoch 197/1000 
	 loss: 29.0790, MinusLogProbMetric: 29.0790, val_loss: 29.3120, val_MinusLogProbMetric: 29.3120

Epoch 197: val_loss did not improve from 29.07182
196/196 - 34s - loss: 29.0790 - MinusLogProbMetric: 29.0790 - val_loss: 29.3120 - val_MinusLogProbMetric: 29.3120 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 198/1000
2023-09-29 02:41:25.354 
Epoch 198/1000 
	 loss: 29.1761, MinusLogProbMetric: 29.1761, val_loss: 29.8998, val_MinusLogProbMetric: 29.8998

Epoch 198: val_loss did not improve from 29.07182
196/196 - 34s - loss: 29.1761 - MinusLogProbMetric: 29.1761 - val_loss: 29.8998 - val_MinusLogProbMetric: 29.8998 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 199/1000
2023-09-29 02:41:56.459 
Epoch 199/1000 
	 loss: 28.9818, MinusLogProbMetric: 28.9818, val_loss: 30.5239, val_MinusLogProbMetric: 30.5239

Epoch 199: val_loss did not improve from 29.07182
196/196 - 31s - loss: 28.9818 - MinusLogProbMetric: 28.9818 - val_loss: 30.5239 - val_MinusLogProbMetric: 30.5239 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 200/1000
2023-09-29 02:42:28.553 
Epoch 200/1000 
	 loss: 28.8544, MinusLogProbMetric: 28.8544, val_loss: 29.9783, val_MinusLogProbMetric: 29.9783

Epoch 200: val_loss did not improve from 29.07182
196/196 - 32s - loss: 28.8544 - MinusLogProbMetric: 28.8544 - val_loss: 29.9783 - val_MinusLogProbMetric: 29.9783 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 201/1000
2023-09-29 02:43:01.663 
Epoch 201/1000 
	 loss: 28.9295, MinusLogProbMetric: 28.9295, val_loss: 29.6226, val_MinusLogProbMetric: 29.6226

Epoch 201: val_loss did not improve from 29.07182
196/196 - 33s - loss: 28.9295 - MinusLogProbMetric: 28.9295 - val_loss: 29.6226 - val_MinusLogProbMetric: 29.6226 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 202/1000
2023-09-29 02:43:32.807 
Epoch 202/1000 
	 loss: 28.8662, MinusLogProbMetric: 28.8662, val_loss: 29.3724, val_MinusLogProbMetric: 29.3724

Epoch 202: val_loss did not improve from 29.07182
196/196 - 31s - loss: 28.8662 - MinusLogProbMetric: 28.8662 - val_loss: 29.3724 - val_MinusLogProbMetric: 29.3724 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 203/1000
2023-09-29 02:44:03.305 
Epoch 203/1000 
	 loss: 28.8100, MinusLogProbMetric: 28.8100, val_loss: 30.0761, val_MinusLogProbMetric: 30.0761

Epoch 203: val_loss did not improve from 29.07182
196/196 - 30s - loss: 28.8100 - MinusLogProbMetric: 28.8100 - val_loss: 30.0761 - val_MinusLogProbMetric: 30.0761 - lr: 0.0010 - 30s/epoch - 156ms/step
Epoch 204/1000
2023-09-29 02:44:38.582 
Epoch 204/1000 
	 loss: 28.9857, MinusLogProbMetric: 28.9857, val_loss: 29.2173, val_MinusLogProbMetric: 29.2173

Epoch 204: val_loss did not improve from 29.07182
196/196 - 35s - loss: 28.9857 - MinusLogProbMetric: 28.9857 - val_loss: 29.2173 - val_MinusLogProbMetric: 29.2173 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 205/1000
2023-09-29 02:45:14.139 
Epoch 205/1000 
	 loss: 28.9881, MinusLogProbMetric: 28.9881, val_loss: 29.5101, val_MinusLogProbMetric: 29.5101

Epoch 205: val_loss did not improve from 29.07182
196/196 - 36s - loss: 28.9881 - MinusLogProbMetric: 28.9881 - val_loss: 29.5101 - val_MinusLogProbMetric: 29.5101 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 206/1000
2023-09-29 02:45:49.353 
Epoch 206/1000 
	 loss: 29.0131, MinusLogProbMetric: 29.0131, val_loss: 29.0728, val_MinusLogProbMetric: 29.0728

Epoch 206: val_loss did not improve from 29.07182
196/196 - 35s - loss: 29.0131 - MinusLogProbMetric: 29.0131 - val_loss: 29.0728 - val_MinusLogProbMetric: 29.0728 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 207/1000
2023-09-29 02:46:24.809 
Epoch 207/1000 
	 loss: 28.8622, MinusLogProbMetric: 28.8622, val_loss: 29.1006, val_MinusLogProbMetric: 29.1006

Epoch 207: val_loss did not improve from 29.07182
196/196 - 35s - loss: 28.8622 - MinusLogProbMetric: 28.8622 - val_loss: 29.1006 - val_MinusLogProbMetric: 29.1006 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 208/1000
2023-09-29 02:46:59.397 
Epoch 208/1000 
	 loss: 28.8843, MinusLogProbMetric: 28.8843, val_loss: 29.7115, val_MinusLogProbMetric: 29.7115

Epoch 208: val_loss did not improve from 29.07182
196/196 - 35s - loss: 28.8843 - MinusLogProbMetric: 28.8843 - val_loss: 29.7115 - val_MinusLogProbMetric: 29.7115 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 209/1000
2023-09-29 02:47:34.729 
Epoch 209/1000 
	 loss: 28.7900, MinusLogProbMetric: 28.7900, val_loss: 29.9559, val_MinusLogProbMetric: 29.9559

Epoch 209: val_loss did not improve from 29.07182
196/196 - 35s - loss: 28.7900 - MinusLogProbMetric: 28.7900 - val_loss: 29.9559 - val_MinusLogProbMetric: 29.9559 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 210/1000
2023-09-29 02:48:09.300 
Epoch 210/1000 
	 loss: 29.2239, MinusLogProbMetric: 29.2239, val_loss: 29.0943, val_MinusLogProbMetric: 29.0943

Epoch 210: val_loss did not improve from 29.07182
196/196 - 35s - loss: 29.2239 - MinusLogProbMetric: 29.2239 - val_loss: 29.0943 - val_MinusLogProbMetric: 29.0943 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 211/1000
2023-09-29 02:48:42.904 
Epoch 211/1000 
	 loss: 29.0951, MinusLogProbMetric: 29.0951, val_loss: 29.2264, val_MinusLogProbMetric: 29.2264

Epoch 211: val_loss did not improve from 29.07182
196/196 - 34s - loss: 29.0951 - MinusLogProbMetric: 29.0951 - val_loss: 29.2264 - val_MinusLogProbMetric: 29.2264 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 212/1000
2023-09-29 02:49:13.510 
Epoch 212/1000 
	 loss: 28.8987, MinusLogProbMetric: 28.8987, val_loss: 29.2906, val_MinusLogProbMetric: 29.2906

Epoch 212: val_loss did not improve from 29.07182
196/196 - 31s - loss: 28.8987 - MinusLogProbMetric: 28.8987 - val_loss: 29.2906 - val_MinusLogProbMetric: 29.2906 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 213/1000
2023-09-29 02:49:45.704 
Epoch 213/1000 
	 loss: 28.8775, MinusLogProbMetric: 28.8775, val_loss: 30.5923, val_MinusLogProbMetric: 30.5923

Epoch 213: val_loss did not improve from 29.07182
196/196 - 32s - loss: 28.8775 - MinusLogProbMetric: 28.8775 - val_loss: 30.5923 - val_MinusLogProbMetric: 30.5923 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 214/1000
2023-09-29 02:50:18.204 
Epoch 214/1000 
	 loss: 29.1252, MinusLogProbMetric: 29.1252, val_loss: 29.8719, val_MinusLogProbMetric: 29.8719

Epoch 214: val_loss did not improve from 29.07182
196/196 - 33s - loss: 29.1252 - MinusLogProbMetric: 29.1252 - val_loss: 29.8719 - val_MinusLogProbMetric: 29.8719 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 215/1000
2023-09-29 02:50:50.301 
Epoch 215/1000 
	 loss: 29.1402, MinusLogProbMetric: 29.1402, val_loss: 29.5175, val_MinusLogProbMetric: 29.5175

Epoch 215: val_loss did not improve from 29.07182
196/196 - 32s - loss: 29.1402 - MinusLogProbMetric: 29.1402 - val_loss: 29.5175 - val_MinusLogProbMetric: 29.5175 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 216/1000
2023-09-29 02:51:22.677 
Epoch 216/1000 
	 loss: 28.7625, MinusLogProbMetric: 28.7625, val_loss: 29.1535, val_MinusLogProbMetric: 29.1535

Epoch 216: val_loss did not improve from 29.07182
196/196 - 32s - loss: 28.7625 - MinusLogProbMetric: 28.7625 - val_loss: 29.1535 - val_MinusLogProbMetric: 29.1535 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 217/1000
2023-09-29 02:51:55.864 
Epoch 217/1000 
	 loss: 28.9246, MinusLogProbMetric: 28.9246, val_loss: 29.0662, val_MinusLogProbMetric: 29.0662

Epoch 217: val_loss improved from 29.07182 to 29.06616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 28.9246 - MinusLogProbMetric: 28.9246 - val_loss: 29.0662 - val_MinusLogProbMetric: 29.0662 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 218/1000
2023-09-29 02:52:30.458 
Epoch 218/1000 
	 loss: 28.9062, MinusLogProbMetric: 28.9062, val_loss: 30.0480, val_MinusLogProbMetric: 30.0480

Epoch 218: val_loss did not improve from 29.06616
196/196 - 34s - loss: 28.9062 - MinusLogProbMetric: 28.9062 - val_loss: 30.0480 - val_MinusLogProbMetric: 30.0480 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 219/1000
2023-09-29 02:53:03.333 
Epoch 219/1000 
	 loss: 28.8184, MinusLogProbMetric: 28.8184, val_loss: 29.4200, val_MinusLogProbMetric: 29.4200

Epoch 219: val_loss did not improve from 29.06616
196/196 - 33s - loss: 28.8184 - MinusLogProbMetric: 28.8184 - val_loss: 29.4200 - val_MinusLogProbMetric: 29.4200 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 220/1000
2023-09-29 02:53:36.891 
Epoch 220/1000 
	 loss: 28.8452, MinusLogProbMetric: 28.8452, val_loss: 29.4844, val_MinusLogProbMetric: 29.4844

Epoch 220: val_loss did not improve from 29.06616
196/196 - 34s - loss: 28.8452 - MinusLogProbMetric: 28.8452 - val_loss: 29.4844 - val_MinusLogProbMetric: 29.4844 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 221/1000
2023-09-29 02:54:12.514 
Epoch 221/1000 
	 loss: 28.9617, MinusLogProbMetric: 28.9617, val_loss: 29.3464, val_MinusLogProbMetric: 29.3464

Epoch 221: val_loss did not improve from 29.06616
196/196 - 36s - loss: 28.9617 - MinusLogProbMetric: 28.9617 - val_loss: 29.3464 - val_MinusLogProbMetric: 29.3464 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 222/1000
2023-09-29 02:54:47.880 
Epoch 222/1000 
	 loss: 28.7960, MinusLogProbMetric: 28.7960, val_loss: 29.3240, val_MinusLogProbMetric: 29.3240

Epoch 222: val_loss did not improve from 29.06616
196/196 - 35s - loss: 28.7960 - MinusLogProbMetric: 28.7960 - val_loss: 29.3240 - val_MinusLogProbMetric: 29.3240 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 223/1000
2023-09-29 02:55:23.632 
Epoch 223/1000 
	 loss: 28.8005, MinusLogProbMetric: 28.8005, val_loss: 30.1741, val_MinusLogProbMetric: 30.1741

Epoch 223: val_loss did not improve from 29.06616
196/196 - 36s - loss: 28.8005 - MinusLogProbMetric: 28.8005 - val_loss: 30.1741 - val_MinusLogProbMetric: 30.1741 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 224/1000
2023-09-29 02:55:58.141 
Epoch 224/1000 
	 loss: 28.9571, MinusLogProbMetric: 28.9571, val_loss: 29.6785, val_MinusLogProbMetric: 29.6785

Epoch 224: val_loss did not improve from 29.06616
196/196 - 35s - loss: 28.9571 - MinusLogProbMetric: 28.9571 - val_loss: 29.6785 - val_MinusLogProbMetric: 29.6785 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 225/1000
2023-09-29 02:56:33.252 
Epoch 225/1000 
	 loss: 28.8885, MinusLogProbMetric: 28.8885, val_loss: 30.0960, val_MinusLogProbMetric: 30.0960

Epoch 225: val_loss did not improve from 29.06616
196/196 - 35s - loss: 28.8885 - MinusLogProbMetric: 28.8885 - val_loss: 30.0960 - val_MinusLogProbMetric: 30.0960 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 226/1000
2023-09-29 02:57:08.728 
Epoch 226/1000 
	 loss: 28.7438, MinusLogProbMetric: 28.7438, val_loss: 29.2982, val_MinusLogProbMetric: 29.2982

Epoch 226: val_loss did not improve from 29.06616
196/196 - 35s - loss: 28.7438 - MinusLogProbMetric: 28.7438 - val_loss: 29.2982 - val_MinusLogProbMetric: 29.2982 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 227/1000
2023-09-29 02:57:44.330 
Epoch 227/1000 
	 loss: 28.8854, MinusLogProbMetric: 28.8854, val_loss: 30.3645, val_MinusLogProbMetric: 30.3645

Epoch 227: val_loss did not improve from 29.06616
196/196 - 36s - loss: 28.8854 - MinusLogProbMetric: 28.8854 - val_loss: 30.3645 - val_MinusLogProbMetric: 30.3645 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 228/1000
2023-09-29 02:58:20.190 
Epoch 228/1000 
	 loss: 28.9571, MinusLogProbMetric: 28.9571, val_loss: 29.1333, val_MinusLogProbMetric: 29.1333

Epoch 228: val_loss did not improve from 29.06616
196/196 - 36s - loss: 28.9571 - MinusLogProbMetric: 28.9571 - val_loss: 29.1333 - val_MinusLogProbMetric: 29.1333 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 229/1000
2023-09-29 02:58:54.964 
Epoch 229/1000 
	 loss: 28.8041, MinusLogProbMetric: 28.8041, val_loss: 29.5695, val_MinusLogProbMetric: 29.5695

Epoch 229: val_loss did not improve from 29.06616
196/196 - 35s - loss: 28.8041 - MinusLogProbMetric: 28.8041 - val_loss: 29.5695 - val_MinusLogProbMetric: 29.5695 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 230/1000
2023-09-29 02:59:30.519 
Epoch 230/1000 
	 loss: 28.8942, MinusLogProbMetric: 28.8942, val_loss: 29.9402, val_MinusLogProbMetric: 29.9402

Epoch 230: val_loss did not improve from 29.06616
196/196 - 36s - loss: 28.8942 - MinusLogProbMetric: 28.8942 - val_loss: 29.9402 - val_MinusLogProbMetric: 29.9402 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 231/1000
2023-09-29 03:00:03.180 
Epoch 231/1000 
	 loss: 28.8390, MinusLogProbMetric: 28.8390, val_loss: 29.0160, val_MinusLogProbMetric: 29.0160

Epoch 231: val_loss improved from 29.06616 to 29.01600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 28.8390 - MinusLogProbMetric: 28.8390 - val_loss: 29.0160 - val_MinusLogProbMetric: 29.0160 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 232/1000
2023-09-29 03:00:37.780 
Epoch 232/1000 
	 loss: 28.8204, MinusLogProbMetric: 28.8204, val_loss: 28.9896, val_MinusLogProbMetric: 28.9896

Epoch 232: val_loss improved from 29.01600 to 28.98955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 35s - loss: 28.8204 - MinusLogProbMetric: 28.8204 - val_loss: 28.9896 - val_MinusLogProbMetric: 28.9896 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 233/1000
2023-09-29 03:01:13.289 
Epoch 233/1000 
	 loss: 28.8382, MinusLogProbMetric: 28.8382, val_loss: 29.1459, val_MinusLogProbMetric: 29.1459

Epoch 233: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.8382 - MinusLogProbMetric: 28.8382 - val_loss: 29.1459 - val_MinusLogProbMetric: 29.1459 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 234/1000
2023-09-29 03:01:48.577 
Epoch 234/1000 
	 loss: 28.8059, MinusLogProbMetric: 28.8059, val_loss: 29.5772, val_MinusLogProbMetric: 29.5772

Epoch 234: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.8059 - MinusLogProbMetric: 28.8059 - val_loss: 29.5772 - val_MinusLogProbMetric: 29.5772 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 235/1000
2023-09-29 03:02:23.296 
Epoch 235/1000 
	 loss: 28.6248, MinusLogProbMetric: 28.6248, val_loss: 29.0489, val_MinusLogProbMetric: 29.0489

Epoch 235: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.6248 - MinusLogProbMetric: 28.6248 - val_loss: 29.0489 - val_MinusLogProbMetric: 29.0489 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 236/1000
2023-09-29 03:02:58.631 
Epoch 236/1000 
	 loss: 29.0166, MinusLogProbMetric: 29.0166, val_loss: 29.2351, val_MinusLogProbMetric: 29.2351

Epoch 236: val_loss did not improve from 28.98955
196/196 - 35s - loss: 29.0166 - MinusLogProbMetric: 29.0166 - val_loss: 29.2351 - val_MinusLogProbMetric: 29.2351 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 237/1000
2023-09-29 03:03:32.359 
Epoch 237/1000 
	 loss: 28.7052, MinusLogProbMetric: 28.7052, val_loss: 29.2772, val_MinusLogProbMetric: 29.2772

Epoch 237: val_loss did not improve from 28.98955
196/196 - 34s - loss: 28.7052 - MinusLogProbMetric: 28.7052 - val_loss: 29.2772 - val_MinusLogProbMetric: 29.2772 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 238/1000
2023-09-29 03:04:07.919 
Epoch 238/1000 
	 loss: 28.7254, MinusLogProbMetric: 28.7254, val_loss: 29.7918, val_MinusLogProbMetric: 29.7918

Epoch 238: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.7254 - MinusLogProbMetric: 28.7254 - val_loss: 29.7918 - val_MinusLogProbMetric: 29.7918 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 239/1000
2023-09-29 03:04:43.619 
Epoch 239/1000 
	 loss: 28.8322, MinusLogProbMetric: 28.8322, val_loss: 29.6074, val_MinusLogProbMetric: 29.6074

Epoch 239: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.8322 - MinusLogProbMetric: 28.8322 - val_loss: 29.6074 - val_MinusLogProbMetric: 29.6074 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 240/1000
2023-09-29 03:05:19.317 
Epoch 240/1000 
	 loss: 28.7410, MinusLogProbMetric: 28.7410, val_loss: 29.3451, val_MinusLogProbMetric: 29.3451

Epoch 240: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.7410 - MinusLogProbMetric: 28.7410 - val_loss: 29.3451 - val_MinusLogProbMetric: 29.3451 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 241/1000
2023-09-29 03:05:55.035 
Epoch 241/1000 
	 loss: 28.8898, MinusLogProbMetric: 28.8898, val_loss: 29.5274, val_MinusLogProbMetric: 29.5274

Epoch 241: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.8898 - MinusLogProbMetric: 28.8898 - val_loss: 29.5274 - val_MinusLogProbMetric: 29.5274 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 242/1000
2023-09-29 03:06:29.396 
Epoch 242/1000 
	 loss: 28.7582, MinusLogProbMetric: 28.7582, val_loss: 29.8693, val_MinusLogProbMetric: 29.8693

Epoch 242: val_loss did not improve from 28.98955
196/196 - 34s - loss: 28.7582 - MinusLogProbMetric: 28.7582 - val_loss: 29.8693 - val_MinusLogProbMetric: 29.8693 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 243/1000
2023-09-29 03:07:03.914 
Epoch 243/1000 
	 loss: 28.8734, MinusLogProbMetric: 28.8734, val_loss: 29.3094, val_MinusLogProbMetric: 29.3094

Epoch 243: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.8734 - MinusLogProbMetric: 28.8734 - val_loss: 29.3094 - val_MinusLogProbMetric: 29.3094 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 244/1000
2023-09-29 03:07:39.099 
Epoch 244/1000 
	 loss: 28.7367, MinusLogProbMetric: 28.7367, val_loss: 29.3146, val_MinusLogProbMetric: 29.3146

Epoch 244: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.7367 - MinusLogProbMetric: 28.7367 - val_loss: 29.3146 - val_MinusLogProbMetric: 29.3146 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 245/1000
2023-09-29 03:08:13.051 
Epoch 245/1000 
	 loss: 28.7262, MinusLogProbMetric: 28.7262, val_loss: 29.3406, val_MinusLogProbMetric: 29.3406

Epoch 245: val_loss did not improve from 28.98955
196/196 - 34s - loss: 28.7262 - MinusLogProbMetric: 28.7262 - val_loss: 29.3406 - val_MinusLogProbMetric: 29.3406 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 246/1000
2023-09-29 03:08:48.009 
Epoch 246/1000 
	 loss: 28.6881, MinusLogProbMetric: 28.6881, val_loss: 30.5649, val_MinusLogProbMetric: 30.5649

Epoch 246: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.6881 - MinusLogProbMetric: 28.6881 - val_loss: 30.5649 - val_MinusLogProbMetric: 30.5649 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 247/1000
2023-09-29 03:09:23.650 
Epoch 247/1000 
	 loss: 28.8299, MinusLogProbMetric: 28.8299, val_loss: 29.3157, val_MinusLogProbMetric: 29.3157

Epoch 247: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.8299 - MinusLogProbMetric: 28.8299 - val_loss: 29.3157 - val_MinusLogProbMetric: 29.3157 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 248/1000
2023-09-29 03:09:58.328 
Epoch 248/1000 
	 loss: 28.5887, MinusLogProbMetric: 28.5887, val_loss: 29.8169, val_MinusLogProbMetric: 29.8169

Epoch 248: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.5887 - MinusLogProbMetric: 28.5887 - val_loss: 29.8169 - val_MinusLogProbMetric: 29.8169 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 249/1000
2023-09-29 03:10:32.305 
Epoch 249/1000 
	 loss: 28.6775, MinusLogProbMetric: 28.6775, val_loss: 29.5212, val_MinusLogProbMetric: 29.5212

Epoch 249: val_loss did not improve from 28.98955
196/196 - 34s - loss: 28.6775 - MinusLogProbMetric: 28.6775 - val_loss: 29.5212 - val_MinusLogProbMetric: 29.5212 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 250/1000
2023-09-29 03:11:07.825 
Epoch 250/1000 
	 loss: 28.7449, MinusLogProbMetric: 28.7449, val_loss: 30.6852, val_MinusLogProbMetric: 30.6852

Epoch 250: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.7449 - MinusLogProbMetric: 28.7449 - val_loss: 30.6852 - val_MinusLogProbMetric: 30.6852 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 251/1000
2023-09-29 03:11:43.707 
Epoch 251/1000 
	 loss: 28.6890, MinusLogProbMetric: 28.6890, val_loss: 30.2004, val_MinusLogProbMetric: 30.2004

Epoch 251: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.6890 - MinusLogProbMetric: 28.6890 - val_loss: 30.2004 - val_MinusLogProbMetric: 30.2004 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 252/1000
2023-09-29 03:12:19.358 
Epoch 252/1000 
	 loss: 28.8085, MinusLogProbMetric: 28.8085, val_loss: 29.0995, val_MinusLogProbMetric: 29.0995

Epoch 252: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.8085 - MinusLogProbMetric: 28.8085 - val_loss: 29.0995 - val_MinusLogProbMetric: 29.0995 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 253/1000
2023-09-29 03:12:54.982 
Epoch 253/1000 
	 loss: 28.5210, MinusLogProbMetric: 28.5210, val_loss: 29.1797, val_MinusLogProbMetric: 29.1797

Epoch 253: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.5210 - MinusLogProbMetric: 28.5210 - val_loss: 29.1797 - val_MinusLogProbMetric: 29.1797 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 254/1000
2023-09-29 03:13:30.527 
Epoch 254/1000 
	 loss: 28.8361, MinusLogProbMetric: 28.8361, val_loss: 29.3058, val_MinusLogProbMetric: 29.3058

Epoch 254: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.8361 - MinusLogProbMetric: 28.8361 - val_loss: 29.3058 - val_MinusLogProbMetric: 29.3058 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 255/1000
2023-09-29 03:14:06.014 
Epoch 255/1000 
	 loss: 28.6419, MinusLogProbMetric: 28.6419, val_loss: 29.9365, val_MinusLogProbMetric: 29.9365

Epoch 255: val_loss did not improve from 28.98955
196/196 - 35s - loss: 28.6419 - MinusLogProbMetric: 28.6419 - val_loss: 29.9365 - val_MinusLogProbMetric: 29.9365 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 256/1000
2023-09-29 03:14:41.705 
Epoch 256/1000 
	 loss: 28.4686, MinusLogProbMetric: 28.4686, val_loss: 29.2026, val_MinusLogProbMetric: 29.2026

Epoch 256: val_loss did not improve from 28.98955
196/196 - 36s - loss: 28.4686 - MinusLogProbMetric: 28.4686 - val_loss: 29.2026 - val_MinusLogProbMetric: 29.2026 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 257/1000
2023-09-29 03:15:16.678 
Epoch 257/1000 
	 loss: 28.5798, MinusLogProbMetric: 28.5798, val_loss: 28.8643, val_MinusLogProbMetric: 28.8643

Epoch 257: val_loss improved from 28.98955 to 28.86426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 36s - loss: 28.5798 - MinusLogProbMetric: 28.5798 - val_loss: 28.8643 - val_MinusLogProbMetric: 28.8643 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 258/1000
2023-09-29 03:15:52.694 
Epoch 258/1000 
	 loss: 28.7144, MinusLogProbMetric: 28.7144, val_loss: 29.1083, val_MinusLogProbMetric: 29.1083

Epoch 258: val_loss did not improve from 28.86426
196/196 - 35s - loss: 28.7144 - MinusLogProbMetric: 28.7144 - val_loss: 29.1083 - val_MinusLogProbMetric: 29.1083 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 259/1000
2023-09-29 03:16:27.223 
Epoch 259/1000 
	 loss: 28.4827, MinusLogProbMetric: 28.4827, val_loss: 29.3412, val_MinusLogProbMetric: 29.3412

Epoch 259: val_loss did not improve from 28.86426
196/196 - 35s - loss: 28.4827 - MinusLogProbMetric: 28.4827 - val_loss: 29.3412 - val_MinusLogProbMetric: 29.3412 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 260/1000
2023-09-29 03:17:01.295 
Epoch 260/1000 
	 loss: 28.6299, MinusLogProbMetric: 28.6299, val_loss: 29.6808, val_MinusLogProbMetric: 29.6808

Epoch 260: val_loss did not improve from 28.86426
196/196 - 34s - loss: 28.6299 - MinusLogProbMetric: 28.6299 - val_loss: 29.6808 - val_MinusLogProbMetric: 29.6808 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 261/1000
2023-09-29 03:17:35.171 
Epoch 261/1000 
	 loss: 28.6160, MinusLogProbMetric: 28.6160, val_loss: 29.2482, val_MinusLogProbMetric: 29.2482

Epoch 261: val_loss did not improve from 28.86426
196/196 - 34s - loss: 28.6160 - MinusLogProbMetric: 28.6160 - val_loss: 29.2482 - val_MinusLogProbMetric: 29.2482 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 262/1000
2023-09-29 03:18:10.390 
Epoch 262/1000 
	 loss: 28.4458, MinusLogProbMetric: 28.4458, val_loss: 28.9173, val_MinusLogProbMetric: 28.9173

Epoch 262: val_loss did not improve from 28.86426
196/196 - 35s - loss: 28.4458 - MinusLogProbMetric: 28.4458 - val_loss: 28.9173 - val_MinusLogProbMetric: 28.9173 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 263/1000
2023-09-29 03:18:45.902 
Epoch 263/1000 
	 loss: 28.6020, MinusLogProbMetric: 28.6020, val_loss: 28.9213, val_MinusLogProbMetric: 28.9213

Epoch 263: val_loss did not improve from 28.86426
196/196 - 36s - loss: 28.6020 - MinusLogProbMetric: 28.6020 - val_loss: 28.9213 - val_MinusLogProbMetric: 28.9213 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 264/1000
2023-09-29 03:19:20.017 
Epoch 264/1000 
	 loss: 28.5708, MinusLogProbMetric: 28.5708, val_loss: 29.0325, val_MinusLogProbMetric: 29.0325

Epoch 264: val_loss did not improve from 28.86426
196/196 - 34s - loss: 28.5708 - MinusLogProbMetric: 28.5708 - val_loss: 29.0325 - val_MinusLogProbMetric: 29.0325 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 265/1000
2023-09-29 03:19:54.270 
Epoch 265/1000 
	 loss: 28.6692, MinusLogProbMetric: 28.6692, val_loss: 28.9804, val_MinusLogProbMetric: 28.9804

Epoch 265: val_loss did not improve from 28.86426
196/196 - 34s - loss: 28.6692 - MinusLogProbMetric: 28.6692 - val_loss: 28.9804 - val_MinusLogProbMetric: 28.9804 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 266/1000
2023-09-29 03:20:29.454 
Epoch 266/1000 
	 loss: 28.5404, MinusLogProbMetric: 28.5404, val_loss: 28.8602, val_MinusLogProbMetric: 28.8602

Epoch 266: val_loss improved from 28.86426 to 28.86017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 36s - loss: 28.5404 - MinusLogProbMetric: 28.5404 - val_loss: 28.8602 - val_MinusLogProbMetric: 28.8602 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 267/1000
2023-09-29 03:21:05.551 
Epoch 267/1000 
	 loss: 28.5233, MinusLogProbMetric: 28.5233, val_loss: 28.9045, val_MinusLogProbMetric: 28.9045

Epoch 267: val_loss did not improve from 28.86017
196/196 - 36s - loss: 28.5233 - MinusLogProbMetric: 28.5233 - val_loss: 28.9045 - val_MinusLogProbMetric: 28.9045 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 268/1000
2023-09-29 03:21:41.349 
Epoch 268/1000 
	 loss: 28.6563, MinusLogProbMetric: 28.6563, val_loss: 28.7386, val_MinusLogProbMetric: 28.7386

Epoch 268: val_loss improved from 28.86017 to 28.73858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 36s - loss: 28.6563 - MinusLogProbMetric: 28.6563 - val_loss: 28.7386 - val_MinusLogProbMetric: 28.7386 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 269/1000
2023-09-29 03:22:17.179 
Epoch 269/1000 
	 loss: 28.5446, MinusLogProbMetric: 28.5446, val_loss: 30.8435, val_MinusLogProbMetric: 30.8435

Epoch 269: val_loss did not improve from 28.73858
196/196 - 35s - loss: 28.5446 - MinusLogProbMetric: 28.5446 - val_loss: 30.8435 - val_MinusLogProbMetric: 30.8435 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 270/1000
2023-09-29 03:22:51.894 
Epoch 270/1000 
	 loss: 28.6248, MinusLogProbMetric: 28.6248, val_loss: 29.2046, val_MinusLogProbMetric: 29.2046

Epoch 270: val_loss did not improve from 28.73858
196/196 - 35s - loss: 28.6248 - MinusLogProbMetric: 28.6248 - val_loss: 29.2046 - val_MinusLogProbMetric: 29.2046 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 271/1000
2023-09-29 03:23:27.795 
Epoch 271/1000 
	 loss: 28.5083, MinusLogProbMetric: 28.5083, val_loss: 28.8370, val_MinusLogProbMetric: 28.8370

Epoch 271: val_loss did not improve from 28.73858
196/196 - 36s - loss: 28.5083 - MinusLogProbMetric: 28.5083 - val_loss: 28.8370 - val_MinusLogProbMetric: 28.8370 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 272/1000
2023-09-29 03:24:03.511 
Epoch 272/1000 
	 loss: 28.4765, MinusLogProbMetric: 28.4765, val_loss: 28.9184, val_MinusLogProbMetric: 28.9184

Epoch 272: val_loss did not improve from 28.73858
196/196 - 36s - loss: 28.4765 - MinusLogProbMetric: 28.4765 - val_loss: 28.9184 - val_MinusLogProbMetric: 28.9184 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 273/1000
2023-09-29 03:24:38.754 
Epoch 273/1000 
	 loss: 28.5700, MinusLogProbMetric: 28.5700, val_loss: 28.7003, val_MinusLogProbMetric: 28.7003

Epoch 273: val_loss improved from 28.73858 to 28.70025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 36s - loss: 28.5700 - MinusLogProbMetric: 28.5700 - val_loss: 28.7003 - val_MinusLogProbMetric: 28.7003 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 274/1000
2023-09-29 03:25:10.786 
Epoch 274/1000 
	 loss: 28.5970, MinusLogProbMetric: 28.5970, val_loss: 29.6803, val_MinusLogProbMetric: 29.6803

Epoch 274: val_loss did not improve from 28.70025
196/196 - 31s - loss: 28.5970 - MinusLogProbMetric: 28.5970 - val_loss: 29.6803 - val_MinusLogProbMetric: 29.6803 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 275/1000
2023-09-29 03:25:44.363 
Epoch 275/1000 
	 loss: 28.5888, MinusLogProbMetric: 28.5888, val_loss: 28.8321, val_MinusLogProbMetric: 28.8321

Epoch 275: val_loss did not improve from 28.70025
196/196 - 34s - loss: 28.5888 - MinusLogProbMetric: 28.5888 - val_loss: 28.8321 - val_MinusLogProbMetric: 28.8321 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 276/1000
2023-09-29 03:26:19.077 
Epoch 276/1000 
	 loss: 28.6465, MinusLogProbMetric: 28.6465, val_loss: 28.9799, val_MinusLogProbMetric: 28.9799

Epoch 276: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.6465 - MinusLogProbMetric: 28.6465 - val_loss: 28.9799 - val_MinusLogProbMetric: 28.9799 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 277/1000
2023-09-29 03:26:51.636 
Epoch 277/1000 
	 loss: 28.4876, MinusLogProbMetric: 28.4876, val_loss: 29.6038, val_MinusLogProbMetric: 29.6038

Epoch 277: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.4876 - MinusLogProbMetric: 28.4876 - val_loss: 29.6038 - val_MinusLogProbMetric: 29.6038 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 278/1000
2023-09-29 03:27:21.553 
Epoch 278/1000 
	 loss: 28.6869, MinusLogProbMetric: 28.6869, val_loss: 29.9383, val_MinusLogProbMetric: 29.9383

Epoch 278: val_loss did not improve from 28.70025
196/196 - 30s - loss: 28.6869 - MinusLogProbMetric: 28.6869 - val_loss: 29.9383 - val_MinusLogProbMetric: 29.9383 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 279/1000
2023-09-29 03:27:53.919 
Epoch 279/1000 
	 loss: 28.4949, MinusLogProbMetric: 28.4949, val_loss: 29.8250, val_MinusLogProbMetric: 29.8250

Epoch 279: val_loss did not improve from 28.70025
196/196 - 32s - loss: 28.4949 - MinusLogProbMetric: 28.4949 - val_loss: 29.8250 - val_MinusLogProbMetric: 29.8250 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 280/1000
2023-09-29 03:28:26.817 
Epoch 280/1000 
	 loss: 28.4716, MinusLogProbMetric: 28.4716, val_loss: 29.0103, val_MinusLogProbMetric: 29.0103

Epoch 280: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.4716 - MinusLogProbMetric: 28.4716 - val_loss: 29.0103 - val_MinusLogProbMetric: 29.0103 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 281/1000
2023-09-29 03:28:59.063 
Epoch 281/1000 
	 loss: 28.3243, MinusLogProbMetric: 28.3243, val_loss: 28.9947, val_MinusLogProbMetric: 28.9947

Epoch 281: val_loss did not improve from 28.70025
196/196 - 32s - loss: 28.3243 - MinusLogProbMetric: 28.3243 - val_loss: 28.9947 - val_MinusLogProbMetric: 28.9947 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 282/1000
2023-09-29 03:29:28.259 
Epoch 282/1000 
	 loss: 28.4993, MinusLogProbMetric: 28.4993, val_loss: 29.7701, val_MinusLogProbMetric: 29.7701

Epoch 282: val_loss did not improve from 28.70025
196/196 - 29s - loss: 28.4993 - MinusLogProbMetric: 28.4993 - val_loss: 29.7701 - val_MinusLogProbMetric: 29.7701 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 283/1000
2023-09-29 03:29:59.382 
Epoch 283/1000 
	 loss: 28.5155, MinusLogProbMetric: 28.5155, val_loss: 29.1531, val_MinusLogProbMetric: 29.1531

Epoch 283: val_loss did not improve from 28.70025
196/196 - 31s - loss: 28.5155 - MinusLogProbMetric: 28.5155 - val_loss: 29.1531 - val_MinusLogProbMetric: 29.1531 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 284/1000
2023-09-29 03:30:34.943 
Epoch 284/1000 
	 loss: 28.4894, MinusLogProbMetric: 28.4894, val_loss: 29.2825, val_MinusLogProbMetric: 29.2825

Epoch 284: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.4894 - MinusLogProbMetric: 28.4894 - val_loss: 29.2825 - val_MinusLogProbMetric: 29.2825 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 285/1000
2023-09-29 03:31:10.708 
Epoch 285/1000 
	 loss: 28.5488, MinusLogProbMetric: 28.5488, val_loss: 28.9877, val_MinusLogProbMetric: 28.9877

Epoch 285: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.5488 - MinusLogProbMetric: 28.5488 - val_loss: 28.9877 - val_MinusLogProbMetric: 28.9877 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 286/1000
2023-09-29 03:31:46.513 
Epoch 286/1000 
	 loss: 28.3852, MinusLogProbMetric: 28.3852, val_loss: 30.5756, val_MinusLogProbMetric: 30.5756

Epoch 286: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.3852 - MinusLogProbMetric: 28.3852 - val_loss: 30.5756 - val_MinusLogProbMetric: 30.5756 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 287/1000
2023-09-29 03:32:22.598 
Epoch 287/1000 
	 loss: 28.5244, MinusLogProbMetric: 28.5244, val_loss: 29.2146, val_MinusLogProbMetric: 29.2146

Epoch 287: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.5244 - MinusLogProbMetric: 28.5244 - val_loss: 29.2146 - val_MinusLogProbMetric: 29.2146 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 288/1000
2023-09-29 03:32:58.183 
Epoch 288/1000 
	 loss: 28.3716, MinusLogProbMetric: 28.3716, val_loss: 28.7880, val_MinusLogProbMetric: 28.7880

Epoch 288: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.3716 - MinusLogProbMetric: 28.3716 - val_loss: 28.7880 - val_MinusLogProbMetric: 28.7880 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 289/1000
2023-09-29 03:33:34.015 
Epoch 289/1000 
	 loss: 28.5035, MinusLogProbMetric: 28.5035, val_loss: 28.8848, val_MinusLogProbMetric: 28.8848

Epoch 289: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.5035 - MinusLogProbMetric: 28.5035 - val_loss: 28.8848 - val_MinusLogProbMetric: 28.8848 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 290/1000
2023-09-29 03:34:09.363 
Epoch 290/1000 
	 loss: 28.3782, MinusLogProbMetric: 28.3782, val_loss: 29.4553, val_MinusLogProbMetric: 29.4553

Epoch 290: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.3782 - MinusLogProbMetric: 28.3782 - val_loss: 29.4553 - val_MinusLogProbMetric: 29.4553 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 291/1000
2023-09-29 03:34:45.099 
Epoch 291/1000 
	 loss: 28.5192, MinusLogProbMetric: 28.5192, val_loss: 28.9126, val_MinusLogProbMetric: 28.9126

Epoch 291: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.5192 - MinusLogProbMetric: 28.5192 - val_loss: 28.9126 - val_MinusLogProbMetric: 28.9126 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 292/1000
2023-09-29 03:35:20.419 
Epoch 292/1000 
	 loss: 28.3841, MinusLogProbMetric: 28.3841, val_loss: 29.2444, val_MinusLogProbMetric: 29.2444

Epoch 292: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.3841 - MinusLogProbMetric: 28.3841 - val_loss: 29.2444 - val_MinusLogProbMetric: 29.2444 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 293/1000
2023-09-29 03:35:55.452 
Epoch 293/1000 
	 loss: 28.3644, MinusLogProbMetric: 28.3644, val_loss: 28.8939, val_MinusLogProbMetric: 28.8939

Epoch 293: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.3644 - MinusLogProbMetric: 28.3644 - val_loss: 28.8939 - val_MinusLogProbMetric: 28.8939 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 294/1000
2023-09-29 03:36:28.526 
Epoch 294/1000 
	 loss: 28.3913, MinusLogProbMetric: 28.3913, val_loss: 29.1982, val_MinusLogProbMetric: 29.1982

Epoch 294: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.3913 - MinusLogProbMetric: 28.3913 - val_loss: 29.1982 - val_MinusLogProbMetric: 29.1982 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 295/1000
2023-09-29 03:37:03.205 
Epoch 295/1000 
	 loss: 28.5170, MinusLogProbMetric: 28.5170, val_loss: 29.9817, val_MinusLogProbMetric: 29.9817

Epoch 295: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.5170 - MinusLogProbMetric: 28.5170 - val_loss: 29.9817 - val_MinusLogProbMetric: 29.9817 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 296/1000
2023-09-29 03:37:36.659 
Epoch 296/1000 
	 loss: 28.6644, MinusLogProbMetric: 28.6644, val_loss: 28.9654, val_MinusLogProbMetric: 28.9654

Epoch 296: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.6644 - MinusLogProbMetric: 28.6644 - val_loss: 28.9654 - val_MinusLogProbMetric: 28.9654 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 297/1000
2023-09-29 03:38:12.390 
Epoch 297/1000 
	 loss: 28.5286, MinusLogProbMetric: 28.5286, val_loss: 29.1756, val_MinusLogProbMetric: 29.1756

Epoch 297: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.5286 - MinusLogProbMetric: 28.5286 - val_loss: 29.1756 - val_MinusLogProbMetric: 29.1756 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 298/1000
2023-09-29 03:38:47.527 
Epoch 298/1000 
	 loss: 28.5200, MinusLogProbMetric: 28.5200, val_loss: 28.8944, val_MinusLogProbMetric: 28.8944

Epoch 298: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.5200 - MinusLogProbMetric: 28.5200 - val_loss: 28.8944 - val_MinusLogProbMetric: 28.8944 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 299/1000
2023-09-29 03:39:22.985 
Epoch 299/1000 
	 loss: 28.5199, MinusLogProbMetric: 28.5199, val_loss: 28.8193, val_MinusLogProbMetric: 28.8193

Epoch 299: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.5199 - MinusLogProbMetric: 28.5199 - val_loss: 28.8193 - val_MinusLogProbMetric: 28.8193 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 300/1000
2023-09-29 03:39:58.244 
Epoch 300/1000 
	 loss: 28.4395, MinusLogProbMetric: 28.4395, val_loss: 28.7562, val_MinusLogProbMetric: 28.7562

Epoch 300: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.4395 - MinusLogProbMetric: 28.4395 - val_loss: 28.7562 - val_MinusLogProbMetric: 28.7562 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 301/1000
2023-09-29 03:40:33.653 
Epoch 301/1000 
	 loss: 28.2476, MinusLogProbMetric: 28.2476, val_loss: 28.8926, val_MinusLogProbMetric: 28.8926

Epoch 301: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.2476 - MinusLogProbMetric: 28.2476 - val_loss: 28.8926 - val_MinusLogProbMetric: 28.8926 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 302/1000
2023-09-29 03:41:08.954 
Epoch 302/1000 
	 loss: 28.3487, MinusLogProbMetric: 28.3487, val_loss: 29.3263, val_MinusLogProbMetric: 29.3263

Epoch 302: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.3487 - MinusLogProbMetric: 28.3487 - val_loss: 29.3263 - val_MinusLogProbMetric: 29.3263 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 303/1000
2023-09-29 03:41:44.098 
Epoch 303/1000 
	 loss: 28.4518, MinusLogProbMetric: 28.4518, val_loss: 29.1519, val_MinusLogProbMetric: 29.1519

Epoch 303: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.4518 - MinusLogProbMetric: 28.4518 - val_loss: 29.1519 - val_MinusLogProbMetric: 29.1519 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 304/1000
2023-09-29 03:42:19.381 
Epoch 304/1000 
	 loss: 28.3565, MinusLogProbMetric: 28.3565, val_loss: 28.7481, val_MinusLogProbMetric: 28.7481

Epoch 304: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.3565 - MinusLogProbMetric: 28.3565 - val_loss: 28.7481 - val_MinusLogProbMetric: 28.7481 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 305/1000
2023-09-29 03:42:54.453 
Epoch 305/1000 
	 loss: 28.2177, MinusLogProbMetric: 28.2177, val_loss: 28.7176, val_MinusLogProbMetric: 28.7176

Epoch 305: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.2177 - MinusLogProbMetric: 28.2177 - val_loss: 28.7176 - val_MinusLogProbMetric: 28.7176 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 306/1000
2023-09-29 03:43:29.897 
Epoch 306/1000 
	 loss: 28.4879, MinusLogProbMetric: 28.4879, val_loss: 29.6666, val_MinusLogProbMetric: 29.6666

Epoch 306: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.4879 - MinusLogProbMetric: 28.4879 - val_loss: 29.6666 - val_MinusLogProbMetric: 29.6666 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 307/1000
2023-09-29 03:44:05.494 
Epoch 307/1000 
	 loss: 28.2786, MinusLogProbMetric: 28.2786, val_loss: 29.0324, val_MinusLogProbMetric: 29.0324

Epoch 307: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.2786 - MinusLogProbMetric: 28.2786 - val_loss: 29.0324 - val_MinusLogProbMetric: 29.0324 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 308/1000
2023-09-29 03:44:40.919 
Epoch 308/1000 
	 loss: 28.4850, MinusLogProbMetric: 28.4850, val_loss: 29.0253, val_MinusLogProbMetric: 29.0253

Epoch 308: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.4850 - MinusLogProbMetric: 28.4850 - val_loss: 29.0253 - val_MinusLogProbMetric: 29.0253 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 309/1000
2023-09-29 03:45:16.437 
Epoch 309/1000 
	 loss: 28.2297, MinusLogProbMetric: 28.2297, val_loss: 29.1090, val_MinusLogProbMetric: 29.1090

Epoch 309: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.2297 - MinusLogProbMetric: 28.2297 - val_loss: 29.1090 - val_MinusLogProbMetric: 29.1090 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 310/1000
2023-09-29 03:45:51.903 
Epoch 310/1000 
	 loss: 28.3084, MinusLogProbMetric: 28.3084, val_loss: 30.6310, val_MinusLogProbMetric: 30.6310

Epoch 310: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.3084 - MinusLogProbMetric: 28.3084 - val_loss: 30.6310 - val_MinusLogProbMetric: 30.6310 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 311/1000
2023-09-29 03:46:27.449 
Epoch 311/1000 
	 loss: 28.3927, MinusLogProbMetric: 28.3927, val_loss: 29.7627, val_MinusLogProbMetric: 29.7627

Epoch 311: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.3927 - MinusLogProbMetric: 28.3927 - val_loss: 29.7627 - val_MinusLogProbMetric: 29.7627 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 312/1000
2023-09-29 03:47:03.041 
Epoch 312/1000 
	 loss: 28.4635, MinusLogProbMetric: 28.4635, val_loss: 31.0488, val_MinusLogProbMetric: 31.0488

Epoch 312: val_loss did not improve from 28.70025
196/196 - 36s - loss: 28.4635 - MinusLogProbMetric: 28.4635 - val_loss: 31.0488 - val_MinusLogProbMetric: 31.0488 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 313/1000
2023-09-29 03:47:38.210 
Epoch 313/1000 
	 loss: 29.8046, MinusLogProbMetric: 29.8046, val_loss: 29.5569, val_MinusLogProbMetric: 29.5569

Epoch 313: val_loss did not improve from 28.70025
196/196 - 35s - loss: 29.8046 - MinusLogProbMetric: 29.8046 - val_loss: 29.5569 - val_MinusLogProbMetric: 29.5569 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 314/1000
2023-09-29 03:48:11.737 
Epoch 314/1000 
	 loss: 28.6912, MinusLogProbMetric: 28.6912, val_loss: 29.4104, val_MinusLogProbMetric: 29.4104

Epoch 314: val_loss did not improve from 28.70025
196/196 - 34s - loss: 28.6912 - MinusLogProbMetric: 28.6912 - val_loss: 29.4104 - val_MinusLogProbMetric: 29.4104 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 315/1000
2023-09-29 03:48:44.408 
Epoch 315/1000 
	 loss: 28.6620, MinusLogProbMetric: 28.6620, val_loss: 28.8900, val_MinusLogProbMetric: 28.8900

Epoch 315: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.6620 - MinusLogProbMetric: 28.6620 - val_loss: 28.8900 - val_MinusLogProbMetric: 28.8900 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 316/1000
2023-09-29 03:49:19.399 
Epoch 316/1000 
	 loss: 28.4837, MinusLogProbMetric: 28.4837, val_loss: 28.9365, val_MinusLogProbMetric: 28.9365

Epoch 316: val_loss did not improve from 28.70025
196/196 - 35s - loss: 28.4837 - MinusLogProbMetric: 28.4837 - val_loss: 28.9365 - val_MinusLogProbMetric: 28.9365 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 317/1000
2023-09-29 03:49:51.221 
Epoch 317/1000 
	 loss: 28.4623, MinusLogProbMetric: 28.4623, val_loss: 29.0257, val_MinusLogProbMetric: 29.0257

Epoch 317: val_loss did not improve from 28.70025
196/196 - 32s - loss: 28.4623 - MinusLogProbMetric: 28.4623 - val_loss: 29.0257 - val_MinusLogProbMetric: 29.0257 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 318/1000
2023-09-29 03:50:25.197 
Epoch 318/1000 
	 loss: 28.5195, MinusLogProbMetric: 28.5195, val_loss: 29.2034, val_MinusLogProbMetric: 29.2034

Epoch 318: val_loss did not improve from 28.70025
196/196 - 34s - loss: 28.5195 - MinusLogProbMetric: 28.5195 - val_loss: 29.2034 - val_MinusLogProbMetric: 29.2034 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 319/1000
2023-09-29 03:50:58.620 
Epoch 319/1000 
	 loss: 28.4207, MinusLogProbMetric: 28.4207, val_loss: 28.8754, val_MinusLogProbMetric: 28.8754

Epoch 319: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.4207 - MinusLogProbMetric: 28.4207 - val_loss: 28.8754 - val_MinusLogProbMetric: 28.8754 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 320/1000
2023-09-29 03:51:32.050 
Epoch 320/1000 
	 loss: 28.5055, MinusLogProbMetric: 28.5055, val_loss: 28.9838, val_MinusLogProbMetric: 28.9838

Epoch 320: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.5055 - MinusLogProbMetric: 28.5055 - val_loss: 28.9838 - val_MinusLogProbMetric: 28.9838 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 321/1000
2023-09-29 03:52:05.355 
Epoch 321/1000 
	 loss: 28.4743, MinusLogProbMetric: 28.4743, val_loss: 29.6847, val_MinusLogProbMetric: 29.6847

Epoch 321: val_loss did not improve from 28.70025
196/196 - 33s - loss: 28.4743 - MinusLogProbMetric: 28.4743 - val_loss: 29.6847 - val_MinusLogProbMetric: 29.6847 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 322/1000
2023-09-29 03:52:36.540 
Epoch 322/1000 
	 loss: 28.7784, MinusLogProbMetric: 28.7784, val_loss: 29.2910, val_MinusLogProbMetric: 29.2910

Epoch 322: val_loss did not improve from 28.70025
196/196 - 31s - loss: 28.7784 - MinusLogProbMetric: 28.7784 - val_loss: 29.2910 - val_MinusLogProbMetric: 29.2910 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 323/1000
2023-09-29 03:53:08.012 
Epoch 323/1000 
	 loss: 28.4960, MinusLogProbMetric: 28.4960, val_loss: 28.8343, val_MinusLogProbMetric: 28.8343

Epoch 323: val_loss did not improve from 28.70025
196/196 - 31s - loss: 28.4960 - MinusLogProbMetric: 28.4960 - val_loss: 28.8343 - val_MinusLogProbMetric: 28.8343 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 324/1000
2023-09-29 03:53:41.717 
Epoch 324/1000 
	 loss: 27.7504, MinusLogProbMetric: 27.7504, val_loss: 28.7983, val_MinusLogProbMetric: 28.7983

Epoch 324: val_loss did not improve from 28.70025
196/196 - 34s - loss: 27.7504 - MinusLogProbMetric: 27.7504 - val_loss: 28.7983 - val_MinusLogProbMetric: 28.7983 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 325/1000
2023-09-29 03:54:14.655 
Epoch 325/1000 
	 loss: 27.7365, MinusLogProbMetric: 27.7365, val_loss: 28.6656, val_MinusLogProbMetric: 28.6656

Epoch 325: val_loss improved from 28.70025 to 28.66560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 27.7365 - MinusLogProbMetric: 27.7365 - val_loss: 28.6656 - val_MinusLogProbMetric: 28.6656 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 326/1000
2023-09-29 03:54:48.186 
Epoch 326/1000 
	 loss: 27.7712, MinusLogProbMetric: 27.7712, val_loss: 28.6744, val_MinusLogProbMetric: 28.6744

Epoch 326: val_loss did not improve from 28.66560
196/196 - 33s - loss: 27.7712 - MinusLogProbMetric: 27.7712 - val_loss: 28.6744 - val_MinusLogProbMetric: 28.6744 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 327/1000
2023-09-29 03:55:20.090 
Epoch 327/1000 
	 loss: 27.7761, MinusLogProbMetric: 27.7761, val_loss: 28.7456, val_MinusLogProbMetric: 28.7456

Epoch 327: val_loss did not improve from 28.66560
196/196 - 32s - loss: 27.7761 - MinusLogProbMetric: 27.7761 - val_loss: 28.7456 - val_MinusLogProbMetric: 28.7456 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 328/1000
2023-09-29 03:55:54.722 
Epoch 328/1000 
	 loss: 27.6783, MinusLogProbMetric: 27.6783, val_loss: 28.7238, val_MinusLogProbMetric: 28.7238

Epoch 328: val_loss did not improve from 28.66560
196/196 - 35s - loss: 27.6783 - MinusLogProbMetric: 27.6783 - val_loss: 28.7238 - val_MinusLogProbMetric: 28.7238 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 329/1000
2023-09-29 03:56:26.642 
Epoch 329/1000 
	 loss: 27.7380, MinusLogProbMetric: 27.7380, val_loss: 28.4770, val_MinusLogProbMetric: 28.4770

Epoch 329: val_loss improved from 28.66560 to 28.47700, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 32s - loss: 27.7380 - MinusLogProbMetric: 27.7380 - val_loss: 28.4770 - val_MinusLogProbMetric: 28.4770 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 330/1000
2023-09-29 03:57:01.419 
Epoch 330/1000 
	 loss: 27.7803, MinusLogProbMetric: 27.7803, val_loss: 29.0394, val_MinusLogProbMetric: 29.0394

Epoch 330: val_loss did not improve from 28.47700
196/196 - 34s - loss: 27.7803 - MinusLogProbMetric: 27.7803 - val_loss: 29.0394 - val_MinusLogProbMetric: 29.0394 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 331/1000
2023-09-29 03:57:34.817 
Epoch 331/1000 
	 loss: 27.7326, MinusLogProbMetric: 27.7326, val_loss: 28.6801, val_MinusLogProbMetric: 28.6801

Epoch 331: val_loss did not improve from 28.47700
196/196 - 33s - loss: 27.7326 - MinusLogProbMetric: 27.7326 - val_loss: 28.6801 - val_MinusLogProbMetric: 28.6801 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 332/1000
2023-09-29 03:58:09.316 
Epoch 332/1000 
	 loss: 27.6693, MinusLogProbMetric: 27.6693, val_loss: 28.4505, val_MinusLogProbMetric: 28.4505

Epoch 332: val_loss improved from 28.47700 to 28.45055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 35s - loss: 27.6693 - MinusLogProbMetric: 27.6693 - val_loss: 28.4505 - val_MinusLogProbMetric: 28.4505 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 333/1000
2023-09-29 03:58:44.066 
Epoch 333/1000 
	 loss: 27.8676, MinusLogProbMetric: 27.8676, val_loss: 28.6576, val_MinusLogProbMetric: 28.6576

Epoch 333: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.8676 - MinusLogProbMetric: 27.8676 - val_loss: 28.6576 - val_MinusLogProbMetric: 28.6576 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 334/1000
2023-09-29 03:59:17.765 
Epoch 334/1000 
	 loss: 27.7833, MinusLogProbMetric: 27.7833, val_loss: 28.4889, val_MinusLogProbMetric: 28.4889

Epoch 334: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.7833 - MinusLogProbMetric: 27.7833 - val_loss: 28.4889 - val_MinusLogProbMetric: 28.4889 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 335/1000
2023-09-29 03:59:49.903 
Epoch 335/1000 
	 loss: 27.7185, MinusLogProbMetric: 27.7185, val_loss: 28.5396, val_MinusLogProbMetric: 28.5396

Epoch 335: val_loss did not improve from 28.45055
196/196 - 32s - loss: 27.7185 - MinusLogProbMetric: 27.7185 - val_loss: 28.5396 - val_MinusLogProbMetric: 28.5396 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 336/1000
2023-09-29 04:00:23.973 
Epoch 336/1000 
	 loss: 27.7303, MinusLogProbMetric: 27.7303, val_loss: 28.6892, val_MinusLogProbMetric: 28.6892

Epoch 336: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.7303 - MinusLogProbMetric: 27.7303 - val_loss: 28.6892 - val_MinusLogProbMetric: 28.6892 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 337/1000
2023-09-29 04:00:58.083 
Epoch 337/1000 
	 loss: 27.6907, MinusLogProbMetric: 27.6907, val_loss: 28.8552, val_MinusLogProbMetric: 28.8552

Epoch 337: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.6907 - MinusLogProbMetric: 27.6907 - val_loss: 28.8552 - val_MinusLogProbMetric: 28.8552 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 338/1000
2023-09-29 04:01:31.355 
Epoch 338/1000 
	 loss: 27.7098, MinusLogProbMetric: 27.7098, val_loss: 28.5438, val_MinusLogProbMetric: 28.5438

Epoch 338: val_loss did not improve from 28.45055
196/196 - 33s - loss: 27.7098 - MinusLogProbMetric: 27.7098 - val_loss: 28.5438 - val_MinusLogProbMetric: 28.5438 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 339/1000
2023-09-29 04:02:05.997 
Epoch 339/1000 
	 loss: 27.7844, MinusLogProbMetric: 27.7844, val_loss: 29.0620, val_MinusLogProbMetric: 29.0620

Epoch 339: val_loss did not improve from 28.45055
196/196 - 35s - loss: 27.7844 - MinusLogProbMetric: 27.7844 - val_loss: 29.0620 - val_MinusLogProbMetric: 29.0620 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 340/1000
2023-09-29 04:02:40.912 
Epoch 340/1000 
	 loss: 27.7552, MinusLogProbMetric: 27.7552, val_loss: 28.5501, val_MinusLogProbMetric: 28.5501

Epoch 340: val_loss did not improve from 28.45055
196/196 - 35s - loss: 27.7552 - MinusLogProbMetric: 27.7552 - val_loss: 28.5501 - val_MinusLogProbMetric: 28.5501 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 341/1000
2023-09-29 04:03:15.603 
Epoch 341/1000 
	 loss: 27.6660, MinusLogProbMetric: 27.6660, val_loss: 28.8970, val_MinusLogProbMetric: 28.8970

Epoch 341: val_loss did not improve from 28.45055
196/196 - 35s - loss: 27.6660 - MinusLogProbMetric: 27.6660 - val_loss: 28.8970 - val_MinusLogProbMetric: 28.8970 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 342/1000
2023-09-29 04:03:49.573 
Epoch 342/1000 
	 loss: 27.7042, MinusLogProbMetric: 27.7042, val_loss: 28.7171, val_MinusLogProbMetric: 28.7171

Epoch 342: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.7042 - MinusLogProbMetric: 27.7042 - val_loss: 28.7171 - val_MinusLogProbMetric: 28.7171 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 343/1000
2023-09-29 04:04:23.474 
Epoch 343/1000 
	 loss: 27.6376, MinusLogProbMetric: 27.6376, val_loss: 28.5175, val_MinusLogProbMetric: 28.5175

Epoch 343: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.6376 - MinusLogProbMetric: 27.6376 - val_loss: 28.5175 - val_MinusLogProbMetric: 28.5175 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 344/1000
2023-09-29 04:04:57.679 
Epoch 344/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.8316, val_MinusLogProbMetric: 28.8316

Epoch 344: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.8316 - val_MinusLogProbMetric: 28.8316 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 345/1000
2023-09-29 04:05:32.282 
Epoch 345/1000 
	 loss: 27.6798, MinusLogProbMetric: 27.6798, val_loss: 29.4264, val_MinusLogProbMetric: 29.4264

Epoch 345: val_loss did not improve from 28.45055
196/196 - 35s - loss: 27.6798 - MinusLogProbMetric: 27.6798 - val_loss: 29.4264 - val_MinusLogProbMetric: 29.4264 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 346/1000
2023-09-29 04:06:07.226 
Epoch 346/1000 
	 loss: 27.7509, MinusLogProbMetric: 27.7509, val_loss: 28.4945, val_MinusLogProbMetric: 28.4945

Epoch 346: val_loss did not improve from 28.45055
196/196 - 35s - loss: 27.7509 - MinusLogProbMetric: 27.7509 - val_loss: 28.4945 - val_MinusLogProbMetric: 28.4945 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 347/1000
2023-09-29 04:06:40.652 
Epoch 347/1000 
	 loss: 27.6939, MinusLogProbMetric: 27.6939, val_loss: 29.0579, val_MinusLogProbMetric: 29.0579

Epoch 347: val_loss did not improve from 28.45055
196/196 - 33s - loss: 27.6939 - MinusLogProbMetric: 27.6939 - val_loss: 29.0579 - val_MinusLogProbMetric: 29.0579 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 348/1000
2023-09-29 04:07:15.570 
Epoch 348/1000 
	 loss: 27.7282, MinusLogProbMetric: 27.7282, val_loss: 28.7963, val_MinusLogProbMetric: 28.7963

Epoch 348: val_loss did not improve from 28.45055
196/196 - 35s - loss: 27.7282 - MinusLogProbMetric: 27.7282 - val_loss: 28.7963 - val_MinusLogProbMetric: 28.7963 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 349/1000
2023-09-29 04:07:50.243 
Epoch 349/1000 
	 loss: 27.6295, MinusLogProbMetric: 27.6295, val_loss: 29.1158, val_MinusLogProbMetric: 29.1158

Epoch 349: val_loss did not improve from 28.45055
196/196 - 35s - loss: 27.6295 - MinusLogProbMetric: 27.6295 - val_loss: 29.1158 - val_MinusLogProbMetric: 29.1158 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 350/1000
2023-09-29 04:08:23.097 
Epoch 350/1000 
	 loss: 27.7170, MinusLogProbMetric: 27.7170, val_loss: 28.7101, val_MinusLogProbMetric: 28.7101

Epoch 350: val_loss did not improve from 28.45055
196/196 - 33s - loss: 27.7170 - MinusLogProbMetric: 27.7170 - val_loss: 28.7101 - val_MinusLogProbMetric: 28.7101 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 351/1000
2023-09-29 04:08:57.542 
Epoch 351/1000 
	 loss: 27.6797, MinusLogProbMetric: 27.6797, val_loss: 29.2893, val_MinusLogProbMetric: 29.2893

Epoch 351: val_loss did not improve from 28.45055
196/196 - 34s - loss: 27.6797 - MinusLogProbMetric: 27.6797 - val_loss: 29.2893 - val_MinusLogProbMetric: 29.2893 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 352/1000
2023-09-29 04:09:28.952 
Epoch 352/1000 
	 loss: 27.6591, MinusLogProbMetric: 27.6591, val_loss: 28.6690, val_MinusLogProbMetric: 28.6690

Epoch 352: val_loss did not improve from 28.45055
196/196 - 31s - loss: 27.6591 - MinusLogProbMetric: 27.6591 - val_loss: 28.6690 - val_MinusLogProbMetric: 28.6690 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 353/1000
2023-09-29 04:10:03.110 
Epoch 353/1000 
	 loss: 27.6407, MinusLogProbMetric: 27.6407, val_loss: 28.4426, val_MinusLogProbMetric: 28.4426

Epoch 353: val_loss improved from 28.45055 to 28.44260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 35s - loss: 27.6407 - MinusLogProbMetric: 27.6407 - val_loss: 28.4426 - val_MinusLogProbMetric: 28.4426 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 354/1000
2023-09-29 04:10:37.918 
Epoch 354/1000 
	 loss: 27.7504, MinusLogProbMetric: 27.7504, val_loss: 28.5462, val_MinusLogProbMetric: 28.5462

Epoch 354: val_loss did not improve from 28.44260
196/196 - 34s - loss: 27.7504 - MinusLogProbMetric: 27.7504 - val_loss: 28.5462 - val_MinusLogProbMetric: 28.5462 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 355/1000
2023-09-29 04:11:12.533 
Epoch 355/1000 
	 loss: 27.7158, MinusLogProbMetric: 27.7158, val_loss: 28.5811, val_MinusLogProbMetric: 28.5811

Epoch 355: val_loss did not improve from 28.44260
196/196 - 35s - loss: 27.7158 - MinusLogProbMetric: 27.7158 - val_loss: 28.5811 - val_MinusLogProbMetric: 28.5811 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 356/1000
2023-09-29 04:11:44.477 
Epoch 356/1000 
	 loss: 27.6103, MinusLogProbMetric: 27.6103, val_loss: 28.5995, val_MinusLogProbMetric: 28.5995

Epoch 356: val_loss did not improve from 28.44260
196/196 - 32s - loss: 27.6103 - MinusLogProbMetric: 27.6103 - val_loss: 28.5995 - val_MinusLogProbMetric: 28.5995 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 357/1000
2023-09-29 04:12:17.969 
Epoch 357/1000 
	 loss: 27.6083, MinusLogProbMetric: 27.6083, val_loss: 28.9081, val_MinusLogProbMetric: 28.9081

Epoch 357: val_loss did not improve from 28.44260
196/196 - 33s - loss: 27.6083 - MinusLogProbMetric: 27.6083 - val_loss: 28.9081 - val_MinusLogProbMetric: 28.9081 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 358/1000
2023-09-29 04:12:50.620 
Epoch 358/1000 
	 loss: 27.6915, MinusLogProbMetric: 27.6915, val_loss: 28.6238, val_MinusLogProbMetric: 28.6238

Epoch 358: val_loss did not improve from 28.44260
196/196 - 33s - loss: 27.6915 - MinusLogProbMetric: 27.6915 - val_loss: 28.6238 - val_MinusLogProbMetric: 28.6238 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 359/1000
2023-09-29 04:13:23.943 
Epoch 359/1000 
	 loss: 27.6685, MinusLogProbMetric: 27.6685, val_loss: 29.8169, val_MinusLogProbMetric: 29.8169

Epoch 359: val_loss did not improve from 28.44260
196/196 - 33s - loss: 27.6685 - MinusLogProbMetric: 27.6685 - val_loss: 29.8169 - val_MinusLogProbMetric: 29.8169 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 360/1000
2023-09-29 04:13:56.515 
Epoch 360/1000 
	 loss: 27.6514, MinusLogProbMetric: 27.6514, val_loss: 28.5831, val_MinusLogProbMetric: 28.5831

Epoch 360: val_loss did not improve from 28.44260
196/196 - 33s - loss: 27.6514 - MinusLogProbMetric: 27.6514 - val_loss: 28.5831 - val_MinusLogProbMetric: 28.5831 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 361/1000
2023-09-29 04:14:31.022 
Epoch 361/1000 
	 loss: 27.6216, MinusLogProbMetric: 27.6216, val_loss: 28.4902, val_MinusLogProbMetric: 28.4902

Epoch 361: val_loss did not improve from 28.44260
196/196 - 35s - loss: 27.6216 - MinusLogProbMetric: 27.6216 - val_loss: 28.4902 - val_MinusLogProbMetric: 28.4902 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 362/1000
2023-09-29 04:15:02.916 
Epoch 362/1000 
	 loss: 27.6649, MinusLogProbMetric: 27.6649, val_loss: 28.7083, val_MinusLogProbMetric: 28.7083

Epoch 362: val_loss did not improve from 28.44260
196/196 - 32s - loss: 27.6649 - MinusLogProbMetric: 27.6649 - val_loss: 28.7083 - val_MinusLogProbMetric: 28.7083 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 363/1000
2023-09-29 04:15:37.337 
Epoch 363/1000 
	 loss: 27.6690, MinusLogProbMetric: 27.6690, val_loss: 28.4939, val_MinusLogProbMetric: 28.4939

Epoch 363: val_loss did not improve from 28.44260
196/196 - 34s - loss: 27.6690 - MinusLogProbMetric: 27.6690 - val_loss: 28.4939 - val_MinusLogProbMetric: 28.4939 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 364/1000
2023-09-29 04:16:09.027 
Epoch 364/1000 
	 loss: 27.5583, MinusLogProbMetric: 27.5583, val_loss: 28.5590, val_MinusLogProbMetric: 28.5590

Epoch 364: val_loss did not improve from 28.44260
196/196 - 32s - loss: 27.5583 - MinusLogProbMetric: 27.5583 - val_loss: 28.5590 - val_MinusLogProbMetric: 28.5590 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 365/1000
2023-09-29 04:16:42.254 
Epoch 365/1000 
	 loss: 27.6846, MinusLogProbMetric: 27.6846, val_loss: 29.4809, val_MinusLogProbMetric: 29.4809

Epoch 365: val_loss did not improve from 28.44260
196/196 - 33s - loss: 27.6846 - MinusLogProbMetric: 27.6846 - val_loss: 29.4809 - val_MinusLogProbMetric: 29.4809 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 366/1000
2023-09-29 04:17:15.983 
Epoch 366/1000 
	 loss: 27.6133, MinusLogProbMetric: 27.6133, val_loss: 29.1573, val_MinusLogProbMetric: 29.1573

Epoch 366: val_loss did not improve from 28.44260
196/196 - 34s - loss: 27.6133 - MinusLogProbMetric: 27.6133 - val_loss: 29.1573 - val_MinusLogProbMetric: 29.1573 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 367/1000
2023-09-29 04:17:50.580 
Epoch 367/1000 
	 loss: 27.6298, MinusLogProbMetric: 27.6298, val_loss: 28.5595, val_MinusLogProbMetric: 28.5595

Epoch 367: val_loss did not improve from 28.44260
196/196 - 35s - loss: 27.6298 - MinusLogProbMetric: 27.6298 - val_loss: 28.5595 - val_MinusLogProbMetric: 28.5595 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 368/1000
2023-09-29 04:18:22.541 
Epoch 368/1000 
	 loss: 27.5339, MinusLogProbMetric: 27.5339, val_loss: 28.6994, val_MinusLogProbMetric: 28.6994

Epoch 368: val_loss did not improve from 28.44260
196/196 - 32s - loss: 27.5339 - MinusLogProbMetric: 27.5339 - val_loss: 28.6994 - val_MinusLogProbMetric: 28.6994 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 369/1000
2023-09-29 04:18:56.866 
Epoch 369/1000 
	 loss: 27.6125, MinusLogProbMetric: 27.6125, val_loss: 29.1022, val_MinusLogProbMetric: 29.1022

Epoch 369: val_loss did not improve from 28.44260
196/196 - 34s - loss: 27.6125 - MinusLogProbMetric: 27.6125 - val_loss: 29.1022 - val_MinusLogProbMetric: 29.1022 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 370/1000
2023-09-29 04:19:30.177 
Epoch 370/1000 
	 loss: 27.6810, MinusLogProbMetric: 27.6810, val_loss: 28.9963, val_MinusLogProbMetric: 28.9963

Epoch 370: val_loss did not improve from 28.44260
196/196 - 33s - loss: 27.6810 - MinusLogProbMetric: 27.6810 - val_loss: 28.9963 - val_MinusLogProbMetric: 28.9963 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 371/1000
2023-09-29 04:20:02.194 
Epoch 371/1000 
	 loss: 27.7062, MinusLogProbMetric: 27.7062, val_loss: 29.0423, val_MinusLogProbMetric: 29.0423

Epoch 371: val_loss did not improve from 28.44260
196/196 - 32s - loss: 27.7062 - MinusLogProbMetric: 27.7062 - val_loss: 29.0423 - val_MinusLogProbMetric: 29.0423 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 372/1000
2023-09-29 04:20:36.962 
Epoch 372/1000 
	 loss: 27.6439, MinusLogProbMetric: 27.6439, val_loss: 28.8754, val_MinusLogProbMetric: 28.8754

Epoch 372: val_loss did not improve from 28.44260
196/196 - 35s - loss: 27.6439 - MinusLogProbMetric: 27.6439 - val_loss: 28.8754 - val_MinusLogProbMetric: 28.8754 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 373/1000
2023-09-29 04:21:10.133 
Epoch 373/1000 
	 loss: 27.6262, MinusLogProbMetric: 27.6262, val_loss: 28.4505, val_MinusLogProbMetric: 28.4505

Epoch 373: val_loss did not improve from 28.44260
196/196 - 33s - loss: 27.6262 - MinusLogProbMetric: 27.6262 - val_loss: 28.4505 - val_MinusLogProbMetric: 28.4505 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 374/1000
2023-09-29 04:21:43.673 
Epoch 374/1000 
	 loss: 27.6115, MinusLogProbMetric: 27.6115, val_loss: 28.6400, val_MinusLogProbMetric: 28.6400

Epoch 374: val_loss did not improve from 28.44260
196/196 - 34s - loss: 27.6115 - MinusLogProbMetric: 27.6115 - val_loss: 28.6400 - val_MinusLogProbMetric: 28.6400 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 375/1000
2023-09-29 04:22:18.575 
Epoch 375/1000 
	 loss: 27.5815, MinusLogProbMetric: 27.5815, val_loss: 28.9738, val_MinusLogProbMetric: 28.9738

Epoch 375: val_loss did not improve from 28.44260
196/196 - 35s - loss: 27.5815 - MinusLogProbMetric: 27.5815 - val_loss: 28.9738 - val_MinusLogProbMetric: 28.9738 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 376/1000
2023-09-29 04:22:53.237 
Epoch 376/1000 
	 loss: 27.6362, MinusLogProbMetric: 27.6362, val_loss: 28.4221, val_MinusLogProbMetric: 28.4221

Epoch 376: val_loss improved from 28.44260 to 28.42212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 35s - loss: 27.6362 - MinusLogProbMetric: 27.6362 - val_loss: 28.4221 - val_MinusLogProbMetric: 28.4221 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 377/1000
2023-09-29 04:23:27.619 
Epoch 377/1000 
	 loss: 27.6315, MinusLogProbMetric: 27.6315, val_loss: 28.6820, val_MinusLogProbMetric: 28.6820

Epoch 377: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.6315 - MinusLogProbMetric: 27.6315 - val_loss: 28.6820 - val_MinusLogProbMetric: 28.6820 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 378/1000
2023-09-29 04:24:01.381 
Epoch 378/1000 
	 loss: 27.5476, MinusLogProbMetric: 27.5476, val_loss: 28.4940, val_MinusLogProbMetric: 28.4940

Epoch 378: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5476 - MinusLogProbMetric: 27.5476 - val_loss: 28.4940 - val_MinusLogProbMetric: 28.4940 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 379/1000
2023-09-29 04:24:34.746 
Epoch 379/1000 
	 loss: 27.5782, MinusLogProbMetric: 27.5782, val_loss: 29.4120, val_MinusLogProbMetric: 29.4120

Epoch 379: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.5782 - MinusLogProbMetric: 27.5782 - val_loss: 29.4120 - val_MinusLogProbMetric: 29.4120 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 380/1000
2023-09-29 04:25:05.745 
Epoch 380/1000 
	 loss: 27.6620, MinusLogProbMetric: 27.6620, val_loss: 29.5038, val_MinusLogProbMetric: 29.5038

Epoch 380: val_loss did not improve from 28.42212
196/196 - 31s - loss: 27.6620 - MinusLogProbMetric: 27.6620 - val_loss: 29.5038 - val_MinusLogProbMetric: 29.5038 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 381/1000
2023-09-29 04:25:38.496 
Epoch 381/1000 
	 loss: 27.6296, MinusLogProbMetric: 27.6296, val_loss: 28.4542, val_MinusLogProbMetric: 28.4542

Epoch 381: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.6296 - MinusLogProbMetric: 27.6296 - val_loss: 28.4542 - val_MinusLogProbMetric: 28.4542 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 382/1000
2023-09-29 04:26:12.691 
Epoch 382/1000 
	 loss: 27.5707, MinusLogProbMetric: 27.5707, val_loss: 28.4688, val_MinusLogProbMetric: 28.4688

Epoch 382: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5707 - MinusLogProbMetric: 27.5707 - val_loss: 28.4688 - val_MinusLogProbMetric: 28.4688 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 383/1000
2023-09-29 04:26:46.491 
Epoch 383/1000 
	 loss: 27.5846, MinusLogProbMetric: 27.5846, val_loss: 28.4251, val_MinusLogProbMetric: 28.4251

Epoch 383: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5846 - MinusLogProbMetric: 27.5846 - val_loss: 28.4251 - val_MinusLogProbMetric: 28.4251 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 384/1000
2023-09-29 04:27:20.341 
Epoch 384/1000 
	 loss: 27.5947, MinusLogProbMetric: 27.5947, val_loss: 28.4784, val_MinusLogProbMetric: 28.4784

Epoch 384: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5947 - MinusLogProbMetric: 27.5947 - val_loss: 28.4784 - val_MinusLogProbMetric: 28.4784 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 385/1000
2023-09-29 04:27:55.300 
Epoch 385/1000 
	 loss: 27.5336, MinusLogProbMetric: 27.5336, val_loss: 29.8867, val_MinusLogProbMetric: 29.8867

Epoch 385: val_loss did not improve from 28.42212
196/196 - 35s - loss: 27.5336 - MinusLogProbMetric: 27.5336 - val_loss: 29.8867 - val_MinusLogProbMetric: 29.8867 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 386/1000
2023-09-29 04:28:27.425 
Epoch 386/1000 
	 loss: 27.5429, MinusLogProbMetric: 27.5429, val_loss: 28.7054, val_MinusLogProbMetric: 28.7054

Epoch 386: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5429 - MinusLogProbMetric: 27.5429 - val_loss: 28.7054 - val_MinusLogProbMetric: 28.7054 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 387/1000
2023-09-29 04:29:01.671 
Epoch 387/1000 
	 loss: 27.5971, MinusLogProbMetric: 27.5971, val_loss: 28.7250, val_MinusLogProbMetric: 28.7250

Epoch 387: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5971 - MinusLogProbMetric: 27.5971 - val_loss: 28.7250 - val_MinusLogProbMetric: 28.7250 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 388/1000
2023-09-29 04:29:36.130 
Epoch 388/1000 
	 loss: 27.5407, MinusLogProbMetric: 27.5407, val_loss: 28.4852, val_MinusLogProbMetric: 28.4852

Epoch 388: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5407 - MinusLogProbMetric: 27.5407 - val_loss: 28.4852 - val_MinusLogProbMetric: 28.4852 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 389/1000
2023-09-29 04:30:08.734 
Epoch 389/1000 
	 loss: 27.7129, MinusLogProbMetric: 27.7129, val_loss: 28.4454, val_MinusLogProbMetric: 28.4454

Epoch 389: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.7129 - MinusLogProbMetric: 27.7129 - val_loss: 28.4454 - val_MinusLogProbMetric: 28.4454 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 390/1000
2023-09-29 04:30:38.873 
Epoch 390/1000 
	 loss: 27.5543, MinusLogProbMetric: 27.5543, val_loss: 28.4443, val_MinusLogProbMetric: 28.4443

Epoch 390: val_loss did not improve from 28.42212
196/196 - 30s - loss: 27.5543 - MinusLogProbMetric: 27.5543 - val_loss: 28.4443 - val_MinusLogProbMetric: 28.4443 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 391/1000
2023-09-29 04:31:09.552 
Epoch 391/1000 
	 loss: 27.5447, MinusLogProbMetric: 27.5447, val_loss: 28.4727, val_MinusLogProbMetric: 28.4727

Epoch 391: val_loss did not improve from 28.42212
196/196 - 31s - loss: 27.5447 - MinusLogProbMetric: 27.5447 - val_loss: 28.4727 - val_MinusLogProbMetric: 28.4727 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 392/1000
2023-09-29 04:31:39.366 
Epoch 392/1000 
	 loss: 27.5807, MinusLogProbMetric: 27.5807, val_loss: 28.7403, val_MinusLogProbMetric: 28.7403

Epoch 392: val_loss did not improve from 28.42212
196/196 - 30s - loss: 27.5807 - MinusLogProbMetric: 27.5807 - val_loss: 28.7403 - val_MinusLogProbMetric: 28.7403 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 393/1000
2023-09-29 04:32:10.031 
Epoch 393/1000 
	 loss: 27.5590, MinusLogProbMetric: 27.5590, val_loss: 28.5961, val_MinusLogProbMetric: 28.5961

Epoch 393: val_loss did not improve from 28.42212
196/196 - 31s - loss: 27.5590 - MinusLogProbMetric: 27.5590 - val_loss: 28.5961 - val_MinusLogProbMetric: 28.5961 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 394/1000
2023-09-29 04:32:40.232 
Epoch 394/1000 
	 loss: 27.5943, MinusLogProbMetric: 27.5943, val_loss: 28.5551, val_MinusLogProbMetric: 28.5551

Epoch 394: val_loss did not improve from 28.42212
196/196 - 30s - loss: 27.5943 - MinusLogProbMetric: 27.5943 - val_loss: 28.5551 - val_MinusLogProbMetric: 28.5551 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 395/1000
2023-09-29 04:33:09.874 
Epoch 395/1000 
	 loss: 27.5626, MinusLogProbMetric: 27.5626, val_loss: 28.5442, val_MinusLogProbMetric: 28.5442

Epoch 395: val_loss did not improve from 28.42212
196/196 - 30s - loss: 27.5626 - MinusLogProbMetric: 27.5626 - val_loss: 28.5442 - val_MinusLogProbMetric: 28.5442 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 396/1000
2023-09-29 04:33:38.910 
Epoch 396/1000 
	 loss: 27.5234, MinusLogProbMetric: 27.5234, val_loss: 28.9562, val_MinusLogProbMetric: 28.9562

Epoch 396: val_loss did not improve from 28.42212
196/196 - 29s - loss: 27.5234 - MinusLogProbMetric: 27.5234 - val_loss: 28.9562 - val_MinusLogProbMetric: 28.9562 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 397/1000
2023-09-29 04:34:07.344 
Epoch 397/1000 
	 loss: 27.5594, MinusLogProbMetric: 27.5594, val_loss: 28.5533, val_MinusLogProbMetric: 28.5533

Epoch 397: val_loss did not improve from 28.42212
196/196 - 28s - loss: 27.5594 - MinusLogProbMetric: 27.5594 - val_loss: 28.5533 - val_MinusLogProbMetric: 28.5533 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 398/1000
2023-09-29 04:34:37.264 
Epoch 398/1000 
	 loss: 27.4768, MinusLogProbMetric: 27.4768, val_loss: 28.5368, val_MinusLogProbMetric: 28.5368

Epoch 398: val_loss did not improve from 28.42212
196/196 - 30s - loss: 27.4768 - MinusLogProbMetric: 27.4768 - val_loss: 28.5368 - val_MinusLogProbMetric: 28.5368 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 399/1000
2023-09-29 04:35:04.995 
Epoch 399/1000 
	 loss: 27.6425, MinusLogProbMetric: 27.6425, val_loss: 28.7442, val_MinusLogProbMetric: 28.7442

Epoch 399: val_loss did not improve from 28.42212
196/196 - 28s - loss: 27.6425 - MinusLogProbMetric: 27.6425 - val_loss: 28.7442 - val_MinusLogProbMetric: 28.7442 - lr: 5.0000e-04 - 28s/epoch - 141ms/step
Epoch 400/1000
2023-09-29 04:35:33.412 
Epoch 400/1000 
	 loss: 27.5381, MinusLogProbMetric: 27.5381, val_loss: 28.7335, val_MinusLogProbMetric: 28.7335

Epoch 400: val_loss did not improve from 28.42212
196/196 - 28s - loss: 27.5381 - MinusLogProbMetric: 27.5381 - val_loss: 28.7335 - val_MinusLogProbMetric: 28.7335 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 401/1000
2023-09-29 04:36:02.895 
Epoch 401/1000 
	 loss: 27.4957, MinusLogProbMetric: 27.4957, val_loss: 29.3699, val_MinusLogProbMetric: 29.3699

Epoch 401: val_loss did not improve from 28.42212
196/196 - 29s - loss: 27.4957 - MinusLogProbMetric: 27.4957 - val_loss: 29.3699 - val_MinusLogProbMetric: 29.3699 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 402/1000
2023-09-29 04:36:31.302 
Epoch 402/1000 
	 loss: 27.5306, MinusLogProbMetric: 27.5306, val_loss: 28.4399, val_MinusLogProbMetric: 28.4399

Epoch 402: val_loss did not improve from 28.42212
196/196 - 28s - loss: 27.5306 - MinusLogProbMetric: 27.5306 - val_loss: 28.4399 - val_MinusLogProbMetric: 28.4399 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 403/1000
2023-09-29 04:37:02.153 
Epoch 403/1000 
	 loss: 27.5713, MinusLogProbMetric: 27.5713, val_loss: 29.2083, val_MinusLogProbMetric: 29.2083

Epoch 403: val_loss did not improve from 28.42212
196/196 - 31s - loss: 27.5713 - MinusLogProbMetric: 27.5713 - val_loss: 29.2083 - val_MinusLogProbMetric: 29.2083 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 404/1000
2023-09-29 04:37:35.900 
Epoch 404/1000 
	 loss: 27.5633, MinusLogProbMetric: 27.5633, val_loss: 28.6586, val_MinusLogProbMetric: 28.6586

Epoch 404: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5633 - MinusLogProbMetric: 27.5633 - val_loss: 28.6586 - val_MinusLogProbMetric: 28.6586 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 405/1000
2023-09-29 04:38:08.335 
Epoch 405/1000 
	 loss: 27.5460, MinusLogProbMetric: 27.5460, val_loss: 29.1735, val_MinusLogProbMetric: 29.1735

Epoch 405: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5460 - MinusLogProbMetric: 27.5460 - val_loss: 29.1735 - val_MinusLogProbMetric: 29.1735 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 406/1000
2023-09-29 04:38:41.089 
Epoch 406/1000 
	 loss: 27.5966, MinusLogProbMetric: 27.5966, val_loss: 28.4647, val_MinusLogProbMetric: 28.4647

Epoch 406: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.5966 - MinusLogProbMetric: 27.5966 - val_loss: 28.4647 - val_MinusLogProbMetric: 28.4647 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 407/1000
2023-09-29 04:39:14.506 
Epoch 407/1000 
	 loss: 27.5733, MinusLogProbMetric: 27.5733, val_loss: 28.9996, val_MinusLogProbMetric: 28.9996

Epoch 407: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.5733 - MinusLogProbMetric: 27.5733 - val_loss: 28.9996 - val_MinusLogProbMetric: 28.9996 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 408/1000
2023-09-29 04:39:47.260 
Epoch 408/1000 
	 loss: 27.5714, MinusLogProbMetric: 27.5714, val_loss: 28.6887, val_MinusLogProbMetric: 28.6887

Epoch 408: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.5714 - MinusLogProbMetric: 27.5714 - val_loss: 28.6887 - val_MinusLogProbMetric: 28.6887 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 409/1000
2023-09-29 04:40:19.181 
Epoch 409/1000 
	 loss: 27.5188, MinusLogProbMetric: 27.5188, val_loss: 28.8013, val_MinusLogProbMetric: 28.8013

Epoch 409: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5188 - MinusLogProbMetric: 27.5188 - val_loss: 28.8013 - val_MinusLogProbMetric: 28.8013 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 410/1000
2023-09-29 04:40:50.770 
Epoch 410/1000 
	 loss: 27.4435, MinusLogProbMetric: 27.4435, val_loss: 29.4412, val_MinusLogProbMetric: 29.4412

Epoch 410: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.4435 - MinusLogProbMetric: 27.4435 - val_loss: 29.4412 - val_MinusLogProbMetric: 29.4412 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 411/1000
2023-09-29 04:41:24.426 
Epoch 411/1000 
	 loss: 27.5713, MinusLogProbMetric: 27.5713, val_loss: 28.4925, val_MinusLogProbMetric: 28.4925

Epoch 411: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5713 - MinusLogProbMetric: 27.5713 - val_loss: 28.4925 - val_MinusLogProbMetric: 28.4925 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 412/1000
2023-09-29 04:41:56.975 
Epoch 412/1000 
	 loss: 27.5099, MinusLogProbMetric: 27.5099, val_loss: 28.6086, val_MinusLogProbMetric: 28.6086

Epoch 412: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.5099 - MinusLogProbMetric: 27.5099 - val_loss: 28.6086 - val_MinusLogProbMetric: 28.6086 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 413/1000
2023-09-29 04:42:30.760 
Epoch 413/1000 
	 loss: 27.5820, MinusLogProbMetric: 27.5820, val_loss: 28.4544, val_MinusLogProbMetric: 28.4544

Epoch 413: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.5820 - MinusLogProbMetric: 27.5820 - val_loss: 28.4544 - val_MinusLogProbMetric: 28.4544 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 414/1000
2023-09-29 04:43:02.942 
Epoch 414/1000 
	 loss: 27.5764, MinusLogProbMetric: 27.5764, val_loss: 28.5975, val_MinusLogProbMetric: 28.5975

Epoch 414: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5764 - MinusLogProbMetric: 27.5764 - val_loss: 28.5975 - val_MinusLogProbMetric: 28.5975 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 415/1000
2023-09-29 04:43:33.142 
Epoch 415/1000 
	 loss: 27.5462, MinusLogProbMetric: 27.5462, val_loss: 29.0373, val_MinusLogProbMetric: 29.0373

Epoch 415: val_loss did not improve from 28.42212
196/196 - 30s - loss: 27.5462 - MinusLogProbMetric: 27.5462 - val_loss: 29.0373 - val_MinusLogProbMetric: 29.0373 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 416/1000
2023-09-29 04:44:05.529 
Epoch 416/1000 
	 loss: 27.4580, MinusLogProbMetric: 27.4580, val_loss: 28.6429, val_MinusLogProbMetric: 28.6429

Epoch 416: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.4580 - MinusLogProbMetric: 27.4580 - val_loss: 28.6429 - val_MinusLogProbMetric: 28.6429 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 417/1000
2023-09-29 04:44:37.171 
Epoch 417/1000 
	 loss: 27.5822, MinusLogProbMetric: 27.5822, val_loss: 28.8178, val_MinusLogProbMetric: 28.8178

Epoch 417: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5822 - MinusLogProbMetric: 27.5822 - val_loss: 28.8178 - val_MinusLogProbMetric: 28.8178 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 418/1000
2023-09-29 04:45:08.817 
Epoch 418/1000 
	 loss: 27.5334, MinusLogProbMetric: 27.5334, val_loss: 28.5548, val_MinusLogProbMetric: 28.5548

Epoch 418: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5334 - MinusLogProbMetric: 27.5334 - val_loss: 28.5548 - val_MinusLogProbMetric: 28.5548 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 419/1000
2023-09-29 04:45:40.405 
Epoch 419/1000 
	 loss: 27.5054, MinusLogProbMetric: 27.5054, val_loss: 28.4767, val_MinusLogProbMetric: 28.4767

Epoch 419: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5054 - MinusLogProbMetric: 27.5054 - val_loss: 28.4767 - val_MinusLogProbMetric: 28.4767 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 420/1000
2023-09-29 04:46:12.661 
Epoch 420/1000 
	 loss: 27.5383, MinusLogProbMetric: 27.5383, val_loss: 28.6085, val_MinusLogProbMetric: 28.6085

Epoch 420: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5383 - MinusLogProbMetric: 27.5383 - val_loss: 28.6085 - val_MinusLogProbMetric: 28.6085 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 421/1000
2023-09-29 04:46:43.628 
Epoch 421/1000 
	 loss: 27.5095, MinusLogProbMetric: 27.5095, val_loss: 28.5725, val_MinusLogProbMetric: 28.5725

Epoch 421: val_loss did not improve from 28.42212
196/196 - 31s - loss: 27.5095 - MinusLogProbMetric: 27.5095 - val_loss: 28.5725 - val_MinusLogProbMetric: 28.5725 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 422/1000
2023-09-29 04:47:15.576 
Epoch 422/1000 
	 loss: 27.5023, MinusLogProbMetric: 27.5023, val_loss: 28.6462, val_MinusLogProbMetric: 28.6462

Epoch 422: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.5023 - MinusLogProbMetric: 27.5023 - val_loss: 28.6462 - val_MinusLogProbMetric: 28.6462 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 423/1000
2023-09-29 04:47:48.197 
Epoch 423/1000 
	 loss: 27.4440, MinusLogProbMetric: 27.4440, val_loss: 28.6212, val_MinusLogProbMetric: 28.6212

Epoch 423: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.4440 - MinusLogProbMetric: 27.4440 - val_loss: 28.6212 - val_MinusLogProbMetric: 28.6212 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 424/1000
2023-09-29 04:48:21.884 
Epoch 424/1000 
	 loss: 27.4908, MinusLogProbMetric: 27.4908, val_loss: 29.0660, val_MinusLogProbMetric: 29.0660

Epoch 424: val_loss did not improve from 28.42212
196/196 - 34s - loss: 27.4908 - MinusLogProbMetric: 27.4908 - val_loss: 29.0660 - val_MinusLogProbMetric: 29.0660 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 425/1000
2023-09-29 04:48:53.984 
Epoch 425/1000 
	 loss: 27.4928, MinusLogProbMetric: 27.4928, val_loss: 28.5905, val_MinusLogProbMetric: 28.5905

Epoch 425: val_loss did not improve from 28.42212
196/196 - 32s - loss: 27.4928 - MinusLogProbMetric: 27.4928 - val_loss: 28.5905 - val_MinusLogProbMetric: 28.5905 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 426/1000
2023-09-29 04:49:26.654 
Epoch 426/1000 
	 loss: 27.4476, MinusLogProbMetric: 27.4476, val_loss: 28.9668, val_MinusLogProbMetric: 28.9668

Epoch 426: val_loss did not improve from 28.42212
196/196 - 33s - loss: 27.4476 - MinusLogProbMetric: 27.4476 - val_loss: 28.9668 - val_MinusLogProbMetric: 28.9668 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 427/1000
2023-09-29 04:50:00.130 
Epoch 427/1000 
	 loss: 27.2133, MinusLogProbMetric: 27.2133, val_loss: 28.3666, val_MinusLogProbMetric: 28.3666

Epoch 427: val_loss improved from 28.42212 to 28.36656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 27.2133 - MinusLogProbMetric: 27.2133 - val_loss: 28.3666 - val_MinusLogProbMetric: 28.3666 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 428/1000
2023-09-29 04:50:32.895 
Epoch 428/1000 
	 loss: 27.1739, MinusLogProbMetric: 27.1739, val_loss: 28.3240, val_MinusLogProbMetric: 28.3240

Epoch 428: val_loss improved from 28.36656 to 28.32399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 27.1739 - MinusLogProbMetric: 27.1739 - val_loss: 28.3240 - val_MinusLogProbMetric: 28.3240 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 429/1000
2023-09-29 04:51:03.446 
Epoch 429/1000 
	 loss: 27.1866, MinusLogProbMetric: 27.1866, val_loss: 28.3560, val_MinusLogProbMetric: 28.3560

Epoch 429: val_loss did not improve from 28.32399
196/196 - 30s - loss: 27.1866 - MinusLogProbMetric: 27.1866 - val_loss: 28.3560 - val_MinusLogProbMetric: 28.3560 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 430/1000
2023-09-29 04:51:37.242 
Epoch 430/1000 
	 loss: 27.1851, MinusLogProbMetric: 27.1851, val_loss: 28.3722, val_MinusLogProbMetric: 28.3722

Epoch 430: val_loss did not improve from 28.32399
196/196 - 34s - loss: 27.1851 - MinusLogProbMetric: 27.1851 - val_loss: 28.3722 - val_MinusLogProbMetric: 28.3722 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 431/1000
2023-09-29 04:52:07.911 
Epoch 431/1000 
	 loss: 27.1875, MinusLogProbMetric: 27.1875, val_loss: 28.3602, val_MinusLogProbMetric: 28.3602

Epoch 431: val_loss did not improve from 28.32399
196/196 - 31s - loss: 27.1875 - MinusLogProbMetric: 27.1875 - val_loss: 28.3602 - val_MinusLogProbMetric: 28.3602 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 432/1000
2023-09-29 04:52:41.149 
Epoch 432/1000 
	 loss: 27.1892, MinusLogProbMetric: 27.1892, val_loss: 28.4058, val_MinusLogProbMetric: 28.4058

Epoch 432: val_loss did not improve from 28.32399
196/196 - 33s - loss: 27.1892 - MinusLogProbMetric: 27.1892 - val_loss: 28.4058 - val_MinusLogProbMetric: 28.4058 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 433/1000
2023-09-29 04:53:13.695 
Epoch 433/1000 
	 loss: 27.1828, MinusLogProbMetric: 27.1828, val_loss: 28.3333, val_MinusLogProbMetric: 28.3333

Epoch 433: val_loss did not improve from 28.32399
196/196 - 33s - loss: 27.1828 - MinusLogProbMetric: 27.1828 - val_loss: 28.3333 - val_MinusLogProbMetric: 28.3333 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 434/1000
2023-09-29 04:53:47.082 
Epoch 434/1000 
	 loss: 27.1755, MinusLogProbMetric: 27.1755, val_loss: 28.3191, val_MinusLogProbMetric: 28.3191

Epoch 434: val_loss improved from 28.32399 to 28.31908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 27.1755 - MinusLogProbMetric: 27.1755 - val_loss: 28.3191 - val_MinusLogProbMetric: 28.3191 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 435/1000
2023-09-29 04:54:18.783 
Epoch 435/1000 
	 loss: 27.1755, MinusLogProbMetric: 27.1755, val_loss: 28.3538, val_MinusLogProbMetric: 28.3538

Epoch 435: val_loss did not improve from 28.31908
196/196 - 31s - loss: 27.1755 - MinusLogProbMetric: 27.1755 - val_loss: 28.3538 - val_MinusLogProbMetric: 28.3538 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 436/1000
2023-09-29 04:54:49.410 
Epoch 436/1000 
	 loss: 27.1766, MinusLogProbMetric: 27.1766, val_loss: 28.3069, val_MinusLogProbMetric: 28.3069

Epoch 436: val_loss improved from 28.31908 to 28.30691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 31s - loss: 27.1766 - MinusLogProbMetric: 27.1766 - val_loss: 28.3069 - val_MinusLogProbMetric: 28.3069 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 437/1000
2023-09-29 04:55:24.044 
Epoch 437/1000 
	 loss: 27.1794, MinusLogProbMetric: 27.1794, val_loss: 28.3190, val_MinusLogProbMetric: 28.3190

Epoch 437: val_loss did not improve from 28.30691
196/196 - 34s - loss: 27.1794 - MinusLogProbMetric: 27.1794 - val_loss: 28.3190 - val_MinusLogProbMetric: 28.3190 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 438/1000
2023-09-29 04:55:56.924 
Epoch 438/1000 
	 loss: 27.1525, MinusLogProbMetric: 27.1525, val_loss: 28.3456, val_MinusLogProbMetric: 28.3456

Epoch 438: val_loss did not improve from 28.30691
196/196 - 33s - loss: 27.1525 - MinusLogProbMetric: 27.1525 - val_loss: 28.3456 - val_MinusLogProbMetric: 28.3456 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 439/1000
2023-09-29 04:56:29.762 
Epoch 439/1000 
	 loss: 27.1825, MinusLogProbMetric: 27.1825, val_loss: 28.4605, val_MinusLogProbMetric: 28.4605

Epoch 439: val_loss did not improve from 28.30691
196/196 - 33s - loss: 27.1825 - MinusLogProbMetric: 27.1825 - val_loss: 28.4605 - val_MinusLogProbMetric: 28.4605 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 440/1000
2023-09-29 04:57:01.620 
Epoch 440/1000 
	 loss: 27.1899, MinusLogProbMetric: 27.1899, val_loss: 28.3193, val_MinusLogProbMetric: 28.3193

Epoch 440: val_loss did not improve from 28.30691
196/196 - 32s - loss: 27.1899 - MinusLogProbMetric: 27.1899 - val_loss: 28.3193 - val_MinusLogProbMetric: 28.3193 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 441/1000
2023-09-29 04:57:32.680 
Epoch 441/1000 
	 loss: 27.2003, MinusLogProbMetric: 27.2003, val_loss: 28.3204, val_MinusLogProbMetric: 28.3204

Epoch 441: val_loss did not improve from 28.30691
196/196 - 31s - loss: 27.2003 - MinusLogProbMetric: 27.2003 - val_loss: 28.3204 - val_MinusLogProbMetric: 28.3204 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 442/1000
2023-09-29 04:58:04.556 
Epoch 442/1000 
	 loss: 27.1729, MinusLogProbMetric: 27.1729, val_loss: 28.3523, val_MinusLogProbMetric: 28.3523

Epoch 442: val_loss did not improve from 28.30691
196/196 - 32s - loss: 27.1729 - MinusLogProbMetric: 27.1729 - val_loss: 28.3523 - val_MinusLogProbMetric: 28.3523 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 443/1000
2023-09-29 04:58:38.299 
Epoch 443/1000 
	 loss: 27.1595, MinusLogProbMetric: 27.1595, val_loss: 28.4743, val_MinusLogProbMetric: 28.4743

Epoch 443: val_loss did not improve from 28.30691
196/196 - 34s - loss: 27.1595 - MinusLogProbMetric: 27.1595 - val_loss: 28.4743 - val_MinusLogProbMetric: 28.4743 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 444/1000
2023-09-29 04:59:11.207 
Epoch 444/1000 
	 loss: 27.1602, MinusLogProbMetric: 27.1602, val_loss: 28.3375, val_MinusLogProbMetric: 28.3375

Epoch 444: val_loss did not improve from 28.30691
196/196 - 33s - loss: 27.1602 - MinusLogProbMetric: 27.1602 - val_loss: 28.3375 - val_MinusLogProbMetric: 28.3375 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 445/1000
2023-09-29 04:59:44.020 
Epoch 445/1000 
	 loss: 27.1500, MinusLogProbMetric: 27.1500, val_loss: 28.3000, val_MinusLogProbMetric: 28.3000

Epoch 445: val_loss improved from 28.30691 to 28.30000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 27.1500 - MinusLogProbMetric: 27.1500 - val_loss: 28.3000 - val_MinusLogProbMetric: 28.3000 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 446/1000
2023-09-29 05:00:15.958 
Epoch 446/1000 
	 loss: 27.1474, MinusLogProbMetric: 27.1474, val_loss: 28.3592, val_MinusLogProbMetric: 28.3592

Epoch 446: val_loss did not improve from 28.30000
196/196 - 31s - loss: 27.1474 - MinusLogProbMetric: 27.1474 - val_loss: 28.3592 - val_MinusLogProbMetric: 28.3592 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 447/1000
2023-09-29 05:00:48.081 
Epoch 447/1000 
	 loss: 27.2014, MinusLogProbMetric: 27.2014, val_loss: 28.3254, val_MinusLogProbMetric: 28.3254

Epoch 447: val_loss did not improve from 28.30000
196/196 - 32s - loss: 27.2014 - MinusLogProbMetric: 27.2014 - val_loss: 28.3254 - val_MinusLogProbMetric: 28.3254 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 448/1000
2023-09-29 05:01:20.647 
Epoch 448/1000 
	 loss: 27.1818, MinusLogProbMetric: 27.1818, val_loss: 28.4678, val_MinusLogProbMetric: 28.4678

Epoch 448: val_loss did not improve from 28.30000
196/196 - 33s - loss: 27.1818 - MinusLogProbMetric: 27.1818 - val_loss: 28.4678 - val_MinusLogProbMetric: 28.4678 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 449/1000
2023-09-29 05:01:51.874 
Epoch 449/1000 
	 loss: 27.1786, MinusLogProbMetric: 27.1786, val_loss: 28.2956, val_MinusLogProbMetric: 28.2956

Epoch 449: val_loss improved from 28.30000 to 28.29562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 32s - loss: 27.1786 - MinusLogProbMetric: 27.1786 - val_loss: 28.2956 - val_MinusLogProbMetric: 28.2956 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 450/1000
2023-09-29 05:02:23.197 
Epoch 450/1000 
	 loss: 27.1500, MinusLogProbMetric: 27.1500, val_loss: 28.3506, val_MinusLogProbMetric: 28.3506

Epoch 450: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1500 - MinusLogProbMetric: 27.1500 - val_loss: 28.3506 - val_MinusLogProbMetric: 28.3506 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 451/1000
2023-09-29 05:02:53.764 
Epoch 451/1000 
	 loss: 27.2169, MinusLogProbMetric: 27.2169, val_loss: 28.3906, val_MinusLogProbMetric: 28.3906

Epoch 451: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.2169 - MinusLogProbMetric: 27.2169 - val_loss: 28.3906 - val_MinusLogProbMetric: 28.3906 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 452/1000
2023-09-29 05:03:25.491 
Epoch 452/1000 
	 loss: 27.1637, MinusLogProbMetric: 27.1637, val_loss: 28.3050, val_MinusLogProbMetric: 28.3050

Epoch 452: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1637 - MinusLogProbMetric: 27.1637 - val_loss: 28.3050 - val_MinusLogProbMetric: 28.3050 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 453/1000
2023-09-29 05:04:00.231 
Epoch 453/1000 
	 loss: 27.1525, MinusLogProbMetric: 27.1525, val_loss: 28.4731, val_MinusLogProbMetric: 28.4731

Epoch 453: val_loss did not improve from 28.29562
196/196 - 35s - loss: 27.1525 - MinusLogProbMetric: 27.1525 - val_loss: 28.4731 - val_MinusLogProbMetric: 28.4731 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 454/1000
2023-09-29 05:04:33.170 
Epoch 454/1000 
	 loss: 27.1699, MinusLogProbMetric: 27.1699, val_loss: 28.5621, val_MinusLogProbMetric: 28.5621

Epoch 454: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1699 - MinusLogProbMetric: 27.1699 - val_loss: 28.5621 - val_MinusLogProbMetric: 28.5621 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 455/1000
2023-09-29 05:05:05.460 
Epoch 455/1000 
	 loss: 27.1367, MinusLogProbMetric: 27.1367, val_loss: 28.4209, val_MinusLogProbMetric: 28.4209

Epoch 455: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1367 - MinusLogProbMetric: 27.1367 - val_loss: 28.4209 - val_MinusLogProbMetric: 28.4209 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 456/1000
2023-09-29 05:05:37.409 
Epoch 456/1000 
	 loss: 27.1718, MinusLogProbMetric: 27.1718, val_loss: 28.3960, val_MinusLogProbMetric: 28.3960

Epoch 456: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1718 - MinusLogProbMetric: 27.1718 - val_loss: 28.3960 - val_MinusLogProbMetric: 28.3960 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 457/1000
2023-09-29 05:06:07.808 
Epoch 457/1000 
	 loss: 27.1395, MinusLogProbMetric: 27.1395, val_loss: 28.3262, val_MinusLogProbMetric: 28.3262

Epoch 457: val_loss did not improve from 28.29562
196/196 - 30s - loss: 27.1395 - MinusLogProbMetric: 27.1395 - val_loss: 28.3262 - val_MinusLogProbMetric: 28.3262 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 458/1000
2023-09-29 05:06:38.107 
Epoch 458/1000 
	 loss: 27.1649, MinusLogProbMetric: 27.1649, val_loss: 28.3181, val_MinusLogProbMetric: 28.3181

Epoch 458: val_loss did not improve from 28.29562
196/196 - 30s - loss: 27.1649 - MinusLogProbMetric: 27.1649 - val_loss: 28.3181 - val_MinusLogProbMetric: 28.3181 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 459/1000
2023-09-29 05:07:09.369 
Epoch 459/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.5561, val_MinusLogProbMetric: 28.5561

Epoch 459: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.5561 - val_MinusLogProbMetric: 28.5561 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 460/1000
2023-09-29 05:07:40.650 
Epoch 460/1000 
	 loss: 27.1603, MinusLogProbMetric: 27.1603, val_loss: 28.4692, val_MinusLogProbMetric: 28.4692

Epoch 460: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1603 - MinusLogProbMetric: 27.1603 - val_loss: 28.4692 - val_MinusLogProbMetric: 28.4692 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 461/1000
2023-09-29 05:08:12.225 
Epoch 461/1000 
	 loss: 27.1529, MinusLogProbMetric: 27.1529, val_loss: 28.3045, val_MinusLogProbMetric: 28.3045

Epoch 461: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1529 - MinusLogProbMetric: 27.1529 - val_loss: 28.3045 - val_MinusLogProbMetric: 28.3045 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 462/1000
2023-09-29 05:08:45.910 
Epoch 462/1000 
	 loss: 27.1595, MinusLogProbMetric: 27.1595, val_loss: 28.3106, val_MinusLogProbMetric: 28.3106

Epoch 462: val_loss did not improve from 28.29562
196/196 - 34s - loss: 27.1595 - MinusLogProbMetric: 27.1595 - val_loss: 28.3106 - val_MinusLogProbMetric: 28.3106 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 463/1000
2023-09-29 05:09:15.836 
Epoch 463/1000 
	 loss: 27.1564, MinusLogProbMetric: 27.1564, val_loss: 28.3985, val_MinusLogProbMetric: 28.3985

Epoch 463: val_loss did not improve from 28.29562
196/196 - 30s - loss: 27.1564 - MinusLogProbMetric: 27.1564 - val_loss: 28.3985 - val_MinusLogProbMetric: 28.3985 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 464/1000
2023-09-29 05:09:47.571 
Epoch 464/1000 
	 loss: 27.1789, MinusLogProbMetric: 27.1789, val_loss: 28.3645, val_MinusLogProbMetric: 28.3645

Epoch 464: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1789 - MinusLogProbMetric: 27.1789 - val_loss: 28.3645 - val_MinusLogProbMetric: 28.3645 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 465/1000
2023-09-29 05:10:19.608 
Epoch 465/1000 
	 loss: 27.1779, MinusLogProbMetric: 27.1779, val_loss: 28.3457, val_MinusLogProbMetric: 28.3457

Epoch 465: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1779 - MinusLogProbMetric: 27.1779 - val_loss: 28.3457 - val_MinusLogProbMetric: 28.3457 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 466/1000
2023-09-29 05:10:50.924 
Epoch 466/1000 
	 loss: 27.1426, MinusLogProbMetric: 27.1426, val_loss: 28.3715, val_MinusLogProbMetric: 28.3715

Epoch 466: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1426 - MinusLogProbMetric: 27.1426 - val_loss: 28.3715 - val_MinusLogProbMetric: 28.3715 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 467/1000
2023-09-29 05:11:21.955 
Epoch 467/1000 
	 loss: 27.1549, MinusLogProbMetric: 27.1549, val_loss: 28.3213, val_MinusLogProbMetric: 28.3213

Epoch 467: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1549 - MinusLogProbMetric: 27.1549 - val_loss: 28.3213 - val_MinusLogProbMetric: 28.3213 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 468/1000
2023-09-29 05:11:53.922 
Epoch 468/1000 
	 loss: 27.1204, MinusLogProbMetric: 27.1204, val_loss: 28.3754, val_MinusLogProbMetric: 28.3754

Epoch 468: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1204 - MinusLogProbMetric: 27.1204 - val_loss: 28.3754 - val_MinusLogProbMetric: 28.3754 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 469/1000
2023-09-29 05:12:26.659 
Epoch 469/1000 
	 loss: 27.1659, MinusLogProbMetric: 27.1659, val_loss: 28.5710, val_MinusLogProbMetric: 28.5710

Epoch 469: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1659 - MinusLogProbMetric: 27.1659 - val_loss: 28.5710 - val_MinusLogProbMetric: 28.5710 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 470/1000
2023-09-29 05:12:57.377 
Epoch 470/1000 
	 loss: 27.1587, MinusLogProbMetric: 27.1587, val_loss: 28.4489, val_MinusLogProbMetric: 28.4489

Epoch 470: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1587 - MinusLogProbMetric: 27.1587 - val_loss: 28.4489 - val_MinusLogProbMetric: 28.4489 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 471/1000
2023-09-29 05:13:29.612 
Epoch 471/1000 
	 loss: 27.1402, MinusLogProbMetric: 27.1402, val_loss: 28.3376, val_MinusLogProbMetric: 28.3376

Epoch 471: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1402 - MinusLogProbMetric: 27.1402 - val_loss: 28.3376 - val_MinusLogProbMetric: 28.3376 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 472/1000
2023-09-29 05:14:01.987 
Epoch 472/1000 
	 loss: 27.1564, MinusLogProbMetric: 27.1564, val_loss: 28.3990, val_MinusLogProbMetric: 28.3990

Epoch 472: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1564 - MinusLogProbMetric: 27.1564 - val_loss: 28.3990 - val_MinusLogProbMetric: 28.3990 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 473/1000
2023-09-29 05:14:34.444 
Epoch 473/1000 
	 loss: 27.1646, MinusLogProbMetric: 27.1646, val_loss: 28.3954, val_MinusLogProbMetric: 28.3954

Epoch 473: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1646 - MinusLogProbMetric: 27.1646 - val_loss: 28.3954 - val_MinusLogProbMetric: 28.3954 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 474/1000
2023-09-29 05:15:07.532 
Epoch 474/1000 
	 loss: 27.1401, MinusLogProbMetric: 27.1401, val_loss: 28.3375, val_MinusLogProbMetric: 28.3375

Epoch 474: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1401 - MinusLogProbMetric: 27.1401 - val_loss: 28.3375 - val_MinusLogProbMetric: 28.3375 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 475/1000
2023-09-29 05:15:40.233 
Epoch 475/1000 
	 loss: 27.1527, MinusLogProbMetric: 27.1527, val_loss: 28.4377, val_MinusLogProbMetric: 28.4377

Epoch 475: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1527 - MinusLogProbMetric: 27.1527 - val_loss: 28.4377 - val_MinusLogProbMetric: 28.4377 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 476/1000
2023-09-29 05:16:12.042 
Epoch 476/1000 
	 loss: 27.1671, MinusLogProbMetric: 27.1671, val_loss: 28.4603, val_MinusLogProbMetric: 28.4603

Epoch 476: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1671 - MinusLogProbMetric: 27.1671 - val_loss: 28.4603 - val_MinusLogProbMetric: 28.4603 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 477/1000
2023-09-29 05:16:45.243 
Epoch 477/1000 
	 loss: 27.1419, MinusLogProbMetric: 27.1419, val_loss: 28.3825, val_MinusLogProbMetric: 28.3825

Epoch 477: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1419 - MinusLogProbMetric: 27.1419 - val_loss: 28.3825 - val_MinusLogProbMetric: 28.3825 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 478/1000
2023-09-29 05:17:17.810 
Epoch 478/1000 
	 loss: 27.1489, MinusLogProbMetric: 27.1489, val_loss: 28.3468, val_MinusLogProbMetric: 28.3468

Epoch 478: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1489 - MinusLogProbMetric: 27.1489 - val_loss: 28.3468 - val_MinusLogProbMetric: 28.3468 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 479/1000
2023-09-29 05:17:51.486 
Epoch 479/1000 
	 loss: 27.1295, MinusLogProbMetric: 27.1295, val_loss: 28.3499, val_MinusLogProbMetric: 28.3499

Epoch 479: val_loss did not improve from 28.29562
196/196 - 34s - loss: 27.1295 - MinusLogProbMetric: 27.1295 - val_loss: 28.3499 - val_MinusLogProbMetric: 28.3499 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 480/1000
2023-09-29 05:18:25.484 
Epoch 480/1000 
	 loss: 27.1234, MinusLogProbMetric: 27.1234, val_loss: 28.3093, val_MinusLogProbMetric: 28.3093

Epoch 480: val_loss did not improve from 28.29562
196/196 - 34s - loss: 27.1234 - MinusLogProbMetric: 27.1234 - val_loss: 28.3093 - val_MinusLogProbMetric: 28.3093 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 481/1000
2023-09-29 05:18:59.398 
Epoch 481/1000 
	 loss: 27.1186, MinusLogProbMetric: 27.1186, val_loss: 28.3516, val_MinusLogProbMetric: 28.3516

Epoch 481: val_loss did not improve from 28.29562
196/196 - 34s - loss: 27.1186 - MinusLogProbMetric: 27.1186 - val_loss: 28.3516 - val_MinusLogProbMetric: 28.3516 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 482/1000
2023-09-29 05:19:32.676 
Epoch 482/1000 
	 loss: 27.1686, MinusLogProbMetric: 27.1686, val_loss: 28.9378, val_MinusLogProbMetric: 28.9378

Epoch 482: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1686 - MinusLogProbMetric: 27.1686 - val_loss: 28.9378 - val_MinusLogProbMetric: 28.9378 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 483/1000
2023-09-29 05:20:06.451 
Epoch 483/1000 
	 loss: 27.1902, MinusLogProbMetric: 27.1902, val_loss: 28.4258, val_MinusLogProbMetric: 28.4258

Epoch 483: val_loss did not improve from 28.29562
196/196 - 34s - loss: 27.1902 - MinusLogProbMetric: 27.1902 - val_loss: 28.4258 - val_MinusLogProbMetric: 28.4258 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 484/1000
2023-09-29 05:20:39.249 
Epoch 484/1000 
	 loss: 27.1361, MinusLogProbMetric: 27.1361, val_loss: 28.4280, val_MinusLogProbMetric: 28.4280

Epoch 484: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1361 - MinusLogProbMetric: 27.1361 - val_loss: 28.4280 - val_MinusLogProbMetric: 28.4280 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 485/1000
2023-09-29 05:21:12.966 
Epoch 485/1000 
	 loss: 27.1648, MinusLogProbMetric: 27.1648, val_loss: 28.3503, val_MinusLogProbMetric: 28.3503

Epoch 485: val_loss did not improve from 28.29562
196/196 - 34s - loss: 27.1648 - MinusLogProbMetric: 27.1648 - val_loss: 28.3503 - val_MinusLogProbMetric: 28.3503 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 486/1000
2023-09-29 05:21:44.577 
Epoch 486/1000 
	 loss: 27.1270, MinusLogProbMetric: 27.1270, val_loss: 28.4905, val_MinusLogProbMetric: 28.4905

Epoch 486: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1270 - MinusLogProbMetric: 27.1270 - val_loss: 28.4905 - val_MinusLogProbMetric: 28.4905 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 487/1000
2023-09-29 05:22:16.211 
Epoch 487/1000 
	 loss: 27.1148, MinusLogProbMetric: 27.1148, val_loss: 28.3892, val_MinusLogProbMetric: 28.3892

Epoch 487: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1148 - MinusLogProbMetric: 27.1148 - val_loss: 28.3892 - val_MinusLogProbMetric: 28.3892 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 488/1000
2023-09-29 05:22:47.055 
Epoch 488/1000 
	 loss: 27.1315, MinusLogProbMetric: 27.1315, val_loss: 28.4187, val_MinusLogProbMetric: 28.4187

Epoch 488: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1315 - MinusLogProbMetric: 27.1315 - val_loss: 28.4187 - val_MinusLogProbMetric: 28.4187 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 489/1000
2023-09-29 05:23:19.464 
Epoch 489/1000 
	 loss: 27.1430, MinusLogProbMetric: 27.1430, val_loss: 28.4544, val_MinusLogProbMetric: 28.4544

Epoch 489: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1430 - MinusLogProbMetric: 27.1430 - val_loss: 28.4544 - val_MinusLogProbMetric: 28.4544 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 490/1000
2023-09-29 05:23:51.320 
Epoch 490/1000 
	 loss: 27.1291, MinusLogProbMetric: 27.1291, val_loss: 28.3946, val_MinusLogProbMetric: 28.3946

Epoch 490: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1291 - MinusLogProbMetric: 27.1291 - val_loss: 28.3946 - val_MinusLogProbMetric: 28.3946 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 491/1000
2023-09-29 05:24:23.386 
Epoch 491/1000 
	 loss: 27.1246, MinusLogProbMetric: 27.1246, val_loss: 28.4939, val_MinusLogProbMetric: 28.4939

Epoch 491: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1246 - MinusLogProbMetric: 27.1246 - val_loss: 28.4939 - val_MinusLogProbMetric: 28.4939 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 492/1000
2023-09-29 05:24:54.081 
Epoch 492/1000 
	 loss: 27.1475, MinusLogProbMetric: 27.1475, val_loss: 28.4367, val_MinusLogProbMetric: 28.4367

Epoch 492: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1475 - MinusLogProbMetric: 27.1475 - val_loss: 28.4367 - val_MinusLogProbMetric: 28.4367 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 493/1000
2023-09-29 05:25:25.788 
Epoch 493/1000 
	 loss: 27.1235, MinusLogProbMetric: 27.1235, val_loss: 28.3785, val_MinusLogProbMetric: 28.3785

Epoch 493: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1235 - MinusLogProbMetric: 27.1235 - val_loss: 28.3785 - val_MinusLogProbMetric: 28.3785 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 494/1000
2023-09-29 05:25:57.849 
Epoch 494/1000 
	 loss: 27.1266, MinusLogProbMetric: 27.1266, val_loss: 28.4438, val_MinusLogProbMetric: 28.4438

Epoch 494: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1266 - MinusLogProbMetric: 27.1266 - val_loss: 28.4438 - val_MinusLogProbMetric: 28.4438 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 495/1000
2023-09-29 05:26:32.446 
Epoch 495/1000 
	 loss: 27.1250, MinusLogProbMetric: 27.1250, val_loss: 28.3983, val_MinusLogProbMetric: 28.3983

Epoch 495: val_loss did not improve from 28.29562
196/196 - 35s - loss: 27.1250 - MinusLogProbMetric: 27.1250 - val_loss: 28.3983 - val_MinusLogProbMetric: 28.3983 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 496/1000
2023-09-29 05:27:05.185 
Epoch 496/1000 
	 loss: 27.1297, MinusLogProbMetric: 27.1297, val_loss: 28.5831, val_MinusLogProbMetric: 28.5831

Epoch 496: val_loss did not improve from 28.29562
196/196 - 33s - loss: 27.1297 - MinusLogProbMetric: 27.1297 - val_loss: 28.5831 - val_MinusLogProbMetric: 28.5831 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 497/1000
2023-09-29 05:27:36.848 
Epoch 497/1000 
	 loss: 27.1339, MinusLogProbMetric: 27.1339, val_loss: 28.3236, val_MinusLogProbMetric: 28.3236

Epoch 497: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1339 - MinusLogProbMetric: 27.1339 - val_loss: 28.3236 - val_MinusLogProbMetric: 28.3236 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 498/1000
2023-09-29 05:28:08.313 
Epoch 498/1000 
	 loss: 27.1269, MinusLogProbMetric: 27.1269, val_loss: 28.3551, val_MinusLogProbMetric: 28.3551

Epoch 498: val_loss did not improve from 28.29562
196/196 - 31s - loss: 27.1269 - MinusLogProbMetric: 27.1269 - val_loss: 28.3551 - val_MinusLogProbMetric: 28.3551 - lr: 2.5000e-04 - 31s/epoch - 161ms/step
Epoch 499/1000
2023-09-29 05:28:40.238 
Epoch 499/1000 
	 loss: 27.1157, MinusLogProbMetric: 27.1157, val_loss: 28.3476, val_MinusLogProbMetric: 28.3476

Epoch 499: val_loss did not improve from 28.29562
196/196 - 32s - loss: 27.1157 - MinusLogProbMetric: 27.1157 - val_loss: 28.3476 - val_MinusLogProbMetric: 28.3476 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 500/1000
2023-09-29 05:29:14.575 
Epoch 500/1000 
	 loss: 27.0242, MinusLogProbMetric: 27.0242, val_loss: 28.4109, val_MinusLogProbMetric: 28.4109

Epoch 500: val_loss did not improve from 28.29562
196/196 - 34s - loss: 27.0242 - MinusLogProbMetric: 27.0242 - val_loss: 28.4109 - val_MinusLogProbMetric: 28.4109 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 501/1000
2023-09-29 05:29:47.449 
Epoch 501/1000 
	 loss: 27.0228, MinusLogProbMetric: 27.0228, val_loss: 28.2951, val_MinusLogProbMetric: 28.2951

Epoch 501: val_loss improved from 28.29562 to 28.29512, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 27.0228 - MinusLogProbMetric: 27.0228 - val_loss: 28.2951 - val_MinusLogProbMetric: 28.2951 - lr: 1.2500e-04 - 33s/epoch - 171ms/step
Epoch 502/1000
2023-09-29 05:30:21.877 
Epoch 502/1000 
	 loss: 27.0154, MinusLogProbMetric: 27.0154, val_loss: 28.2919, val_MinusLogProbMetric: 28.2919

Epoch 502: val_loss improved from 28.29512 to 28.29188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 27.0154 - MinusLogProbMetric: 27.0154 - val_loss: 28.2919 - val_MinusLogProbMetric: 28.2919 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 503/1000
2023-09-29 05:30:56.860 
Epoch 503/1000 
	 loss: 26.9957, MinusLogProbMetric: 26.9957, val_loss: 28.3549, val_MinusLogProbMetric: 28.3549

Epoch 503: val_loss did not improve from 28.29188
196/196 - 34s - loss: 26.9957 - MinusLogProbMetric: 26.9957 - val_loss: 28.3549 - val_MinusLogProbMetric: 28.3549 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 504/1000
2023-09-29 05:31:30.364 
Epoch 504/1000 
	 loss: 27.0064, MinusLogProbMetric: 27.0064, val_loss: 28.2993, val_MinusLogProbMetric: 28.2993

Epoch 504: val_loss did not improve from 28.29188
196/196 - 34s - loss: 27.0064 - MinusLogProbMetric: 27.0064 - val_loss: 28.2993 - val_MinusLogProbMetric: 28.2993 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 505/1000
2023-09-29 05:32:03.965 
Epoch 505/1000 
	 loss: 27.0016, MinusLogProbMetric: 27.0016, val_loss: 28.3294, val_MinusLogProbMetric: 28.3294

Epoch 505: val_loss did not improve from 28.29188
196/196 - 34s - loss: 27.0016 - MinusLogProbMetric: 27.0016 - val_loss: 28.3294 - val_MinusLogProbMetric: 28.3294 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 506/1000
2023-09-29 05:32:36.104 
Epoch 506/1000 
	 loss: 26.9986, MinusLogProbMetric: 26.9986, val_loss: 28.2964, val_MinusLogProbMetric: 28.2964

Epoch 506: val_loss did not improve from 28.29188
196/196 - 32s - loss: 26.9986 - MinusLogProbMetric: 26.9986 - val_loss: 28.2964 - val_MinusLogProbMetric: 28.2964 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 507/1000
2023-09-29 05:33:08.580 
Epoch 507/1000 
	 loss: 26.9982, MinusLogProbMetric: 26.9982, val_loss: 28.3399, val_MinusLogProbMetric: 28.3399

Epoch 507: val_loss did not improve from 28.29188
196/196 - 32s - loss: 26.9982 - MinusLogProbMetric: 26.9982 - val_loss: 28.3399 - val_MinusLogProbMetric: 28.3399 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 508/1000
2023-09-29 05:33:41.504 
Epoch 508/1000 
	 loss: 27.0115, MinusLogProbMetric: 27.0115, val_loss: 28.2885, val_MinusLogProbMetric: 28.2885

Epoch 508: val_loss improved from 28.29188 to 28.28849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 33s - loss: 27.0115 - MinusLogProbMetric: 27.0115 - val_loss: 28.2885 - val_MinusLogProbMetric: 28.2885 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 509/1000
2023-09-29 05:34:13.326 
Epoch 509/1000 
	 loss: 27.0178, MinusLogProbMetric: 27.0178, val_loss: 28.2953, val_MinusLogProbMetric: 28.2953

Epoch 509: val_loss did not improve from 28.28849
196/196 - 31s - loss: 27.0178 - MinusLogProbMetric: 27.0178 - val_loss: 28.2953 - val_MinusLogProbMetric: 28.2953 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 510/1000
2023-09-29 05:34:45.889 
Epoch 510/1000 
	 loss: 27.0128, MinusLogProbMetric: 27.0128, val_loss: 28.3120, val_MinusLogProbMetric: 28.3120

Epoch 510: val_loss did not improve from 28.28849
196/196 - 33s - loss: 27.0128 - MinusLogProbMetric: 27.0128 - val_loss: 28.3120 - val_MinusLogProbMetric: 28.3120 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 511/1000
2023-09-29 05:35:18.999 
Epoch 511/1000 
	 loss: 26.9952, MinusLogProbMetric: 26.9952, val_loss: 28.3116, val_MinusLogProbMetric: 28.3116

Epoch 511: val_loss did not improve from 28.28849
196/196 - 33s - loss: 26.9952 - MinusLogProbMetric: 26.9952 - val_loss: 28.3116 - val_MinusLogProbMetric: 28.3116 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 512/1000
2023-09-29 05:35:50.868 
Epoch 512/1000 
	 loss: 26.9996, MinusLogProbMetric: 26.9996, val_loss: 28.3935, val_MinusLogProbMetric: 28.3935

Epoch 512: val_loss did not improve from 28.28849
196/196 - 32s - loss: 26.9996 - MinusLogProbMetric: 26.9996 - val_loss: 28.3935 - val_MinusLogProbMetric: 28.3935 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 513/1000
2023-09-29 05:36:23.482 
Epoch 513/1000 
	 loss: 27.0062, MinusLogProbMetric: 27.0062, val_loss: 28.2951, val_MinusLogProbMetric: 28.2951

Epoch 513: val_loss did not improve from 28.28849
196/196 - 33s - loss: 27.0062 - MinusLogProbMetric: 27.0062 - val_loss: 28.2951 - val_MinusLogProbMetric: 28.2951 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 514/1000
2023-09-29 05:36:57.238 
Epoch 514/1000 
	 loss: 27.0200, MinusLogProbMetric: 27.0200, val_loss: 28.2800, val_MinusLogProbMetric: 28.2800

Epoch 514: val_loss improved from 28.28849 to 28.27999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 34s - loss: 27.0200 - MinusLogProbMetric: 27.0200 - val_loss: 28.2800 - val_MinusLogProbMetric: 28.2800 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 515/1000
2023-09-29 05:37:29.216 
Epoch 515/1000 
	 loss: 27.0011, MinusLogProbMetric: 27.0011, val_loss: 28.3826, val_MinusLogProbMetric: 28.3826

Epoch 515: val_loss did not improve from 28.27999
196/196 - 31s - loss: 27.0011 - MinusLogProbMetric: 27.0011 - val_loss: 28.3826 - val_MinusLogProbMetric: 28.3826 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 516/1000
2023-09-29 05:38:01.655 
Epoch 516/1000 
	 loss: 27.0071, MinusLogProbMetric: 27.0071, val_loss: 28.3920, val_MinusLogProbMetric: 28.3920

Epoch 516: val_loss did not improve from 28.27999
196/196 - 32s - loss: 27.0071 - MinusLogProbMetric: 27.0071 - val_loss: 28.3920 - val_MinusLogProbMetric: 28.3920 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 517/1000
2023-09-29 05:38:35.554 
Epoch 517/1000 
	 loss: 27.0223, MinusLogProbMetric: 27.0223, val_loss: 28.3575, val_MinusLogProbMetric: 28.3575

Epoch 517: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0223 - MinusLogProbMetric: 27.0223 - val_loss: 28.3575 - val_MinusLogProbMetric: 28.3575 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 518/1000
2023-09-29 05:39:07.727 
Epoch 518/1000 
	 loss: 27.0175, MinusLogProbMetric: 27.0175, val_loss: 28.3624, val_MinusLogProbMetric: 28.3624

Epoch 518: val_loss did not improve from 28.27999
196/196 - 32s - loss: 27.0175 - MinusLogProbMetric: 27.0175 - val_loss: 28.3624 - val_MinusLogProbMetric: 28.3624 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 519/1000
2023-09-29 05:39:42.252 
Epoch 519/1000 
	 loss: 27.0024, MinusLogProbMetric: 27.0024, val_loss: 28.2965, val_MinusLogProbMetric: 28.2965

Epoch 519: val_loss did not improve from 28.27999
196/196 - 35s - loss: 27.0024 - MinusLogProbMetric: 27.0024 - val_loss: 28.2965 - val_MinusLogProbMetric: 28.2965 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 520/1000
2023-09-29 05:40:15.016 
Epoch 520/1000 
	 loss: 26.9987, MinusLogProbMetric: 26.9987, val_loss: 28.2946, val_MinusLogProbMetric: 28.2946

Epoch 520: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9987 - MinusLogProbMetric: 26.9987 - val_loss: 28.2946 - val_MinusLogProbMetric: 28.2946 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 521/1000
2023-09-29 05:40:49.216 
Epoch 521/1000 
	 loss: 27.0130, MinusLogProbMetric: 27.0130, val_loss: 28.4035, val_MinusLogProbMetric: 28.4035

Epoch 521: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0130 - MinusLogProbMetric: 27.0130 - val_loss: 28.4035 - val_MinusLogProbMetric: 28.4035 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 522/1000
2023-09-29 05:41:21.930 
Epoch 522/1000 
	 loss: 27.0044, MinusLogProbMetric: 27.0044, val_loss: 28.2914, val_MinusLogProbMetric: 28.2914

Epoch 522: val_loss did not improve from 28.27999
196/196 - 33s - loss: 27.0044 - MinusLogProbMetric: 27.0044 - val_loss: 28.2914 - val_MinusLogProbMetric: 28.2914 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 523/1000
2023-09-29 05:41:55.590 
Epoch 523/1000 
	 loss: 27.0083, MinusLogProbMetric: 27.0083, val_loss: 28.3082, val_MinusLogProbMetric: 28.3082

Epoch 523: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0083 - MinusLogProbMetric: 27.0083 - val_loss: 28.3082 - val_MinusLogProbMetric: 28.3082 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 524/1000
2023-09-29 05:42:29.468 
Epoch 524/1000 
	 loss: 26.9909, MinusLogProbMetric: 26.9909, val_loss: 28.2865, val_MinusLogProbMetric: 28.2865

Epoch 524: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9909 - MinusLogProbMetric: 26.9909 - val_loss: 28.2865 - val_MinusLogProbMetric: 28.2865 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 525/1000
2023-09-29 05:43:01.962 
Epoch 525/1000 
	 loss: 26.9879, MinusLogProbMetric: 26.9879, val_loss: 28.3103, val_MinusLogProbMetric: 28.3103

Epoch 525: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9879 - MinusLogProbMetric: 26.9879 - val_loss: 28.3103 - val_MinusLogProbMetric: 28.3103 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 526/1000
2023-09-29 05:43:35.594 
Epoch 526/1000 
	 loss: 26.9873, MinusLogProbMetric: 26.9873, val_loss: 28.3071, val_MinusLogProbMetric: 28.3071

Epoch 526: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9873 - MinusLogProbMetric: 26.9873 - val_loss: 28.3071 - val_MinusLogProbMetric: 28.3071 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 527/1000
2023-09-29 05:44:08.374 
Epoch 527/1000 
	 loss: 26.9909, MinusLogProbMetric: 26.9909, val_loss: 28.2854, val_MinusLogProbMetric: 28.2854

Epoch 527: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9909 - MinusLogProbMetric: 26.9909 - val_loss: 28.2854 - val_MinusLogProbMetric: 28.2854 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 528/1000
2023-09-29 05:44:41.655 
Epoch 528/1000 
	 loss: 26.9886, MinusLogProbMetric: 26.9886, val_loss: 28.2954, val_MinusLogProbMetric: 28.2954

Epoch 528: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9886 - MinusLogProbMetric: 26.9886 - val_loss: 28.2954 - val_MinusLogProbMetric: 28.2954 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 529/1000
2023-09-29 05:45:15.722 
Epoch 529/1000 
	 loss: 27.0032, MinusLogProbMetric: 27.0032, val_loss: 28.2862, val_MinusLogProbMetric: 28.2862

Epoch 529: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0032 - MinusLogProbMetric: 27.0032 - val_loss: 28.2862 - val_MinusLogProbMetric: 28.2862 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 530/1000
2023-09-29 05:45:49.574 
Epoch 530/1000 
	 loss: 27.0026, MinusLogProbMetric: 27.0026, val_loss: 28.2959, val_MinusLogProbMetric: 28.2959

Epoch 530: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0026 - MinusLogProbMetric: 27.0026 - val_loss: 28.2959 - val_MinusLogProbMetric: 28.2959 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 531/1000
2023-09-29 05:46:23.213 
Epoch 531/1000 
	 loss: 26.9901, MinusLogProbMetric: 26.9901, val_loss: 28.3452, val_MinusLogProbMetric: 28.3452

Epoch 531: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9901 - MinusLogProbMetric: 26.9901 - val_loss: 28.3452 - val_MinusLogProbMetric: 28.3452 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 532/1000
2023-09-29 05:46:57.916 
Epoch 532/1000 
	 loss: 26.9938, MinusLogProbMetric: 26.9938, val_loss: 28.3708, val_MinusLogProbMetric: 28.3708

Epoch 532: val_loss did not improve from 28.27999
196/196 - 35s - loss: 26.9938 - MinusLogProbMetric: 26.9938 - val_loss: 28.3708 - val_MinusLogProbMetric: 28.3708 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 533/1000
2023-09-29 05:47:31.141 
Epoch 533/1000 
	 loss: 26.9949, MinusLogProbMetric: 26.9949, val_loss: 28.3012, val_MinusLogProbMetric: 28.3012

Epoch 533: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9949 - MinusLogProbMetric: 26.9949 - val_loss: 28.3012 - val_MinusLogProbMetric: 28.3012 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 534/1000
2023-09-29 05:48:05.302 
Epoch 534/1000 
	 loss: 26.9899, MinusLogProbMetric: 26.9899, val_loss: 28.3176, val_MinusLogProbMetric: 28.3176

Epoch 534: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9899 - MinusLogProbMetric: 26.9899 - val_loss: 28.3176 - val_MinusLogProbMetric: 28.3176 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 535/1000
2023-09-29 05:48:38.077 
Epoch 535/1000 
	 loss: 27.0130, MinusLogProbMetric: 27.0130, val_loss: 28.2922, val_MinusLogProbMetric: 28.2922

Epoch 535: val_loss did not improve from 28.27999
196/196 - 33s - loss: 27.0130 - MinusLogProbMetric: 27.0130 - val_loss: 28.2922 - val_MinusLogProbMetric: 28.2922 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 536/1000
2023-09-29 05:49:09.320 
Epoch 536/1000 
	 loss: 27.0070, MinusLogProbMetric: 27.0070, val_loss: 28.3913, val_MinusLogProbMetric: 28.3913

Epoch 536: val_loss did not improve from 28.27999
196/196 - 31s - loss: 27.0070 - MinusLogProbMetric: 27.0070 - val_loss: 28.3913 - val_MinusLogProbMetric: 28.3913 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 537/1000
2023-09-29 05:49:42.970 
Epoch 537/1000 
	 loss: 27.0104, MinusLogProbMetric: 27.0104, val_loss: 28.4086, val_MinusLogProbMetric: 28.4086

Epoch 537: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0104 - MinusLogProbMetric: 27.0104 - val_loss: 28.4086 - val_MinusLogProbMetric: 28.4086 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 538/1000
2023-09-29 05:50:16.507 
Epoch 538/1000 
	 loss: 27.0060, MinusLogProbMetric: 27.0060, val_loss: 28.3221, val_MinusLogProbMetric: 28.3221

Epoch 538: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0060 - MinusLogProbMetric: 27.0060 - val_loss: 28.3221 - val_MinusLogProbMetric: 28.3221 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 539/1000
2023-09-29 05:50:48.769 
Epoch 539/1000 
	 loss: 27.0096, MinusLogProbMetric: 27.0096, val_loss: 28.3244, val_MinusLogProbMetric: 28.3244

Epoch 539: val_loss did not improve from 28.27999
196/196 - 32s - loss: 27.0096 - MinusLogProbMetric: 27.0096 - val_loss: 28.3244 - val_MinusLogProbMetric: 28.3244 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 540/1000
2023-09-29 05:51:22.185 
Epoch 540/1000 
	 loss: 26.9951, MinusLogProbMetric: 26.9951, val_loss: 28.3500, val_MinusLogProbMetric: 28.3500

Epoch 540: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9951 - MinusLogProbMetric: 26.9951 - val_loss: 28.3500 - val_MinusLogProbMetric: 28.3500 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 541/1000
2023-09-29 05:51:56.650 
Epoch 541/1000 
	 loss: 26.9948, MinusLogProbMetric: 26.9948, val_loss: 28.3591, val_MinusLogProbMetric: 28.3591

Epoch 541: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9948 - MinusLogProbMetric: 26.9948 - val_loss: 28.3591 - val_MinusLogProbMetric: 28.3591 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 542/1000
2023-09-29 05:52:28.767 
Epoch 542/1000 
	 loss: 26.9866, MinusLogProbMetric: 26.9866, val_loss: 28.2844, val_MinusLogProbMetric: 28.2844

Epoch 542: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9866 - MinusLogProbMetric: 26.9866 - val_loss: 28.2844 - val_MinusLogProbMetric: 28.2844 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 543/1000
2023-09-29 05:53:01.996 
Epoch 543/1000 
	 loss: 27.0000, MinusLogProbMetric: 27.0000, val_loss: 28.3902, val_MinusLogProbMetric: 28.3902

Epoch 543: val_loss did not improve from 28.27999
196/196 - 33s - loss: 27.0000 - MinusLogProbMetric: 27.0000 - val_loss: 28.3902 - val_MinusLogProbMetric: 28.3902 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 544/1000
2023-09-29 05:53:36.464 
Epoch 544/1000 
	 loss: 26.9784, MinusLogProbMetric: 26.9784, val_loss: 28.2944, val_MinusLogProbMetric: 28.2944

Epoch 544: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9784 - MinusLogProbMetric: 26.9784 - val_loss: 28.2944 - val_MinusLogProbMetric: 28.2944 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 545/1000
2023-09-29 05:54:08.545 
Epoch 545/1000 
	 loss: 26.9970, MinusLogProbMetric: 26.9970, val_loss: 28.3605, val_MinusLogProbMetric: 28.3605

Epoch 545: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9970 - MinusLogProbMetric: 26.9970 - val_loss: 28.3605 - val_MinusLogProbMetric: 28.3605 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 546/1000
2023-09-29 05:54:42.154 
Epoch 546/1000 
	 loss: 26.9922, MinusLogProbMetric: 26.9922, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 546: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9922 - MinusLogProbMetric: 26.9922 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 547/1000
2023-09-29 05:55:14.370 
Epoch 547/1000 
	 loss: 26.9853, MinusLogProbMetric: 26.9853, val_loss: 28.3243, val_MinusLogProbMetric: 28.3243

Epoch 547: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9853 - MinusLogProbMetric: 26.9853 - val_loss: 28.3243 - val_MinusLogProbMetric: 28.3243 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 548/1000
2023-09-29 05:55:47.279 
Epoch 548/1000 
	 loss: 26.9988, MinusLogProbMetric: 26.9988, val_loss: 28.3106, val_MinusLogProbMetric: 28.3106

Epoch 548: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9988 - MinusLogProbMetric: 26.9988 - val_loss: 28.3106 - val_MinusLogProbMetric: 28.3106 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 549/1000
2023-09-29 05:56:19.005 
Epoch 549/1000 
	 loss: 26.9788, MinusLogProbMetric: 26.9788, val_loss: 28.3025, val_MinusLogProbMetric: 28.3025

Epoch 549: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9788 - MinusLogProbMetric: 26.9788 - val_loss: 28.3025 - val_MinusLogProbMetric: 28.3025 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 550/1000
2023-09-29 05:56:51.471 
Epoch 550/1000 
	 loss: 26.9690, MinusLogProbMetric: 26.9690, val_loss: 28.3346, val_MinusLogProbMetric: 28.3346

Epoch 550: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9690 - MinusLogProbMetric: 26.9690 - val_loss: 28.3346 - val_MinusLogProbMetric: 28.3346 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 551/1000
2023-09-29 05:57:23.782 
Epoch 551/1000 
	 loss: 26.9849, MinusLogProbMetric: 26.9849, val_loss: 28.2993, val_MinusLogProbMetric: 28.2993

Epoch 551: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9849 - MinusLogProbMetric: 26.9849 - val_loss: 28.2993 - val_MinusLogProbMetric: 28.2993 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 552/1000
2023-09-29 05:57:56.332 
Epoch 552/1000 
	 loss: 26.9740, MinusLogProbMetric: 26.9740, val_loss: 28.2839, val_MinusLogProbMetric: 28.2839

Epoch 552: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9740 - MinusLogProbMetric: 26.9740 - val_loss: 28.2839 - val_MinusLogProbMetric: 28.2839 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 553/1000
2023-09-29 05:58:30.864 
Epoch 553/1000 
	 loss: 26.9792, MinusLogProbMetric: 26.9792, val_loss: 28.3299, val_MinusLogProbMetric: 28.3299

Epoch 553: val_loss did not improve from 28.27999
196/196 - 35s - loss: 26.9792 - MinusLogProbMetric: 26.9792 - val_loss: 28.3299 - val_MinusLogProbMetric: 28.3299 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 554/1000
2023-09-29 05:59:05.306 
Epoch 554/1000 
	 loss: 26.9882, MinusLogProbMetric: 26.9882, val_loss: 28.3246, val_MinusLogProbMetric: 28.3246

Epoch 554: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9882 - MinusLogProbMetric: 26.9882 - val_loss: 28.3246 - val_MinusLogProbMetric: 28.3246 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 555/1000
2023-09-29 05:59:38.509 
Epoch 555/1000 
	 loss: 26.9808, MinusLogProbMetric: 26.9808, val_loss: 28.4376, val_MinusLogProbMetric: 28.4376

Epoch 555: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9808 - MinusLogProbMetric: 26.9808 - val_loss: 28.4376 - val_MinusLogProbMetric: 28.4376 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 556/1000
2023-09-29 06:00:12.794 
Epoch 556/1000 
	 loss: 26.9995, MinusLogProbMetric: 26.9995, val_loss: 28.3199, val_MinusLogProbMetric: 28.3199

Epoch 556: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9995 - MinusLogProbMetric: 26.9995 - val_loss: 28.3199 - val_MinusLogProbMetric: 28.3199 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 557/1000
2023-09-29 06:00:47.259 
Epoch 557/1000 
	 loss: 26.9842, MinusLogProbMetric: 26.9842, val_loss: 28.3281, val_MinusLogProbMetric: 28.3281

Epoch 557: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9842 - MinusLogProbMetric: 26.9842 - val_loss: 28.3281 - val_MinusLogProbMetric: 28.3281 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 558/1000
2023-09-29 06:01:20.771 
Epoch 558/1000 
	 loss: 27.0135, MinusLogProbMetric: 27.0135, val_loss: 28.3622, val_MinusLogProbMetric: 28.3622

Epoch 558: val_loss did not improve from 28.27999
196/196 - 34s - loss: 27.0135 - MinusLogProbMetric: 27.0135 - val_loss: 28.3622 - val_MinusLogProbMetric: 28.3622 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 559/1000
2023-09-29 06:01:54.282 
Epoch 559/1000 
	 loss: 26.9875, MinusLogProbMetric: 26.9875, val_loss: 28.3390, val_MinusLogProbMetric: 28.3390

Epoch 559: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9875 - MinusLogProbMetric: 26.9875 - val_loss: 28.3390 - val_MinusLogProbMetric: 28.3390 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 560/1000
2023-09-29 06:02:25.506 
Epoch 560/1000 
	 loss: 26.9800, MinusLogProbMetric: 26.9800, val_loss: 28.4821, val_MinusLogProbMetric: 28.4821

Epoch 560: val_loss did not improve from 28.27999
196/196 - 31s - loss: 26.9800 - MinusLogProbMetric: 26.9800 - val_loss: 28.4821 - val_MinusLogProbMetric: 28.4821 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 561/1000
2023-09-29 06:02:59.626 
Epoch 561/1000 
	 loss: 26.9759, MinusLogProbMetric: 26.9759, val_loss: 28.3187, val_MinusLogProbMetric: 28.3187

Epoch 561: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9759 - MinusLogProbMetric: 26.9759 - val_loss: 28.3187 - val_MinusLogProbMetric: 28.3187 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 562/1000
2023-09-29 06:03:33.326 
Epoch 562/1000 
	 loss: 26.9876, MinusLogProbMetric: 26.9876, val_loss: 28.3059, val_MinusLogProbMetric: 28.3059

Epoch 562: val_loss did not improve from 28.27999
196/196 - 34s - loss: 26.9876 - MinusLogProbMetric: 26.9876 - val_loss: 28.3059 - val_MinusLogProbMetric: 28.3059 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 563/1000
2023-09-29 06:04:05.213 
Epoch 563/1000 
	 loss: 26.9798, MinusLogProbMetric: 26.9798, val_loss: 28.3020, val_MinusLogProbMetric: 28.3020

Epoch 563: val_loss did not improve from 28.27999
196/196 - 32s - loss: 26.9798 - MinusLogProbMetric: 26.9798 - val_loss: 28.3020 - val_MinusLogProbMetric: 28.3020 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 564/1000
2023-09-29 06:04:37.831 
Epoch 564/1000 
	 loss: 26.9841, MinusLogProbMetric: 26.9841, val_loss: 28.3648, val_MinusLogProbMetric: 28.3648

Epoch 564: val_loss did not improve from 28.27999
196/196 - 33s - loss: 26.9841 - MinusLogProbMetric: 26.9841 - val_loss: 28.3648 - val_MinusLogProbMetric: 28.3648 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 565/1000
2023-09-29 06:05:09.111 
Epoch 565/1000 
	 loss: 26.9319, MinusLogProbMetric: 26.9319, val_loss: 28.2736, val_MinusLogProbMetric: 28.2736

Epoch 565: val_loss improved from 28.27999 to 28.27359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 32s - loss: 26.9319 - MinusLogProbMetric: 26.9319 - val_loss: 28.2736 - val_MinusLogProbMetric: 28.2736 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 566/1000
2023-09-29 06:05:42.721 
Epoch 566/1000 
	 loss: 26.9357, MinusLogProbMetric: 26.9357, val_loss: 28.2742, val_MinusLogProbMetric: 28.2742

Epoch 566: val_loss did not improve from 28.27359
196/196 - 33s - loss: 26.9357 - MinusLogProbMetric: 26.9357 - val_loss: 28.2742 - val_MinusLogProbMetric: 28.2742 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 567/1000
2023-09-29 06:06:16.369 
Epoch 567/1000 
	 loss: 26.9363, MinusLogProbMetric: 26.9363, val_loss: 28.3311, val_MinusLogProbMetric: 28.3311

Epoch 567: val_loss did not improve from 28.27359
196/196 - 34s - loss: 26.9363 - MinusLogProbMetric: 26.9363 - val_loss: 28.3311 - val_MinusLogProbMetric: 28.3311 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 568/1000
2023-09-29 06:06:51.235 
Epoch 568/1000 
	 loss: 26.9323, MinusLogProbMetric: 26.9323, val_loss: 28.2777, val_MinusLogProbMetric: 28.2777

Epoch 568: val_loss did not improve from 28.27359
196/196 - 35s - loss: 26.9323 - MinusLogProbMetric: 26.9323 - val_loss: 28.2777 - val_MinusLogProbMetric: 28.2777 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 569/1000
2023-09-29 06:07:22.720 
Epoch 569/1000 
	 loss: 26.9340, MinusLogProbMetric: 26.9340, val_loss: 28.2823, val_MinusLogProbMetric: 28.2823

Epoch 569: val_loss did not improve from 28.27359
196/196 - 31s - loss: 26.9340 - MinusLogProbMetric: 26.9340 - val_loss: 28.2823 - val_MinusLogProbMetric: 28.2823 - lr: 6.2500e-05 - 31s/epoch - 161ms/step
Epoch 570/1000
2023-09-29 06:07:54.740 
Epoch 570/1000 
	 loss: 26.9332, MinusLogProbMetric: 26.9332, val_loss: 28.2752, val_MinusLogProbMetric: 28.2752

Epoch 570: val_loss did not improve from 28.27359
196/196 - 32s - loss: 26.9332 - MinusLogProbMetric: 26.9332 - val_loss: 28.2752 - val_MinusLogProbMetric: 28.2752 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 571/1000
2023-09-29 06:08:27.009 
Epoch 571/1000 
	 loss: 26.9333, MinusLogProbMetric: 26.9333, val_loss: 28.3046, val_MinusLogProbMetric: 28.3046

Epoch 571: val_loss did not improve from 28.27359
196/196 - 32s - loss: 26.9333 - MinusLogProbMetric: 26.9333 - val_loss: 28.3046 - val_MinusLogProbMetric: 28.3046 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 572/1000
2023-09-29 06:08:59.470 
Epoch 572/1000 
	 loss: 26.9251, MinusLogProbMetric: 26.9251, val_loss: 28.2918, val_MinusLogProbMetric: 28.2918

Epoch 572: val_loss did not improve from 28.27359
196/196 - 32s - loss: 26.9251 - MinusLogProbMetric: 26.9251 - val_loss: 28.2918 - val_MinusLogProbMetric: 28.2918 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 573/1000
2023-09-29 06:09:32.896 
Epoch 573/1000 
	 loss: 26.9262, MinusLogProbMetric: 26.9262, val_loss: 28.2977, val_MinusLogProbMetric: 28.2977

Epoch 573: val_loss did not improve from 28.27359
196/196 - 33s - loss: 26.9262 - MinusLogProbMetric: 26.9262 - val_loss: 28.2977 - val_MinusLogProbMetric: 28.2977 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 574/1000
2023-09-29 06:10:07.582 
Epoch 574/1000 
	 loss: 26.9273, MinusLogProbMetric: 26.9273, val_loss: 28.2881, val_MinusLogProbMetric: 28.2881

Epoch 574: val_loss did not improve from 28.27359
196/196 - 35s - loss: 26.9273 - MinusLogProbMetric: 26.9273 - val_loss: 28.2881 - val_MinusLogProbMetric: 28.2881 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 575/1000
2023-09-29 06:10:41.680 
Epoch 575/1000 
	 loss: 26.9270, MinusLogProbMetric: 26.9270, val_loss: 28.2965, val_MinusLogProbMetric: 28.2965

Epoch 575: val_loss did not improve from 28.27359
196/196 - 34s - loss: 26.9270 - MinusLogProbMetric: 26.9270 - val_loss: 28.2965 - val_MinusLogProbMetric: 28.2965 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 576/1000
2023-09-29 06:11:13.644 
Epoch 576/1000 
	 loss: 26.9300, MinusLogProbMetric: 26.9300, val_loss: 28.3103, val_MinusLogProbMetric: 28.3103

Epoch 576: val_loss did not improve from 28.27359
196/196 - 32s - loss: 26.9300 - MinusLogProbMetric: 26.9300 - val_loss: 28.3103 - val_MinusLogProbMetric: 28.3103 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 577/1000
2023-09-29 06:11:45.220 
Epoch 577/1000 
	 loss: 26.9226, MinusLogProbMetric: 26.9226, val_loss: 28.2845, val_MinusLogProbMetric: 28.2845

Epoch 577: val_loss did not improve from 28.27359
196/196 - 32s - loss: 26.9226 - MinusLogProbMetric: 26.9226 - val_loss: 28.2845 - val_MinusLogProbMetric: 28.2845 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 578/1000
2023-09-29 06:12:16.603 
Epoch 578/1000 
	 loss: 26.9290, MinusLogProbMetric: 26.9290, val_loss: 28.2918, val_MinusLogProbMetric: 28.2918

Epoch 578: val_loss did not improve from 28.27359
196/196 - 31s - loss: 26.9290 - MinusLogProbMetric: 26.9290 - val_loss: 28.2918 - val_MinusLogProbMetric: 28.2918 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 579/1000
2023-09-29 06:12:48.176 
Epoch 579/1000 
	 loss: 26.9293, MinusLogProbMetric: 26.9293, val_loss: 28.2777, val_MinusLogProbMetric: 28.2777

Epoch 579: val_loss did not improve from 28.27359
196/196 - 32s - loss: 26.9293 - MinusLogProbMetric: 26.9293 - val_loss: 28.2777 - val_MinusLogProbMetric: 28.2777 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 580/1000
2023-09-29 06:13:22.133 
Epoch 580/1000 
	 loss: 26.9209, MinusLogProbMetric: 26.9209, val_loss: 28.2755, val_MinusLogProbMetric: 28.2755

Epoch 580: val_loss did not improve from 28.27359
196/196 - 34s - loss: 26.9209 - MinusLogProbMetric: 26.9209 - val_loss: 28.2755 - val_MinusLogProbMetric: 28.2755 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 581/1000
2023-09-29 06:13:57.186 
Epoch 581/1000 
	 loss: 26.9214, MinusLogProbMetric: 26.9214, val_loss: 28.3004, val_MinusLogProbMetric: 28.3004

Epoch 581: val_loss did not improve from 28.27359
196/196 - 35s - loss: 26.9214 - MinusLogProbMetric: 26.9214 - val_loss: 28.3004 - val_MinusLogProbMetric: 28.3004 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 582/1000
2023-09-29 06:14:31.769 
Epoch 582/1000 
	 loss: 26.9225, MinusLogProbMetric: 26.9225, val_loss: 28.3117, val_MinusLogProbMetric: 28.3117

Epoch 582: val_loss did not improve from 28.27359
196/196 - 35s - loss: 26.9225 - MinusLogProbMetric: 26.9225 - val_loss: 28.3117 - val_MinusLogProbMetric: 28.3117 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 583/1000
2023-09-29 06:15:07.264 
Epoch 583/1000 
	 loss: 26.9256, MinusLogProbMetric: 26.9256, val_loss: 28.3194, val_MinusLogProbMetric: 28.3194

Epoch 583: val_loss did not improve from 28.27359
196/196 - 35s - loss: 26.9256 - MinusLogProbMetric: 26.9256 - val_loss: 28.3194 - val_MinusLogProbMetric: 28.3194 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 584/1000
2023-09-29 06:15:42.910 
Epoch 584/1000 
	 loss: 26.9251, MinusLogProbMetric: 26.9251, val_loss: 28.3199, val_MinusLogProbMetric: 28.3199

Epoch 584: val_loss did not improve from 28.27359
196/196 - 36s - loss: 26.9251 - MinusLogProbMetric: 26.9251 - val_loss: 28.3199 - val_MinusLogProbMetric: 28.3199 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 585/1000
2023-09-29 06:16:18.514 
Epoch 585/1000 
	 loss: 26.9232, MinusLogProbMetric: 26.9232, val_loss: 28.2727, val_MinusLogProbMetric: 28.2727

Epoch 585: val_loss improved from 28.27359 to 28.27272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 36s - loss: 26.9232 - MinusLogProbMetric: 26.9232 - val_loss: 28.2727 - val_MinusLogProbMetric: 28.2727 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 586/1000
2023-09-29 06:16:52.757 
Epoch 586/1000 
	 loss: 26.9225, MinusLogProbMetric: 26.9225, val_loss: 28.2921, val_MinusLogProbMetric: 28.2921

Epoch 586: val_loss did not improve from 28.27272
196/196 - 34s - loss: 26.9225 - MinusLogProbMetric: 26.9225 - val_loss: 28.2921 - val_MinusLogProbMetric: 28.2921 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 587/1000
2023-09-29 06:17:25.252 
Epoch 587/1000 
	 loss: 26.9251, MinusLogProbMetric: 26.9251, val_loss: 28.2939, val_MinusLogProbMetric: 28.2939

Epoch 587: val_loss did not improve from 28.27272
196/196 - 32s - loss: 26.9251 - MinusLogProbMetric: 26.9251 - val_loss: 28.2939 - val_MinusLogProbMetric: 28.2939 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 588/1000
2023-09-29 06:18:00.325 
Epoch 588/1000 
	 loss: 26.9261, MinusLogProbMetric: 26.9261, val_loss: 28.2939, val_MinusLogProbMetric: 28.2939

Epoch 588: val_loss did not improve from 28.27272
196/196 - 35s - loss: 26.9261 - MinusLogProbMetric: 26.9261 - val_loss: 28.2939 - val_MinusLogProbMetric: 28.2939 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 589/1000
2023-09-29 06:18:36.203 
Epoch 589/1000 
	 loss: 26.9244, MinusLogProbMetric: 26.9244, val_loss: 28.3011, val_MinusLogProbMetric: 28.3011

Epoch 589: val_loss did not improve from 28.27272
196/196 - 36s - loss: 26.9244 - MinusLogProbMetric: 26.9244 - val_loss: 28.3011 - val_MinusLogProbMetric: 28.3011 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 590/1000
2023-09-29 06:19:10.886 
Epoch 590/1000 
	 loss: 26.9311, MinusLogProbMetric: 26.9311, val_loss: 28.2814, val_MinusLogProbMetric: 28.2814

Epoch 590: val_loss did not improve from 28.27272
196/196 - 35s - loss: 26.9311 - MinusLogProbMetric: 26.9311 - val_loss: 28.2814 - val_MinusLogProbMetric: 28.2814 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 591/1000
2023-09-29 06:19:45.859 
Epoch 591/1000 
	 loss: 26.9286, MinusLogProbMetric: 26.9286, val_loss: 28.2802, val_MinusLogProbMetric: 28.2802

Epoch 591: val_loss did not improve from 28.27272
196/196 - 35s - loss: 26.9286 - MinusLogProbMetric: 26.9286 - val_loss: 28.2802 - val_MinusLogProbMetric: 28.2802 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 592/1000
2023-09-29 06:20:20.949 
Epoch 592/1000 
	 loss: 26.9211, MinusLogProbMetric: 26.9211, val_loss: 28.2966, val_MinusLogProbMetric: 28.2966

Epoch 592: val_loss did not improve from 28.27272
196/196 - 35s - loss: 26.9211 - MinusLogProbMetric: 26.9211 - val_loss: 28.2966 - val_MinusLogProbMetric: 28.2966 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 593/1000
2023-09-29 06:20:54.017 
Epoch 593/1000 
	 loss: 26.9252, MinusLogProbMetric: 26.9252, val_loss: 28.3088, val_MinusLogProbMetric: 28.3088

Epoch 593: val_loss did not improve from 28.27272
196/196 - 33s - loss: 26.9252 - MinusLogProbMetric: 26.9252 - val_loss: 28.3088 - val_MinusLogProbMetric: 28.3088 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 594/1000
2023-09-29 06:21:28.937 
Epoch 594/1000 
	 loss: 26.9212, MinusLogProbMetric: 26.9212, val_loss: 28.3082, val_MinusLogProbMetric: 28.3082

Epoch 594: val_loss did not improve from 28.27272
196/196 - 35s - loss: 26.9212 - MinusLogProbMetric: 26.9212 - val_loss: 28.3082 - val_MinusLogProbMetric: 28.3082 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 595/1000
2023-09-29 06:22:03.022 
Epoch 595/1000 
	 loss: 26.9260, MinusLogProbMetric: 26.9260, val_loss: 28.2771, val_MinusLogProbMetric: 28.2771

Epoch 595: val_loss did not improve from 28.27272
196/196 - 34s - loss: 26.9260 - MinusLogProbMetric: 26.9260 - val_loss: 28.2771 - val_MinusLogProbMetric: 28.2771 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 596/1000
2023-09-29 06:22:37.466 
Epoch 596/1000 
	 loss: 26.9278, MinusLogProbMetric: 26.9278, val_loss: 28.2687, val_MinusLogProbMetric: 28.2687

Epoch 596: val_loss improved from 28.27272 to 28.26874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_330/weights/best_weights.h5
196/196 - 35s - loss: 26.9278 - MinusLogProbMetric: 26.9278 - val_loss: 28.2687 - val_MinusLogProbMetric: 28.2687 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 597/1000
2023-09-29 06:23:12.221 
Epoch 597/1000 
	 loss: 26.9374, MinusLogProbMetric: 26.9374, val_loss: 28.2876, val_MinusLogProbMetric: 28.2876

Epoch 597: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.9374 - MinusLogProbMetric: 26.9374 - val_loss: 28.2876 - val_MinusLogProbMetric: 28.2876 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 598/1000
2023-09-29 06:23:47.754 
Epoch 598/1000 
	 loss: 26.9398, MinusLogProbMetric: 26.9398, val_loss: 28.3196, val_MinusLogProbMetric: 28.3196

Epoch 598: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9398 - MinusLogProbMetric: 26.9398 - val_loss: 28.3196 - val_MinusLogProbMetric: 28.3196 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 599/1000
2023-09-29 06:24:21.998 
Epoch 599/1000 
	 loss: 26.9389, MinusLogProbMetric: 26.9389, val_loss: 28.2974, val_MinusLogProbMetric: 28.2974

Epoch 599: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.9389 - MinusLogProbMetric: 26.9389 - val_loss: 28.2974 - val_MinusLogProbMetric: 28.2974 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 600/1000
2023-09-29 06:24:57.720 
Epoch 600/1000 
	 loss: 26.9266, MinusLogProbMetric: 26.9266, val_loss: 28.3206, val_MinusLogProbMetric: 28.3206

Epoch 600: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9266 - MinusLogProbMetric: 26.9266 - val_loss: 28.3206 - val_MinusLogProbMetric: 28.3206 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 601/1000
2023-09-29 06:25:33.612 
Epoch 601/1000 
	 loss: 26.9275, MinusLogProbMetric: 26.9275, val_loss: 28.2895, val_MinusLogProbMetric: 28.2895

Epoch 601: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9275 - MinusLogProbMetric: 26.9275 - val_loss: 28.2895 - val_MinusLogProbMetric: 28.2895 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 602/1000
2023-09-29 06:26:09.293 
Epoch 602/1000 
	 loss: 26.9274, MinusLogProbMetric: 26.9274, val_loss: 28.2764, val_MinusLogProbMetric: 28.2764

Epoch 602: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9274 - MinusLogProbMetric: 26.9274 - val_loss: 28.2764 - val_MinusLogProbMetric: 28.2764 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 603/1000
2023-09-29 06:26:45.001 
Epoch 603/1000 
	 loss: 26.9292, MinusLogProbMetric: 26.9292, val_loss: 28.2729, val_MinusLogProbMetric: 28.2729

Epoch 603: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9292 - MinusLogProbMetric: 26.9292 - val_loss: 28.2729 - val_MinusLogProbMetric: 28.2729 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 604/1000
2023-09-29 06:27:20.684 
Epoch 604/1000 
	 loss: 26.9218, MinusLogProbMetric: 26.9218, val_loss: 28.2764, val_MinusLogProbMetric: 28.2764

Epoch 604: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9218 - MinusLogProbMetric: 26.9218 - val_loss: 28.2764 - val_MinusLogProbMetric: 28.2764 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 605/1000
2023-09-29 06:27:55.870 
Epoch 605/1000 
	 loss: 26.9284, MinusLogProbMetric: 26.9284, val_loss: 28.3778, val_MinusLogProbMetric: 28.3778

Epoch 605: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9284 - MinusLogProbMetric: 26.9284 - val_loss: 28.3778 - val_MinusLogProbMetric: 28.3778 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 606/1000
2023-09-29 06:28:31.558 
Epoch 606/1000 
	 loss: 26.9316, MinusLogProbMetric: 26.9316, val_loss: 28.3096, val_MinusLogProbMetric: 28.3096

Epoch 606: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9316 - MinusLogProbMetric: 26.9316 - val_loss: 28.3096 - val_MinusLogProbMetric: 28.3096 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 607/1000
2023-09-29 06:29:07.020 
Epoch 607/1000 
	 loss: 26.9228, MinusLogProbMetric: 26.9228, val_loss: 28.2955, val_MinusLogProbMetric: 28.2955

Epoch 607: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9228 - MinusLogProbMetric: 26.9228 - val_loss: 28.2955 - val_MinusLogProbMetric: 28.2955 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 608/1000
2023-09-29 06:29:42.431 
Epoch 608/1000 
	 loss: 26.9236, MinusLogProbMetric: 26.9236, val_loss: 28.3253, val_MinusLogProbMetric: 28.3253

Epoch 608: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9236 - MinusLogProbMetric: 26.9236 - val_loss: 28.3253 - val_MinusLogProbMetric: 28.3253 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 609/1000
2023-09-29 06:30:18.211 
Epoch 609/1000 
	 loss: 26.9145, MinusLogProbMetric: 26.9145, val_loss: 28.3075, val_MinusLogProbMetric: 28.3075

Epoch 609: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9145 - MinusLogProbMetric: 26.9145 - val_loss: 28.3075 - val_MinusLogProbMetric: 28.3075 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 610/1000
2023-09-29 06:30:53.949 
Epoch 610/1000 
	 loss: 26.9222, MinusLogProbMetric: 26.9222, val_loss: 28.2803, val_MinusLogProbMetric: 28.2803

Epoch 610: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9222 - MinusLogProbMetric: 26.9222 - val_loss: 28.2803 - val_MinusLogProbMetric: 28.2803 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 611/1000
2023-09-29 06:31:29.800 
Epoch 611/1000 
	 loss: 26.9167, MinusLogProbMetric: 26.9167, val_loss: 28.2930, val_MinusLogProbMetric: 28.2930

Epoch 611: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9167 - MinusLogProbMetric: 26.9167 - val_loss: 28.2930 - val_MinusLogProbMetric: 28.2930 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 612/1000
2023-09-29 06:32:05.361 
Epoch 612/1000 
	 loss: 26.9159, MinusLogProbMetric: 26.9159, val_loss: 28.2810, val_MinusLogProbMetric: 28.2810

Epoch 612: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9159 - MinusLogProbMetric: 26.9159 - val_loss: 28.2810 - val_MinusLogProbMetric: 28.2810 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 613/1000
2023-09-29 06:32:40.997 
Epoch 613/1000 
	 loss: 26.9186, MinusLogProbMetric: 26.9186, val_loss: 28.3046, val_MinusLogProbMetric: 28.3046

Epoch 613: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9186 - MinusLogProbMetric: 26.9186 - val_loss: 28.3046 - val_MinusLogProbMetric: 28.3046 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 614/1000
2023-09-29 06:33:16.959 
Epoch 614/1000 
	 loss: 26.9144, MinusLogProbMetric: 26.9144, val_loss: 28.3141, val_MinusLogProbMetric: 28.3141

Epoch 614: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9144 - MinusLogProbMetric: 26.9144 - val_loss: 28.3141 - val_MinusLogProbMetric: 28.3141 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 615/1000
2023-09-29 06:33:52.600 
Epoch 615/1000 
	 loss: 26.9139, MinusLogProbMetric: 26.9139, val_loss: 28.2908, val_MinusLogProbMetric: 28.2908

Epoch 615: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9139 - MinusLogProbMetric: 26.9139 - val_loss: 28.2908 - val_MinusLogProbMetric: 28.2908 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 616/1000
2023-09-29 06:34:28.454 
Epoch 616/1000 
	 loss: 26.9137, MinusLogProbMetric: 26.9137, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 616: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9137 - MinusLogProbMetric: 26.9137 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 617/1000
2023-09-29 06:35:04.136 
Epoch 617/1000 
	 loss: 26.9149, MinusLogProbMetric: 26.9149, val_loss: 28.2967, val_MinusLogProbMetric: 28.2967

Epoch 617: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9149 - MinusLogProbMetric: 26.9149 - val_loss: 28.2967 - val_MinusLogProbMetric: 28.2967 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 618/1000
2023-09-29 06:35:39.850 
Epoch 618/1000 
	 loss: 26.9101, MinusLogProbMetric: 26.9101, val_loss: 28.2907, val_MinusLogProbMetric: 28.2907

Epoch 618: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9101 - MinusLogProbMetric: 26.9101 - val_loss: 28.2907 - val_MinusLogProbMetric: 28.2907 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 619/1000
2023-09-29 06:36:15.544 
Epoch 619/1000 
	 loss: 26.9164, MinusLogProbMetric: 26.9164, val_loss: 28.2712, val_MinusLogProbMetric: 28.2712

Epoch 619: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9164 - MinusLogProbMetric: 26.9164 - val_loss: 28.2712 - val_MinusLogProbMetric: 28.2712 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 620/1000
2023-09-29 06:36:51.259 
Epoch 620/1000 
	 loss: 26.9133, MinusLogProbMetric: 26.9133, val_loss: 28.2858, val_MinusLogProbMetric: 28.2858

Epoch 620: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9133 - MinusLogProbMetric: 26.9133 - val_loss: 28.2858 - val_MinusLogProbMetric: 28.2858 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 621/1000
2023-09-29 06:37:26.884 
Epoch 621/1000 
	 loss: 26.9108, MinusLogProbMetric: 26.9108, val_loss: 28.2803, val_MinusLogProbMetric: 28.2803

Epoch 621: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9108 - MinusLogProbMetric: 26.9108 - val_loss: 28.2803 - val_MinusLogProbMetric: 28.2803 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 622/1000
2023-09-29 06:38:02.750 
Epoch 622/1000 
	 loss: 26.9159, MinusLogProbMetric: 26.9159, val_loss: 28.3057, val_MinusLogProbMetric: 28.3057

Epoch 622: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9159 - MinusLogProbMetric: 26.9159 - val_loss: 28.3057 - val_MinusLogProbMetric: 28.3057 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 623/1000
2023-09-29 06:38:38.301 
Epoch 623/1000 
	 loss: 26.9109, MinusLogProbMetric: 26.9109, val_loss: 28.2760, val_MinusLogProbMetric: 28.2760

Epoch 623: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9109 - MinusLogProbMetric: 26.9109 - val_loss: 28.2760 - val_MinusLogProbMetric: 28.2760 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 624/1000
2023-09-29 06:39:13.829 
Epoch 624/1000 
	 loss: 26.9187, MinusLogProbMetric: 26.9187, val_loss: 28.3354, val_MinusLogProbMetric: 28.3354

Epoch 624: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9187 - MinusLogProbMetric: 26.9187 - val_loss: 28.3354 - val_MinusLogProbMetric: 28.3354 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 625/1000
2023-09-29 06:39:49.607 
Epoch 625/1000 
	 loss: 26.9164, MinusLogProbMetric: 26.9164, val_loss: 28.2918, val_MinusLogProbMetric: 28.2918

Epoch 625: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9164 - MinusLogProbMetric: 26.9164 - val_loss: 28.2918 - val_MinusLogProbMetric: 28.2918 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 626/1000
2023-09-29 06:40:25.180 
Epoch 626/1000 
	 loss: 26.9164, MinusLogProbMetric: 26.9164, val_loss: 28.3078, val_MinusLogProbMetric: 28.3078

Epoch 626: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9164 - MinusLogProbMetric: 26.9164 - val_loss: 28.3078 - val_MinusLogProbMetric: 28.3078 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 627/1000
2023-09-29 06:41:00.953 
Epoch 627/1000 
	 loss: 26.9132, MinusLogProbMetric: 26.9132, val_loss: 28.2984, val_MinusLogProbMetric: 28.2984

Epoch 627: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9132 - MinusLogProbMetric: 26.9132 - val_loss: 28.2984 - val_MinusLogProbMetric: 28.2984 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 628/1000
2023-09-29 06:41:36.598 
Epoch 628/1000 
	 loss: 26.9145, MinusLogProbMetric: 26.9145, val_loss: 28.3209, val_MinusLogProbMetric: 28.3209

Epoch 628: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9145 - MinusLogProbMetric: 26.9145 - val_loss: 28.3209 - val_MinusLogProbMetric: 28.3209 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 629/1000
2023-09-29 06:42:12.304 
Epoch 629/1000 
	 loss: 26.9189, MinusLogProbMetric: 26.9189, val_loss: 28.2841, val_MinusLogProbMetric: 28.2841

Epoch 629: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9189 - MinusLogProbMetric: 26.9189 - val_loss: 28.2841 - val_MinusLogProbMetric: 28.2841 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 630/1000
2023-09-29 06:42:48.121 
Epoch 630/1000 
	 loss: 26.9142, MinusLogProbMetric: 26.9142, val_loss: 28.3071, val_MinusLogProbMetric: 28.3071

Epoch 630: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9142 - MinusLogProbMetric: 26.9142 - val_loss: 28.3071 - val_MinusLogProbMetric: 28.3071 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 631/1000
2023-09-29 06:43:23.704 
Epoch 631/1000 
	 loss: 26.9183, MinusLogProbMetric: 26.9183, val_loss: 28.3199, val_MinusLogProbMetric: 28.3199

Epoch 631: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9183 - MinusLogProbMetric: 26.9183 - val_loss: 28.3199 - val_MinusLogProbMetric: 28.3199 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 632/1000
2023-09-29 06:43:59.433 
Epoch 632/1000 
	 loss: 26.9156, MinusLogProbMetric: 26.9156, val_loss: 28.2944, val_MinusLogProbMetric: 28.2944

Epoch 632: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9156 - MinusLogProbMetric: 26.9156 - val_loss: 28.2944 - val_MinusLogProbMetric: 28.2944 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 633/1000
2023-09-29 06:44:35.181 
Epoch 633/1000 
	 loss: 26.9169, MinusLogProbMetric: 26.9169, val_loss: 28.2956, val_MinusLogProbMetric: 28.2956

Epoch 633: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9169 - MinusLogProbMetric: 26.9169 - val_loss: 28.2956 - val_MinusLogProbMetric: 28.2956 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 634/1000
2023-09-29 06:45:10.867 
Epoch 634/1000 
	 loss: 26.9125, MinusLogProbMetric: 26.9125, val_loss: 28.3023, val_MinusLogProbMetric: 28.3023

Epoch 634: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9125 - MinusLogProbMetric: 26.9125 - val_loss: 28.3023 - val_MinusLogProbMetric: 28.3023 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 635/1000
2023-09-29 06:45:46.562 
Epoch 635/1000 
	 loss: 26.9133, MinusLogProbMetric: 26.9133, val_loss: 28.3075, val_MinusLogProbMetric: 28.3075

Epoch 635: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9133 - MinusLogProbMetric: 26.9133 - val_loss: 28.3075 - val_MinusLogProbMetric: 28.3075 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 636/1000
2023-09-29 06:46:21.827 
Epoch 636/1000 
	 loss: 26.9123, MinusLogProbMetric: 26.9123, val_loss: 28.2826, val_MinusLogProbMetric: 28.2826

Epoch 636: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9123 - MinusLogProbMetric: 26.9123 - val_loss: 28.2826 - val_MinusLogProbMetric: 28.2826 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 637/1000
2023-09-29 06:46:57.275 
Epoch 637/1000 
	 loss: 26.9152, MinusLogProbMetric: 26.9152, val_loss: 28.3227, val_MinusLogProbMetric: 28.3227

Epoch 637: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9152 - MinusLogProbMetric: 26.9152 - val_loss: 28.3227 - val_MinusLogProbMetric: 28.3227 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 638/1000
2023-09-29 06:47:32.319 
Epoch 638/1000 
	 loss: 26.9068, MinusLogProbMetric: 26.9068, val_loss: 28.3216, val_MinusLogProbMetric: 28.3216

Epoch 638: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9068 - MinusLogProbMetric: 26.9068 - val_loss: 28.3216 - val_MinusLogProbMetric: 28.3216 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 639/1000
2023-09-29 06:48:07.834 
Epoch 639/1000 
	 loss: 26.9158, MinusLogProbMetric: 26.9158, val_loss: 28.3098, val_MinusLogProbMetric: 28.3098

Epoch 639: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9158 - MinusLogProbMetric: 26.9158 - val_loss: 28.3098 - val_MinusLogProbMetric: 28.3098 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 640/1000
2023-09-29 06:48:43.539 
Epoch 640/1000 
	 loss: 26.9084, MinusLogProbMetric: 26.9084, val_loss: 28.2884, val_MinusLogProbMetric: 28.2884

Epoch 640: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9084 - MinusLogProbMetric: 26.9084 - val_loss: 28.2884 - val_MinusLogProbMetric: 28.2884 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 641/1000
2023-09-29 06:49:18.915 
Epoch 641/1000 
	 loss: 26.9096, MinusLogProbMetric: 26.9096, val_loss: 28.2983, val_MinusLogProbMetric: 28.2983

Epoch 641: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9096 - MinusLogProbMetric: 26.9096 - val_loss: 28.2983 - val_MinusLogProbMetric: 28.2983 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 642/1000
2023-09-29 06:49:54.130 
Epoch 642/1000 
	 loss: 26.9196, MinusLogProbMetric: 26.9196, val_loss: 28.2991, val_MinusLogProbMetric: 28.2991

Epoch 642: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9196 - MinusLogProbMetric: 26.9196 - val_loss: 28.2991 - val_MinusLogProbMetric: 28.2991 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 643/1000
2023-09-29 06:50:29.188 
Epoch 643/1000 
	 loss: 26.9093, MinusLogProbMetric: 26.9093, val_loss: 28.2841, val_MinusLogProbMetric: 28.2841

Epoch 643: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9093 - MinusLogProbMetric: 26.9093 - val_loss: 28.2841 - val_MinusLogProbMetric: 28.2841 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 644/1000
2023-09-29 06:51:04.787 
Epoch 644/1000 
	 loss: 26.9058, MinusLogProbMetric: 26.9058, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 644: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9058 - MinusLogProbMetric: 26.9058 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 645/1000
2023-09-29 06:51:40.259 
Epoch 645/1000 
	 loss: 26.9117, MinusLogProbMetric: 26.9117, val_loss: 28.2785, val_MinusLogProbMetric: 28.2785

Epoch 645: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.9117 - MinusLogProbMetric: 26.9117 - val_loss: 28.2785 - val_MinusLogProbMetric: 28.2785 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 646/1000
2023-09-29 06:52:15.877 
Epoch 646/1000 
	 loss: 26.9097, MinusLogProbMetric: 26.9097, val_loss: 28.2969, val_MinusLogProbMetric: 28.2969

Epoch 646: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.9097 - MinusLogProbMetric: 26.9097 - val_loss: 28.2969 - val_MinusLogProbMetric: 28.2969 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 647/1000
2023-09-29 06:52:51.163 
Epoch 647/1000 
	 loss: 26.8880, MinusLogProbMetric: 26.8880, val_loss: 28.2893, val_MinusLogProbMetric: 28.2893

Epoch 647: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8880 - MinusLogProbMetric: 26.8880 - val_loss: 28.2893 - val_MinusLogProbMetric: 28.2893 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 648/1000
2023-09-29 06:53:26.221 
Epoch 648/1000 
	 loss: 26.8879, MinusLogProbMetric: 26.8879, val_loss: 28.2840, val_MinusLogProbMetric: 28.2840

Epoch 648: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8879 - MinusLogProbMetric: 26.8879 - val_loss: 28.2840 - val_MinusLogProbMetric: 28.2840 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 649/1000
2023-09-29 06:54:01.769 
Epoch 649/1000 
	 loss: 26.8908, MinusLogProbMetric: 26.8908, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 649: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8908 - MinusLogProbMetric: 26.8908 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 650/1000
2023-09-29 06:54:37.034 
Epoch 650/1000 
	 loss: 26.8928, MinusLogProbMetric: 26.8928, val_loss: 28.2892, val_MinusLogProbMetric: 28.2892

Epoch 650: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8928 - MinusLogProbMetric: 26.8928 - val_loss: 28.2892 - val_MinusLogProbMetric: 28.2892 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 651/1000
2023-09-29 06:55:12.614 
Epoch 651/1000 
	 loss: 26.8938, MinusLogProbMetric: 26.8938, val_loss: 28.2819, val_MinusLogProbMetric: 28.2819

Epoch 651: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8938 - MinusLogProbMetric: 26.8938 - val_loss: 28.2819 - val_MinusLogProbMetric: 28.2819 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 652/1000
2023-09-29 06:55:48.078 
Epoch 652/1000 
	 loss: 26.8880, MinusLogProbMetric: 26.8880, val_loss: 28.2845, val_MinusLogProbMetric: 28.2845

Epoch 652: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8880 - MinusLogProbMetric: 26.8880 - val_loss: 28.2845 - val_MinusLogProbMetric: 28.2845 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 653/1000
2023-09-29 06:56:23.876 
Epoch 653/1000 
	 loss: 26.8878, MinusLogProbMetric: 26.8878, val_loss: 28.2778, val_MinusLogProbMetric: 28.2778

Epoch 653: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8878 - MinusLogProbMetric: 26.8878 - val_loss: 28.2778 - val_MinusLogProbMetric: 28.2778 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 654/1000
2023-09-29 06:56:59.589 
Epoch 654/1000 
	 loss: 26.8894, MinusLogProbMetric: 26.8894, val_loss: 28.2772, val_MinusLogProbMetric: 28.2772

Epoch 654: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8894 - MinusLogProbMetric: 26.8894 - val_loss: 28.2772 - val_MinusLogProbMetric: 28.2772 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 655/1000
2023-09-29 06:57:35.052 
Epoch 655/1000 
	 loss: 26.8924, MinusLogProbMetric: 26.8924, val_loss: 28.2830, val_MinusLogProbMetric: 28.2830

Epoch 655: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8924 - MinusLogProbMetric: 26.8924 - val_loss: 28.2830 - val_MinusLogProbMetric: 28.2830 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 656/1000
2023-09-29 06:58:10.578 
Epoch 656/1000 
	 loss: 26.8908, MinusLogProbMetric: 26.8908, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 656: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8908 - MinusLogProbMetric: 26.8908 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 657/1000
2023-09-29 06:58:46.520 
Epoch 657/1000 
	 loss: 26.8919, MinusLogProbMetric: 26.8919, val_loss: 28.2903, val_MinusLogProbMetric: 28.2903

Epoch 657: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8919 - MinusLogProbMetric: 26.8919 - val_loss: 28.2903 - val_MinusLogProbMetric: 28.2903 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 658/1000
2023-09-29 06:59:22.244 
Epoch 658/1000 
	 loss: 26.8932, MinusLogProbMetric: 26.8932, val_loss: 28.2838, val_MinusLogProbMetric: 28.2838

Epoch 658: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8932 - MinusLogProbMetric: 26.8932 - val_loss: 28.2838 - val_MinusLogProbMetric: 28.2838 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 659/1000
2023-09-29 06:59:57.498 
Epoch 659/1000 
	 loss: 26.8942, MinusLogProbMetric: 26.8942, val_loss: 28.2875, val_MinusLogProbMetric: 28.2875

Epoch 659: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8942 - MinusLogProbMetric: 26.8942 - val_loss: 28.2875 - val_MinusLogProbMetric: 28.2875 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 660/1000
2023-09-29 07:00:32.974 
Epoch 660/1000 
	 loss: 26.8916, MinusLogProbMetric: 26.8916, val_loss: 28.2836, val_MinusLogProbMetric: 28.2836

Epoch 660: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8916 - MinusLogProbMetric: 26.8916 - val_loss: 28.2836 - val_MinusLogProbMetric: 28.2836 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 661/1000
2023-09-29 07:01:09.365 
Epoch 661/1000 
	 loss: 26.8906, MinusLogProbMetric: 26.8906, val_loss: 28.2739, val_MinusLogProbMetric: 28.2739

Epoch 661: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8906 - MinusLogProbMetric: 26.8906 - val_loss: 28.2739 - val_MinusLogProbMetric: 28.2739 - lr: 3.1250e-05 - 36s/epoch - 186ms/step
Epoch 662/1000
2023-09-29 07:01:45.184 
Epoch 662/1000 
	 loss: 26.8903, MinusLogProbMetric: 26.8903, val_loss: 28.2824, val_MinusLogProbMetric: 28.2824

Epoch 662: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8903 - MinusLogProbMetric: 26.8903 - val_loss: 28.2824 - val_MinusLogProbMetric: 28.2824 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 663/1000
2023-09-29 07:02:16.331 
Epoch 663/1000 
	 loss: 26.8842, MinusLogProbMetric: 26.8842, val_loss: 28.2786, val_MinusLogProbMetric: 28.2786

Epoch 663: val_loss did not improve from 28.26874
196/196 - 31s - loss: 26.8842 - MinusLogProbMetric: 26.8842 - val_loss: 28.2786 - val_MinusLogProbMetric: 28.2786 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 664/1000
2023-09-29 07:02:48.439 
Epoch 664/1000 
	 loss: 26.8883, MinusLogProbMetric: 26.8883, val_loss: 28.2773, val_MinusLogProbMetric: 28.2773

Epoch 664: val_loss did not improve from 28.26874
196/196 - 32s - loss: 26.8883 - MinusLogProbMetric: 26.8883 - val_loss: 28.2773 - val_MinusLogProbMetric: 28.2773 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 665/1000
2023-09-29 07:03:23.482 
Epoch 665/1000 
	 loss: 26.8904, MinusLogProbMetric: 26.8904, val_loss: 28.2769, val_MinusLogProbMetric: 28.2769

Epoch 665: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8904 - MinusLogProbMetric: 26.8904 - val_loss: 28.2769 - val_MinusLogProbMetric: 28.2769 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 666/1000
2023-09-29 07:03:58.883 
Epoch 666/1000 
	 loss: 26.8818, MinusLogProbMetric: 26.8818, val_loss: 28.2719, val_MinusLogProbMetric: 28.2719

Epoch 666: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8818 - MinusLogProbMetric: 26.8818 - val_loss: 28.2719 - val_MinusLogProbMetric: 28.2719 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 667/1000
2023-09-29 07:04:34.131 
Epoch 667/1000 
	 loss: 26.8862, MinusLogProbMetric: 26.8862, val_loss: 28.2764, val_MinusLogProbMetric: 28.2764

Epoch 667: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8862 - MinusLogProbMetric: 26.8862 - val_loss: 28.2764 - val_MinusLogProbMetric: 28.2764 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 668/1000
2023-09-29 07:05:09.440 
Epoch 668/1000 
	 loss: 26.8869, MinusLogProbMetric: 26.8869, val_loss: 28.2812, val_MinusLogProbMetric: 28.2812

Epoch 668: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8869 - MinusLogProbMetric: 26.8869 - val_loss: 28.2812 - val_MinusLogProbMetric: 28.2812 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 669/1000
2023-09-29 07:05:44.938 
Epoch 669/1000 
	 loss: 26.8859, MinusLogProbMetric: 26.8859, val_loss: 28.2759, val_MinusLogProbMetric: 28.2759

Epoch 669: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8859 - MinusLogProbMetric: 26.8859 - val_loss: 28.2759 - val_MinusLogProbMetric: 28.2759 - lr: 3.1250e-05 - 35s/epoch - 181ms/step
Epoch 670/1000
2023-09-29 07:06:20.720 
Epoch 670/1000 
	 loss: 26.8856, MinusLogProbMetric: 26.8856, val_loss: 28.2754, val_MinusLogProbMetric: 28.2754

Epoch 670: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8856 - MinusLogProbMetric: 26.8856 - val_loss: 28.2754 - val_MinusLogProbMetric: 28.2754 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 671/1000
2023-09-29 07:06:56.249 
Epoch 671/1000 
	 loss: 26.8863, MinusLogProbMetric: 26.8863, val_loss: 28.2794, val_MinusLogProbMetric: 28.2794

Epoch 671: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8863 - MinusLogProbMetric: 26.8863 - val_loss: 28.2794 - val_MinusLogProbMetric: 28.2794 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 672/1000
2023-09-29 07:07:31.838 
Epoch 672/1000 
	 loss: 26.8863, MinusLogProbMetric: 26.8863, val_loss: 28.2798, val_MinusLogProbMetric: 28.2798

Epoch 672: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8863 - MinusLogProbMetric: 26.8863 - val_loss: 28.2798 - val_MinusLogProbMetric: 28.2798 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 673/1000
2023-09-29 07:08:06.675 
Epoch 673/1000 
	 loss: 26.8850, MinusLogProbMetric: 26.8850, val_loss: 28.2761, val_MinusLogProbMetric: 28.2761

Epoch 673: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8850 - MinusLogProbMetric: 26.8850 - val_loss: 28.2761 - val_MinusLogProbMetric: 28.2761 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 674/1000
2023-09-29 07:08:42.358 
Epoch 674/1000 
	 loss: 26.8835, MinusLogProbMetric: 26.8835, val_loss: 28.2778, val_MinusLogProbMetric: 28.2778

Epoch 674: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8835 - MinusLogProbMetric: 26.8835 - val_loss: 28.2778 - val_MinusLogProbMetric: 28.2778 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 675/1000
2023-09-29 07:09:17.915 
Epoch 675/1000 
	 loss: 26.8836, MinusLogProbMetric: 26.8836, val_loss: 28.2814, val_MinusLogProbMetric: 28.2814

Epoch 675: val_loss did not improve from 28.26874
196/196 - 36s - loss: 26.8836 - MinusLogProbMetric: 26.8836 - val_loss: 28.2814 - val_MinusLogProbMetric: 28.2814 - lr: 3.1250e-05 - 36s/epoch - 181ms/step
Epoch 676/1000
2023-09-29 07:09:51.764 
Epoch 676/1000 
	 loss: 26.8823, MinusLogProbMetric: 26.8823, val_loss: 28.2878, val_MinusLogProbMetric: 28.2878

Epoch 676: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.8823 - MinusLogProbMetric: 26.8823 - val_loss: 28.2878 - val_MinusLogProbMetric: 28.2878 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 677/1000
2023-09-29 07:10:26.678 
Epoch 677/1000 
	 loss: 26.8832, MinusLogProbMetric: 26.8832, val_loss: 28.2834, val_MinusLogProbMetric: 28.2834

Epoch 677: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8832 - MinusLogProbMetric: 26.8832 - val_loss: 28.2834 - val_MinusLogProbMetric: 28.2834 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 678/1000
2023-09-29 07:11:00.124 
Epoch 678/1000 
	 loss: 26.8848, MinusLogProbMetric: 26.8848, val_loss: 28.3002, val_MinusLogProbMetric: 28.3002

Epoch 678: val_loss did not improve from 28.26874
196/196 - 33s - loss: 26.8848 - MinusLogProbMetric: 26.8848 - val_loss: 28.3002 - val_MinusLogProbMetric: 28.3002 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 679/1000
2023-09-29 07:11:33.524 
Epoch 679/1000 
	 loss: 26.8873, MinusLogProbMetric: 26.8873, val_loss: 28.2829, val_MinusLogProbMetric: 28.2829

Epoch 679: val_loss did not improve from 28.26874
196/196 - 33s - loss: 26.8873 - MinusLogProbMetric: 26.8873 - val_loss: 28.2829 - val_MinusLogProbMetric: 28.2829 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 680/1000
2023-09-29 07:12:07.078 
Epoch 680/1000 
	 loss: 26.8857, MinusLogProbMetric: 26.8857, val_loss: 28.3043, val_MinusLogProbMetric: 28.3043

Epoch 680: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.8857 - MinusLogProbMetric: 26.8857 - val_loss: 28.3043 - val_MinusLogProbMetric: 28.3043 - lr: 3.1250e-05 - 34s/epoch - 171ms/step
Epoch 681/1000
2023-09-29 07:12:39.142 
Epoch 681/1000 
	 loss: 26.8861, MinusLogProbMetric: 26.8861, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 681: val_loss did not improve from 28.26874
196/196 - 32s - loss: 26.8861 - MinusLogProbMetric: 26.8861 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 682/1000
2023-09-29 07:13:13.351 
Epoch 682/1000 
	 loss: 26.8844, MinusLogProbMetric: 26.8844, val_loss: 28.2978, val_MinusLogProbMetric: 28.2978

Epoch 682: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.8844 - MinusLogProbMetric: 26.8844 - val_loss: 28.2978 - val_MinusLogProbMetric: 28.2978 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 683/1000
2023-09-29 07:13:47.993 
Epoch 683/1000 
	 loss: 26.8860, MinusLogProbMetric: 26.8860, val_loss: 28.2749, val_MinusLogProbMetric: 28.2749

Epoch 683: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8860 - MinusLogProbMetric: 26.8860 - val_loss: 28.2749 - val_MinusLogProbMetric: 28.2749 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 684/1000
2023-09-29 07:14:21.981 
Epoch 684/1000 
	 loss: 26.8821, MinusLogProbMetric: 26.8821, val_loss: 28.2761, val_MinusLogProbMetric: 28.2761

Epoch 684: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.8821 - MinusLogProbMetric: 26.8821 - val_loss: 28.2761 - val_MinusLogProbMetric: 28.2761 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 685/1000
2023-09-29 07:14:56.439 
Epoch 685/1000 
	 loss: 26.8828, MinusLogProbMetric: 26.8828, val_loss: 28.2905, val_MinusLogProbMetric: 28.2905

Epoch 685: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.8828 - MinusLogProbMetric: 26.8828 - val_loss: 28.2905 - val_MinusLogProbMetric: 28.2905 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 686/1000
2023-09-29 07:15:29.043 
Epoch 686/1000 
	 loss: 26.8827, MinusLogProbMetric: 26.8827, val_loss: 28.2790, val_MinusLogProbMetric: 28.2790

Epoch 686: val_loss did not improve from 28.26874
196/196 - 33s - loss: 26.8827 - MinusLogProbMetric: 26.8827 - val_loss: 28.2790 - val_MinusLogProbMetric: 28.2790 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 687/1000
2023-09-29 07:16:02.263 
Epoch 687/1000 
	 loss: 26.8815, MinusLogProbMetric: 26.8815, val_loss: 28.2799, val_MinusLogProbMetric: 28.2799

Epoch 687: val_loss did not improve from 28.26874
196/196 - 33s - loss: 26.8815 - MinusLogProbMetric: 26.8815 - val_loss: 28.2799 - val_MinusLogProbMetric: 28.2799 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 688/1000
2023-09-29 07:16:35.960 
Epoch 688/1000 
	 loss: 26.8832, MinusLogProbMetric: 26.8832, val_loss: 28.2832, val_MinusLogProbMetric: 28.2832

Epoch 688: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.8832 - MinusLogProbMetric: 26.8832 - val_loss: 28.2832 - val_MinusLogProbMetric: 28.2832 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 689/1000
2023-09-29 07:17:10.675 
Epoch 689/1000 
	 loss: 26.8808, MinusLogProbMetric: 26.8808, val_loss: 28.2855, val_MinusLogProbMetric: 28.2855

Epoch 689: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8808 - MinusLogProbMetric: 26.8808 - val_loss: 28.2855 - val_MinusLogProbMetric: 28.2855 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 690/1000
2023-09-29 07:17:48.579 
Epoch 690/1000 
	 loss: 26.8834, MinusLogProbMetric: 26.8834, val_loss: 28.2853, val_MinusLogProbMetric: 28.2853

Epoch 690: val_loss did not improve from 28.26874
196/196 - 38s - loss: 26.8834 - MinusLogProbMetric: 26.8834 - val_loss: 28.2853 - val_MinusLogProbMetric: 28.2853 - lr: 3.1250e-05 - 38s/epoch - 193ms/step
Epoch 691/1000
2023-09-29 07:18:23.859 
Epoch 691/1000 
	 loss: 26.8835, MinusLogProbMetric: 26.8835, val_loss: 28.2786, val_MinusLogProbMetric: 28.2786

Epoch 691: val_loss did not improve from 28.26874
196/196 - 35s - loss: 26.8835 - MinusLogProbMetric: 26.8835 - val_loss: 28.2786 - val_MinusLogProbMetric: 28.2786 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 692/1000
2023-09-29 07:18:57.298 
Epoch 692/1000 
	 loss: 26.8821, MinusLogProbMetric: 26.8821, val_loss: 28.3068, val_MinusLogProbMetric: 28.3068

Epoch 692: val_loss did not improve from 28.26874
196/196 - 33s - loss: 26.8821 - MinusLogProbMetric: 26.8821 - val_loss: 28.3068 - val_MinusLogProbMetric: 28.3068 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 693/1000
2023-09-29 07:19:27.413 
Epoch 693/1000 
	 loss: 26.8792, MinusLogProbMetric: 26.8792, val_loss: 28.2981, val_MinusLogProbMetric: 28.2981

Epoch 693: val_loss did not improve from 28.26874
196/196 - 30s - loss: 26.8792 - MinusLogProbMetric: 26.8792 - val_loss: 28.2981 - val_MinusLogProbMetric: 28.2981 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 694/1000
2023-09-29 07:20:01.022 
Epoch 694/1000 
	 loss: 26.8812, MinusLogProbMetric: 26.8812, val_loss: 28.2843, val_MinusLogProbMetric: 28.2843

Epoch 694: val_loss did not improve from 28.26874
196/196 - 34s - loss: 26.8812 - MinusLogProbMetric: 26.8812 - val_loss: 28.2843 - val_MinusLogProbMetric: 28.2843 - lr: 3.1250e-05 - 34s/epoch - 171ms/step
Epoch 695/1000
2023-09-29 07:20:30.178 
Epoch 695/1000 
	 loss: 26.8816, MinusLogProbMetric: 26.8816, val_loss: 28.2869, val_MinusLogProbMetric: 28.2869

Epoch 695: val_loss did not improve from 28.26874
196/196 - 29s - loss: 26.8816 - MinusLogProbMetric: 26.8816 - val_loss: 28.2869 - val_MinusLogProbMetric: 28.2869 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 696/1000
2023-09-29 07:21:01.780 
Epoch 696/1000 
	 loss: 26.8813, MinusLogProbMetric: 26.8813, val_loss: 28.2728, val_MinusLogProbMetric: 28.2728

Epoch 696: val_loss did not improve from 28.26874
Restoring model weights from the end of the best epoch: 596.
196/196 - 32s - loss: 26.8813 - MinusLogProbMetric: 26.8813 - val_loss: 28.2728 - val_MinusLogProbMetric: 28.2728 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 696: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7fc0c42612d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 14.893619492999278 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7fc0c4261f30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 8.20826219097944 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7fc0c42623b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 5.779372229008004 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7fc0c4261bd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 6.599610546021722 seconds.
Training succeeded with seed 187.
Model trained in 22891.84 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 36.52 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 36.73 s.
===========
Run 330/720 done in 22932.65 s.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

===========
Generating train data for run 334.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_334
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_94"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_95 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7fc069885000>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc06932d630>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc06932d630>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc069476920>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1db74ebf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1db74f160>, <keras.callbacks.ModelCheckpoint object at 0x7fc1db74f220>, <keras.callbacks.EarlyStopping object at 0x7fc1db74f490>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1db74f4c0>, <keras.callbacks.TerminateOnNaN object at 0x7fc1db74f100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_334/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 334/720 with hyperparameters:
timestamp = 2023-09-29 07:21:46.545680
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:23:54.955 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6667.7783, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 6667.7783 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 128s/epoch - 655ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 334.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_334
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_105"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_106 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7fc1db37a2c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc33c610d00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc33c610d00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc294611480>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1daec0e50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1daec13c0>, <keras.callbacks.ModelCheckpoint object at 0x7fc1daec1480>, <keras.callbacks.EarlyStopping object at 0x7fc1daec16f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1daec1720>, <keras.callbacks.TerminateOnNaN object at 0x7fc1daec1360>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_334/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 334/720 with hyperparameters:
timestamp = 2023-09-29 07:24:04.341263
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 90: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:26:33.321 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4126.8369, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 4126.8369 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 149s/epoch - 759ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 334.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_334
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_116"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_117 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7fc2940d96c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc23875a380>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc23875a380>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc34477c160>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc3447746a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc344774c10>, <keras.callbacks.ModelCheckpoint object at 0x7fc344774cd0>, <keras.callbacks.EarlyStopping object at 0x7fc344774f40>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc344774f70>, <keras.callbacks.TerminateOnNaN object at 0x7fc344774bb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_334/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 334/720 with hyperparameters:
timestamp = 2023-09-29 07:26:42.692336
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 24: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:28:53.759 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6345.9819, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 6345.9819 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 131s/epoch - 667ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 334.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_334
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_127"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_128 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7fc398461510>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb9b90100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb9b90100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc3a849ffa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1d898c610>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1d898cb80>, <keras.callbacks.ModelCheckpoint object at 0x7fc1d898cc40>, <keras.callbacks.EarlyStopping object at 0x7fc1d898ceb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1d898cee0>, <keras.callbacks.TerminateOnNaN object at 0x7fc1d898cb20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_334/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 334/720 with hyperparameters:
timestamp = 2023-09-29 07:29:02.862102
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 07:32:06.195 
Epoch 1/1000 
	 loss: 5158.4878, MinusLogProbMetric: 5158.4878, val_loss: 3540.9036, val_MinusLogProbMetric: 3540.9036

Epoch 1: val_loss improved from inf to 3540.90356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 184s - loss: 5158.4878 - MinusLogProbMetric: 5158.4878 - val_loss: 3540.9036 - val_MinusLogProbMetric: 3540.9036 - lr: 3.7037e-05 - 184s/epoch - 939ms/step
Epoch 2/1000
2023-09-29 07:33:07.246 
Epoch 2/1000 
	 loss: 2682.7629, MinusLogProbMetric: 2682.7629, val_loss: 2204.0359, val_MinusLogProbMetric: 2204.0359

Epoch 2: val_loss improved from 3540.90356 to 2204.03589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 61s - loss: 2682.7629 - MinusLogProbMetric: 2682.7629 - val_loss: 2204.0359 - val_MinusLogProbMetric: 2204.0359 - lr: 3.7037e-05 - 61s/epoch - 309ms/step
Epoch 3/1000
2023-09-29 07:34:08.809 
Epoch 3/1000 
	 loss: 1812.4540, MinusLogProbMetric: 1812.4540, val_loss: 1302.4557, val_MinusLogProbMetric: 1302.4557

Epoch 3: val_loss improved from 2204.03589 to 1302.45569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 62s - loss: 1812.4540 - MinusLogProbMetric: 1812.4540 - val_loss: 1302.4557 - val_MinusLogProbMetric: 1302.4557 - lr: 3.7037e-05 - 62s/epoch - 316ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 134: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 07:34:52.106 
Epoch 4/1000 
	 loss: nan, MinusLogProbMetric: 1349.0299, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 4: val_loss did not improve from 1302.45569
196/196 - 42s - loss: nan - MinusLogProbMetric: 1349.0299 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 42s/epoch - 216ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 334.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_334
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_138"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_139 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7fc00cfd53f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc44c187f40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc44c187f40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc39810fdc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc238275420>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc238274c70>, <keras.callbacks.ModelCheckpoint object at 0x7fc238274910>, <keras.callbacks.EarlyStopping object at 0x7fc238274490>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc238274550>, <keras.callbacks.TerminateOnNaN object at 0x7fc238274c40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 334/720 with hyperparameters:
timestamp = 2023-09-29 07:35:02.006960
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 07:37:55.809 
Epoch 1/1000 
	 loss: 1175.4800, MinusLogProbMetric: 1175.4800, val_loss: 1418.5496, val_MinusLogProbMetric: 1418.5496

Epoch 1: val_loss improved from inf to 1418.54956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 174s - loss: 1175.4800 - MinusLogProbMetric: 1175.4800 - val_loss: 1418.5496 - val_MinusLogProbMetric: 1418.5496 - lr: 1.2346e-05 - 174s/epoch - 889ms/step
Epoch 2/1000
2023-09-29 07:38:50.191 
Epoch 2/1000 
	 loss: 1140.8726, MinusLogProbMetric: 1140.8726, val_loss: 977.1577, val_MinusLogProbMetric: 977.1577

Epoch 2: val_loss improved from 1418.54956 to 977.15771, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 1140.8726 - MinusLogProbMetric: 1140.8726 - val_loss: 977.1577 - val_MinusLogProbMetric: 977.1577 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 3/1000
2023-09-29 07:39:42.738 
Epoch 3/1000 
	 loss: 954.5206, MinusLogProbMetric: 954.5206, val_loss: 972.1788, val_MinusLogProbMetric: 972.1788

Epoch 3: val_loss improved from 977.15771 to 972.17877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 52s - loss: 954.5206 - MinusLogProbMetric: 954.5206 - val_loss: 972.1788 - val_MinusLogProbMetric: 972.1788 - lr: 1.2346e-05 - 52s/epoch - 267ms/step
Epoch 4/1000
2023-09-29 07:40:38.517 
Epoch 4/1000 
	 loss: 888.6458, MinusLogProbMetric: 888.6458, val_loss: 992.5967, val_MinusLogProbMetric: 992.5967

Epoch 4: val_loss did not improve from 972.17877
196/196 - 55s - loss: 888.6458 - MinusLogProbMetric: 888.6458 - val_loss: 992.5967 - val_MinusLogProbMetric: 992.5967 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 5/1000
2023-09-29 07:41:31.308 
Epoch 5/1000 
	 loss: 795.7911, MinusLogProbMetric: 795.7911, val_loss: 776.8524, val_MinusLogProbMetric: 776.8524

Epoch 5: val_loss improved from 972.17877 to 776.85242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 795.7911 - MinusLogProbMetric: 795.7911 - val_loss: 776.8524 - val_MinusLogProbMetric: 776.8524 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 6/1000
2023-09-29 07:42:24.472 
Epoch 6/1000 
	 loss: 746.3207, MinusLogProbMetric: 746.3207, val_loss: 783.6906, val_MinusLogProbMetric: 783.6906

Epoch 6: val_loss did not improve from 776.85242
196/196 - 52s - loss: 746.3207 - MinusLogProbMetric: 746.3207 - val_loss: 783.6906 - val_MinusLogProbMetric: 783.6906 - lr: 1.2346e-05 - 52s/epoch - 267ms/step
Epoch 7/1000
2023-09-29 07:43:17.746 
Epoch 7/1000 
	 loss: 722.7848, MinusLogProbMetric: 722.7848, val_loss: 683.8863, val_MinusLogProbMetric: 683.8863

Epoch 7: val_loss improved from 776.85242 to 683.88629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 722.7848 - MinusLogProbMetric: 722.7848 - val_loss: 683.8863 - val_MinusLogProbMetric: 683.8863 - lr: 1.2346e-05 - 54s/epoch - 276ms/step
Epoch 8/1000
2023-09-29 07:44:14.127 
Epoch 8/1000 
	 loss: 701.0074, MinusLogProbMetric: 701.0074, val_loss: 770.1583, val_MinusLogProbMetric: 770.1583

Epoch 8: val_loss did not improve from 683.88629
196/196 - 55s - loss: 701.0074 - MinusLogProbMetric: 701.0074 - val_loss: 770.1583 - val_MinusLogProbMetric: 770.1583 - lr: 1.2346e-05 - 55s/epoch - 283ms/step
Epoch 9/1000
2023-09-29 07:45:10.018 
Epoch 9/1000 
	 loss: 689.5181, MinusLogProbMetric: 689.5181, val_loss: 669.2953, val_MinusLogProbMetric: 669.2953

Epoch 9: val_loss improved from 683.88629 to 669.29535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 57s - loss: 689.5181 - MinusLogProbMetric: 689.5181 - val_loss: 669.2953 - val_MinusLogProbMetric: 669.2953 - lr: 1.2346e-05 - 57s/epoch - 290ms/step
Epoch 10/1000
2023-09-29 07:46:06.157 
Epoch 10/1000 
	 loss: 660.9889, MinusLogProbMetric: 660.9889, val_loss: 666.8742, val_MinusLogProbMetric: 666.8742

Epoch 10: val_loss improved from 669.29535 to 666.87421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 660.9889 - MinusLogProbMetric: 660.9889 - val_loss: 666.8742 - val_MinusLogProbMetric: 666.8742 - lr: 1.2346e-05 - 56s/epoch - 286ms/step
Epoch 11/1000
2023-09-29 07:47:02.388 
Epoch 11/1000 
	 loss: 641.5248, MinusLogProbMetric: 641.5248, val_loss: 622.8131, val_MinusLogProbMetric: 622.8131

Epoch 11: val_loss improved from 666.87421 to 622.81311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 641.5248 - MinusLogProbMetric: 641.5248 - val_loss: 622.8131 - val_MinusLogProbMetric: 622.8131 - lr: 1.2346e-05 - 56s/epoch - 287ms/step
Epoch 12/1000
2023-09-29 07:47:58.103 
Epoch 12/1000 
	 loss: 613.9736, MinusLogProbMetric: 613.9736, val_loss: 594.9824, val_MinusLogProbMetric: 594.9824

Epoch 12: val_loss improved from 622.81311 to 594.98236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 613.9736 - MinusLogProbMetric: 613.9736 - val_loss: 594.9824 - val_MinusLogProbMetric: 594.9824 - lr: 1.2346e-05 - 56s/epoch - 285ms/step
Epoch 13/1000
2023-09-29 07:48:52.700 
Epoch 13/1000 
	 loss: 581.8812, MinusLogProbMetric: 581.8812, val_loss: 576.6100, val_MinusLogProbMetric: 576.6100

Epoch 13: val_loss improved from 594.98236 to 576.61005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 581.8812 - MinusLogProbMetric: 581.8812 - val_loss: 576.6100 - val_MinusLogProbMetric: 576.6100 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 14/1000
2023-09-29 07:49:48.928 
Epoch 14/1000 
	 loss: 577.6819, MinusLogProbMetric: 577.6819, val_loss: 564.9502, val_MinusLogProbMetric: 564.9502

Epoch 14: val_loss improved from 576.61005 to 564.95020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 577.6819 - MinusLogProbMetric: 577.6819 - val_loss: 564.9502 - val_MinusLogProbMetric: 564.9502 - lr: 1.2346e-05 - 56s/epoch - 287ms/step
Epoch 15/1000
2023-09-29 07:50:43.446 
Epoch 15/1000 
	 loss: 557.3168, MinusLogProbMetric: 557.3168, val_loss: 545.6075, val_MinusLogProbMetric: 545.6075

Epoch 15: val_loss improved from 564.95020 to 545.60754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 55s - loss: 557.3168 - MinusLogProbMetric: 557.3168 - val_loss: 545.6075 - val_MinusLogProbMetric: 545.6075 - lr: 1.2346e-05 - 55s/epoch - 278ms/step
Epoch 16/1000
2023-09-29 07:51:37.754 
Epoch 16/1000 
	 loss: 628.1974, MinusLogProbMetric: 628.1974, val_loss: 608.0204, val_MinusLogProbMetric: 608.0204

Epoch 16: val_loss did not improve from 545.60754
196/196 - 53s - loss: 628.1974 - MinusLogProbMetric: 628.1974 - val_loss: 608.0204 - val_MinusLogProbMetric: 608.0204 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 17/1000
2023-09-29 07:52:31.968 
Epoch 17/1000 
	 loss: 626.5333, MinusLogProbMetric: 626.5333, val_loss: 637.1700, val_MinusLogProbMetric: 637.1700

Epoch 17: val_loss did not improve from 545.60754
196/196 - 54s - loss: 626.5333 - MinusLogProbMetric: 626.5333 - val_loss: 637.1700 - val_MinusLogProbMetric: 637.1700 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 18/1000
2023-09-29 07:53:29.062 
Epoch 18/1000 
	 loss: 572.4958, MinusLogProbMetric: 572.4958, val_loss: 559.4952, val_MinusLogProbMetric: 559.4952

Epoch 18: val_loss did not improve from 545.60754
196/196 - 57s - loss: 572.4958 - MinusLogProbMetric: 572.4958 - val_loss: 559.4952 - val_MinusLogProbMetric: 559.4952 - lr: 1.2346e-05 - 57s/epoch - 291ms/step
Epoch 19/1000
2023-09-29 07:54:22.474 
Epoch 19/1000 
	 loss: 539.2519, MinusLogProbMetric: 539.2519, val_loss: 516.6379, val_MinusLogProbMetric: 516.6379

Epoch 19: val_loss improved from 545.60754 to 516.63788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 539.2519 - MinusLogProbMetric: 539.2519 - val_loss: 516.6379 - val_MinusLogProbMetric: 516.6379 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 20/1000
2023-09-29 07:55:16.487 
Epoch 20/1000 
	 loss: 509.5822, MinusLogProbMetric: 509.5822, val_loss: 495.3860, val_MinusLogProbMetric: 495.3860

Epoch 20: val_loss improved from 516.63788 to 495.38599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 509.5822 - MinusLogProbMetric: 509.5822 - val_loss: 495.3860 - val_MinusLogProbMetric: 495.3860 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 21/1000
2023-09-29 07:56:09.198 
Epoch 21/1000 
	 loss: 484.9016, MinusLogProbMetric: 484.9016, val_loss: 474.4614, val_MinusLogProbMetric: 474.4614

Epoch 21: val_loss improved from 495.38599 to 474.46136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 53s - loss: 484.9016 - MinusLogProbMetric: 484.9016 - val_loss: 474.4614 - val_MinusLogProbMetric: 474.4614 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 22/1000
2023-09-29 07:57:02.919 
Epoch 22/1000 
	 loss: 468.5117, MinusLogProbMetric: 468.5117, val_loss: 461.4862, val_MinusLogProbMetric: 461.4862

Epoch 22: val_loss improved from 474.46136 to 461.48621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 468.5117 - MinusLogProbMetric: 468.5117 - val_loss: 461.4862 - val_MinusLogProbMetric: 461.4862 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 23/1000
2023-09-29 07:57:59.868 
Epoch 23/1000 
	 loss: 461.6781, MinusLogProbMetric: 461.6781, val_loss: 453.7307, val_MinusLogProbMetric: 453.7307

Epoch 23: val_loss improved from 461.48621 to 453.73074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 57s - loss: 461.6781 - MinusLogProbMetric: 461.6781 - val_loss: 453.7307 - val_MinusLogProbMetric: 453.7307 - lr: 1.2346e-05 - 57s/epoch - 290ms/step
Epoch 24/1000
2023-09-29 07:58:53.788 
Epoch 24/1000 
	 loss: 449.0602, MinusLogProbMetric: 449.0602, val_loss: 445.1323, val_MinusLogProbMetric: 445.1323

Epoch 24: val_loss improved from 453.73074 to 445.13226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 449.0602 - MinusLogProbMetric: 449.0602 - val_loss: 445.1323 - val_MinusLogProbMetric: 445.1323 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 25/1000
2023-09-29 07:59:50.912 
Epoch 25/1000 
	 loss: 439.9181, MinusLogProbMetric: 439.9181, val_loss: 435.8395, val_MinusLogProbMetric: 435.8395

Epoch 25: val_loss improved from 445.13226 to 435.83948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 57s - loss: 439.9181 - MinusLogProbMetric: 439.9181 - val_loss: 435.8395 - val_MinusLogProbMetric: 435.8395 - lr: 1.2346e-05 - 57s/epoch - 292ms/step
Epoch 26/1000
2023-09-29 08:00:45.156 
Epoch 26/1000 
	 loss: 440.8980, MinusLogProbMetric: 440.8980, val_loss: 476.0682, val_MinusLogProbMetric: 476.0682

Epoch 26: val_loss did not improve from 435.83948
196/196 - 53s - loss: 440.8980 - MinusLogProbMetric: 440.8980 - val_loss: 476.0682 - val_MinusLogProbMetric: 476.0682 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 27/1000
2023-09-29 08:01:40.131 
Epoch 27/1000 
	 loss: 448.5861, MinusLogProbMetric: 448.5861, val_loss: 436.2277, val_MinusLogProbMetric: 436.2277

Epoch 27: val_loss did not improve from 435.83948
196/196 - 55s - loss: 448.5861 - MinusLogProbMetric: 448.5861 - val_loss: 436.2277 - val_MinusLogProbMetric: 436.2277 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 28/1000
2023-09-29 08:02:33.249 
Epoch 28/1000 
	 loss: 428.2442, MinusLogProbMetric: 428.2442, val_loss: 423.6007, val_MinusLogProbMetric: 423.6007

Epoch 28: val_loss improved from 435.83948 to 423.60068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 428.2442 - MinusLogProbMetric: 428.2442 - val_loss: 423.6007 - val_MinusLogProbMetric: 423.6007 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 29/1000
2023-09-29 08:03:26.951 
Epoch 29/1000 
	 loss: 421.2747, MinusLogProbMetric: 421.2747, val_loss: 417.5642, val_MinusLogProbMetric: 417.5642

Epoch 29: val_loss improved from 423.60068 to 417.56424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 421.2747 - MinusLogProbMetric: 421.2747 - val_loss: 417.5642 - val_MinusLogProbMetric: 417.5642 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 30/1000
2023-09-29 08:04:21.793 
Epoch 30/1000 
	 loss: 412.5370, MinusLogProbMetric: 412.5370, val_loss: 408.0983, val_MinusLogProbMetric: 408.0983

Epoch 30: val_loss improved from 417.56424 to 408.09827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 55s - loss: 412.5370 - MinusLogProbMetric: 412.5370 - val_loss: 408.0983 - val_MinusLogProbMetric: 408.0983 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 31/1000
2023-09-29 08:05:17.382 
Epoch 31/1000 
	 loss: 405.5470, MinusLogProbMetric: 405.5470, val_loss: 402.2018, val_MinusLogProbMetric: 402.2018

Epoch 31: val_loss improved from 408.09827 to 402.20175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 405.5470 - MinusLogProbMetric: 405.5470 - val_loss: 402.2018 - val_MinusLogProbMetric: 402.2018 - lr: 1.2346e-05 - 56s/epoch - 283ms/step
Epoch 32/1000
2023-09-29 08:06:13.652 
Epoch 32/1000 
	 loss: 401.9850, MinusLogProbMetric: 401.9850, val_loss: 397.6028, val_MinusLogProbMetric: 397.6028

Epoch 32: val_loss improved from 402.20175 to 397.60281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 401.9850 - MinusLogProbMetric: 401.9850 - val_loss: 397.6028 - val_MinusLogProbMetric: 397.6028 - lr: 1.2346e-05 - 56s/epoch - 287ms/step
Epoch 33/1000
2023-09-29 08:07:09.576 
Epoch 33/1000 
	 loss: 394.3336, MinusLogProbMetric: 394.3336, val_loss: 392.0340, val_MinusLogProbMetric: 392.0340

Epoch 33: val_loss improved from 397.60281 to 392.03400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 394.3336 - MinusLogProbMetric: 394.3336 - val_loss: 392.0340 - val_MinusLogProbMetric: 392.0340 - lr: 1.2346e-05 - 56s/epoch - 286ms/step
Epoch 34/1000
2023-09-29 08:08:05.340 
Epoch 34/1000 
	 loss: 392.0732, MinusLogProbMetric: 392.0732, val_loss: 398.9626, val_MinusLogProbMetric: 398.9626

Epoch 34: val_loss did not improve from 392.03400
196/196 - 55s - loss: 392.0732 - MinusLogProbMetric: 392.0732 - val_loss: 398.9626 - val_MinusLogProbMetric: 398.9626 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 35/1000
2023-09-29 08:08:59.569 
Epoch 35/1000 
	 loss: 389.8419, MinusLogProbMetric: 389.8419, val_loss: 382.7479, val_MinusLogProbMetric: 382.7479

Epoch 35: val_loss improved from 392.03400 to 382.74792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 55s - loss: 389.8419 - MinusLogProbMetric: 389.8419 - val_loss: 382.7479 - val_MinusLogProbMetric: 382.7479 - lr: 1.2346e-05 - 55s/epoch - 281ms/step
Epoch 36/1000
2023-09-29 08:09:53.132 
Epoch 36/1000 
	 loss: 380.9991, MinusLogProbMetric: 380.9991, val_loss: 378.5265, val_MinusLogProbMetric: 378.5265

Epoch 36: val_loss improved from 382.74792 to 378.52652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 380.9991 - MinusLogProbMetric: 380.9991 - val_loss: 378.5265 - val_MinusLogProbMetric: 378.5265 - lr: 1.2346e-05 - 54s/epoch - 273ms/step
Epoch 37/1000
2023-09-29 08:10:47.908 
Epoch 37/1000 
	 loss: 376.2535, MinusLogProbMetric: 376.2535, val_loss: 373.9395, val_MinusLogProbMetric: 373.9395

Epoch 37: val_loss improved from 378.52652 to 373.93945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 55s - loss: 376.2535 - MinusLogProbMetric: 376.2535 - val_loss: 373.9395 - val_MinusLogProbMetric: 373.9395 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 38/1000
2023-09-29 08:11:44.180 
Epoch 38/1000 
	 loss: 373.3250, MinusLogProbMetric: 373.3250, val_loss: 371.3901, val_MinusLogProbMetric: 371.3901

Epoch 38: val_loss improved from 373.93945 to 371.39014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 373.3250 - MinusLogProbMetric: 373.3250 - val_loss: 371.3901 - val_MinusLogProbMetric: 371.3901 - lr: 1.2346e-05 - 56s/epoch - 287ms/step
Epoch 39/1000
2023-09-29 08:12:37.073 
Epoch 39/1000 
	 loss: 373.6919, MinusLogProbMetric: 373.6919, val_loss: 370.7853, val_MinusLogProbMetric: 370.7853

Epoch 39: val_loss improved from 371.39014 to 370.78528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 53s - loss: 373.6919 - MinusLogProbMetric: 373.6919 - val_loss: 370.7853 - val_MinusLogProbMetric: 370.7853 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 40/1000
2023-09-29 08:13:30.800 
Epoch 40/1000 
	 loss: 372.9234, MinusLogProbMetric: 372.9234, val_loss: 370.6268, val_MinusLogProbMetric: 370.6268

Epoch 40: val_loss improved from 370.78528 to 370.62683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 372.9234 - MinusLogProbMetric: 372.9234 - val_loss: 370.6268 - val_MinusLogProbMetric: 370.6268 - lr: 1.2346e-05 - 54s/epoch - 274ms/step
Epoch 41/1000
2023-09-29 08:14:23.992 
Epoch 41/1000 
	 loss: 422.0374, MinusLogProbMetric: 422.0374, val_loss: 414.2817, val_MinusLogProbMetric: 414.2817

Epoch 41: val_loss did not improve from 370.62683
196/196 - 52s - loss: 422.0374 - MinusLogProbMetric: 422.0374 - val_loss: 414.2817 - val_MinusLogProbMetric: 414.2817 - lr: 1.2346e-05 - 52s/epoch - 268ms/step
Epoch 42/1000
2023-09-29 08:15:18.851 
Epoch 42/1000 
	 loss: 404.9059, MinusLogProbMetric: 404.9059, val_loss: 392.1831, val_MinusLogProbMetric: 392.1831

Epoch 42: val_loss did not improve from 370.62683
196/196 - 55s - loss: 404.9059 - MinusLogProbMetric: 404.9059 - val_loss: 392.1831 - val_MinusLogProbMetric: 392.1831 - lr: 1.2346e-05 - 55s/epoch - 280ms/step
Epoch 43/1000
2023-09-29 08:16:11.667 
Epoch 43/1000 
	 loss: 384.2290, MinusLogProbMetric: 384.2290, val_loss: 378.4746, val_MinusLogProbMetric: 378.4746

Epoch 43: val_loss did not improve from 370.62683
196/196 - 53s - loss: 384.2290 - MinusLogProbMetric: 384.2290 - val_loss: 378.4746 - val_MinusLogProbMetric: 378.4746 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 44/1000
2023-09-29 08:17:04.401 
Epoch 44/1000 
	 loss: 374.3452, MinusLogProbMetric: 374.3452, val_loss: 370.9564, val_MinusLogProbMetric: 370.9564

Epoch 44: val_loss did not improve from 370.62683
196/196 - 53s - loss: 374.3452 - MinusLogProbMetric: 374.3452 - val_loss: 370.9564 - val_MinusLogProbMetric: 370.9564 - lr: 1.2346e-05 - 53s/epoch - 269ms/step
Epoch 45/1000
2023-09-29 08:17:56.477 
Epoch 45/1000 
	 loss: 368.8746, MinusLogProbMetric: 368.8746, val_loss: 367.1236, val_MinusLogProbMetric: 367.1236

Epoch 45: val_loss improved from 370.62683 to 367.12360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 53s - loss: 368.8746 - MinusLogProbMetric: 368.8746 - val_loss: 367.1236 - val_MinusLogProbMetric: 367.1236 - lr: 1.2346e-05 - 53s/epoch - 270ms/step
Epoch 46/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 110: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 08:18:28.188 
Epoch 46/1000 
	 loss: nan, MinusLogProbMetric: 365.1831, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 46: val_loss did not improve from 367.12360
196/196 - 31s - loss: nan - MinusLogProbMetric: 365.1831 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 31s/epoch - 158ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 334.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_334/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_334
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_149"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_150 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7fc49420fa00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc1d878a920>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc1d878a920>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc4941fd840>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc22a44a9b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc22a4487c0>, <keras.callbacks.ModelCheckpoint object at 0x7fc22a449510>, <keras.callbacks.EarlyStopping object at 0x7fc22a44b4c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc22a44b850>, <keras.callbacks.TerminateOnNaN object at 0x7fc22a44bd00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 334/720 with hyperparameters:
timestamp = 2023-09-29 08:18:37.630803
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 08:21:34.889 
Epoch 1/1000 
	 loss: 347.7325, MinusLogProbMetric: 347.7325, val_loss: 333.3756, val_MinusLogProbMetric: 333.3756

Epoch 1: val_loss improved from inf to 333.37558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 178s - loss: 347.7325 - MinusLogProbMetric: 347.7325 - val_loss: 333.3756 - val_MinusLogProbMetric: 333.3756 - lr: 4.1152e-06 - 178s/epoch - 908ms/step
Epoch 2/1000
2023-09-29 08:22:32.556 
Epoch 2/1000 
	 loss: 325.2466, MinusLogProbMetric: 325.2466, val_loss: 313.7150, val_MinusLogProbMetric: 313.7150

Epoch 2: val_loss improved from 333.37558 to 313.71503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 57s - loss: 325.2466 - MinusLogProbMetric: 325.2466 - val_loss: 313.7150 - val_MinusLogProbMetric: 313.7150 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 3/1000
2023-09-29 08:23:26.624 
Epoch 3/1000 
	 loss: 337.6185, MinusLogProbMetric: 337.6185, val_loss: 319.7444, val_MinusLogProbMetric: 319.7444

Epoch 3: val_loss did not improve from 313.71503
196/196 - 53s - loss: 337.6185 - MinusLogProbMetric: 337.6185 - val_loss: 319.7444 - val_MinusLogProbMetric: 319.7444 - lr: 4.1152e-06 - 53s/epoch - 272ms/step
Epoch 4/1000
2023-09-29 08:24:20.867 
Epoch 4/1000 
	 loss: 308.2708, MinusLogProbMetric: 308.2708, val_loss: 300.2622, val_MinusLogProbMetric: 300.2622

Epoch 4: val_loss improved from 313.71503 to 300.26218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 55s - loss: 308.2708 - MinusLogProbMetric: 308.2708 - val_loss: 300.2622 - val_MinusLogProbMetric: 300.2622 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 5/1000
2023-09-29 08:25:14.874 
Epoch 5/1000 
	 loss: 292.1340, MinusLogProbMetric: 292.1340, val_loss: 284.6640, val_MinusLogProbMetric: 284.6640

Epoch 5: val_loss improved from 300.26218 to 284.66397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 54s - loss: 292.1340 - MinusLogProbMetric: 292.1340 - val_loss: 284.6640 - val_MinusLogProbMetric: 284.6640 - lr: 4.1152e-06 - 54s/epoch - 276ms/step
Epoch 6/1000
2023-09-29 08:26:13.775 
Epoch 6/1000 
	 loss: 280.0208, MinusLogProbMetric: 280.0208, val_loss: 275.4889, val_MinusLogProbMetric: 275.4889

Epoch 6: val_loss improved from 284.66397 to 275.48892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 59s - loss: 280.0208 - MinusLogProbMetric: 280.0208 - val_loss: 275.4889 - val_MinusLogProbMetric: 275.4889 - lr: 4.1152e-06 - 59s/epoch - 302ms/step
Epoch 7/1000
2023-09-29 08:27:13.601 
Epoch 7/1000 
	 loss: 270.7789, MinusLogProbMetric: 270.7789, val_loss: 266.7770, val_MinusLogProbMetric: 266.7770

Epoch 7: val_loss improved from 275.48892 to 266.77704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 60s - loss: 270.7789 - MinusLogProbMetric: 270.7789 - val_loss: 266.7770 - val_MinusLogProbMetric: 266.7770 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 8/1000
2023-09-29 08:28:19.164 
Epoch 8/1000 
	 loss: 269.1017, MinusLogProbMetric: 269.1017, val_loss: 267.8369, val_MinusLogProbMetric: 267.8369

Epoch 8: val_loss did not improve from 266.77704
196/196 - 64s - loss: 269.1017 - MinusLogProbMetric: 269.1017 - val_loss: 267.8369 - val_MinusLogProbMetric: 267.8369 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 9/1000
2023-09-29 08:29:19.484 
Epoch 9/1000 
	 loss: 261.4786, MinusLogProbMetric: 261.4786, val_loss: 258.6348, val_MinusLogProbMetric: 258.6348

Epoch 9: val_loss improved from 266.77704 to 258.63483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 61s - loss: 261.4786 - MinusLogProbMetric: 261.4786 - val_loss: 258.6348 - val_MinusLogProbMetric: 258.6348 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 10/1000
2023-09-29 08:30:17.198 
Epoch 10/1000 
	 loss: 255.6831, MinusLogProbMetric: 255.6831, val_loss: 273.7185, val_MinusLogProbMetric: 273.7185

Epoch 10: val_loss did not improve from 258.63483
196/196 - 57s - loss: 255.6831 - MinusLogProbMetric: 255.6831 - val_loss: 273.7185 - val_MinusLogProbMetric: 273.7185 - lr: 4.1152e-06 - 57s/epoch - 290ms/step
Epoch 11/1000
2023-09-29 08:31:18.457 
Epoch 11/1000 
	 loss: 256.6767, MinusLogProbMetric: 256.6767, val_loss: 250.2827, val_MinusLogProbMetric: 250.2827

Epoch 11: val_loss improved from 258.63483 to 250.28267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 62s - loss: 256.6767 - MinusLogProbMetric: 256.6767 - val_loss: 250.2827 - val_MinusLogProbMetric: 250.2827 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 12/1000
2023-09-29 08:32:23.120 
Epoch 12/1000 
	 loss: 270.0176, MinusLogProbMetric: 270.0176, val_loss: 321.4271, val_MinusLogProbMetric: 321.4271

Epoch 12: val_loss did not improve from 250.28267
196/196 - 64s - loss: 270.0176 - MinusLogProbMetric: 270.0176 - val_loss: 321.4271 - val_MinusLogProbMetric: 321.4271 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 13/1000
2023-09-29 08:33:20.730 
Epoch 13/1000 
	 loss: 399.2229, MinusLogProbMetric: 399.2229, val_loss: 349.3506, val_MinusLogProbMetric: 349.3506

Epoch 13: val_loss did not improve from 250.28267
196/196 - 58s - loss: 399.2229 - MinusLogProbMetric: 399.2229 - val_loss: 349.3506 - val_MinusLogProbMetric: 349.3506 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 14/1000
2023-09-29 08:34:18.376 
Epoch 14/1000 
	 loss: 334.3623, MinusLogProbMetric: 334.3623, val_loss: 322.3867, val_MinusLogProbMetric: 322.3867

Epoch 14: val_loss did not improve from 250.28267
196/196 - 58s - loss: 334.3623 - MinusLogProbMetric: 334.3623 - val_loss: 322.3867 - val_MinusLogProbMetric: 322.3867 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 15/1000
2023-09-29 08:35:22.715 
Epoch 15/1000 
	 loss: 312.6074, MinusLogProbMetric: 312.6074, val_loss: 304.7072, val_MinusLogProbMetric: 304.7072

Epoch 15: val_loss did not improve from 250.28267
196/196 - 64s - loss: 312.6074 - MinusLogProbMetric: 312.6074 - val_loss: 304.7072 - val_MinusLogProbMetric: 304.7072 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 16/1000
2023-09-29 08:36:26.138 
Epoch 16/1000 
	 loss: 300.9485, MinusLogProbMetric: 300.9485, val_loss: 294.4778, val_MinusLogProbMetric: 294.4778

Epoch 16: val_loss did not improve from 250.28267
196/196 - 63s - loss: 300.9485 - MinusLogProbMetric: 300.9485 - val_loss: 294.4778 - val_MinusLogProbMetric: 294.4778 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 17/1000
2023-09-29 08:37:26.396 
Epoch 17/1000 
	 loss: 289.1644, MinusLogProbMetric: 289.1644, val_loss: 284.6218, val_MinusLogProbMetric: 284.6218

Epoch 17: val_loss did not improve from 250.28267
196/196 - 60s - loss: 289.1644 - MinusLogProbMetric: 289.1644 - val_loss: 284.6218 - val_MinusLogProbMetric: 284.6218 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 18/1000
2023-09-29 08:38:23.868 
Epoch 18/1000 
	 loss: 289.9181, MinusLogProbMetric: 289.9181, val_loss: 291.8303, val_MinusLogProbMetric: 291.8303

Epoch 18: val_loss did not improve from 250.28267
196/196 - 57s - loss: 289.9181 - MinusLogProbMetric: 289.9181 - val_loss: 291.8303 - val_MinusLogProbMetric: 291.8303 - lr: 4.1152e-06 - 57s/epoch - 293ms/step
Epoch 19/1000
2023-09-29 08:39:26.741 
Epoch 19/1000 
	 loss: 286.1715, MinusLogProbMetric: 286.1715, val_loss: 283.4481, val_MinusLogProbMetric: 283.4481

Epoch 19: val_loss did not improve from 250.28267
196/196 - 63s - loss: 286.1715 - MinusLogProbMetric: 286.1715 - val_loss: 283.4481 - val_MinusLogProbMetric: 283.4481 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 20/1000
2023-09-29 08:40:31.838 
Epoch 20/1000 
	 loss: 275.5493, MinusLogProbMetric: 275.5493, val_loss: 272.4485, val_MinusLogProbMetric: 272.4485

Epoch 20: val_loss did not improve from 250.28267
196/196 - 65s - loss: 275.5493 - MinusLogProbMetric: 275.5493 - val_loss: 272.4485 - val_MinusLogProbMetric: 272.4485 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 21/1000
2023-09-29 08:41:31.458 
Epoch 21/1000 
	 loss: 269.3127, MinusLogProbMetric: 269.3127, val_loss: 268.3087, val_MinusLogProbMetric: 268.3087

Epoch 21: val_loss did not improve from 250.28267
196/196 - 60s - loss: 269.3127 - MinusLogProbMetric: 269.3127 - val_loss: 268.3087 - val_MinusLogProbMetric: 268.3087 - lr: 4.1152e-06 - 60s/epoch - 304ms/step
Epoch 22/1000
2023-09-29 08:42:33.228 
Epoch 22/1000 
	 loss: 265.6655, MinusLogProbMetric: 265.6655, val_loss: 263.2484, val_MinusLogProbMetric: 263.2484

Epoch 22: val_loss did not improve from 250.28267
196/196 - 62s - loss: 265.6655 - MinusLogProbMetric: 265.6655 - val_loss: 263.2484 - val_MinusLogProbMetric: 263.2484 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 23/1000
2023-09-29 08:43:37.236 
Epoch 23/1000 
	 loss: 261.1342, MinusLogProbMetric: 261.1342, val_loss: 259.1270, val_MinusLogProbMetric: 259.1270

Epoch 23: val_loss did not improve from 250.28267
196/196 - 64s - loss: 261.1342 - MinusLogProbMetric: 261.1342 - val_loss: 259.1270 - val_MinusLogProbMetric: 259.1270 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 24/1000
2023-09-29 08:44:40.098 
Epoch 24/1000 
	 loss: 260.2728, MinusLogProbMetric: 260.2728, val_loss: 258.4944, val_MinusLogProbMetric: 258.4944

Epoch 24: val_loss did not improve from 250.28267
196/196 - 63s - loss: 260.2728 - MinusLogProbMetric: 260.2728 - val_loss: 258.4944 - val_MinusLogProbMetric: 258.4944 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 25/1000
2023-09-29 08:45:38.296 
Epoch 25/1000 
	 loss: 254.5573, MinusLogProbMetric: 254.5573, val_loss: 252.2375, val_MinusLogProbMetric: 252.2375

Epoch 25: val_loss did not improve from 250.28267
196/196 - 58s - loss: 254.5573 - MinusLogProbMetric: 254.5573 - val_loss: 252.2375 - val_MinusLogProbMetric: 252.2375 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 26/1000
2023-09-29 08:46:37.976 
Epoch 26/1000 
	 loss: 253.3460, MinusLogProbMetric: 253.3460, val_loss: 253.4543, val_MinusLogProbMetric: 253.4543

Epoch 26: val_loss did not improve from 250.28267
196/196 - 60s - loss: 253.3460 - MinusLogProbMetric: 253.3460 - val_loss: 253.4543 - val_MinusLogProbMetric: 253.4543 - lr: 4.1152e-06 - 60s/epoch - 304ms/step
Epoch 27/1000
2023-09-29 08:47:45.205 
Epoch 27/1000 
	 loss: 249.8601, MinusLogProbMetric: 249.8601, val_loss: 247.7738, val_MinusLogProbMetric: 247.7738

Epoch 27: val_loss improved from 250.28267 to 247.77382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 249.8601 - MinusLogProbMetric: 249.8601 - val_loss: 247.7738 - val_MinusLogProbMetric: 247.7738 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 28/1000
2023-09-29 08:48:52.275 
Epoch 28/1000 
	 loss: 246.5344, MinusLogProbMetric: 246.5344, val_loss: 245.4263, val_MinusLogProbMetric: 245.4263

Epoch 28: val_loss improved from 247.77382 to 245.42627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 246.5344 - MinusLogProbMetric: 246.5344 - val_loss: 245.4263 - val_MinusLogProbMetric: 245.4263 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 29/1000
2023-09-29 08:50:00.352 
Epoch 29/1000 
	 loss: 250.0740, MinusLogProbMetric: 250.0740, val_loss: 249.3280, val_MinusLogProbMetric: 249.3280

Epoch 29: val_loss did not improve from 245.42627
196/196 - 67s - loss: 250.0740 - MinusLogProbMetric: 250.0740 - val_loss: 249.3280 - val_MinusLogProbMetric: 249.3280 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 30/1000
2023-09-29 08:51:01.750 
Epoch 30/1000 
	 loss: 248.3035, MinusLogProbMetric: 248.3035, val_loss: 251.1244, val_MinusLogProbMetric: 251.1244

Epoch 30: val_loss did not improve from 245.42627
196/196 - 61s - loss: 248.3035 - MinusLogProbMetric: 248.3035 - val_loss: 251.1244 - val_MinusLogProbMetric: 251.1244 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 31/1000
2023-09-29 08:52:10.195 
Epoch 31/1000 
	 loss: 246.4283, MinusLogProbMetric: 246.4283, val_loss: 243.1606, val_MinusLogProbMetric: 243.1606

Epoch 31: val_loss improved from 245.42627 to 243.16063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 246.4283 - MinusLogProbMetric: 246.4283 - val_loss: 243.1606 - val_MinusLogProbMetric: 243.1606 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 32/1000
2023-09-29 08:53:19.892 
Epoch 32/1000 
	 loss: 239.6114, MinusLogProbMetric: 239.6114, val_loss: 237.2859, val_MinusLogProbMetric: 237.2859

Epoch 32: val_loss improved from 243.16063 to 237.28592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 239.6114 - MinusLogProbMetric: 239.6114 - val_loss: 237.2859 - val_MinusLogProbMetric: 237.2859 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 33/1000
2023-09-29 08:54:29.674 
Epoch 33/1000 
	 loss: 240.9435, MinusLogProbMetric: 240.9435, val_loss: 248.2077, val_MinusLogProbMetric: 248.2077

Epoch 33: val_loss did not improve from 237.28592
196/196 - 68s - loss: 240.9435 - MinusLogProbMetric: 240.9435 - val_loss: 248.2077 - val_MinusLogProbMetric: 248.2077 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 34/1000
2023-09-29 08:55:37.861 
Epoch 34/1000 
	 loss: 255.6517, MinusLogProbMetric: 255.6517, val_loss: 296.5205, val_MinusLogProbMetric: 296.5205

Epoch 34: val_loss did not improve from 237.28592
196/196 - 68s - loss: 255.6517 - MinusLogProbMetric: 255.6517 - val_loss: 296.5205 - val_MinusLogProbMetric: 296.5205 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 35/1000
2023-09-29 08:56:46.588 
Epoch 35/1000 
	 loss: 266.9093, MinusLogProbMetric: 266.9093, val_loss: 253.5935, val_MinusLogProbMetric: 253.5935

Epoch 35: val_loss did not improve from 237.28592
196/196 - 69s - loss: 266.9093 - MinusLogProbMetric: 266.9093 - val_loss: 253.5935 - val_MinusLogProbMetric: 253.5935 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 36/1000
2023-09-29 08:57:55.182 
Epoch 36/1000 
	 loss: 247.4593, MinusLogProbMetric: 247.4593, val_loss: 242.9731, val_MinusLogProbMetric: 242.9731

Epoch 36: val_loss did not improve from 237.28592
196/196 - 69s - loss: 247.4593 - MinusLogProbMetric: 247.4593 - val_loss: 242.9731 - val_MinusLogProbMetric: 242.9731 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 37/1000
2023-09-29 08:59:02.659 
Epoch 37/1000 
	 loss: 240.3883, MinusLogProbMetric: 240.3883, val_loss: 238.7512, val_MinusLogProbMetric: 238.7512

Epoch 37: val_loss did not improve from 237.28592
196/196 - 67s - loss: 240.3883 - MinusLogProbMetric: 240.3883 - val_loss: 238.7512 - val_MinusLogProbMetric: 238.7512 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 38/1000
2023-09-29 09:00:07.886 
Epoch 38/1000 
	 loss: 236.9134, MinusLogProbMetric: 236.9134, val_loss: 235.3277, val_MinusLogProbMetric: 235.3277

Epoch 38: val_loss improved from 237.28592 to 235.32774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 236.9134 - MinusLogProbMetric: 236.9134 - val_loss: 235.3277 - val_MinusLogProbMetric: 235.3277 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 39/1000
2023-09-29 09:01:12.155 
Epoch 39/1000 
	 loss: 234.3200, MinusLogProbMetric: 234.3200, val_loss: 233.8920, val_MinusLogProbMetric: 233.8920

Epoch 39: val_loss improved from 235.32774 to 233.89195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 64s - loss: 234.3200 - MinusLogProbMetric: 234.3200 - val_loss: 233.8920 - val_MinusLogProbMetric: 233.8920 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 40/1000
2023-09-29 09:02:12.055 
Epoch 40/1000 
	 loss: 232.4155, MinusLogProbMetric: 232.4155, val_loss: 230.0801, val_MinusLogProbMetric: 230.0801

Epoch 40: val_loss improved from 233.89195 to 230.08008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 60s - loss: 232.4155 - MinusLogProbMetric: 232.4155 - val_loss: 230.0801 - val_MinusLogProbMetric: 230.0801 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 41/1000
2023-09-29 09:03:13.995 
Epoch 41/1000 
	 loss: 228.9032, MinusLogProbMetric: 228.9032, val_loss: 227.9205, val_MinusLogProbMetric: 227.9205

Epoch 41: val_loss improved from 230.08008 to 227.92046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 62s - loss: 228.9032 - MinusLogProbMetric: 228.9032 - val_loss: 227.9205 - val_MinusLogProbMetric: 227.9205 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 42/1000
2023-09-29 09:04:23.329 
Epoch 42/1000 
	 loss: 226.9497, MinusLogProbMetric: 226.9497, val_loss: 226.0654, val_MinusLogProbMetric: 226.0654

Epoch 42: val_loss improved from 227.92046 to 226.06535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 226.9497 - MinusLogProbMetric: 226.9497 - val_loss: 226.0654 - val_MinusLogProbMetric: 226.0654 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 43/1000
2023-09-29 09:05:30.025 
Epoch 43/1000 
	 loss: 225.0897, MinusLogProbMetric: 225.0897, val_loss: 224.9133, val_MinusLogProbMetric: 224.9133

Epoch 43: val_loss improved from 226.06535 to 224.91333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 225.0897 - MinusLogProbMetric: 225.0897 - val_loss: 224.9133 - val_MinusLogProbMetric: 224.9133 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 44/1000
2023-09-29 09:06:30.024 
Epoch 44/1000 
	 loss: 223.4406, MinusLogProbMetric: 223.4406, val_loss: 222.7538, val_MinusLogProbMetric: 222.7538

Epoch 44: val_loss improved from 224.91333 to 222.75383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 60s - loss: 223.4406 - MinusLogProbMetric: 223.4406 - val_loss: 222.7538 - val_MinusLogProbMetric: 222.7538 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 45/1000
2023-09-29 09:07:31.607 
Epoch 45/1000 
	 loss: 222.4878, MinusLogProbMetric: 222.4878, val_loss: 222.2161, val_MinusLogProbMetric: 222.2161

Epoch 45: val_loss improved from 222.75383 to 222.21611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 62s - loss: 222.4878 - MinusLogProbMetric: 222.4878 - val_loss: 222.2161 - val_MinusLogProbMetric: 222.2161 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 46/1000
2023-09-29 09:08:39.011 
Epoch 46/1000 
	 loss: 223.2625, MinusLogProbMetric: 223.2625, val_loss: 226.5086, val_MinusLogProbMetric: 226.5086

Epoch 46: val_loss did not improve from 222.21611
196/196 - 66s - loss: 223.2625 - MinusLogProbMetric: 223.2625 - val_loss: 226.5086 - val_MinusLogProbMetric: 226.5086 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 47/1000
2023-09-29 09:09:47.618 
Epoch 47/1000 
	 loss: 219.7714, MinusLogProbMetric: 219.7714, val_loss: 218.5701, val_MinusLogProbMetric: 218.5701

Epoch 47: val_loss improved from 222.21611 to 218.57008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 219.7714 - MinusLogProbMetric: 219.7714 - val_loss: 218.5701 - val_MinusLogProbMetric: 218.5701 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 48/1000
2023-09-29 09:10:53.358 
Epoch 48/1000 
	 loss: 229.3719, MinusLogProbMetric: 229.3719, val_loss: 222.5130, val_MinusLogProbMetric: 222.5130

Epoch 48: val_loss did not improve from 218.57008
196/196 - 65s - loss: 229.3719 - MinusLogProbMetric: 229.3719 - val_loss: 222.5130 - val_MinusLogProbMetric: 222.5130 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 49/1000
2023-09-29 09:11:59.217 
Epoch 49/1000 
	 loss: 219.3719, MinusLogProbMetric: 219.3719, val_loss: 217.9625, val_MinusLogProbMetric: 217.9625

Epoch 49: val_loss improved from 218.57008 to 217.96246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 219.3719 - MinusLogProbMetric: 219.3719 - val_loss: 217.9625 - val_MinusLogProbMetric: 217.9625 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 50/1000
2023-09-29 09:13:05.758 
Epoch 50/1000 
	 loss: 217.0473, MinusLogProbMetric: 217.0473, val_loss: 216.3326, val_MinusLogProbMetric: 216.3326

Epoch 50: val_loss improved from 217.96246 to 216.33261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 217.0473 - MinusLogProbMetric: 217.0473 - val_loss: 216.3326 - val_MinusLogProbMetric: 216.3326 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 51/1000
2023-09-29 09:14:11.991 
Epoch 51/1000 
	 loss: 215.4706, MinusLogProbMetric: 215.4706, val_loss: 215.1596, val_MinusLogProbMetric: 215.1596

Epoch 51: val_loss improved from 216.33261 to 215.15958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 215.4706 - MinusLogProbMetric: 215.4706 - val_loss: 215.1596 - val_MinusLogProbMetric: 215.1596 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 52/1000
2023-09-29 09:15:21.691 
Epoch 52/1000 
	 loss: 214.5749, MinusLogProbMetric: 214.5749, val_loss: 214.2345, val_MinusLogProbMetric: 214.2345

Epoch 52: val_loss improved from 215.15958 to 214.23453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 214.5749 - MinusLogProbMetric: 214.5749 - val_loss: 214.2345 - val_MinusLogProbMetric: 214.2345 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 53/1000
2023-09-29 09:16:31.212 
Epoch 53/1000 
	 loss: 213.6571, MinusLogProbMetric: 213.6571, val_loss: 213.4068, val_MinusLogProbMetric: 213.4068

Epoch 53: val_loss improved from 214.23453 to 213.40678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 213.6571 - MinusLogProbMetric: 213.6571 - val_loss: 213.4068 - val_MinusLogProbMetric: 213.4068 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 54/1000
2023-09-29 09:17:37.727 
Epoch 54/1000 
	 loss: 212.0996, MinusLogProbMetric: 212.0996, val_loss: 211.2157, val_MinusLogProbMetric: 211.2157

Epoch 54: val_loss improved from 213.40678 to 211.21573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 212.0996 - MinusLogProbMetric: 212.0996 - val_loss: 211.2157 - val_MinusLogProbMetric: 211.2157 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 55/1000
2023-09-29 09:18:47.561 
Epoch 55/1000 
	 loss: 210.4317, MinusLogProbMetric: 210.4317, val_loss: 209.9366, val_MinusLogProbMetric: 209.9366

Epoch 55: val_loss improved from 211.21573 to 209.93660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 210.4317 - MinusLogProbMetric: 210.4317 - val_loss: 209.9366 - val_MinusLogProbMetric: 209.9366 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 56/1000
2023-09-29 09:19:57.823 
Epoch 56/1000 
	 loss: 210.0482, MinusLogProbMetric: 210.0482, val_loss: 209.1244, val_MinusLogProbMetric: 209.1244

Epoch 56: val_loss improved from 209.93660 to 209.12440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 210.0482 - MinusLogProbMetric: 210.0482 - val_loss: 209.1244 - val_MinusLogProbMetric: 209.1244 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 57/1000
2023-09-29 09:21:08.018 
Epoch 57/1000 
	 loss: 208.6870, MinusLogProbMetric: 208.6870, val_loss: 210.3454, val_MinusLogProbMetric: 210.3454

Epoch 57: val_loss did not improve from 209.12440
196/196 - 69s - loss: 208.6870 - MinusLogProbMetric: 208.6870 - val_loss: 210.3454 - val_MinusLogProbMetric: 210.3454 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 58/1000
2023-09-29 09:22:09.568 
Epoch 58/1000 
	 loss: 207.8438, MinusLogProbMetric: 207.8438, val_loss: 209.9585, val_MinusLogProbMetric: 209.9585

Epoch 58: val_loss did not improve from 209.12440
196/196 - 62s - loss: 207.8438 - MinusLogProbMetric: 207.8438 - val_loss: 209.9585 - val_MinusLogProbMetric: 209.9585 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 59/1000
2023-09-29 09:23:08.810 
Epoch 59/1000 
	 loss: 206.9162, MinusLogProbMetric: 206.9162, val_loss: 206.3582, val_MinusLogProbMetric: 206.3582

Epoch 59: val_loss improved from 209.12440 to 206.35817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 60s - loss: 206.9162 - MinusLogProbMetric: 206.9162 - val_loss: 206.3582 - val_MinusLogProbMetric: 206.3582 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 60/1000
2023-09-29 09:24:10.637 
Epoch 60/1000 
	 loss: 205.6873, MinusLogProbMetric: 205.6873, val_loss: 205.5067, val_MinusLogProbMetric: 205.5067

Epoch 60: val_loss improved from 206.35817 to 205.50665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 62s - loss: 205.6873 - MinusLogProbMetric: 205.6873 - val_loss: 205.5067 - val_MinusLogProbMetric: 205.5067 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 61/1000
2023-09-29 09:25:17.087 
Epoch 61/1000 
	 loss: 204.8685, MinusLogProbMetric: 204.8685, val_loss: 204.6902, val_MinusLogProbMetric: 204.6902

Epoch 61: val_loss improved from 205.50665 to 204.69016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 204.8685 - MinusLogProbMetric: 204.8685 - val_loss: 204.6902 - val_MinusLogProbMetric: 204.6902 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 62/1000
2023-09-29 09:26:25.318 
Epoch 62/1000 
	 loss: 204.1033, MinusLogProbMetric: 204.1033, val_loss: 203.7901, val_MinusLogProbMetric: 203.7901

Epoch 62: val_loss improved from 204.69016 to 203.79013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 204.1033 - MinusLogProbMetric: 204.1033 - val_loss: 203.7901 - val_MinusLogProbMetric: 203.7901 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 63/1000
2023-09-29 09:27:35.508 
Epoch 63/1000 
	 loss: 203.1560, MinusLogProbMetric: 203.1560, val_loss: 202.7701, val_MinusLogProbMetric: 202.7701

Epoch 63: val_loss improved from 203.79013 to 202.77011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 203.1560 - MinusLogProbMetric: 203.1560 - val_loss: 202.7701 - val_MinusLogProbMetric: 202.7701 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 64/1000
2023-09-29 09:28:45.186 
Epoch 64/1000 
	 loss: 203.2582, MinusLogProbMetric: 203.2582, val_loss: 203.4850, val_MinusLogProbMetric: 203.4850

Epoch 64: val_loss did not improve from 202.77011
196/196 - 69s - loss: 203.2582 - MinusLogProbMetric: 203.2582 - val_loss: 203.4850 - val_MinusLogProbMetric: 203.4850 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 65/1000
2023-09-29 09:29:54.177 
Epoch 65/1000 
	 loss: 202.0970, MinusLogProbMetric: 202.0970, val_loss: 201.8434, val_MinusLogProbMetric: 201.8434

Epoch 65: val_loss improved from 202.77011 to 201.84335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 202.0970 - MinusLogProbMetric: 202.0970 - val_loss: 201.8434 - val_MinusLogProbMetric: 201.8434 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 66/1000
2023-09-29 09:31:03.331 
Epoch 66/1000 
	 loss: 201.0982, MinusLogProbMetric: 201.0982, val_loss: 200.9152, val_MinusLogProbMetric: 200.9152

Epoch 66: val_loss improved from 201.84335 to 200.91521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 201.0982 - MinusLogProbMetric: 201.0982 - val_loss: 200.9152 - val_MinusLogProbMetric: 200.9152 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 67/1000
2023-09-29 09:32:11.002 
Epoch 67/1000 
	 loss: 200.1708, MinusLogProbMetric: 200.1708, val_loss: 200.2082, val_MinusLogProbMetric: 200.2082

Epoch 67: val_loss improved from 200.91521 to 200.20822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 200.1708 - MinusLogProbMetric: 200.1708 - val_loss: 200.2082 - val_MinusLogProbMetric: 200.2082 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 68/1000
2023-09-29 09:33:17.186 
Epoch 68/1000 
	 loss: 211.0504, MinusLogProbMetric: 211.0504, val_loss: 217.4301, val_MinusLogProbMetric: 217.4301

Epoch 68: val_loss did not improve from 200.20822
196/196 - 65s - loss: 211.0504 - MinusLogProbMetric: 211.0504 - val_loss: 217.4301 - val_MinusLogProbMetric: 217.4301 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 69/1000
2023-09-29 09:34:22.073 
Epoch 69/1000 
	 loss: 207.6043, MinusLogProbMetric: 207.6043, val_loss: 205.3615, val_MinusLogProbMetric: 205.3615

Epoch 69: val_loss did not improve from 200.20822
196/196 - 65s - loss: 207.6043 - MinusLogProbMetric: 207.6043 - val_loss: 205.3615 - val_MinusLogProbMetric: 205.3615 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 70/1000
2023-09-29 09:35:28.641 
Epoch 70/1000 
	 loss: 205.2099, MinusLogProbMetric: 205.2099, val_loss: 200.3114, val_MinusLogProbMetric: 200.3114

Epoch 70: val_loss did not improve from 200.20822
196/196 - 67s - loss: 205.2099 - MinusLogProbMetric: 205.2099 - val_loss: 200.3114 - val_MinusLogProbMetric: 200.3114 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 71/1000
2023-09-29 09:36:35.652 
Epoch 71/1000 
	 loss: 198.4954, MinusLogProbMetric: 198.4954, val_loss: 197.8400, val_MinusLogProbMetric: 197.8400

Epoch 71: val_loss improved from 200.20822 to 197.84001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 198.4954 - MinusLogProbMetric: 198.4954 - val_loss: 197.8400 - val_MinusLogProbMetric: 197.8400 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 72/1000
2023-09-29 09:37:43.022 
Epoch 72/1000 
	 loss: 197.3157, MinusLogProbMetric: 197.3157, val_loss: 197.3102, val_MinusLogProbMetric: 197.3102

Epoch 72: val_loss improved from 197.84001 to 197.31018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 197.3157 - MinusLogProbMetric: 197.3157 - val_loss: 197.3102 - val_MinusLogProbMetric: 197.3102 - lr: 4.1152e-06 - 68s/epoch - 344ms/step
Epoch 73/1000
2023-09-29 09:38:52.125 
Epoch 73/1000 
	 loss: 196.2357, MinusLogProbMetric: 196.2357, val_loss: 195.9019, val_MinusLogProbMetric: 195.9019

Epoch 73: val_loss improved from 197.31018 to 195.90190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 196.2357 - MinusLogProbMetric: 196.2357 - val_loss: 195.9019 - val_MinusLogProbMetric: 195.9019 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 74/1000
2023-09-29 09:40:01.205 
Epoch 74/1000 
	 loss: 195.3601, MinusLogProbMetric: 195.3601, val_loss: 195.4245, val_MinusLogProbMetric: 195.4245

Epoch 74: val_loss improved from 195.90190 to 195.42455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 195.3601 - MinusLogProbMetric: 195.3601 - val_loss: 195.4245 - val_MinusLogProbMetric: 195.4245 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 75/1000
2023-09-29 09:41:11.227 
Epoch 75/1000 
	 loss: 194.5551, MinusLogProbMetric: 194.5551, val_loss: 194.3841, val_MinusLogProbMetric: 194.3841

Epoch 75: val_loss improved from 195.42455 to 194.38406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 194.5551 - MinusLogProbMetric: 194.5551 - val_loss: 194.3841 - val_MinusLogProbMetric: 194.3841 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 76/1000
2023-09-29 09:42:18.227 
Epoch 76/1000 
	 loss: 193.8753, MinusLogProbMetric: 193.8753, val_loss: 193.9254, val_MinusLogProbMetric: 193.9254

Epoch 76: val_loss improved from 194.38406 to 193.92535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 193.8753 - MinusLogProbMetric: 193.8753 - val_loss: 193.9254 - val_MinusLogProbMetric: 193.9254 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 77/1000
2023-09-29 09:43:24.772 
Epoch 77/1000 
	 loss: 193.0783, MinusLogProbMetric: 193.0783, val_loss: 193.0133, val_MinusLogProbMetric: 193.0133

Epoch 77: val_loss improved from 193.92535 to 193.01334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 193.0783 - MinusLogProbMetric: 193.0783 - val_loss: 193.0133 - val_MinusLogProbMetric: 193.0133 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 78/1000
2023-09-29 09:44:31.835 
Epoch 78/1000 
	 loss: 193.0284, MinusLogProbMetric: 193.0284, val_loss: 192.9758, val_MinusLogProbMetric: 192.9758

Epoch 78: val_loss improved from 193.01334 to 192.97575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 193.0284 - MinusLogProbMetric: 193.0284 - val_loss: 192.9758 - val_MinusLogProbMetric: 192.9758 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 79/1000
2023-09-29 09:45:37.659 
Epoch 79/1000 
	 loss: 192.1147, MinusLogProbMetric: 192.1147, val_loss: 191.8288, val_MinusLogProbMetric: 191.8288

Epoch 79: val_loss improved from 192.97575 to 191.82878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 192.1147 - MinusLogProbMetric: 192.1147 - val_loss: 191.8288 - val_MinusLogProbMetric: 191.8288 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 80/1000
2023-09-29 09:46:46.949 
Epoch 80/1000 
	 loss: 191.1802, MinusLogProbMetric: 191.1802, val_loss: 191.3023, val_MinusLogProbMetric: 191.3023

Epoch 80: val_loss improved from 191.82878 to 191.30234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 191.1802 - MinusLogProbMetric: 191.1802 - val_loss: 191.3023 - val_MinusLogProbMetric: 191.3023 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 81/1000
2023-09-29 09:47:55.247 
Epoch 81/1000 
	 loss: 190.4411, MinusLogProbMetric: 190.4411, val_loss: 190.5063, val_MinusLogProbMetric: 190.5063

Epoch 81: val_loss improved from 191.30234 to 190.50627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 190.4411 - MinusLogProbMetric: 190.4411 - val_loss: 190.5063 - val_MinusLogProbMetric: 190.5063 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 82/1000
2023-09-29 09:49:05.131 
Epoch 82/1000 
	 loss: 189.8709, MinusLogProbMetric: 189.8709, val_loss: 189.8604, val_MinusLogProbMetric: 189.8604

Epoch 82: val_loss improved from 190.50627 to 189.86044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 189.8709 - MinusLogProbMetric: 189.8709 - val_loss: 189.8604 - val_MinusLogProbMetric: 189.8604 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 83/1000
2023-09-29 09:50:13.314 
Epoch 83/1000 
	 loss: 189.8997, MinusLogProbMetric: 189.8997, val_loss: 189.8051, val_MinusLogProbMetric: 189.8051

Epoch 83: val_loss improved from 189.86044 to 189.80505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 189.8997 - MinusLogProbMetric: 189.8997 - val_loss: 189.8051 - val_MinusLogProbMetric: 189.8051 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 84/1000
2023-09-29 09:51:23.676 
Epoch 84/1000 
	 loss: 188.9340, MinusLogProbMetric: 188.9340, val_loss: 189.1125, val_MinusLogProbMetric: 189.1125

Epoch 84: val_loss improved from 189.80505 to 189.11252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 188.9340 - MinusLogProbMetric: 188.9340 - val_loss: 189.1125 - val_MinusLogProbMetric: 189.1125 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 85/1000
2023-09-29 09:52:33.571 
Epoch 85/1000 
	 loss: 188.2425, MinusLogProbMetric: 188.2425, val_loss: 188.1632, val_MinusLogProbMetric: 188.1632

Epoch 85: val_loss improved from 189.11252 to 188.16319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 188.2425 - MinusLogProbMetric: 188.2425 - val_loss: 188.1632 - val_MinusLogProbMetric: 188.1632 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 86/1000
2023-09-29 09:53:38.125 
Epoch 86/1000 
	 loss: 189.7831, MinusLogProbMetric: 189.7831, val_loss: 187.6295, val_MinusLogProbMetric: 187.6295

Epoch 86: val_loss improved from 188.16319 to 187.62947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 65s - loss: 189.7831 - MinusLogProbMetric: 189.7831 - val_loss: 187.6295 - val_MinusLogProbMetric: 187.6295 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 87/1000
2023-09-29 09:54:44.920 
Epoch 87/1000 
	 loss: 187.8166, MinusLogProbMetric: 187.8166, val_loss: 187.0266, val_MinusLogProbMetric: 187.0266

Epoch 87: val_loss improved from 187.62947 to 187.02655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 187.8166 - MinusLogProbMetric: 187.8166 - val_loss: 187.0266 - val_MinusLogProbMetric: 187.0266 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 88/1000
2023-09-29 09:55:50.680 
Epoch 88/1000 
	 loss: 186.5621, MinusLogProbMetric: 186.5621, val_loss: 186.3908, val_MinusLogProbMetric: 186.3908

Epoch 88: val_loss improved from 187.02655 to 186.39084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 186.5621 - MinusLogProbMetric: 186.5621 - val_loss: 186.3908 - val_MinusLogProbMetric: 186.3908 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 89/1000
2023-09-29 09:56:55.644 
Epoch 89/1000 
	 loss: 189.3609, MinusLogProbMetric: 189.3609, val_loss: 187.7201, val_MinusLogProbMetric: 187.7201

Epoch 89: val_loss did not improve from 186.39084
196/196 - 64s - loss: 189.3609 - MinusLogProbMetric: 189.3609 - val_loss: 187.7201 - val_MinusLogProbMetric: 187.7201 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 90/1000
2023-09-29 09:58:00.330 
Epoch 90/1000 
	 loss: 186.3216, MinusLogProbMetric: 186.3216, val_loss: 186.1157, val_MinusLogProbMetric: 186.1157

Epoch 90: val_loss improved from 186.39084 to 186.11574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 186.3216 - MinusLogProbMetric: 186.3216 - val_loss: 186.1157 - val_MinusLogProbMetric: 186.1157 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 91/1000
2023-09-29 09:59:08.958 
Epoch 91/1000 
	 loss: 185.5004, MinusLogProbMetric: 185.5004, val_loss: 185.3559, val_MinusLogProbMetric: 185.3559

Epoch 91: val_loss improved from 186.11574 to 185.35587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 185.5004 - MinusLogProbMetric: 185.5004 - val_loss: 185.3559 - val_MinusLogProbMetric: 185.3559 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 92/1000
2023-09-29 10:00:18.565 
Epoch 92/1000 
	 loss: 185.2639, MinusLogProbMetric: 185.2639, val_loss: 184.9787, val_MinusLogProbMetric: 184.9787

Epoch 92: val_loss improved from 185.35587 to 184.97865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 185.2639 - MinusLogProbMetric: 185.2639 - val_loss: 184.9787 - val_MinusLogProbMetric: 184.9787 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 93/1000
2023-09-29 10:01:27.337 
Epoch 93/1000 
	 loss: 184.3027, MinusLogProbMetric: 184.3027, val_loss: 183.8585, val_MinusLogProbMetric: 183.8585

Epoch 93: val_loss improved from 184.97865 to 183.85854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 184.3027 - MinusLogProbMetric: 184.3027 - val_loss: 183.8585 - val_MinusLogProbMetric: 183.8585 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 94/1000
2023-09-29 10:02:35.330 
Epoch 94/1000 
	 loss: 183.2240, MinusLogProbMetric: 183.2240, val_loss: 183.2492, val_MinusLogProbMetric: 183.2492

Epoch 94: val_loss improved from 183.85854 to 183.24918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 183.2240 - MinusLogProbMetric: 183.2240 - val_loss: 183.2492 - val_MinusLogProbMetric: 183.2492 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 95/1000
2023-09-29 10:03:41.639 
Epoch 95/1000 
	 loss: 185.3255, MinusLogProbMetric: 185.3255, val_loss: 191.1153, val_MinusLogProbMetric: 191.1153

Epoch 95: val_loss did not improve from 183.24918
196/196 - 65s - loss: 185.3255 - MinusLogProbMetric: 185.3255 - val_loss: 191.1153 - val_MinusLogProbMetric: 191.1153 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 96/1000
2023-09-29 10:04:48.200 
Epoch 96/1000 
	 loss: 184.1556, MinusLogProbMetric: 184.1556, val_loss: 182.3227, val_MinusLogProbMetric: 182.3227

Epoch 96: val_loss improved from 183.24918 to 182.32269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 184.1556 - MinusLogProbMetric: 184.1556 - val_loss: 182.3227 - val_MinusLogProbMetric: 182.3227 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 97/1000
2023-09-29 10:05:56.591 
Epoch 97/1000 
	 loss: 181.9695, MinusLogProbMetric: 181.9695, val_loss: 182.4313, val_MinusLogProbMetric: 182.4313

Epoch 97: val_loss did not improve from 182.32269
196/196 - 67s - loss: 181.9695 - MinusLogProbMetric: 181.9695 - val_loss: 182.4313 - val_MinusLogProbMetric: 182.4313 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 98/1000
2023-09-29 10:07:02.517 
Epoch 98/1000 
	 loss: 181.4013, MinusLogProbMetric: 181.4013, val_loss: 181.0259, val_MinusLogProbMetric: 181.0259

Epoch 98: val_loss improved from 182.32269 to 181.02591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 181.4013 - MinusLogProbMetric: 181.4013 - val_loss: 181.0259 - val_MinusLogProbMetric: 181.0259 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 99/1000
2023-09-29 10:08:08.038 
Epoch 99/1000 
	 loss: 180.5544, MinusLogProbMetric: 180.5544, val_loss: 180.4467, val_MinusLogProbMetric: 180.4467

Epoch 99: val_loss improved from 181.02591 to 180.44673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 180.5544 - MinusLogProbMetric: 180.5544 - val_loss: 180.4467 - val_MinusLogProbMetric: 180.4467 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 100/1000
2023-09-29 10:09:13.716 
Epoch 100/1000 
	 loss: 180.0184, MinusLogProbMetric: 180.0184, val_loss: 180.0100, val_MinusLogProbMetric: 180.0100

Epoch 100: val_loss improved from 180.44673 to 180.01004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 180.0184 - MinusLogProbMetric: 180.0184 - val_loss: 180.0100 - val_MinusLogProbMetric: 180.0100 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 101/1000
2023-09-29 10:10:22.523 
Epoch 101/1000 
	 loss: 179.4075, MinusLogProbMetric: 179.4075, val_loss: 179.5403, val_MinusLogProbMetric: 179.5403

Epoch 101: val_loss improved from 180.01004 to 179.54031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 69s - loss: 179.4075 - MinusLogProbMetric: 179.4075 - val_loss: 179.5403 - val_MinusLogProbMetric: 179.5403 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 102/1000
2023-09-29 10:11:32.311 
Epoch 102/1000 
	 loss: 178.8081, MinusLogProbMetric: 178.8081, val_loss: 178.8329, val_MinusLogProbMetric: 178.8329

Epoch 102: val_loss improved from 179.54031 to 178.83295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 178.8081 - MinusLogProbMetric: 178.8081 - val_loss: 178.8329 - val_MinusLogProbMetric: 178.8329 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 103/1000
2023-09-29 10:12:42.363 
Epoch 103/1000 
	 loss: 178.3392, MinusLogProbMetric: 178.3392, val_loss: 178.3723, val_MinusLogProbMetric: 178.3723

Epoch 103: val_loss improved from 178.83295 to 178.37231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 178.3392 - MinusLogProbMetric: 178.3392 - val_loss: 178.3723 - val_MinusLogProbMetric: 178.3723 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 104/1000
2023-09-29 10:13:52.464 
Epoch 104/1000 
	 loss: 177.7349, MinusLogProbMetric: 177.7349, val_loss: 178.1304, val_MinusLogProbMetric: 178.1304

Epoch 104: val_loss improved from 178.37231 to 178.13039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 177.7349 - MinusLogProbMetric: 177.7349 - val_loss: 178.1304 - val_MinusLogProbMetric: 178.1304 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 105/1000
2023-09-29 10:14:58.708 
Epoch 105/1000 
	 loss: 177.5995, MinusLogProbMetric: 177.5995, val_loss: 177.4177, val_MinusLogProbMetric: 177.4177

Epoch 105: val_loss improved from 178.13039 to 177.41768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 177.5995 - MinusLogProbMetric: 177.5995 - val_loss: 177.4177 - val_MinusLogProbMetric: 177.4177 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 106/1000
2023-09-29 10:16:03.774 
Epoch 106/1000 
	 loss: 179.0642, MinusLogProbMetric: 179.0642, val_loss: 178.2297, val_MinusLogProbMetric: 178.2297

Epoch 106: val_loss did not improve from 177.41768
196/196 - 64s - loss: 179.0642 - MinusLogProbMetric: 179.0642 - val_loss: 178.2297 - val_MinusLogProbMetric: 178.2297 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 107/1000
2023-09-29 10:17:08.755 
Epoch 107/1000 
	 loss: 176.7512, MinusLogProbMetric: 176.7512, val_loss: 176.6703, val_MinusLogProbMetric: 176.6703

Epoch 107: val_loss improved from 177.41768 to 176.67035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 176.7512 - MinusLogProbMetric: 176.7512 - val_loss: 176.6703 - val_MinusLogProbMetric: 176.6703 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 108/1000
2023-09-29 10:18:12.913 
Epoch 108/1000 
	 loss: 176.0513, MinusLogProbMetric: 176.0513, val_loss: 176.3100, val_MinusLogProbMetric: 176.3100

Epoch 108: val_loss improved from 176.67035 to 176.30995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 64s - loss: 176.0513 - MinusLogProbMetric: 176.0513 - val_loss: 176.3100 - val_MinusLogProbMetric: 176.3100 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 109/1000
2023-09-29 10:19:17.305 
Epoch 109/1000 
	 loss: 175.6323, MinusLogProbMetric: 175.6323, val_loss: 175.8701, val_MinusLogProbMetric: 175.8701

Epoch 109: val_loss improved from 176.30995 to 175.87015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 64s - loss: 175.6323 - MinusLogProbMetric: 175.6323 - val_loss: 175.8701 - val_MinusLogProbMetric: 175.8701 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 110/1000
2023-09-29 10:20:26.113 
Epoch 110/1000 
	 loss: 175.6429, MinusLogProbMetric: 175.6429, val_loss: 178.3374, val_MinusLogProbMetric: 178.3374

Epoch 110: val_loss did not improve from 175.87015
196/196 - 68s - loss: 175.6429 - MinusLogProbMetric: 175.6429 - val_loss: 178.3374 - val_MinusLogProbMetric: 178.3374 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 111/1000
2023-09-29 10:21:32.279 
Epoch 111/1000 
	 loss: 175.5073, MinusLogProbMetric: 175.5073, val_loss: 175.2614, val_MinusLogProbMetric: 175.2614

Epoch 111: val_loss improved from 175.87015 to 175.26137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 175.5073 - MinusLogProbMetric: 175.5073 - val_loss: 175.2614 - val_MinusLogProbMetric: 175.2614 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 112/1000
2023-09-29 10:22:37.864 
Epoch 112/1000 
	 loss: 174.5316, MinusLogProbMetric: 174.5316, val_loss: 174.5832, val_MinusLogProbMetric: 174.5832

Epoch 112: val_loss improved from 175.26137 to 174.58324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 65s - loss: 174.5316 - MinusLogProbMetric: 174.5316 - val_loss: 174.5832 - val_MinusLogProbMetric: 174.5832 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 113/1000
2023-09-29 10:23:44.618 
Epoch 113/1000 
	 loss: 174.2955, MinusLogProbMetric: 174.2955, val_loss: 174.4516, val_MinusLogProbMetric: 174.4516

Epoch 113: val_loss improved from 174.58324 to 174.45160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 174.2955 - MinusLogProbMetric: 174.2955 - val_loss: 174.4516 - val_MinusLogProbMetric: 174.4516 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 114/1000
2023-09-29 10:24:52.877 
Epoch 114/1000 
	 loss: 174.0656, MinusLogProbMetric: 174.0656, val_loss: 175.6454, val_MinusLogProbMetric: 175.6454

Epoch 114: val_loss did not improve from 174.45160
196/196 - 67s - loss: 174.0656 - MinusLogProbMetric: 174.0656 - val_loss: 175.6454 - val_MinusLogProbMetric: 175.6454 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 115/1000
2023-09-29 10:25:58.194 
Epoch 115/1000 
	 loss: 173.3988, MinusLogProbMetric: 173.3988, val_loss: 173.3539, val_MinusLogProbMetric: 173.3539

Epoch 115: val_loss improved from 174.45160 to 173.35390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 173.3988 - MinusLogProbMetric: 173.3988 - val_loss: 173.3539 - val_MinusLogProbMetric: 173.3539 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 116/1000
2023-09-29 10:27:08.281 
Epoch 116/1000 
	 loss: 172.8871, MinusLogProbMetric: 172.8871, val_loss: 173.2131, val_MinusLogProbMetric: 173.2131

Epoch 116: val_loss improved from 173.35390 to 173.21307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 70s - loss: 172.8871 - MinusLogProbMetric: 172.8871 - val_loss: 173.2131 - val_MinusLogProbMetric: 173.2131 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 117/1000
2023-09-29 10:28:14.833 
Epoch 117/1000 
	 loss: 172.3339, MinusLogProbMetric: 172.3339, val_loss: 172.7182, val_MinusLogProbMetric: 172.7182

Epoch 117: val_loss improved from 173.21307 to 172.71817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 172.3339 - MinusLogProbMetric: 172.3339 - val_loss: 172.7182 - val_MinusLogProbMetric: 172.7182 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 118/1000
2023-09-29 10:29:19.847 
Epoch 118/1000 
	 loss: 173.8761, MinusLogProbMetric: 173.8761, val_loss: 172.4274, val_MinusLogProbMetric: 172.4274

Epoch 118: val_loss improved from 172.71817 to 172.42737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 65s - loss: 173.8761 - MinusLogProbMetric: 173.8761 - val_loss: 172.4274 - val_MinusLogProbMetric: 172.4274 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 119/1000
2023-09-29 10:30:28.209 
Epoch 119/1000 
	 loss: 171.7888, MinusLogProbMetric: 171.7888, val_loss: 171.8957, val_MinusLogProbMetric: 171.8957

Epoch 119: val_loss improved from 172.42737 to 171.89571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 171.7888 - MinusLogProbMetric: 171.7888 - val_loss: 171.8957 - val_MinusLogProbMetric: 171.8957 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 120/1000
2023-09-29 10:31:36.710 
Epoch 120/1000 
	 loss: 171.2476, MinusLogProbMetric: 171.2476, val_loss: 173.9847, val_MinusLogProbMetric: 173.9847

Epoch 120: val_loss did not improve from 171.89571
196/196 - 67s - loss: 171.2476 - MinusLogProbMetric: 171.2476 - val_loss: 173.9847 - val_MinusLogProbMetric: 173.9847 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 121/1000
2023-09-29 10:32:39.775 
Epoch 121/1000 
	 loss: 172.4946, MinusLogProbMetric: 172.4946, val_loss: 175.5845, val_MinusLogProbMetric: 175.5845

Epoch 121: val_loss did not improve from 171.89571
196/196 - 63s - loss: 172.4946 - MinusLogProbMetric: 172.4946 - val_loss: 175.5845 - val_MinusLogProbMetric: 175.5845 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 122/1000
2023-09-29 10:33:45.629 
Epoch 122/1000 
	 loss: 180.7320, MinusLogProbMetric: 180.7320, val_loss: 177.6657, val_MinusLogProbMetric: 177.6657

Epoch 122: val_loss did not improve from 171.89571
196/196 - 66s - loss: 180.7320 - MinusLogProbMetric: 180.7320 - val_loss: 177.6657 - val_MinusLogProbMetric: 177.6657 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 123/1000
2023-09-29 10:34:52.940 
Epoch 123/1000 
	 loss: 175.5416, MinusLogProbMetric: 175.5416, val_loss: 173.4817, val_MinusLogProbMetric: 173.4817

Epoch 123: val_loss did not improve from 171.89571
196/196 - 67s - loss: 175.5416 - MinusLogProbMetric: 175.5416 - val_loss: 173.4817 - val_MinusLogProbMetric: 173.4817 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 124/1000
2023-09-29 10:35:58.494 
Epoch 124/1000 
	 loss: 170.2701, MinusLogProbMetric: 170.2701, val_loss: 169.8118, val_MinusLogProbMetric: 169.8118

Epoch 124: val_loss improved from 171.89571 to 169.81183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 170.2701 - MinusLogProbMetric: 170.2701 - val_loss: 169.8118 - val_MinusLogProbMetric: 169.8118 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 125/1000
2023-09-29 10:37:01.671 
Epoch 125/1000 
	 loss: 170.8974, MinusLogProbMetric: 170.8974, val_loss: 170.3356, val_MinusLogProbMetric: 170.3356

Epoch 125: val_loss did not improve from 169.81183
196/196 - 62s - loss: 170.8974 - MinusLogProbMetric: 170.8974 - val_loss: 170.3356 - val_MinusLogProbMetric: 170.3356 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 126/1000
2023-09-29 10:38:05.095 
Epoch 126/1000 
	 loss: 169.5257, MinusLogProbMetric: 169.5257, val_loss: 169.6293, val_MinusLogProbMetric: 169.6293

Epoch 126: val_loss improved from 169.81183 to 169.62926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 65s - loss: 169.5257 - MinusLogProbMetric: 169.5257 - val_loss: 169.6293 - val_MinusLogProbMetric: 169.6293 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 127/1000
2023-09-29 10:39:11.721 
Epoch 127/1000 
	 loss: 169.2590, MinusLogProbMetric: 169.2590, val_loss: 169.0061, val_MinusLogProbMetric: 169.0061

Epoch 127: val_loss improved from 169.62926 to 169.00612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 66s - loss: 169.2590 - MinusLogProbMetric: 169.2590 - val_loss: 169.0061 - val_MinusLogProbMetric: 169.0061 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 128/1000
2023-09-29 10:40:18.677 
Epoch 128/1000 
	 loss: 168.7996, MinusLogProbMetric: 168.7996, val_loss: 168.7126, val_MinusLogProbMetric: 168.7126

Epoch 128: val_loss improved from 169.00612 to 168.71257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 168.7996 - MinusLogProbMetric: 168.7996 - val_loss: 168.7126 - val_MinusLogProbMetric: 168.7126 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 129/1000
2023-09-29 10:41:25.859 
Epoch 129/1000 
	 loss: 167.9292, MinusLogProbMetric: 167.9292, val_loss: 167.9242, val_MinusLogProbMetric: 167.9242

Epoch 129: val_loss improved from 168.71257 to 167.92418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 67s - loss: 167.9292 - MinusLogProbMetric: 167.9292 - val_loss: 167.9242 - val_MinusLogProbMetric: 167.9242 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 130/1000
2023-09-29 10:42:31.339 
Epoch 130/1000 
	 loss: 173.6053, MinusLogProbMetric: 173.6053, val_loss: 173.9340, val_MinusLogProbMetric: 173.9340

Epoch 130: val_loss did not improve from 167.92418
196/196 - 64s - loss: 173.6053 - MinusLogProbMetric: 173.6053 - val_loss: 173.9340 - val_MinusLogProbMetric: 173.9340 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 131/1000
2023-09-29 10:43:39.457 
Epoch 131/1000 
	 loss: 171.7168, MinusLogProbMetric: 171.7168, val_loss: 171.3755, val_MinusLogProbMetric: 171.3755

Epoch 131: val_loss did not improve from 167.92418
196/196 - 68s - loss: 171.7168 - MinusLogProbMetric: 171.7168 - val_loss: 171.3755 - val_MinusLogProbMetric: 171.3755 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 132/1000
2023-09-29 10:44:46.287 
Epoch 132/1000 
	 loss: 169.5105, MinusLogProbMetric: 169.5105, val_loss: 168.2985, val_MinusLogProbMetric: 168.2985

Epoch 132: val_loss did not improve from 167.92418
196/196 - 67s - loss: 169.5105 - MinusLogProbMetric: 169.5105 - val_loss: 168.2985 - val_MinusLogProbMetric: 168.2985 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 133/1000
2023-09-29 10:45:53.335 
Epoch 133/1000 
	 loss: 167.8233, MinusLogProbMetric: 167.8233, val_loss: 167.8030, val_MinusLogProbMetric: 167.8030

Epoch 133: val_loss improved from 167.92418 to 167.80304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 68s - loss: 167.8233 - MinusLogProbMetric: 167.8233 - val_loss: 167.8030 - val_MinusLogProbMetric: 167.8030 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 134/1000
2023-09-29 10:47:02.315 
Epoch 134/1000 
	 loss: 194.2194, MinusLogProbMetric: 194.2194, val_loss: 188.6640, val_MinusLogProbMetric: 188.6640

Epoch 134: val_loss did not improve from 167.80304
196/196 - 68s - loss: 194.2194 - MinusLogProbMetric: 194.2194 - val_loss: 188.6640 - val_MinusLogProbMetric: 188.6640 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 135/1000
2023-09-29 10:48:09.528 
Epoch 135/1000 
	 loss: 182.5857, MinusLogProbMetric: 182.5857, val_loss: 180.0317, val_MinusLogProbMetric: 180.0317

Epoch 135: val_loss did not improve from 167.80304
196/196 - 67s - loss: 182.5857 - MinusLogProbMetric: 182.5857 - val_loss: 180.0317 - val_MinusLogProbMetric: 180.0317 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 136/1000
2023-09-29 10:49:15.538 
Epoch 136/1000 
	 loss: 178.7094, MinusLogProbMetric: 178.7094, val_loss: 177.7702, val_MinusLogProbMetric: 177.7702

Epoch 136: val_loss did not improve from 167.80304
196/196 - 66s - loss: 178.7094 - MinusLogProbMetric: 178.7094 - val_loss: 177.7702 - val_MinusLogProbMetric: 177.7702 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 137/1000
2023-09-29 10:50:20.409 
Epoch 137/1000 
	 loss: 176.4101, MinusLogProbMetric: 176.4101, val_loss: 175.6469, val_MinusLogProbMetric: 175.6469

Epoch 137: val_loss did not improve from 167.80304
196/196 - 65s - loss: 176.4101 - MinusLogProbMetric: 176.4101 - val_loss: 175.6469 - val_MinusLogProbMetric: 175.6469 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 138/1000
2023-09-29 10:51:28.485 
Epoch 138/1000 
	 loss: 174.4468, MinusLogProbMetric: 174.4468, val_loss: 174.0667, val_MinusLogProbMetric: 174.0667

Epoch 138: val_loss did not improve from 167.80304
196/196 - 68s - loss: 174.4468 - MinusLogProbMetric: 174.4468 - val_loss: 174.0667 - val_MinusLogProbMetric: 174.0667 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 139/1000
2023-09-29 10:52:34.572 
Epoch 139/1000 
	 loss: 173.3944, MinusLogProbMetric: 173.3944, val_loss: 173.2903, val_MinusLogProbMetric: 173.2903

Epoch 139: val_loss did not improve from 167.80304
196/196 - 66s - loss: 173.3944 - MinusLogProbMetric: 173.3944 - val_loss: 173.2903 - val_MinusLogProbMetric: 173.2903 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 140/1000
2023-09-29 10:53:41.358 
Epoch 140/1000 
	 loss: 172.4072, MinusLogProbMetric: 172.4072, val_loss: 172.5077, val_MinusLogProbMetric: 172.5077

Epoch 140: val_loss did not improve from 167.80304
196/196 - 67s - loss: 172.4072 - MinusLogProbMetric: 172.4072 - val_loss: 172.5077 - val_MinusLogProbMetric: 172.5077 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 141/1000
2023-09-29 10:54:40.287 
Epoch 141/1000 
	 loss: 171.6255, MinusLogProbMetric: 171.6255, val_loss: 172.0475, val_MinusLogProbMetric: 172.0475

Epoch 141: val_loss did not improve from 167.80304
196/196 - 59s - loss: 171.6255 - MinusLogProbMetric: 171.6255 - val_loss: 172.0475 - val_MinusLogProbMetric: 172.0475 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 142/1000
2023-09-29 10:55:34.508 
Epoch 142/1000 
	 loss: 170.7359, MinusLogProbMetric: 170.7359, val_loss: 170.0381, val_MinusLogProbMetric: 170.0381

Epoch 142: val_loss did not improve from 167.80304
196/196 - 54s - loss: 170.7359 - MinusLogProbMetric: 170.7359 - val_loss: 170.0381 - val_MinusLogProbMetric: 170.0381 - lr: 4.1152e-06 - 54s/epoch - 277ms/step
Epoch 143/1000
2023-09-29 10:56:29.263 
Epoch 143/1000 
	 loss: 169.5097, MinusLogProbMetric: 169.5097, val_loss: 167.4757, val_MinusLogProbMetric: 167.4757

Epoch 143: val_loss improved from 167.80304 to 167.47574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_334/weights/best_weights.h5
196/196 - 56s - loss: 169.5097 - MinusLogProbMetric: 169.5097 - val_loss: 167.4757 - val_MinusLogProbMetric: 167.4757 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 144/1000
2023-09-29 10:57:26.595 
Epoch 144/1000 
	 loss: 333.8855, MinusLogProbMetric: 333.8855, val_loss: 533.0190, val_MinusLogProbMetric: 533.0190

Epoch 144: val_loss did not improve from 167.47574
196/196 - 56s - loss: 333.8855 - MinusLogProbMetric: 333.8855 - val_loss: 533.0190 - val_MinusLogProbMetric: 533.0190 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 145/1000
2023-09-29 10:58:19.779 
Epoch 145/1000 
	 loss: 377.7495, MinusLogProbMetric: 377.7495, val_loss: 312.3481, val_MinusLogProbMetric: 312.3481

Epoch 145: val_loss did not improve from 167.47574
196/196 - 53s - loss: 377.7495 - MinusLogProbMetric: 377.7495 - val_loss: 312.3481 - val_MinusLogProbMetric: 312.3481 - lr: 4.1152e-06 - 53s/epoch - 271ms/step
Epoch 146/1000
2023-09-29 10:59:13.309 
Epoch 146/1000 
	 loss: 297.1148, MinusLogProbMetric: 297.1148, val_loss: 380.5258, val_MinusLogProbMetric: 380.5258

Epoch 146: val_loss did not improve from 167.47574
196/196 - 54s - loss: 297.1148 - MinusLogProbMetric: 297.1148 - val_loss: 380.5258 - val_MinusLogProbMetric: 380.5258 - lr: 4.1152e-06 - 54s/epoch - 273ms/step
Epoch 147/1000
2023-09-29 11:00:11.847 
Epoch 147/1000 
	 loss: 348.4694, MinusLogProbMetric: 348.4694, val_loss: 330.5736, val_MinusLogProbMetric: 330.5736

Epoch 147: val_loss did not improve from 167.47574
196/196 - 59s - loss: 348.4694 - MinusLogProbMetric: 348.4694 - val_loss: 330.5736 - val_MinusLogProbMetric: 330.5736 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 148/1000
2023-09-29 11:01:09.655 
Epoch 148/1000 
	 loss: 305.6672, MinusLogProbMetric: 305.6672, val_loss: 292.5885, val_MinusLogProbMetric: 292.5885

Epoch 148: val_loss did not improve from 167.47574
196/196 - 58s - loss: 305.6672 - MinusLogProbMetric: 305.6672 - val_loss: 292.5885 - val_MinusLogProbMetric: 292.5885 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 149/1000
2023-09-29 11:02:07.289 
Epoch 149/1000 
	 loss: 283.9350, MinusLogProbMetric: 283.9350, val_loss: 277.1091, val_MinusLogProbMetric: 277.1091

Epoch 149: val_loss did not improve from 167.47574
196/196 - 58s - loss: 283.9350 - MinusLogProbMetric: 283.9350 - val_loss: 277.1091 - val_MinusLogProbMetric: 277.1091 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 150/1000
2023-09-29 11:03:05.700 
Epoch 150/1000 
	 loss: 271.9387, MinusLogProbMetric: 271.9387, val_loss: 266.9001, val_MinusLogProbMetric: 266.9001

Epoch 150: val_loss did not improve from 167.47574
196/196 - 58s - loss: 271.9387 - MinusLogProbMetric: 271.9387 - val_loss: 266.9001 - val_MinusLogProbMetric: 266.9001 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 151/1000
2023-09-29 11:04:07.764 
Epoch 151/1000 
	 loss: 262.3700, MinusLogProbMetric: 262.3700, val_loss: 258.8961, val_MinusLogProbMetric: 258.8961

Epoch 151: val_loss did not improve from 167.47574
196/196 - 62s - loss: 262.3700 - MinusLogProbMetric: 262.3700 - val_loss: 258.8961 - val_MinusLogProbMetric: 258.8961 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 152/1000
2023-09-29 11:05:03.526 
Epoch 152/1000 
	 loss: 255.8912, MinusLogProbMetric: 255.8912, val_loss: 253.6129, val_MinusLogProbMetric: 253.6129

Epoch 152: val_loss did not improve from 167.47574
196/196 - 56s - loss: 255.8912 - MinusLogProbMetric: 255.8912 - val_loss: 253.6129 - val_MinusLogProbMetric: 253.6129 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 153/1000
2023-09-29 11:05:59.127 
Epoch 153/1000 
	 loss: 251.0421, MinusLogProbMetric: 251.0421, val_loss: 249.1976, val_MinusLogProbMetric: 249.1976

Epoch 153: val_loss did not improve from 167.47574
196/196 - 56s - loss: 251.0421 - MinusLogProbMetric: 251.0421 - val_loss: 249.1976 - val_MinusLogProbMetric: 249.1976 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 154/1000
2023-09-29 11:06:59.503 
Epoch 154/1000 
	 loss: 247.0315, MinusLogProbMetric: 247.0315, val_loss: 245.6343, val_MinusLogProbMetric: 245.6343

Epoch 154: val_loss did not improve from 167.47574
196/196 - 60s - loss: 247.0315 - MinusLogProbMetric: 247.0315 - val_loss: 245.6343 - val_MinusLogProbMetric: 245.6343 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 155/1000
2023-09-29 11:08:00.814 
Epoch 155/1000 
	 loss: 243.8301, MinusLogProbMetric: 243.8301, val_loss: 242.8045, val_MinusLogProbMetric: 242.8045

Epoch 155: val_loss did not improve from 167.47574
196/196 - 61s - loss: 243.8301 - MinusLogProbMetric: 243.8301 - val_loss: 242.8045 - val_MinusLogProbMetric: 242.8045 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 156/1000
2023-09-29 11:08:56.046 
Epoch 156/1000 
	 loss: 241.2835, MinusLogProbMetric: 241.2835, val_loss: 240.1998, val_MinusLogProbMetric: 240.1998

Epoch 156: val_loss did not improve from 167.47574
196/196 - 55s - loss: 241.2835 - MinusLogProbMetric: 241.2835 - val_loss: 240.1998 - val_MinusLogProbMetric: 240.1998 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 157/1000
2023-09-29 11:09:52.560 
Epoch 157/1000 
	 loss: 238.3515, MinusLogProbMetric: 238.3515, val_loss: 236.9928, val_MinusLogProbMetric: 236.9928

Epoch 157: val_loss did not improve from 167.47574
196/196 - 57s - loss: 238.3515 - MinusLogProbMetric: 238.3515 - val_loss: 236.9928 - val_MinusLogProbMetric: 236.9928 - lr: 4.1152e-06 - 57s/epoch - 288ms/step
Epoch 158/1000
2023-09-29 11:10:50.314 
Epoch 158/1000 
	 loss: 235.5382, MinusLogProbMetric: 235.5382, val_loss: 241.4105, val_MinusLogProbMetric: 241.4105

Epoch 158: val_loss did not improve from 167.47574
196/196 - 58s - loss: 235.5382 - MinusLogProbMetric: 235.5382 - val_loss: 241.4105 - val_MinusLogProbMetric: 241.4105 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 159/1000
2023-09-29 11:11:43.528 
Epoch 159/1000 
	 loss: 234.3193, MinusLogProbMetric: 234.3193, val_loss: 231.6310, val_MinusLogProbMetric: 231.6310

Epoch 159: val_loss did not improve from 167.47574
196/196 - 53s - loss: 234.3193 - MinusLogProbMetric: 234.3193 - val_loss: 231.6310 - val_MinusLogProbMetric: 231.6310 - lr: 4.1152e-06 - 53s/epoch - 271ms/step
Epoch 160/1000
2023-09-29 11:12:39.235 
Epoch 160/1000 
	 loss: 230.0946, MinusLogProbMetric: 230.0946, val_loss: 228.9618, val_MinusLogProbMetric: 228.9618

Epoch 160: val_loss did not improve from 167.47574
196/196 - 56s - loss: 230.0946 - MinusLogProbMetric: 230.0946 - val_loss: 228.9618 - val_MinusLogProbMetric: 228.9618 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 161/1000
2023-09-29 11:13:32.812 
Epoch 161/1000 
	 loss: 227.6108, MinusLogProbMetric: 227.6108, val_loss: 226.6298, val_MinusLogProbMetric: 226.6298

Epoch 161: val_loss did not improve from 167.47574
196/196 - 54s - loss: 227.6108 - MinusLogProbMetric: 227.6108 - val_loss: 226.6298 - val_MinusLogProbMetric: 226.6298 - lr: 4.1152e-06 - 54s/epoch - 273ms/step
Epoch 162/1000
2023-09-29 11:14:29.056 
Epoch 162/1000 
	 loss: 225.6590, MinusLogProbMetric: 225.6590, val_loss: 227.9904, val_MinusLogProbMetric: 227.9904

Epoch 162: val_loss did not improve from 167.47574
196/196 - 56s - loss: 225.6590 - MinusLogProbMetric: 225.6590 - val_loss: 227.9904 - val_MinusLogProbMetric: 227.9904 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 163/1000
2023-09-29 11:15:30.017 
Epoch 163/1000 
	 loss: 232.6842, MinusLogProbMetric: 232.6842, val_loss: 224.0797, val_MinusLogProbMetric: 224.0797

Epoch 163: val_loss did not improve from 167.47574
196/196 - 61s - loss: 232.6842 - MinusLogProbMetric: 232.6842 - val_loss: 224.0797 - val_MinusLogProbMetric: 224.0797 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 164/1000
2023-09-29 11:16:25.886 
Epoch 164/1000 
	 loss: 222.6115, MinusLogProbMetric: 222.6115, val_loss: 221.9166, val_MinusLogProbMetric: 221.9166

Epoch 164: val_loss did not improve from 167.47574
196/196 - 56s - loss: 222.6115 - MinusLogProbMetric: 222.6115 - val_loss: 221.9166 - val_MinusLogProbMetric: 221.9166 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 165/1000
2023-09-29 11:17:19.875 
Epoch 165/1000 
	 loss: 220.7743, MinusLogProbMetric: 220.7743, val_loss: 220.0928, val_MinusLogProbMetric: 220.0928

Epoch 165: val_loss did not improve from 167.47574
196/196 - 54s - loss: 220.7743 - MinusLogProbMetric: 220.7743 - val_loss: 220.0928 - val_MinusLogProbMetric: 220.0928 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 166/1000
2023-09-29 11:18:15.017 
Epoch 166/1000 
	 loss: 219.8285, MinusLogProbMetric: 219.8285, val_loss: 219.7200, val_MinusLogProbMetric: 219.7200

Epoch 166: val_loss did not improve from 167.47574
196/196 - 55s - loss: 219.8285 - MinusLogProbMetric: 219.8285 - val_loss: 219.7200 - val_MinusLogProbMetric: 219.7200 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 167/1000
2023-09-29 11:19:13.387 
Epoch 167/1000 
	 loss: 217.7430, MinusLogProbMetric: 217.7430, val_loss: 216.7783, val_MinusLogProbMetric: 216.7783

Epoch 167: val_loss did not improve from 167.47574
196/196 - 58s - loss: 217.7430 - MinusLogProbMetric: 217.7430 - val_loss: 216.7783 - val_MinusLogProbMetric: 216.7783 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 168/1000
2023-09-29 11:20:11.054 
Epoch 168/1000 
	 loss: 215.9078, MinusLogProbMetric: 215.9078, val_loss: 218.8405, val_MinusLogProbMetric: 218.8405

Epoch 168: val_loss did not improve from 167.47574
196/196 - 58s - loss: 215.9078 - MinusLogProbMetric: 215.9078 - val_loss: 218.8405 - val_MinusLogProbMetric: 218.8405 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 169/1000
2023-09-29 11:21:07.431 
Epoch 169/1000 
	 loss: 214.7250, MinusLogProbMetric: 214.7250, val_loss: 213.7984, val_MinusLogProbMetric: 213.7984

Epoch 169: val_loss did not improve from 167.47574
196/196 - 56s - loss: 214.7250 - MinusLogProbMetric: 214.7250 - val_loss: 213.7984 - val_MinusLogProbMetric: 213.7984 - lr: 4.1152e-06 - 56s/epoch - 288ms/step
Epoch 170/1000
2023-09-29 11:22:03.994 
Epoch 170/1000 
	 loss: 213.0904, MinusLogProbMetric: 213.0904, val_loss: 212.5259, val_MinusLogProbMetric: 212.5259

Epoch 170: val_loss did not improve from 167.47574
196/196 - 57s - loss: 213.0904 - MinusLogProbMetric: 213.0904 - val_loss: 212.5259 - val_MinusLogProbMetric: 212.5259 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 171/1000
2023-09-29 11:22:59.841 
Epoch 171/1000 
	 loss: 211.7832, MinusLogProbMetric: 211.7832, val_loss: 211.5196, val_MinusLogProbMetric: 211.5196

Epoch 171: val_loss did not improve from 167.47574
196/196 - 56s - loss: 211.7832 - MinusLogProbMetric: 211.7832 - val_loss: 211.5196 - val_MinusLogProbMetric: 211.5196 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 172/1000
2023-09-29 11:23:57.505 
Epoch 172/1000 
	 loss: 210.4320, MinusLogProbMetric: 210.4320, val_loss: 210.0328, val_MinusLogProbMetric: 210.0328

Epoch 172: val_loss did not improve from 167.47574
196/196 - 58s - loss: 210.4320 - MinusLogProbMetric: 210.4320 - val_loss: 210.0328 - val_MinusLogProbMetric: 210.0328 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 173/1000
2023-09-29 11:24:51.966 
Epoch 173/1000 
	 loss: 209.5936, MinusLogProbMetric: 209.5936, val_loss: 208.8593, val_MinusLogProbMetric: 208.8593

Epoch 173: val_loss did not improve from 167.47574
196/196 - 54s - loss: 209.5936 - MinusLogProbMetric: 209.5936 - val_loss: 208.8593 - val_MinusLogProbMetric: 208.8593 - lr: 4.1152e-06 - 54s/epoch - 278ms/step
Epoch 174/1000
2023-09-29 11:25:49.557 
Epoch 174/1000 
	 loss: 207.9593, MinusLogProbMetric: 207.9593, val_loss: 207.6698, val_MinusLogProbMetric: 207.6698

Epoch 174: val_loss did not improve from 167.47574
196/196 - 58s - loss: 207.9593 - MinusLogProbMetric: 207.9593 - val_loss: 207.6698 - val_MinusLogProbMetric: 207.6698 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 175/1000
2023-09-29 11:26:49.396 
Epoch 175/1000 
	 loss: 206.8603, MinusLogProbMetric: 206.8603, val_loss: 207.2603, val_MinusLogProbMetric: 207.2603

Epoch 175: val_loss did not improve from 167.47574
196/196 - 60s - loss: 206.8603 - MinusLogProbMetric: 206.8603 - val_loss: 207.2603 - val_MinusLogProbMetric: 207.2603 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 176/1000
2023-09-29 11:27:49.601 
Epoch 176/1000 
	 loss: 205.7778, MinusLogProbMetric: 205.7778, val_loss: 205.2072, val_MinusLogProbMetric: 205.2072

Epoch 176: val_loss did not improve from 167.47574
196/196 - 60s - loss: 205.7778 - MinusLogProbMetric: 205.7778 - val_loss: 205.2072 - val_MinusLogProbMetric: 205.2072 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 177/1000
2023-09-29 11:28:49.057 
Epoch 177/1000 
	 loss: 204.7610, MinusLogProbMetric: 204.7610, val_loss: 204.3461, val_MinusLogProbMetric: 204.3461

Epoch 177: val_loss did not improve from 167.47574
196/196 - 59s - loss: 204.7610 - MinusLogProbMetric: 204.7610 - val_loss: 204.3461 - val_MinusLogProbMetric: 204.3461 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 178/1000
2023-09-29 11:29:48.240 
Epoch 178/1000 
	 loss: 203.4264, MinusLogProbMetric: 203.4264, val_loss: 203.2873, val_MinusLogProbMetric: 203.2873

Epoch 178: val_loss did not improve from 167.47574
196/196 - 59s - loss: 203.4264 - MinusLogProbMetric: 203.4264 - val_loss: 203.2873 - val_MinusLogProbMetric: 203.2873 - lr: 4.1152e-06 - 59s/epoch - 302ms/step
Epoch 179/1000
2023-09-29 11:30:43.485 
Epoch 179/1000 
	 loss: 202.5165, MinusLogProbMetric: 202.5165, val_loss: 202.2997, val_MinusLogProbMetric: 202.2997

Epoch 179: val_loss did not improve from 167.47574
196/196 - 55s - loss: 202.5165 - MinusLogProbMetric: 202.5165 - val_loss: 202.2997 - val_MinusLogProbMetric: 202.2997 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 180/1000
2023-09-29 11:31:40.684 
Epoch 180/1000 
	 loss: 201.8119, MinusLogProbMetric: 201.8119, val_loss: 201.4914, val_MinusLogProbMetric: 201.4914

Epoch 180: val_loss did not improve from 167.47574
196/196 - 57s - loss: 201.8119 - MinusLogProbMetric: 201.8119 - val_loss: 201.4914 - val_MinusLogProbMetric: 201.4914 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 181/1000
2023-09-29 11:32:35.100 
Epoch 181/1000 
	 loss: 201.0747, MinusLogProbMetric: 201.0747, val_loss: 201.4841, val_MinusLogProbMetric: 201.4841

Epoch 181: val_loss did not improve from 167.47574
196/196 - 54s - loss: 201.0747 - MinusLogProbMetric: 201.0747 - val_loss: 201.4841 - val_MinusLogProbMetric: 201.4841 - lr: 4.1152e-06 - 54s/epoch - 278ms/step
Epoch 182/1000
2023-09-29 11:33:30.128 
Epoch 182/1000 
	 loss: 203.0911, MinusLogProbMetric: 203.0911, val_loss: 200.6248, val_MinusLogProbMetric: 200.6248

Epoch 182: val_loss did not improve from 167.47574
196/196 - 55s - loss: 203.0911 - MinusLogProbMetric: 203.0911 - val_loss: 200.6248 - val_MinusLogProbMetric: 200.6248 - lr: 4.1152e-06 - 55s/epoch - 281ms/step
Epoch 183/1000
2023-09-29 11:34:25.821 
Epoch 183/1000 
	 loss: 199.5277, MinusLogProbMetric: 199.5277, val_loss: 199.1099, val_MinusLogProbMetric: 199.1099

Epoch 183: val_loss did not improve from 167.47574
196/196 - 56s - loss: 199.5277 - MinusLogProbMetric: 199.5277 - val_loss: 199.1099 - val_MinusLogProbMetric: 199.1099 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 184/1000
2023-09-29 11:35:20.605 
Epoch 184/1000 
	 loss: 198.4429, MinusLogProbMetric: 198.4429, val_loss: 198.0428, val_MinusLogProbMetric: 198.0428

Epoch 184: val_loss did not improve from 167.47574
196/196 - 55s - loss: 198.4429 - MinusLogProbMetric: 198.4429 - val_loss: 198.0428 - val_MinusLogProbMetric: 198.0428 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 185/1000
2023-09-29 11:36:18.609 
Epoch 185/1000 
	 loss: 197.4472, MinusLogProbMetric: 197.4472, val_loss: 197.1370, val_MinusLogProbMetric: 197.1370

Epoch 185: val_loss did not improve from 167.47574
196/196 - 58s - loss: 197.4472 - MinusLogProbMetric: 197.4472 - val_loss: 197.1370 - val_MinusLogProbMetric: 197.1370 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 186/1000
2023-09-29 11:37:18.598 
Epoch 186/1000 
	 loss: 196.4183, MinusLogProbMetric: 196.4183, val_loss: 196.3040, val_MinusLogProbMetric: 196.3040

Epoch 186: val_loss did not improve from 167.47574
196/196 - 60s - loss: 196.4183 - MinusLogProbMetric: 196.4183 - val_loss: 196.3040 - val_MinusLogProbMetric: 196.3040 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 187/1000
2023-09-29 11:38:17.284 
Epoch 187/1000 
	 loss: 195.5494, MinusLogProbMetric: 195.5494, val_loss: 195.3951, val_MinusLogProbMetric: 195.3951

Epoch 187: val_loss did not improve from 167.47574
196/196 - 59s - loss: 195.5494 - MinusLogProbMetric: 195.5494 - val_loss: 195.3951 - val_MinusLogProbMetric: 195.3951 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 188/1000
2023-09-29 11:39:15.797 
Epoch 188/1000 
	 loss: 194.7035, MinusLogProbMetric: 194.7035, val_loss: 194.5304, val_MinusLogProbMetric: 194.5304

Epoch 188: val_loss did not improve from 167.47574
196/196 - 59s - loss: 194.7035 - MinusLogProbMetric: 194.7035 - val_loss: 194.5304 - val_MinusLogProbMetric: 194.5304 - lr: 4.1152e-06 - 59s/epoch - 299ms/step
Epoch 189/1000
2023-09-29 11:40:12.018 
Epoch 189/1000 
	 loss: 194.0319, MinusLogProbMetric: 194.0319, val_loss: 202.5580, val_MinusLogProbMetric: 202.5580

Epoch 189: val_loss did not improve from 167.47574
196/196 - 56s - loss: 194.0319 - MinusLogProbMetric: 194.0319 - val_loss: 202.5580 - val_MinusLogProbMetric: 202.5580 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 190/1000
2023-09-29 11:41:08.368 
Epoch 190/1000 
	 loss: 196.0272, MinusLogProbMetric: 196.0272, val_loss: 201.4204, val_MinusLogProbMetric: 201.4204

Epoch 190: val_loss did not improve from 167.47574
196/196 - 56s - loss: 196.0272 - MinusLogProbMetric: 196.0272 - val_loss: 201.4204 - val_MinusLogProbMetric: 201.4204 - lr: 4.1152e-06 - 56s/epoch - 287ms/step
Epoch 191/1000
2023-09-29 11:42:06.579 
Epoch 191/1000 
	 loss: 194.4448, MinusLogProbMetric: 194.4448, val_loss: 192.8905, val_MinusLogProbMetric: 192.8905

Epoch 191: val_loss did not improve from 167.47574
196/196 - 58s - loss: 194.4448 - MinusLogProbMetric: 194.4448 - val_loss: 192.8905 - val_MinusLogProbMetric: 192.8905 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 192/1000
2023-09-29 11:43:08.629 
Epoch 192/1000 
	 loss: 191.9712, MinusLogProbMetric: 191.9712, val_loss: 191.8286, val_MinusLogProbMetric: 191.8286

Epoch 192: val_loss did not improve from 167.47574
196/196 - 62s - loss: 191.9712 - MinusLogProbMetric: 191.9712 - val_loss: 191.8286 - val_MinusLogProbMetric: 191.8286 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 193/1000
2023-09-29 11:44:07.378 
Epoch 193/1000 
	 loss: 191.4900, MinusLogProbMetric: 191.4900, val_loss: 190.9490, val_MinusLogProbMetric: 190.9490

Epoch 193: val_loss did not improve from 167.47574
196/196 - 59s - loss: 191.4900 - MinusLogProbMetric: 191.4900 - val_loss: 190.9490 - val_MinusLogProbMetric: 190.9490 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 194/1000
2023-09-29 11:45:05.643 
Epoch 194/1000 
	 loss: 191.3369, MinusLogProbMetric: 191.3369, val_loss: 194.3246, val_MinusLogProbMetric: 194.3246

Epoch 194: val_loss did not improve from 167.47574
196/196 - 58s - loss: 191.3369 - MinusLogProbMetric: 191.3369 - val_loss: 194.3246 - val_MinusLogProbMetric: 194.3246 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 195/1000
2023-09-29 11:46:00.992 
Epoch 195/1000 
	 loss: 191.0251, MinusLogProbMetric: 191.0251, val_loss: 190.1589, val_MinusLogProbMetric: 190.1589

Epoch 195: val_loss did not improve from 167.47574
196/196 - 55s - loss: 191.0251 - MinusLogProbMetric: 191.0251 - val_loss: 190.1589 - val_MinusLogProbMetric: 190.1589 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 196/1000
2023-09-29 11:46:56.916 
Epoch 196/1000 
	 loss: 189.5780, MinusLogProbMetric: 189.5780, val_loss: 189.6594, val_MinusLogProbMetric: 189.6594

Epoch 196: val_loss did not improve from 167.47574
196/196 - 56s - loss: 189.5780 - MinusLogProbMetric: 189.5780 - val_loss: 189.6594 - val_MinusLogProbMetric: 189.6594 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 197/1000
2023-09-29 11:47:52.220 
Epoch 197/1000 
	 loss: 189.1336, MinusLogProbMetric: 189.1336, val_loss: 189.2246, val_MinusLogProbMetric: 189.2246

Epoch 197: val_loss did not improve from 167.47574
196/196 - 55s - loss: 189.1336 - MinusLogProbMetric: 189.1336 - val_loss: 189.2246 - val_MinusLogProbMetric: 189.2246 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 198/1000
2023-09-29 11:48:52.583 
Epoch 198/1000 
	 loss: 188.9943, MinusLogProbMetric: 188.9943, val_loss: 189.0499, val_MinusLogProbMetric: 189.0499

Epoch 198: val_loss did not improve from 167.47574
196/196 - 60s - loss: 188.9943 - MinusLogProbMetric: 188.9943 - val_loss: 189.0499 - val_MinusLogProbMetric: 189.0499 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 199/1000
2023-09-29 11:49:46.359 
Epoch 199/1000 
	 loss: 188.4251, MinusLogProbMetric: 188.4251, val_loss: 188.5392, val_MinusLogProbMetric: 188.5392

Epoch 199: val_loss did not improve from 167.47574
196/196 - 54s - loss: 188.4251 - MinusLogProbMetric: 188.4251 - val_loss: 188.5392 - val_MinusLogProbMetric: 188.5392 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 200/1000
2023-09-29 11:50:40.308 
Epoch 200/1000 
	 loss: 188.1251, MinusLogProbMetric: 188.1251, val_loss: 188.2106, val_MinusLogProbMetric: 188.2106

Epoch 200: val_loss did not improve from 167.47574
196/196 - 54s - loss: 188.1251 - MinusLogProbMetric: 188.1251 - val_loss: 188.2106 - val_MinusLogProbMetric: 188.2106 - lr: 2.0576e-06 - 54s/epoch - 275ms/step
Epoch 201/1000
2023-09-29 11:51:34.966 
Epoch 201/1000 
	 loss: 187.7762, MinusLogProbMetric: 187.7762, val_loss: 187.8421, val_MinusLogProbMetric: 187.8421

Epoch 201: val_loss did not improve from 167.47574
196/196 - 55s - loss: 187.7762 - MinusLogProbMetric: 187.7762 - val_loss: 187.8421 - val_MinusLogProbMetric: 187.8421 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 202/1000
2023-09-29 11:52:29.900 
Epoch 202/1000 
	 loss: 187.6628, MinusLogProbMetric: 187.6628, val_loss: 188.8381, val_MinusLogProbMetric: 188.8381

Epoch 202: val_loss did not improve from 167.47574
196/196 - 55s - loss: 187.6628 - MinusLogProbMetric: 187.6628 - val_loss: 188.8381 - val_MinusLogProbMetric: 188.8381 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 203/1000
2023-09-29 11:53:22.324 
Epoch 203/1000 
	 loss: 187.4444, MinusLogProbMetric: 187.4444, val_loss: 187.3195, val_MinusLogProbMetric: 187.3195

Epoch 203: val_loss did not improve from 167.47574
196/196 - 52s - loss: 187.4444 - MinusLogProbMetric: 187.4444 - val_loss: 187.3195 - val_MinusLogProbMetric: 187.3195 - lr: 2.0576e-06 - 52s/epoch - 267ms/step
Epoch 204/1000
2023-09-29 11:54:17.938 
Epoch 204/1000 
	 loss: 186.8783, MinusLogProbMetric: 186.8783, val_loss: 187.0769, val_MinusLogProbMetric: 187.0769

Epoch 204: val_loss did not improve from 167.47574
196/196 - 56s - loss: 186.8783 - MinusLogProbMetric: 186.8783 - val_loss: 187.0769 - val_MinusLogProbMetric: 187.0769 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 205/1000
2023-09-29 11:55:13.225 
Epoch 205/1000 
	 loss: 186.5155, MinusLogProbMetric: 186.5155, val_loss: 186.6123, val_MinusLogProbMetric: 186.6123

Epoch 205: val_loss did not improve from 167.47574
196/196 - 55s - loss: 186.5155 - MinusLogProbMetric: 186.5155 - val_loss: 186.6123 - val_MinusLogProbMetric: 186.6123 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 206/1000
2023-09-29 11:56:06.729 
Epoch 206/1000 
	 loss: 187.4022, MinusLogProbMetric: 187.4022, val_loss: 186.9272, val_MinusLogProbMetric: 186.9272

Epoch 206: val_loss did not improve from 167.47574
196/196 - 54s - loss: 187.4022 - MinusLogProbMetric: 187.4022 - val_loss: 186.9272 - val_MinusLogProbMetric: 186.9272 - lr: 2.0576e-06 - 54s/epoch - 273ms/step
Epoch 207/1000
2023-09-29 11:57:01.955 
Epoch 207/1000 
	 loss: 186.3901, MinusLogProbMetric: 186.3901, val_loss: 186.0022, val_MinusLogProbMetric: 186.0022

Epoch 207: val_loss did not improve from 167.47574
196/196 - 55s - loss: 186.3901 - MinusLogProbMetric: 186.3901 - val_loss: 186.0022 - val_MinusLogProbMetric: 186.0022 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 208/1000
2023-09-29 11:57:56.922 
Epoch 208/1000 
	 loss: 185.5884, MinusLogProbMetric: 185.5884, val_loss: 185.6454, val_MinusLogProbMetric: 185.6454

Epoch 208: val_loss did not improve from 167.47574
196/196 - 55s - loss: 185.5884 - MinusLogProbMetric: 185.5884 - val_loss: 185.6454 - val_MinusLogProbMetric: 185.6454 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 209/1000
2023-09-29 11:58:54.111 
Epoch 209/1000 
	 loss: 185.1942, MinusLogProbMetric: 185.1942, val_loss: 185.2800, val_MinusLogProbMetric: 185.2800

Epoch 209: val_loss did not improve from 167.47574
196/196 - 57s - loss: 185.1942 - MinusLogProbMetric: 185.1942 - val_loss: 185.2800 - val_MinusLogProbMetric: 185.2800 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 210/1000
2023-09-29 11:59:52.951 
Epoch 210/1000 
	 loss: 184.8555, MinusLogProbMetric: 184.8555, val_loss: 184.9191, val_MinusLogProbMetric: 184.9191

Epoch 210: val_loss did not improve from 167.47574
196/196 - 59s - loss: 184.8555 - MinusLogProbMetric: 184.8555 - val_loss: 184.9191 - val_MinusLogProbMetric: 184.9191 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 211/1000
2023-09-29 12:00:48.640 
Epoch 211/1000 
	 loss: 184.5594, MinusLogProbMetric: 184.5594, val_loss: 184.6903, val_MinusLogProbMetric: 184.6903

Epoch 211: val_loss did not improve from 167.47574
196/196 - 56s - loss: 184.5594 - MinusLogProbMetric: 184.5594 - val_loss: 184.6903 - val_MinusLogProbMetric: 184.6903 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 212/1000
2023-09-29 12:01:44.636 
Epoch 212/1000 
	 loss: 185.4994, MinusLogProbMetric: 185.4994, val_loss: 184.3594, val_MinusLogProbMetric: 184.3594

Epoch 212: val_loss did not improve from 167.47574
196/196 - 56s - loss: 185.4994 - MinusLogProbMetric: 185.4994 - val_loss: 184.3594 - val_MinusLogProbMetric: 184.3594 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 213/1000
2023-09-29 12:02:40.546 
Epoch 213/1000 
	 loss: 183.8750, MinusLogProbMetric: 183.8750, val_loss: 183.9550, val_MinusLogProbMetric: 183.9550

Epoch 213: val_loss did not improve from 167.47574
196/196 - 56s - loss: 183.8750 - MinusLogProbMetric: 183.8750 - val_loss: 183.9550 - val_MinusLogProbMetric: 183.9550 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 214/1000
2023-09-29 12:03:33.850 
Epoch 214/1000 
	 loss: 183.6025, MinusLogProbMetric: 183.6025, val_loss: 183.6496, val_MinusLogProbMetric: 183.6496

Epoch 214: val_loss did not improve from 167.47574
196/196 - 53s - loss: 183.6025 - MinusLogProbMetric: 183.6025 - val_loss: 183.6496 - val_MinusLogProbMetric: 183.6496 - lr: 2.0576e-06 - 53s/epoch - 272ms/step
Epoch 215/1000
2023-09-29 12:04:28.640 
Epoch 215/1000 
	 loss: 183.1732, MinusLogProbMetric: 183.1732, val_loss: 183.3012, val_MinusLogProbMetric: 183.3012

Epoch 215: val_loss did not improve from 167.47574
196/196 - 55s - loss: 183.1732 - MinusLogProbMetric: 183.1732 - val_loss: 183.3012 - val_MinusLogProbMetric: 183.3012 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 216/1000
2023-09-29 12:05:22.941 
Epoch 216/1000 
	 loss: 183.0552, MinusLogProbMetric: 183.0552, val_loss: 183.6707, val_MinusLogProbMetric: 183.6707

Epoch 216: val_loss did not improve from 167.47574
196/196 - 54s - loss: 183.0552 - MinusLogProbMetric: 183.0552 - val_loss: 183.6707 - val_MinusLogProbMetric: 183.6707 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 217/1000
2023-09-29 12:06:18.913 
Epoch 217/1000 
	 loss: 182.6900, MinusLogProbMetric: 182.6900, val_loss: 182.7811, val_MinusLogProbMetric: 182.7811

Epoch 217: val_loss did not improve from 167.47574
196/196 - 56s - loss: 182.6900 - MinusLogProbMetric: 182.6900 - val_loss: 182.7811 - val_MinusLogProbMetric: 182.7811 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 218/1000
2023-09-29 12:07:12.664 
Epoch 218/1000 
	 loss: 189.2150, MinusLogProbMetric: 189.2150, val_loss: 188.9023, val_MinusLogProbMetric: 188.9023

Epoch 218: val_loss did not improve from 167.47574
196/196 - 54s - loss: 189.2150 - MinusLogProbMetric: 189.2150 - val_loss: 188.9023 - val_MinusLogProbMetric: 188.9023 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 219/1000
2023-09-29 12:08:06.671 
Epoch 219/1000 
	 loss: 239.9119, MinusLogProbMetric: 239.9119, val_loss: 199.9139, val_MinusLogProbMetric: 199.9139

Epoch 219: val_loss did not improve from 167.47574
196/196 - 54s - loss: 239.9119 - MinusLogProbMetric: 239.9119 - val_loss: 199.9139 - val_MinusLogProbMetric: 199.9139 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 220/1000
2023-09-29 12:08:59.756 
Epoch 220/1000 
	 loss: 193.0921, MinusLogProbMetric: 193.0921, val_loss: 189.0622, val_MinusLogProbMetric: 189.0622

Epoch 220: val_loss did not improve from 167.47574
196/196 - 53s - loss: 193.0921 - MinusLogProbMetric: 193.0921 - val_loss: 189.0622 - val_MinusLogProbMetric: 189.0622 - lr: 2.0576e-06 - 53s/epoch - 271ms/step
Epoch 221/1000
2023-09-29 12:09:56.475 
Epoch 221/1000 
	 loss: 187.0106, MinusLogProbMetric: 187.0106, val_loss: 186.0310, val_MinusLogProbMetric: 186.0310

Epoch 221: val_loss did not improve from 167.47574
196/196 - 57s - loss: 187.0106 - MinusLogProbMetric: 187.0106 - val_loss: 186.0310 - val_MinusLogProbMetric: 186.0310 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 222/1000
2023-09-29 12:10:52.077 
Epoch 222/1000 
	 loss: 184.9656, MinusLogProbMetric: 184.9656, val_loss: 185.1752, val_MinusLogProbMetric: 185.1752

Epoch 222: val_loss did not improve from 167.47574
196/196 - 56s - loss: 184.9656 - MinusLogProbMetric: 184.9656 - val_loss: 185.1752 - val_MinusLogProbMetric: 185.1752 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 223/1000
2023-09-29 12:11:50.084 
Epoch 223/1000 
	 loss: 184.3739, MinusLogProbMetric: 184.3739, val_loss: 183.9863, val_MinusLogProbMetric: 183.9863

Epoch 223: val_loss did not improve from 167.47574
196/196 - 58s - loss: 184.3739 - MinusLogProbMetric: 184.3739 - val_loss: 183.9863 - val_MinusLogProbMetric: 183.9863 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 224/1000
2023-09-29 12:12:45.610 
Epoch 224/1000 
	 loss: 184.6686, MinusLogProbMetric: 184.6686, val_loss: 183.5433, val_MinusLogProbMetric: 183.5433

Epoch 224: val_loss did not improve from 167.47574
196/196 - 56s - loss: 184.6686 - MinusLogProbMetric: 184.6686 - val_loss: 183.5433 - val_MinusLogProbMetric: 183.5433 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 225/1000
2023-09-29 12:13:40.803 
Epoch 225/1000 
	 loss: 182.9944, MinusLogProbMetric: 182.9944, val_loss: 182.8284, val_MinusLogProbMetric: 182.8284

Epoch 225: val_loss did not improve from 167.47574
196/196 - 55s - loss: 182.9944 - MinusLogProbMetric: 182.9944 - val_loss: 182.8284 - val_MinusLogProbMetric: 182.8284 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 226/1000
2023-09-29 12:14:35.442 
Epoch 226/1000 
	 loss: 182.3414, MinusLogProbMetric: 182.3414, val_loss: 182.3299, val_MinusLogProbMetric: 182.3299

Epoch 226: val_loss did not improve from 167.47574
196/196 - 55s - loss: 182.3414 - MinusLogProbMetric: 182.3414 - val_loss: 182.3299 - val_MinusLogProbMetric: 182.3299 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 227/1000
2023-09-29 12:15:29.063 
Epoch 227/1000 
	 loss: 182.6904, MinusLogProbMetric: 182.6904, val_loss: 184.2343, val_MinusLogProbMetric: 184.2343

Epoch 227: val_loss did not improve from 167.47574
196/196 - 54s - loss: 182.6904 - MinusLogProbMetric: 182.6904 - val_loss: 184.2343 - val_MinusLogProbMetric: 184.2343 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 228/1000
2023-09-29 12:16:22.326 
Epoch 228/1000 
	 loss: 182.0574, MinusLogProbMetric: 182.0574, val_loss: 181.9378, val_MinusLogProbMetric: 181.9378

Epoch 228: val_loss did not improve from 167.47574
196/196 - 53s - loss: 182.0574 - MinusLogProbMetric: 182.0574 - val_loss: 181.9378 - val_MinusLogProbMetric: 181.9378 - lr: 2.0576e-06 - 53s/epoch - 272ms/step
Epoch 229/1000
2023-09-29 12:17:14.394 
Epoch 229/1000 
	 loss: 181.3565, MinusLogProbMetric: 181.3565, val_loss: 181.4474, val_MinusLogProbMetric: 181.4474

Epoch 229: val_loss did not improve from 167.47574
196/196 - 52s - loss: 181.3565 - MinusLogProbMetric: 181.3565 - val_loss: 181.4474 - val_MinusLogProbMetric: 181.4474 - lr: 2.0576e-06 - 52s/epoch - 266ms/step
Epoch 230/1000
2023-09-29 12:18:07.110 
Epoch 230/1000 
	 loss: 182.1674, MinusLogProbMetric: 182.1674, val_loss: 181.4217, val_MinusLogProbMetric: 181.4217

Epoch 230: val_loss did not improve from 167.47574
196/196 - 53s - loss: 182.1674 - MinusLogProbMetric: 182.1674 - val_loss: 181.4217 - val_MinusLogProbMetric: 181.4217 - lr: 2.0576e-06 - 53s/epoch - 269ms/step
Epoch 231/1000
2023-09-29 12:19:02.252 
Epoch 231/1000 
	 loss: 181.0478, MinusLogProbMetric: 181.0478, val_loss: 180.7555, val_MinusLogProbMetric: 180.7555

Epoch 231: val_loss did not improve from 167.47574
196/196 - 55s - loss: 181.0478 - MinusLogProbMetric: 181.0478 - val_loss: 180.7555 - val_MinusLogProbMetric: 180.7555 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 232/1000
2023-09-29 12:19:54.512 
Epoch 232/1000 
	 loss: 180.3026, MinusLogProbMetric: 180.3026, val_loss: 180.3985, val_MinusLogProbMetric: 180.3985

Epoch 232: val_loss did not improve from 167.47574
196/196 - 52s - loss: 180.3026 - MinusLogProbMetric: 180.3026 - val_loss: 180.3985 - val_MinusLogProbMetric: 180.3985 - lr: 2.0576e-06 - 52s/epoch - 267ms/step
Epoch 233/1000
2023-09-29 12:20:48.044 
Epoch 233/1000 
	 loss: 180.0749, MinusLogProbMetric: 180.0749, val_loss: 180.1357, val_MinusLogProbMetric: 180.1357

Epoch 233: val_loss did not improve from 167.47574
196/196 - 54s - loss: 180.0749 - MinusLogProbMetric: 180.0749 - val_loss: 180.1357 - val_MinusLogProbMetric: 180.1357 - lr: 2.0576e-06 - 54s/epoch - 273ms/step
Epoch 234/1000
2023-09-29 12:21:45.405 
Epoch 234/1000 
	 loss: 179.6997, MinusLogProbMetric: 179.6997, val_loss: 179.8290, val_MinusLogProbMetric: 179.8290

Epoch 234: val_loss did not improve from 167.47574
196/196 - 57s - loss: 179.6997 - MinusLogProbMetric: 179.6997 - val_loss: 179.8290 - val_MinusLogProbMetric: 179.8290 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 235/1000
2023-09-29 12:22:40.358 
Epoch 235/1000 
	 loss: 179.4777, MinusLogProbMetric: 179.4777, val_loss: 179.5446, val_MinusLogProbMetric: 179.5446

Epoch 235: val_loss did not improve from 167.47574
196/196 - 55s - loss: 179.4777 - MinusLogProbMetric: 179.4777 - val_loss: 179.5446 - val_MinusLogProbMetric: 179.5446 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 236/1000
2023-09-29 12:23:36.661 
Epoch 236/1000 
	 loss: 179.2545, MinusLogProbMetric: 179.2545, val_loss: 179.3326, val_MinusLogProbMetric: 179.3326

Epoch 236: val_loss did not improve from 167.47574
196/196 - 56s - loss: 179.2545 - MinusLogProbMetric: 179.2545 - val_loss: 179.3326 - val_MinusLogProbMetric: 179.3326 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 237/1000
2023-09-29 12:24:42.415 
Epoch 237/1000 
	 loss: 179.0527, MinusLogProbMetric: 179.0527, val_loss: 179.0691, val_MinusLogProbMetric: 179.0691

Epoch 237: val_loss did not improve from 167.47574
196/196 - 66s - loss: 179.0527 - MinusLogProbMetric: 179.0527 - val_loss: 179.0691 - val_MinusLogProbMetric: 179.0691 - lr: 2.0576e-06 - 66s/epoch - 335ms/step
Epoch 238/1000
2023-09-29 12:25:49.656 
Epoch 238/1000 
	 loss: 178.7071, MinusLogProbMetric: 178.7071, val_loss: 178.8947, val_MinusLogProbMetric: 178.8947

Epoch 238: val_loss did not improve from 167.47574
196/196 - 67s - loss: 178.7071 - MinusLogProbMetric: 178.7071 - val_loss: 178.8947 - val_MinusLogProbMetric: 178.8947 - lr: 2.0576e-06 - 67s/epoch - 343ms/step
Epoch 239/1000
2023-09-29 12:26:56.654 
Epoch 239/1000 
	 loss: 178.5248, MinusLogProbMetric: 178.5248, val_loss: 178.5836, val_MinusLogProbMetric: 178.5836

Epoch 239: val_loss did not improve from 167.47574
196/196 - 67s - loss: 178.5248 - MinusLogProbMetric: 178.5248 - val_loss: 178.5836 - val_MinusLogProbMetric: 178.5836 - lr: 2.0576e-06 - 67s/epoch - 342ms/step
Epoch 240/1000
2023-09-29 12:28:03.496 
Epoch 240/1000 
	 loss: 178.3757, MinusLogProbMetric: 178.3757, val_loss: 178.8901, val_MinusLogProbMetric: 178.8901

Epoch 240: val_loss did not improve from 167.47574
196/196 - 67s - loss: 178.3757 - MinusLogProbMetric: 178.3757 - val_loss: 178.8901 - val_MinusLogProbMetric: 178.8901 - lr: 2.0576e-06 - 67s/epoch - 341ms/step
Epoch 241/1000
2023-09-29 12:29:10.421 
Epoch 241/1000 
	 loss: 178.1370, MinusLogProbMetric: 178.1370, val_loss: 178.3385, val_MinusLogProbMetric: 178.3385

Epoch 241: val_loss did not improve from 167.47574
196/196 - 67s - loss: 178.1370 - MinusLogProbMetric: 178.1370 - val_loss: 178.3385 - val_MinusLogProbMetric: 178.3385 - lr: 2.0576e-06 - 67s/epoch - 341ms/step
Epoch 242/1000
2023-09-29 12:30:16.706 
Epoch 242/1000 
	 loss: 178.1067, MinusLogProbMetric: 178.1067, val_loss: 178.5848, val_MinusLogProbMetric: 178.5848

Epoch 242: val_loss did not improve from 167.47574
196/196 - 66s - loss: 178.1067 - MinusLogProbMetric: 178.1067 - val_loss: 178.5848 - val_MinusLogProbMetric: 178.5848 - lr: 2.0576e-06 - 66s/epoch - 338ms/step
Epoch 243/1000
2023-09-29 12:31:22.516 
Epoch 243/1000 
	 loss: 178.8691, MinusLogProbMetric: 178.8691, val_loss: 180.5850, val_MinusLogProbMetric: 180.5850

Epoch 243: val_loss did not improve from 167.47574
Restoring model weights from the end of the best epoch: 143.
196/196 - 66s - loss: 178.8691 - MinusLogProbMetric: 178.8691 - val_loss: 180.5850 - val_MinusLogProbMetric: 180.5850 - lr: 2.0576e-06 - 66s/epoch - 339ms/step
Epoch 243: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 35.65001072199084 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 15.842610236024484 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 12.715159454965033 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 14.281010444974527 seconds.
Training succeeded with seed 187.
Model trained in 15165.67 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 80.77 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 80.99 s.
===========
Run 334/720 done in 18665.44 s.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

===========
Generating train data for run 337.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_337/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_337/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_337/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_337
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_155"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_156 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7fc917207d30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc304122410>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc304122410>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc0b4525f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc0157523e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc015752950>, <keras.callbacks.ModelCheckpoint object at 0x7fc015752a10>, <keras.callbacks.EarlyStopping object at 0x7fc015752c80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc015752cb0>, <keras.callbacks.TerminateOnNaN object at 0x7fc0157528f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_337/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 337/720 with hyperparameters:
timestamp = 2023-09-29 12:32:49.150014
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 9: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 12:33:53.627 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5371.7222, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 5371.7222 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 64s/epoch - 328ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 337.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_337/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_337/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_337/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_337
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_161"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_162 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7fc2964bee00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc34448bbe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc34448bbe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc084726ec0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc08430a050>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc08430a5c0>, <keras.callbacks.ModelCheckpoint object at 0x7fc08430a680>, <keras.callbacks.EarlyStopping object at 0x7fc08430a8f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc08430a920>, <keras.callbacks.TerminateOnNaN object at 0x7fc08430a560>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_337/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 337/720 with hyperparameters:
timestamp = 2023-09-29 12:33:58.553882
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-09-29 12:35:44.105 
Epoch 1/1000 
	 loss: 1448.2664, MinusLogProbMetric: 1448.2664, val_loss: 348.8628, val_MinusLogProbMetric: 348.8628

Epoch 1: val_loss improved from inf to 348.86276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 106s - loss: 1448.2664 - MinusLogProbMetric: 1448.2664 - val_loss: 348.8628 - val_MinusLogProbMetric: 348.8628 - lr: 3.3333e-04 - 106s/epoch - 540ms/step
Epoch 2/1000
2023-09-29 12:36:20.357 
Epoch 2/1000 
	 loss: 285.4670, MinusLogProbMetric: 285.4670, val_loss: 246.8171, val_MinusLogProbMetric: 246.8171

Epoch 2: val_loss improved from 348.86276 to 246.81708, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 285.4670 - MinusLogProbMetric: 285.4670 - val_loss: 246.8171 - val_MinusLogProbMetric: 246.8171 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 3/1000
2023-09-29 12:36:55.678 
Epoch 3/1000 
	 loss: 217.6924, MinusLogProbMetric: 217.6924, val_loss: 186.1928, val_MinusLogProbMetric: 186.1928

Epoch 3: val_loss improved from 246.81708 to 186.19284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 217.6924 - MinusLogProbMetric: 217.6924 - val_loss: 186.1928 - val_MinusLogProbMetric: 186.1928 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 4/1000
2023-09-29 12:37:31.719 
Epoch 4/1000 
	 loss: 170.1627, MinusLogProbMetric: 170.1627, val_loss: 167.3406, val_MinusLogProbMetric: 167.3406

Epoch 4: val_loss improved from 186.19284 to 167.34061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 170.1627 - MinusLogProbMetric: 170.1627 - val_loss: 167.3406 - val_MinusLogProbMetric: 167.3406 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 5/1000
2023-09-29 12:38:07.779 
Epoch 5/1000 
	 loss: 150.1656, MinusLogProbMetric: 150.1656, val_loss: 139.4185, val_MinusLogProbMetric: 139.4185

Epoch 5: val_loss improved from 167.34061 to 139.41847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 150.1656 - MinusLogProbMetric: 150.1656 - val_loss: 139.4185 - val_MinusLogProbMetric: 139.4185 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 6/1000
2023-09-29 12:38:43.113 
Epoch 6/1000 
	 loss: 133.9167, MinusLogProbMetric: 133.9167, val_loss: 127.7012, val_MinusLogProbMetric: 127.7012

Epoch 6: val_loss improved from 139.41847 to 127.70117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 133.9167 - MinusLogProbMetric: 133.9167 - val_loss: 127.7012 - val_MinusLogProbMetric: 127.7012 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 7/1000
2023-09-29 12:39:17.453 
Epoch 7/1000 
	 loss: 129.1621, MinusLogProbMetric: 129.1621, val_loss: 120.0207, val_MinusLogProbMetric: 120.0207

Epoch 7: val_loss improved from 127.70117 to 120.02067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 129.1621 - MinusLogProbMetric: 129.1621 - val_loss: 120.0207 - val_MinusLogProbMetric: 120.0207 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 8/1000
2023-09-29 12:39:54.034 
Epoch 8/1000 
	 loss: 113.4500, MinusLogProbMetric: 113.4500, val_loss: 109.2483, val_MinusLogProbMetric: 109.2483

Epoch 8: val_loss improved from 120.02067 to 109.24834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 113.4500 - MinusLogProbMetric: 113.4500 - val_loss: 109.2483 - val_MinusLogProbMetric: 109.2483 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 9/1000
2023-09-29 12:40:30.845 
Epoch 9/1000 
	 loss: 104.4434, MinusLogProbMetric: 104.4434, val_loss: 100.2590, val_MinusLogProbMetric: 100.2590

Epoch 9: val_loss improved from 109.24834 to 100.25896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 104.4434 - MinusLogProbMetric: 104.4434 - val_loss: 100.2590 - val_MinusLogProbMetric: 100.2590 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 10/1000
2023-09-29 12:41:07.765 
Epoch 10/1000 
	 loss: 97.7793, MinusLogProbMetric: 97.7793, val_loss: 93.8024, val_MinusLogProbMetric: 93.8024

Epoch 10: val_loss improved from 100.25896 to 93.80239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 97.7793 - MinusLogProbMetric: 97.7793 - val_loss: 93.8024 - val_MinusLogProbMetric: 93.8024 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 11/1000
2023-09-29 12:41:44.504 
Epoch 11/1000 
	 loss: 91.7206, MinusLogProbMetric: 91.7206, val_loss: 89.2217, val_MinusLogProbMetric: 89.2217

Epoch 11: val_loss improved from 93.80239 to 89.22173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 91.7206 - MinusLogProbMetric: 91.7206 - val_loss: 89.2217 - val_MinusLogProbMetric: 89.2217 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 12/1000
2023-09-29 12:42:21.125 
Epoch 12/1000 
	 loss: 86.7887, MinusLogProbMetric: 86.7887, val_loss: 86.9777, val_MinusLogProbMetric: 86.9777

Epoch 12: val_loss improved from 89.22173 to 86.97775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 86.7887 - MinusLogProbMetric: 86.7887 - val_loss: 86.9777 - val_MinusLogProbMetric: 86.9777 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 13/1000
2023-09-29 12:42:57.647 
Epoch 13/1000 
	 loss: 82.8146, MinusLogProbMetric: 82.8146, val_loss: 93.4693, val_MinusLogProbMetric: 93.4693

Epoch 13: val_loss did not improve from 86.97775
196/196 - 36s - loss: 82.8146 - MinusLogProbMetric: 82.8146 - val_loss: 93.4693 - val_MinusLogProbMetric: 93.4693 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 14/1000
2023-09-29 12:43:33.831 
Epoch 14/1000 
	 loss: 80.0899, MinusLogProbMetric: 80.0899, val_loss: 79.5064, val_MinusLogProbMetric: 79.5064

Epoch 14: val_loss improved from 86.97775 to 79.50643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 80.0899 - MinusLogProbMetric: 80.0899 - val_loss: 79.5064 - val_MinusLogProbMetric: 79.5064 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 15/1000
2023-09-29 12:44:10.190 
Epoch 15/1000 
	 loss: 75.9006, MinusLogProbMetric: 75.9006, val_loss: 77.4949, val_MinusLogProbMetric: 77.4949

Epoch 15: val_loss improved from 79.50643 to 77.49490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 75.9006 - MinusLogProbMetric: 75.9006 - val_loss: 77.4949 - val_MinusLogProbMetric: 77.4949 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 16/1000
2023-09-29 12:44:47.199 
Epoch 16/1000 
	 loss: 72.9123, MinusLogProbMetric: 72.9123, val_loss: 72.1132, val_MinusLogProbMetric: 72.1132

Epoch 16: val_loss improved from 77.49490 to 72.11317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 72.9123 - MinusLogProbMetric: 72.9123 - val_loss: 72.1132 - val_MinusLogProbMetric: 72.1132 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 17/1000
2023-09-29 12:45:24.049 
Epoch 17/1000 
	 loss: 70.3212, MinusLogProbMetric: 70.3212, val_loss: 70.1211, val_MinusLogProbMetric: 70.1211

Epoch 17: val_loss improved from 72.11317 to 70.12108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 70.3212 - MinusLogProbMetric: 70.3212 - val_loss: 70.1211 - val_MinusLogProbMetric: 70.1211 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 18/1000
2023-09-29 12:46:01.319 
Epoch 18/1000 
	 loss: 68.1683, MinusLogProbMetric: 68.1683, val_loss: 67.1740, val_MinusLogProbMetric: 67.1740

Epoch 18: val_loss improved from 70.12108 to 67.17397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 68.1683 - MinusLogProbMetric: 68.1683 - val_loss: 67.1740 - val_MinusLogProbMetric: 67.1740 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 19/1000
2023-09-29 12:46:37.906 
Epoch 19/1000 
	 loss: 66.1280, MinusLogProbMetric: 66.1280, val_loss: 64.9416, val_MinusLogProbMetric: 64.9416

Epoch 19: val_loss improved from 67.17397 to 64.94164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 66.1280 - MinusLogProbMetric: 66.1280 - val_loss: 64.9416 - val_MinusLogProbMetric: 64.9416 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 20/1000
2023-09-29 12:47:14.892 
Epoch 20/1000 
	 loss: 64.0732, MinusLogProbMetric: 64.0732, val_loss: 62.7664, val_MinusLogProbMetric: 62.7664

Epoch 20: val_loss improved from 64.94164 to 62.76640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 64.0732 - MinusLogProbMetric: 64.0732 - val_loss: 62.7664 - val_MinusLogProbMetric: 62.7664 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 21/1000
2023-09-29 12:47:51.413 
Epoch 21/1000 
	 loss: 62.4753, MinusLogProbMetric: 62.4753, val_loss: 62.5557, val_MinusLogProbMetric: 62.5557

Epoch 21: val_loss improved from 62.76640 to 62.55569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 62.4753 - MinusLogProbMetric: 62.4753 - val_loss: 62.5557 - val_MinusLogProbMetric: 62.5557 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 22/1000
2023-09-29 12:48:28.122 
Epoch 22/1000 
	 loss: 61.0176, MinusLogProbMetric: 61.0176, val_loss: 62.1716, val_MinusLogProbMetric: 62.1716

Epoch 22: val_loss improved from 62.55569 to 62.17156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 61.0176 - MinusLogProbMetric: 61.0176 - val_loss: 62.1716 - val_MinusLogProbMetric: 62.1716 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 23/1000
2023-09-29 12:49:04.770 
Epoch 23/1000 
	 loss: 59.4676, MinusLogProbMetric: 59.4676, val_loss: 61.3609, val_MinusLogProbMetric: 61.3609

Epoch 23: val_loss improved from 62.17156 to 61.36088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 59.4676 - MinusLogProbMetric: 59.4676 - val_loss: 61.3609 - val_MinusLogProbMetric: 61.3609 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 24/1000
2023-09-29 12:49:41.157 
Epoch 24/1000 
	 loss: 58.1052, MinusLogProbMetric: 58.1052, val_loss: 57.7547, val_MinusLogProbMetric: 57.7547

Epoch 24: val_loss improved from 61.36088 to 57.75465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 58.1052 - MinusLogProbMetric: 58.1052 - val_loss: 57.7547 - val_MinusLogProbMetric: 57.7547 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 25/1000
2023-09-29 12:50:17.882 
Epoch 25/1000 
	 loss: 56.7272, MinusLogProbMetric: 56.7272, val_loss: 56.8635, val_MinusLogProbMetric: 56.8635

Epoch 25: val_loss improved from 57.75465 to 56.86353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 56.7272 - MinusLogProbMetric: 56.7272 - val_loss: 56.8635 - val_MinusLogProbMetric: 56.8635 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 26/1000
2023-09-29 12:50:54.222 
Epoch 26/1000 
	 loss: 55.7968, MinusLogProbMetric: 55.7968, val_loss: 54.8689, val_MinusLogProbMetric: 54.8689

Epoch 26: val_loss improved from 56.86353 to 54.86894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 55.7968 - MinusLogProbMetric: 55.7968 - val_loss: 54.8689 - val_MinusLogProbMetric: 54.8689 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 27/1000
2023-09-29 12:51:30.712 
Epoch 27/1000 
	 loss: 54.5797, MinusLogProbMetric: 54.5797, val_loss: 54.9249, val_MinusLogProbMetric: 54.9249

Epoch 27: val_loss did not improve from 54.86894
196/196 - 36s - loss: 54.5797 - MinusLogProbMetric: 54.5797 - val_loss: 54.9249 - val_MinusLogProbMetric: 54.9249 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 28/1000
2023-09-29 12:52:06.754 
Epoch 28/1000 
	 loss: 53.6101, MinusLogProbMetric: 53.6101, val_loss: 53.9218, val_MinusLogProbMetric: 53.9218

Epoch 28: val_loss improved from 54.86894 to 53.92175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 53.6101 - MinusLogProbMetric: 53.6101 - val_loss: 53.9218 - val_MinusLogProbMetric: 53.9218 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 29/1000
2023-09-29 12:52:43.444 
Epoch 29/1000 
	 loss: 52.8592, MinusLogProbMetric: 52.8592, val_loss: 52.7534, val_MinusLogProbMetric: 52.7534

Epoch 29: val_loss improved from 53.92175 to 52.75335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 52.8592 - MinusLogProbMetric: 52.8592 - val_loss: 52.7534 - val_MinusLogProbMetric: 52.7534 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 30/1000
2023-09-29 12:53:20.081 
Epoch 30/1000 
	 loss: 51.7843, MinusLogProbMetric: 51.7843, val_loss: 51.4473, val_MinusLogProbMetric: 51.4473

Epoch 30: val_loss improved from 52.75335 to 51.44726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 51.7843 - MinusLogProbMetric: 51.7843 - val_loss: 51.4473 - val_MinusLogProbMetric: 51.4473 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 31/1000
2023-09-29 12:53:56.865 
Epoch 31/1000 
	 loss: 51.1765, MinusLogProbMetric: 51.1765, val_loss: 51.7565, val_MinusLogProbMetric: 51.7565

Epoch 31: val_loss did not improve from 51.44726
196/196 - 36s - loss: 51.1765 - MinusLogProbMetric: 51.1765 - val_loss: 51.7565 - val_MinusLogProbMetric: 51.7565 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 32/1000
2023-09-29 12:54:32.573 
Epoch 32/1000 
	 loss: 50.4053, MinusLogProbMetric: 50.4053, val_loss: 51.1036, val_MinusLogProbMetric: 51.1036

Epoch 32: val_loss improved from 51.44726 to 51.10355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 50.4053 - MinusLogProbMetric: 50.4053 - val_loss: 51.1036 - val_MinusLogProbMetric: 51.1036 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 33/1000
2023-09-29 12:55:09.099 
Epoch 33/1000 
	 loss: 69.2013, MinusLogProbMetric: 69.2013, val_loss: 55.8184, val_MinusLogProbMetric: 55.8184

Epoch 33: val_loss did not improve from 51.10355
196/196 - 36s - loss: 69.2013 - MinusLogProbMetric: 69.2013 - val_loss: 55.8184 - val_MinusLogProbMetric: 55.8184 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 34/1000
2023-09-29 12:55:45.020 
Epoch 34/1000 
	 loss: 53.7983, MinusLogProbMetric: 53.7983, val_loss: 54.0400, val_MinusLogProbMetric: 54.0400

Epoch 34: val_loss did not improve from 51.10355
196/196 - 36s - loss: 53.7983 - MinusLogProbMetric: 53.7983 - val_loss: 54.0400 - val_MinusLogProbMetric: 54.0400 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 35/1000
2023-09-29 12:56:20.060 
Epoch 35/1000 
	 loss: 51.6218, MinusLogProbMetric: 51.6218, val_loss: 51.2792, val_MinusLogProbMetric: 51.2792

Epoch 35: val_loss did not improve from 51.10355
196/196 - 35s - loss: 51.6218 - MinusLogProbMetric: 51.6218 - val_loss: 51.2792 - val_MinusLogProbMetric: 51.2792 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 36/1000
2023-09-29 12:56:55.769 
Epoch 36/1000 
	 loss: 50.1232, MinusLogProbMetric: 50.1232, val_loss: 49.9222, val_MinusLogProbMetric: 49.9222

Epoch 36: val_loss improved from 51.10355 to 49.92220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 50.1232 - MinusLogProbMetric: 50.1232 - val_loss: 49.9222 - val_MinusLogProbMetric: 49.9222 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 37/1000
2023-09-29 12:57:32.267 
Epoch 37/1000 
	 loss: 48.8018, MinusLogProbMetric: 48.8018, val_loss: 48.8865, val_MinusLogProbMetric: 48.8865

Epoch 37: val_loss improved from 49.92220 to 48.88646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 48.8018 - MinusLogProbMetric: 48.8018 - val_loss: 48.8865 - val_MinusLogProbMetric: 48.8865 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 38/1000
2023-09-29 12:58:08.926 
Epoch 38/1000 
	 loss: 48.1563, MinusLogProbMetric: 48.1563, val_loss: 48.0084, val_MinusLogProbMetric: 48.0084

Epoch 38: val_loss improved from 48.88646 to 48.00841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 48.1563 - MinusLogProbMetric: 48.1563 - val_loss: 48.0084 - val_MinusLogProbMetric: 48.0084 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 39/1000
2023-09-29 12:58:45.751 
Epoch 39/1000 
	 loss: 47.5021, MinusLogProbMetric: 47.5021, val_loss: 47.6935, val_MinusLogProbMetric: 47.6935

Epoch 39: val_loss improved from 48.00841 to 47.69347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 47.5021 - MinusLogProbMetric: 47.5021 - val_loss: 47.6935 - val_MinusLogProbMetric: 47.6935 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 40/1000
2023-09-29 12:59:22.189 
Epoch 40/1000 
	 loss: 47.4727, MinusLogProbMetric: 47.4727, val_loss: 47.5147, val_MinusLogProbMetric: 47.5147

Epoch 40: val_loss improved from 47.69347 to 47.51466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 47.4727 - MinusLogProbMetric: 47.4727 - val_loss: 47.5147 - val_MinusLogProbMetric: 47.5147 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 41/1000
2023-09-29 12:59:58.852 
Epoch 41/1000 
	 loss: 46.3746, MinusLogProbMetric: 46.3746, val_loss: 46.3775, val_MinusLogProbMetric: 46.3775

Epoch 41: val_loss improved from 47.51466 to 46.37751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 46.3746 - MinusLogProbMetric: 46.3746 - val_loss: 46.3775 - val_MinusLogProbMetric: 46.3775 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 42/1000
2023-09-29 13:00:35.518 
Epoch 42/1000 
	 loss: 45.8960, MinusLogProbMetric: 45.8960, val_loss: 45.7101, val_MinusLogProbMetric: 45.7101

Epoch 42: val_loss improved from 46.37751 to 45.71013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 45.8960 - MinusLogProbMetric: 45.8960 - val_loss: 45.7101 - val_MinusLogProbMetric: 45.7101 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 43/1000
2023-09-29 13:01:12.063 
Epoch 43/1000 
	 loss: 45.3794, MinusLogProbMetric: 45.3794, val_loss: 45.8815, val_MinusLogProbMetric: 45.8815

Epoch 43: val_loss did not improve from 45.71013
196/196 - 36s - loss: 45.3794 - MinusLogProbMetric: 45.3794 - val_loss: 45.8815 - val_MinusLogProbMetric: 45.8815 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 44/1000
2023-09-29 13:01:47.887 
Epoch 44/1000 
	 loss: 44.9570, MinusLogProbMetric: 44.9570, val_loss: 47.1013, val_MinusLogProbMetric: 47.1013

Epoch 44: val_loss did not improve from 45.71013
196/196 - 36s - loss: 44.9570 - MinusLogProbMetric: 44.9570 - val_loss: 47.1013 - val_MinusLogProbMetric: 47.1013 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 45/1000
2023-09-29 13:02:23.956 
Epoch 45/1000 
	 loss: 44.6152, MinusLogProbMetric: 44.6152, val_loss: 44.4075, val_MinusLogProbMetric: 44.4075

Epoch 45: val_loss improved from 45.71013 to 44.40753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 44.6152 - MinusLogProbMetric: 44.6152 - val_loss: 44.4075 - val_MinusLogProbMetric: 44.4075 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 46/1000
2023-09-29 13:03:00.633 
Epoch 46/1000 
	 loss: 44.2146, MinusLogProbMetric: 44.2146, val_loss: 44.1572, val_MinusLogProbMetric: 44.1572

Epoch 46: val_loss improved from 44.40753 to 44.15724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 44.2146 - MinusLogProbMetric: 44.2146 - val_loss: 44.1572 - val_MinusLogProbMetric: 44.1572 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 47/1000
2023-09-29 13:03:37.899 
Epoch 47/1000 
	 loss: 43.9371, MinusLogProbMetric: 43.9371, val_loss: 43.8768, val_MinusLogProbMetric: 43.8768

Epoch 47: val_loss improved from 44.15724 to 43.87677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 43.9371 - MinusLogProbMetric: 43.9371 - val_loss: 43.8768 - val_MinusLogProbMetric: 43.8768 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 48/1000
2023-09-29 13:04:14.630 
Epoch 48/1000 
	 loss: 43.5354, MinusLogProbMetric: 43.5354, val_loss: 43.9913, val_MinusLogProbMetric: 43.9913

Epoch 48: val_loss did not improve from 43.87677
196/196 - 36s - loss: 43.5354 - MinusLogProbMetric: 43.5354 - val_loss: 43.9913 - val_MinusLogProbMetric: 43.9913 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 49/1000
2023-09-29 13:04:50.555 
Epoch 49/1000 
	 loss: 43.3846, MinusLogProbMetric: 43.3846, val_loss: 43.9005, val_MinusLogProbMetric: 43.9005

Epoch 49: val_loss did not improve from 43.87677
196/196 - 36s - loss: 43.3846 - MinusLogProbMetric: 43.3846 - val_loss: 43.9005 - val_MinusLogProbMetric: 43.9005 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 50/1000
2023-09-29 13:05:26.291 
Epoch 50/1000 
	 loss: 43.0568, MinusLogProbMetric: 43.0568, val_loss: 42.6479, val_MinusLogProbMetric: 42.6479

Epoch 50: val_loss improved from 43.87677 to 42.64790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 43.0568 - MinusLogProbMetric: 43.0568 - val_loss: 42.6479 - val_MinusLogProbMetric: 42.6479 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 51/1000
2023-09-29 13:06:03.266 
Epoch 51/1000 
	 loss: 43.2039, MinusLogProbMetric: 43.2039, val_loss: 43.5332, val_MinusLogProbMetric: 43.5332

Epoch 51: val_loss did not improve from 42.64790
196/196 - 36s - loss: 43.2039 - MinusLogProbMetric: 43.2039 - val_loss: 43.5332 - val_MinusLogProbMetric: 43.5332 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 52/1000
2023-09-29 13:06:38.989 
Epoch 52/1000 
	 loss: 42.5101, MinusLogProbMetric: 42.5101, val_loss: 42.4040, val_MinusLogProbMetric: 42.4040

Epoch 52: val_loss improved from 42.64790 to 42.40398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 42.5101 - MinusLogProbMetric: 42.5101 - val_loss: 42.4040 - val_MinusLogProbMetric: 42.4040 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 53/1000
2023-09-29 13:07:15.297 
Epoch 53/1000 
	 loss: 42.2172, MinusLogProbMetric: 42.2172, val_loss: 42.9030, val_MinusLogProbMetric: 42.9030

Epoch 53: val_loss did not improve from 42.40398
196/196 - 36s - loss: 42.2172 - MinusLogProbMetric: 42.2172 - val_loss: 42.9030 - val_MinusLogProbMetric: 42.9030 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 54/1000
2023-09-29 13:07:51.109 
Epoch 54/1000 
	 loss: 41.9158, MinusLogProbMetric: 41.9158, val_loss: 42.5869, val_MinusLogProbMetric: 42.5869

Epoch 54: val_loss did not improve from 42.40398
196/196 - 36s - loss: 41.9158 - MinusLogProbMetric: 41.9158 - val_loss: 42.5869 - val_MinusLogProbMetric: 42.5869 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 55/1000
2023-09-29 13:08:26.770 
Epoch 55/1000 
	 loss: 41.7298, MinusLogProbMetric: 41.7298, val_loss: 41.6324, val_MinusLogProbMetric: 41.6324

Epoch 55: val_loss improved from 42.40398 to 41.63238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 41.7298 - MinusLogProbMetric: 41.7298 - val_loss: 41.6324 - val_MinusLogProbMetric: 41.6324 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 56/1000
2023-09-29 13:09:03.201 
Epoch 56/1000 
	 loss: 41.5750, MinusLogProbMetric: 41.5750, val_loss: 42.1656, val_MinusLogProbMetric: 42.1656

Epoch 56: val_loss did not improve from 41.63238
196/196 - 36s - loss: 41.5750 - MinusLogProbMetric: 41.5750 - val_loss: 42.1656 - val_MinusLogProbMetric: 42.1656 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 57/1000
2023-09-29 13:09:38.976 
Epoch 57/1000 
	 loss: 41.0999, MinusLogProbMetric: 41.0999, val_loss: 42.1801, val_MinusLogProbMetric: 42.1801

Epoch 57: val_loss did not improve from 41.63238
196/196 - 36s - loss: 41.0999 - MinusLogProbMetric: 41.0999 - val_loss: 42.1801 - val_MinusLogProbMetric: 42.1801 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 58/1000
2023-09-29 13:10:15.010 
Epoch 58/1000 
	 loss: 40.8345, MinusLogProbMetric: 40.8345, val_loss: 40.9569, val_MinusLogProbMetric: 40.9569

Epoch 58: val_loss improved from 41.63238 to 40.95693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 40.8345 - MinusLogProbMetric: 40.8345 - val_loss: 40.9569 - val_MinusLogProbMetric: 40.9569 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 59/1000
2023-09-29 13:10:50.966 
Epoch 59/1000 
	 loss: 40.7070, MinusLogProbMetric: 40.7070, val_loss: 41.7083, val_MinusLogProbMetric: 41.7083

Epoch 59: val_loss did not improve from 40.95693
196/196 - 35s - loss: 40.7070 - MinusLogProbMetric: 40.7070 - val_loss: 41.7083 - val_MinusLogProbMetric: 41.7083 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 60/1000
2023-09-29 13:11:26.924 
Epoch 60/1000 
	 loss: 40.6360, MinusLogProbMetric: 40.6360, val_loss: 40.1175, val_MinusLogProbMetric: 40.1175

Epoch 60: val_loss improved from 40.95693 to 40.11752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 40.6360 - MinusLogProbMetric: 40.6360 - val_loss: 40.1175 - val_MinusLogProbMetric: 40.1175 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 61/1000
2023-09-29 13:12:03.352 
Epoch 61/1000 
	 loss: 40.4410, MinusLogProbMetric: 40.4410, val_loss: 40.6032, val_MinusLogProbMetric: 40.6032

Epoch 61: val_loss did not improve from 40.11752
196/196 - 36s - loss: 40.4410 - MinusLogProbMetric: 40.4410 - val_loss: 40.6032 - val_MinusLogProbMetric: 40.6032 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 62/1000
2023-09-29 13:12:39.210 
Epoch 62/1000 
	 loss: 41.8681, MinusLogProbMetric: 41.8681, val_loss: 40.8891, val_MinusLogProbMetric: 40.8891

Epoch 62: val_loss did not improve from 40.11752
196/196 - 36s - loss: 41.8681 - MinusLogProbMetric: 41.8681 - val_loss: 40.8891 - val_MinusLogProbMetric: 40.8891 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 63/1000
2023-09-29 13:13:14.808 
Epoch 63/1000 
	 loss: 40.0640, MinusLogProbMetric: 40.0640, val_loss: 40.3960, val_MinusLogProbMetric: 40.3960

Epoch 63: val_loss did not improve from 40.11752
196/196 - 36s - loss: 40.0640 - MinusLogProbMetric: 40.0640 - val_loss: 40.3960 - val_MinusLogProbMetric: 40.3960 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 64/1000
2023-09-29 13:13:50.797 
Epoch 64/1000 
	 loss: 39.6532, MinusLogProbMetric: 39.6532, val_loss: 39.9595, val_MinusLogProbMetric: 39.9595

Epoch 64: val_loss improved from 40.11752 to 39.95951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 39.6532 - MinusLogProbMetric: 39.6532 - val_loss: 39.9595 - val_MinusLogProbMetric: 39.9595 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 65/1000
2023-09-29 13:14:27.609 
Epoch 65/1000 
	 loss: 39.5204, MinusLogProbMetric: 39.5204, val_loss: 40.0264, val_MinusLogProbMetric: 40.0264

Epoch 65: val_loss did not improve from 39.95951
196/196 - 36s - loss: 39.5204 - MinusLogProbMetric: 39.5204 - val_loss: 40.0264 - val_MinusLogProbMetric: 40.0264 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 66/1000
2023-09-29 13:15:03.464 
Epoch 66/1000 
	 loss: 39.3387, MinusLogProbMetric: 39.3387, val_loss: 40.8722, val_MinusLogProbMetric: 40.8722

Epoch 66: val_loss did not improve from 39.95951
196/196 - 36s - loss: 39.3387 - MinusLogProbMetric: 39.3387 - val_loss: 40.8722 - val_MinusLogProbMetric: 40.8722 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 67/1000
2023-09-29 13:15:39.445 
Epoch 67/1000 
	 loss: 39.3388, MinusLogProbMetric: 39.3388, val_loss: 39.3744, val_MinusLogProbMetric: 39.3744

Epoch 67: val_loss improved from 39.95951 to 39.37445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 39.3388 - MinusLogProbMetric: 39.3388 - val_loss: 39.3744 - val_MinusLogProbMetric: 39.3744 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 68/1000
2023-09-29 13:16:16.500 
Epoch 68/1000 
	 loss: 39.3271, MinusLogProbMetric: 39.3271, val_loss: 39.1601, val_MinusLogProbMetric: 39.1601

Epoch 68: val_loss improved from 39.37445 to 39.16007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 39.3271 - MinusLogProbMetric: 39.3271 - val_loss: 39.1601 - val_MinusLogProbMetric: 39.1601 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 69/1000
2023-09-29 13:16:53.337 
Epoch 69/1000 
	 loss: 38.7479, MinusLogProbMetric: 38.7479, val_loss: 38.8410, val_MinusLogProbMetric: 38.8410

Epoch 69: val_loss improved from 39.16007 to 38.84095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 38.7479 - MinusLogProbMetric: 38.7479 - val_loss: 38.8410 - val_MinusLogProbMetric: 38.8410 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 70/1000
2023-09-29 13:17:29.963 
Epoch 70/1000 
	 loss: 39.9227, MinusLogProbMetric: 39.9227, val_loss: 39.8059, val_MinusLogProbMetric: 39.8059

Epoch 70: val_loss did not improve from 38.84095
196/196 - 36s - loss: 39.9227 - MinusLogProbMetric: 39.9227 - val_loss: 39.8059 - val_MinusLogProbMetric: 39.8059 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 71/1000
2023-09-29 13:18:05.285 
Epoch 71/1000 
	 loss: 38.6801, MinusLogProbMetric: 38.6801, val_loss: 39.1157, val_MinusLogProbMetric: 39.1157

Epoch 71: val_loss did not improve from 38.84095
196/196 - 35s - loss: 38.6801 - MinusLogProbMetric: 38.6801 - val_loss: 39.1157 - val_MinusLogProbMetric: 39.1157 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 72/1000
2023-09-29 13:18:41.203 
Epoch 72/1000 
	 loss: 38.5303, MinusLogProbMetric: 38.5303, val_loss: 39.0661, val_MinusLogProbMetric: 39.0661

Epoch 72: val_loss did not improve from 38.84095
196/196 - 36s - loss: 38.5303 - MinusLogProbMetric: 38.5303 - val_loss: 39.0661 - val_MinusLogProbMetric: 39.0661 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 73/1000
2023-09-29 13:19:16.842 
Epoch 73/1000 
	 loss: 38.4224, MinusLogProbMetric: 38.4224, val_loss: 39.4321, val_MinusLogProbMetric: 39.4321

Epoch 73: val_loss did not improve from 38.84095
196/196 - 36s - loss: 38.4224 - MinusLogProbMetric: 38.4224 - val_loss: 39.4321 - val_MinusLogProbMetric: 39.4321 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 74/1000
2023-09-29 13:19:52.832 
Epoch 74/1000 
	 loss: 38.6237, MinusLogProbMetric: 38.6237, val_loss: 38.7120, val_MinusLogProbMetric: 38.7120

Epoch 74: val_loss improved from 38.84095 to 38.71196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 38.6237 - MinusLogProbMetric: 38.6237 - val_loss: 38.7120 - val_MinusLogProbMetric: 38.7120 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 75/1000
2023-09-29 13:20:29.261 
Epoch 75/1000 
	 loss: 39.1870, MinusLogProbMetric: 39.1870, val_loss: 38.5322, val_MinusLogProbMetric: 38.5322

Epoch 75: val_loss improved from 38.71196 to 38.53225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 39.1870 - MinusLogProbMetric: 39.1870 - val_loss: 38.5322 - val_MinusLogProbMetric: 38.5322 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 76/1000
2023-09-29 13:21:06.539 
Epoch 76/1000 
	 loss: 38.1078, MinusLogProbMetric: 38.1078, val_loss: 39.0790, val_MinusLogProbMetric: 39.0790

Epoch 76: val_loss did not improve from 38.53225
196/196 - 36s - loss: 38.1078 - MinusLogProbMetric: 38.1078 - val_loss: 39.0790 - val_MinusLogProbMetric: 39.0790 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 77/1000
2023-09-29 13:21:42.343 
Epoch 77/1000 
	 loss: 37.6520, MinusLogProbMetric: 37.6520, val_loss: 39.5843, val_MinusLogProbMetric: 39.5843

Epoch 77: val_loss did not improve from 38.53225
196/196 - 36s - loss: 37.6520 - MinusLogProbMetric: 37.6520 - val_loss: 39.5843 - val_MinusLogProbMetric: 39.5843 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 78/1000
2023-09-29 13:22:18.345 
Epoch 78/1000 
	 loss: 38.1354, MinusLogProbMetric: 38.1354, val_loss: 38.7623, val_MinusLogProbMetric: 38.7623

Epoch 78: val_loss did not improve from 38.53225
196/196 - 36s - loss: 38.1354 - MinusLogProbMetric: 38.1354 - val_loss: 38.7623 - val_MinusLogProbMetric: 38.7623 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 79/1000
2023-09-29 13:22:54.224 
Epoch 79/1000 
	 loss: 38.6297, MinusLogProbMetric: 38.6297, val_loss: 38.6456, val_MinusLogProbMetric: 38.6456

Epoch 79: val_loss did not improve from 38.53225
196/196 - 36s - loss: 38.6297 - MinusLogProbMetric: 38.6297 - val_loss: 38.6456 - val_MinusLogProbMetric: 38.6456 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 80/1000
2023-09-29 13:23:29.835 
Epoch 80/1000 
	 loss: 37.7187, MinusLogProbMetric: 37.7187, val_loss: 37.8598, val_MinusLogProbMetric: 37.8598

Epoch 80: val_loss improved from 38.53225 to 37.85984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 37.7187 - MinusLogProbMetric: 37.7187 - val_loss: 37.8598 - val_MinusLogProbMetric: 37.8598 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 81/1000
2023-09-29 13:24:06.800 
Epoch 81/1000 
	 loss: 37.3238, MinusLogProbMetric: 37.3238, val_loss: 37.0520, val_MinusLogProbMetric: 37.0520

Epoch 81: val_loss improved from 37.85984 to 37.05203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 37.3238 - MinusLogProbMetric: 37.3238 - val_loss: 37.0520 - val_MinusLogProbMetric: 37.0520 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 82/1000
2023-09-29 13:24:43.408 
Epoch 82/1000 
	 loss: 37.3895, MinusLogProbMetric: 37.3895, val_loss: 37.3681, val_MinusLogProbMetric: 37.3681

Epoch 82: val_loss did not improve from 37.05203
196/196 - 36s - loss: 37.3895 - MinusLogProbMetric: 37.3895 - val_loss: 37.3681 - val_MinusLogProbMetric: 37.3681 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 83/1000
2023-09-29 13:25:19.421 
Epoch 83/1000 
	 loss: 37.3292, MinusLogProbMetric: 37.3292, val_loss: 37.7906, val_MinusLogProbMetric: 37.7906

Epoch 83: val_loss did not improve from 37.05203
196/196 - 36s - loss: 37.3292 - MinusLogProbMetric: 37.3292 - val_loss: 37.7906 - val_MinusLogProbMetric: 37.7906 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 84/1000
2023-09-29 13:25:55.200 
Epoch 84/1000 
	 loss: 37.1364, MinusLogProbMetric: 37.1364, val_loss: 38.1581, val_MinusLogProbMetric: 38.1581

Epoch 84: val_loss did not improve from 37.05203
196/196 - 36s - loss: 37.1364 - MinusLogProbMetric: 37.1364 - val_loss: 38.1581 - val_MinusLogProbMetric: 38.1581 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 85/1000
2023-09-29 13:26:30.487 
Epoch 85/1000 
	 loss: 37.0180, MinusLogProbMetric: 37.0180, val_loss: 37.1511, val_MinusLogProbMetric: 37.1511

Epoch 85: val_loss did not improve from 37.05203
196/196 - 35s - loss: 37.0180 - MinusLogProbMetric: 37.0180 - val_loss: 37.1511 - val_MinusLogProbMetric: 37.1511 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 86/1000
2023-09-29 13:27:06.285 
Epoch 86/1000 
	 loss: 36.7393, MinusLogProbMetric: 36.7393, val_loss: 37.1267, val_MinusLogProbMetric: 37.1267

Epoch 86: val_loss did not improve from 37.05203
196/196 - 36s - loss: 36.7393 - MinusLogProbMetric: 36.7393 - val_loss: 37.1267 - val_MinusLogProbMetric: 37.1267 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 87/1000
2023-09-29 13:27:42.249 
Epoch 87/1000 
	 loss: 36.6916, MinusLogProbMetric: 36.6916, val_loss: 41.0631, val_MinusLogProbMetric: 41.0631

Epoch 87: val_loss did not improve from 37.05203
196/196 - 36s - loss: 36.6916 - MinusLogProbMetric: 36.6916 - val_loss: 41.0631 - val_MinusLogProbMetric: 41.0631 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 88/1000
2023-09-29 13:28:17.812 
Epoch 88/1000 
	 loss: 36.9927, MinusLogProbMetric: 36.9927, val_loss: 37.5570, val_MinusLogProbMetric: 37.5570

Epoch 88: val_loss did not improve from 37.05203
196/196 - 36s - loss: 36.9927 - MinusLogProbMetric: 36.9927 - val_loss: 37.5570 - val_MinusLogProbMetric: 37.5570 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 89/1000
2023-09-29 13:28:53.851 
Epoch 89/1000 
	 loss: 37.1053, MinusLogProbMetric: 37.1053, val_loss: 37.2866, val_MinusLogProbMetric: 37.2866

Epoch 89: val_loss did not improve from 37.05203
196/196 - 36s - loss: 37.1053 - MinusLogProbMetric: 37.1053 - val_loss: 37.2866 - val_MinusLogProbMetric: 37.2866 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 90/1000
2023-09-29 13:29:29.840 
Epoch 90/1000 
	 loss: 36.7976, MinusLogProbMetric: 36.7976, val_loss: 38.0383, val_MinusLogProbMetric: 38.0383

Epoch 90: val_loss did not improve from 37.05203
196/196 - 36s - loss: 36.7976 - MinusLogProbMetric: 36.7976 - val_loss: 38.0383 - val_MinusLogProbMetric: 38.0383 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 91/1000
2023-09-29 13:30:05.576 
Epoch 91/1000 
	 loss: 36.5095, MinusLogProbMetric: 36.5095, val_loss: 37.1503, val_MinusLogProbMetric: 37.1503

Epoch 91: val_loss did not improve from 37.05203
196/196 - 36s - loss: 36.5095 - MinusLogProbMetric: 36.5095 - val_loss: 37.1503 - val_MinusLogProbMetric: 37.1503 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 92/1000
2023-09-29 13:30:41.406 
Epoch 92/1000 
	 loss: 37.6273, MinusLogProbMetric: 37.6273, val_loss: 38.6147, val_MinusLogProbMetric: 38.6147

Epoch 92: val_loss did not improve from 37.05203
196/196 - 36s - loss: 37.6273 - MinusLogProbMetric: 37.6273 - val_loss: 38.6147 - val_MinusLogProbMetric: 38.6147 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 93/1000
2023-09-29 13:31:17.380 
Epoch 93/1000 
	 loss: 36.3668, MinusLogProbMetric: 36.3668, val_loss: 36.3395, val_MinusLogProbMetric: 36.3395

Epoch 93: val_loss improved from 37.05203 to 36.33949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 36.3668 - MinusLogProbMetric: 36.3668 - val_loss: 36.3395 - val_MinusLogProbMetric: 36.3395 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 94/1000
2023-09-29 13:31:54.082 
Epoch 94/1000 
	 loss: 36.1306, MinusLogProbMetric: 36.1306, val_loss: 36.2078, val_MinusLogProbMetric: 36.2078

Epoch 94: val_loss improved from 36.33949 to 36.20784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 36.1306 - MinusLogProbMetric: 36.1306 - val_loss: 36.2078 - val_MinusLogProbMetric: 36.2078 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 95/1000
2023-09-29 13:32:30.643 
Epoch 95/1000 
	 loss: 37.1431, MinusLogProbMetric: 37.1431, val_loss: 35.7779, val_MinusLogProbMetric: 35.7779

Epoch 95: val_loss improved from 36.20784 to 35.77791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 37.1431 - MinusLogProbMetric: 37.1431 - val_loss: 35.7779 - val_MinusLogProbMetric: 35.7779 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 96/1000
2023-09-29 13:33:07.250 
Epoch 96/1000 
	 loss: 36.2118, MinusLogProbMetric: 36.2118, val_loss: 36.7788, val_MinusLogProbMetric: 36.7788

Epoch 96: val_loss did not improve from 35.77791
196/196 - 36s - loss: 36.2118 - MinusLogProbMetric: 36.2118 - val_loss: 36.7788 - val_MinusLogProbMetric: 36.7788 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 97/1000
2023-09-29 13:33:42.922 
Epoch 97/1000 
	 loss: 36.0987, MinusLogProbMetric: 36.0987, val_loss: 35.9805, val_MinusLogProbMetric: 35.9805

Epoch 97: val_loss did not improve from 35.77791
196/196 - 36s - loss: 36.0987 - MinusLogProbMetric: 36.0987 - val_loss: 35.9805 - val_MinusLogProbMetric: 35.9805 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 98/1000
2023-09-29 13:34:19.067 
Epoch 98/1000 
	 loss: 36.0788, MinusLogProbMetric: 36.0788, val_loss: 36.3986, val_MinusLogProbMetric: 36.3986

Epoch 98: val_loss did not improve from 35.77791
196/196 - 36s - loss: 36.0788 - MinusLogProbMetric: 36.0788 - val_loss: 36.3986 - val_MinusLogProbMetric: 36.3986 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 99/1000
2023-09-29 13:34:54.719 
Epoch 99/1000 
	 loss: 37.5508, MinusLogProbMetric: 37.5508, val_loss: 36.2046, val_MinusLogProbMetric: 36.2046

Epoch 99: val_loss did not improve from 35.77791
196/196 - 36s - loss: 37.5508 - MinusLogProbMetric: 37.5508 - val_loss: 36.2046 - val_MinusLogProbMetric: 36.2046 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 100/1000
2023-09-29 13:35:29.763 
Epoch 100/1000 
	 loss: 36.0667, MinusLogProbMetric: 36.0667, val_loss: 35.9839, val_MinusLogProbMetric: 35.9839

Epoch 100: val_loss did not improve from 35.77791
196/196 - 35s - loss: 36.0667 - MinusLogProbMetric: 36.0667 - val_loss: 35.9839 - val_MinusLogProbMetric: 35.9839 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 101/1000
2023-09-29 13:36:05.663 
Epoch 101/1000 
	 loss: 35.7034, MinusLogProbMetric: 35.7034, val_loss: 36.5492, val_MinusLogProbMetric: 36.5492

Epoch 101: val_loss did not improve from 35.77791
196/196 - 36s - loss: 35.7034 - MinusLogProbMetric: 35.7034 - val_loss: 36.5492 - val_MinusLogProbMetric: 36.5492 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 102/1000
2023-09-29 13:36:41.885 
Epoch 102/1000 
	 loss: 35.7188, MinusLogProbMetric: 35.7188, val_loss: 36.3353, val_MinusLogProbMetric: 36.3353

Epoch 102: val_loss did not improve from 35.77791
196/196 - 36s - loss: 35.7188 - MinusLogProbMetric: 35.7188 - val_loss: 36.3353 - val_MinusLogProbMetric: 36.3353 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 103/1000
2023-09-29 13:37:17.724 
Epoch 103/1000 
	 loss: 37.4034, MinusLogProbMetric: 37.4034, val_loss: 44.4881, val_MinusLogProbMetric: 44.4881

Epoch 103: val_loss did not improve from 35.77791
196/196 - 36s - loss: 37.4034 - MinusLogProbMetric: 37.4034 - val_loss: 44.4881 - val_MinusLogProbMetric: 44.4881 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 104/1000
2023-09-29 13:37:53.419 
Epoch 104/1000 
	 loss: 36.0951, MinusLogProbMetric: 36.0951, val_loss: 35.7131, val_MinusLogProbMetric: 35.7131

Epoch 104: val_loss improved from 35.77791 to 35.71314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 36.0951 - MinusLogProbMetric: 36.0951 - val_loss: 35.7131 - val_MinusLogProbMetric: 35.7131 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 105/1000
2023-09-29 13:38:30.003 
Epoch 105/1000 
	 loss: 35.3961, MinusLogProbMetric: 35.3961, val_loss: 37.6540, val_MinusLogProbMetric: 37.6540

Epoch 105: val_loss did not improve from 35.71314
196/196 - 36s - loss: 35.3961 - MinusLogProbMetric: 35.3961 - val_loss: 37.6540 - val_MinusLogProbMetric: 37.6540 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 106/1000
2023-09-29 13:39:06.065 
Epoch 106/1000 
	 loss: 35.2516, MinusLogProbMetric: 35.2516, val_loss: 35.6100, val_MinusLogProbMetric: 35.6100

Epoch 106: val_loss improved from 35.71314 to 35.60999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 37s - loss: 35.2516 - MinusLogProbMetric: 35.2516 - val_loss: 35.6100 - val_MinusLogProbMetric: 35.6100 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 107/1000
2023-09-29 13:39:39.724 
Epoch 107/1000 
	 loss: 35.3503, MinusLogProbMetric: 35.3503, val_loss: 35.8620, val_MinusLogProbMetric: 35.8620

Epoch 107: val_loss did not improve from 35.60999
196/196 - 32s - loss: 35.3503 - MinusLogProbMetric: 35.3503 - val_loss: 35.8620 - val_MinusLogProbMetric: 35.8620 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 108/1000
2023-09-29 13:40:13.117 
Epoch 108/1000 
	 loss: 35.7259, MinusLogProbMetric: 35.7259, val_loss: 35.3760, val_MinusLogProbMetric: 35.3760

Epoch 108: val_loss improved from 35.60999 to 35.37603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 35.7259 - MinusLogProbMetric: 35.7259 - val_loss: 35.3760 - val_MinusLogProbMetric: 35.3760 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 109/1000
2023-09-29 13:40:44.083 
Epoch 109/1000 
	 loss: 35.1845, MinusLogProbMetric: 35.1845, val_loss: 35.7712, val_MinusLogProbMetric: 35.7712

Epoch 109: val_loss did not improve from 35.37603
196/196 - 30s - loss: 35.1845 - MinusLogProbMetric: 35.1845 - val_loss: 35.7712 - val_MinusLogProbMetric: 35.7712 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 110/1000
2023-09-29 13:41:14.490 
Epoch 110/1000 
	 loss: 35.5910, MinusLogProbMetric: 35.5910, val_loss: 34.9890, val_MinusLogProbMetric: 34.9890

Epoch 110: val_loss improved from 35.37603 to 34.98895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 31s - loss: 35.5910 - MinusLogProbMetric: 35.5910 - val_loss: 34.9890 - val_MinusLogProbMetric: 34.9890 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 111/1000
2023-09-29 13:41:46.491 
Epoch 111/1000 
	 loss: 35.1727, MinusLogProbMetric: 35.1727, val_loss: 37.1075, val_MinusLogProbMetric: 37.1075

Epoch 111: val_loss did not improve from 34.98895
196/196 - 31s - loss: 35.1727 - MinusLogProbMetric: 35.1727 - val_loss: 37.1075 - val_MinusLogProbMetric: 37.1075 - lr: 3.3333e-04 - 31s/epoch - 161ms/step
Epoch 112/1000
2023-09-29 13:42:18.866 
Epoch 112/1000 
	 loss: 35.3503, MinusLogProbMetric: 35.3503, val_loss: 35.4681, val_MinusLogProbMetric: 35.4681

Epoch 112: val_loss did not improve from 34.98895
196/196 - 32s - loss: 35.3503 - MinusLogProbMetric: 35.3503 - val_loss: 35.4681 - val_MinusLogProbMetric: 35.4681 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 113/1000
2023-09-29 13:42:50.239 
Epoch 113/1000 
	 loss: 34.8472, MinusLogProbMetric: 34.8472, val_loss: 35.8259, val_MinusLogProbMetric: 35.8259

Epoch 113: val_loss did not improve from 34.98895
196/196 - 31s - loss: 34.8472 - MinusLogProbMetric: 34.8472 - val_loss: 35.8259 - val_MinusLogProbMetric: 35.8259 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 114/1000
2023-09-29 13:43:21.796 
Epoch 114/1000 
	 loss: 35.3775, MinusLogProbMetric: 35.3775, val_loss: 35.1428, val_MinusLogProbMetric: 35.1428

Epoch 114: val_loss did not improve from 34.98895
196/196 - 32s - loss: 35.3775 - MinusLogProbMetric: 35.3775 - val_loss: 35.1428 - val_MinusLogProbMetric: 35.1428 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 115/1000
2023-09-29 13:43:53.091 
Epoch 115/1000 
	 loss: 35.0331, MinusLogProbMetric: 35.0331, val_loss: 35.5229, val_MinusLogProbMetric: 35.5229

Epoch 115: val_loss did not improve from 34.98895
196/196 - 31s - loss: 35.0331 - MinusLogProbMetric: 35.0331 - val_loss: 35.5229 - val_MinusLogProbMetric: 35.5229 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 116/1000
2023-09-29 13:44:24.718 
Epoch 116/1000 
	 loss: 35.8442, MinusLogProbMetric: 35.8442, val_loss: 36.3249, val_MinusLogProbMetric: 36.3249

Epoch 116: val_loss did not improve from 34.98895
196/196 - 32s - loss: 35.8442 - MinusLogProbMetric: 35.8442 - val_loss: 36.3249 - val_MinusLogProbMetric: 36.3249 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 117/1000
2023-09-29 13:44:56.745 
Epoch 117/1000 
	 loss: 34.8914, MinusLogProbMetric: 34.8914, val_loss: 35.2568, val_MinusLogProbMetric: 35.2568

Epoch 117: val_loss did not improve from 34.98895
196/196 - 32s - loss: 34.8914 - MinusLogProbMetric: 34.8914 - val_loss: 35.2568 - val_MinusLogProbMetric: 35.2568 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 118/1000
2023-09-29 13:45:29.078 
Epoch 118/1000 
	 loss: 34.7356, MinusLogProbMetric: 34.7356, val_loss: 35.4212, val_MinusLogProbMetric: 35.4212

Epoch 118: val_loss did not improve from 34.98895
196/196 - 32s - loss: 34.7356 - MinusLogProbMetric: 34.7356 - val_loss: 35.4212 - val_MinusLogProbMetric: 35.4212 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 119/1000
2023-09-29 13:46:03.236 
Epoch 119/1000 
	 loss: 34.8067, MinusLogProbMetric: 34.8067, val_loss: 35.5632, val_MinusLogProbMetric: 35.5632

Epoch 119: val_loss did not improve from 34.98895
196/196 - 34s - loss: 34.8067 - MinusLogProbMetric: 34.8067 - val_loss: 35.5632 - val_MinusLogProbMetric: 35.5632 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 120/1000
2023-09-29 13:46:37.457 
Epoch 120/1000 
	 loss: 34.6765, MinusLogProbMetric: 34.6765, val_loss: 34.9035, val_MinusLogProbMetric: 34.9035

Epoch 120: val_loss improved from 34.98895 to 34.90347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 34.6765 - MinusLogProbMetric: 34.6765 - val_loss: 34.9035 - val_MinusLogProbMetric: 34.9035 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 121/1000
2023-09-29 13:47:11.688 
Epoch 121/1000 
	 loss: 34.8967, MinusLogProbMetric: 34.8967, val_loss: 34.9367, val_MinusLogProbMetric: 34.9367

Epoch 121: val_loss did not improve from 34.90347
196/196 - 34s - loss: 34.8967 - MinusLogProbMetric: 34.8967 - val_loss: 34.9367 - val_MinusLogProbMetric: 34.9367 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 122/1000
2023-09-29 13:47:44.900 
Epoch 122/1000 
	 loss: 34.8077, MinusLogProbMetric: 34.8077, val_loss: 34.9580, val_MinusLogProbMetric: 34.9580

Epoch 122: val_loss did not improve from 34.90347
196/196 - 33s - loss: 34.8077 - MinusLogProbMetric: 34.8077 - val_loss: 34.9580 - val_MinusLogProbMetric: 34.9580 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 123/1000
2023-09-29 13:48:20.046 
Epoch 123/1000 
	 loss: 34.6197, MinusLogProbMetric: 34.6197, val_loss: 35.4908, val_MinusLogProbMetric: 35.4908

Epoch 123: val_loss did not improve from 34.90347
196/196 - 35s - loss: 34.6197 - MinusLogProbMetric: 34.6197 - val_loss: 35.4908 - val_MinusLogProbMetric: 35.4908 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 124/1000
2023-09-29 13:48:54.630 
Epoch 124/1000 
	 loss: 34.7225, MinusLogProbMetric: 34.7225, val_loss: 34.8750, val_MinusLogProbMetric: 34.8750

Epoch 124: val_loss improved from 34.90347 to 34.87495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 34.7225 - MinusLogProbMetric: 34.7225 - val_loss: 34.8750 - val_MinusLogProbMetric: 34.8750 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 125/1000
2023-09-29 13:49:29.647 
Epoch 125/1000 
	 loss: 34.7767, MinusLogProbMetric: 34.7767, val_loss: 35.1406, val_MinusLogProbMetric: 35.1406

Epoch 125: val_loss did not improve from 34.87495
196/196 - 34s - loss: 34.7767 - MinusLogProbMetric: 34.7767 - val_loss: 35.1406 - val_MinusLogProbMetric: 35.1406 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 126/1000
2023-09-29 13:50:04.156 
Epoch 126/1000 
	 loss: 34.2913, MinusLogProbMetric: 34.2913, val_loss: 35.3314, val_MinusLogProbMetric: 35.3314

Epoch 126: val_loss did not improve from 34.87495
196/196 - 35s - loss: 34.2913 - MinusLogProbMetric: 34.2913 - val_loss: 35.3314 - val_MinusLogProbMetric: 35.3314 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 127/1000
2023-09-29 13:50:38.440 
Epoch 127/1000 
	 loss: 34.4121, MinusLogProbMetric: 34.4121, val_loss: 34.6127, val_MinusLogProbMetric: 34.6127

Epoch 127: val_loss improved from 34.87495 to 34.61266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 34.4121 - MinusLogProbMetric: 34.4121 - val_loss: 34.6127 - val_MinusLogProbMetric: 34.6127 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 128/1000
2023-09-29 13:51:13.607 
Epoch 128/1000 
	 loss: 34.5306, MinusLogProbMetric: 34.5306, val_loss: 34.8407, val_MinusLogProbMetric: 34.8407

Epoch 128: val_loss did not improve from 34.61266
196/196 - 34s - loss: 34.5306 - MinusLogProbMetric: 34.5306 - val_loss: 34.8407 - val_MinusLogProbMetric: 34.8407 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 129/1000
2023-09-29 13:51:44.785 
Epoch 129/1000 
	 loss: 34.4209, MinusLogProbMetric: 34.4209, val_loss: 34.9458, val_MinusLogProbMetric: 34.9458

Epoch 129: val_loss did not improve from 34.61266
196/196 - 31s - loss: 34.4209 - MinusLogProbMetric: 34.4209 - val_loss: 34.9458 - val_MinusLogProbMetric: 34.9458 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 130/1000
2023-09-29 13:52:19.080 
Epoch 130/1000 
	 loss: 34.4945, MinusLogProbMetric: 34.4945, val_loss: 34.5505, val_MinusLogProbMetric: 34.5505

Epoch 130: val_loss improved from 34.61266 to 34.55048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 34.4945 - MinusLogProbMetric: 34.4945 - val_loss: 34.5505 - val_MinusLogProbMetric: 34.5505 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 131/1000
2023-09-29 13:52:54.275 
Epoch 131/1000 
	 loss: 34.6512, MinusLogProbMetric: 34.6512, val_loss: 34.0679, val_MinusLogProbMetric: 34.0679

Epoch 131: val_loss improved from 34.55048 to 34.06789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 34.6512 - MinusLogProbMetric: 34.6512 - val_loss: 34.0679 - val_MinusLogProbMetric: 34.0679 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 132/1000
2023-09-29 13:53:29.409 
Epoch 132/1000 
	 loss: 34.4151, MinusLogProbMetric: 34.4151, val_loss: 33.9388, val_MinusLogProbMetric: 33.9388

Epoch 132: val_loss improved from 34.06789 to 33.93877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 34.4151 - MinusLogProbMetric: 34.4151 - val_loss: 33.9388 - val_MinusLogProbMetric: 33.9388 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 133/1000
2023-09-29 13:54:05.132 
Epoch 133/1000 
	 loss: 34.1977, MinusLogProbMetric: 34.1977, val_loss: 34.5615, val_MinusLogProbMetric: 34.5615

Epoch 133: val_loss did not improve from 33.93877
196/196 - 35s - loss: 34.1977 - MinusLogProbMetric: 34.1977 - val_loss: 34.5615 - val_MinusLogProbMetric: 34.5615 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 134/1000
2023-09-29 13:54:39.482 
Epoch 134/1000 
	 loss: 34.4798, MinusLogProbMetric: 34.4798, val_loss: 36.3944, val_MinusLogProbMetric: 36.3944

Epoch 134: val_loss did not improve from 33.93877
196/196 - 34s - loss: 34.4798 - MinusLogProbMetric: 34.4798 - val_loss: 36.3944 - val_MinusLogProbMetric: 36.3944 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 135/1000
2023-09-29 13:55:14.602 
Epoch 135/1000 
	 loss: 34.5508, MinusLogProbMetric: 34.5508, val_loss: 34.1765, val_MinusLogProbMetric: 34.1765

Epoch 135: val_loss did not improve from 33.93877
196/196 - 35s - loss: 34.5508 - MinusLogProbMetric: 34.5508 - val_loss: 34.1765 - val_MinusLogProbMetric: 34.1765 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 136/1000
2023-09-29 13:55:49.447 
Epoch 136/1000 
	 loss: 34.1516, MinusLogProbMetric: 34.1516, val_loss: 34.5613, val_MinusLogProbMetric: 34.5613

Epoch 136: val_loss did not improve from 33.93877
196/196 - 35s - loss: 34.1516 - MinusLogProbMetric: 34.1516 - val_loss: 34.5613 - val_MinusLogProbMetric: 34.5613 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 137/1000
2023-09-29 13:56:23.981 
Epoch 137/1000 
	 loss: 33.9325, MinusLogProbMetric: 33.9325, val_loss: 33.9434, val_MinusLogProbMetric: 33.9434

Epoch 137: val_loss did not improve from 33.93877
196/196 - 35s - loss: 33.9325 - MinusLogProbMetric: 33.9325 - val_loss: 33.9434 - val_MinusLogProbMetric: 33.9434 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 138/1000
2023-09-29 13:56:58.127 
Epoch 138/1000 
	 loss: 33.9123, MinusLogProbMetric: 33.9123, val_loss: 35.2836, val_MinusLogProbMetric: 35.2836

Epoch 138: val_loss did not improve from 33.93877
196/196 - 34s - loss: 33.9123 - MinusLogProbMetric: 33.9123 - val_loss: 35.2836 - val_MinusLogProbMetric: 35.2836 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 139/1000
2023-09-29 13:57:32.924 
Epoch 139/1000 
	 loss: 34.3581, MinusLogProbMetric: 34.3581, val_loss: 33.8502, val_MinusLogProbMetric: 33.8502

Epoch 139: val_loss improved from 33.93877 to 33.85016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 34.3581 - MinusLogProbMetric: 34.3581 - val_loss: 33.8502 - val_MinusLogProbMetric: 33.8502 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 140/1000
2023-09-29 13:58:07.619 
Epoch 140/1000 
	 loss: 34.1442, MinusLogProbMetric: 34.1442, val_loss: 34.7648, val_MinusLogProbMetric: 34.7648

Epoch 140: val_loss did not improve from 33.85016
196/196 - 34s - loss: 34.1442 - MinusLogProbMetric: 34.1442 - val_loss: 34.7648 - val_MinusLogProbMetric: 34.7648 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 141/1000
2023-09-29 13:58:42.138 
Epoch 141/1000 
	 loss: 34.1068, MinusLogProbMetric: 34.1068, val_loss: 33.9405, val_MinusLogProbMetric: 33.9405

Epoch 141: val_loss did not improve from 33.85016
196/196 - 35s - loss: 34.1068 - MinusLogProbMetric: 34.1068 - val_loss: 33.9405 - val_MinusLogProbMetric: 33.9405 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 142/1000
2023-09-29 13:59:16.465 
Epoch 142/1000 
	 loss: 33.8149, MinusLogProbMetric: 33.8149, val_loss: 34.8864, val_MinusLogProbMetric: 34.8864

Epoch 142: val_loss did not improve from 33.85016
196/196 - 34s - loss: 33.8149 - MinusLogProbMetric: 33.8149 - val_loss: 34.8864 - val_MinusLogProbMetric: 34.8864 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 143/1000
2023-09-29 13:59:49.508 
Epoch 143/1000 
	 loss: 34.1428, MinusLogProbMetric: 34.1428, val_loss: 33.7068, val_MinusLogProbMetric: 33.7068

Epoch 143: val_loss improved from 33.85016 to 33.70683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 34.1428 - MinusLogProbMetric: 34.1428 - val_loss: 33.7068 - val_MinusLogProbMetric: 33.7068 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 144/1000
2023-09-29 14:00:23.047 
Epoch 144/1000 
	 loss: 34.2263, MinusLogProbMetric: 34.2263, val_loss: 34.6663, val_MinusLogProbMetric: 34.6663

Epoch 144: val_loss did not improve from 33.70683
196/196 - 33s - loss: 34.2263 - MinusLogProbMetric: 34.2263 - val_loss: 34.6663 - val_MinusLogProbMetric: 34.6663 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 145/1000
2023-09-29 14:00:55.597 
Epoch 145/1000 
	 loss: 33.8555, MinusLogProbMetric: 33.8555, val_loss: 34.3801, val_MinusLogProbMetric: 34.3801

Epoch 145: val_loss did not improve from 33.70683
196/196 - 33s - loss: 33.8555 - MinusLogProbMetric: 33.8555 - val_loss: 34.3801 - val_MinusLogProbMetric: 34.3801 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 146/1000
2023-09-29 14:01:29.186 
Epoch 146/1000 
	 loss: 33.7969, MinusLogProbMetric: 33.7969, val_loss: 33.8721, val_MinusLogProbMetric: 33.8721

Epoch 146: val_loss did not improve from 33.70683
196/196 - 34s - loss: 33.7969 - MinusLogProbMetric: 33.7969 - val_loss: 33.8721 - val_MinusLogProbMetric: 33.8721 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 147/1000
2023-09-29 14:02:02.250 
Epoch 147/1000 
	 loss: 33.7957, MinusLogProbMetric: 33.7957, val_loss: 33.5653, val_MinusLogProbMetric: 33.5653

Epoch 147: val_loss improved from 33.70683 to 33.56532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 33.7957 - MinusLogProbMetric: 33.7957 - val_loss: 33.5653 - val_MinusLogProbMetric: 33.5653 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 148/1000
2023-09-29 14:02:37.977 
Epoch 148/1000 
	 loss: 33.5354, MinusLogProbMetric: 33.5354, val_loss: 33.7586, val_MinusLogProbMetric: 33.7586

Epoch 148: val_loss did not improve from 33.56532
196/196 - 35s - loss: 33.5354 - MinusLogProbMetric: 33.5354 - val_loss: 33.7586 - val_MinusLogProbMetric: 33.7586 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 149/1000
2023-09-29 14:03:11.468 
Epoch 149/1000 
	 loss: 33.4415, MinusLogProbMetric: 33.4415, val_loss: 35.4400, val_MinusLogProbMetric: 35.4400

Epoch 149: val_loss did not improve from 33.56532
196/196 - 33s - loss: 33.4415 - MinusLogProbMetric: 33.4415 - val_loss: 35.4400 - val_MinusLogProbMetric: 35.4400 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 150/1000
2023-09-29 14:03:46.339 
Epoch 150/1000 
	 loss: 33.9032, MinusLogProbMetric: 33.9032, val_loss: 34.9234, val_MinusLogProbMetric: 34.9234

Epoch 150: val_loss did not improve from 33.56532
196/196 - 35s - loss: 33.9032 - MinusLogProbMetric: 33.9032 - val_loss: 34.9234 - val_MinusLogProbMetric: 34.9234 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 151/1000
2023-09-29 14:04:21.121 
Epoch 151/1000 
	 loss: 33.9174, MinusLogProbMetric: 33.9174, val_loss: 33.5558, val_MinusLogProbMetric: 33.5558

Epoch 151: val_loss improved from 33.56532 to 33.55577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 33.9174 - MinusLogProbMetric: 33.9174 - val_loss: 33.5558 - val_MinusLogProbMetric: 33.5558 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 152/1000
2023-09-29 14:04:56.627 
Epoch 152/1000 
	 loss: 33.5214, MinusLogProbMetric: 33.5214, val_loss: 34.7087, val_MinusLogProbMetric: 34.7087

Epoch 152: val_loss did not improve from 33.55577
196/196 - 35s - loss: 33.5214 - MinusLogProbMetric: 33.5214 - val_loss: 34.7087 - val_MinusLogProbMetric: 34.7087 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 153/1000
2023-09-29 14:05:30.909 
Epoch 153/1000 
	 loss: 33.6542, MinusLogProbMetric: 33.6542, val_loss: 34.4362, val_MinusLogProbMetric: 34.4362

Epoch 153: val_loss did not improve from 33.55577
196/196 - 34s - loss: 33.6542 - MinusLogProbMetric: 33.6542 - val_loss: 34.4362 - val_MinusLogProbMetric: 34.4362 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 154/1000
2023-09-29 14:06:03.601 
Epoch 154/1000 
	 loss: 33.6101, MinusLogProbMetric: 33.6101, val_loss: 33.8695, val_MinusLogProbMetric: 33.8695

Epoch 154: val_loss did not improve from 33.55577
196/196 - 33s - loss: 33.6101 - MinusLogProbMetric: 33.6101 - val_loss: 33.8695 - val_MinusLogProbMetric: 33.8695 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 155/1000
2023-09-29 14:06:36.933 
Epoch 155/1000 
	 loss: 33.7718, MinusLogProbMetric: 33.7718, val_loss: 33.9111, val_MinusLogProbMetric: 33.9111

Epoch 155: val_loss did not improve from 33.55577
196/196 - 33s - loss: 33.7718 - MinusLogProbMetric: 33.7718 - val_loss: 33.9111 - val_MinusLogProbMetric: 33.9111 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 156/1000
2023-09-29 14:07:11.421 
Epoch 156/1000 
	 loss: 33.5074, MinusLogProbMetric: 33.5074, val_loss: 33.4418, val_MinusLogProbMetric: 33.4418

Epoch 156: val_loss improved from 33.55577 to 33.44182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 33.5074 - MinusLogProbMetric: 33.5074 - val_loss: 33.4418 - val_MinusLogProbMetric: 33.4418 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 157/1000
2023-09-29 14:07:45.916 
Epoch 157/1000 
	 loss: 33.7937, MinusLogProbMetric: 33.7937, val_loss: 34.3734, val_MinusLogProbMetric: 34.3734

Epoch 157: val_loss did not improve from 33.44182
196/196 - 34s - loss: 33.7937 - MinusLogProbMetric: 33.7937 - val_loss: 34.3734 - val_MinusLogProbMetric: 34.3734 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 158/1000
2023-09-29 14:08:19.944 
Epoch 158/1000 
	 loss: 33.3676, MinusLogProbMetric: 33.3676, val_loss: 33.8087, val_MinusLogProbMetric: 33.8087

Epoch 158: val_loss did not improve from 33.44182
196/196 - 34s - loss: 33.3676 - MinusLogProbMetric: 33.3676 - val_loss: 33.8087 - val_MinusLogProbMetric: 33.8087 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 159/1000
2023-09-29 14:08:51.306 
Epoch 159/1000 
	 loss: 33.7017, MinusLogProbMetric: 33.7017, val_loss: 34.7208, val_MinusLogProbMetric: 34.7208

Epoch 159: val_loss did not improve from 33.44182
196/196 - 31s - loss: 33.7017 - MinusLogProbMetric: 33.7017 - val_loss: 34.7208 - val_MinusLogProbMetric: 34.7208 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 160/1000
2023-09-29 14:09:22.396 
Epoch 160/1000 
	 loss: 33.5643, MinusLogProbMetric: 33.5643, val_loss: 34.0448, val_MinusLogProbMetric: 34.0448

Epoch 160: val_loss did not improve from 33.44182
196/196 - 31s - loss: 33.5643 - MinusLogProbMetric: 33.5643 - val_loss: 34.0448 - val_MinusLogProbMetric: 34.0448 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 161/1000
2023-09-29 14:09:56.024 
Epoch 161/1000 
	 loss: 33.5941, MinusLogProbMetric: 33.5941, val_loss: 33.4105, val_MinusLogProbMetric: 33.4105

Epoch 161: val_loss improved from 33.44182 to 33.41054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 33.5941 - MinusLogProbMetric: 33.5941 - val_loss: 33.4105 - val_MinusLogProbMetric: 33.4105 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 162/1000
2023-09-29 14:10:30.077 
Epoch 162/1000 
	 loss: 33.2672, MinusLogProbMetric: 33.2672, val_loss: 33.5400, val_MinusLogProbMetric: 33.5400

Epoch 162: val_loss did not improve from 33.41054
196/196 - 34s - loss: 33.2672 - MinusLogProbMetric: 33.2672 - val_loss: 33.5400 - val_MinusLogProbMetric: 33.5400 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 163/1000
2023-09-29 14:11:00.648 
Epoch 163/1000 
	 loss: 33.6036, MinusLogProbMetric: 33.6036, val_loss: 34.8694, val_MinusLogProbMetric: 34.8694

Epoch 163: val_loss did not improve from 33.41054
196/196 - 31s - loss: 33.6036 - MinusLogProbMetric: 33.6036 - val_loss: 34.8694 - val_MinusLogProbMetric: 34.8694 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 164/1000
2023-09-29 14:11:30.373 
Epoch 164/1000 
	 loss: 33.8285, MinusLogProbMetric: 33.8285, val_loss: 33.9194, val_MinusLogProbMetric: 33.9194

Epoch 164: val_loss did not improve from 33.41054
196/196 - 30s - loss: 33.8285 - MinusLogProbMetric: 33.8285 - val_loss: 33.9194 - val_MinusLogProbMetric: 33.9194 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 165/1000
2023-09-29 14:12:00.213 
Epoch 165/1000 
	 loss: 33.2556, MinusLogProbMetric: 33.2556, val_loss: 33.6467, val_MinusLogProbMetric: 33.6467

Epoch 165: val_loss did not improve from 33.41054
196/196 - 30s - loss: 33.2556 - MinusLogProbMetric: 33.2556 - val_loss: 33.6467 - val_MinusLogProbMetric: 33.6467 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 166/1000
2023-09-29 14:12:30.366 
Epoch 166/1000 
	 loss: 33.1010, MinusLogProbMetric: 33.1010, val_loss: 33.1329, val_MinusLogProbMetric: 33.1329

Epoch 166: val_loss improved from 33.41054 to 33.13292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 31s - loss: 33.1010 - MinusLogProbMetric: 33.1010 - val_loss: 33.1329 - val_MinusLogProbMetric: 33.1329 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 167/1000
2023-09-29 14:13:04.084 
Epoch 167/1000 
	 loss: 33.3377, MinusLogProbMetric: 33.3377, val_loss: 33.3802, val_MinusLogProbMetric: 33.3802

Epoch 167: val_loss did not improve from 33.13292
196/196 - 33s - loss: 33.3377 - MinusLogProbMetric: 33.3377 - val_loss: 33.3802 - val_MinusLogProbMetric: 33.3802 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 168/1000
2023-09-29 14:13:33.924 
Epoch 168/1000 
	 loss: 33.2909, MinusLogProbMetric: 33.2909, val_loss: 33.1443, val_MinusLogProbMetric: 33.1443

Epoch 168: val_loss did not improve from 33.13292
196/196 - 30s - loss: 33.2909 - MinusLogProbMetric: 33.2909 - val_loss: 33.1443 - val_MinusLogProbMetric: 33.1443 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 169/1000
2023-09-29 14:14:03.535 
Epoch 169/1000 
	 loss: 33.3081, MinusLogProbMetric: 33.3081, val_loss: 34.1389, val_MinusLogProbMetric: 34.1389

Epoch 169: val_loss did not improve from 33.13292
196/196 - 30s - loss: 33.3081 - MinusLogProbMetric: 33.3081 - val_loss: 34.1389 - val_MinusLogProbMetric: 34.1389 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 170/1000
2023-09-29 14:14:32.947 
Epoch 170/1000 
	 loss: 37.3992, MinusLogProbMetric: 37.3992, val_loss: 36.9846, val_MinusLogProbMetric: 36.9846

Epoch 170: val_loss did not improve from 33.13292
196/196 - 29s - loss: 37.3992 - MinusLogProbMetric: 37.3992 - val_loss: 36.9846 - val_MinusLogProbMetric: 36.9846 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 171/1000
2023-09-29 14:15:02.464 
Epoch 171/1000 
	 loss: 34.0702, MinusLogProbMetric: 34.0702, val_loss: 33.7621, val_MinusLogProbMetric: 33.7621

Epoch 171: val_loss did not improve from 33.13292
196/196 - 30s - loss: 34.0702 - MinusLogProbMetric: 34.0702 - val_loss: 33.7621 - val_MinusLogProbMetric: 33.7621 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 172/1000
2023-09-29 14:15:31.581 
Epoch 172/1000 
	 loss: 33.4328, MinusLogProbMetric: 33.4328, val_loss: 33.3205, val_MinusLogProbMetric: 33.3205

Epoch 172: val_loss did not improve from 33.13292
196/196 - 29s - loss: 33.4328 - MinusLogProbMetric: 33.4328 - val_loss: 33.3205 - val_MinusLogProbMetric: 33.3205 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 173/1000
2023-09-29 14:16:01.177 
Epoch 173/1000 
	 loss: 32.9544, MinusLogProbMetric: 32.9544, val_loss: 33.4088, val_MinusLogProbMetric: 33.4088

Epoch 173: val_loss did not improve from 33.13292
196/196 - 30s - loss: 32.9544 - MinusLogProbMetric: 32.9544 - val_loss: 33.4088 - val_MinusLogProbMetric: 33.4088 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 174/1000
2023-09-29 14:16:32.853 
Epoch 174/1000 
	 loss: 32.8968, MinusLogProbMetric: 32.8968, val_loss: 32.9694, val_MinusLogProbMetric: 32.9694

Epoch 174: val_loss improved from 33.13292 to 32.96937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 33s - loss: 32.8968 - MinusLogProbMetric: 32.8968 - val_loss: 32.9694 - val_MinusLogProbMetric: 32.9694 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 175/1000
2023-09-29 14:17:03.683 
Epoch 175/1000 
	 loss: 33.8539, MinusLogProbMetric: 33.8539, val_loss: 33.0042, val_MinusLogProbMetric: 33.0042

Epoch 175: val_loss did not improve from 32.96937
196/196 - 30s - loss: 33.8539 - MinusLogProbMetric: 33.8539 - val_loss: 33.0042 - val_MinusLogProbMetric: 33.0042 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 176/1000
2023-09-29 14:17:35.086 
Epoch 176/1000 
	 loss: 32.9535, MinusLogProbMetric: 32.9535, val_loss: 32.8313, val_MinusLogProbMetric: 32.8313

Epoch 176: val_loss improved from 32.96937 to 32.83127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 33s - loss: 32.9535 - MinusLogProbMetric: 32.9535 - val_loss: 32.8313 - val_MinusLogProbMetric: 32.8313 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 177/1000
2023-09-29 14:18:09.078 
Epoch 177/1000 
	 loss: 32.6856, MinusLogProbMetric: 32.6856, val_loss: 33.4081, val_MinusLogProbMetric: 33.4081

Epoch 177: val_loss did not improve from 32.83127
196/196 - 33s - loss: 32.6856 - MinusLogProbMetric: 32.6856 - val_loss: 33.4081 - val_MinusLogProbMetric: 33.4081 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 178/1000
2023-09-29 14:18:39.574 
Epoch 178/1000 
	 loss: 32.9026, MinusLogProbMetric: 32.9026, val_loss: 33.3419, val_MinusLogProbMetric: 33.3419

Epoch 178: val_loss did not improve from 32.83127
196/196 - 30s - loss: 32.9026 - MinusLogProbMetric: 32.9026 - val_loss: 33.3419 - val_MinusLogProbMetric: 33.3419 - lr: 3.3333e-04 - 30s/epoch - 156ms/step
Epoch 179/1000
2023-09-29 14:19:09.559 
Epoch 179/1000 
	 loss: 33.0860, MinusLogProbMetric: 33.0860, val_loss: 33.5716, val_MinusLogProbMetric: 33.5716

Epoch 179: val_loss did not improve from 32.83127
196/196 - 30s - loss: 33.0860 - MinusLogProbMetric: 33.0860 - val_loss: 33.5716 - val_MinusLogProbMetric: 33.5716 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 180/1000
2023-09-29 14:19:39.806 
Epoch 180/1000 
	 loss: 33.0963, MinusLogProbMetric: 33.0963, val_loss: 33.0543, val_MinusLogProbMetric: 33.0543

Epoch 180: val_loss did not improve from 32.83127
196/196 - 30s - loss: 33.0963 - MinusLogProbMetric: 33.0963 - val_loss: 33.0543 - val_MinusLogProbMetric: 33.0543 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 181/1000
2023-09-29 14:20:09.596 
Epoch 181/1000 
	 loss: 33.1591, MinusLogProbMetric: 33.1591, val_loss: 33.0132, val_MinusLogProbMetric: 33.0132

Epoch 181: val_loss did not improve from 32.83127
196/196 - 30s - loss: 33.1591 - MinusLogProbMetric: 33.1591 - val_loss: 33.0132 - val_MinusLogProbMetric: 33.0132 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 182/1000
2023-09-29 14:20:40.233 
Epoch 182/1000 
	 loss: 32.6658, MinusLogProbMetric: 32.6658, val_loss: 33.1583, val_MinusLogProbMetric: 33.1583

Epoch 182: val_loss did not improve from 32.83127
196/196 - 31s - loss: 32.6658 - MinusLogProbMetric: 32.6658 - val_loss: 33.1583 - val_MinusLogProbMetric: 33.1583 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 183/1000
2023-09-29 14:21:11.206 
Epoch 183/1000 
	 loss: 32.6397, MinusLogProbMetric: 32.6397, val_loss: 33.1857, val_MinusLogProbMetric: 33.1857

Epoch 183: val_loss did not improve from 32.83127
196/196 - 31s - loss: 32.6397 - MinusLogProbMetric: 32.6397 - val_loss: 33.1857 - val_MinusLogProbMetric: 33.1857 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 184/1000
2023-09-29 14:21:40.773 
Epoch 184/1000 
	 loss: 32.8745, MinusLogProbMetric: 32.8745, val_loss: 33.3221, val_MinusLogProbMetric: 33.3221

Epoch 184: val_loss did not improve from 32.83127
196/196 - 30s - loss: 32.8745 - MinusLogProbMetric: 32.8745 - val_loss: 33.3221 - val_MinusLogProbMetric: 33.3221 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 185/1000
2023-09-29 14:22:11.024 
Epoch 185/1000 
	 loss: 32.8725, MinusLogProbMetric: 32.8725, val_loss: 32.9169, val_MinusLogProbMetric: 32.9169

Epoch 185: val_loss did not improve from 32.83127
196/196 - 30s - loss: 32.8725 - MinusLogProbMetric: 32.8725 - val_loss: 32.9169 - val_MinusLogProbMetric: 32.9169 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 186/1000
2023-09-29 14:22:41.231 
Epoch 186/1000 
	 loss: 32.8957, MinusLogProbMetric: 32.8957, val_loss: 32.7544, val_MinusLogProbMetric: 32.7544

Epoch 186: val_loss improved from 32.83127 to 32.75436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 31s - loss: 32.8957 - MinusLogProbMetric: 32.8957 - val_loss: 32.7544 - val_MinusLogProbMetric: 32.7544 - lr: 3.3333e-04 - 31s/epoch - 161ms/step
Epoch 187/1000
2023-09-29 14:23:11.943 
Epoch 187/1000 
	 loss: 32.6340, MinusLogProbMetric: 32.6340, val_loss: 33.2931, val_MinusLogProbMetric: 33.2931

Epoch 187: val_loss did not improve from 32.75436
196/196 - 29s - loss: 32.6340 - MinusLogProbMetric: 32.6340 - val_loss: 33.2931 - val_MinusLogProbMetric: 33.2931 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 188/1000
2023-09-29 14:23:41.750 
Epoch 188/1000 
	 loss: 32.7079, MinusLogProbMetric: 32.7079, val_loss: 33.0516, val_MinusLogProbMetric: 33.0516

Epoch 188: val_loss did not improve from 32.75436
196/196 - 30s - loss: 32.7079 - MinusLogProbMetric: 32.7079 - val_loss: 33.0516 - val_MinusLogProbMetric: 33.0516 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 189/1000
2023-09-29 14:24:11.296 
Epoch 189/1000 
	 loss: 32.5464, MinusLogProbMetric: 32.5464, val_loss: 32.8148, val_MinusLogProbMetric: 32.8148

Epoch 189: val_loss did not improve from 32.75436
196/196 - 30s - loss: 32.5464 - MinusLogProbMetric: 32.5464 - val_loss: 32.8148 - val_MinusLogProbMetric: 32.8148 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 190/1000
2023-09-29 14:24:41.288 
Epoch 190/1000 
	 loss: 32.4735, MinusLogProbMetric: 32.4735, val_loss: 33.2198, val_MinusLogProbMetric: 33.2198

Epoch 190: val_loss did not improve from 32.75436
196/196 - 30s - loss: 32.4735 - MinusLogProbMetric: 32.4735 - val_loss: 33.2198 - val_MinusLogProbMetric: 33.2198 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 191/1000
2023-09-29 14:25:11.255 
Epoch 191/1000 
	 loss: 32.4397, MinusLogProbMetric: 32.4397, val_loss: 32.9566, val_MinusLogProbMetric: 32.9566

Epoch 191: val_loss did not improve from 32.75436
196/196 - 30s - loss: 32.4397 - MinusLogProbMetric: 32.4397 - val_loss: 32.9566 - val_MinusLogProbMetric: 32.9566 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 192/1000
2023-09-29 14:25:41.218 
Epoch 192/1000 
	 loss: 32.5162, MinusLogProbMetric: 32.5162, val_loss: 32.8135, val_MinusLogProbMetric: 32.8135

Epoch 192: val_loss did not improve from 32.75436
196/196 - 30s - loss: 32.5162 - MinusLogProbMetric: 32.5162 - val_loss: 32.8135 - val_MinusLogProbMetric: 32.8135 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 193/1000
2023-09-29 14:26:11.068 
Epoch 193/1000 
	 loss: 32.6316, MinusLogProbMetric: 32.6316, val_loss: 38.0734, val_MinusLogProbMetric: 38.0734

Epoch 193: val_loss did not improve from 32.75436
196/196 - 30s - loss: 32.6316 - MinusLogProbMetric: 32.6316 - val_loss: 38.0734 - val_MinusLogProbMetric: 38.0734 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 194/1000
2023-09-29 14:26:42.177 
Epoch 194/1000 
	 loss: 32.7467, MinusLogProbMetric: 32.7467, val_loss: 32.9245, val_MinusLogProbMetric: 32.9245

Epoch 194: val_loss did not improve from 32.75436
196/196 - 31s - loss: 32.7467 - MinusLogProbMetric: 32.7467 - val_loss: 32.9245 - val_MinusLogProbMetric: 32.9245 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 195/1000
2023-09-29 14:27:12.332 
Epoch 195/1000 
	 loss: 32.3734, MinusLogProbMetric: 32.3734, val_loss: 34.3959, val_MinusLogProbMetric: 34.3959

Epoch 195: val_loss did not improve from 32.75436
196/196 - 30s - loss: 32.3734 - MinusLogProbMetric: 32.3734 - val_loss: 34.3959 - val_MinusLogProbMetric: 34.3959 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 196/1000
2023-09-29 14:27:43.760 
Epoch 196/1000 
	 loss: 32.5121, MinusLogProbMetric: 32.5121, val_loss: 32.7332, val_MinusLogProbMetric: 32.7332

Epoch 196: val_loss improved from 32.75436 to 32.73318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 32s - loss: 32.5121 - MinusLogProbMetric: 32.5121 - val_loss: 32.7332 - val_MinusLogProbMetric: 32.7332 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 197/1000
2023-09-29 14:28:15.457 
Epoch 197/1000 
	 loss: 32.3658, MinusLogProbMetric: 32.3658, val_loss: 32.7519, val_MinusLogProbMetric: 32.7519

Epoch 197: val_loss did not improve from 32.73318
196/196 - 31s - loss: 32.3658 - MinusLogProbMetric: 32.3658 - val_loss: 32.7519 - val_MinusLogProbMetric: 32.7519 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 198/1000
2023-09-29 14:28:46.709 
Epoch 198/1000 
	 loss: 32.4893, MinusLogProbMetric: 32.4893, val_loss: 32.3600, val_MinusLogProbMetric: 32.3600

Epoch 198: val_loss improved from 32.73318 to 32.36000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 32s - loss: 32.4893 - MinusLogProbMetric: 32.4893 - val_loss: 32.3600 - val_MinusLogProbMetric: 32.3600 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 199/1000
2023-09-29 14:29:19.594 
Epoch 199/1000 
	 loss: 32.4272, MinusLogProbMetric: 32.4272, val_loss: 32.6594, val_MinusLogProbMetric: 32.6594

Epoch 199: val_loss did not improve from 32.36000
196/196 - 32s - loss: 32.4272 - MinusLogProbMetric: 32.4272 - val_loss: 32.6594 - val_MinusLogProbMetric: 32.6594 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 200/1000
2023-09-29 14:29:50.114 
Epoch 200/1000 
	 loss: 32.3741, MinusLogProbMetric: 32.3741, val_loss: 33.2724, val_MinusLogProbMetric: 33.2724

Epoch 200: val_loss did not improve from 32.36000
196/196 - 31s - loss: 32.3741 - MinusLogProbMetric: 32.3741 - val_loss: 33.2724 - val_MinusLogProbMetric: 33.2724 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 201/1000
2023-09-29 14:30:19.691 
Epoch 201/1000 
	 loss: 32.4651, MinusLogProbMetric: 32.4651, val_loss: 33.2823, val_MinusLogProbMetric: 33.2823

Epoch 201: val_loss did not improve from 32.36000
196/196 - 30s - loss: 32.4651 - MinusLogProbMetric: 32.4651 - val_loss: 33.2823 - val_MinusLogProbMetric: 33.2823 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 202/1000
2023-09-29 14:30:49.108 
Epoch 202/1000 
	 loss: 32.4638, MinusLogProbMetric: 32.4638, val_loss: 32.6283, val_MinusLogProbMetric: 32.6283

Epoch 202: val_loss did not improve from 32.36000
196/196 - 29s - loss: 32.4638 - MinusLogProbMetric: 32.4638 - val_loss: 32.6283 - val_MinusLogProbMetric: 32.6283 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 203/1000
2023-09-29 14:31:20.223 
Epoch 203/1000 
	 loss: 32.2207, MinusLogProbMetric: 32.2207, val_loss: 32.9292, val_MinusLogProbMetric: 32.9292

Epoch 203: val_loss did not improve from 32.36000
196/196 - 31s - loss: 32.2207 - MinusLogProbMetric: 32.2207 - val_loss: 32.9292 - val_MinusLogProbMetric: 32.9292 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 204/1000
2023-09-29 14:31:49.744 
Epoch 204/1000 
	 loss: 32.2048, MinusLogProbMetric: 32.2048, val_loss: 32.6306, val_MinusLogProbMetric: 32.6306

Epoch 204: val_loss did not improve from 32.36000
196/196 - 30s - loss: 32.2048 - MinusLogProbMetric: 32.2048 - val_loss: 32.6306 - val_MinusLogProbMetric: 32.6306 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 205/1000
2023-09-29 14:32:19.203 
Epoch 205/1000 
	 loss: 32.1475, MinusLogProbMetric: 32.1475, val_loss: 32.9858, val_MinusLogProbMetric: 32.9858

Epoch 205: val_loss did not improve from 32.36000
196/196 - 29s - loss: 32.1475 - MinusLogProbMetric: 32.1475 - val_loss: 32.9858 - val_MinusLogProbMetric: 32.9858 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 206/1000
2023-09-29 14:32:48.991 
Epoch 206/1000 
	 loss: 32.1557, MinusLogProbMetric: 32.1557, val_loss: 32.1327, val_MinusLogProbMetric: 32.1327

Epoch 206: val_loss improved from 32.36000 to 32.13269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 30s - loss: 32.1557 - MinusLogProbMetric: 32.1557 - val_loss: 32.1327 - val_MinusLogProbMetric: 32.1327 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 207/1000
2023-09-29 14:33:20.648 
Epoch 207/1000 
	 loss: 32.3221, MinusLogProbMetric: 32.3221, val_loss: 32.4504, val_MinusLogProbMetric: 32.4504

Epoch 207: val_loss did not improve from 32.13269
196/196 - 31s - loss: 32.3221 - MinusLogProbMetric: 32.3221 - val_loss: 32.4504 - val_MinusLogProbMetric: 32.4504 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 208/1000
2023-09-29 14:33:50.107 
Epoch 208/1000 
	 loss: 32.4287, MinusLogProbMetric: 32.4287, val_loss: 33.1276, val_MinusLogProbMetric: 33.1276

Epoch 208: val_loss did not improve from 32.13269
196/196 - 29s - loss: 32.4287 - MinusLogProbMetric: 32.4287 - val_loss: 33.1276 - val_MinusLogProbMetric: 33.1276 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 209/1000
2023-09-29 14:34:19.495 
Epoch 209/1000 
	 loss: 32.2738, MinusLogProbMetric: 32.2738, val_loss: 33.3872, val_MinusLogProbMetric: 33.3872

Epoch 209: val_loss did not improve from 32.13269
196/196 - 29s - loss: 32.2738 - MinusLogProbMetric: 32.2738 - val_loss: 33.3872 - val_MinusLogProbMetric: 33.3872 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 210/1000
2023-09-29 14:34:50.693 
Epoch 210/1000 
	 loss: 32.2754, MinusLogProbMetric: 32.2754, val_loss: 32.8628, val_MinusLogProbMetric: 32.8628

Epoch 210: val_loss did not improve from 32.13269
196/196 - 31s - loss: 32.2754 - MinusLogProbMetric: 32.2754 - val_loss: 32.8628 - val_MinusLogProbMetric: 32.8628 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 211/1000
2023-09-29 14:35:21.220 
Epoch 211/1000 
	 loss: 32.3743, MinusLogProbMetric: 32.3743, val_loss: 32.5152, val_MinusLogProbMetric: 32.5152

Epoch 211: val_loss did not improve from 32.13269
196/196 - 31s - loss: 32.3743 - MinusLogProbMetric: 32.3743 - val_loss: 32.5152 - val_MinusLogProbMetric: 32.5152 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 212/1000
2023-09-29 14:35:51.061 
Epoch 212/1000 
	 loss: 32.3471, MinusLogProbMetric: 32.3471, val_loss: 33.4808, val_MinusLogProbMetric: 33.4808

Epoch 212: val_loss did not improve from 32.13269
196/196 - 30s - loss: 32.3471 - MinusLogProbMetric: 32.3471 - val_loss: 33.4808 - val_MinusLogProbMetric: 33.4808 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 213/1000
2023-09-29 14:36:20.562 
Epoch 213/1000 
	 loss: 32.0088, MinusLogProbMetric: 32.0088, val_loss: 34.0405, val_MinusLogProbMetric: 34.0405

Epoch 213: val_loss did not improve from 32.13269
196/196 - 29s - loss: 32.0088 - MinusLogProbMetric: 32.0088 - val_loss: 34.0405 - val_MinusLogProbMetric: 34.0405 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 214/1000
2023-09-29 14:36:50.380 
Epoch 214/1000 
	 loss: 32.1874, MinusLogProbMetric: 32.1874, val_loss: 32.0496, val_MinusLogProbMetric: 32.0496

Epoch 214: val_loss improved from 32.13269 to 32.04959, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 30s - loss: 32.1874 - MinusLogProbMetric: 32.1874 - val_loss: 32.0496 - val_MinusLogProbMetric: 32.0496 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 215/1000
2023-09-29 14:37:21.029 
Epoch 215/1000 
	 loss: 32.2206, MinusLogProbMetric: 32.2206, val_loss: 32.5975, val_MinusLogProbMetric: 32.5975

Epoch 215: val_loss did not improve from 32.04959
196/196 - 30s - loss: 32.2206 - MinusLogProbMetric: 32.2206 - val_loss: 32.5975 - val_MinusLogProbMetric: 32.5975 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 216/1000
2023-09-29 14:37:52.542 
Epoch 216/1000 
	 loss: 32.0601, MinusLogProbMetric: 32.0601, val_loss: 32.2987, val_MinusLogProbMetric: 32.2987

Epoch 216: val_loss did not improve from 32.04959
196/196 - 32s - loss: 32.0601 - MinusLogProbMetric: 32.0601 - val_loss: 32.2987 - val_MinusLogProbMetric: 32.2987 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 217/1000
2023-09-29 14:38:23.171 
Epoch 217/1000 
	 loss: 32.0719, MinusLogProbMetric: 32.0719, val_loss: 32.6829, val_MinusLogProbMetric: 32.6829

Epoch 217: val_loss did not improve from 32.04959
196/196 - 31s - loss: 32.0719 - MinusLogProbMetric: 32.0719 - val_loss: 32.6829 - val_MinusLogProbMetric: 32.6829 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 218/1000
2023-09-29 14:38:53.249 
Epoch 218/1000 
	 loss: 32.3581, MinusLogProbMetric: 32.3581, val_loss: 35.6011, val_MinusLogProbMetric: 35.6011

Epoch 218: val_loss did not improve from 32.04959
196/196 - 30s - loss: 32.3581 - MinusLogProbMetric: 32.3581 - val_loss: 35.6011 - val_MinusLogProbMetric: 35.6011 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 219/1000
2023-09-29 14:39:23.917 
Epoch 219/1000 
	 loss: 32.2114, MinusLogProbMetric: 32.2114, val_loss: 32.0295, val_MinusLogProbMetric: 32.0295

Epoch 219: val_loss improved from 32.04959 to 32.02954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 31s - loss: 32.2114 - MinusLogProbMetric: 32.2114 - val_loss: 32.0295 - val_MinusLogProbMetric: 32.0295 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 220/1000
2023-09-29 14:39:54.805 
Epoch 220/1000 
	 loss: 31.9742, MinusLogProbMetric: 31.9742, val_loss: 32.8588, val_MinusLogProbMetric: 32.8588

Epoch 220: val_loss did not improve from 32.02954
196/196 - 30s - loss: 31.9742 - MinusLogProbMetric: 31.9742 - val_loss: 32.8588 - val_MinusLogProbMetric: 32.8588 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 221/1000
2023-09-29 14:40:27.366 
Epoch 221/1000 
	 loss: 31.9305, MinusLogProbMetric: 31.9305, val_loss: 32.4502, val_MinusLogProbMetric: 32.4502

Epoch 221: val_loss did not improve from 32.02954
196/196 - 33s - loss: 31.9305 - MinusLogProbMetric: 31.9305 - val_loss: 32.4502 - val_MinusLogProbMetric: 32.4502 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 222/1000
2023-09-29 14:40:57.736 
Epoch 222/1000 
	 loss: 32.3085, MinusLogProbMetric: 32.3085, val_loss: 32.3585, val_MinusLogProbMetric: 32.3585

Epoch 222: val_loss did not improve from 32.02954
196/196 - 30s - loss: 32.3085 - MinusLogProbMetric: 32.3085 - val_loss: 32.3585 - val_MinusLogProbMetric: 32.3585 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 223/1000
2023-09-29 14:41:30.136 
Epoch 223/1000 
	 loss: 32.0919, MinusLogProbMetric: 32.0919, val_loss: 33.5409, val_MinusLogProbMetric: 33.5409

Epoch 223: val_loss did not improve from 32.02954
196/196 - 32s - loss: 32.0919 - MinusLogProbMetric: 32.0919 - val_loss: 33.5409 - val_MinusLogProbMetric: 33.5409 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 224/1000
2023-09-29 14:41:59.881 
Epoch 224/1000 
	 loss: 31.9992, MinusLogProbMetric: 31.9992, val_loss: 32.8543, val_MinusLogProbMetric: 32.8543

Epoch 224: val_loss did not improve from 32.02954
196/196 - 30s - loss: 31.9992 - MinusLogProbMetric: 31.9992 - val_loss: 32.8543 - val_MinusLogProbMetric: 32.8543 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 225/1000
2023-09-29 14:42:29.429 
Epoch 225/1000 
	 loss: 31.6602, MinusLogProbMetric: 31.6602, val_loss: 33.7242, val_MinusLogProbMetric: 33.7242

Epoch 225: val_loss did not improve from 32.02954
196/196 - 30s - loss: 31.6602 - MinusLogProbMetric: 31.6602 - val_loss: 33.7242 - val_MinusLogProbMetric: 33.7242 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 226/1000
2023-09-29 14:43:01.127 
Epoch 226/1000 
	 loss: 31.8601, MinusLogProbMetric: 31.8601, val_loss: 31.8203, val_MinusLogProbMetric: 31.8203

Epoch 226: val_loss improved from 32.02954 to 31.82025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 32s - loss: 31.8601 - MinusLogProbMetric: 31.8601 - val_loss: 31.8203 - val_MinusLogProbMetric: 31.8203 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 227/1000
2023-09-29 14:43:31.466 
Epoch 227/1000 
	 loss: 31.8876, MinusLogProbMetric: 31.8876, val_loss: 32.0303, val_MinusLogProbMetric: 32.0303

Epoch 227: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8876 - MinusLogProbMetric: 31.8876 - val_loss: 32.0303 - val_MinusLogProbMetric: 32.0303 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 228/1000
2023-09-29 14:44:01.577 
Epoch 228/1000 
	 loss: 31.8676, MinusLogProbMetric: 31.8676, val_loss: 32.9294, val_MinusLogProbMetric: 32.9294

Epoch 228: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8676 - MinusLogProbMetric: 31.8676 - val_loss: 32.9294 - val_MinusLogProbMetric: 32.9294 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 229/1000
2023-09-29 14:44:31.358 
Epoch 229/1000 
	 loss: 31.9486, MinusLogProbMetric: 31.9486, val_loss: 32.1627, val_MinusLogProbMetric: 32.1627

Epoch 229: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.9486 - MinusLogProbMetric: 31.9486 - val_loss: 32.1627 - val_MinusLogProbMetric: 32.1627 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 230/1000
2023-09-29 14:45:01.160 
Epoch 230/1000 
	 loss: 31.8152, MinusLogProbMetric: 31.8152, val_loss: 32.2224, val_MinusLogProbMetric: 32.2224

Epoch 230: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8152 - MinusLogProbMetric: 31.8152 - val_loss: 32.2224 - val_MinusLogProbMetric: 32.2224 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 231/1000
2023-09-29 14:45:31.371 
Epoch 231/1000 
	 loss: 31.6616, MinusLogProbMetric: 31.6616, val_loss: 32.3293, val_MinusLogProbMetric: 32.3293

Epoch 231: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.6616 - MinusLogProbMetric: 31.6616 - val_loss: 32.3293 - val_MinusLogProbMetric: 32.3293 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 232/1000
2023-09-29 14:46:02.528 
Epoch 232/1000 
	 loss: 31.9557, MinusLogProbMetric: 31.9557, val_loss: 31.9370, val_MinusLogProbMetric: 31.9370

Epoch 232: val_loss did not improve from 31.82025
196/196 - 31s - loss: 31.9557 - MinusLogProbMetric: 31.9557 - val_loss: 31.9370 - val_MinusLogProbMetric: 31.9370 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 233/1000
2023-09-29 14:46:32.325 
Epoch 233/1000 
	 loss: 31.7852, MinusLogProbMetric: 31.7852, val_loss: 32.2263, val_MinusLogProbMetric: 32.2263

Epoch 233: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.7852 - MinusLogProbMetric: 31.7852 - val_loss: 32.2263 - val_MinusLogProbMetric: 32.2263 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 234/1000
2023-09-29 14:47:02.106 
Epoch 234/1000 
	 loss: 31.8454, MinusLogProbMetric: 31.8454, val_loss: 32.0804, val_MinusLogProbMetric: 32.0804

Epoch 234: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8454 - MinusLogProbMetric: 31.8454 - val_loss: 32.0804 - val_MinusLogProbMetric: 32.0804 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 235/1000
2023-09-29 14:47:32.194 
Epoch 235/1000 
	 loss: 31.8841, MinusLogProbMetric: 31.8841, val_loss: 32.0090, val_MinusLogProbMetric: 32.0090

Epoch 235: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8841 - MinusLogProbMetric: 31.8841 - val_loss: 32.0090 - val_MinusLogProbMetric: 32.0090 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 236/1000
2023-09-29 14:48:02.298 
Epoch 236/1000 
	 loss: 31.8053, MinusLogProbMetric: 31.8053, val_loss: 32.2455, val_MinusLogProbMetric: 32.2455

Epoch 236: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8053 - MinusLogProbMetric: 31.8053 - val_loss: 32.2455 - val_MinusLogProbMetric: 32.2455 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 237/1000
2023-09-29 14:48:32.428 
Epoch 237/1000 
	 loss: 31.8942, MinusLogProbMetric: 31.8942, val_loss: 32.4299, val_MinusLogProbMetric: 32.4299

Epoch 237: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8942 - MinusLogProbMetric: 31.8942 - val_loss: 32.4299 - val_MinusLogProbMetric: 32.4299 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 238/1000
2023-09-29 14:49:02.566 
Epoch 238/1000 
	 loss: 31.8852, MinusLogProbMetric: 31.8852, val_loss: 32.4453, val_MinusLogProbMetric: 32.4453

Epoch 238: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8852 - MinusLogProbMetric: 31.8852 - val_loss: 32.4453 - val_MinusLogProbMetric: 32.4453 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 239/1000
2023-09-29 14:49:34.964 
Epoch 239/1000 
	 loss: 32.4373, MinusLogProbMetric: 32.4373, val_loss: 31.8959, val_MinusLogProbMetric: 31.8959

Epoch 239: val_loss did not improve from 31.82025
196/196 - 32s - loss: 32.4373 - MinusLogProbMetric: 32.4373 - val_loss: 31.8959 - val_MinusLogProbMetric: 31.8959 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 240/1000
2023-09-29 14:50:06.334 
Epoch 240/1000 
	 loss: 31.7787, MinusLogProbMetric: 31.7787, val_loss: 32.1600, val_MinusLogProbMetric: 32.1600

Epoch 240: val_loss did not improve from 31.82025
196/196 - 31s - loss: 31.7787 - MinusLogProbMetric: 31.7787 - val_loss: 32.1600 - val_MinusLogProbMetric: 32.1600 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 241/1000
2023-09-29 14:50:35.927 
Epoch 241/1000 
	 loss: 31.8265, MinusLogProbMetric: 31.8265, val_loss: 33.1042, val_MinusLogProbMetric: 33.1042

Epoch 241: val_loss did not improve from 31.82025
196/196 - 30s - loss: 31.8265 - MinusLogProbMetric: 31.8265 - val_loss: 33.1042 - val_MinusLogProbMetric: 33.1042 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 242/1000
2023-09-29 14:51:05.587 
Epoch 242/1000 
	 loss: 32.0271, MinusLogProbMetric: 32.0271, val_loss: 32.2542, val_MinusLogProbMetric: 32.2542

Epoch 242: val_loss did not improve from 31.82025
196/196 - 30s - loss: 32.0271 - MinusLogProbMetric: 32.0271 - val_loss: 32.2542 - val_MinusLogProbMetric: 32.2542 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 243/1000
2023-09-29 14:51:35.775 
Epoch 243/1000 
	 loss: 31.8168, MinusLogProbMetric: 31.8168, val_loss: 31.4509, val_MinusLogProbMetric: 31.4509

Epoch 243: val_loss improved from 31.82025 to 31.45092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 31s - loss: 31.8168 - MinusLogProbMetric: 31.8168 - val_loss: 31.4509 - val_MinusLogProbMetric: 31.4509 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 244/1000
2023-09-29 14:52:06.564 
Epoch 244/1000 
	 loss: 31.7420, MinusLogProbMetric: 31.7420, val_loss: 31.9211, val_MinusLogProbMetric: 31.9211

Epoch 244: val_loss did not improve from 31.45092
196/196 - 30s - loss: 31.7420 - MinusLogProbMetric: 31.7420 - val_loss: 31.9211 - val_MinusLogProbMetric: 31.9211 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 245/1000
2023-09-29 14:52:37.403 
Epoch 245/1000 
	 loss: 31.7979, MinusLogProbMetric: 31.7979, val_loss: 32.2110, val_MinusLogProbMetric: 32.2110

Epoch 245: val_loss did not improve from 31.45092
196/196 - 31s - loss: 31.7979 - MinusLogProbMetric: 31.7979 - val_loss: 32.2110 - val_MinusLogProbMetric: 32.2110 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 246/1000
2023-09-29 14:53:07.911 
Epoch 246/1000 
	 loss: 31.7050, MinusLogProbMetric: 31.7050, val_loss: 32.8449, val_MinusLogProbMetric: 32.8449

Epoch 246: val_loss did not improve from 31.45092
196/196 - 31s - loss: 31.7050 - MinusLogProbMetric: 31.7050 - val_loss: 32.8449 - val_MinusLogProbMetric: 32.8449 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 247/1000
2023-09-29 14:53:40.077 
Epoch 247/1000 
	 loss: 31.4513, MinusLogProbMetric: 31.4513, val_loss: 31.6261, val_MinusLogProbMetric: 31.6261

Epoch 247: val_loss did not improve from 31.45092
196/196 - 32s - loss: 31.4513 - MinusLogProbMetric: 31.4513 - val_loss: 31.6261 - val_MinusLogProbMetric: 31.6261 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 248/1000
2023-09-29 14:54:09.669 
Epoch 248/1000 
	 loss: 31.4373, MinusLogProbMetric: 31.4373, val_loss: 32.1901, val_MinusLogProbMetric: 32.1901

Epoch 248: val_loss did not improve from 31.45092
196/196 - 30s - loss: 31.4373 - MinusLogProbMetric: 31.4373 - val_loss: 32.1901 - val_MinusLogProbMetric: 32.1901 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 249/1000
2023-09-29 14:54:39.816 
Epoch 249/1000 
	 loss: 31.7536, MinusLogProbMetric: 31.7536, val_loss: 32.6806, val_MinusLogProbMetric: 32.6806

Epoch 249: val_loss did not improve from 31.45092
196/196 - 30s - loss: 31.7536 - MinusLogProbMetric: 31.7536 - val_loss: 32.6806 - val_MinusLogProbMetric: 32.6806 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 250/1000
2023-09-29 14:55:11.540 
Epoch 250/1000 
	 loss: 31.4614, MinusLogProbMetric: 31.4614, val_loss: 32.4066, val_MinusLogProbMetric: 32.4066

Epoch 250: val_loss did not improve from 31.45092
196/196 - 32s - loss: 31.4614 - MinusLogProbMetric: 31.4614 - val_loss: 32.4066 - val_MinusLogProbMetric: 32.4066 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 251/1000
2023-09-29 14:55:42.778 
Epoch 251/1000 
	 loss: 31.5201, MinusLogProbMetric: 31.5201, val_loss: 32.9373, val_MinusLogProbMetric: 32.9373

Epoch 251: val_loss did not improve from 31.45092
196/196 - 31s - loss: 31.5201 - MinusLogProbMetric: 31.5201 - val_loss: 32.9373 - val_MinusLogProbMetric: 32.9373 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 252/1000
2023-09-29 14:56:12.642 
Epoch 252/1000 
	 loss: 31.5581, MinusLogProbMetric: 31.5581, val_loss: 32.0899, val_MinusLogProbMetric: 32.0899

Epoch 252: val_loss did not improve from 31.45092
196/196 - 30s - loss: 31.5581 - MinusLogProbMetric: 31.5581 - val_loss: 32.0899 - val_MinusLogProbMetric: 32.0899 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 253/1000
2023-09-29 14:56:44.321 
Epoch 253/1000 
	 loss: 31.4237, MinusLogProbMetric: 31.4237, val_loss: 31.2613, val_MinusLogProbMetric: 31.2613

Epoch 253: val_loss improved from 31.45092 to 31.26131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 32s - loss: 31.4237 - MinusLogProbMetric: 31.4237 - val_loss: 31.2613 - val_MinusLogProbMetric: 31.2613 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 254/1000
2023-09-29 14:57:15.221 
Epoch 254/1000 
	 loss: 31.4870, MinusLogProbMetric: 31.4870, val_loss: 31.6866, val_MinusLogProbMetric: 31.6866

Epoch 254: val_loss did not improve from 31.26131
196/196 - 30s - loss: 31.4870 - MinusLogProbMetric: 31.4870 - val_loss: 31.6866 - val_MinusLogProbMetric: 31.6866 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 255/1000
2023-09-29 14:57:48.031 
Epoch 255/1000 
	 loss: 31.8305, MinusLogProbMetric: 31.8305, val_loss: 31.6505, val_MinusLogProbMetric: 31.6505

Epoch 255: val_loss did not improve from 31.26131
196/196 - 33s - loss: 31.8305 - MinusLogProbMetric: 31.8305 - val_loss: 31.6505 - val_MinusLogProbMetric: 31.6505 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 256/1000
2023-09-29 14:58:18.811 
Epoch 256/1000 
	 loss: 31.5192, MinusLogProbMetric: 31.5192, val_loss: 31.3197, val_MinusLogProbMetric: 31.3197

Epoch 256: val_loss did not improve from 31.26131
196/196 - 31s - loss: 31.5192 - MinusLogProbMetric: 31.5192 - val_loss: 31.3197 - val_MinusLogProbMetric: 31.3197 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 257/1000
2023-09-29 14:58:49.388 
Epoch 257/1000 
	 loss: 31.3415, MinusLogProbMetric: 31.3415, val_loss: 31.7557, val_MinusLogProbMetric: 31.7557

Epoch 257: val_loss did not improve from 31.26131
196/196 - 31s - loss: 31.3415 - MinusLogProbMetric: 31.3415 - val_loss: 31.7557 - val_MinusLogProbMetric: 31.7557 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 258/1000
2023-09-29 14:59:19.615 
Epoch 258/1000 
	 loss: 31.6084, MinusLogProbMetric: 31.6084, val_loss: 32.9960, val_MinusLogProbMetric: 32.9960

Epoch 258: val_loss did not improve from 31.26131
196/196 - 30s - loss: 31.6084 - MinusLogProbMetric: 31.6084 - val_loss: 32.9960 - val_MinusLogProbMetric: 32.9960 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 259/1000
2023-09-29 14:59:52.090 
Epoch 259/1000 
	 loss: 31.3858, MinusLogProbMetric: 31.3858, val_loss: 31.8247, val_MinusLogProbMetric: 31.8247

Epoch 259: val_loss did not improve from 31.26131
196/196 - 32s - loss: 31.3858 - MinusLogProbMetric: 31.3858 - val_loss: 31.8247 - val_MinusLogProbMetric: 31.8247 - lr: 3.3333e-04 - 32s/epoch - 166ms/step
Epoch 260/1000
2023-09-29 15:00:22.937 
Epoch 260/1000 
	 loss: 31.3314, MinusLogProbMetric: 31.3314, val_loss: 32.5416, val_MinusLogProbMetric: 32.5416

Epoch 260: val_loss did not improve from 31.26131
196/196 - 31s - loss: 31.3314 - MinusLogProbMetric: 31.3314 - val_loss: 32.5416 - val_MinusLogProbMetric: 32.5416 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 261/1000
2023-09-29 15:00:54.169 
Epoch 261/1000 
	 loss: 31.3559, MinusLogProbMetric: 31.3559, val_loss: 32.2681, val_MinusLogProbMetric: 32.2681

Epoch 261: val_loss did not improve from 31.26131
196/196 - 31s - loss: 31.3559 - MinusLogProbMetric: 31.3559 - val_loss: 32.2681 - val_MinusLogProbMetric: 32.2681 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 262/1000
2023-09-29 15:01:25.270 
Epoch 262/1000 
	 loss: 31.2668, MinusLogProbMetric: 31.2668, val_loss: 31.5361, val_MinusLogProbMetric: 31.5361

Epoch 262: val_loss did not improve from 31.26131
196/196 - 31s - loss: 31.2668 - MinusLogProbMetric: 31.2668 - val_loss: 31.5361 - val_MinusLogProbMetric: 31.5361 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 263/1000
2023-09-29 15:01:55.445 
Epoch 263/1000 
	 loss: 31.8223, MinusLogProbMetric: 31.8223, val_loss: 31.5596, val_MinusLogProbMetric: 31.5596

Epoch 263: val_loss did not improve from 31.26131
196/196 - 30s - loss: 31.8223 - MinusLogProbMetric: 31.8223 - val_loss: 31.5596 - val_MinusLogProbMetric: 31.5596 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 264/1000
2023-09-29 15:02:26.079 
Epoch 264/1000 
	 loss: 31.5231, MinusLogProbMetric: 31.5231, val_loss: 32.5133, val_MinusLogProbMetric: 32.5133

Epoch 264: val_loss did not improve from 31.26131
196/196 - 31s - loss: 31.5231 - MinusLogProbMetric: 31.5231 - val_loss: 32.5133 - val_MinusLogProbMetric: 32.5133 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 265/1000
2023-09-29 15:02:58.032 
Epoch 265/1000 
	 loss: 31.3878, MinusLogProbMetric: 31.3878, val_loss: 31.4982, val_MinusLogProbMetric: 31.4982

Epoch 265: val_loss did not improve from 31.26131
196/196 - 32s - loss: 31.3878 - MinusLogProbMetric: 31.3878 - val_loss: 31.4982 - val_MinusLogProbMetric: 31.4982 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 266/1000
2023-09-29 15:03:30.197 
Epoch 266/1000 
	 loss: 31.3275, MinusLogProbMetric: 31.3275, val_loss: 31.8814, val_MinusLogProbMetric: 31.8814

Epoch 266: val_loss did not improve from 31.26131
196/196 - 32s - loss: 31.3275 - MinusLogProbMetric: 31.3275 - val_loss: 31.8814 - val_MinusLogProbMetric: 31.8814 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 267/1000
2023-09-29 15:04:03.947 
Epoch 267/1000 
	 loss: 31.4280, MinusLogProbMetric: 31.4280, val_loss: 31.2911, val_MinusLogProbMetric: 31.2911

Epoch 267: val_loss did not improve from 31.26131
196/196 - 34s - loss: 31.4280 - MinusLogProbMetric: 31.4280 - val_loss: 31.2911 - val_MinusLogProbMetric: 31.2911 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 268/1000
2023-09-29 15:04:38.690 
Epoch 268/1000 
	 loss: 31.4157, MinusLogProbMetric: 31.4157, val_loss: 36.6612, val_MinusLogProbMetric: 36.6612

Epoch 268: val_loss did not improve from 31.26131
196/196 - 35s - loss: 31.4157 - MinusLogProbMetric: 31.4157 - val_loss: 36.6612 - val_MinusLogProbMetric: 36.6612 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 269/1000
2023-09-29 15:05:10.242 
Epoch 269/1000 
	 loss: 32.0135, MinusLogProbMetric: 32.0135, val_loss: 31.8349, val_MinusLogProbMetric: 31.8349

Epoch 269: val_loss did not improve from 31.26131
196/196 - 32s - loss: 32.0135 - MinusLogProbMetric: 32.0135 - val_loss: 31.8349 - val_MinusLogProbMetric: 31.8349 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 270/1000
2023-09-29 15:05:42.335 
Epoch 270/1000 
	 loss: 31.4637, MinusLogProbMetric: 31.4637, val_loss: 31.7842, val_MinusLogProbMetric: 31.7842

Epoch 270: val_loss did not improve from 31.26131
196/196 - 32s - loss: 31.4637 - MinusLogProbMetric: 31.4637 - val_loss: 31.7842 - val_MinusLogProbMetric: 31.7842 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 271/1000
2023-09-29 15:06:17.017 
Epoch 271/1000 
	 loss: 31.3772, MinusLogProbMetric: 31.3772, val_loss: 31.7508, val_MinusLogProbMetric: 31.7508

Epoch 271: val_loss did not improve from 31.26131
196/196 - 35s - loss: 31.3772 - MinusLogProbMetric: 31.3772 - val_loss: 31.7508 - val_MinusLogProbMetric: 31.7508 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 272/1000
2023-09-29 15:06:47.395 
Epoch 272/1000 
	 loss: 31.2414, MinusLogProbMetric: 31.2414, val_loss: 32.1829, val_MinusLogProbMetric: 32.1829

Epoch 272: val_loss did not improve from 31.26131
196/196 - 30s - loss: 31.2414 - MinusLogProbMetric: 31.2414 - val_loss: 32.1829 - val_MinusLogProbMetric: 32.1829 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 273/1000
2023-09-29 15:07:20.862 
Epoch 273/1000 
	 loss: 31.2457, MinusLogProbMetric: 31.2457, val_loss: 31.8222, val_MinusLogProbMetric: 31.8222

Epoch 273: val_loss did not improve from 31.26131
196/196 - 33s - loss: 31.2457 - MinusLogProbMetric: 31.2457 - val_loss: 31.8222 - val_MinusLogProbMetric: 31.8222 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 274/1000
2023-09-29 15:07:52.442 
Epoch 274/1000 
	 loss: 31.0792, MinusLogProbMetric: 31.0792, val_loss: 32.3565, val_MinusLogProbMetric: 32.3565

Epoch 274: val_loss did not improve from 31.26131
196/196 - 32s - loss: 31.0792 - MinusLogProbMetric: 31.0792 - val_loss: 32.3565 - val_MinusLogProbMetric: 32.3565 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 275/1000
2023-09-29 15:08:27.078 
Epoch 275/1000 
	 loss: 31.2427, MinusLogProbMetric: 31.2427, val_loss: 31.8693, val_MinusLogProbMetric: 31.8693

Epoch 275: val_loss did not improve from 31.26131
196/196 - 35s - loss: 31.2427 - MinusLogProbMetric: 31.2427 - val_loss: 31.8693 - val_MinusLogProbMetric: 31.8693 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 276/1000
2023-09-29 15:09:01.773 
Epoch 276/1000 
	 loss: 31.1773, MinusLogProbMetric: 31.1773, val_loss: 31.7172, val_MinusLogProbMetric: 31.7172

Epoch 276: val_loss did not improve from 31.26131
196/196 - 35s - loss: 31.1773 - MinusLogProbMetric: 31.1773 - val_loss: 31.7172 - val_MinusLogProbMetric: 31.7172 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 277/1000
2023-09-29 15:09:33.622 
Epoch 277/1000 
	 loss: 31.1903, MinusLogProbMetric: 31.1903, val_loss: 31.8969, val_MinusLogProbMetric: 31.8969

Epoch 277: val_loss did not improve from 31.26131
196/196 - 32s - loss: 31.1903 - MinusLogProbMetric: 31.1903 - val_loss: 31.8969 - val_MinusLogProbMetric: 31.8969 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 278/1000
2023-09-29 15:10:06.763 
Epoch 278/1000 
	 loss: 31.3266, MinusLogProbMetric: 31.3266, val_loss: 31.5200, val_MinusLogProbMetric: 31.5200

Epoch 278: val_loss did not improve from 31.26131
196/196 - 33s - loss: 31.3266 - MinusLogProbMetric: 31.3266 - val_loss: 31.5200 - val_MinusLogProbMetric: 31.5200 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 279/1000
2023-09-29 15:10:39.625 
Epoch 279/1000 
	 loss: 31.0819, MinusLogProbMetric: 31.0819, val_loss: 31.3382, val_MinusLogProbMetric: 31.3382

Epoch 279: val_loss did not improve from 31.26131
196/196 - 33s - loss: 31.0819 - MinusLogProbMetric: 31.0819 - val_loss: 31.3382 - val_MinusLogProbMetric: 31.3382 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 280/1000
2023-09-29 15:11:12.778 
Epoch 280/1000 
	 loss: 31.0522, MinusLogProbMetric: 31.0522, val_loss: 31.9695, val_MinusLogProbMetric: 31.9695

Epoch 280: val_loss did not improve from 31.26131
196/196 - 33s - loss: 31.0522 - MinusLogProbMetric: 31.0522 - val_loss: 31.9695 - val_MinusLogProbMetric: 31.9695 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 281/1000
2023-09-29 15:11:46.898 
Epoch 281/1000 
	 loss: 31.3067, MinusLogProbMetric: 31.3067, val_loss: 31.5027, val_MinusLogProbMetric: 31.5027

Epoch 281: val_loss did not improve from 31.26131
196/196 - 34s - loss: 31.3067 - MinusLogProbMetric: 31.3067 - val_loss: 31.5027 - val_MinusLogProbMetric: 31.5027 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 282/1000
2023-09-29 15:12:19.021 
Epoch 282/1000 
	 loss: 31.2383, MinusLogProbMetric: 31.2383, val_loss: 31.8233, val_MinusLogProbMetric: 31.8233

Epoch 282: val_loss did not improve from 31.26131
196/196 - 32s - loss: 31.2383 - MinusLogProbMetric: 31.2383 - val_loss: 31.8233 - val_MinusLogProbMetric: 31.8233 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 283/1000
2023-09-29 15:12:53.134 
Epoch 283/1000 
	 loss: 31.2666, MinusLogProbMetric: 31.2666, val_loss: 32.1028, val_MinusLogProbMetric: 32.1028

Epoch 283: val_loss did not improve from 31.26131
196/196 - 34s - loss: 31.2666 - MinusLogProbMetric: 31.2666 - val_loss: 32.1028 - val_MinusLogProbMetric: 32.1028 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 284/1000
2023-09-29 15:13:26.370 
Epoch 284/1000 
	 loss: 31.6603, MinusLogProbMetric: 31.6603, val_loss: 31.4083, val_MinusLogProbMetric: 31.4083

Epoch 284: val_loss did not improve from 31.26131
196/196 - 33s - loss: 31.6603 - MinusLogProbMetric: 31.6603 - val_loss: 31.4083 - val_MinusLogProbMetric: 31.4083 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 285/1000
2023-09-29 15:13:59.779 
Epoch 285/1000 
	 loss: 31.0754, MinusLogProbMetric: 31.0754, val_loss: 31.0734, val_MinusLogProbMetric: 31.0734

Epoch 285: val_loss improved from 31.26131 to 31.07335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 31.0754 - MinusLogProbMetric: 31.0754 - val_loss: 31.0734 - val_MinusLogProbMetric: 31.0734 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 286/1000
2023-09-29 15:14:34.027 
Epoch 286/1000 
	 loss: 30.9534, MinusLogProbMetric: 30.9534, val_loss: 32.1504, val_MinusLogProbMetric: 32.1504

Epoch 286: val_loss did not improve from 31.07335
196/196 - 33s - loss: 30.9534 - MinusLogProbMetric: 30.9534 - val_loss: 32.1504 - val_MinusLogProbMetric: 32.1504 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 287/1000
2023-09-29 15:15:05.669 
Epoch 287/1000 
	 loss: 31.2570, MinusLogProbMetric: 31.2570, val_loss: 31.1445, val_MinusLogProbMetric: 31.1445

Epoch 287: val_loss did not improve from 31.07335
196/196 - 32s - loss: 31.2570 - MinusLogProbMetric: 31.2570 - val_loss: 31.1445 - val_MinusLogProbMetric: 31.1445 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 288/1000
2023-09-29 15:15:35.452 
Epoch 288/1000 
	 loss: 30.9872, MinusLogProbMetric: 30.9872, val_loss: 31.4590, val_MinusLogProbMetric: 31.4590

Epoch 288: val_loss did not improve from 31.07335
196/196 - 30s - loss: 30.9872 - MinusLogProbMetric: 30.9872 - val_loss: 31.4590 - val_MinusLogProbMetric: 31.4590 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 289/1000
2023-09-29 15:16:08.036 
Epoch 289/1000 
	 loss: 31.0575, MinusLogProbMetric: 31.0575, val_loss: 31.7717, val_MinusLogProbMetric: 31.7717

Epoch 289: val_loss did not improve from 31.07335
196/196 - 33s - loss: 31.0575 - MinusLogProbMetric: 31.0575 - val_loss: 31.7717 - val_MinusLogProbMetric: 31.7717 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 290/1000
2023-09-29 15:16:41.459 
Epoch 290/1000 
	 loss: 31.0141, MinusLogProbMetric: 31.0141, val_loss: 30.8539, val_MinusLogProbMetric: 30.8539

Epoch 290: val_loss improved from 31.07335 to 30.85393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 31.0141 - MinusLogProbMetric: 31.0141 - val_loss: 30.8539 - val_MinusLogProbMetric: 30.8539 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 291/1000
2023-09-29 15:17:13.808 
Epoch 291/1000 
	 loss: 30.9640, MinusLogProbMetric: 30.9640, val_loss: 31.6652, val_MinusLogProbMetric: 31.6652

Epoch 291: val_loss did not improve from 30.85393
196/196 - 32s - loss: 30.9640 - MinusLogProbMetric: 30.9640 - val_loss: 31.6652 - val_MinusLogProbMetric: 31.6652 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 292/1000
2023-09-29 15:17:48.250 
Epoch 292/1000 
	 loss: 31.1750, MinusLogProbMetric: 31.1750, val_loss: 31.7895, val_MinusLogProbMetric: 31.7895

Epoch 292: val_loss did not improve from 30.85393
196/196 - 34s - loss: 31.1750 - MinusLogProbMetric: 31.1750 - val_loss: 31.7895 - val_MinusLogProbMetric: 31.7895 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 293/1000
2023-09-29 15:18:22.987 
Epoch 293/1000 
	 loss: 30.9070, MinusLogProbMetric: 30.9070, val_loss: 31.2191, val_MinusLogProbMetric: 31.2191

Epoch 293: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.9070 - MinusLogProbMetric: 30.9070 - val_loss: 31.2191 - val_MinusLogProbMetric: 31.2191 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 294/1000
2023-09-29 15:18:57.921 
Epoch 294/1000 
	 loss: 31.3633, MinusLogProbMetric: 31.3633, val_loss: 31.2309, val_MinusLogProbMetric: 31.2309

Epoch 294: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.3633 - MinusLogProbMetric: 31.3633 - val_loss: 31.2309 - val_MinusLogProbMetric: 31.2309 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 295/1000
2023-09-29 15:19:32.702 
Epoch 295/1000 
	 loss: 31.1494, MinusLogProbMetric: 31.1494, val_loss: 31.3274, val_MinusLogProbMetric: 31.3274

Epoch 295: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.1494 - MinusLogProbMetric: 31.1494 - val_loss: 31.3274 - val_MinusLogProbMetric: 31.3274 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 296/1000
2023-09-29 15:20:07.306 
Epoch 296/1000 
	 loss: 30.9385, MinusLogProbMetric: 30.9385, val_loss: 31.2080, val_MinusLogProbMetric: 31.2080

Epoch 296: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.9385 - MinusLogProbMetric: 30.9385 - val_loss: 31.2080 - val_MinusLogProbMetric: 31.2080 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 297/1000
2023-09-29 15:20:42.126 
Epoch 297/1000 
	 loss: 31.0222, MinusLogProbMetric: 31.0222, val_loss: 31.2655, val_MinusLogProbMetric: 31.2655

Epoch 297: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.0222 - MinusLogProbMetric: 31.0222 - val_loss: 31.2655 - val_MinusLogProbMetric: 31.2655 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 298/1000
2023-09-29 15:21:16.661 
Epoch 298/1000 
	 loss: 30.9716, MinusLogProbMetric: 30.9716, val_loss: 31.6596, val_MinusLogProbMetric: 31.6596

Epoch 298: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.9716 - MinusLogProbMetric: 30.9716 - val_loss: 31.6596 - val_MinusLogProbMetric: 31.6596 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 299/1000
2023-09-29 15:21:51.309 
Epoch 299/1000 
	 loss: 31.0760, MinusLogProbMetric: 31.0760, val_loss: 31.1187, val_MinusLogProbMetric: 31.1187

Epoch 299: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.0760 - MinusLogProbMetric: 31.0760 - val_loss: 31.1187 - val_MinusLogProbMetric: 31.1187 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 300/1000
2023-09-29 15:22:25.748 
Epoch 300/1000 
	 loss: 30.9195, MinusLogProbMetric: 30.9195, val_loss: 31.7483, val_MinusLogProbMetric: 31.7483

Epoch 300: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.9195 - MinusLogProbMetric: 30.9195 - val_loss: 31.7483 - val_MinusLogProbMetric: 31.7483 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 301/1000
2023-09-29 15:22:59.257 
Epoch 301/1000 
	 loss: 30.7767, MinusLogProbMetric: 30.7767, val_loss: 31.3825, val_MinusLogProbMetric: 31.3825

Epoch 301: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.7767 - MinusLogProbMetric: 30.7767 - val_loss: 31.3825 - val_MinusLogProbMetric: 31.3825 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 302/1000
2023-09-29 15:23:33.892 
Epoch 302/1000 
	 loss: 30.8841, MinusLogProbMetric: 30.8841, val_loss: 31.4438, val_MinusLogProbMetric: 31.4438

Epoch 302: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.8841 - MinusLogProbMetric: 30.8841 - val_loss: 31.4438 - val_MinusLogProbMetric: 31.4438 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 303/1000
2023-09-29 15:24:08.589 
Epoch 303/1000 
	 loss: 31.0800, MinusLogProbMetric: 31.0800, val_loss: 31.7790, val_MinusLogProbMetric: 31.7790

Epoch 303: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.0800 - MinusLogProbMetric: 31.0800 - val_loss: 31.7790 - val_MinusLogProbMetric: 31.7790 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 304/1000
2023-09-29 15:24:43.085 
Epoch 304/1000 
	 loss: 30.9929, MinusLogProbMetric: 30.9929, val_loss: 31.4875, val_MinusLogProbMetric: 31.4875

Epoch 304: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.9929 - MinusLogProbMetric: 30.9929 - val_loss: 31.4875 - val_MinusLogProbMetric: 31.4875 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 305/1000
2023-09-29 15:25:17.802 
Epoch 305/1000 
	 loss: 31.0058, MinusLogProbMetric: 31.0058, val_loss: 31.2255, val_MinusLogProbMetric: 31.2255

Epoch 305: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.0058 - MinusLogProbMetric: 31.0058 - val_loss: 31.2255 - val_MinusLogProbMetric: 31.2255 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 306/1000
2023-09-29 15:25:52.674 
Epoch 306/1000 
	 loss: 30.8044, MinusLogProbMetric: 30.8044, val_loss: 31.7308, val_MinusLogProbMetric: 31.7308

Epoch 306: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.8044 - MinusLogProbMetric: 30.8044 - val_loss: 31.7308 - val_MinusLogProbMetric: 31.7308 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 307/1000
2023-09-29 15:26:27.297 
Epoch 307/1000 
	 loss: 30.8189, MinusLogProbMetric: 30.8189, val_loss: 31.4581, val_MinusLogProbMetric: 31.4581

Epoch 307: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.8189 - MinusLogProbMetric: 30.8189 - val_loss: 31.4581 - val_MinusLogProbMetric: 31.4581 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 308/1000
2023-09-29 15:27:01.905 
Epoch 308/1000 
	 loss: 30.9208, MinusLogProbMetric: 30.9208, val_loss: 32.0382, val_MinusLogProbMetric: 32.0382

Epoch 308: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.9208 - MinusLogProbMetric: 30.9208 - val_loss: 32.0382 - val_MinusLogProbMetric: 32.0382 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 309/1000
2023-09-29 15:27:36.487 
Epoch 309/1000 
	 loss: 31.0741, MinusLogProbMetric: 31.0741, val_loss: 31.8788, val_MinusLogProbMetric: 31.8788

Epoch 309: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.0741 - MinusLogProbMetric: 31.0741 - val_loss: 31.8788 - val_MinusLogProbMetric: 31.8788 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 310/1000
2023-09-29 15:28:11.126 
Epoch 310/1000 
	 loss: 30.9950, MinusLogProbMetric: 30.9950, val_loss: 31.1303, val_MinusLogProbMetric: 31.1303

Epoch 310: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.9950 - MinusLogProbMetric: 30.9950 - val_loss: 31.1303 - val_MinusLogProbMetric: 31.1303 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 311/1000
2023-09-29 15:28:45.728 
Epoch 311/1000 
	 loss: 30.6241, MinusLogProbMetric: 30.6241, val_loss: 30.9269, val_MinusLogProbMetric: 30.9269

Epoch 311: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.6241 - MinusLogProbMetric: 30.6241 - val_loss: 30.9269 - val_MinusLogProbMetric: 30.9269 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 312/1000
2023-09-29 15:29:20.601 
Epoch 312/1000 
	 loss: 30.8692, MinusLogProbMetric: 30.8692, val_loss: 30.8988, val_MinusLogProbMetric: 30.8988

Epoch 312: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.8692 - MinusLogProbMetric: 30.8692 - val_loss: 30.8988 - val_MinusLogProbMetric: 30.8988 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 313/1000
2023-09-29 15:29:55.297 
Epoch 313/1000 
	 loss: 30.7290, MinusLogProbMetric: 30.7290, val_loss: 31.9256, val_MinusLogProbMetric: 31.9256

Epoch 313: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7290 - MinusLogProbMetric: 30.7290 - val_loss: 31.9256 - val_MinusLogProbMetric: 31.9256 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 314/1000
2023-09-29 15:30:29.922 
Epoch 314/1000 
	 loss: 30.7329, MinusLogProbMetric: 30.7329, val_loss: 32.0877, val_MinusLogProbMetric: 32.0877

Epoch 314: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7329 - MinusLogProbMetric: 30.7329 - val_loss: 32.0877 - val_MinusLogProbMetric: 32.0877 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 315/1000
2023-09-29 15:31:04.339 
Epoch 315/1000 
	 loss: 30.8578, MinusLogProbMetric: 30.8578, val_loss: 31.8634, val_MinusLogProbMetric: 31.8634

Epoch 315: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.8578 - MinusLogProbMetric: 30.8578 - val_loss: 31.8634 - val_MinusLogProbMetric: 31.8634 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 316/1000
2023-09-29 15:31:38.925 
Epoch 316/1000 
	 loss: 30.8285, MinusLogProbMetric: 30.8285, val_loss: 31.0448, val_MinusLogProbMetric: 31.0448

Epoch 316: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.8285 - MinusLogProbMetric: 30.8285 - val_loss: 31.0448 - val_MinusLogProbMetric: 31.0448 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 317/1000
2023-09-29 15:32:13.337 
Epoch 317/1000 
	 loss: 30.5547, MinusLogProbMetric: 30.5547, val_loss: 31.2546, val_MinusLogProbMetric: 31.2546

Epoch 317: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.5547 - MinusLogProbMetric: 30.5547 - val_loss: 31.2546 - val_MinusLogProbMetric: 31.2546 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 318/1000
2023-09-29 15:32:47.645 
Epoch 318/1000 
	 loss: 30.7352, MinusLogProbMetric: 30.7352, val_loss: 31.1239, val_MinusLogProbMetric: 31.1239

Epoch 318: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.7352 - MinusLogProbMetric: 30.7352 - val_loss: 31.1239 - val_MinusLogProbMetric: 31.1239 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 319/1000
2023-09-29 15:33:21.855 
Epoch 319/1000 
	 loss: 30.8797, MinusLogProbMetric: 30.8797, val_loss: 31.5840, val_MinusLogProbMetric: 31.5840

Epoch 319: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.8797 - MinusLogProbMetric: 30.8797 - val_loss: 31.5840 - val_MinusLogProbMetric: 31.5840 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 320/1000
2023-09-29 15:33:55.158 
Epoch 320/1000 
	 loss: 30.7313, MinusLogProbMetric: 30.7313, val_loss: 31.1266, val_MinusLogProbMetric: 31.1266

Epoch 320: val_loss did not improve from 30.85393
196/196 - 33s - loss: 30.7313 - MinusLogProbMetric: 30.7313 - val_loss: 31.1266 - val_MinusLogProbMetric: 31.1266 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 321/1000
2023-09-29 15:34:29.265 
Epoch 321/1000 
	 loss: 31.0416, MinusLogProbMetric: 31.0416, val_loss: 33.3956, val_MinusLogProbMetric: 33.3956

Epoch 321: val_loss did not improve from 30.85393
196/196 - 34s - loss: 31.0416 - MinusLogProbMetric: 31.0416 - val_loss: 33.3956 - val_MinusLogProbMetric: 33.3956 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 322/1000
2023-09-29 15:35:04.041 
Epoch 322/1000 
	 loss: 30.6347, MinusLogProbMetric: 30.6347, val_loss: 31.1747, val_MinusLogProbMetric: 31.1747

Epoch 322: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.6347 - MinusLogProbMetric: 30.6347 - val_loss: 31.1747 - val_MinusLogProbMetric: 31.1747 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 323/1000
2023-09-29 15:35:38.597 
Epoch 323/1000 
	 loss: 30.6617, MinusLogProbMetric: 30.6617, val_loss: 31.5392, val_MinusLogProbMetric: 31.5392

Epoch 323: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.6617 - MinusLogProbMetric: 30.6617 - val_loss: 31.5392 - val_MinusLogProbMetric: 31.5392 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 324/1000
2023-09-29 15:36:13.174 
Epoch 324/1000 
	 loss: 30.7207, MinusLogProbMetric: 30.7207, val_loss: 31.2875, val_MinusLogProbMetric: 31.2875

Epoch 324: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7207 - MinusLogProbMetric: 30.7207 - val_loss: 31.2875 - val_MinusLogProbMetric: 31.2875 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 325/1000
2023-09-29 15:36:47.600 
Epoch 325/1000 
	 loss: 30.8033, MinusLogProbMetric: 30.8033, val_loss: 30.8831, val_MinusLogProbMetric: 30.8831

Epoch 325: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.8033 - MinusLogProbMetric: 30.8033 - val_loss: 30.8831 - val_MinusLogProbMetric: 30.8831 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 326/1000
2023-09-29 15:37:22.331 
Epoch 326/1000 
	 loss: 30.5729, MinusLogProbMetric: 30.5729, val_loss: 31.3451, val_MinusLogProbMetric: 31.3451

Epoch 326: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.5729 - MinusLogProbMetric: 30.5729 - val_loss: 31.3451 - val_MinusLogProbMetric: 31.3451 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 327/1000
2023-09-29 15:37:56.018 
Epoch 327/1000 
	 loss: 30.6240, MinusLogProbMetric: 30.6240, val_loss: 31.2527, val_MinusLogProbMetric: 31.2527

Epoch 327: val_loss did not improve from 30.85393
196/196 - 34s - loss: 30.6240 - MinusLogProbMetric: 30.6240 - val_loss: 31.2527 - val_MinusLogProbMetric: 31.2527 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 328/1000
2023-09-29 15:38:30.771 
Epoch 328/1000 
	 loss: 30.7220, MinusLogProbMetric: 30.7220, val_loss: 30.9265, val_MinusLogProbMetric: 30.9265

Epoch 328: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7220 - MinusLogProbMetric: 30.7220 - val_loss: 30.9265 - val_MinusLogProbMetric: 30.9265 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 329/1000
2023-09-29 15:39:05.476 
Epoch 329/1000 
	 loss: 30.7053, MinusLogProbMetric: 30.7053, val_loss: 31.1722, val_MinusLogProbMetric: 31.1722

Epoch 329: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7053 - MinusLogProbMetric: 30.7053 - val_loss: 31.1722 - val_MinusLogProbMetric: 31.1722 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 330/1000
2023-09-29 15:39:40.147 
Epoch 330/1000 
	 loss: 30.7674, MinusLogProbMetric: 30.7674, val_loss: 30.9696, val_MinusLogProbMetric: 30.9696

Epoch 330: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7674 - MinusLogProbMetric: 30.7674 - val_loss: 30.9696 - val_MinusLogProbMetric: 30.9696 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 331/1000
2023-09-29 15:40:14.795 
Epoch 331/1000 
	 loss: 30.8008, MinusLogProbMetric: 30.8008, val_loss: 31.3489, val_MinusLogProbMetric: 31.3489

Epoch 331: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.8008 - MinusLogProbMetric: 30.8008 - val_loss: 31.3489 - val_MinusLogProbMetric: 31.3489 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 332/1000
2023-09-29 15:40:49.669 
Epoch 332/1000 
	 loss: 31.0503, MinusLogProbMetric: 31.0503, val_loss: 30.9885, val_MinusLogProbMetric: 30.9885

Epoch 332: val_loss did not improve from 30.85393
196/196 - 35s - loss: 31.0503 - MinusLogProbMetric: 31.0503 - val_loss: 30.9885 - val_MinusLogProbMetric: 30.9885 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 333/1000
2023-09-29 15:41:24.476 
Epoch 333/1000 
	 loss: 30.5778, MinusLogProbMetric: 30.5778, val_loss: 31.0651, val_MinusLogProbMetric: 31.0651

Epoch 333: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.5778 - MinusLogProbMetric: 30.5778 - val_loss: 31.0651 - val_MinusLogProbMetric: 31.0651 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 334/1000
2023-09-29 15:41:59.234 
Epoch 334/1000 
	 loss: 30.6465, MinusLogProbMetric: 30.6465, val_loss: 31.7523, val_MinusLogProbMetric: 31.7523

Epoch 334: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.6465 - MinusLogProbMetric: 30.6465 - val_loss: 31.7523 - val_MinusLogProbMetric: 31.7523 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 335/1000
2023-09-29 15:42:34.065 
Epoch 335/1000 
	 loss: 30.7376, MinusLogProbMetric: 30.7376, val_loss: 31.4917, val_MinusLogProbMetric: 31.4917

Epoch 335: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7376 - MinusLogProbMetric: 30.7376 - val_loss: 31.4917 - val_MinusLogProbMetric: 31.4917 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 336/1000
2023-09-29 15:43:08.907 
Epoch 336/1000 
	 loss: 30.4634, MinusLogProbMetric: 30.4634, val_loss: 32.3117, val_MinusLogProbMetric: 32.3117

Epoch 336: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.4634 - MinusLogProbMetric: 30.4634 - val_loss: 32.3117 - val_MinusLogProbMetric: 32.3117 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 337/1000
2023-09-29 15:43:43.582 
Epoch 337/1000 
	 loss: 30.7739, MinusLogProbMetric: 30.7739, val_loss: 31.3098, val_MinusLogProbMetric: 31.3098

Epoch 337: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7739 - MinusLogProbMetric: 30.7739 - val_loss: 31.3098 - val_MinusLogProbMetric: 31.3098 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 338/1000
2023-09-29 15:44:18.652 
Epoch 338/1000 
	 loss: 30.7327, MinusLogProbMetric: 30.7327, val_loss: 31.5305, val_MinusLogProbMetric: 31.5305

Epoch 338: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.7327 - MinusLogProbMetric: 30.7327 - val_loss: 31.5305 - val_MinusLogProbMetric: 31.5305 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 339/1000
2023-09-29 15:44:53.475 
Epoch 339/1000 
	 loss: 30.6030, MinusLogProbMetric: 30.6030, val_loss: 31.2994, val_MinusLogProbMetric: 31.2994

Epoch 339: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.6030 - MinusLogProbMetric: 30.6030 - val_loss: 31.2994 - val_MinusLogProbMetric: 31.2994 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 340/1000
2023-09-29 15:45:28.276 
Epoch 340/1000 
	 loss: 30.3498, MinusLogProbMetric: 30.3498, val_loss: 31.4898, val_MinusLogProbMetric: 31.4898

Epoch 340: val_loss did not improve from 30.85393
196/196 - 35s - loss: 30.3498 - MinusLogProbMetric: 30.3498 - val_loss: 31.4898 - val_MinusLogProbMetric: 31.4898 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 341/1000
2023-09-29 15:46:02.640 
Epoch 341/1000 
	 loss: 29.8806, MinusLogProbMetric: 29.8806, val_loss: 30.1878, val_MinusLogProbMetric: 30.1878

Epoch 341: val_loss improved from 30.85393 to 30.18782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 29.8806 - MinusLogProbMetric: 29.8806 - val_loss: 30.1878 - val_MinusLogProbMetric: 30.1878 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 342/1000
2023-09-29 15:46:38.374 
Epoch 342/1000 
	 loss: 29.8449, MinusLogProbMetric: 29.8449, val_loss: 30.1919, val_MinusLogProbMetric: 30.1919

Epoch 342: val_loss did not improve from 30.18782
196/196 - 35s - loss: 29.8449 - MinusLogProbMetric: 29.8449 - val_loss: 30.1919 - val_MinusLogProbMetric: 30.1919 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 343/1000
2023-09-29 15:47:12.981 
Epoch 343/1000 
	 loss: 29.8265, MinusLogProbMetric: 29.8265, val_loss: 30.5006, val_MinusLogProbMetric: 30.5006

Epoch 343: val_loss did not improve from 30.18782
196/196 - 35s - loss: 29.8265 - MinusLogProbMetric: 29.8265 - val_loss: 30.5006 - val_MinusLogProbMetric: 30.5006 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 344/1000
2023-09-29 15:47:47.863 
Epoch 344/1000 
	 loss: 29.8125, MinusLogProbMetric: 29.8125, val_loss: 30.4333, val_MinusLogProbMetric: 30.4333

Epoch 344: val_loss did not improve from 30.18782
196/196 - 35s - loss: 29.8125 - MinusLogProbMetric: 29.8125 - val_loss: 30.4333 - val_MinusLogProbMetric: 30.4333 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 345/1000
2023-09-29 15:48:21.499 
Epoch 345/1000 
	 loss: 29.8183, MinusLogProbMetric: 29.8183, val_loss: 30.1847, val_MinusLogProbMetric: 30.1847

Epoch 345: val_loss improved from 30.18782 to 30.18468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 29.8183 - MinusLogProbMetric: 29.8183 - val_loss: 30.1847 - val_MinusLogProbMetric: 30.1847 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 346/1000
2023-09-29 15:48:56.970 
Epoch 346/1000 
	 loss: 29.8339, MinusLogProbMetric: 29.8339, val_loss: 30.3582, val_MinusLogProbMetric: 30.3582

Epoch 346: val_loss did not improve from 30.18468
196/196 - 35s - loss: 29.8339 - MinusLogProbMetric: 29.8339 - val_loss: 30.3582 - val_MinusLogProbMetric: 30.3582 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 347/1000
2023-09-29 15:49:31.669 
Epoch 347/1000 
	 loss: 29.8286, MinusLogProbMetric: 29.8286, val_loss: 30.2694, val_MinusLogProbMetric: 30.2694

Epoch 347: val_loss did not improve from 30.18468
196/196 - 35s - loss: 29.8286 - MinusLogProbMetric: 29.8286 - val_loss: 30.2694 - val_MinusLogProbMetric: 30.2694 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 348/1000
2023-09-29 15:50:06.453 
Epoch 348/1000 
	 loss: 29.8461, MinusLogProbMetric: 29.8461, val_loss: 30.7805, val_MinusLogProbMetric: 30.7805

Epoch 348: val_loss did not improve from 30.18468
196/196 - 35s - loss: 29.8461 - MinusLogProbMetric: 29.8461 - val_loss: 30.7805 - val_MinusLogProbMetric: 30.7805 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 349/1000
2023-09-29 15:50:41.106 
Epoch 349/1000 
	 loss: 29.8423, MinusLogProbMetric: 29.8423, val_loss: 30.4748, val_MinusLogProbMetric: 30.4748

Epoch 349: val_loss did not improve from 30.18468
196/196 - 35s - loss: 29.8423 - MinusLogProbMetric: 29.8423 - val_loss: 30.4748 - val_MinusLogProbMetric: 30.4748 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 350/1000
2023-09-29 15:51:15.960 
Epoch 350/1000 
	 loss: 29.8252, MinusLogProbMetric: 29.8252, val_loss: 30.2560, val_MinusLogProbMetric: 30.2560

Epoch 350: val_loss did not improve from 30.18468
196/196 - 35s - loss: 29.8252 - MinusLogProbMetric: 29.8252 - val_loss: 30.2560 - val_MinusLogProbMetric: 30.2560 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 351/1000
2023-09-29 15:51:50.699 
Epoch 351/1000 
	 loss: 29.7628, MinusLogProbMetric: 29.7628, val_loss: 30.3204, val_MinusLogProbMetric: 30.3204

Epoch 351: val_loss did not improve from 30.18468
196/196 - 35s - loss: 29.7628 - MinusLogProbMetric: 29.7628 - val_loss: 30.3204 - val_MinusLogProbMetric: 30.3204 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 352/1000
2023-09-29 15:52:25.593 
Epoch 352/1000 
	 loss: 29.8453, MinusLogProbMetric: 29.8453, val_loss: 30.2728, val_MinusLogProbMetric: 30.2728

Epoch 352: val_loss did not improve from 30.18468
196/196 - 35s - loss: 29.8453 - MinusLogProbMetric: 29.8453 - val_loss: 30.2728 - val_MinusLogProbMetric: 30.2728 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 353/1000
2023-09-29 15:53:00.280 
Epoch 353/1000 
	 loss: 29.8083, MinusLogProbMetric: 29.8083, val_loss: 30.1781, val_MinusLogProbMetric: 30.1781

Epoch 353: val_loss improved from 30.18468 to 30.17813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 29.8083 - MinusLogProbMetric: 29.8083 - val_loss: 30.1781 - val_MinusLogProbMetric: 30.1781 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 354/1000
2023-09-29 15:53:35.862 
Epoch 354/1000 
	 loss: 29.8619, MinusLogProbMetric: 29.8619, val_loss: 30.2206, val_MinusLogProbMetric: 30.2206

Epoch 354: val_loss did not improve from 30.17813
196/196 - 35s - loss: 29.8619 - MinusLogProbMetric: 29.8619 - val_loss: 30.2206 - val_MinusLogProbMetric: 30.2206 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 355/1000
2023-09-29 15:54:10.749 
Epoch 355/1000 
	 loss: 29.7371, MinusLogProbMetric: 29.7371, val_loss: 30.1539, val_MinusLogProbMetric: 30.1539

Epoch 355: val_loss improved from 30.17813 to 30.15395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 29.7371 - MinusLogProbMetric: 29.7371 - val_loss: 30.1539 - val_MinusLogProbMetric: 30.1539 - lr: 1.6667e-04 - 36s/epoch - 181ms/step
Epoch 356/1000
2023-09-29 15:54:46.440 
Epoch 356/1000 
	 loss: 29.8144, MinusLogProbMetric: 29.8144, val_loss: 30.3239, val_MinusLogProbMetric: 30.3239

Epoch 356: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.8144 - MinusLogProbMetric: 29.8144 - val_loss: 30.3239 - val_MinusLogProbMetric: 30.3239 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 357/1000
2023-09-29 15:55:21.553 
Epoch 357/1000 
	 loss: 29.8002, MinusLogProbMetric: 29.8002, val_loss: 30.3105, val_MinusLogProbMetric: 30.3105

Epoch 357: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.8002 - MinusLogProbMetric: 29.8002 - val_loss: 30.3105 - val_MinusLogProbMetric: 30.3105 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 358/1000
2023-09-29 15:55:56.364 
Epoch 358/1000 
	 loss: 29.7643, MinusLogProbMetric: 29.7643, val_loss: 30.3058, val_MinusLogProbMetric: 30.3058

Epoch 358: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7643 - MinusLogProbMetric: 29.7643 - val_loss: 30.3058 - val_MinusLogProbMetric: 30.3058 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 359/1000
2023-09-29 15:56:31.470 
Epoch 359/1000 
	 loss: 29.7824, MinusLogProbMetric: 29.7824, val_loss: 30.3590, val_MinusLogProbMetric: 30.3590

Epoch 359: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7824 - MinusLogProbMetric: 29.7824 - val_loss: 30.3590 - val_MinusLogProbMetric: 30.3590 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 360/1000
2023-09-29 15:57:05.528 
Epoch 360/1000 
	 loss: 29.8041, MinusLogProbMetric: 29.8041, val_loss: 30.2768, val_MinusLogProbMetric: 30.2768

Epoch 360: val_loss did not improve from 30.15395
196/196 - 34s - loss: 29.8041 - MinusLogProbMetric: 29.8041 - val_loss: 30.2768 - val_MinusLogProbMetric: 30.2768 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 361/1000
2023-09-29 15:57:40.623 
Epoch 361/1000 
	 loss: 29.7715, MinusLogProbMetric: 29.7715, val_loss: 30.5511, val_MinusLogProbMetric: 30.5511

Epoch 361: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7715 - MinusLogProbMetric: 29.7715 - val_loss: 30.5511 - val_MinusLogProbMetric: 30.5511 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 362/1000
2023-09-29 15:58:15.499 
Epoch 362/1000 
	 loss: 29.8115, MinusLogProbMetric: 29.8115, val_loss: 30.4228, val_MinusLogProbMetric: 30.4228

Epoch 362: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.8115 - MinusLogProbMetric: 29.8115 - val_loss: 30.4228 - val_MinusLogProbMetric: 30.4228 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 363/1000
2023-09-29 15:58:50.369 
Epoch 363/1000 
	 loss: 29.7962, MinusLogProbMetric: 29.7962, val_loss: 30.2082, val_MinusLogProbMetric: 30.2082

Epoch 363: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7962 - MinusLogProbMetric: 29.7962 - val_loss: 30.2082 - val_MinusLogProbMetric: 30.2082 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 364/1000
2023-09-29 15:59:25.399 
Epoch 364/1000 
	 loss: 29.7395, MinusLogProbMetric: 29.7395, val_loss: 30.2764, val_MinusLogProbMetric: 30.2764

Epoch 364: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7395 - MinusLogProbMetric: 29.7395 - val_loss: 30.2764 - val_MinusLogProbMetric: 30.2764 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 365/1000
2023-09-29 16:00:00.417 
Epoch 365/1000 
	 loss: 29.7921, MinusLogProbMetric: 29.7921, val_loss: 30.2115, val_MinusLogProbMetric: 30.2115

Epoch 365: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7921 - MinusLogProbMetric: 29.7921 - val_loss: 30.2115 - val_MinusLogProbMetric: 30.2115 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 366/1000
2023-09-29 16:00:35.792 
Epoch 366/1000 
	 loss: 29.8666, MinusLogProbMetric: 29.8666, val_loss: 30.2103, val_MinusLogProbMetric: 30.2103

Epoch 366: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.8666 - MinusLogProbMetric: 29.8666 - val_loss: 30.2103 - val_MinusLogProbMetric: 30.2103 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 367/1000
2023-09-29 16:01:10.907 
Epoch 367/1000 
	 loss: 29.7605, MinusLogProbMetric: 29.7605, val_loss: 30.6246, val_MinusLogProbMetric: 30.6246

Epoch 367: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7605 - MinusLogProbMetric: 29.7605 - val_loss: 30.6246 - val_MinusLogProbMetric: 30.6246 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 368/1000
2023-09-29 16:01:46.115 
Epoch 368/1000 
	 loss: 29.7674, MinusLogProbMetric: 29.7674, val_loss: 30.4684, val_MinusLogProbMetric: 30.4684

Epoch 368: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7674 - MinusLogProbMetric: 29.7674 - val_loss: 30.4684 - val_MinusLogProbMetric: 30.4684 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 369/1000
2023-09-29 16:02:20.724 
Epoch 369/1000 
	 loss: 29.7395, MinusLogProbMetric: 29.7395, val_loss: 30.4991, val_MinusLogProbMetric: 30.4991

Epoch 369: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7395 - MinusLogProbMetric: 29.7395 - val_loss: 30.4991 - val_MinusLogProbMetric: 30.4991 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 370/1000
2023-09-29 16:02:55.510 
Epoch 370/1000 
	 loss: 29.7319, MinusLogProbMetric: 29.7319, val_loss: 30.1936, val_MinusLogProbMetric: 30.1936

Epoch 370: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7319 - MinusLogProbMetric: 29.7319 - val_loss: 30.1936 - val_MinusLogProbMetric: 30.1936 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 371/1000
2023-09-29 16:03:30.357 
Epoch 371/1000 
	 loss: 29.7323, MinusLogProbMetric: 29.7323, val_loss: 30.1708, val_MinusLogProbMetric: 30.1708

Epoch 371: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7323 - MinusLogProbMetric: 29.7323 - val_loss: 30.1708 - val_MinusLogProbMetric: 30.1708 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 372/1000
2023-09-29 16:04:05.362 
Epoch 372/1000 
	 loss: 29.7174, MinusLogProbMetric: 29.7174, val_loss: 30.5356, val_MinusLogProbMetric: 30.5356

Epoch 372: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7174 - MinusLogProbMetric: 29.7174 - val_loss: 30.5356 - val_MinusLogProbMetric: 30.5356 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 373/1000
2023-09-29 16:04:40.527 
Epoch 373/1000 
	 loss: 29.7311, MinusLogProbMetric: 29.7311, val_loss: 30.2276, val_MinusLogProbMetric: 30.2276

Epoch 373: val_loss did not improve from 30.15395
196/196 - 35s - loss: 29.7311 - MinusLogProbMetric: 29.7311 - val_loss: 30.2276 - val_MinusLogProbMetric: 30.2276 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 374/1000
2023-09-29 16:05:15.854 
Epoch 374/1000 
	 loss: 29.7238, MinusLogProbMetric: 29.7238, val_loss: 30.1338, val_MinusLogProbMetric: 30.1338

Epoch 374: val_loss improved from 30.15395 to 30.13379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 29.7238 - MinusLogProbMetric: 29.7238 - val_loss: 30.1338 - val_MinusLogProbMetric: 30.1338 - lr: 1.6667e-04 - 36s/epoch - 184ms/step
Epoch 375/1000
2023-09-29 16:05:51.690 
Epoch 375/1000 
	 loss: 29.7305, MinusLogProbMetric: 29.7305, val_loss: 30.6615, val_MinusLogProbMetric: 30.6615

Epoch 375: val_loss did not improve from 30.13379
196/196 - 35s - loss: 29.7305 - MinusLogProbMetric: 29.7305 - val_loss: 30.6615 - val_MinusLogProbMetric: 30.6615 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 376/1000
2023-09-29 16:06:26.941 
Epoch 376/1000 
	 loss: 29.7131, MinusLogProbMetric: 29.7131, val_loss: 30.1198, val_MinusLogProbMetric: 30.1198

Epoch 376: val_loss improved from 30.13379 to 30.11978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 29.7131 - MinusLogProbMetric: 29.7131 - val_loss: 30.1198 - val_MinusLogProbMetric: 30.1198 - lr: 1.6667e-04 - 36s/epoch - 183ms/step
Epoch 377/1000
2023-09-29 16:07:02.852 
Epoch 377/1000 
	 loss: 29.7251, MinusLogProbMetric: 29.7251, val_loss: 30.4796, val_MinusLogProbMetric: 30.4796

Epoch 377: val_loss did not improve from 30.11978
196/196 - 35s - loss: 29.7251 - MinusLogProbMetric: 29.7251 - val_loss: 30.4796 - val_MinusLogProbMetric: 30.4796 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 378/1000
2023-09-29 16:07:37.920 
Epoch 378/1000 
	 loss: 29.7515, MinusLogProbMetric: 29.7515, val_loss: 30.2603, val_MinusLogProbMetric: 30.2603

Epoch 378: val_loss did not improve from 30.11978
196/196 - 35s - loss: 29.7515 - MinusLogProbMetric: 29.7515 - val_loss: 30.2603 - val_MinusLogProbMetric: 30.2603 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 379/1000
2023-09-29 16:08:12.992 
Epoch 379/1000 
	 loss: 29.7166, MinusLogProbMetric: 29.7166, val_loss: 30.4900, val_MinusLogProbMetric: 30.4900

Epoch 379: val_loss did not improve from 30.11978
196/196 - 35s - loss: 29.7166 - MinusLogProbMetric: 29.7166 - val_loss: 30.4900 - val_MinusLogProbMetric: 30.4900 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 380/1000
2023-09-29 16:08:48.236 
Epoch 380/1000 
	 loss: 29.7068, MinusLogProbMetric: 29.7068, val_loss: 30.2493, val_MinusLogProbMetric: 30.2493

Epoch 380: val_loss did not improve from 30.11978
196/196 - 35s - loss: 29.7068 - MinusLogProbMetric: 29.7068 - val_loss: 30.2493 - val_MinusLogProbMetric: 30.2493 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 381/1000
2023-09-29 16:09:23.067 
Epoch 381/1000 
	 loss: 29.7176, MinusLogProbMetric: 29.7176, val_loss: 30.0650, val_MinusLogProbMetric: 30.0650

Epoch 381: val_loss improved from 30.11978 to 30.06497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 29.7176 - MinusLogProbMetric: 29.7176 - val_loss: 30.0650 - val_MinusLogProbMetric: 30.0650 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 382/1000
2023-09-29 16:09:59.038 
Epoch 382/1000 
	 loss: 29.7368, MinusLogProbMetric: 29.7368, val_loss: 30.3605, val_MinusLogProbMetric: 30.3605

Epoch 382: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.7368 - MinusLogProbMetric: 29.7368 - val_loss: 30.3605 - val_MinusLogProbMetric: 30.3605 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 383/1000
2023-09-29 16:10:34.160 
Epoch 383/1000 
	 loss: 29.7000, MinusLogProbMetric: 29.7000, val_loss: 30.1259, val_MinusLogProbMetric: 30.1259

Epoch 383: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.7000 - MinusLogProbMetric: 29.7000 - val_loss: 30.1259 - val_MinusLogProbMetric: 30.1259 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 384/1000
2023-09-29 16:11:09.370 
Epoch 384/1000 
	 loss: 29.6882, MinusLogProbMetric: 29.6882, val_loss: 30.2787, val_MinusLogProbMetric: 30.2787

Epoch 384: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6882 - MinusLogProbMetric: 29.6882 - val_loss: 30.2787 - val_MinusLogProbMetric: 30.2787 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 385/1000
2023-09-29 16:11:44.579 
Epoch 385/1000 
	 loss: 29.7919, MinusLogProbMetric: 29.7919, val_loss: 30.2642, val_MinusLogProbMetric: 30.2642

Epoch 385: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.7919 - MinusLogProbMetric: 29.7919 - val_loss: 30.2642 - val_MinusLogProbMetric: 30.2642 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 386/1000
2023-09-29 16:12:19.920 
Epoch 386/1000 
	 loss: 29.7259, MinusLogProbMetric: 29.7259, val_loss: 30.5162, val_MinusLogProbMetric: 30.5162

Epoch 386: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.7259 - MinusLogProbMetric: 29.7259 - val_loss: 30.5162 - val_MinusLogProbMetric: 30.5162 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 387/1000
2023-09-29 16:12:55.156 
Epoch 387/1000 
	 loss: 29.6976, MinusLogProbMetric: 29.6976, val_loss: 30.1557, val_MinusLogProbMetric: 30.1557

Epoch 387: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6976 - MinusLogProbMetric: 29.6976 - val_loss: 30.1557 - val_MinusLogProbMetric: 30.1557 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 388/1000
2023-09-29 16:13:30.343 
Epoch 388/1000 
	 loss: 29.7399, MinusLogProbMetric: 29.7399, val_loss: 30.1291, val_MinusLogProbMetric: 30.1291

Epoch 388: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.7399 - MinusLogProbMetric: 29.7399 - val_loss: 30.1291 - val_MinusLogProbMetric: 30.1291 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 389/1000
2023-09-29 16:14:05.502 
Epoch 389/1000 
	 loss: 29.6908, MinusLogProbMetric: 29.6908, val_loss: 30.2569, val_MinusLogProbMetric: 30.2569

Epoch 389: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6908 - MinusLogProbMetric: 29.6908 - val_loss: 30.2569 - val_MinusLogProbMetric: 30.2569 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 390/1000
2023-09-29 16:14:40.857 
Epoch 390/1000 
	 loss: 29.6934, MinusLogProbMetric: 29.6934, val_loss: 30.3116, val_MinusLogProbMetric: 30.3116

Epoch 390: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6934 - MinusLogProbMetric: 29.6934 - val_loss: 30.3116 - val_MinusLogProbMetric: 30.3116 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 391/1000
2023-09-29 16:15:15.933 
Epoch 391/1000 
	 loss: 29.6662, MinusLogProbMetric: 29.6662, val_loss: 30.0741, val_MinusLogProbMetric: 30.0741

Epoch 391: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6662 - MinusLogProbMetric: 29.6662 - val_loss: 30.0741 - val_MinusLogProbMetric: 30.0741 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 392/1000
2023-09-29 16:15:50.946 
Epoch 392/1000 
	 loss: 29.6818, MinusLogProbMetric: 29.6818, val_loss: 30.4311, val_MinusLogProbMetric: 30.4311

Epoch 392: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6818 - MinusLogProbMetric: 29.6818 - val_loss: 30.4311 - val_MinusLogProbMetric: 30.4311 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 393/1000
2023-09-29 16:16:26.339 
Epoch 393/1000 
	 loss: 29.6110, MinusLogProbMetric: 29.6110, val_loss: 30.1137, val_MinusLogProbMetric: 30.1137

Epoch 393: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6110 - MinusLogProbMetric: 29.6110 - val_loss: 30.1137 - val_MinusLogProbMetric: 30.1137 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 394/1000
2023-09-29 16:17:01.371 
Epoch 394/1000 
	 loss: 29.6469, MinusLogProbMetric: 29.6469, val_loss: 30.1590, val_MinusLogProbMetric: 30.1590

Epoch 394: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6469 - MinusLogProbMetric: 29.6469 - val_loss: 30.1590 - val_MinusLogProbMetric: 30.1590 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 395/1000
2023-09-29 16:17:36.530 
Epoch 395/1000 
	 loss: 29.6721, MinusLogProbMetric: 29.6721, val_loss: 30.2851, val_MinusLogProbMetric: 30.2851

Epoch 395: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6721 - MinusLogProbMetric: 29.6721 - val_loss: 30.2851 - val_MinusLogProbMetric: 30.2851 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 396/1000
2023-09-29 16:18:11.779 
Epoch 396/1000 
	 loss: 29.7443, MinusLogProbMetric: 29.7443, val_loss: 30.3019, val_MinusLogProbMetric: 30.3019

Epoch 396: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.7443 - MinusLogProbMetric: 29.7443 - val_loss: 30.3019 - val_MinusLogProbMetric: 30.3019 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 397/1000
2023-09-29 16:18:47.120 
Epoch 397/1000 
	 loss: 29.6927, MinusLogProbMetric: 29.6927, val_loss: 30.4957, val_MinusLogProbMetric: 30.4957

Epoch 397: val_loss did not improve from 30.06497
196/196 - 35s - loss: 29.6927 - MinusLogProbMetric: 29.6927 - val_loss: 30.4957 - val_MinusLogProbMetric: 30.4957 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 398/1000
2023-09-29 16:19:22.340 
Epoch 398/1000 
	 loss: 29.7913, MinusLogProbMetric: 29.7913, val_loss: 29.9309, val_MinusLogProbMetric: 29.9309

Epoch 398: val_loss improved from 30.06497 to 29.93092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 29.7913 - MinusLogProbMetric: 29.7913 - val_loss: 29.9309 - val_MinusLogProbMetric: 29.9309 - lr: 1.6667e-04 - 36s/epoch - 183ms/step
Epoch 399/1000
2023-09-29 16:19:58.298 
Epoch 399/1000 
	 loss: 29.6573, MinusLogProbMetric: 29.6573, val_loss: 30.1311, val_MinusLogProbMetric: 30.1311

Epoch 399: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6573 - MinusLogProbMetric: 29.6573 - val_loss: 30.1311 - val_MinusLogProbMetric: 30.1311 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 400/1000
2023-09-29 16:20:33.674 
Epoch 400/1000 
	 loss: 29.7443, MinusLogProbMetric: 29.7443, val_loss: 30.2009, val_MinusLogProbMetric: 30.2009

Epoch 400: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.7443 - MinusLogProbMetric: 29.7443 - val_loss: 30.2009 - val_MinusLogProbMetric: 30.2009 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 401/1000
2023-09-29 16:21:08.886 
Epoch 401/1000 
	 loss: 29.6290, MinusLogProbMetric: 29.6290, val_loss: 30.1797, val_MinusLogProbMetric: 30.1797

Epoch 401: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6290 - MinusLogProbMetric: 29.6290 - val_loss: 30.1797 - val_MinusLogProbMetric: 30.1797 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 402/1000
2023-09-29 16:21:43.926 
Epoch 402/1000 
	 loss: 29.6684, MinusLogProbMetric: 29.6684, val_loss: 30.4949, val_MinusLogProbMetric: 30.4949

Epoch 402: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6684 - MinusLogProbMetric: 29.6684 - val_loss: 30.4949 - val_MinusLogProbMetric: 30.4949 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 403/1000
2023-09-29 16:22:19.275 
Epoch 403/1000 
	 loss: 29.6480, MinusLogProbMetric: 29.6480, val_loss: 30.3555, val_MinusLogProbMetric: 30.3555

Epoch 403: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6480 - MinusLogProbMetric: 29.6480 - val_loss: 30.3555 - val_MinusLogProbMetric: 30.3555 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 404/1000
2023-09-29 16:22:54.336 
Epoch 404/1000 
	 loss: 29.6477, MinusLogProbMetric: 29.6477, val_loss: 30.1073, val_MinusLogProbMetric: 30.1073

Epoch 404: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6477 - MinusLogProbMetric: 29.6477 - val_loss: 30.1073 - val_MinusLogProbMetric: 30.1073 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 405/1000
2023-09-29 16:23:29.579 
Epoch 405/1000 
	 loss: 29.6084, MinusLogProbMetric: 29.6084, val_loss: 30.2031, val_MinusLogProbMetric: 30.2031

Epoch 405: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6084 - MinusLogProbMetric: 29.6084 - val_loss: 30.2031 - val_MinusLogProbMetric: 30.2031 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 406/1000
2023-09-29 16:24:04.664 
Epoch 406/1000 
	 loss: 29.6712, MinusLogProbMetric: 29.6712, val_loss: 30.3721, val_MinusLogProbMetric: 30.3721

Epoch 406: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6712 - MinusLogProbMetric: 29.6712 - val_loss: 30.3721 - val_MinusLogProbMetric: 30.3721 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 407/1000
2023-09-29 16:24:40.048 
Epoch 407/1000 
	 loss: 29.6270, MinusLogProbMetric: 29.6270, val_loss: 30.2284, val_MinusLogProbMetric: 30.2284

Epoch 407: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6270 - MinusLogProbMetric: 29.6270 - val_loss: 30.2284 - val_MinusLogProbMetric: 30.2284 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 408/1000
2023-09-29 16:25:15.363 
Epoch 408/1000 
	 loss: 29.6542, MinusLogProbMetric: 29.6542, val_loss: 30.2625, val_MinusLogProbMetric: 30.2625

Epoch 408: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6542 - MinusLogProbMetric: 29.6542 - val_loss: 30.2625 - val_MinusLogProbMetric: 30.2625 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 409/1000
2023-09-29 16:25:50.304 
Epoch 409/1000 
	 loss: 29.6386, MinusLogProbMetric: 29.6386, val_loss: 30.4301, val_MinusLogProbMetric: 30.4301

Epoch 409: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6386 - MinusLogProbMetric: 29.6386 - val_loss: 30.4301 - val_MinusLogProbMetric: 30.4301 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 410/1000
2023-09-29 16:26:25.319 
Epoch 410/1000 
	 loss: 29.7019, MinusLogProbMetric: 29.7019, val_loss: 30.1192, val_MinusLogProbMetric: 30.1192

Epoch 410: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.7019 - MinusLogProbMetric: 29.7019 - val_loss: 30.1192 - val_MinusLogProbMetric: 30.1192 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 411/1000
2023-09-29 16:27:00.682 
Epoch 411/1000 
	 loss: 29.6689, MinusLogProbMetric: 29.6689, val_loss: 30.0347, val_MinusLogProbMetric: 30.0347

Epoch 411: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6689 - MinusLogProbMetric: 29.6689 - val_loss: 30.0347 - val_MinusLogProbMetric: 30.0347 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 412/1000
2023-09-29 16:27:36.006 
Epoch 412/1000 
	 loss: 29.6344, MinusLogProbMetric: 29.6344, val_loss: 29.9843, val_MinusLogProbMetric: 29.9843

Epoch 412: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6344 - MinusLogProbMetric: 29.6344 - val_loss: 29.9843 - val_MinusLogProbMetric: 29.9843 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 413/1000
2023-09-29 16:28:11.204 
Epoch 413/1000 
	 loss: 29.7249, MinusLogProbMetric: 29.7249, val_loss: 29.9833, val_MinusLogProbMetric: 29.9833

Epoch 413: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.7249 - MinusLogProbMetric: 29.7249 - val_loss: 29.9833 - val_MinusLogProbMetric: 29.9833 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 414/1000
2023-09-29 16:28:46.361 
Epoch 414/1000 
	 loss: 29.6166, MinusLogProbMetric: 29.6166, val_loss: 30.1069, val_MinusLogProbMetric: 30.1069

Epoch 414: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6166 - MinusLogProbMetric: 29.6166 - val_loss: 30.1069 - val_MinusLogProbMetric: 30.1069 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 415/1000
2023-09-29 16:29:21.740 
Epoch 415/1000 
	 loss: 29.5660, MinusLogProbMetric: 29.5660, val_loss: 30.0424, val_MinusLogProbMetric: 30.0424

Epoch 415: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.5660 - MinusLogProbMetric: 29.5660 - val_loss: 30.0424 - val_MinusLogProbMetric: 30.0424 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 416/1000
2023-09-29 16:29:57.012 
Epoch 416/1000 
	 loss: 29.6194, MinusLogProbMetric: 29.6194, val_loss: 30.1647, val_MinusLogProbMetric: 30.1647

Epoch 416: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6194 - MinusLogProbMetric: 29.6194 - val_loss: 30.1647 - val_MinusLogProbMetric: 30.1647 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 417/1000
2023-09-29 16:30:32.307 
Epoch 417/1000 
	 loss: 29.5976, MinusLogProbMetric: 29.5976, val_loss: 30.2029, val_MinusLogProbMetric: 30.2029

Epoch 417: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.5976 - MinusLogProbMetric: 29.5976 - val_loss: 30.2029 - val_MinusLogProbMetric: 30.2029 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 418/1000
2023-09-29 16:31:07.576 
Epoch 418/1000 
	 loss: 29.6633, MinusLogProbMetric: 29.6633, val_loss: 30.0826, val_MinusLogProbMetric: 30.0826

Epoch 418: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6633 - MinusLogProbMetric: 29.6633 - val_loss: 30.0826 - val_MinusLogProbMetric: 30.0826 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 419/1000
2023-09-29 16:31:42.881 
Epoch 419/1000 
	 loss: 29.6478, MinusLogProbMetric: 29.6478, val_loss: 30.1397, val_MinusLogProbMetric: 30.1397

Epoch 419: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6478 - MinusLogProbMetric: 29.6478 - val_loss: 30.1397 - val_MinusLogProbMetric: 30.1397 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 420/1000
2023-09-29 16:32:18.300 
Epoch 420/1000 
	 loss: 29.5712, MinusLogProbMetric: 29.5712, val_loss: 29.9801, val_MinusLogProbMetric: 29.9801

Epoch 420: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.5712 - MinusLogProbMetric: 29.5712 - val_loss: 29.9801 - val_MinusLogProbMetric: 29.9801 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 421/1000
2023-09-29 16:32:53.435 
Epoch 421/1000 
	 loss: 29.5778, MinusLogProbMetric: 29.5778, val_loss: 30.5089, val_MinusLogProbMetric: 30.5089

Epoch 421: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.5778 - MinusLogProbMetric: 29.5778 - val_loss: 30.5089 - val_MinusLogProbMetric: 30.5089 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 422/1000
2023-09-29 16:33:28.609 
Epoch 422/1000 
	 loss: 29.5621, MinusLogProbMetric: 29.5621, val_loss: 30.2055, val_MinusLogProbMetric: 30.2055

Epoch 422: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.5621 - MinusLogProbMetric: 29.5621 - val_loss: 30.2055 - val_MinusLogProbMetric: 30.2055 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 423/1000
2023-09-29 16:34:03.701 
Epoch 423/1000 
	 loss: 29.5881, MinusLogProbMetric: 29.5881, val_loss: 30.6694, val_MinusLogProbMetric: 30.6694

Epoch 423: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.5881 - MinusLogProbMetric: 29.5881 - val_loss: 30.6694 - val_MinusLogProbMetric: 30.6694 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 424/1000
2023-09-29 16:34:38.803 
Epoch 424/1000 
	 loss: 29.6083, MinusLogProbMetric: 29.6083, val_loss: 30.1861, val_MinusLogProbMetric: 30.1861

Epoch 424: val_loss did not improve from 29.93092
196/196 - 35s - loss: 29.6083 - MinusLogProbMetric: 29.6083 - val_loss: 30.1861 - val_MinusLogProbMetric: 30.1861 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 425/1000
2023-09-29 16:35:13.633 
Epoch 425/1000 
	 loss: 29.5688, MinusLogProbMetric: 29.5688, val_loss: 29.8470, val_MinusLogProbMetric: 29.8470

Epoch 425: val_loss improved from 29.93092 to 29.84699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 29.5688 - MinusLogProbMetric: 29.5688 - val_loss: 29.8470 - val_MinusLogProbMetric: 29.8470 - lr: 1.6667e-04 - 36s/epoch - 181ms/step
Epoch 426/1000
2023-09-29 16:35:49.550 
Epoch 426/1000 
	 loss: 29.6237, MinusLogProbMetric: 29.6237, val_loss: 30.1854, val_MinusLogProbMetric: 30.1854

Epoch 426: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.6237 - MinusLogProbMetric: 29.6237 - val_loss: 30.1854 - val_MinusLogProbMetric: 30.1854 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 427/1000
2023-09-29 16:36:24.868 
Epoch 427/1000 
	 loss: 29.6030, MinusLogProbMetric: 29.6030, val_loss: 29.9858, val_MinusLogProbMetric: 29.9858

Epoch 427: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.6030 - MinusLogProbMetric: 29.6030 - val_loss: 29.9858 - val_MinusLogProbMetric: 29.9858 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 428/1000
2023-09-29 16:36:59.725 
Epoch 428/1000 
	 loss: 29.5319, MinusLogProbMetric: 29.5319, val_loss: 29.8739, val_MinusLogProbMetric: 29.8739

Epoch 428: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5319 - MinusLogProbMetric: 29.5319 - val_loss: 29.8739 - val_MinusLogProbMetric: 29.8739 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 429/1000
2023-09-29 16:37:34.556 
Epoch 429/1000 
	 loss: 29.5473, MinusLogProbMetric: 29.5473, val_loss: 30.1658, val_MinusLogProbMetric: 30.1658

Epoch 429: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5473 - MinusLogProbMetric: 29.5473 - val_loss: 30.1658 - val_MinusLogProbMetric: 30.1658 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 430/1000
2023-09-29 16:38:09.652 
Epoch 430/1000 
	 loss: 29.5464, MinusLogProbMetric: 29.5464, val_loss: 30.0821, val_MinusLogProbMetric: 30.0821

Epoch 430: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5464 - MinusLogProbMetric: 29.5464 - val_loss: 30.0821 - val_MinusLogProbMetric: 30.0821 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 431/1000
2023-09-29 16:38:44.752 
Epoch 431/1000 
	 loss: 29.5619, MinusLogProbMetric: 29.5619, val_loss: 30.0763, val_MinusLogProbMetric: 30.0763

Epoch 431: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5619 - MinusLogProbMetric: 29.5619 - val_loss: 30.0763 - val_MinusLogProbMetric: 30.0763 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 432/1000
2023-09-29 16:39:18.558 
Epoch 432/1000 
	 loss: 29.6591, MinusLogProbMetric: 29.6591, val_loss: 30.1753, val_MinusLogProbMetric: 30.1753

Epoch 432: val_loss did not improve from 29.84699
196/196 - 34s - loss: 29.6591 - MinusLogProbMetric: 29.6591 - val_loss: 30.1753 - val_MinusLogProbMetric: 30.1753 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 433/1000
2023-09-29 16:39:53.536 
Epoch 433/1000 
	 loss: 29.5260, MinusLogProbMetric: 29.5260, val_loss: 30.0430, val_MinusLogProbMetric: 30.0430

Epoch 433: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5260 - MinusLogProbMetric: 29.5260 - val_loss: 30.0430 - val_MinusLogProbMetric: 30.0430 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 434/1000
2023-09-29 16:40:27.322 
Epoch 434/1000 
	 loss: 29.5910, MinusLogProbMetric: 29.5910, val_loss: 30.1603, val_MinusLogProbMetric: 30.1603

Epoch 434: val_loss did not improve from 29.84699
196/196 - 34s - loss: 29.5910 - MinusLogProbMetric: 29.5910 - val_loss: 30.1603 - val_MinusLogProbMetric: 30.1603 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 435/1000
2023-09-29 16:41:01.863 
Epoch 435/1000 
	 loss: 29.5067, MinusLogProbMetric: 29.5067, val_loss: 29.8846, val_MinusLogProbMetric: 29.8846

Epoch 435: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5067 - MinusLogProbMetric: 29.5067 - val_loss: 29.8846 - val_MinusLogProbMetric: 29.8846 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 436/1000
2023-09-29 16:41:36.302 
Epoch 436/1000 
	 loss: 29.6224, MinusLogProbMetric: 29.6224, val_loss: 30.1667, val_MinusLogProbMetric: 30.1667

Epoch 436: val_loss did not improve from 29.84699
196/196 - 34s - loss: 29.6224 - MinusLogProbMetric: 29.6224 - val_loss: 30.1667 - val_MinusLogProbMetric: 30.1667 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 437/1000
2023-09-29 16:42:11.140 
Epoch 437/1000 
	 loss: 29.5872, MinusLogProbMetric: 29.5872, val_loss: 30.0060, val_MinusLogProbMetric: 30.0060

Epoch 437: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5872 - MinusLogProbMetric: 29.5872 - val_loss: 30.0060 - val_MinusLogProbMetric: 30.0060 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 438/1000
2023-09-29 16:42:45.181 
Epoch 438/1000 
	 loss: 29.4921, MinusLogProbMetric: 29.4921, val_loss: 29.9947, val_MinusLogProbMetric: 29.9947

Epoch 438: val_loss did not improve from 29.84699
196/196 - 34s - loss: 29.4921 - MinusLogProbMetric: 29.4921 - val_loss: 29.9947 - val_MinusLogProbMetric: 29.9947 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 439/1000
2023-09-29 16:43:20.171 
Epoch 439/1000 
	 loss: 29.5581, MinusLogProbMetric: 29.5581, val_loss: 29.9620, val_MinusLogProbMetric: 29.9620

Epoch 439: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5581 - MinusLogProbMetric: 29.5581 - val_loss: 29.9620 - val_MinusLogProbMetric: 29.9620 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 440/1000
2023-09-29 16:43:54.270 
Epoch 440/1000 
	 loss: 29.5192, MinusLogProbMetric: 29.5192, val_loss: 29.9329, val_MinusLogProbMetric: 29.9329

Epoch 440: val_loss did not improve from 29.84699
196/196 - 34s - loss: 29.5192 - MinusLogProbMetric: 29.5192 - val_loss: 29.9329 - val_MinusLogProbMetric: 29.9329 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 441/1000
2023-09-29 16:44:28.098 
Epoch 441/1000 
	 loss: 29.4771, MinusLogProbMetric: 29.4771, val_loss: 30.0466, val_MinusLogProbMetric: 30.0466

Epoch 441: val_loss did not improve from 29.84699
196/196 - 34s - loss: 29.4771 - MinusLogProbMetric: 29.4771 - val_loss: 30.0466 - val_MinusLogProbMetric: 30.0466 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 442/1000
2023-09-29 16:45:02.974 
Epoch 442/1000 
	 loss: 29.6077, MinusLogProbMetric: 29.6077, val_loss: 29.9611, val_MinusLogProbMetric: 29.9611

Epoch 442: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.6077 - MinusLogProbMetric: 29.6077 - val_loss: 29.9611 - val_MinusLogProbMetric: 29.9611 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 443/1000
2023-09-29 16:45:37.969 
Epoch 443/1000 
	 loss: 29.4915, MinusLogProbMetric: 29.4915, val_loss: 30.0305, val_MinusLogProbMetric: 30.0305

Epoch 443: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.4915 - MinusLogProbMetric: 29.4915 - val_loss: 30.0305 - val_MinusLogProbMetric: 30.0305 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 444/1000
2023-09-29 16:46:11.187 
Epoch 444/1000 
	 loss: 29.5343, MinusLogProbMetric: 29.5343, val_loss: 30.0490, val_MinusLogProbMetric: 30.0490

Epoch 444: val_loss did not improve from 29.84699
196/196 - 33s - loss: 29.5343 - MinusLogProbMetric: 29.5343 - val_loss: 30.0490 - val_MinusLogProbMetric: 30.0490 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 445/1000
2023-09-29 16:46:46.098 
Epoch 445/1000 
	 loss: 29.4991, MinusLogProbMetric: 29.4991, val_loss: 30.1199, val_MinusLogProbMetric: 30.1199

Epoch 445: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.4991 - MinusLogProbMetric: 29.4991 - val_loss: 30.1199 - val_MinusLogProbMetric: 30.1199 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 446/1000
2023-09-29 16:47:20.157 
Epoch 446/1000 
	 loss: 29.5663, MinusLogProbMetric: 29.5663, val_loss: 29.8588, val_MinusLogProbMetric: 29.8588

Epoch 446: val_loss did not improve from 29.84699
196/196 - 34s - loss: 29.5663 - MinusLogProbMetric: 29.5663 - val_loss: 29.8588 - val_MinusLogProbMetric: 29.8588 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 447/1000
2023-09-29 16:47:55.074 
Epoch 447/1000 
	 loss: 29.6085, MinusLogProbMetric: 29.6085, val_loss: 30.3918, val_MinusLogProbMetric: 30.3918

Epoch 447: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.6085 - MinusLogProbMetric: 29.6085 - val_loss: 30.3918 - val_MinusLogProbMetric: 30.3918 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 448/1000
2023-09-29 16:48:30.103 
Epoch 448/1000 
	 loss: 29.5345, MinusLogProbMetric: 29.5345, val_loss: 29.9261, val_MinusLogProbMetric: 29.9261

Epoch 448: val_loss did not improve from 29.84699
196/196 - 35s - loss: 29.5345 - MinusLogProbMetric: 29.5345 - val_loss: 29.9261 - val_MinusLogProbMetric: 29.9261 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 449/1000
2023-09-29 16:49:02.992 
Epoch 449/1000 
	 loss: 29.5704, MinusLogProbMetric: 29.5704, val_loss: 30.0011, val_MinusLogProbMetric: 30.0011

Epoch 449: val_loss did not improve from 29.84699
196/196 - 33s - loss: 29.5704 - MinusLogProbMetric: 29.5704 - val_loss: 30.0011 - val_MinusLogProbMetric: 30.0011 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 450/1000
2023-09-29 16:49:35.972 
Epoch 450/1000 
	 loss: 29.5459, MinusLogProbMetric: 29.5459, val_loss: 30.7446, val_MinusLogProbMetric: 30.7446

Epoch 450: val_loss did not improve from 29.84699
196/196 - 33s - loss: 29.5459 - MinusLogProbMetric: 29.5459 - val_loss: 30.7446 - val_MinusLogProbMetric: 30.7446 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 451/1000
2023-09-29 16:50:11.076 
Epoch 451/1000 
	 loss: 29.5341, MinusLogProbMetric: 29.5341, val_loss: 29.7983, val_MinusLogProbMetric: 29.7983

Epoch 451: val_loss improved from 29.84699 to 29.79830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 29.5341 - MinusLogProbMetric: 29.5341 - val_loss: 29.7983 - val_MinusLogProbMetric: 29.7983 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 452/1000
2023-09-29 16:50:46.969 
Epoch 452/1000 
	 loss: 29.5037, MinusLogProbMetric: 29.5037, val_loss: 30.0524, val_MinusLogProbMetric: 30.0524

Epoch 452: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.5037 - MinusLogProbMetric: 29.5037 - val_loss: 30.0524 - val_MinusLogProbMetric: 30.0524 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 453/1000
2023-09-29 16:51:21.948 
Epoch 453/1000 
	 loss: 29.4741, MinusLogProbMetric: 29.4741, val_loss: 30.2048, val_MinusLogProbMetric: 30.2048

Epoch 453: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4741 - MinusLogProbMetric: 29.4741 - val_loss: 30.2048 - val_MinusLogProbMetric: 30.2048 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 454/1000
2023-09-29 16:51:56.804 
Epoch 454/1000 
	 loss: 29.5292, MinusLogProbMetric: 29.5292, val_loss: 29.9018, val_MinusLogProbMetric: 29.9018

Epoch 454: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.5292 - MinusLogProbMetric: 29.5292 - val_loss: 29.9018 - val_MinusLogProbMetric: 29.9018 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 455/1000
2023-09-29 16:52:31.542 
Epoch 455/1000 
	 loss: 29.4692, MinusLogProbMetric: 29.4692, val_loss: 30.0119, val_MinusLogProbMetric: 30.0119

Epoch 455: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4692 - MinusLogProbMetric: 29.4692 - val_loss: 30.0119 - val_MinusLogProbMetric: 30.0119 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 456/1000
2023-09-29 16:53:04.990 
Epoch 456/1000 
	 loss: 29.4973, MinusLogProbMetric: 29.4973, val_loss: 30.3317, val_MinusLogProbMetric: 30.3317

Epoch 456: val_loss did not improve from 29.79830
196/196 - 33s - loss: 29.4973 - MinusLogProbMetric: 29.4973 - val_loss: 30.3317 - val_MinusLogProbMetric: 30.3317 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 457/1000
2023-09-29 16:53:37.124 
Epoch 457/1000 
	 loss: 29.5405, MinusLogProbMetric: 29.5405, val_loss: 29.8945, val_MinusLogProbMetric: 29.8945

Epoch 457: val_loss did not improve from 29.79830
196/196 - 32s - loss: 29.5405 - MinusLogProbMetric: 29.5405 - val_loss: 29.8945 - val_MinusLogProbMetric: 29.8945 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 458/1000
2023-09-29 16:54:10.583 
Epoch 458/1000 
	 loss: 29.4830, MinusLogProbMetric: 29.4830, val_loss: 29.9753, val_MinusLogProbMetric: 29.9753

Epoch 458: val_loss did not improve from 29.79830
196/196 - 33s - loss: 29.4830 - MinusLogProbMetric: 29.4830 - val_loss: 29.9753 - val_MinusLogProbMetric: 29.9753 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 459/1000
2023-09-29 16:54:45.413 
Epoch 459/1000 
	 loss: 29.4827, MinusLogProbMetric: 29.4827, val_loss: 30.0347, val_MinusLogProbMetric: 30.0347

Epoch 459: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4827 - MinusLogProbMetric: 29.4827 - val_loss: 30.0347 - val_MinusLogProbMetric: 30.0347 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 460/1000
2023-09-29 16:55:19.346 
Epoch 460/1000 
	 loss: 29.5030, MinusLogProbMetric: 29.5030, val_loss: 29.9960, val_MinusLogProbMetric: 29.9960

Epoch 460: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.5030 - MinusLogProbMetric: 29.5030 - val_loss: 29.9960 - val_MinusLogProbMetric: 29.9960 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 461/1000
2023-09-29 16:55:53.856 
Epoch 461/1000 
	 loss: 29.5256, MinusLogProbMetric: 29.5256, val_loss: 30.0165, val_MinusLogProbMetric: 30.0165

Epoch 461: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.5256 - MinusLogProbMetric: 29.5256 - val_loss: 30.0165 - val_MinusLogProbMetric: 30.0165 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 462/1000
2023-09-29 16:56:28.230 
Epoch 462/1000 
	 loss: 29.5593, MinusLogProbMetric: 29.5593, val_loss: 29.8954, val_MinusLogProbMetric: 29.8954

Epoch 462: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.5593 - MinusLogProbMetric: 29.5593 - val_loss: 29.8954 - val_MinusLogProbMetric: 29.8954 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 463/1000
2023-09-29 16:57:02.219 
Epoch 463/1000 
	 loss: 29.4513, MinusLogProbMetric: 29.4513, val_loss: 30.1538, val_MinusLogProbMetric: 30.1538

Epoch 463: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4513 - MinusLogProbMetric: 29.4513 - val_loss: 30.1538 - val_MinusLogProbMetric: 30.1538 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 464/1000
2023-09-29 16:57:35.969 
Epoch 464/1000 
	 loss: 29.4716, MinusLogProbMetric: 29.4716, val_loss: 30.0427, val_MinusLogProbMetric: 30.0427

Epoch 464: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4716 - MinusLogProbMetric: 29.4716 - val_loss: 30.0427 - val_MinusLogProbMetric: 30.0427 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 465/1000
2023-09-29 16:58:10.759 
Epoch 465/1000 
	 loss: 29.4695, MinusLogProbMetric: 29.4695, val_loss: 29.9924, val_MinusLogProbMetric: 29.9924

Epoch 465: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4695 - MinusLogProbMetric: 29.4695 - val_loss: 29.9924 - val_MinusLogProbMetric: 29.9924 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 466/1000
2023-09-29 16:58:45.454 
Epoch 466/1000 
	 loss: 29.5734, MinusLogProbMetric: 29.5734, val_loss: 30.2757, val_MinusLogProbMetric: 30.2757

Epoch 466: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.5734 - MinusLogProbMetric: 29.5734 - val_loss: 30.2757 - val_MinusLogProbMetric: 30.2757 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 467/1000
2023-09-29 16:59:19.537 
Epoch 467/1000 
	 loss: 29.5211, MinusLogProbMetric: 29.5211, val_loss: 29.8015, val_MinusLogProbMetric: 29.8015

Epoch 467: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.5211 - MinusLogProbMetric: 29.5211 - val_loss: 29.8015 - val_MinusLogProbMetric: 29.8015 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 468/1000
2023-09-29 16:59:54.385 
Epoch 468/1000 
	 loss: 29.4278, MinusLogProbMetric: 29.4278, val_loss: 30.0677, val_MinusLogProbMetric: 30.0677

Epoch 468: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4278 - MinusLogProbMetric: 29.4278 - val_loss: 30.0677 - val_MinusLogProbMetric: 30.0677 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 469/1000
2023-09-29 17:00:27.917 
Epoch 469/1000 
	 loss: 29.4937, MinusLogProbMetric: 29.4937, val_loss: 29.9400, val_MinusLogProbMetric: 29.9400

Epoch 469: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4937 - MinusLogProbMetric: 29.4937 - val_loss: 29.9400 - val_MinusLogProbMetric: 29.9400 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 470/1000
2023-09-29 17:01:01.445 
Epoch 470/1000 
	 loss: 29.4716, MinusLogProbMetric: 29.4716, val_loss: 29.8772, val_MinusLogProbMetric: 29.8772

Epoch 470: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4716 - MinusLogProbMetric: 29.4716 - val_loss: 29.8772 - val_MinusLogProbMetric: 29.8772 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 471/1000
2023-09-29 17:01:35.019 
Epoch 471/1000 
	 loss: 29.4832, MinusLogProbMetric: 29.4832, val_loss: 29.9052, val_MinusLogProbMetric: 29.9052

Epoch 471: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4832 - MinusLogProbMetric: 29.4832 - val_loss: 29.9052 - val_MinusLogProbMetric: 29.9052 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 472/1000
2023-09-29 17:02:07.523 
Epoch 472/1000 
	 loss: 29.5392, MinusLogProbMetric: 29.5392, val_loss: 29.9987, val_MinusLogProbMetric: 29.9987

Epoch 472: val_loss did not improve from 29.79830
196/196 - 33s - loss: 29.5392 - MinusLogProbMetric: 29.5392 - val_loss: 29.9987 - val_MinusLogProbMetric: 29.9987 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 473/1000
2023-09-29 17:02:42.096 
Epoch 473/1000 
	 loss: 29.5340, MinusLogProbMetric: 29.5340, val_loss: 29.8989, val_MinusLogProbMetric: 29.8989

Epoch 473: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.5340 - MinusLogProbMetric: 29.5340 - val_loss: 29.8989 - val_MinusLogProbMetric: 29.8989 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 474/1000
2023-09-29 17:03:16.667 
Epoch 474/1000 
	 loss: 29.3981, MinusLogProbMetric: 29.3981, val_loss: 30.3868, val_MinusLogProbMetric: 30.3868

Epoch 474: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.3981 - MinusLogProbMetric: 29.3981 - val_loss: 30.3868 - val_MinusLogProbMetric: 30.3868 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 475/1000
2023-09-29 17:03:51.702 
Epoch 475/1000 
	 loss: 29.5265, MinusLogProbMetric: 29.5265, val_loss: 30.3673, val_MinusLogProbMetric: 30.3673

Epoch 475: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.5265 - MinusLogProbMetric: 29.5265 - val_loss: 30.3673 - val_MinusLogProbMetric: 30.3673 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 476/1000
2023-09-29 17:04:26.706 
Epoch 476/1000 
	 loss: 29.4372, MinusLogProbMetric: 29.4372, val_loss: 30.0966, val_MinusLogProbMetric: 30.0966

Epoch 476: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4372 - MinusLogProbMetric: 29.4372 - val_loss: 30.0966 - val_MinusLogProbMetric: 30.0966 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 477/1000
2023-09-29 17:05:00.357 
Epoch 477/1000 
	 loss: 29.4618, MinusLogProbMetric: 29.4618, val_loss: 29.9449, val_MinusLogProbMetric: 29.9449

Epoch 477: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4618 - MinusLogProbMetric: 29.4618 - val_loss: 29.9449 - val_MinusLogProbMetric: 29.9449 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 478/1000
2023-09-29 17:05:35.677 
Epoch 478/1000 
	 loss: 29.4878, MinusLogProbMetric: 29.4878, val_loss: 31.1184, val_MinusLogProbMetric: 31.1184

Epoch 478: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4878 - MinusLogProbMetric: 29.4878 - val_loss: 31.1184 - val_MinusLogProbMetric: 31.1184 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 479/1000
2023-09-29 17:06:10.467 
Epoch 479/1000 
	 loss: 29.5670, MinusLogProbMetric: 29.5670, val_loss: 30.0988, val_MinusLogProbMetric: 30.0988

Epoch 479: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.5670 - MinusLogProbMetric: 29.5670 - val_loss: 30.0988 - val_MinusLogProbMetric: 30.0988 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 480/1000
2023-09-29 17:06:45.109 
Epoch 480/1000 
	 loss: 29.4199, MinusLogProbMetric: 29.4199, val_loss: 29.9559, val_MinusLogProbMetric: 29.9559

Epoch 480: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4199 - MinusLogProbMetric: 29.4199 - val_loss: 29.9559 - val_MinusLogProbMetric: 29.9559 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 481/1000
2023-09-29 17:07:19.927 
Epoch 481/1000 
	 loss: 29.4569, MinusLogProbMetric: 29.4569, val_loss: 30.0039, val_MinusLogProbMetric: 30.0039

Epoch 481: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4569 - MinusLogProbMetric: 29.4569 - val_loss: 30.0039 - val_MinusLogProbMetric: 30.0039 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 482/1000
2023-09-29 17:07:54.018 
Epoch 482/1000 
	 loss: 29.5807, MinusLogProbMetric: 29.5807, val_loss: 29.8488, val_MinusLogProbMetric: 29.8488

Epoch 482: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.5807 - MinusLogProbMetric: 29.5807 - val_loss: 29.8488 - val_MinusLogProbMetric: 29.8488 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 483/1000
2023-09-29 17:08:29.233 
Epoch 483/1000 
	 loss: 29.4527, MinusLogProbMetric: 29.4527, val_loss: 29.8938, val_MinusLogProbMetric: 29.8938

Epoch 483: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4527 - MinusLogProbMetric: 29.4527 - val_loss: 29.8938 - val_MinusLogProbMetric: 29.8938 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 484/1000
2023-09-29 17:09:03.184 
Epoch 484/1000 
	 loss: 29.4777, MinusLogProbMetric: 29.4777, val_loss: 29.8545, val_MinusLogProbMetric: 29.8545

Epoch 484: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4777 - MinusLogProbMetric: 29.4777 - val_loss: 29.8545 - val_MinusLogProbMetric: 29.8545 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 485/1000
2023-09-29 17:09:37.814 
Epoch 485/1000 
	 loss: 29.4022, MinusLogProbMetric: 29.4022, val_loss: 30.0245, val_MinusLogProbMetric: 30.0245

Epoch 485: val_loss did not improve from 29.79830
196/196 - 35s - loss: 29.4022 - MinusLogProbMetric: 29.4022 - val_loss: 30.0245 - val_MinusLogProbMetric: 30.0245 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 486/1000
2023-09-29 17:10:12.249 
Epoch 486/1000 
	 loss: 29.4108, MinusLogProbMetric: 29.4108, val_loss: 29.9070, val_MinusLogProbMetric: 29.9070

Epoch 486: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4108 - MinusLogProbMetric: 29.4108 - val_loss: 29.9070 - val_MinusLogProbMetric: 29.9070 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 487/1000
2023-09-29 17:10:45.972 
Epoch 487/1000 
	 loss: 29.3989, MinusLogProbMetric: 29.3989, val_loss: 29.9182, val_MinusLogProbMetric: 29.9182

Epoch 487: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.3989 - MinusLogProbMetric: 29.3989 - val_loss: 29.9182 - val_MinusLogProbMetric: 29.9182 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 488/1000
2023-09-29 17:11:20.099 
Epoch 488/1000 
	 loss: 29.4331, MinusLogProbMetric: 29.4331, val_loss: 29.9098, val_MinusLogProbMetric: 29.9098

Epoch 488: val_loss did not improve from 29.79830
196/196 - 34s - loss: 29.4331 - MinusLogProbMetric: 29.4331 - val_loss: 29.9098 - val_MinusLogProbMetric: 29.9098 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 489/1000
2023-09-29 17:11:49.187 
Epoch 489/1000 
	 loss: 29.4506, MinusLogProbMetric: 29.4506, val_loss: 30.3451, val_MinusLogProbMetric: 30.3451

Epoch 489: val_loss did not improve from 29.79830
196/196 - 29s - loss: 29.4506 - MinusLogProbMetric: 29.4506 - val_loss: 30.3451 - val_MinusLogProbMetric: 30.3451 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 490/1000
2023-09-29 17:12:22.259 
Epoch 490/1000 
	 loss: 29.5150, MinusLogProbMetric: 29.5150, val_loss: 29.8428, val_MinusLogProbMetric: 29.8428

Epoch 490: val_loss did not improve from 29.79830
196/196 - 33s - loss: 29.5150 - MinusLogProbMetric: 29.5150 - val_loss: 29.8428 - val_MinusLogProbMetric: 29.8428 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 491/1000
2023-09-29 17:12:52.629 
Epoch 491/1000 
	 loss: 29.3783, MinusLogProbMetric: 29.3783, val_loss: 30.1774, val_MinusLogProbMetric: 30.1774

Epoch 491: val_loss did not improve from 29.79830
196/196 - 30s - loss: 29.3783 - MinusLogProbMetric: 29.3783 - val_loss: 30.1774 - val_MinusLogProbMetric: 30.1774 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 492/1000
2023-09-29 17:13:21.240 
Epoch 492/1000 
	 loss: 29.3519, MinusLogProbMetric: 29.3519, val_loss: 29.7467, val_MinusLogProbMetric: 29.7467

Epoch 492: val_loss improved from 29.79830 to 29.74667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 29s - loss: 29.3519 - MinusLogProbMetric: 29.3519 - val_loss: 29.7467 - val_MinusLogProbMetric: 29.7467 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 493/1000
2023-09-29 17:13:50.647 
Epoch 493/1000 
	 loss: 29.4417, MinusLogProbMetric: 29.4417, val_loss: 29.9768, val_MinusLogProbMetric: 29.9768

Epoch 493: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.4417 - MinusLogProbMetric: 29.4417 - val_loss: 29.9768 - val_MinusLogProbMetric: 29.9768 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 494/1000
2023-09-29 17:14:20.488 
Epoch 494/1000 
	 loss: 29.4307, MinusLogProbMetric: 29.4307, val_loss: 29.9366, val_MinusLogProbMetric: 29.9366

Epoch 494: val_loss did not improve from 29.74667
196/196 - 30s - loss: 29.4307 - MinusLogProbMetric: 29.4307 - val_loss: 29.9366 - val_MinusLogProbMetric: 29.9366 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 495/1000
2023-09-29 17:14:54.038 
Epoch 495/1000 
	 loss: 29.4088, MinusLogProbMetric: 29.4088, val_loss: 29.7863, val_MinusLogProbMetric: 29.7863

Epoch 495: val_loss did not improve from 29.74667
196/196 - 34s - loss: 29.4088 - MinusLogProbMetric: 29.4088 - val_loss: 29.7863 - val_MinusLogProbMetric: 29.7863 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 496/1000
2023-09-29 17:15:22.579 
Epoch 496/1000 
	 loss: 29.3560, MinusLogProbMetric: 29.3560, val_loss: 29.9262, val_MinusLogProbMetric: 29.9262

Epoch 496: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.3560 - MinusLogProbMetric: 29.3560 - val_loss: 29.9262 - val_MinusLogProbMetric: 29.9262 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 497/1000
2023-09-29 17:15:51.877 
Epoch 497/1000 
	 loss: 29.3850, MinusLogProbMetric: 29.3850, val_loss: 29.7926, val_MinusLogProbMetric: 29.7926

Epoch 497: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.3850 - MinusLogProbMetric: 29.3850 - val_loss: 29.7926 - val_MinusLogProbMetric: 29.7926 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 498/1000
2023-09-29 17:16:21.596 
Epoch 498/1000 
	 loss: 29.4027, MinusLogProbMetric: 29.4027, val_loss: 30.1058, val_MinusLogProbMetric: 30.1058

Epoch 498: val_loss did not improve from 29.74667
196/196 - 30s - loss: 29.4027 - MinusLogProbMetric: 29.4027 - val_loss: 30.1058 - val_MinusLogProbMetric: 30.1058 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 499/1000
2023-09-29 17:16:50.883 
Epoch 499/1000 
	 loss: 29.3847, MinusLogProbMetric: 29.3847, val_loss: 29.8381, val_MinusLogProbMetric: 29.8381

Epoch 499: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.3847 - MinusLogProbMetric: 29.3847 - val_loss: 29.8381 - val_MinusLogProbMetric: 29.8381 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 500/1000
2023-09-29 17:17:23.885 
Epoch 500/1000 
	 loss: 29.3675, MinusLogProbMetric: 29.3675, val_loss: 29.9966, val_MinusLogProbMetric: 29.9966

Epoch 500: val_loss did not improve from 29.74667
196/196 - 33s - loss: 29.3675 - MinusLogProbMetric: 29.3675 - val_loss: 29.9966 - val_MinusLogProbMetric: 29.9966 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 501/1000
2023-09-29 17:17:55.278 
Epoch 501/1000 
	 loss: 29.3897, MinusLogProbMetric: 29.3897, val_loss: 29.8783, val_MinusLogProbMetric: 29.8783

Epoch 501: val_loss did not improve from 29.74667
196/196 - 31s - loss: 29.3897 - MinusLogProbMetric: 29.3897 - val_loss: 29.8783 - val_MinusLogProbMetric: 29.8783 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 502/1000
2023-09-29 17:18:24.717 
Epoch 502/1000 
	 loss: 29.4256, MinusLogProbMetric: 29.4256, val_loss: 30.1475, val_MinusLogProbMetric: 30.1475

Epoch 502: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.4256 - MinusLogProbMetric: 29.4256 - val_loss: 30.1475 - val_MinusLogProbMetric: 30.1475 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 503/1000
2023-09-29 17:18:53.344 
Epoch 503/1000 
	 loss: 29.4575, MinusLogProbMetric: 29.4575, val_loss: 31.8433, val_MinusLogProbMetric: 31.8433

Epoch 503: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.4575 - MinusLogProbMetric: 29.4575 - val_loss: 31.8433 - val_MinusLogProbMetric: 31.8433 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 504/1000
2023-09-29 17:19:22.255 
Epoch 504/1000 
	 loss: 29.4061, MinusLogProbMetric: 29.4061, val_loss: 29.9238, val_MinusLogProbMetric: 29.9238

Epoch 504: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.4061 - MinusLogProbMetric: 29.4061 - val_loss: 29.9238 - val_MinusLogProbMetric: 29.9238 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 505/1000
2023-09-29 17:19:53.441 
Epoch 505/1000 
	 loss: 29.4026, MinusLogProbMetric: 29.4026, val_loss: 30.1645, val_MinusLogProbMetric: 30.1645

Epoch 505: val_loss did not improve from 29.74667
196/196 - 31s - loss: 29.4026 - MinusLogProbMetric: 29.4026 - val_loss: 30.1645 - val_MinusLogProbMetric: 30.1645 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 506/1000
2023-09-29 17:20:24.777 
Epoch 506/1000 
	 loss: 29.3683, MinusLogProbMetric: 29.3683, val_loss: 30.0534, val_MinusLogProbMetric: 30.0534

Epoch 506: val_loss did not improve from 29.74667
196/196 - 31s - loss: 29.3683 - MinusLogProbMetric: 29.3683 - val_loss: 30.0534 - val_MinusLogProbMetric: 30.0534 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 507/1000
2023-09-29 17:20:55.709 
Epoch 507/1000 
	 loss: 29.4184, MinusLogProbMetric: 29.4184, val_loss: 30.1265, val_MinusLogProbMetric: 30.1265

Epoch 507: val_loss did not improve from 29.74667
196/196 - 31s - loss: 29.4184 - MinusLogProbMetric: 29.4184 - val_loss: 30.1265 - val_MinusLogProbMetric: 30.1265 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 508/1000
2023-09-29 17:21:25.023 
Epoch 508/1000 
	 loss: 29.3496, MinusLogProbMetric: 29.3496, val_loss: 29.8207, val_MinusLogProbMetric: 29.8207

Epoch 508: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.3496 - MinusLogProbMetric: 29.3496 - val_loss: 29.8207 - val_MinusLogProbMetric: 29.8207 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 509/1000
2023-09-29 17:21:53.890 
Epoch 509/1000 
	 loss: 29.3907, MinusLogProbMetric: 29.3907, val_loss: 29.8977, val_MinusLogProbMetric: 29.8977

Epoch 509: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.3907 - MinusLogProbMetric: 29.3907 - val_loss: 29.8977 - val_MinusLogProbMetric: 29.8977 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 510/1000
2023-09-29 17:22:23.064 
Epoch 510/1000 
	 loss: 29.3615, MinusLogProbMetric: 29.3615, val_loss: 30.0172, val_MinusLogProbMetric: 30.0172

Epoch 510: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.3615 - MinusLogProbMetric: 29.3615 - val_loss: 30.0172 - val_MinusLogProbMetric: 30.0172 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 511/1000
2023-09-29 17:22:52.273 
Epoch 511/1000 
	 loss: 29.4477, MinusLogProbMetric: 29.4477, val_loss: 29.9964, val_MinusLogProbMetric: 29.9964

Epoch 511: val_loss did not improve from 29.74667
196/196 - 29s - loss: 29.4477 - MinusLogProbMetric: 29.4477 - val_loss: 29.9964 - val_MinusLogProbMetric: 29.9964 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 512/1000
2023-09-29 17:23:23.953 
Epoch 512/1000 
	 loss: 29.3983, MinusLogProbMetric: 29.3983, val_loss: 30.7848, val_MinusLogProbMetric: 30.7848

Epoch 512: val_loss did not improve from 29.74667
196/196 - 32s - loss: 29.3983 - MinusLogProbMetric: 29.3983 - val_loss: 30.7848 - val_MinusLogProbMetric: 30.7848 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 513/1000
2023-09-29 17:23:57.906 
Epoch 513/1000 
	 loss: 29.4019, MinusLogProbMetric: 29.4019, val_loss: 29.8671, val_MinusLogProbMetric: 29.8671

Epoch 513: val_loss did not improve from 29.74667
196/196 - 34s - loss: 29.4019 - MinusLogProbMetric: 29.4019 - val_loss: 29.8671 - val_MinusLogProbMetric: 29.8671 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 514/1000
2023-09-29 17:24:30.505 
Epoch 514/1000 
	 loss: 29.3829, MinusLogProbMetric: 29.3829, val_loss: 29.8099, val_MinusLogProbMetric: 29.8099

Epoch 514: val_loss did not improve from 29.74667
196/196 - 33s - loss: 29.3829 - MinusLogProbMetric: 29.3829 - val_loss: 29.8099 - val_MinusLogProbMetric: 29.8099 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 515/1000
2023-09-29 17:25:03.490 
Epoch 515/1000 
	 loss: 29.3332, MinusLogProbMetric: 29.3332, val_loss: 29.6964, val_MinusLogProbMetric: 29.6964

Epoch 515: val_loss improved from 29.74667 to 29.69643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 33s - loss: 29.3332 - MinusLogProbMetric: 29.3332 - val_loss: 29.6964 - val_MinusLogProbMetric: 29.6964 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 516/1000
2023-09-29 17:25:38.072 
Epoch 516/1000 
	 loss: 29.3995, MinusLogProbMetric: 29.3995, val_loss: 30.2966, val_MinusLogProbMetric: 30.2966

Epoch 516: val_loss did not improve from 29.69643
196/196 - 34s - loss: 29.3995 - MinusLogProbMetric: 29.3995 - val_loss: 30.2966 - val_MinusLogProbMetric: 30.2966 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 517/1000
2023-09-29 17:26:11.562 
Epoch 517/1000 
	 loss: 29.3919, MinusLogProbMetric: 29.3919, val_loss: 29.9588, val_MinusLogProbMetric: 29.9588

Epoch 517: val_loss did not improve from 29.69643
196/196 - 33s - loss: 29.3919 - MinusLogProbMetric: 29.3919 - val_loss: 29.9588 - val_MinusLogProbMetric: 29.9588 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 518/1000
2023-09-29 17:26:42.498 
Epoch 518/1000 
	 loss: 29.4714, MinusLogProbMetric: 29.4714, val_loss: 29.7766, val_MinusLogProbMetric: 29.7766

Epoch 518: val_loss did not improve from 29.69643
196/196 - 31s - loss: 29.4714 - MinusLogProbMetric: 29.4714 - val_loss: 29.7766 - val_MinusLogProbMetric: 29.7766 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 519/1000
2023-09-29 17:27:14.203 
Epoch 519/1000 
	 loss: 29.3694, MinusLogProbMetric: 29.3694, val_loss: 29.7204, val_MinusLogProbMetric: 29.7204

Epoch 519: val_loss did not improve from 29.69643
196/196 - 32s - loss: 29.3694 - MinusLogProbMetric: 29.3694 - val_loss: 29.7204 - val_MinusLogProbMetric: 29.7204 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 520/1000
2023-09-29 17:27:43.630 
Epoch 520/1000 
	 loss: 29.3666, MinusLogProbMetric: 29.3666, val_loss: 30.0073, val_MinusLogProbMetric: 30.0073

Epoch 520: val_loss did not improve from 29.69643
196/196 - 29s - loss: 29.3666 - MinusLogProbMetric: 29.3666 - val_loss: 30.0073 - val_MinusLogProbMetric: 30.0073 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 521/1000
2023-09-29 17:28:13.709 
Epoch 521/1000 
	 loss: 29.3057, MinusLogProbMetric: 29.3057, val_loss: 29.8199, val_MinusLogProbMetric: 29.8199

Epoch 521: val_loss did not improve from 29.69643
196/196 - 30s - loss: 29.3057 - MinusLogProbMetric: 29.3057 - val_loss: 29.8199 - val_MinusLogProbMetric: 29.8199 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 522/1000
2023-09-29 17:28:44.426 
Epoch 522/1000 
	 loss: 29.3797, MinusLogProbMetric: 29.3797, val_loss: 29.9013, val_MinusLogProbMetric: 29.9013

Epoch 522: val_loss did not improve from 29.69643
196/196 - 31s - loss: 29.3797 - MinusLogProbMetric: 29.3797 - val_loss: 29.9013 - val_MinusLogProbMetric: 29.9013 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 523/1000
2023-09-29 17:29:13.824 
Epoch 523/1000 
	 loss: 29.3563, MinusLogProbMetric: 29.3563, val_loss: 30.2259, val_MinusLogProbMetric: 30.2259

Epoch 523: val_loss did not improve from 29.69643
196/196 - 29s - loss: 29.3563 - MinusLogProbMetric: 29.3563 - val_loss: 30.2259 - val_MinusLogProbMetric: 30.2259 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 524/1000
2023-09-29 17:29:47.739 
Epoch 524/1000 
	 loss: 29.3796, MinusLogProbMetric: 29.3796, val_loss: 30.2374, val_MinusLogProbMetric: 30.2374

Epoch 524: val_loss did not improve from 29.69643
196/196 - 34s - loss: 29.3796 - MinusLogProbMetric: 29.3796 - val_loss: 30.2374 - val_MinusLogProbMetric: 30.2374 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 525/1000
2023-09-29 17:30:21.682 
Epoch 525/1000 
	 loss: 29.3378, MinusLogProbMetric: 29.3378, val_loss: 29.9250, val_MinusLogProbMetric: 29.9250

Epoch 525: val_loss did not improve from 29.69643
196/196 - 34s - loss: 29.3378 - MinusLogProbMetric: 29.3378 - val_loss: 29.9250 - val_MinusLogProbMetric: 29.9250 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 526/1000
2023-09-29 17:30:55.608 
Epoch 526/1000 
	 loss: 29.3199, MinusLogProbMetric: 29.3199, val_loss: 29.7679, val_MinusLogProbMetric: 29.7679

Epoch 526: val_loss did not improve from 29.69643
196/196 - 34s - loss: 29.3199 - MinusLogProbMetric: 29.3199 - val_loss: 29.7679 - val_MinusLogProbMetric: 29.7679 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 527/1000
2023-09-29 17:31:27.237 
Epoch 527/1000 
	 loss: 29.4058, MinusLogProbMetric: 29.4058, val_loss: 29.8153, val_MinusLogProbMetric: 29.8153

Epoch 527: val_loss did not improve from 29.69643
196/196 - 32s - loss: 29.4058 - MinusLogProbMetric: 29.4058 - val_loss: 29.8153 - val_MinusLogProbMetric: 29.8153 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 528/1000
2023-09-29 17:32:00.026 
Epoch 528/1000 
	 loss: 29.3497, MinusLogProbMetric: 29.3497, val_loss: 29.9075, val_MinusLogProbMetric: 29.9075

Epoch 528: val_loss did not improve from 29.69643
196/196 - 33s - loss: 29.3497 - MinusLogProbMetric: 29.3497 - val_loss: 29.9075 - val_MinusLogProbMetric: 29.9075 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 529/1000
2023-09-29 17:32:32.526 
Epoch 529/1000 
	 loss: 29.3257, MinusLogProbMetric: 29.3257, val_loss: 29.9990, val_MinusLogProbMetric: 29.9990

Epoch 529: val_loss did not improve from 29.69643
196/196 - 32s - loss: 29.3257 - MinusLogProbMetric: 29.3257 - val_loss: 29.9990 - val_MinusLogProbMetric: 29.9990 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 530/1000
2023-09-29 17:33:02.786 
Epoch 530/1000 
	 loss: 29.3154, MinusLogProbMetric: 29.3154, val_loss: 29.7214, val_MinusLogProbMetric: 29.7214

Epoch 530: val_loss did not improve from 29.69643
196/196 - 30s - loss: 29.3154 - MinusLogProbMetric: 29.3154 - val_loss: 29.7214 - val_MinusLogProbMetric: 29.7214 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 531/1000
2023-09-29 17:33:35.742 
Epoch 531/1000 
	 loss: 29.3109, MinusLogProbMetric: 29.3109, val_loss: 29.7356, val_MinusLogProbMetric: 29.7356

Epoch 531: val_loss did not improve from 29.69643
196/196 - 33s - loss: 29.3109 - MinusLogProbMetric: 29.3109 - val_loss: 29.7356 - val_MinusLogProbMetric: 29.7356 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 532/1000
2023-09-29 17:34:06.149 
Epoch 532/1000 
	 loss: 29.3279, MinusLogProbMetric: 29.3279, val_loss: 29.7623, val_MinusLogProbMetric: 29.7623

Epoch 532: val_loss did not improve from 29.69643
196/196 - 30s - loss: 29.3279 - MinusLogProbMetric: 29.3279 - val_loss: 29.7623 - val_MinusLogProbMetric: 29.7623 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 533/1000
2023-09-29 17:34:36.248 
Epoch 533/1000 
	 loss: 29.2843, MinusLogProbMetric: 29.2843, val_loss: 29.6390, val_MinusLogProbMetric: 29.6390

Epoch 533: val_loss improved from 29.69643 to 29.63902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 31s - loss: 29.2843 - MinusLogProbMetric: 29.2843 - val_loss: 29.6390 - val_MinusLogProbMetric: 29.6390 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 534/1000
2023-09-29 17:35:08.838 
Epoch 534/1000 
	 loss: 29.3241, MinusLogProbMetric: 29.3241, val_loss: 30.0799, val_MinusLogProbMetric: 30.0799

Epoch 534: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.3241 - MinusLogProbMetric: 29.3241 - val_loss: 30.0799 - val_MinusLogProbMetric: 30.0799 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 535/1000
2023-09-29 17:35:41.330 
Epoch 535/1000 
	 loss: 29.3406, MinusLogProbMetric: 29.3406, val_loss: 29.9338, val_MinusLogProbMetric: 29.9338

Epoch 535: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.3406 - MinusLogProbMetric: 29.3406 - val_loss: 29.9338 - val_MinusLogProbMetric: 29.9338 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 536/1000
2023-09-29 17:36:11.220 
Epoch 536/1000 
	 loss: 29.4471, MinusLogProbMetric: 29.4471, val_loss: 29.7299, val_MinusLogProbMetric: 29.7299

Epoch 536: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.4471 - MinusLogProbMetric: 29.4471 - val_loss: 29.7299 - val_MinusLogProbMetric: 29.7299 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 537/1000
2023-09-29 17:36:42.611 
Epoch 537/1000 
	 loss: 29.3887, MinusLogProbMetric: 29.3887, val_loss: 29.8744, val_MinusLogProbMetric: 29.8744

Epoch 537: val_loss did not improve from 29.63902
196/196 - 31s - loss: 29.3887 - MinusLogProbMetric: 29.3887 - val_loss: 29.8744 - val_MinusLogProbMetric: 29.8744 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 538/1000
2023-09-29 17:37:15.525 
Epoch 538/1000 
	 loss: 29.2927, MinusLogProbMetric: 29.2927, val_loss: 29.6635, val_MinusLogProbMetric: 29.6635

Epoch 538: val_loss did not improve from 29.63902
196/196 - 33s - loss: 29.2927 - MinusLogProbMetric: 29.2927 - val_loss: 29.6635 - val_MinusLogProbMetric: 29.6635 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 539/1000
2023-09-29 17:37:48.714 
Epoch 539/1000 
	 loss: 29.3062, MinusLogProbMetric: 29.3062, val_loss: 30.0444, val_MinusLogProbMetric: 30.0444

Epoch 539: val_loss did not improve from 29.63902
196/196 - 33s - loss: 29.3062 - MinusLogProbMetric: 29.3062 - val_loss: 30.0444 - val_MinusLogProbMetric: 30.0444 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 540/1000
2023-09-29 17:38:21.181 
Epoch 540/1000 
	 loss: 29.2648, MinusLogProbMetric: 29.2648, val_loss: 30.3983, val_MinusLogProbMetric: 30.3983

Epoch 540: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.2648 - MinusLogProbMetric: 29.2648 - val_loss: 30.3983 - val_MinusLogProbMetric: 30.3983 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 541/1000
2023-09-29 17:38:51.108 
Epoch 541/1000 
	 loss: 29.3485, MinusLogProbMetric: 29.3485, val_loss: 29.8199, val_MinusLogProbMetric: 29.8199

Epoch 541: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.3485 - MinusLogProbMetric: 29.3485 - val_loss: 29.8199 - val_MinusLogProbMetric: 29.8199 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 542/1000
2023-09-29 17:39:22.774 
Epoch 542/1000 
	 loss: 29.3240, MinusLogProbMetric: 29.3240, val_loss: 29.7362, val_MinusLogProbMetric: 29.7362

Epoch 542: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.3240 - MinusLogProbMetric: 29.3240 - val_loss: 29.7362 - val_MinusLogProbMetric: 29.7362 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 543/1000
2023-09-29 17:39:55.142 
Epoch 543/1000 
	 loss: 29.2908, MinusLogProbMetric: 29.2908, val_loss: 29.8283, val_MinusLogProbMetric: 29.8283

Epoch 543: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.2908 - MinusLogProbMetric: 29.2908 - val_loss: 29.8283 - val_MinusLogProbMetric: 29.8283 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 544/1000
2023-09-29 17:40:26.986 
Epoch 544/1000 
	 loss: 29.2557, MinusLogProbMetric: 29.2557, val_loss: 29.8326, val_MinusLogProbMetric: 29.8326

Epoch 544: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.2557 - MinusLogProbMetric: 29.2557 - val_loss: 29.8326 - val_MinusLogProbMetric: 29.8326 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 545/1000
2023-09-29 17:40:58.830 
Epoch 545/1000 
	 loss: 29.2939, MinusLogProbMetric: 29.2939, val_loss: 29.6823, val_MinusLogProbMetric: 29.6823

Epoch 545: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.2939 - MinusLogProbMetric: 29.2939 - val_loss: 29.6823 - val_MinusLogProbMetric: 29.6823 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 546/1000
2023-09-29 17:41:29.232 
Epoch 546/1000 
	 loss: 29.2843, MinusLogProbMetric: 29.2843, val_loss: 29.6696, val_MinusLogProbMetric: 29.6696

Epoch 546: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.2843 - MinusLogProbMetric: 29.2843 - val_loss: 29.6696 - val_MinusLogProbMetric: 29.6696 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 547/1000
2023-09-29 17:41:57.856 
Epoch 547/1000 
	 loss: 29.3050, MinusLogProbMetric: 29.3050, val_loss: 29.6847, val_MinusLogProbMetric: 29.6847

Epoch 547: val_loss did not improve from 29.63902
196/196 - 29s - loss: 29.3050 - MinusLogProbMetric: 29.3050 - val_loss: 29.6847 - val_MinusLogProbMetric: 29.6847 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 548/1000
2023-09-29 17:42:27.501 
Epoch 548/1000 
	 loss: 29.2788, MinusLogProbMetric: 29.2788, val_loss: 29.9825, val_MinusLogProbMetric: 29.9825

Epoch 548: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.2788 - MinusLogProbMetric: 29.2788 - val_loss: 29.9825 - val_MinusLogProbMetric: 29.9825 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 549/1000
2023-09-29 17:42:55.641 
Epoch 549/1000 
	 loss: 29.3084, MinusLogProbMetric: 29.3084, val_loss: 29.7323, val_MinusLogProbMetric: 29.7323

Epoch 549: val_loss did not improve from 29.63902
196/196 - 28s - loss: 29.3084 - MinusLogProbMetric: 29.3084 - val_loss: 29.7323 - val_MinusLogProbMetric: 29.7323 - lr: 1.6667e-04 - 28s/epoch - 144ms/step
Epoch 550/1000
2023-09-29 17:43:24.464 
Epoch 550/1000 
	 loss: 29.3022, MinusLogProbMetric: 29.3022, val_loss: 30.0088, val_MinusLogProbMetric: 30.0088

Epoch 550: val_loss did not improve from 29.63902
196/196 - 29s - loss: 29.3022 - MinusLogProbMetric: 29.3022 - val_loss: 30.0088 - val_MinusLogProbMetric: 30.0088 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 551/1000
2023-09-29 17:43:55.176 
Epoch 551/1000 
	 loss: 29.3227, MinusLogProbMetric: 29.3227, val_loss: 29.7532, val_MinusLogProbMetric: 29.7532

Epoch 551: val_loss did not improve from 29.63902
196/196 - 31s - loss: 29.3227 - MinusLogProbMetric: 29.3227 - val_loss: 29.7532 - val_MinusLogProbMetric: 29.7532 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 552/1000
2023-09-29 17:44:22.594 
Epoch 552/1000 
	 loss: 29.3254, MinusLogProbMetric: 29.3254, val_loss: 29.8217, val_MinusLogProbMetric: 29.8217

Epoch 552: val_loss did not improve from 29.63902
196/196 - 27s - loss: 29.3254 - MinusLogProbMetric: 29.3254 - val_loss: 29.8217 - val_MinusLogProbMetric: 29.8217 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 553/1000
2023-09-29 17:44:50.651 
Epoch 553/1000 
	 loss: 29.2616, MinusLogProbMetric: 29.2616, val_loss: 29.7208, val_MinusLogProbMetric: 29.7208

Epoch 553: val_loss did not improve from 29.63902
196/196 - 28s - loss: 29.2616 - MinusLogProbMetric: 29.2616 - val_loss: 29.7208 - val_MinusLogProbMetric: 29.7208 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 554/1000
2023-09-29 17:45:20.100 
Epoch 554/1000 
	 loss: 29.2716, MinusLogProbMetric: 29.2716, val_loss: 29.7670, val_MinusLogProbMetric: 29.7670

Epoch 554: val_loss did not improve from 29.63902
196/196 - 29s - loss: 29.2716 - MinusLogProbMetric: 29.2716 - val_loss: 29.7670 - val_MinusLogProbMetric: 29.7670 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 555/1000
2023-09-29 17:45:50.209 
Epoch 555/1000 
	 loss: 29.3899, MinusLogProbMetric: 29.3899, val_loss: 30.1723, val_MinusLogProbMetric: 30.1723

Epoch 555: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.3899 - MinusLogProbMetric: 29.3899 - val_loss: 30.1723 - val_MinusLogProbMetric: 30.1723 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 556/1000
2023-09-29 17:46:20.251 
Epoch 556/1000 
	 loss: 29.3066, MinusLogProbMetric: 29.3066, val_loss: 29.6855, val_MinusLogProbMetric: 29.6855

Epoch 556: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.3066 - MinusLogProbMetric: 29.3066 - val_loss: 29.6855 - val_MinusLogProbMetric: 29.6855 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 557/1000
2023-09-29 17:46:52.237 
Epoch 557/1000 
	 loss: 29.2654, MinusLogProbMetric: 29.2654, val_loss: 29.7750, val_MinusLogProbMetric: 29.7750

Epoch 557: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.2654 - MinusLogProbMetric: 29.2654 - val_loss: 29.7750 - val_MinusLogProbMetric: 29.7750 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 558/1000
2023-09-29 17:47:23.083 
Epoch 558/1000 
	 loss: 29.3139, MinusLogProbMetric: 29.3139, val_loss: 29.7756, val_MinusLogProbMetric: 29.7756

Epoch 558: val_loss did not improve from 29.63902
196/196 - 31s - loss: 29.3139 - MinusLogProbMetric: 29.3139 - val_loss: 29.7756 - val_MinusLogProbMetric: 29.7756 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 559/1000
2023-09-29 17:47:54.138 
Epoch 559/1000 
	 loss: 29.2850, MinusLogProbMetric: 29.2850, val_loss: 30.0110, val_MinusLogProbMetric: 30.0110

Epoch 559: val_loss did not improve from 29.63902
196/196 - 31s - loss: 29.2850 - MinusLogProbMetric: 29.2850 - val_loss: 30.0110 - val_MinusLogProbMetric: 30.0110 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 560/1000
2023-09-29 17:48:23.657 
Epoch 560/1000 
	 loss: 29.2631, MinusLogProbMetric: 29.2631, val_loss: 30.1973, val_MinusLogProbMetric: 30.1973

Epoch 560: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.2631 - MinusLogProbMetric: 29.2631 - val_loss: 30.1973 - val_MinusLogProbMetric: 30.1973 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 561/1000
2023-09-29 17:48:55.318 
Epoch 561/1000 
	 loss: 29.3465, MinusLogProbMetric: 29.3465, val_loss: 29.8979, val_MinusLogProbMetric: 29.8979

Epoch 561: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.3465 - MinusLogProbMetric: 29.3465 - val_loss: 29.8979 - val_MinusLogProbMetric: 29.8979 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 562/1000
2023-09-29 17:49:24.189 
Epoch 562/1000 
	 loss: 29.2622, MinusLogProbMetric: 29.2622, val_loss: 29.7113, val_MinusLogProbMetric: 29.7113

Epoch 562: val_loss did not improve from 29.63902
196/196 - 29s - loss: 29.2622 - MinusLogProbMetric: 29.2622 - val_loss: 29.7113 - val_MinusLogProbMetric: 29.7113 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 563/1000
2023-09-29 17:49:54.327 
Epoch 563/1000 
	 loss: 29.3404, MinusLogProbMetric: 29.3404, val_loss: 29.8182, val_MinusLogProbMetric: 29.8182

Epoch 563: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.3404 - MinusLogProbMetric: 29.3404 - val_loss: 29.8182 - val_MinusLogProbMetric: 29.8182 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 564/1000
2023-09-29 17:50:21.948 
Epoch 564/1000 
	 loss: 29.2646, MinusLogProbMetric: 29.2646, val_loss: 30.2393, val_MinusLogProbMetric: 30.2393

Epoch 564: val_loss did not improve from 29.63902
196/196 - 28s - loss: 29.2646 - MinusLogProbMetric: 29.2646 - val_loss: 30.2393 - val_MinusLogProbMetric: 30.2393 - lr: 1.6667e-04 - 28s/epoch - 141ms/step
Epoch 565/1000
2023-09-29 17:50:51.289 
Epoch 565/1000 
	 loss: 29.2981, MinusLogProbMetric: 29.2981, val_loss: 30.1222, val_MinusLogProbMetric: 30.1222

Epoch 565: val_loss did not improve from 29.63902
196/196 - 29s - loss: 29.2981 - MinusLogProbMetric: 29.2981 - val_loss: 30.1222 - val_MinusLogProbMetric: 30.1222 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 566/1000
2023-09-29 17:51:20.595 
Epoch 566/1000 
	 loss: 29.2689, MinusLogProbMetric: 29.2689, val_loss: 29.7090, val_MinusLogProbMetric: 29.7090

Epoch 566: val_loss did not improve from 29.63902
196/196 - 29s - loss: 29.2689 - MinusLogProbMetric: 29.2689 - val_loss: 29.7090 - val_MinusLogProbMetric: 29.7090 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 567/1000
2023-09-29 17:51:50.791 
Epoch 567/1000 
	 loss: 29.3079, MinusLogProbMetric: 29.3079, val_loss: 29.7652, val_MinusLogProbMetric: 29.7652

Epoch 567: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.3079 - MinusLogProbMetric: 29.3079 - val_loss: 29.7652 - val_MinusLogProbMetric: 29.7652 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 568/1000
2023-09-29 17:52:21.379 
Epoch 568/1000 
	 loss: 29.3108, MinusLogProbMetric: 29.3108, val_loss: 30.2858, val_MinusLogProbMetric: 30.2858

Epoch 568: val_loss did not improve from 29.63902
196/196 - 31s - loss: 29.3108 - MinusLogProbMetric: 29.3108 - val_loss: 30.2858 - val_MinusLogProbMetric: 30.2858 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 569/1000
2023-09-29 17:52:53.488 
Epoch 569/1000 
	 loss: 29.3178, MinusLogProbMetric: 29.3178, val_loss: 29.7944, val_MinusLogProbMetric: 29.7944

Epoch 569: val_loss did not improve from 29.63902
196/196 - 32s - loss: 29.3178 - MinusLogProbMetric: 29.3178 - val_loss: 29.7944 - val_MinusLogProbMetric: 29.7944 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 570/1000
2023-09-29 17:53:23.761 
Epoch 570/1000 
	 loss: 29.2169, MinusLogProbMetric: 29.2169, val_loss: 29.7513, val_MinusLogProbMetric: 29.7513

Epoch 570: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.2169 - MinusLogProbMetric: 29.2169 - val_loss: 29.7513 - val_MinusLogProbMetric: 29.7513 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 571/1000
2023-09-29 17:53:54.376 
Epoch 571/1000 
	 loss: 29.2656, MinusLogProbMetric: 29.2656, val_loss: 29.6723, val_MinusLogProbMetric: 29.6723

Epoch 571: val_loss did not improve from 29.63902
196/196 - 31s - loss: 29.2656 - MinusLogProbMetric: 29.2656 - val_loss: 29.6723 - val_MinusLogProbMetric: 29.6723 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 572/1000
2023-09-29 17:54:25.653 
Epoch 572/1000 
	 loss: 29.2886, MinusLogProbMetric: 29.2886, val_loss: 30.1399, val_MinusLogProbMetric: 30.1399

Epoch 572: val_loss did not improve from 29.63902
196/196 - 31s - loss: 29.2886 - MinusLogProbMetric: 29.2886 - val_loss: 30.1399 - val_MinusLogProbMetric: 30.1399 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 573/1000
2023-09-29 17:54:55.657 
Epoch 573/1000 
	 loss: 29.2235, MinusLogProbMetric: 29.2235, val_loss: 29.7825, val_MinusLogProbMetric: 29.7825

Epoch 573: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.2235 - MinusLogProbMetric: 29.2235 - val_loss: 29.7825 - val_MinusLogProbMetric: 29.7825 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 574/1000
2023-09-29 17:55:25.321 
Epoch 574/1000 
	 loss: 29.2140, MinusLogProbMetric: 29.2140, val_loss: 30.0546, val_MinusLogProbMetric: 30.0546

Epoch 574: val_loss did not improve from 29.63902
196/196 - 30s - loss: 29.2140 - MinusLogProbMetric: 29.2140 - val_loss: 30.0546 - val_MinusLogProbMetric: 30.0546 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 575/1000
2023-09-29 17:55:52.911 
Epoch 575/1000 
	 loss: 29.2000, MinusLogProbMetric: 29.2000, val_loss: 29.9750, val_MinusLogProbMetric: 29.9750

Epoch 575: val_loss did not improve from 29.63902
196/196 - 28s - loss: 29.2000 - MinusLogProbMetric: 29.2000 - val_loss: 29.9750 - val_MinusLogProbMetric: 29.9750 - lr: 1.6667e-04 - 28s/epoch - 141ms/step
Epoch 576/1000
2023-09-29 17:56:20.008 
Epoch 576/1000 
	 loss: 29.2203, MinusLogProbMetric: 29.2203, val_loss: 29.9446, val_MinusLogProbMetric: 29.9446

Epoch 576: val_loss did not improve from 29.63902
196/196 - 27s - loss: 29.2203 - MinusLogProbMetric: 29.2203 - val_loss: 29.9446 - val_MinusLogProbMetric: 29.9446 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 577/1000
2023-09-29 17:56:47.253 
Epoch 577/1000 
	 loss: 29.3165, MinusLogProbMetric: 29.3165, val_loss: 29.6167, val_MinusLogProbMetric: 29.6167

Epoch 577: val_loss improved from 29.63902 to 29.61671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 28s - loss: 29.3165 - MinusLogProbMetric: 29.3165 - val_loss: 29.6167 - val_MinusLogProbMetric: 29.6167 - lr: 1.6667e-04 - 28s/epoch - 141ms/step
Epoch 578/1000
2023-09-29 17:57:14.466 
Epoch 578/1000 
	 loss: 29.2249, MinusLogProbMetric: 29.2249, val_loss: 29.7171, val_MinusLogProbMetric: 29.7171

Epoch 578: val_loss did not improve from 29.61671
196/196 - 27s - loss: 29.2249 - MinusLogProbMetric: 29.2249 - val_loss: 29.7171 - val_MinusLogProbMetric: 29.7171 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 579/1000
2023-09-29 17:57:41.315 
Epoch 579/1000 
	 loss: 29.2205, MinusLogProbMetric: 29.2205, val_loss: 29.6864, val_MinusLogProbMetric: 29.6864

Epoch 579: val_loss did not improve from 29.61671
196/196 - 27s - loss: 29.2205 - MinusLogProbMetric: 29.2205 - val_loss: 29.6864 - val_MinusLogProbMetric: 29.6864 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 580/1000
2023-09-29 17:58:08.333 
Epoch 580/1000 
	 loss: 29.2346, MinusLogProbMetric: 29.2346, val_loss: 29.7785, val_MinusLogProbMetric: 29.7785

Epoch 580: val_loss did not improve from 29.61671
196/196 - 27s - loss: 29.2346 - MinusLogProbMetric: 29.2346 - val_loss: 29.7785 - val_MinusLogProbMetric: 29.7785 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 581/1000
2023-09-29 17:58:35.748 
Epoch 581/1000 
	 loss: 29.2422, MinusLogProbMetric: 29.2422, val_loss: 29.6834, val_MinusLogProbMetric: 29.6834

Epoch 581: val_loss did not improve from 29.61671
196/196 - 27s - loss: 29.2422 - MinusLogProbMetric: 29.2422 - val_loss: 29.6834 - val_MinusLogProbMetric: 29.6834 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 582/1000
2023-09-29 17:59:03.258 
Epoch 582/1000 
	 loss: 29.3151, MinusLogProbMetric: 29.3151, val_loss: 29.5968, val_MinusLogProbMetric: 29.5968

Epoch 582: val_loss improved from 29.61671 to 29.59678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 28s - loss: 29.3151 - MinusLogProbMetric: 29.3151 - val_loss: 29.5968 - val_MinusLogProbMetric: 29.5968 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 583/1000
2023-09-29 17:59:30.776 
Epoch 583/1000 
	 loss: 29.2899, MinusLogProbMetric: 29.2899, val_loss: 29.7836, val_MinusLogProbMetric: 29.7836

Epoch 583: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2899 - MinusLogProbMetric: 29.2899 - val_loss: 29.7836 - val_MinusLogProbMetric: 29.7836 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 584/1000
2023-09-29 17:59:58.434 
Epoch 584/1000 
	 loss: 29.2533, MinusLogProbMetric: 29.2533, val_loss: 29.6186, val_MinusLogProbMetric: 29.6186

Epoch 584: val_loss did not improve from 29.59678
196/196 - 28s - loss: 29.2533 - MinusLogProbMetric: 29.2533 - val_loss: 29.6186 - val_MinusLogProbMetric: 29.6186 - lr: 1.6667e-04 - 28s/epoch - 141ms/step
Epoch 585/1000
2023-09-29 18:00:26.346 
Epoch 585/1000 
	 loss: 29.2175, MinusLogProbMetric: 29.2175, val_loss: 29.7123, val_MinusLogProbMetric: 29.7123

Epoch 585: val_loss did not improve from 29.59678
196/196 - 28s - loss: 29.2175 - MinusLogProbMetric: 29.2175 - val_loss: 29.7123 - val_MinusLogProbMetric: 29.7123 - lr: 1.6667e-04 - 28s/epoch - 142ms/step
Epoch 586/1000
2023-09-29 18:00:53.544 
Epoch 586/1000 
	 loss: 29.2968, MinusLogProbMetric: 29.2968, val_loss: 29.9389, val_MinusLogProbMetric: 29.9389

Epoch 586: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2968 - MinusLogProbMetric: 29.2968 - val_loss: 29.9389 - val_MinusLogProbMetric: 29.9389 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 587/1000
2023-09-29 18:01:20.578 
Epoch 587/1000 
	 loss: 29.2581, MinusLogProbMetric: 29.2581, val_loss: 30.1919, val_MinusLogProbMetric: 30.1919

Epoch 587: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2581 - MinusLogProbMetric: 29.2581 - val_loss: 30.1919 - val_MinusLogProbMetric: 30.1919 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 588/1000
2023-09-29 18:01:47.491 
Epoch 588/1000 
	 loss: 29.2446, MinusLogProbMetric: 29.2446, val_loss: 29.8026, val_MinusLogProbMetric: 29.8026

Epoch 588: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2446 - MinusLogProbMetric: 29.2446 - val_loss: 29.8026 - val_MinusLogProbMetric: 29.8026 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 589/1000
2023-09-29 18:02:14.596 
Epoch 589/1000 
	 loss: 29.2782, MinusLogProbMetric: 29.2782, val_loss: 29.7077, val_MinusLogProbMetric: 29.7077

Epoch 589: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2782 - MinusLogProbMetric: 29.2782 - val_loss: 29.7077 - val_MinusLogProbMetric: 29.7077 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 590/1000
2023-09-29 18:02:42.037 
Epoch 590/1000 
	 loss: 29.2321, MinusLogProbMetric: 29.2321, val_loss: 29.7524, val_MinusLogProbMetric: 29.7524

Epoch 590: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2321 - MinusLogProbMetric: 29.2321 - val_loss: 29.7524 - val_MinusLogProbMetric: 29.7524 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 591/1000
2023-09-29 18:03:10.491 
Epoch 591/1000 
	 loss: 29.2120, MinusLogProbMetric: 29.2120, val_loss: 29.6696, val_MinusLogProbMetric: 29.6696

Epoch 591: val_loss did not improve from 29.59678
196/196 - 28s - loss: 29.2120 - MinusLogProbMetric: 29.2120 - val_loss: 29.6696 - val_MinusLogProbMetric: 29.6696 - lr: 1.6667e-04 - 28s/epoch - 145ms/step
Epoch 592/1000
2023-09-29 18:03:38.933 
Epoch 592/1000 
	 loss: 29.2080, MinusLogProbMetric: 29.2080, val_loss: 29.6032, val_MinusLogProbMetric: 29.6032

Epoch 592: val_loss did not improve from 29.59678
196/196 - 28s - loss: 29.2080 - MinusLogProbMetric: 29.2080 - val_loss: 29.6032 - val_MinusLogProbMetric: 29.6032 - lr: 1.6667e-04 - 28s/epoch - 145ms/step
Epoch 593/1000
2023-09-29 18:04:06.180 
Epoch 593/1000 
	 loss: 29.2634, MinusLogProbMetric: 29.2634, val_loss: 29.8918, val_MinusLogProbMetric: 29.8918

Epoch 593: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2634 - MinusLogProbMetric: 29.2634 - val_loss: 29.8918 - val_MinusLogProbMetric: 29.8918 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 594/1000
2023-09-29 18:04:32.965 
Epoch 594/1000 
	 loss: 29.2854, MinusLogProbMetric: 29.2854, val_loss: 29.8300, val_MinusLogProbMetric: 29.8300

Epoch 594: val_loss did not improve from 29.59678
196/196 - 27s - loss: 29.2854 - MinusLogProbMetric: 29.2854 - val_loss: 29.8300 - val_MinusLogProbMetric: 29.8300 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 595/1000
2023-09-29 18:04:59.725 
Epoch 595/1000 
	 loss: 29.1812, MinusLogProbMetric: 29.1812, val_loss: 29.5765, val_MinusLogProbMetric: 29.5765

Epoch 595: val_loss improved from 29.59678 to 29.57654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 28s - loss: 29.1812 - MinusLogProbMetric: 29.1812 - val_loss: 29.5765 - val_MinusLogProbMetric: 29.5765 - lr: 1.6667e-04 - 28s/epoch - 140ms/step
Epoch 596/1000
2023-09-29 18:05:27.321 
Epoch 596/1000 
	 loss: 29.1987, MinusLogProbMetric: 29.1987, val_loss: 29.7522, val_MinusLogProbMetric: 29.7522

Epoch 596: val_loss did not improve from 29.57654
196/196 - 27s - loss: 29.1987 - MinusLogProbMetric: 29.1987 - val_loss: 29.7522 - val_MinusLogProbMetric: 29.7522 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 597/1000
2023-09-29 18:05:54.502 
Epoch 597/1000 
	 loss: 29.1801, MinusLogProbMetric: 29.1801, val_loss: 29.5504, val_MinusLogProbMetric: 29.5504

Epoch 597: val_loss improved from 29.57654 to 29.55043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 28s - loss: 29.1801 - MinusLogProbMetric: 29.1801 - val_loss: 29.5504 - val_MinusLogProbMetric: 29.5504 - lr: 1.6667e-04 - 28s/epoch - 141ms/step
Epoch 598/1000
2023-09-29 18:06:22.146 
Epoch 598/1000 
	 loss: 29.2388, MinusLogProbMetric: 29.2388, val_loss: 29.5826, val_MinusLogProbMetric: 29.5826

Epoch 598: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.2388 - MinusLogProbMetric: 29.2388 - val_loss: 29.5826 - val_MinusLogProbMetric: 29.5826 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 599/1000
2023-09-29 18:06:48.905 
Epoch 599/1000 
	 loss: 29.1970, MinusLogProbMetric: 29.1970, val_loss: 29.8841, val_MinusLogProbMetric: 29.8841

Epoch 599: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1970 - MinusLogProbMetric: 29.1970 - val_loss: 29.8841 - val_MinusLogProbMetric: 29.8841 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 600/1000
2023-09-29 18:07:16.253 
Epoch 600/1000 
	 loss: 29.1946, MinusLogProbMetric: 29.1946, val_loss: 29.6724, val_MinusLogProbMetric: 29.6724

Epoch 600: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1946 - MinusLogProbMetric: 29.1946 - val_loss: 29.6724 - val_MinusLogProbMetric: 29.6724 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 601/1000
2023-09-29 18:07:43.272 
Epoch 601/1000 
	 loss: 29.2271, MinusLogProbMetric: 29.2271, val_loss: 29.6695, val_MinusLogProbMetric: 29.6695

Epoch 601: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.2271 - MinusLogProbMetric: 29.2271 - val_loss: 29.6695 - val_MinusLogProbMetric: 29.6695 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 602/1000
2023-09-29 18:08:10.278 
Epoch 602/1000 
	 loss: 29.1839, MinusLogProbMetric: 29.1839, val_loss: 29.8301, val_MinusLogProbMetric: 29.8301

Epoch 602: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1839 - MinusLogProbMetric: 29.1839 - val_loss: 29.8301 - val_MinusLogProbMetric: 29.8301 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 603/1000
2023-09-29 18:08:37.420 
Epoch 603/1000 
	 loss: 29.4303, MinusLogProbMetric: 29.4303, val_loss: 29.6672, val_MinusLogProbMetric: 29.6672

Epoch 603: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.4303 - MinusLogProbMetric: 29.4303 - val_loss: 29.6672 - val_MinusLogProbMetric: 29.6672 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 604/1000
2023-09-29 18:09:04.270 
Epoch 604/1000 
	 loss: 29.1516, MinusLogProbMetric: 29.1516, val_loss: 29.6154, val_MinusLogProbMetric: 29.6154

Epoch 604: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1516 - MinusLogProbMetric: 29.1516 - val_loss: 29.6154 - val_MinusLogProbMetric: 29.6154 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 605/1000
2023-09-29 18:09:31.441 
Epoch 605/1000 
	 loss: 29.1862, MinusLogProbMetric: 29.1862, val_loss: 29.8804, val_MinusLogProbMetric: 29.8804

Epoch 605: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1862 - MinusLogProbMetric: 29.1862 - val_loss: 29.8804 - val_MinusLogProbMetric: 29.8804 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 606/1000
2023-09-29 18:09:58.053 
Epoch 606/1000 
	 loss: 29.3058, MinusLogProbMetric: 29.3058, val_loss: 29.6950, val_MinusLogProbMetric: 29.6950

Epoch 606: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.3058 - MinusLogProbMetric: 29.3058 - val_loss: 29.6950 - val_MinusLogProbMetric: 29.6950 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 607/1000
2023-09-29 18:10:25.412 
Epoch 607/1000 
	 loss: 29.1523, MinusLogProbMetric: 29.1523, val_loss: 29.7360, val_MinusLogProbMetric: 29.7360

Epoch 607: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1523 - MinusLogProbMetric: 29.1523 - val_loss: 29.7360 - val_MinusLogProbMetric: 29.7360 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 608/1000
2023-09-29 18:10:52.546 
Epoch 608/1000 
	 loss: 29.2243, MinusLogProbMetric: 29.2243, val_loss: 29.8098, val_MinusLogProbMetric: 29.8098

Epoch 608: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.2243 - MinusLogProbMetric: 29.2243 - val_loss: 29.8098 - val_MinusLogProbMetric: 29.8098 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 609/1000
2023-09-29 18:11:19.570 
Epoch 609/1000 
	 loss: 29.1616, MinusLogProbMetric: 29.1616, val_loss: 29.6061, val_MinusLogProbMetric: 29.6061

Epoch 609: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1616 - MinusLogProbMetric: 29.1616 - val_loss: 29.6061 - val_MinusLogProbMetric: 29.6061 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 610/1000
2023-09-29 18:11:46.859 
Epoch 610/1000 
	 loss: 29.2909, MinusLogProbMetric: 29.2909, val_loss: 29.5986, val_MinusLogProbMetric: 29.5986

Epoch 610: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.2909 - MinusLogProbMetric: 29.2909 - val_loss: 29.5986 - val_MinusLogProbMetric: 29.5986 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 611/1000
2023-09-29 18:12:14.560 
Epoch 611/1000 
	 loss: 29.2452, MinusLogProbMetric: 29.2452, val_loss: 29.9934, val_MinusLogProbMetric: 29.9934

Epoch 611: val_loss did not improve from 29.55043
196/196 - 28s - loss: 29.2452 - MinusLogProbMetric: 29.2452 - val_loss: 29.9934 - val_MinusLogProbMetric: 29.9934 - lr: 1.6667e-04 - 28s/epoch - 141ms/step
Epoch 612/1000
2023-09-29 18:12:41.826 
Epoch 612/1000 
	 loss: 29.1855, MinusLogProbMetric: 29.1855, val_loss: 29.7079, val_MinusLogProbMetric: 29.7079

Epoch 612: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1855 - MinusLogProbMetric: 29.1855 - val_loss: 29.7079 - val_MinusLogProbMetric: 29.7079 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 613/1000
2023-09-29 18:13:09.326 
Epoch 613/1000 
	 loss: 29.1254, MinusLogProbMetric: 29.1254, val_loss: 29.6922, val_MinusLogProbMetric: 29.6922

Epoch 613: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1254 - MinusLogProbMetric: 29.1254 - val_loss: 29.6922 - val_MinusLogProbMetric: 29.6922 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 614/1000
2023-09-29 18:13:36.491 
Epoch 614/1000 
	 loss: 29.1490, MinusLogProbMetric: 29.1490, val_loss: 29.7423, val_MinusLogProbMetric: 29.7423

Epoch 614: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1490 - MinusLogProbMetric: 29.1490 - val_loss: 29.7423 - val_MinusLogProbMetric: 29.7423 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 615/1000
2023-09-29 18:14:03.817 
Epoch 615/1000 
	 loss: 29.1486, MinusLogProbMetric: 29.1486, val_loss: 29.6422, val_MinusLogProbMetric: 29.6422

Epoch 615: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1486 - MinusLogProbMetric: 29.1486 - val_loss: 29.6422 - val_MinusLogProbMetric: 29.6422 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 616/1000
2023-09-29 18:14:30.867 
Epoch 616/1000 
	 loss: 29.2434, MinusLogProbMetric: 29.2434, val_loss: 29.7153, val_MinusLogProbMetric: 29.7153

Epoch 616: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.2434 - MinusLogProbMetric: 29.2434 - val_loss: 29.7153 - val_MinusLogProbMetric: 29.7153 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 617/1000
2023-09-29 18:14:57.840 
Epoch 617/1000 
	 loss: 29.1497, MinusLogProbMetric: 29.1497, val_loss: 29.5651, val_MinusLogProbMetric: 29.5651

Epoch 617: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1497 - MinusLogProbMetric: 29.1497 - val_loss: 29.5651 - val_MinusLogProbMetric: 29.5651 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 618/1000
2023-09-29 18:15:25.211 
Epoch 618/1000 
	 loss: 29.1377, MinusLogProbMetric: 29.1377, val_loss: 29.9640, val_MinusLogProbMetric: 29.9640

Epoch 618: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1377 - MinusLogProbMetric: 29.1377 - val_loss: 29.9640 - val_MinusLogProbMetric: 29.9640 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 619/1000
2023-09-29 18:15:51.785 
Epoch 619/1000 
	 loss: 29.2577, MinusLogProbMetric: 29.2577, val_loss: 29.8580, val_MinusLogProbMetric: 29.8580

Epoch 619: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.2577 - MinusLogProbMetric: 29.2577 - val_loss: 29.8580 - val_MinusLogProbMetric: 29.8580 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 620/1000
2023-09-29 18:16:19.727 
Epoch 620/1000 
	 loss: 29.2510, MinusLogProbMetric: 29.2510, val_loss: 29.5859, val_MinusLogProbMetric: 29.5859

Epoch 620: val_loss did not improve from 29.55043
196/196 - 28s - loss: 29.2510 - MinusLogProbMetric: 29.2510 - val_loss: 29.5859 - val_MinusLogProbMetric: 29.5859 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 621/1000
2023-09-29 18:16:46.544 
Epoch 621/1000 
	 loss: 29.1751, MinusLogProbMetric: 29.1751, val_loss: 29.6894, val_MinusLogProbMetric: 29.6894

Epoch 621: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1751 - MinusLogProbMetric: 29.1751 - val_loss: 29.6894 - val_MinusLogProbMetric: 29.6894 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 622/1000
2023-09-29 18:17:12.647 
Epoch 622/1000 
	 loss: 29.1856, MinusLogProbMetric: 29.1856, val_loss: 29.6637, val_MinusLogProbMetric: 29.6637

Epoch 622: val_loss did not improve from 29.55043
196/196 - 26s - loss: 29.1856 - MinusLogProbMetric: 29.1856 - val_loss: 29.6637 - val_MinusLogProbMetric: 29.6637 - lr: 1.6667e-04 - 26s/epoch - 133ms/step
Epoch 623/1000
2023-09-29 18:17:39.080 
Epoch 623/1000 
	 loss: 29.2174, MinusLogProbMetric: 29.2174, val_loss: 29.5733, val_MinusLogProbMetric: 29.5733

Epoch 623: val_loss did not improve from 29.55043
196/196 - 26s - loss: 29.2174 - MinusLogProbMetric: 29.2174 - val_loss: 29.5733 - val_MinusLogProbMetric: 29.5733 - lr: 1.6667e-04 - 26s/epoch - 135ms/step
Epoch 624/1000
2023-09-29 18:18:06.081 
Epoch 624/1000 
	 loss: 29.2275, MinusLogProbMetric: 29.2275, val_loss: 29.8870, val_MinusLogProbMetric: 29.8870

Epoch 624: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.2275 - MinusLogProbMetric: 29.2275 - val_loss: 29.8870 - val_MinusLogProbMetric: 29.8870 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 625/1000
2023-09-29 18:18:37.200 
Epoch 625/1000 
	 loss: 29.1375, MinusLogProbMetric: 29.1375, val_loss: 29.7488, val_MinusLogProbMetric: 29.7488

Epoch 625: val_loss did not improve from 29.55043
196/196 - 31s - loss: 29.1375 - MinusLogProbMetric: 29.1375 - val_loss: 29.7488 - val_MinusLogProbMetric: 29.7488 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 626/1000
2023-09-29 18:19:04.238 
Epoch 626/1000 
	 loss: 29.1462, MinusLogProbMetric: 29.1462, val_loss: 29.7554, val_MinusLogProbMetric: 29.7554

Epoch 626: val_loss did not improve from 29.55043
196/196 - 27s - loss: 29.1462 - MinusLogProbMetric: 29.1462 - val_loss: 29.7554 - val_MinusLogProbMetric: 29.7554 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 627/1000
2023-09-29 18:19:30.326 
Epoch 627/1000 
	 loss: 29.1991, MinusLogProbMetric: 29.1991, val_loss: 29.9572, val_MinusLogProbMetric: 29.9572

Epoch 627: val_loss did not improve from 29.55043
196/196 - 26s - loss: 29.1991 - MinusLogProbMetric: 29.1991 - val_loss: 29.9572 - val_MinusLogProbMetric: 29.9572 - lr: 1.6667e-04 - 26s/epoch - 133ms/step
Epoch 628/1000
2023-09-29 18:19:56.082 
Epoch 628/1000 
	 loss: 29.1515, MinusLogProbMetric: 29.1515, val_loss: 29.5113, val_MinusLogProbMetric: 29.5113

Epoch 628: val_loss improved from 29.55043 to 29.51129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 26s - loss: 29.1515 - MinusLogProbMetric: 29.1515 - val_loss: 29.5113 - val_MinusLogProbMetric: 29.5113 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 629/1000
2023-09-29 18:20:22.741 
Epoch 629/1000 
	 loss: 29.1623, MinusLogProbMetric: 29.1623, val_loss: 29.6787, val_MinusLogProbMetric: 29.6787

Epoch 629: val_loss did not improve from 29.51129
196/196 - 26s - loss: 29.1623 - MinusLogProbMetric: 29.1623 - val_loss: 29.6787 - val_MinusLogProbMetric: 29.6787 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 630/1000
2023-09-29 18:20:48.905 
Epoch 630/1000 
	 loss: 29.1578, MinusLogProbMetric: 29.1578, val_loss: 29.7551, val_MinusLogProbMetric: 29.7551

Epoch 630: val_loss did not improve from 29.51129
196/196 - 26s - loss: 29.1578 - MinusLogProbMetric: 29.1578 - val_loss: 29.7551 - val_MinusLogProbMetric: 29.7551 - lr: 1.6667e-04 - 26s/epoch - 133ms/step
Epoch 631/1000
2023-09-29 18:21:15.503 
Epoch 631/1000 
	 loss: 29.1148, MinusLogProbMetric: 29.1148, val_loss: 29.7897, val_MinusLogProbMetric: 29.7897

Epoch 631: val_loss did not improve from 29.51129
196/196 - 27s - loss: 29.1148 - MinusLogProbMetric: 29.1148 - val_loss: 29.7897 - val_MinusLogProbMetric: 29.7897 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 632/1000
2023-09-29 18:21:43.753 
Epoch 632/1000 
	 loss: 29.1604, MinusLogProbMetric: 29.1604, val_loss: 29.6307, val_MinusLogProbMetric: 29.6307

Epoch 632: val_loss did not improve from 29.51129
196/196 - 28s - loss: 29.1604 - MinusLogProbMetric: 29.1604 - val_loss: 29.6307 - val_MinusLogProbMetric: 29.6307 - lr: 1.6667e-04 - 28s/epoch - 144ms/step
Epoch 633/1000
2023-09-29 18:22:11.188 
Epoch 633/1000 
	 loss: 29.1499, MinusLogProbMetric: 29.1499, val_loss: 29.9802, val_MinusLogProbMetric: 29.9802

Epoch 633: val_loss did not improve from 29.51129
196/196 - 27s - loss: 29.1499 - MinusLogProbMetric: 29.1499 - val_loss: 29.9802 - val_MinusLogProbMetric: 29.9802 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 634/1000
2023-09-29 18:22:37.748 
Epoch 634/1000 
	 loss: 29.2140, MinusLogProbMetric: 29.2140, val_loss: 29.6184, val_MinusLogProbMetric: 29.6184

Epoch 634: val_loss did not improve from 29.51129
196/196 - 27s - loss: 29.2140 - MinusLogProbMetric: 29.2140 - val_loss: 29.6184 - val_MinusLogProbMetric: 29.6184 - lr: 1.6667e-04 - 27s/epoch - 135ms/step
Epoch 635/1000
2023-09-29 18:23:03.971 
Epoch 635/1000 
	 loss: 29.1421, MinusLogProbMetric: 29.1421, val_loss: 29.5212, val_MinusLogProbMetric: 29.5212

Epoch 635: val_loss did not improve from 29.51129
196/196 - 26s - loss: 29.1421 - MinusLogProbMetric: 29.1421 - val_loss: 29.5212 - val_MinusLogProbMetric: 29.5212 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 636/1000
2023-09-29 18:23:30.373 
Epoch 636/1000 
	 loss: 29.2505, MinusLogProbMetric: 29.2505, val_loss: 31.0139, val_MinusLogProbMetric: 31.0139

Epoch 636: val_loss did not improve from 29.51129
196/196 - 26s - loss: 29.2505 - MinusLogProbMetric: 29.2505 - val_loss: 31.0139 - val_MinusLogProbMetric: 31.0139 - lr: 1.6667e-04 - 26s/epoch - 135ms/step
Epoch 637/1000
2023-09-29 18:23:56.489 
Epoch 637/1000 
	 loss: 29.2167, MinusLogProbMetric: 29.2167, val_loss: 29.7085, val_MinusLogProbMetric: 29.7085

Epoch 637: val_loss did not improve from 29.51129
196/196 - 26s - loss: 29.2167 - MinusLogProbMetric: 29.2167 - val_loss: 29.7085 - val_MinusLogProbMetric: 29.7085 - lr: 1.6667e-04 - 26s/epoch - 133ms/step
Epoch 638/1000
2023-09-29 18:24:23.047 
Epoch 638/1000 
	 loss: 29.1366, MinusLogProbMetric: 29.1366, val_loss: 29.5368, val_MinusLogProbMetric: 29.5368

Epoch 638: val_loss did not improve from 29.51129
196/196 - 27s - loss: 29.1366 - MinusLogProbMetric: 29.1366 - val_loss: 29.5368 - val_MinusLogProbMetric: 29.5368 - lr: 1.6667e-04 - 27s/epoch - 135ms/step
Epoch 639/1000
2023-09-29 18:24:50.550 
Epoch 639/1000 
	 loss: 29.1895, MinusLogProbMetric: 29.1895, val_loss: 29.7678, val_MinusLogProbMetric: 29.7678

Epoch 639: val_loss did not improve from 29.51129
196/196 - 28s - loss: 29.1895 - MinusLogProbMetric: 29.1895 - val_loss: 29.7678 - val_MinusLogProbMetric: 29.7678 - lr: 1.6667e-04 - 28s/epoch - 140ms/step
Epoch 640/1000
2023-09-29 18:25:19.188 
Epoch 640/1000 
	 loss: 29.1490, MinusLogProbMetric: 29.1490, val_loss: 29.6379, val_MinusLogProbMetric: 29.6379

Epoch 640: val_loss did not improve from 29.51129
196/196 - 29s - loss: 29.1490 - MinusLogProbMetric: 29.1490 - val_loss: 29.6379 - val_MinusLogProbMetric: 29.6379 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 641/1000
2023-09-29 18:25:48.104 
Epoch 641/1000 
	 loss: 29.1004, MinusLogProbMetric: 29.1004, val_loss: 29.6842, val_MinusLogProbMetric: 29.6842

Epoch 641: val_loss did not improve from 29.51129
196/196 - 29s - loss: 29.1004 - MinusLogProbMetric: 29.1004 - val_loss: 29.6842 - val_MinusLogProbMetric: 29.6842 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 642/1000
2023-09-29 18:26:14.375 
Epoch 642/1000 
	 loss: 29.1430, MinusLogProbMetric: 29.1430, val_loss: 29.4879, val_MinusLogProbMetric: 29.4879

Epoch 642: val_loss improved from 29.51129 to 29.48789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 27s - loss: 29.1430 - MinusLogProbMetric: 29.1430 - val_loss: 29.4879 - val_MinusLogProbMetric: 29.4879 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 643/1000
2023-09-29 18:26:41.132 
Epoch 643/1000 
	 loss: 29.2227, MinusLogProbMetric: 29.2227, val_loss: 29.6325, val_MinusLogProbMetric: 29.6325

Epoch 643: val_loss did not improve from 29.48789
196/196 - 26s - loss: 29.2227 - MinusLogProbMetric: 29.2227 - val_loss: 29.6325 - val_MinusLogProbMetric: 29.6325 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 644/1000
2023-09-29 18:27:07.431 
Epoch 644/1000 
	 loss: 29.0914, MinusLogProbMetric: 29.0914, val_loss: 29.5480, val_MinusLogProbMetric: 29.5480

Epoch 644: val_loss did not improve from 29.48789
196/196 - 26s - loss: 29.0914 - MinusLogProbMetric: 29.0914 - val_loss: 29.5480 - val_MinusLogProbMetric: 29.5480 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 645/1000
2023-09-29 18:27:34.153 
Epoch 645/1000 
	 loss: 29.1992, MinusLogProbMetric: 29.1992, val_loss: 29.5282, val_MinusLogProbMetric: 29.5282

Epoch 645: val_loss did not improve from 29.48789
196/196 - 27s - loss: 29.1992 - MinusLogProbMetric: 29.1992 - val_loss: 29.5282 - val_MinusLogProbMetric: 29.5282 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 646/1000
2023-09-29 18:28:00.630 
Epoch 646/1000 
	 loss: 29.1251, MinusLogProbMetric: 29.1251, val_loss: 29.4653, val_MinusLogProbMetric: 29.4653

Epoch 646: val_loss improved from 29.48789 to 29.46535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 27s - loss: 29.1251 - MinusLogProbMetric: 29.1251 - val_loss: 29.4653 - val_MinusLogProbMetric: 29.4653 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 647/1000
2023-09-29 18:28:31.311 
Epoch 647/1000 
	 loss: 29.1421, MinusLogProbMetric: 29.1421, val_loss: 29.6547, val_MinusLogProbMetric: 29.6547

Epoch 647: val_loss did not improve from 29.46535
196/196 - 30s - loss: 29.1421 - MinusLogProbMetric: 29.1421 - val_loss: 29.6547 - val_MinusLogProbMetric: 29.6547 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 648/1000
2023-09-29 18:28:58.048 
Epoch 648/1000 
	 loss: 29.1184, MinusLogProbMetric: 29.1184, val_loss: 29.6466, val_MinusLogProbMetric: 29.6466

Epoch 648: val_loss did not improve from 29.46535
196/196 - 27s - loss: 29.1184 - MinusLogProbMetric: 29.1184 - val_loss: 29.6466 - val_MinusLogProbMetric: 29.6466 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 649/1000
2023-09-29 18:29:24.271 
Epoch 649/1000 
	 loss: 29.1364, MinusLogProbMetric: 29.1364, val_loss: 29.8496, val_MinusLogProbMetric: 29.8496

Epoch 649: val_loss did not improve from 29.46535
196/196 - 26s - loss: 29.1364 - MinusLogProbMetric: 29.1364 - val_loss: 29.8496 - val_MinusLogProbMetric: 29.8496 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 650/1000
2023-09-29 18:29:50.588 
Epoch 650/1000 
	 loss: 29.2121, MinusLogProbMetric: 29.2121, val_loss: 29.5070, val_MinusLogProbMetric: 29.5070

Epoch 650: val_loss did not improve from 29.46535
196/196 - 26s - loss: 29.2121 - MinusLogProbMetric: 29.2121 - val_loss: 29.5070 - val_MinusLogProbMetric: 29.5070 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 651/1000
2023-09-29 18:30:17.268 
Epoch 651/1000 
	 loss: 29.1717, MinusLogProbMetric: 29.1717, val_loss: 29.5974, val_MinusLogProbMetric: 29.5974

Epoch 651: val_loss did not improve from 29.46535
196/196 - 27s - loss: 29.1717 - MinusLogProbMetric: 29.1717 - val_loss: 29.5974 - val_MinusLogProbMetric: 29.5974 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 652/1000
2023-09-29 18:30:47.802 
Epoch 652/1000 
	 loss: 29.1376, MinusLogProbMetric: 29.1376, val_loss: 29.7534, val_MinusLogProbMetric: 29.7534

Epoch 652: val_loss did not improve from 29.46535
196/196 - 31s - loss: 29.1376 - MinusLogProbMetric: 29.1376 - val_loss: 29.7534 - val_MinusLogProbMetric: 29.7534 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 653/1000
2023-09-29 18:31:18.592 
Epoch 653/1000 
	 loss: 29.1301, MinusLogProbMetric: 29.1301, val_loss: 29.6652, val_MinusLogProbMetric: 29.6652

Epoch 653: val_loss did not improve from 29.46535
196/196 - 31s - loss: 29.1301 - MinusLogProbMetric: 29.1301 - val_loss: 29.6652 - val_MinusLogProbMetric: 29.6652 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 654/1000
2023-09-29 18:31:47.377 
Epoch 654/1000 
	 loss: 29.1067, MinusLogProbMetric: 29.1067, val_loss: 29.6240, val_MinusLogProbMetric: 29.6240

Epoch 654: val_loss did not improve from 29.46535
196/196 - 29s - loss: 29.1067 - MinusLogProbMetric: 29.1067 - val_loss: 29.6240 - val_MinusLogProbMetric: 29.6240 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 655/1000
2023-09-29 18:32:13.613 
Epoch 655/1000 
	 loss: 29.1383, MinusLogProbMetric: 29.1383, val_loss: 30.3135, val_MinusLogProbMetric: 30.3135

Epoch 655: val_loss did not improve from 29.46535
196/196 - 26s - loss: 29.1383 - MinusLogProbMetric: 29.1383 - val_loss: 30.3135 - val_MinusLogProbMetric: 30.3135 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 656/1000
2023-09-29 18:32:39.988 
Epoch 656/1000 
	 loss: 29.1298, MinusLogProbMetric: 29.1298, val_loss: 30.0336, val_MinusLogProbMetric: 30.0336

Epoch 656: val_loss did not improve from 29.46535
196/196 - 26s - loss: 29.1298 - MinusLogProbMetric: 29.1298 - val_loss: 30.0336 - val_MinusLogProbMetric: 30.0336 - lr: 1.6667e-04 - 26s/epoch - 135ms/step
Epoch 657/1000
2023-09-29 18:33:06.218 
Epoch 657/1000 
	 loss: 29.1667, MinusLogProbMetric: 29.1667, val_loss: 29.5943, val_MinusLogProbMetric: 29.5943

Epoch 657: val_loss did not improve from 29.46535
196/196 - 26s - loss: 29.1667 - MinusLogProbMetric: 29.1667 - val_loss: 29.5943 - val_MinusLogProbMetric: 29.5943 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 658/1000
2023-09-29 18:33:32.801 
Epoch 658/1000 
	 loss: 29.0607, MinusLogProbMetric: 29.0607, val_loss: 29.4889, val_MinusLogProbMetric: 29.4889

Epoch 658: val_loss did not improve from 29.46535
196/196 - 27s - loss: 29.0607 - MinusLogProbMetric: 29.0607 - val_loss: 29.4889 - val_MinusLogProbMetric: 29.4889 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 659/1000
2023-09-29 18:34:00.636 
Epoch 659/1000 
	 loss: 29.1152, MinusLogProbMetric: 29.1152, val_loss: 29.8461, val_MinusLogProbMetric: 29.8461

Epoch 659: val_loss did not improve from 29.46535
196/196 - 28s - loss: 29.1152 - MinusLogProbMetric: 29.1152 - val_loss: 29.8461 - val_MinusLogProbMetric: 29.8461 - lr: 1.6667e-04 - 28s/epoch - 142ms/step
Epoch 660/1000
2023-09-29 18:34:30.668 
Epoch 660/1000 
	 loss: 29.1393, MinusLogProbMetric: 29.1393, val_loss: 29.4161, val_MinusLogProbMetric: 29.4161

Epoch 660: val_loss improved from 29.46535 to 29.41611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 31s - loss: 29.1393 - MinusLogProbMetric: 29.1393 - val_loss: 29.4161 - val_MinusLogProbMetric: 29.4161 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 661/1000
2023-09-29 18:34:59.976 
Epoch 661/1000 
	 loss: 29.1079, MinusLogProbMetric: 29.1079, val_loss: 29.5377, val_MinusLogProbMetric: 29.5377

Epoch 661: val_loss did not improve from 29.41611
196/196 - 29s - loss: 29.1079 - MinusLogProbMetric: 29.1079 - val_loss: 29.5377 - val_MinusLogProbMetric: 29.5377 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 662/1000
2023-09-29 18:35:26.028 
Epoch 662/1000 
	 loss: 29.0595, MinusLogProbMetric: 29.0595, val_loss: 29.7155, val_MinusLogProbMetric: 29.7155

Epoch 662: val_loss did not improve from 29.41611
196/196 - 26s - loss: 29.0595 - MinusLogProbMetric: 29.0595 - val_loss: 29.7155 - val_MinusLogProbMetric: 29.7155 - lr: 1.6667e-04 - 26s/epoch - 133ms/step
Epoch 663/1000
2023-09-29 18:35:52.201 
Epoch 663/1000 
	 loss: 29.1310, MinusLogProbMetric: 29.1310, val_loss: 29.5755, val_MinusLogProbMetric: 29.5755

Epoch 663: val_loss did not improve from 29.41611
196/196 - 26s - loss: 29.1310 - MinusLogProbMetric: 29.1310 - val_loss: 29.5755 - val_MinusLogProbMetric: 29.5755 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 664/1000
2023-09-29 18:36:18.576 
Epoch 664/1000 
	 loss: 29.1441, MinusLogProbMetric: 29.1441, val_loss: 29.4942, val_MinusLogProbMetric: 29.4942

Epoch 664: val_loss did not improve from 29.41611
196/196 - 26s - loss: 29.1441 - MinusLogProbMetric: 29.1441 - val_loss: 29.4942 - val_MinusLogProbMetric: 29.4942 - lr: 1.6667e-04 - 26s/epoch - 135ms/step
Epoch 665/1000
2023-09-29 18:36:46.105 
Epoch 665/1000 
	 loss: 29.0863, MinusLogProbMetric: 29.0863, val_loss: 29.7174, val_MinusLogProbMetric: 29.7174

Epoch 665: val_loss did not improve from 29.41611
196/196 - 28s - loss: 29.0863 - MinusLogProbMetric: 29.0863 - val_loss: 29.7174 - val_MinusLogProbMetric: 29.7174 - lr: 1.6667e-04 - 28s/epoch - 140ms/step
Epoch 666/1000
2023-09-29 18:37:14.472 
Epoch 666/1000 
	 loss: 29.0505, MinusLogProbMetric: 29.0505, val_loss: 29.4887, val_MinusLogProbMetric: 29.4887

Epoch 666: val_loss did not improve from 29.41611
196/196 - 28s - loss: 29.0505 - MinusLogProbMetric: 29.0505 - val_loss: 29.4887 - val_MinusLogProbMetric: 29.4887 - lr: 1.6667e-04 - 28s/epoch - 145ms/step
Epoch 667/1000
2023-09-29 18:37:43.845 
Epoch 667/1000 
	 loss: 29.0557, MinusLogProbMetric: 29.0557, val_loss: 29.5256, val_MinusLogProbMetric: 29.5256

Epoch 667: val_loss did not improve from 29.41611
196/196 - 29s - loss: 29.0557 - MinusLogProbMetric: 29.0557 - val_loss: 29.5256 - val_MinusLogProbMetric: 29.5256 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 668/1000
2023-09-29 18:38:13.618 
Epoch 668/1000 
	 loss: 29.1387, MinusLogProbMetric: 29.1387, val_loss: 29.7661, val_MinusLogProbMetric: 29.7661

Epoch 668: val_loss did not improve from 29.41611
196/196 - 30s - loss: 29.1387 - MinusLogProbMetric: 29.1387 - val_loss: 29.7661 - val_MinusLogProbMetric: 29.7661 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 669/1000
2023-09-29 18:38:46.749 
Epoch 669/1000 
	 loss: 29.0989, MinusLogProbMetric: 29.0989, val_loss: 29.5622, val_MinusLogProbMetric: 29.5622

Epoch 669: val_loss did not improve from 29.41611
196/196 - 33s - loss: 29.0989 - MinusLogProbMetric: 29.0989 - val_loss: 29.5622 - val_MinusLogProbMetric: 29.5622 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 670/1000
2023-09-29 18:39:14.806 
Epoch 670/1000 
	 loss: 29.1127, MinusLogProbMetric: 29.1127, val_loss: 29.6303, val_MinusLogProbMetric: 29.6303

Epoch 670: val_loss did not improve from 29.41611
196/196 - 28s - loss: 29.1127 - MinusLogProbMetric: 29.1127 - val_loss: 29.6303 - val_MinusLogProbMetric: 29.6303 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 671/1000
2023-09-29 18:39:42.484 
Epoch 671/1000 
	 loss: 29.0735, MinusLogProbMetric: 29.0735, val_loss: 29.4132, val_MinusLogProbMetric: 29.4132

Epoch 671: val_loss improved from 29.41611 to 29.41320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 28s - loss: 29.0735 - MinusLogProbMetric: 29.0735 - val_loss: 29.4132 - val_MinusLogProbMetric: 29.4132 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 672/1000
2023-09-29 18:40:10.814 
Epoch 672/1000 
	 loss: 29.1636, MinusLogProbMetric: 29.1636, val_loss: 30.1923, val_MinusLogProbMetric: 30.1923

Epoch 672: val_loss did not improve from 29.41320
196/196 - 28s - loss: 29.1636 - MinusLogProbMetric: 29.1636 - val_loss: 30.1923 - val_MinusLogProbMetric: 30.1923 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 673/1000
2023-09-29 18:40:40.968 
Epoch 673/1000 
	 loss: 29.1726, MinusLogProbMetric: 29.1726, val_loss: 29.8318, val_MinusLogProbMetric: 29.8318

Epoch 673: val_loss did not improve from 29.41320
196/196 - 30s - loss: 29.1726 - MinusLogProbMetric: 29.1726 - val_loss: 29.8318 - val_MinusLogProbMetric: 29.8318 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 674/1000
2023-09-29 18:41:13.422 
Epoch 674/1000 
	 loss: 29.1048, MinusLogProbMetric: 29.1048, val_loss: 29.4855, val_MinusLogProbMetric: 29.4855

Epoch 674: val_loss did not improve from 29.41320
196/196 - 32s - loss: 29.1048 - MinusLogProbMetric: 29.1048 - val_loss: 29.4855 - val_MinusLogProbMetric: 29.4855 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 675/1000
2023-09-29 18:41:47.591 
Epoch 675/1000 
	 loss: 29.0677, MinusLogProbMetric: 29.0677, val_loss: 29.6132, val_MinusLogProbMetric: 29.6132

Epoch 675: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.0677 - MinusLogProbMetric: 29.0677 - val_loss: 29.6132 - val_MinusLogProbMetric: 29.6132 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 676/1000
2023-09-29 18:42:21.707 
Epoch 676/1000 
	 loss: 29.0518, MinusLogProbMetric: 29.0518, val_loss: 29.6480, val_MinusLogProbMetric: 29.6480

Epoch 676: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.0518 - MinusLogProbMetric: 29.0518 - val_loss: 29.6480 - val_MinusLogProbMetric: 29.6480 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 677/1000
2023-09-29 18:42:55.782 
Epoch 677/1000 
	 loss: 29.1298, MinusLogProbMetric: 29.1298, val_loss: 29.8565, val_MinusLogProbMetric: 29.8565

Epoch 677: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.1298 - MinusLogProbMetric: 29.1298 - val_loss: 29.8565 - val_MinusLogProbMetric: 29.8565 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 678/1000
2023-09-29 18:43:27.770 
Epoch 678/1000 
	 loss: 29.0839, MinusLogProbMetric: 29.0839, val_loss: 29.5175, val_MinusLogProbMetric: 29.5175

Epoch 678: val_loss did not improve from 29.41320
196/196 - 32s - loss: 29.0839 - MinusLogProbMetric: 29.0839 - val_loss: 29.5175 - val_MinusLogProbMetric: 29.5175 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 679/1000
2023-09-29 18:44:00.761 
Epoch 679/1000 
	 loss: 29.1448, MinusLogProbMetric: 29.1448, val_loss: 29.5407, val_MinusLogProbMetric: 29.5407

Epoch 679: val_loss did not improve from 29.41320
196/196 - 33s - loss: 29.1448 - MinusLogProbMetric: 29.1448 - val_loss: 29.5407 - val_MinusLogProbMetric: 29.5407 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 680/1000
2023-09-29 18:44:28.798 
Epoch 680/1000 
	 loss: 29.0749, MinusLogProbMetric: 29.0749, val_loss: 29.5474, val_MinusLogProbMetric: 29.5474

Epoch 680: val_loss did not improve from 29.41320
196/196 - 28s - loss: 29.0749 - MinusLogProbMetric: 29.0749 - val_loss: 29.5474 - val_MinusLogProbMetric: 29.5474 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 681/1000
2023-09-29 18:44:56.543 
Epoch 681/1000 
	 loss: 29.1153, MinusLogProbMetric: 29.1153, val_loss: 29.8415, val_MinusLogProbMetric: 29.8415

Epoch 681: val_loss did not improve from 29.41320
196/196 - 28s - loss: 29.1153 - MinusLogProbMetric: 29.1153 - val_loss: 29.8415 - val_MinusLogProbMetric: 29.8415 - lr: 1.6667e-04 - 28s/epoch - 142ms/step
Epoch 682/1000
2023-09-29 18:45:24.586 
Epoch 682/1000 
	 loss: 29.0562, MinusLogProbMetric: 29.0562, val_loss: 29.7493, val_MinusLogProbMetric: 29.7493

Epoch 682: val_loss did not improve from 29.41320
196/196 - 28s - loss: 29.0562 - MinusLogProbMetric: 29.0562 - val_loss: 29.7493 - val_MinusLogProbMetric: 29.7493 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 683/1000
2023-09-29 18:45:53.279 
Epoch 683/1000 
	 loss: 29.0884, MinusLogProbMetric: 29.0884, val_loss: 29.5429, val_MinusLogProbMetric: 29.5429

Epoch 683: val_loss did not improve from 29.41320
196/196 - 29s - loss: 29.0884 - MinusLogProbMetric: 29.0884 - val_loss: 29.5429 - val_MinusLogProbMetric: 29.5429 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 684/1000
2023-09-29 18:46:27.025 
Epoch 684/1000 
	 loss: 29.0998, MinusLogProbMetric: 29.0998, val_loss: 29.4859, val_MinusLogProbMetric: 29.4859

Epoch 684: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.0998 - MinusLogProbMetric: 29.0998 - val_loss: 29.4859 - val_MinusLogProbMetric: 29.4859 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 685/1000
2023-09-29 18:47:01.655 
Epoch 685/1000 
	 loss: 29.0875, MinusLogProbMetric: 29.0875, val_loss: 29.5408, val_MinusLogProbMetric: 29.5408

Epoch 685: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.0875 - MinusLogProbMetric: 29.0875 - val_loss: 29.5408 - val_MinusLogProbMetric: 29.5408 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 686/1000
2023-09-29 18:47:36.313 
Epoch 686/1000 
	 loss: 29.0724, MinusLogProbMetric: 29.0724, val_loss: 29.5079, val_MinusLogProbMetric: 29.5079

Epoch 686: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.0724 - MinusLogProbMetric: 29.0724 - val_loss: 29.5079 - val_MinusLogProbMetric: 29.5079 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 687/1000
2023-09-29 18:48:10.772 
Epoch 687/1000 
	 loss: 29.0559, MinusLogProbMetric: 29.0559, val_loss: 29.5197, val_MinusLogProbMetric: 29.5197

Epoch 687: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.0559 - MinusLogProbMetric: 29.0559 - val_loss: 29.5197 - val_MinusLogProbMetric: 29.5197 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 688/1000
2023-09-29 18:48:45.170 
Epoch 688/1000 
	 loss: 29.0837, MinusLogProbMetric: 29.0837, val_loss: 29.6460, val_MinusLogProbMetric: 29.6460

Epoch 688: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.0837 - MinusLogProbMetric: 29.0837 - val_loss: 29.6460 - val_MinusLogProbMetric: 29.6460 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 689/1000
2023-09-29 18:49:16.095 
Epoch 689/1000 
	 loss: 29.0469, MinusLogProbMetric: 29.0469, val_loss: 29.7616, val_MinusLogProbMetric: 29.7616

Epoch 689: val_loss did not improve from 29.41320
196/196 - 31s - loss: 29.0469 - MinusLogProbMetric: 29.0469 - val_loss: 29.7616 - val_MinusLogProbMetric: 29.7616 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 690/1000
2023-09-29 18:49:50.721 
Epoch 690/1000 
	 loss: 29.0754, MinusLogProbMetric: 29.0754, val_loss: 29.6197, val_MinusLogProbMetric: 29.6197

Epoch 690: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.0754 - MinusLogProbMetric: 29.0754 - val_loss: 29.6197 - val_MinusLogProbMetric: 29.6197 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 691/1000
2023-09-29 18:50:25.559 
Epoch 691/1000 
	 loss: 29.0655, MinusLogProbMetric: 29.0655, val_loss: 29.5631, val_MinusLogProbMetric: 29.5631

Epoch 691: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.0655 - MinusLogProbMetric: 29.0655 - val_loss: 29.5631 - val_MinusLogProbMetric: 29.5631 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 692/1000
2023-09-29 18:51:00.313 
Epoch 692/1000 
	 loss: 29.0290, MinusLogProbMetric: 29.0290, val_loss: 29.5752, val_MinusLogProbMetric: 29.5752

Epoch 692: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.0290 - MinusLogProbMetric: 29.0290 - val_loss: 29.5752 - val_MinusLogProbMetric: 29.5752 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 693/1000
2023-09-29 18:51:35.115 
Epoch 693/1000 
	 loss: 29.1007, MinusLogProbMetric: 29.1007, val_loss: 30.1879, val_MinusLogProbMetric: 30.1879

Epoch 693: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.1007 - MinusLogProbMetric: 29.1007 - val_loss: 30.1879 - val_MinusLogProbMetric: 30.1879 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 694/1000
2023-09-29 18:52:09.279 
Epoch 694/1000 
	 loss: 29.1412, MinusLogProbMetric: 29.1412, val_loss: 29.6033, val_MinusLogProbMetric: 29.6033

Epoch 694: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.1412 - MinusLogProbMetric: 29.1412 - val_loss: 29.6033 - val_MinusLogProbMetric: 29.6033 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 695/1000
2023-09-29 18:52:38.108 
Epoch 695/1000 
	 loss: 29.0434, MinusLogProbMetric: 29.0434, val_loss: 29.4896, val_MinusLogProbMetric: 29.4896

Epoch 695: val_loss did not improve from 29.41320
196/196 - 29s - loss: 29.0434 - MinusLogProbMetric: 29.0434 - val_loss: 29.4896 - val_MinusLogProbMetric: 29.4896 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 696/1000
2023-09-29 18:53:05.642 
Epoch 696/1000 
	 loss: 29.0523, MinusLogProbMetric: 29.0523, val_loss: 29.4981, val_MinusLogProbMetric: 29.4981

Epoch 696: val_loss did not improve from 29.41320
196/196 - 28s - loss: 29.0523 - MinusLogProbMetric: 29.0523 - val_loss: 29.4981 - val_MinusLogProbMetric: 29.4981 - lr: 1.6667e-04 - 28s/epoch - 140ms/step
Epoch 697/1000
2023-09-29 18:53:33.673 
Epoch 697/1000 
	 loss: 29.0109, MinusLogProbMetric: 29.0109, val_loss: 29.5166, val_MinusLogProbMetric: 29.5166

Epoch 697: val_loss did not improve from 29.41320
196/196 - 28s - loss: 29.0109 - MinusLogProbMetric: 29.0109 - val_loss: 29.5166 - val_MinusLogProbMetric: 29.5166 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 698/1000
2023-09-29 18:54:02.392 
Epoch 698/1000 
	 loss: 29.0194, MinusLogProbMetric: 29.0194, val_loss: 29.4533, val_MinusLogProbMetric: 29.4533

Epoch 698: val_loss did not improve from 29.41320
196/196 - 29s - loss: 29.0194 - MinusLogProbMetric: 29.0194 - val_loss: 29.4533 - val_MinusLogProbMetric: 29.4533 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 699/1000
2023-09-29 18:54:35.888 
Epoch 699/1000 
	 loss: 29.0626, MinusLogProbMetric: 29.0626, val_loss: 29.5338, val_MinusLogProbMetric: 29.5338

Epoch 699: val_loss did not improve from 29.41320
196/196 - 33s - loss: 29.0626 - MinusLogProbMetric: 29.0626 - val_loss: 29.5338 - val_MinusLogProbMetric: 29.5338 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 700/1000
2023-09-29 18:55:10.678 
Epoch 700/1000 
	 loss: 29.1031, MinusLogProbMetric: 29.1031, val_loss: 29.5613, val_MinusLogProbMetric: 29.5613

Epoch 700: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.1031 - MinusLogProbMetric: 29.1031 - val_loss: 29.5613 - val_MinusLogProbMetric: 29.5613 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 701/1000
2023-09-29 18:55:44.378 
Epoch 701/1000 
	 loss: 29.0201, MinusLogProbMetric: 29.0201, val_loss: 29.7635, val_MinusLogProbMetric: 29.7635

Epoch 701: val_loss did not improve from 29.41320
196/196 - 34s - loss: 29.0201 - MinusLogProbMetric: 29.0201 - val_loss: 29.7635 - val_MinusLogProbMetric: 29.7635 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 702/1000
2023-09-29 18:56:17.300 
Epoch 702/1000 
	 loss: 28.9846, MinusLogProbMetric: 28.9846, val_loss: 30.3647, val_MinusLogProbMetric: 30.3647

Epoch 702: val_loss did not improve from 29.41320
196/196 - 33s - loss: 28.9846 - MinusLogProbMetric: 28.9846 - val_loss: 30.3647 - val_MinusLogProbMetric: 30.3647 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 703/1000
2023-09-29 18:56:52.184 
Epoch 703/1000 
	 loss: 29.1222, MinusLogProbMetric: 29.1222, val_loss: 29.5242, val_MinusLogProbMetric: 29.5242

Epoch 703: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.1222 - MinusLogProbMetric: 29.1222 - val_loss: 29.5242 - val_MinusLogProbMetric: 29.5242 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 704/1000
2023-09-29 18:57:24.197 
Epoch 704/1000 
	 loss: 29.0219, MinusLogProbMetric: 29.0219, val_loss: 29.5812, val_MinusLogProbMetric: 29.5812

Epoch 704: val_loss did not improve from 29.41320
196/196 - 32s - loss: 29.0219 - MinusLogProbMetric: 29.0219 - val_loss: 29.5812 - val_MinusLogProbMetric: 29.5812 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 705/1000
2023-09-29 18:57:58.811 
Epoch 705/1000 
	 loss: 29.0956, MinusLogProbMetric: 29.0956, val_loss: 29.4774, val_MinusLogProbMetric: 29.4774

Epoch 705: val_loss did not improve from 29.41320
196/196 - 35s - loss: 29.0956 - MinusLogProbMetric: 29.0956 - val_loss: 29.4774 - val_MinusLogProbMetric: 29.4774 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 706/1000
2023-09-29 18:58:33.368 
Epoch 706/1000 
	 loss: 29.0282, MinusLogProbMetric: 29.0282, val_loss: 29.3588, val_MinusLogProbMetric: 29.3588

Epoch 706: val_loss improved from 29.41320 to 29.35882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 29.0282 - MinusLogProbMetric: 29.0282 - val_loss: 29.3588 - val_MinusLogProbMetric: 29.3588 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 707/1000
2023-09-29 18:59:08.781 
Epoch 707/1000 
	 loss: 29.0755, MinusLogProbMetric: 29.0755, val_loss: 29.6349, val_MinusLogProbMetric: 29.6349

Epoch 707: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0755 - MinusLogProbMetric: 29.0755 - val_loss: 29.6349 - val_MinusLogProbMetric: 29.6349 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 708/1000
2023-09-29 18:59:43.534 
Epoch 708/1000 
	 loss: 28.9947, MinusLogProbMetric: 28.9947, val_loss: 29.7114, val_MinusLogProbMetric: 29.7114

Epoch 708: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9947 - MinusLogProbMetric: 28.9947 - val_loss: 29.7114 - val_MinusLogProbMetric: 29.7114 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 709/1000
2023-09-29 19:00:18.505 
Epoch 709/1000 
	 loss: 29.0242, MinusLogProbMetric: 29.0242, val_loss: 29.4827, val_MinusLogProbMetric: 29.4827

Epoch 709: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0242 - MinusLogProbMetric: 29.0242 - val_loss: 29.4827 - val_MinusLogProbMetric: 29.4827 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 710/1000
2023-09-29 19:00:53.497 
Epoch 710/1000 
	 loss: 29.0448, MinusLogProbMetric: 29.0448, val_loss: 29.5143, val_MinusLogProbMetric: 29.5143

Epoch 710: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0448 - MinusLogProbMetric: 29.0448 - val_loss: 29.5143 - val_MinusLogProbMetric: 29.5143 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 711/1000
2023-09-29 19:01:28.283 
Epoch 711/1000 
	 loss: 29.0370, MinusLogProbMetric: 29.0370, val_loss: 29.5894, val_MinusLogProbMetric: 29.5894

Epoch 711: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0370 - MinusLogProbMetric: 29.0370 - val_loss: 29.5894 - val_MinusLogProbMetric: 29.5894 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 712/1000
2023-09-29 19:02:02.788 
Epoch 712/1000 
	 loss: 29.0670, MinusLogProbMetric: 29.0670, val_loss: 29.7598, val_MinusLogProbMetric: 29.7598

Epoch 712: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0670 - MinusLogProbMetric: 29.0670 - val_loss: 29.7598 - val_MinusLogProbMetric: 29.7598 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 713/1000
2023-09-29 19:02:37.925 
Epoch 713/1000 
	 loss: 29.0074, MinusLogProbMetric: 29.0074, val_loss: 29.8118, val_MinusLogProbMetric: 29.8118

Epoch 713: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0074 - MinusLogProbMetric: 29.0074 - val_loss: 29.8118 - val_MinusLogProbMetric: 29.8118 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 714/1000
2023-09-29 19:03:12.561 
Epoch 714/1000 
	 loss: 29.0739, MinusLogProbMetric: 29.0739, val_loss: 29.8142, val_MinusLogProbMetric: 29.8142

Epoch 714: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0739 - MinusLogProbMetric: 29.0739 - val_loss: 29.8142 - val_MinusLogProbMetric: 29.8142 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 715/1000
2023-09-29 19:03:47.431 
Epoch 715/1000 
	 loss: 29.0222, MinusLogProbMetric: 29.0222, val_loss: 30.6626, val_MinusLogProbMetric: 30.6626

Epoch 715: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0222 - MinusLogProbMetric: 29.0222 - val_loss: 30.6626 - val_MinusLogProbMetric: 30.6626 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 716/1000
2023-09-29 19:04:21.150 
Epoch 716/1000 
	 loss: 29.0493, MinusLogProbMetric: 29.0493, val_loss: 29.7803, val_MinusLogProbMetric: 29.7803

Epoch 716: val_loss did not improve from 29.35882
196/196 - 34s - loss: 29.0493 - MinusLogProbMetric: 29.0493 - val_loss: 29.7803 - val_MinusLogProbMetric: 29.7803 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 717/1000
2023-09-29 19:04:55.550 
Epoch 717/1000 
	 loss: 29.0274, MinusLogProbMetric: 29.0274, val_loss: 29.5035, val_MinusLogProbMetric: 29.5035

Epoch 717: val_loss did not improve from 29.35882
196/196 - 34s - loss: 29.0274 - MinusLogProbMetric: 29.0274 - val_loss: 29.5035 - val_MinusLogProbMetric: 29.5035 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 718/1000
2023-09-29 19:05:29.603 
Epoch 718/1000 
	 loss: 29.0452, MinusLogProbMetric: 29.0452, val_loss: 29.8235, val_MinusLogProbMetric: 29.8235

Epoch 718: val_loss did not improve from 29.35882
196/196 - 34s - loss: 29.0452 - MinusLogProbMetric: 29.0452 - val_loss: 29.8235 - val_MinusLogProbMetric: 29.8235 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 719/1000
2023-09-29 19:06:04.595 
Epoch 719/1000 
	 loss: 28.9884, MinusLogProbMetric: 28.9884, val_loss: 30.0046, val_MinusLogProbMetric: 30.0046

Epoch 719: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9884 - MinusLogProbMetric: 28.9884 - val_loss: 30.0046 - val_MinusLogProbMetric: 30.0046 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 720/1000
2023-09-29 19:06:39.557 
Epoch 720/1000 
	 loss: 29.0196, MinusLogProbMetric: 29.0196, val_loss: 29.5538, val_MinusLogProbMetric: 29.5538

Epoch 720: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0196 - MinusLogProbMetric: 29.0196 - val_loss: 29.5538 - val_MinusLogProbMetric: 29.5538 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 721/1000
2023-09-29 19:07:14.413 
Epoch 721/1000 
	 loss: 29.0217, MinusLogProbMetric: 29.0217, val_loss: 29.6394, val_MinusLogProbMetric: 29.6394

Epoch 721: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0217 - MinusLogProbMetric: 29.0217 - val_loss: 29.6394 - val_MinusLogProbMetric: 29.6394 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 722/1000
2023-09-29 19:07:49.114 
Epoch 722/1000 
	 loss: 29.0210, MinusLogProbMetric: 29.0210, val_loss: 29.4675, val_MinusLogProbMetric: 29.4675

Epoch 722: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0210 - MinusLogProbMetric: 29.0210 - val_loss: 29.4675 - val_MinusLogProbMetric: 29.4675 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 723/1000
2023-09-29 19:08:23.782 
Epoch 723/1000 
	 loss: 29.0003, MinusLogProbMetric: 29.0003, val_loss: 29.6276, val_MinusLogProbMetric: 29.6276

Epoch 723: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0003 - MinusLogProbMetric: 29.0003 - val_loss: 29.6276 - val_MinusLogProbMetric: 29.6276 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 724/1000
2023-09-29 19:08:59.192 
Epoch 724/1000 
	 loss: 29.0082, MinusLogProbMetric: 29.0082, val_loss: 29.6527, val_MinusLogProbMetric: 29.6527

Epoch 724: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0082 - MinusLogProbMetric: 29.0082 - val_loss: 29.6527 - val_MinusLogProbMetric: 29.6527 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 725/1000
2023-09-29 19:09:34.222 
Epoch 725/1000 
	 loss: 28.9928, MinusLogProbMetric: 28.9928, val_loss: 29.8006, val_MinusLogProbMetric: 29.8006

Epoch 725: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9928 - MinusLogProbMetric: 28.9928 - val_loss: 29.8006 - val_MinusLogProbMetric: 29.8006 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 726/1000
2023-09-29 19:10:08.790 
Epoch 726/1000 
	 loss: 29.0843, MinusLogProbMetric: 29.0843, val_loss: 29.6872, val_MinusLogProbMetric: 29.6872

Epoch 726: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0843 - MinusLogProbMetric: 29.0843 - val_loss: 29.6872 - val_MinusLogProbMetric: 29.6872 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 727/1000
2023-09-29 19:10:43.243 
Epoch 727/1000 
	 loss: 29.2159, MinusLogProbMetric: 29.2159, val_loss: 29.7689, val_MinusLogProbMetric: 29.7689

Epoch 727: val_loss did not improve from 29.35882
196/196 - 34s - loss: 29.2159 - MinusLogProbMetric: 29.2159 - val_loss: 29.7689 - val_MinusLogProbMetric: 29.7689 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 728/1000
2023-09-29 19:11:18.113 
Epoch 728/1000 
	 loss: 29.0314, MinusLogProbMetric: 29.0314, val_loss: 29.6657, val_MinusLogProbMetric: 29.6657

Epoch 728: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0314 - MinusLogProbMetric: 29.0314 - val_loss: 29.6657 - val_MinusLogProbMetric: 29.6657 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 729/1000
2023-09-29 19:11:53.086 
Epoch 729/1000 
	 loss: 29.0095, MinusLogProbMetric: 29.0095, val_loss: 29.5423, val_MinusLogProbMetric: 29.5423

Epoch 729: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0095 - MinusLogProbMetric: 29.0095 - val_loss: 29.5423 - val_MinusLogProbMetric: 29.5423 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 730/1000
2023-09-29 19:12:28.061 
Epoch 730/1000 
	 loss: 29.0019, MinusLogProbMetric: 29.0019, val_loss: 29.5037, val_MinusLogProbMetric: 29.5037

Epoch 730: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0019 - MinusLogProbMetric: 29.0019 - val_loss: 29.5037 - val_MinusLogProbMetric: 29.5037 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 731/1000
2023-09-29 19:13:03.123 
Epoch 731/1000 
	 loss: 29.0350, MinusLogProbMetric: 29.0350, val_loss: 29.6722, val_MinusLogProbMetric: 29.6722

Epoch 731: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0350 - MinusLogProbMetric: 29.0350 - val_loss: 29.6722 - val_MinusLogProbMetric: 29.6722 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 732/1000
2023-09-29 19:13:38.269 
Epoch 732/1000 
	 loss: 28.9990, MinusLogProbMetric: 28.9990, val_loss: 29.5170, val_MinusLogProbMetric: 29.5170

Epoch 732: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9990 - MinusLogProbMetric: 28.9990 - val_loss: 29.5170 - val_MinusLogProbMetric: 29.5170 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 733/1000
2023-09-29 19:14:13.077 
Epoch 733/1000 
	 loss: 28.9692, MinusLogProbMetric: 28.9692, val_loss: 29.7252, val_MinusLogProbMetric: 29.7252

Epoch 733: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9692 - MinusLogProbMetric: 28.9692 - val_loss: 29.7252 - val_MinusLogProbMetric: 29.7252 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 734/1000
2023-09-29 19:14:48.070 
Epoch 734/1000 
	 loss: 29.0887, MinusLogProbMetric: 29.0887, val_loss: 29.7170, val_MinusLogProbMetric: 29.7170

Epoch 734: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0887 - MinusLogProbMetric: 29.0887 - val_loss: 29.7170 - val_MinusLogProbMetric: 29.7170 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 735/1000
2023-09-29 19:15:22.134 
Epoch 735/1000 
	 loss: 29.0056, MinusLogProbMetric: 29.0056, val_loss: 29.7493, val_MinusLogProbMetric: 29.7493

Epoch 735: val_loss did not improve from 29.35882
196/196 - 34s - loss: 29.0056 - MinusLogProbMetric: 29.0056 - val_loss: 29.7493 - val_MinusLogProbMetric: 29.7493 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 736/1000
2023-09-29 19:15:56.950 
Epoch 736/1000 
	 loss: 29.0187, MinusLogProbMetric: 29.0187, val_loss: 29.5536, val_MinusLogProbMetric: 29.5536

Epoch 736: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0187 - MinusLogProbMetric: 29.0187 - val_loss: 29.5536 - val_MinusLogProbMetric: 29.5536 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 737/1000
2023-09-29 19:16:32.074 
Epoch 737/1000 
	 loss: 29.0178, MinusLogProbMetric: 29.0178, val_loss: 29.6060, val_MinusLogProbMetric: 29.6060

Epoch 737: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0178 - MinusLogProbMetric: 29.0178 - val_loss: 29.6060 - val_MinusLogProbMetric: 29.6060 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 738/1000
2023-09-29 19:17:07.109 
Epoch 738/1000 
	 loss: 28.9551, MinusLogProbMetric: 28.9551, val_loss: 29.9082, val_MinusLogProbMetric: 29.9082

Epoch 738: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9551 - MinusLogProbMetric: 28.9551 - val_loss: 29.9082 - val_MinusLogProbMetric: 29.9082 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 739/1000
2023-09-29 19:17:41.939 
Epoch 739/1000 
	 loss: 28.9744, MinusLogProbMetric: 28.9744, val_loss: 29.4262, val_MinusLogProbMetric: 29.4262

Epoch 739: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9744 - MinusLogProbMetric: 28.9744 - val_loss: 29.4262 - val_MinusLogProbMetric: 29.4262 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 740/1000
2023-09-29 19:18:17.072 
Epoch 740/1000 
	 loss: 28.9827, MinusLogProbMetric: 28.9827, val_loss: 29.3966, val_MinusLogProbMetric: 29.3966

Epoch 740: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9827 - MinusLogProbMetric: 28.9827 - val_loss: 29.3966 - val_MinusLogProbMetric: 29.3966 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 741/1000
2023-09-29 19:18:50.318 
Epoch 741/1000 
	 loss: 29.0224, MinusLogProbMetric: 29.0224, val_loss: 29.5336, val_MinusLogProbMetric: 29.5336

Epoch 741: val_loss did not improve from 29.35882
196/196 - 33s - loss: 29.0224 - MinusLogProbMetric: 29.0224 - val_loss: 29.5336 - val_MinusLogProbMetric: 29.5336 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 742/1000
2023-09-29 19:19:25.081 
Epoch 742/1000 
	 loss: 28.9782, MinusLogProbMetric: 28.9782, val_loss: 29.6398, val_MinusLogProbMetric: 29.6398

Epoch 742: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9782 - MinusLogProbMetric: 28.9782 - val_loss: 29.6398 - val_MinusLogProbMetric: 29.6398 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 743/1000
2023-09-29 19:19:59.745 
Epoch 743/1000 
	 loss: 28.9975, MinusLogProbMetric: 28.9975, val_loss: 29.4580, val_MinusLogProbMetric: 29.4580

Epoch 743: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9975 - MinusLogProbMetric: 28.9975 - val_loss: 29.4580 - val_MinusLogProbMetric: 29.4580 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 744/1000
2023-09-29 19:20:33.422 
Epoch 744/1000 
	 loss: 28.9511, MinusLogProbMetric: 28.9511, val_loss: 29.3810, val_MinusLogProbMetric: 29.3810

Epoch 744: val_loss did not improve from 29.35882
196/196 - 34s - loss: 28.9511 - MinusLogProbMetric: 28.9511 - val_loss: 29.3810 - val_MinusLogProbMetric: 29.3810 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 745/1000
2023-09-29 19:21:07.538 
Epoch 745/1000 
	 loss: 28.9950, MinusLogProbMetric: 28.9950, val_loss: 29.3964, val_MinusLogProbMetric: 29.3964

Epoch 745: val_loss did not improve from 29.35882
196/196 - 34s - loss: 28.9950 - MinusLogProbMetric: 28.9950 - val_loss: 29.3964 - val_MinusLogProbMetric: 29.3964 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 746/1000
2023-09-29 19:21:41.937 
Epoch 746/1000 
	 loss: 28.9966, MinusLogProbMetric: 28.9966, val_loss: 29.7715, val_MinusLogProbMetric: 29.7715

Epoch 746: val_loss did not improve from 29.35882
196/196 - 34s - loss: 28.9966 - MinusLogProbMetric: 28.9966 - val_loss: 29.7715 - val_MinusLogProbMetric: 29.7715 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 747/1000
2023-09-29 19:22:16.756 
Epoch 747/1000 
	 loss: 28.9760, MinusLogProbMetric: 28.9760, val_loss: 29.4108, val_MinusLogProbMetric: 29.4108

Epoch 747: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9760 - MinusLogProbMetric: 28.9760 - val_loss: 29.4108 - val_MinusLogProbMetric: 29.4108 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 748/1000
2023-09-29 19:22:49.317 
Epoch 748/1000 
	 loss: 29.0765, MinusLogProbMetric: 29.0765, val_loss: 30.0943, val_MinusLogProbMetric: 30.0943

Epoch 748: val_loss did not improve from 29.35882
196/196 - 33s - loss: 29.0765 - MinusLogProbMetric: 29.0765 - val_loss: 30.0943 - val_MinusLogProbMetric: 30.0943 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 749/1000
2023-09-29 19:23:24.045 
Epoch 749/1000 
	 loss: 29.0836, MinusLogProbMetric: 29.0836, val_loss: 29.4688, val_MinusLogProbMetric: 29.4688

Epoch 749: val_loss did not improve from 29.35882
196/196 - 35s - loss: 29.0836 - MinusLogProbMetric: 29.0836 - val_loss: 29.4688 - val_MinusLogProbMetric: 29.4688 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 750/1000
2023-09-29 19:23:57.369 
Epoch 750/1000 
	 loss: 29.0204, MinusLogProbMetric: 29.0204, val_loss: 29.5670, val_MinusLogProbMetric: 29.5670

Epoch 750: val_loss did not improve from 29.35882
196/196 - 33s - loss: 29.0204 - MinusLogProbMetric: 29.0204 - val_loss: 29.5670 - val_MinusLogProbMetric: 29.5670 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 751/1000
2023-09-29 19:24:32.390 
Epoch 751/1000 
	 loss: 28.9719, MinusLogProbMetric: 28.9719, val_loss: 29.5354, val_MinusLogProbMetric: 29.5354

Epoch 751: val_loss did not improve from 29.35882
196/196 - 35s - loss: 28.9719 - MinusLogProbMetric: 28.9719 - val_loss: 29.5354 - val_MinusLogProbMetric: 29.5354 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 752/1000
2023-09-29 19:25:06.580 
Epoch 752/1000 
	 loss: 29.0287, MinusLogProbMetric: 29.0287, val_loss: 29.8523, val_MinusLogProbMetric: 29.8523

Epoch 752: val_loss did not improve from 29.35882
196/196 - 34s - loss: 29.0287 - MinusLogProbMetric: 29.0287 - val_loss: 29.8523 - val_MinusLogProbMetric: 29.8523 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 753/1000
2023-09-29 19:25:37.063 
Epoch 753/1000 
	 loss: 29.0030, MinusLogProbMetric: 29.0030, val_loss: 29.4424, val_MinusLogProbMetric: 29.4424

Epoch 753: val_loss did not improve from 29.35882
196/196 - 30s - loss: 29.0030 - MinusLogProbMetric: 29.0030 - val_loss: 29.4424 - val_MinusLogProbMetric: 29.4424 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 754/1000
2023-09-29 19:26:09.837 
Epoch 754/1000 
	 loss: 28.9965, MinusLogProbMetric: 28.9965, val_loss: 29.3326, val_MinusLogProbMetric: 29.3326

Epoch 754: val_loss improved from 29.35882 to 29.33261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 33s - loss: 28.9965 - MinusLogProbMetric: 28.9965 - val_loss: 29.3326 - val_MinusLogProbMetric: 29.3326 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 755/1000
2023-09-29 19:26:44.402 
Epoch 755/1000 
	 loss: 28.9847, MinusLogProbMetric: 28.9847, val_loss: 29.6927, val_MinusLogProbMetric: 29.6927

Epoch 755: val_loss did not improve from 29.33261
196/196 - 34s - loss: 28.9847 - MinusLogProbMetric: 28.9847 - val_loss: 29.6927 - val_MinusLogProbMetric: 29.6927 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 756/1000
2023-09-29 19:27:16.997 
Epoch 756/1000 
	 loss: 29.0466, MinusLogProbMetric: 29.0466, val_loss: 29.4570, val_MinusLogProbMetric: 29.4570

Epoch 756: val_loss did not improve from 29.33261
196/196 - 33s - loss: 29.0466 - MinusLogProbMetric: 29.0466 - val_loss: 29.4570 - val_MinusLogProbMetric: 29.4570 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 757/1000
2023-09-29 19:27:51.107 
Epoch 757/1000 
	 loss: 28.9413, MinusLogProbMetric: 28.9413, val_loss: 29.6441, val_MinusLogProbMetric: 29.6441

Epoch 757: val_loss did not improve from 29.33261
196/196 - 34s - loss: 28.9413 - MinusLogProbMetric: 28.9413 - val_loss: 29.6441 - val_MinusLogProbMetric: 29.6441 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 758/1000
2023-09-29 19:28:21.723 
Epoch 758/1000 
	 loss: 29.0769, MinusLogProbMetric: 29.0769, val_loss: 29.5402, val_MinusLogProbMetric: 29.5402

Epoch 758: val_loss did not improve from 29.33261
196/196 - 31s - loss: 29.0769 - MinusLogProbMetric: 29.0769 - val_loss: 29.5402 - val_MinusLogProbMetric: 29.5402 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 759/1000
2023-09-29 19:28:55.470 
Epoch 759/1000 
	 loss: 28.9271, MinusLogProbMetric: 28.9271, val_loss: 29.9158, val_MinusLogProbMetric: 29.9158

Epoch 759: val_loss did not improve from 29.33261
196/196 - 34s - loss: 28.9271 - MinusLogProbMetric: 28.9271 - val_loss: 29.9158 - val_MinusLogProbMetric: 29.9158 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 760/1000
2023-09-29 19:29:30.214 
Epoch 760/1000 
	 loss: 29.0314, MinusLogProbMetric: 29.0314, val_loss: 29.5854, val_MinusLogProbMetric: 29.5854

Epoch 760: val_loss did not improve from 29.33261
196/196 - 35s - loss: 29.0314 - MinusLogProbMetric: 29.0314 - val_loss: 29.5854 - val_MinusLogProbMetric: 29.5854 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 761/1000
2023-09-29 19:30:00.658 
Epoch 761/1000 
	 loss: 28.9697, MinusLogProbMetric: 28.9697, val_loss: 29.6775, val_MinusLogProbMetric: 29.6775

Epoch 761: val_loss did not improve from 29.33261
196/196 - 30s - loss: 28.9697 - MinusLogProbMetric: 28.9697 - val_loss: 29.6775 - val_MinusLogProbMetric: 29.6775 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 762/1000
2023-09-29 19:30:31.721 
Epoch 762/1000 
	 loss: 29.0304, MinusLogProbMetric: 29.0304, val_loss: 29.6037, val_MinusLogProbMetric: 29.6037

Epoch 762: val_loss did not improve from 29.33261
196/196 - 31s - loss: 29.0304 - MinusLogProbMetric: 29.0304 - val_loss: 29.6037 - val_MinusLogProbMetric: 29.6037 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 763/1000
2023-09-29 19:31:02.396 
Epoch 763/1000 
	 loss: 28.9775, MinusLogProbMetric: 28.9775, val_loss: 29.4871, val_MinusLogProbMetric: 29.4871

Epoch 763: val_loss did not improve from 29.33261
196/196 - 31s - loss: 28.9775 - MinusLogProbMetric: 28.9775 - val_loss: 29.4871 - val_MinusLogProbMetric: 29.4871 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 764/1000
2023-09-29 19:31:33.836 
Epoch 764/1000 
	 loss: 28.9487, MinusLogProbMetric: 28.9487, val_loss: 29.3672, val_MinusLogProbMetric: 29.3672

Epoch 764: val_loss did not improve from 29.33261
196/196 - 31s - loss: 28.9487 - MinusLogProbMetric: 28.9487 - val_loss: 29.3672 - val_MinusLogProbMetric: 29.3672 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 765/1000
2023-09-29 19:32:05.592 
Epoch 765/1000 
	 loss: 28.9849, MinusLogProbMetric: 28.9849, val_loss: 29.5090, val_MinusLogProbMetric: 29.5090

Epoch 765: val_loss did not improve from 29.33261
196/196 - 32s - loss: 28.9849 - MinusLogProbMetric: 28.9849 - val_loss: 29.5090 - val_MinusLogProbMetric: 29.5090 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 766/1000
2023-09-29 19:32:36.872 
Epoch 766/1000 
	 loss: 29.1710, MinusLogProbMetric: 29.1710, val_loss: 29.5313, val_MinusLogProbMetric: 29.5313

Epoch 766: val_loss did not improve from 29.33261
196/196 - 31s - loss: 29.1710 - MinusLogProbMetric: 29.1710 - val_loss: 29.5313 - val_MinusLogProbMetric: 29.5313 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 767/1000
2023-09-29 19:33:08.094 
Epoch 767/1000 
	 loss: 28.9496, MinusLogProbMetric: 28.9496, val_loss: 29.3920, val_MinusLogProbMetric: 29.3920

Epoch 767: val_loss did not improve from 29.33261
196/196 - 31s - loss: 28.9496 - MinusLogProbMetric: 28.9496 - val_loss: 29.3920 - val_MinusLogProbMetric: 29.3920 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 768/1000
2023-09-29 19:33:42.872 
Epoch 768/1000 
	 loss: 29.0736, MinusLogProbMetric: 29.0736, val_loss: 29.4645, val_MinusLogProbMetric: 29.4645

Epoch 768: val_loss did not improve from 29.33261
196/196 - 35s - loss: 29.0736 - MinusLogProbMetric: 29.0736 - val_loss: 29.4645 - val_MinusLogProbMetric: 29.4645 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 769/1000
2023-09-29 19:34:15.825 
Epoch 769/1000 
	 loss: 28.9248, MinusLogProbMetric: 28.9248, val_loss: 29.4375, val_MinusLogProbMetric: 29.4375

Epoch 769: val_loss did not improve from 29.33261
196/196 - 33s - loss: 28.9248 - MinusLogProbMetric: 28.9248 - val_loss: 29.4375 - val_MinusLogProbMetric: 29.4375 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 770/1000
2023-09-29 19:34:46.694 
Epoch 770/1000 
	 loss: 28.9447, MinusLogProbMetric: 28.9447, val_loss: 29.4496, val_MinusLogProbMetric: 29.4496

Epoch 770: val_loss did not improve from 29.33261
196/196 - 31s - loss: 28.9447 - MinusLogProbMetric: 28.9447 - val_loss: 29.4496 - val_MinusLogProbMetric: 29.4496 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 771/1000
2023-09-29 19:35:19.128 
Epoch 771/1000 
	 loss: 28.9165, MinusLogProbMetric: 28.9165, val_loss: 29.5052, val_MinusLogProbMetric: 29.5052

Epoch 771: val_loss did not improve from 29.33261
196/196 - 32s - loss: 28.9165 - MinusLogProbMetric: 28.9165 - val_loss: 29.5052 - val_MinusLogProbMetric: 29.5052 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 772/1000
2023-09-29 19:35:54.242 
Epoch 772/1000 
	 loss: 28.9673, MinusLogProbMetric: 28.9673, val_loss: 29.5287, val_MinusLogProbMetric: 29.5287

Epoch 772: val_loss did not improve from 29.33261
196/196 - 35s - loss: 28.9673 - MinusLogProbMetric: 28.9673 - val_loss: 29.5287 - val_MinusLogProbMetric: 29.5287 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 773/1000
2023-09-29 19:36:27.481 
Epoch 773/1000 
	 loss: 28.9760, MinusLogProbMetric: 28.9760, val_loss: 29.7307, val_MinusLogProbMetric: 29.7307

Epoch 773: val_loss did not improve from 29.33261
196/196 - 33s - loss: 28.9760 - MinusLogProbMetric: 28.9760 - val_loss: 29.7307 - val_MinusLogProbMetric: 29.7307 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 774/1000
2023-09-29 19:37:00.830 
Epoch 774/1000 
	 loss: 29.0119, MinusLogProbMetric: 29.0119, val_loss: 29.5451, val_MinusLogProbMetric: 29.5451

Epoch 774: val_loss did not improve from 29.33261
196/196 - 33s - loss: 29.0119 - MinusLogProbMetric: 29.0119 - val_loss: 29.5451 - val_MinusLogProbMetric: 29.5451 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 775/1000
2023-09-29 19:37:32.887 
Epoch 775/1000 
	 loss: 28.9998, MinusLogProbMetric: 28.9998, val_loss: 29.5242, val_MinusLogProbMetric: 29.5242

Epoch 775: val_loss did not improve from 29.33261
196/196 - 32s - loss: 28.9998 - MinusLogProbMetric: 28.9998 - val_loss: 29.5242 - val_MinusLogProbMetric: 29.5242 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 776/1000
2023-09-29 19:38:05.324 
Epoch 776/1000 
	 loss: 28.9721, MinusLogProbMetric: 28.9721, val_loss: 29.6273, val_MinusLogProbMetric: 29.6273

Epoch 776: val_loss did not improve from 29.33261
196/196 - 32s - loss: 28.9721 - MinusLogProbMetric: 28.9721 - val_loss: 29.6273 - val_MinusLogProbMetric: 29.6273 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 777/1000
2023-09-29 19:38:39.791 
Epoch 777/1000 
	 loss: 29.2164, MinusLogProbMetric: 29.2164, val_loss: 29.3167, val_MinusLogProbMetric: 29.3167

Epoch 777: val_loss improved from 29.33261 to 29.31666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 29.2164 - MinusLogProbMetric: 29.2164 - val_loss: 29.3167 - val_MinusLogProbMetric: 29.3167 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 778/1000
2023-09-29 19:39:13.762 
Epoch 778/1000 
	 loss: 29.0080, MinusLogProbMetric: 29.0080, val_loss: 29.4433, val_MinusLogProbMetric: 29.4433

Epoch 778: val_loss did not improve from 29.31666
196/196 - 33s - loss: 29.0080 - MinusLogProbMetric: 29.0080 - val_loss: 29.4433 - val_MinusLogProbMetric: 29.4433 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 779/1000
2023-09-29 19:39:47.378 
Epoch 779/1000 
	 loss: 28.9404, MinusLogProbMetric: 28.9404, val_loss: 29.4400, val_MinusLogProbMetric: 29.4400

Epoch 779: val_loss did not improve from 29.31666
196/196 - 34s - loss: 28.9404 - MinusLogProbMetric: 28.9404 - val_loss: 29.4400 - val_MinusLogProbMetric: 29.4400 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 780/1000
2023-09-29 19:40:21.837 
Epoch 780/1000 
	 loss: 28.9436, MinusLogProbMetric: 28.9436, val_loss: 29.4622, val_MinusLogProbMetric: 29.4622

Epoch 780: val_loss did not improve from 29.31666
196/196 - 34s - loss: 28.9436 - MinusLogProbMetric: 28.9436 - val_loss: 29.4622 - val_MinusLogProbMetric: 29.4622 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 781/1000
2023-09-29 19:40:53.427 
Epoch 781/1000 
	 loss: 28.9734, MinusLogProbMetric: 28.9734, val_loss: 29.5887, val_MinusLogProbMetric: 29.5887

Epoch 781: val_loss did not improve from 29.31666
196/196 - 32s - loss: 28.9734 - MinusLogProbMetric: 28.9734 - val_loss: 29.5887 - val_MinusLogProbMetric: 29.5887 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 782/1000
2023-09-29 19:41:25.908 
Epoch 782/1000 
	 loss: 29.0149, MinusLogProbMetric: 29.0149, val_loss: 29.4801, val_MinusLogProbMetric: 29.4801

Epoch 782: val_loss did not improve from 29.31666
196/196 - 32s - loss: 29.0149 - MinusLogProbMetric: 29.0149 - val_loss: 29.4801 - val_MinusLogProbMetric: 29.4801 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 783/1000
2023-09-29 19:42:00.841 
Epoch 783/1000 
	 loss: 28.9469, MinusLogProbMetric: 28.9469, val_loss: 29.4452, val_MinusLogProbMetric: 29.4452

Epoch 783: val_loss did not improve from 29.31666
196/196 - 35s - loss: 28.9469 - MinusLogProbMetric: 28.9469 - val_loss: 29.4452 - val_MinusLogProbMetric: 29.4452 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 784/1000
2023-09-29 19:42:33.787 
Epoch 784/1000 
	 loss: 28.9379, MinusLogProbMetric: 28.9379, val_loss: 29.4450, val_MinusLogProbMetric: 29.4450

Epoch 784: val_loss did not improve from 29.31666
196/196 - 33s - loss: 28.9379 - MinusLogProbMetric: 28.9379 - val_loss: 29.4450 - val_MinusLogProbMetric: 29.4450 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 785/1000
2023-09-29 19:43:07.019 
Epoch 785/1000 
	 loss: 28.9152, MinusLogProbMetric: 28.9152, val_loss: 29.7416, val_MinusLogProbMetric: 29.7416

Epoch 785: val_loss did not improve from 29.31666
196/196 - 33s - loss: 28.9152 - MinusLogProbMetric: 28.9152 - val_loss: 29.7416 - val_MinusLogProbMetric: 29.7416 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 786/1000
2023-09-29 19:43:39.943 
Epoch 786/1000 
	 loss: 28.9248, MinusLogProbMetric: 28.9248, val_loss: 29.7583, val_MinusLogProbMetric: 29.7583

Epoch 786: val_loss did not improve from 29.31666
196/196 - 33s - loss: 28.9248 - MinusLogProbMetric: 28.9248 - val_loss: 29.7583 - val_MinusLogProbMetric: 29.7583 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 787/1000
2023-09-29 19:44:09.870 
Epoch 787/1000 
	 loss: 28.9086, MinusLogProbMetric: 28.9086, val_loss: 29.5652, val_MinusLogProbMetric: 29.5652

Epoch 787: val_loss did not improve from 29.31666
196/196 - 30s - loss: 28.9086 - MinusLogProbMetric: 28.9086 - val_loss: 29.5652 - val_MinusLogProbMetric: 29.5652 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 788/1000
2023-09-29 19:44:41.886 
Epoch 788/1000 
	 loss: 28.9712, MinusLogProbMetric: 28.9712, val_loss: 29.4602, val_MinusLogProbMetric: 29.4602

Epoch 788: val_loss did not improve from 29.31666
196/196 - 32s - loss: 28.9712 - MinusLogProbMetric: 28.9712 - val_loss: 29.4602 - val_MinusLogProbMetric: 29.4602 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 789/1000
2023-09-29 19:45:13.048 
Epoch 789/1000 
	 loss: 29.0051, MinusLogProbMetric: 29.0051, val_loss: 29.5227, val_MinusLogProbMetric: 29.5227

Epoch 789: val_loss did not improve from 29.31666
196/196 - 31s - loss: 29.0051 - MinusLogProbMetric: 29.0051 - val_loss: 29.5227 - val_MinusLogProbMetric: 29.5227 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 790/1000
2023-09-29 19:45:43.607 
Epoch 790/1000 
	 loss: 28.9764, MinusLogProbMetric: 28.9764, val_loss: 29.9273, val_MinusLogProbMetric: 29.9273

Epoch 790: val_loss did not improve from 29.31666
196/196 - 31s - loss: 28.9764 - MinusLogProbMetric: 28.9764 - val_loss: 29.9273 - val_MinusLogProbMetric: 29.9273 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 791/1000
2023-09-29 19:46:16.651 
Epoch 791/1000 
	 loss: 28.9395, MinusLogProbMetric: 28.9395, val_loss: 29.5412, val_MinusLogProbMetric: 29.5412

Epoch 791: val_loss did not improve from 29.31666
196/196 - 33s - loss: 28.9395 - MinusLogProbMetric: 28.9395 - val_loss: 29.5412 - val_MinusLogProbMetric: 29.5412 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 792/1000
2023-09-29 19:46:49.842 
Epoch 792/1000 
	 loss: 29.0109, MinusLogProbMetric: 29.0109, val_loss: 29.3730, val_MinusLogProbMetric: 29.3730

Epoch 792: val_loss did not improve from 29.31666
196/196 - 33s - loss: 29.0109 - MinusLogProbMetric: 29.0109 - val_loss: 29.3730 - val_MinusLogProbMetric: 29.3730 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 793/1000
2023-09-29 19:47:24.413 
Epoch 793/1000 
	 loss: 28.9772, MinusLogProbMetric: 28.9772, val_loss: 29.3058, val_MinusLogProbMetric: 29.3058

Epoch 793: val_loss improved from 29.31666 to 29.30585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 28.9772 - MinusLogProbMetric: 28.9772 - val_loss: 29.3058 - val_MinusLogProbMetric: 29.3058 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 794/1000
2023-09-29 19:48:00.177 
Epoch 794/1000 
	 loss: 29.0942, MinusLogProbMetric: 29.0942, val_loss: 29.5924, val_MinusLogProbMetric: 29.5924

Epoch 794: val_loss did not improve from 29.30585
196/196 - 35s - loss: 29.0942 - MinusLogProbMetric: 29.0942 - val_loss: 29.5924 - val_MinusLogProbMetric: 29.5924 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 795/1000
2023-09-29 19:48:32.171 
Epoch 795/1000 
	 loss: 28.9186, MinusLogProbMetric: 28.9186, val_loss: 29.2799, val_MinusLogProbMetric: 29.2799

Epoch 795: val_loss improved from 29.30585 to 29.27990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 33s - loss: 28.9186 - MinusLogProbMetric: 28.9186 - val_loss: 29.2799 - val_MinusLogProbMetric: 29.2799 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 796/1000
2023-09-29 19:49:05.794 
Epoch 796/1000 
	 loss: 28.9403, MinusLogProbMetric: 28.9403, val_loss: 29.3451, val_MinusLogProbMetric: 29.3451

Epoch 796: val_loss did not improve from 29.27990
196/196 - 33s - loss: 28.9403 - MinusLogProbMetric: 28.9403 - val_loss: 29.3451 - val_MinusLogProbMetric: 29.3451 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 797/1000
2023-09-29 19:49:36.858 
Epoch 797/1000 
	 loss: 28.9130, MinusLogProbMetric: 28.9130, val_loss: 29.4760, val_MinusLogProbMetric: 29.4760

Epoch 797: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9130 - MinusLogProbMetric: 28.9130 - val_loss: 29.4760 - val_MinusLogProbMetric: 29.4760 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 798/1000
2023-09-29 19:50:11.325 
Epoch 798/1000 
	 loss: 29.0432, MinusLogProbMetric: 29.0432, val_loss: 29.6122, val_MinusLogProbMetric: 29.6122

Epoch 798: val_loss did not improve from 29.27990
196/196 - 34s - loss: 29.0432 - MinusLogProbMetric: 29.0432 - val_loss: 29.6122 - val_MinusLogProbMetric: 29.6122 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 799/1000
2023-09-29 19:50:45.501 
Epoch 799/1000 
	 loss: 28.9483, MinusLogProbMetric: 28.9483, val_loss: 29.4408, val_MinusLogProbMetric: 29.4408

Epoch 799: val_loss did not improve from 29.27990
196/196 - 34s - loss: 28.9483 - MinusLogProbMetric: 28.9483 - val_loss: 29.4408 - val_MinusLogProbMetric: 29.4408 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 800/1000
2023-09-29 19:51:17.622 
Epoch 800/1000 
	 loss: 29.0337, MinusLogProbMetric: 29.0337, val_loss: 29.5928, val_MinusLogProbMetric: 29.5928

Epoch 800: val_loss did not improve from 29.27990
196/196 - 32s - loss: 29.0337 - MinusLogProbMetric: 29.0337 - val_loss: 29.5928 - val_MinusLogProbMetric: 29.5928 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 801/1000
2023-09-29 19:51:50.738 
Epoch 801/1000 
	 loss: 28.9814, MinusLogProbMetric: 28.9814, val_loss: 29.5108, val_MinusLogProbMetric: 29.5108

Epoch 801: val_loss did not improve from 29.27990
196/196 - 33s - loss: 28.9814 - MinusLogProbMetric: 28.9814 - val_loss: 29.5108 - val_MinusLogProbMetric: 29.5108 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 802/1000
2023-09-29 19:52:24.885 
Epoch 802/1000 
	 loss: 28.8935, MinusLogProbMetric: 28.8935, val_loss: 29.5416, val_MinusLogProbMetric: 29.5416

Epoch 802: val_loss did not improve from 29.27990
196/196 - 34s - loss: 28.8935 - MinusLogProbMetric: 28.8935 - val_loss: 29.5416 - val_MinusLogProbMetric: 29.5416 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 803/1000
2023-09-29 19:52:59.109 
Epoch 803/1000 
	 loss: 29.0799, MinusLogProbMetric: 29.0799, val_loss: 29.4039, val_MinusLogProbMetric: 29.4039

Epoch 803: val_loss did not improve from 29.27990
196/196 - 34s - loss: 29.0799 - MinusLogProbMetric: 29.0799 - val_loss: 29.4039 - val_MinusLogProbMetric: 29.4039 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 804/1000
2023-09-29 19:53:31.365 
Epoch 804/1000 
	 loss: 28.9958, MinusLogProbMetric: 28.9958, val_loss: 29.5097, val_MinusLogProbMetric: 29.5097

Epoch 804: val_loss did not improve from 29.27990
196/196 - 32s - loss: 28.9958 - MinusLogProbMetric: 28.9958 - val_loss: 29.5097 - val_MinusLogProbMetric: 29.5097 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 805/1000
2023-09-29 19:54:04.025 
Epoch 805/1000 
	 loss: 28.9485, MinusLogProbMetric: 28.9485, val_loss: 29.5195, val_MinusLogProbMetric: 29.5195

Epoch 805: val_loss did not improve from 29.27990
196/196 - 33s - loss: 28.9485 - MinusLogProbMetric: 28.9485 - val_loss: 29.5195 - val_MinusLogProbMetric: 29.5195 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 806/1000
2023-09-29 19:54:37.584 
Epoch 806/1000 
	 loss: 28.8937, MinusLogProbMetric: 28.8937, val_loss: 29.7198, val_MinusLogProbMetric: 29.7198

Epoch 806: val_loss did not improve from 29.27990
196/196 - 34s - loss: 28.8937 - MinusLogProbMetric: 28.8937 - val_loss: 29.7198 - val_MinusLogProbMetric: 29.7198 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 807/1000
2023-09-29 19:55:06.789 
Epoch 807/1000 
	 loss: 28.9018, MinusLogProbMetric: 28.9018, val_loss: 29.4957, val_MinusLogProbMetric: 29.4957

Epoch 807: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.9018 - MinusLogProbMetric: 28.9018 - val_loss: 29.4957 - val_MinusLogProbMetric: 29.4957 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 808/1000
2023-09-29 19:55:37.236 
Epoch 808/1000 
	 loss: 28.9283, MinusLogProbMetric: 28.9283, val_loss: 29.5616, val_MinusLogProbMetric: 29.5616

Epoch 808: val_loss did not improve from 29.27990
196/196 - 30s - loss: 28.9283 - MinusLogProbMetric: 28.9283 - val_loss: 29.5616 - val_MinusLogProbMetric: 29.5616 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 809/1000
2023-09-29 19:56:06.672 
Epoch 809/1000 
	 loss: 28.9106, MinusLogProbMetric: 28.9106, val_loss: 29.2804, val_MinusLogProbMetric: 29.2804

Epoch 809: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.9106 - MinusLogProbMetric: 28.9106 - val_loss: 29.2804 - val_MinusLogProbMetric: 29.2804 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 810/1000
2023-09-29 19:56:37.490 
Epoch 810/1000 
	 loss: 28.9014, MinusLogProbMetric: 28.9014, val_loss: 29.3042, val_MinusLogProbMetric: 29.3042

Epoch 810: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9014 - MinusLogProbMetric: 28.9014 - val_loss: 29.3042 - val_MinusLogProbMetric: 29.3042 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 811/1000
2023-09-29 19:57:08.380 
Epoch 811/1000 
	 loss: 28.9211, MinusLogProbMetric: 28.9211, val_loss: 29.3104, val_MinusLogProbMetric: 29.3104

Epoch 811: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9211 - MinusLogProbMetric: 28.9211 - val_loss: 29.3104 - val_MinusLogProbMetric: 29.3104 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 812/1000
2023-09-29 19:57:37.875 
Epoch 812/1000 
	 loss: 28.9934, MinusLogProbMetric: 28.9934, val_loss: 29.3645, val_MinusLogProbMetric: 29.3645

Epoch 812: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.9934 - MinusLogProbMetric: 28.9934 - val_loss: 29.3645 - val_MinusLogProbMetric: 29.3645 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 813/1000
2023-09-29 19:58:08.569 
Epoch 813/1000 
	 loss: 28.9294, MinusLogProbMetric: 28.9294, val_loss: 29.4199, val_MinusLogProbMetric: 29.4199

Epoch 813: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9294 - MinusLogProbMetric: 28.9294 - val_loss: 29.4199 - val_MinusLogProbMetric: 29.4199 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 814/1000
2023-09-29 19:58:38.955 
Epoch 814/1000 
	 loss: 28.9255, MinusLogProbMetric: 28.9255, val_loss: 29.3333, val_MinusLogProbMetric: 29.3333

Epoch 814: val_loss did not improve from 29.27990
196/196 - 30s - loss: 28.9255 - MinusLogProbMetric: 28.9255 - val_loss: 29.3333 - val_MinusLogProbMetric: 29.3333 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 815/1000
2023-09-29 19:59:07.433 
Epoch 815/1000 
	 loss: 28.9750, MinusLogProbMetric: 28.9750, val_loss: 30.1228, val_MinusLogProbMetric: 30.1228

Epoch 815: val_loss did not improve from 29.27990
196/196 - 28s - loss: 28.9750 - MinusLogProbMetric: 28.9750 - val_loss: 30.1228 - val_MinusLogProbMetric: 30.1228 - lr: 1.6667e-04 - 28s/epoch - 145ms/step
Epoch 816/1000
2023-09-29 19:59:37.574 
Epoch 816/1000 
	 loss: 29.0153, MinusLogProbMetric: 29.0153, val_loss: 29.2999, val_MinusLogProbMetric: 29.2999

Epoch 816: val_loss did not improve from 29.27990
196/196 - 30s - loss: 29.0153 - MinusLogProbMetric: 29.0153 - val_loss: 29.2999 - val_MinusLogProbMetric: 29.2999 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 817/1000
2023-09-29 20:00:09.052 
Epoch 817/1000 
	 loss: 28.9657, MinusLogProbMetric: 28.9657, val_loss: 29.4814, val_MinusLogProbMetric: 29.4814

Epoch 817: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9657 - MinusLogProbMetric: 28.9657 - val_loss: 29.4814 - val_MinusLogProbMetric: 29.4814 - lr: 1.6667e-04 - 31s/epoch - 161ms/step
Epoch 818/1000
2023-09-29 20:00:42.375 
Epoch 818/1000 
	 loss: 28.8934, MinusLogProbMetric: 28.8934, val_loss: 29.3164, val_MinusLogProbMetric: 29.3164

Epoch 818: val_loss did not improve from 29.27990
196/196 - 33s - loss: 28.8934 - MinusLogProbMetric: 28.8934 - val_loss: 29.3164 - val_MinusLogProbMetric: 29.3164 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 819/1000
2023-09-29 20:01:13.124 
Epoch 819/1000 
	 loss: 28.9437, MinusLogProbMetric: 28.9437, val_loss: 29.3790, val_MinusLogProbMetric: 29.3790

Epoch 819: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9437 - MinusLogProbMetric: 28.9437 - val_loss: 29.3790 - val_MinusLogProbMetric: 29.3790 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 820/1000
2023-09-29 20:01:43.527 
Epoch 820/1000 
	 loss: 28.9191, MinusLogProbMetric: 28.9191, val_loss: 29.3589, val_MinusLogProbMetric: 29.3589

Epoch 820: val_loss did not improve from 29.27990
196/196 - 30s - loss: 28.9191 - MinusLogProbMetric: 28.9191 - val_loss: 29.3589 - val_MinusLogProbMetric: 29.3589 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 821/1000
2023-09-29 20:02:14.371 
Epoch 821/1000 
	 loss: 28.8879, MinusLogProbMetric: 28.8879, val_loss: 29.5336, val_MinusLogProbMetric: 29.5336

Epoch 821: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.8879 - MinusLogProbMetric: 28.8879 - val_loss: 29.5336 - val_MinusLogProbMetric: 29.5336 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 822/1000
2023-09-29 20:02:42.647 
Epoch 822/1000 
	 loss: 28.8919, MinusLogProbMetric: 28.8919, val_loss: 29.3179, val_MinusLogProbMetric: 29.3179

Epoch 822: val_loss did not improve from 29.27990
196/196 - 28s - loss: 28.8919 - MinusLogProbMetric: 28.8919 - val_loss: 29.3179 - val_MinusLogProbMetric: 29.3179 - lr: 1.6667e-04 - 28s/epoch - 144ms/step
Epoch 823/1000
2023-09-29 20:03:14.407 
Epoch 823/1000 
	 loss: 28.9544, MinusLogProbMetric: 28.9544, val_loss: 29.7973, val_MinusLogProbMetric: 29.7973

Epoch 823: val_loss did not improve from 29.27990
196/196 - 32s - loss: 28.9544 - MinusLogProbMetric: 28.9544 - val_loss: 29.7973 - val_MinusLogProbMetric: 29.7973 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 824/1000
2023-09-29 20:03:45.004 
Epoch 824/1000 
	 loss: 28.8985, MinusLogProbMetric: 28.8985, val_loss: 29.4412, val_MinusLogProbMetric: 29.4412

Epoch 824: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.8985 - MinusLogProbMetric: 28.8985 - val_loss: 29.4412 - val_MinusLogProbMetric: 29.4412 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 825/1000
2023-09-29 20:04:15.346 
Epoch 825/1000 
	 loss: 28.9450, MinusLogProbMetric: 28.9450, val_loss: 29.2987, val_MinusLogProbMetric: 29.2987

Epoch 825: val_loss did not improve from 29.27990
196/196 - 30s - loss: 28.9450 - MinusLogProbMetric: 28.9450 - val_loss: 29.2987 - val_MinusLogProbMetric: 29.2987 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 826/1000
2023-09-29 20:04:46.233 
Epoch 826/1000 
	 loss: 28.9446, MinusLogProbMetric: 28.9446, val_loss: 29.3916, val_MinusLogProbMetric: 29.3916

Epoch 826: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9446 - MinusLogProbMetric: 28.9446 - val_loss: 29.3916 - val_MinusLogProbMetric: 29.3916 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 827/1000
2023-09-29 20:05:15.047 
Epoch 827/1000 
	 loss: 28.8620, MinusLogProbMetric: 28.8620, val_loss: 29.4464, val_MinusLogProbMetric: 29.4464

Epoch 827: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.8620 - MinusLogProbMetric: 28.8620 - val_loss: 29.4464 - val_MinusLogProbMetric: 29.4464 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 828/1000
2023-09-29 20:05:43.024 
Epoch 828/1000 
	 loss: 28.9785, MinusLogProbMetric: 28.9785, val_loss: 29.4847, val_MinusLogProbMetric: 29.4847

Epoch 828: val_loss did not improve from 29.27990
196/196 - 28s - loss: 28.9785 - MinusLogProbMetric: 28.9785 - val_loss: 29.4847 - val_MinusLogProbMetric: 29.4847 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 829/1000
2023-09-29 20:06:14.010 
Epoch 829/1000 
	 loss: 28.8694, MinusLogProbMetric: 28.8694, val_loss: 29.5779, val_MinusLogProbMetric: 29.5779

Epoch 829: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.8694 - MinusLogProbMetric: 28.8694 - val_loss: 29.5779 - val_MinusLogProbMetric: 29.5779 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 830/1000
2023-09-29 20:06:47.945 
Epoch 830/1000 
	 loss: 28.9194, MinusLogProbMetric: 28.9194, val_loss: 29.5131, val_MinusLogProbMetric: 29.5131

Epoch 830: val_loss did not improve from 29.27990
196/196 - 34s - loss: 28.9194 - MinusLogProbMetric: 28.9194 - val_loss: 29.5131 - val_MinusLogProbMetric: 29.5131 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 831/1000
2023-09-29 20:07:19.179 
Epoch 831/1000 
	 loss: 28.9356, MinusLogProbMetric: 28.9356, val_loss: 29.3227, val_MinusLogProbMetric: 29.3227

Epoch 831: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.9356 - MinusLogProbMetric: 28.9356 - val_loss: 29.3227 - val_MinusLogProbMetric: 29.3227 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 832/1000
2023-09-29 20:07:49.190 
Epoch 832/1000 
	 loss: 28.8693, MinusLogProbMetric: 28.8693, val_loss: 29.3605, val_MinusLogProbMetric: 29.3605

Epoch 832: val_loss did not improve from 29.27990
196/196 - 30s - loss: 28.8693 - MinusLogProbMetric: 28.8693 - val_loss: 29.3605 - val_MinusLogProbMetric: 29.3605 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 833/1000
2023-09-29 20:08:18.785 
Epoch 833/1000 
	 loss: 28.8751, MinusLogProbMetric: 28.8751, val_loss: 31.1377, val_MinusLogProbMetric: 31.1377

Epoch 833: val_loss did not improve from 29.27990
196/196 - 30s - loss: 28.8751 - MinusLogProbMetric: 28.8751 - val_loss: 31.1377 - val_MinusLogProbMetric: 31.1377 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 834/1000
2023-09-29 20:08:46.656 
Epoch 834/1000 
	 loss: 29.1852, MinusLogProbMetric: 29.1852, val_loss: 29.5812, val_MinusLogProbMetric: 29.5812

Epoch 834: val_loss did not improve from 29.27990
196/196 - 28s - loss: 29.1852 - MinusLogProbMetric: 29.1852 - val_loss: 29.5812 - val_MinusLogProbMetric: 29.5812 - lr: 1.6667e-04 - 28s/epoch - 142ms/step
Epoch 835/1000
2023-09-29 20:09:17.846 
Epoch 835/1000 
	 loss: 28.8888, MinusLogProbMetric: 28.8888, val_loss: 29.4722, val_MinusLogProbMetric: 29.4722

Epoch 835: val_loss did not improve from 29.27990
196/196 - 31s - loss: 28.8888 - MinusLogProbMetric: 28.8888 - val_loss: 29.4722 - val_MinusLogProbMetric: 29.4722 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 836/1000
2023-09-29 20:09:45.135 
Epoch 836/1000 
	 loss: 28.9562, MinusLogProbMetric: 28.9562, val_loss: 29.3999, val_MinusLogProbMetric: 29.3999

Epoch 836: val_loss did not improve from 29.27990
196/196 - 27s - loss: 28.9562 - MinusLogProbMetric: 28.9562 - val_loss: 29.3999 - val_MinusLogProbMetric: 29.3999 - lr: 1.6667e-04 - 27s/epoch - 139ms/step
Epoch 837/1000
2023-09-29 20:10:14.064 
Epoch 837/1000 
	 loss: 28.8612, MinusLogProbMetric: 28.8612, val_loss: 29.4762, val_MinusLogProbMetric: 29.4762

Epoch 837: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.8612 - MinusLogProbMetric: 28.8612 - val_loss: 29.4762 - val_MinusLogProbMetric: 29.4762 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 838/1000
2023-09-29 20:10:43.248 
Epoch 838/1000 
	 loss: 28.9498, MinusLogProbMetric: 28.9498, val_loss: 29.6403, val_MinusLogProbMetric: 29.6403

Epoch 838: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.9498 - MinusLogProbMetric: 28.9498 - val_loss: 29.6403 - val_MinusLogProbMetric: 29.6403 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 839/1000
2023-09-29 20:11:12.641 
Epoch 839/1000 
	 loss: 28.9459, MinusLogProbMetric: 28.9459, val_loss: 29.5336, val_MinusLogProbMetric: 29.5336

Epoch 839: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.9459 - MinusLogProbMetric: 28.9459 - val_loss: 29.5336 - val_MinusLogProbMetric: 29.5336 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 840/1000
2023-09-29 20:11:41.152 
Epoch 840/1000 
	 loss: 28.8519, MinusLogProbMetric: 28.8519, val_loss: 29.4173, val_MinusLogProbMetric: 29.4173

Epoch 840: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.8519 - MinusLogProbMetric: 28.8519 - val_loss: 29.4173 - val_MinusLogProbMetric: 29.4173 - lr: 1.6667e-04 - 29s/epoch - 145ms/step
Epoch 841/1000
2023-09-29 20:12:11.315 
Epoch 841/1000 
	 loss: 28.9120, MinusLogProbMetric: 28.9120, val_loss: 29.3187, val_MinusLogProbMetric: 29.3187

Epoch 841: val_loss did not improve from 29.27990
196/196 - 30s - loss: 28.9120 - MinusLogProbMetric: 28.9120 - val_loss: 29.3187 - val_MinusLogProbMetric: 29.3187 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 842/1000
2023-09-29 20:12:40.646 
Epoch 842/1000 
	 loss: 28.9781, MinusLogProbMetric: 28.9781, val_loss: 29.5955, val_MinusLogProbMetric: 29.5955

Epoch 842: val_loss did not improve from 29.27990
196/196 - 29s - loss: 28.9781 - MinusLogProbMetric: 28.9781 - val_loss: 29.5955 - val_MinusLogProbMetric: 29.5955 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 843/1000
2023-09-29 20:13:14.485 
Epoch 843/1000 
	 loss: 28.8545, MinusLogProbMetric: 28.8545, val_loss: 29.2032, val_MinusLogProbMetric: 29.2032

Epoch 843: val_loss improved from 29.27990 to 29.20317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 34s - loss: 28.8545 - MinusLogProbMetric: 28.8545 - val_loss: 29.2032 - val_MinusLogProbMetric: 29.2032 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 844/1000
2023-09-29 20:13:46.422 
Epoch 844/1000 
	 loss: 28.8950, MinusLogProbMetric: 28.8950, val_loss: 29.2165, val_MinusLogProbMetric: 29.2165

Epoch 844: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8950 - MinusLogProbMetric: 28.8950 - val_loss: 29.2165 - val_MinusLogProbMetric: 29.2165 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 845/1000
2023-09-29 20:14:17.042 
Epoch 845/1000 
	 loss: 28.9172, MinusLogProbMetric: 28.9172, val_loss: 29.5070, val_MinusLogProbMetric: 29.5070

Epoch 845: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.9172 - MinusLogProbMetric: 28.9172 - val_loss: 29.5070 - val_MinusLogProbMetric: 29.5070 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 846/1000
2023-09-29 20:14:47.242 
Epoch 846/1000 
	 loss: 28.9020, MinusLogProbMetric: 28.9020, val_loss: 29.2887, val_MinusLogProbMetric: 29.2887

Epoch 846: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.9020 - MinusLogProbMetric: 28.9020 - val_loss: 29.2887 - val_MinusLogProbMetric: 29.2887 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 847/1000
2023-09-29 20:15:18.880 
Epoch 847/1000 
	 loss: 28.8699, MinusLogProbMetric: 28.8699, val_loss: 29.4101, val_MinusLogProbMetric: 29.4101

Epoch 847: val_loss did not improve from 29.20317
196/196 - 32s - loss: 28.8699 - MinusLogProbMetric: 28.8699 - val_loss: 29.4101 - val_MinusLogProbMetric: 29.4101 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 848/1000
2023-09-29 20:15:48.787 
Epoch 848/1000 
	 loss: 28.8785, MinusLogProbMetric: 28.8785, val_loss: 29.2341, val_MinusLogProbMetric: 29.2341

Epoch 848: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8785 - MinusLogProbMetric: 28.8785 - val_loss: 29.2341 - val_MinusLogProbMetric: 29.2341 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 849/1000
2023-09-29 20:16:20.034 
Epoch 849/1000 
	 loss: 28.8876, MinusLogProbMetric: 28.8876, val_loss: 29.2487, val_MinusLogProbMetric: 29.2487

Epoch 849: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8876 - MinusLogProbMetric: 28.8876 - val_loss: 29.2487 - val_MinusLogProbMetric: 29.2487 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 850/1000
2023-09-29 20:16:48.645 
Epoch 850/1000 
	 loss: 28.8512, MinusLogProbMetric: 28.8512, val_loss: 29.4582, val_MinusLogProbMetric: 29.4582

Epoch 850: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.8512 - MinusLogProbMetric: 28.8512 - val_loss: 29.4582 - val_MinusLogProbMetric: 29.4582 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 851/1000
2023-09-29 20:17:17.306 
Epoch 851/1000 
	 loss: 28.8564, MinusLogProbMetric: 28.8564, val_loss: 29.6568, val_MinusLogProbMetric: 29.6568

Epoch 851: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.8564 - MinusLogProbMetric: 28.8564 - val_loss: 29.6568 - val_MinusLogProbMetric: 29.6568 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 852/1000
2023-09-29 20:17:46.271 
Epoch 852/1000 
	 loss: 28.9174, MinusLogProbMetric: 28.9174, val_loss: 29.4508, val_MinusLogProbMetric: 29.4508

Epoch 852: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.9174 - MinusLogProbMetric: 28.9174 - val_loss: 29.4508 - val_MinusLogProbMetric: 29.4508 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 853/1000
2023-09-29 20:18:15.075 
Epoch 853/1000 
	 loss: 28.9174, MinusLogProbMetric: 28.9174, val_loss: 29.2041, val_MinusLogProbMetric: 29.2041

Epoch 853: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.9174 - MinusLogProbMetric: 28.9174 - val_loss: 29.2041 - val_MinusLogProbMetric: 29.2041 - lr: 1.6667e-04 - 29s/epoch - 147ms/step
Epoch 854/1000
2023-09-29 20:18:44.255 
Epoch 854/1000 
	 loss: 28.9860, MinusLogProbMetric: 28.9860, val_loss: 29.3162, val_MinusLogProbMetric: 29.3162

Epoch 854: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.9860 - MinusLogProbMetric: 28.9860 - val_loss: 29.3162 - val_MinusLogProbMetric: 29.3162 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 855/1000
2023-09-29 20:19:15.777 
Epoch 855/1000 
	 loss: 28.8201, MinusLogProbMetric: 28.8201, val_loss: 29.2902, val_MinusLogProbMetric: 29.2902

Epoch 855: val_loss did not improve from 29.20317
196/196 - 32s - loss: 28.8201 - MinusLogProbMetric: 28.8201 - val_loss: 29.2902 - val_MinusLogProbMetric: 29.2902 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 856/1000
2023-09-29 20:19:45.975 
Epoch 856/1000 
	 loss: 28.8673, MinusLogProbMetric: 28.8673, val_loss: 29.3127, val_MinusLogProbMetric: 29.3127

Epoch 856: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8673 - MinusLogProbMetric: 28.8673 - val_loss: 29.3127 - val_MinusLogProbMetric: 29.3127 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 857/1000
2023-09-29 20:20:18.570 
Epoch 857/1000 
	 loss: 28.8773, MinusLogProbMetric: 28.8773, val_loss: 30.3321, val_MinusLogProbMetric: 30.3321

Epoch 857: val_loss did not improve from 29.20317
196/196 - 33s - loss: 28.8773 - MinusLogProbMetric: 28.8773 - val_loss: 30.3321 - val_MinusLogProbMetric: 30.3321 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 858/1000
2023-09-29 20:20:48.755 
Epoch 858/1000 
	 loss: 28.8692, MinusLogProbMetric: 28.8692, val_loss: 29.3068, val_MinusLogProbMetric: 29.3068

Epoch 858: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8692 - MinusLogProbMetric: 28.8692 - val_loss: 29.3068 - val_MinusLogProbMetric: 29.3068 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 859/1000
2023-09-29 20:21:19.735 
Epoch 859/1000 
	 loss: 28.8324, MinusLogProbMetric: 28.8324, val_loss: 29.5361, val_MinusLogProbMetric: 29.5361

Epoch 859: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8324 - MinusLogProbMetric: 28.8324 - val_loss: 29.5361 - val_MinusLogProbMetric: 29.5361 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 860/1000
2023-09-29 20:21:52.819 
Epoch 860/1000 
	 loss: 28.9183, MinusLogProbMetric: 28.9183, val_loss: 29.6153, val_MinusLogProbMetric: 29.6153

Epoch 860: val_loss did not improve from 29.20317
196/196 - 33s - loss: 28.9183 - MinusLogProbMetric: 28.9183 - val_loss: 29.6153 - val_MinusLogProbMetric: 29.6153 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 861/1000
2023-09-29 20:22:21.827 
Epoch 861/1000 
	 loss: 28.7938, MinusLogProbMetric: 28.7938, val_loss: 29.4251, val_MinusLogProbMetric: 29.4251

Epoch 861: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.7938 - MinusLogProbMetric: 28.7938 - val_loss: 29.4251 - val_MinusLogProbMetric: 29.4251 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 862/1000
2023-09-29 20:22:51.940 
Epoch 862/1000 
	 loss: 28.8915, MinusLogProbMetric: 28.8915, val_loss: 29.2765, val_MinusLogProbMetric: 29.2765

Epoch 862: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8915 - MinusLogProbMetric: 28.8915 - val_loss: 29.2765 - val_MinusLogProbMetric: 29.2765 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 863/1000
2023-09-29 20:23:20.243 
Epoch 863/1000 
	 loss: 28.8055, MinusLogProbMetric: 28.8055, val_loss: 29.5354, val_MinusLogProbMetric: 29.5354

Epoch 863: val_loss did not improve from 29.20317
196/196 - 28s - loss: 28.8055 - MinusLogProbMetric: 28.8055 - val_loss: 29.5354 - val_MinusLogProbMetric: 29.5354 - lr: 1.6667e-04 - 28s/epoch - 144ms/step
Epoch 864/1000
2023-09-29 20:23:49.472 
Epoch 864/1000 
	 loss: 28.8885, MinusLogProbMetric: 28.8885, val_loss: 29.2913, val_MinusLogProbMetric: 29.2913

Epoch 864: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.8885 - MinusLogProbMetric: 28.8885 - val_loss: 29.2913 - val_MinusLogProbMetric: 29.2913 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 865/1000
2023-09-29 20:24:20.182 
Epoch 865/1000 
	 loss: 28.8346, MinusLogProbMetric: 28.8346, val_loss: 29.5042, val_MinusLogProbMetric: 29.5042

Epoch 865: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8346 - MinusLogProbMetric: 28.8346 - val_loss: 29.5042 - val_MinusLogProbMetric: 29.5042 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 866/1000
2023-09-29 20:24:49.238 
Epoch 866/1000 
	 loss: 28.8270, MinusLogProbMetric: 28.8270, val_loss: 29.4381, val_MinusLogProbMetric: 29.4381

Epoch 866: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.8270 - MinusLogProbMetric: 28.8270 - val_loss: 29.4381 - val_MinusLogProbMetric: 29.4381 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 867/1000
2023-09-29 20:25:21.551 
Epoch 867/1000 
	 loss: 28.8620, MinusLogProbMetric: 28.8620, val_loss: 29.3059, val_MinusLogProbMetric: 29.3059

Epoch 867: val_loss did not improve from 29.20317
196/196 - 32s - loss: 28.8620 - MinusLogProbMetric: 28.8620 - val_loss: 29.3059 - val_MinusLogProbMetric: 29.3059 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 868/1000
2023-09-29 20:25:53.313 
Epoch 868/1000 
	 loss: 28.8267, MinusLogProbMetric: 28.8267, val_loss: 29.6360, val_MinusLogProbMetric: 29.6360

Epoch 868: val_loss did not improve from 29.20317
196/196 - 32s - loss: 28.8267 - MinusLogProbMetric: 28.8267 - val_loss: 29.6360 - val_MinusLogProbMetric: 29.6360 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 869/1000
2023-09-29 20:26:21.267 
Epoch 869/1000 
	 loss: 28.8495, MinusLogProbMetric: 28.8495, val_loss: 29.2604, val_MinusLogProbMetric: 29.2604

Epoch 869: val_loss did not improve from 29.20317
196/196 - 28s - loss: 28.8495 - MinusLogProbMetric: 28.8495 - val_loss: 29.2604 - val_MinusLogProbMetric: 29.2604 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 870/1000
2023-09-29 20:26:52.558 
Epoch 870/1000 
	 loss: 28.8608, MinusLogProbMetric: 28.8608, val_loss: 30.1847, val_MinusLogProbMetric: 30.1847

Epoch 870: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8608 - MinusLogProbMetric: 28.8608 - val_loss: 30.1847 - val_MinusLogProbMetric: 30.1847 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 871/1000
2023-09-29 20:27:23.376 
Epoch 871/1000 
	 loss: 28.9199, MinusLogProbMetric: 28.9199, val_loss: 29.3844, val_MinusLogProbMetric: 29.3844

Epoch 871: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.9199 - MinusLogProbMetric: 28.9199 - val_loss: 29.3844 - val_MinusLogProbMetric: 29.3844 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 872/1000
2023-09-29 20:27:55.177 
Epoch 872/1000 
	 loss: 28.8741, MinusLogProbMetric: 28.8741, val_loss: 29.3440, val_MinusLogProbMetric: 29.3440

Epoch 872: val_loss did not improve from 29.20317
196/196 - 32s - loss: 28.8741 - MinusLogProbMetric: 28.8741 - val_loss: 29.3440 - val_MinusLogProbMetric: 29.3440 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 873/1000
2023-09-29 20:28:24.324 
Epoch 873/1000 
	 loss: 28.8004, MinusLogProbMetric: 28.8004, val_loss: 29.3805, val_MinusLogProbMetric: 29.3805

Epoch 873: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.8004 - MinusLogProbMetric: 28.8004 - val_loss: 29.3805 - val_MinusLogProbMetric: 29.3805 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 874/1000
2023-09-29 20:28:52.985 
Epoch 874/1000 
	 loss: 28.9619, MinusLogProbMetric: 28.9619, val_loss: 29.3745, val_MinusLogProbMetric: 29.3745

Epoch 874: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.9619 - MinusLogProbMetric: 28.9619 - val_loss: 29.3745 - val_MinusLogProbMetric: 29.3745 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 875/1000
2023-09-29 20:29:25.531 
Epoch 875/1000 
	 loss: 28.8239, MinusLogProbMetric: 28.8239, val_loss: 29.4108, val_MinusLogProbMetric: 29.4108

Epoch 875: val_loss did not improve from 29.20317
196/196 - 33s - loss: 28.8239 - MinusLogProbMetric: 28.8239 - val_loss: 29.4108 - val_MinusLogProbMetric: 29.4108 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 876/1000
2023-09-29 20:29:55.557 
Epoch 876/1000 
	 loss: 28.8126, MinusLogProbMetric: 28.8126, val_loss: 29.3353, val_MinusLogProbMetric: 29.3353

Epoch 876: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8126 - MinusLogProbMetric: 28.8126 - val_loss: 29.3353 - val_MinusLogProbMetric: 29.3353 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 877/1000
2023-09-29 20:30:26.464 
Epoch 877/1000 
	 loss: 28.8702, MinusLogProbMetric: 28.8702, val_loss: 29.3695, val_MinusLogProbMetric: 29.3695

Epoch 877: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8702 - MinusLogProbMetric: 28.8702 - val_loss: 29.3695 - val_MinusLogProbMetric: 29.3695 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 878/1000
2023-09-29 20:30:55.629 
Epoch 878/1000 
	 loss: 28.8271, MinusLogProbMetric: 28.8271, val_loss: 29.3864, val_MinusLogProbMetric: 29.3864

Epoch 878: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.8271 - MinusLogProbMetric: 28.8271 - val_loss: 29.3864 - val_MinusLogProbMetric: 29.3864 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 879/1000
2023-09-29 20:31:27.553 
Epoch 879/1000 
	 loss: 28.8730, MinusLogProbMetric: 28.8730, val_loss: 29.4156, val_MinusLogProbMetric: 29.4156

Epoch 879: val_loss did not improve from 29.20317
196/196 - 32s - loss: 28.8730 - MinusLogProbMetric: 28.8730 - val_loss: 29.4156 - val_MinusLogProbMetric: 29.4156 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 880/1000
2023-09-29 20:32:00.471 
Epoch 880/1000 
	 loss: 28.8565, MinusLogProbMetric: 28.8565, val_loss: 29.2131, val_MinusLogProbMetric: 29.2131

Epoch 880: val_loss did not improve from 29.20317
196/196 - 33s - loss: 28.8565 - MinusLogProbMetric: 28.8565 - val_loss: 29.2131 - val_MinusLogProbMetric: 29.2131 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 881/1000
2023-09-29 20:32:31.753 
Epoch 881/1000 
	 loss: 28.8836, MinusLogProbMetric: 28.8836, val_loss: 29.3094, val_MinusLogProbMetric: 29.3094

Epoch 881: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8836 - MinusLogProbMetric: 28.8836 - val_loss: 29.3094 - val_MinusLogProbMetric: 29.3094 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 882/1000
2023-09-29 20:33:01.950 
Epoch 882/1000 
	 loss: 28.8765, MinusLogProbMetric: 28.8765, val_loss: 29.4411, val_MinusLogProbMetric: 29.4411

Epoch 882: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8765 - MinusLogProbMetric: 28.8765 - val_loss: 29.4411 - val_MinusLogProbMetric: 29.4411 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 883/1000
2023-09-29 20:33:30.540 
Epoch 883/1000 
	 loss: 28.9969, MinusLogProbMetric: 28.9969, val_loss: 29.3510, val_MinusLogProbMetric: 29.3510

Epoch 883: val_loss did not improve from 29.20317
196/196 - 29s - loss: 28.9969 - MinusLogProbMetric: 28.9969 - val_loss: 29.3510 - val_MinusLogProbMetric: 29.3510 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 884/1000
2023-09-29 20:34:01.599 
Epoch 884/1000 
	 loss: 28.8120, MinusLogProbMetric: 28.8120, val_loss: 29.4898, val_MinusLogProbMetric: 29.4898

Epoch 884: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8120 - MinusLogProbMetric: 28.8120 - val_loss: 29.4898 - val_MinusLogProbMetric: 29.4898 - lr: 1.6667e-04 - 31s/epoch - 158ms/step
Epoch 885/1000
2023-09-29 20:34:32.698 
Epoch 885/1000 
	 loss: 28.8843, MinusLogProbMetric: 28.8843, val_loss: 29.3780, val_MinusLogProbMetric: 29.3780

Epoch 885: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8843 - MinusLogProbMetric: 28.8843 - val_loss: 29.3780 - val_MinusLogProbMetric: 29.3780 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 886/1000
2023-09-29 20:35:02.756 
Epoch 886/1000 
	 loss: 28.8802, MinusLogProbMetric: 28.8802, val_loss: 29.8783, val_MinusLogProbMetric: 29.8783

Epoch 886: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8802 - MinusLogProbMetric: 28.8802 - val_loss: 29.8783 - val_MinusLogProbMetric: 29.8783 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 887/1000
2023-09-29 20:35:32.606 
Epoch 887/1000 
	 loss: 28.8338, MinusLogProbMetric: 28.8338, val_loss: 29.4036, val_MinusLogProbMetric: 29.4036

Epoch 887: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8338 - MinusLogProbMetric: 28.8338 - val_loss: 29.4036 - val_MinusLogProbMetric: 29.4036 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 888/1000
2023-09-29 20:36:02.766 
Epoch 888/1000 
	 loss: 28.8373, MinusLogProbMetric: 28.8373, val_loss: 29.3844, val_MinusLogProbMetric: 29.3844

Epoch 888: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8373 - MinusLogProbMetric: 28.8373 - val_loss: 29.3844 - val_MinusLogProbMetric: 29.3844 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 889/1000
2023-09-29 20:36:32.457 
Epoch 889/1000 
	 loss: 28.8199, MinusLogProbMetric: 28.8199, val_loss: 29.2731, val_MinusLogProbMetric: 29.2731

Epoch 889: val_loss did not improve from 29.20317
196/196 - 30s - loss: 28.8199 - MinusLogProbMetric: 28.8199 - val_loss: 29.2731 - val_MinusLogProbMetric: 29.2731 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 890/1000
2023-09-29 20:37:03.694 
Epoch 890/1000 
	 loss: 28.9256, MinusLogProbMetric: 28.9256, val_loss: 29.3578, val_MinusLogProbMetric: 29.3578

Epoch 890: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.9256 - MinusLogProbMetric: 28.9256 - val_loss: 29.3578 - val_MinusLogProbMetric: 29.3578 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 891/1000
2023-09-29 20:37:35.101 
Epoch 891/1000 
	 loss: 28.8496, MinusLogProbMetric: 28.8496, val_loss: 29.2431, val_MinusLogProbMetric: 29.2431

Epoch 891: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.8496 - MinusLogProbMetric: 28.8496 - val_loss: 29.2431 - val_MinusLogProbMetric: 29.2431 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 892/1000
2023-09-29 20:38:06.321 
Epoch 892/1000 
	 loss: 28.7902, MinusLogProbMetric: 28.7902, val_loss: 29.3387, val_MinusLogProbMetric: 29.3387

Epoch 892: val_loss did not improve from 29.20317
196/196 - 31s - loss: 28.7902 - MinusLogProbMetric: 28.7902 - val_loss: 29.3387 - val_MinusLogProbMetric: 29.3387 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 893/1000
2023-09-29 20:38:38.096 
Epoch 893/1000 
	 loss: 28.8414, MinusLogProbMetric: 28.8414, val_loss: 29.2848, val_MinusLogProbMetric: 29.2848

Epoch 893: val_loss did not improve from 29.20317
196/196 - 32s - loss: 28.8414 - MinusLogProbMetric: 28.8414 - val_loss: 29.2848 - val_MinusLogProbMetric: 29.2848 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 894/1000
2023-09-29 20:39:09.330 
Epoch 894/1000 
	 loss: 28.5995, MinusLogProbMetric: 28.5995, val_loss: 29.1281, val_MinusLogProbMetric: 29.1281

Epoch 894: val_loss improved from 29.20317 to 29.12807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 32s - loss: 28.5995 - MinusLogProbMetric: 28.5995 - val_loss: 29.1281 - val_MinusLogProbMetric: 29.1281 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 895/1000
2023-09-29 20:39:40.777 
Epoch 895/1000 
	 loss: 28.5850, MinusLogProbMetric: 28.5850, val_loss: 29.1432, val_MinusLogProbMetric: 29.1432

Epoch 895: val_loss did not improve from 29.12807
196/196 - 31s - loss: 28.5850 - MinusLogProbMetric: 28.5850 - val_loss: 29.1432 - val_MinusLogProbMetric: 29.1432 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 896/1000
2023-09-29 20:40:12.982 
Epoch 896/1000 
	 loss: 28.5905, MinusLogProbMetric: 28.5905, val_loss: 29.1372, val_MinusLogProbMetric: 29.1372

Epoch 896: val_loss did not improve from 29.12807
196/196 - 32s - loss: 28.5905 - MinusLogProbMetric: 28.5905 - val_loss: 29.1372 - val_MinusLogProbMetric: 29.1372 - lr: 8.3333e-05 - 32s/epoch - 164ms/step
Epoch 897/1000
2023-09-29 20:40:42.798 
Epoch 897/1000 
	 loss: 28.6020, MinusLogProbMetric: 28.6020, val_loss: 29.1301, val_MinusLogProbMetric: 29.1301

Epoch 897: val_loss did not improve from 29.12807
196/196 - 30s - loss: 28.6020 - MinusLogProbMetric: 28.6020 - val_loss: 29.1301 - val_MinusLogProbMetric: 29.1301 - lr: 8.3333e-05 - 30s/epoch - 152ms/step
Epoch 898/1000
2023-09-29 20:41:15.191 
Epoch 898/1000 
	 loss: 28.5769, MinusLogProbMetric: 28.5769, val_loss: 29.2102, val_MinusLogProbMetric: 29.2102

Epoch 898: val_loss did not improve from 29.12807
196/196 - 32s - loss: 28.5769 - MinusLogProbMetric: 28.5769 - val_loss: 29.2102 - val_MinusLogProbMetric: 29.2102 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 899/1000
2023-09-29 20:41:49.265 
Epoch 899/1000 
	 loss: 28.5723, MinusLogProbMetric: 28.5723, val_loss: 29.2084, val_MinusLogProbMetric: 29.2084

Epoch 899: val_loss did not improve from 29.12807
196/196 - 34s - loss: 28.5723 - MinusLogProbMetric: 28.5723 - val_loss: 29.2084 - val_MinusLogProbMetric: 29.2084 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 900/1000
2023-09-29 20:42:20.716 
Epoch 900/1000 
	 loss: 28.5893, MinusLogProbMetric: 28.5893, val_loss: 29.1757, val_MinusLogProbMetric: 29.1757

Epoch 900: val_loss did not improve from 29.12807
196/196 - 31s - loss: 28.5893 - MinusLogProbMetric: 28.5893 - val_loss: 29.1757 - val_MinusLogProbMetric: 29.1757 - lr: 8.3333e-05 - 31s/epoch - 160ms/step
Epoch 901/1000
2023-09-29 20:42:52.470 
Epoch 901/1000 
	 loss: 28.5817, MinusLogProbMetric: 28.5817, val_loss: 29.0749, val_MinusLogProbMetric: 29.0749

Epoch 901: val_loss improved from 29.12807 to 29.07488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 33s - loss: 28.5817 - MinusLogProbMetric: 28.5817 - val_loss: 29.0749 - val_MinusLogProbMetric: 29.0749 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 902/1000
2023-09-29 20:43:22.706 
Epoch 902/1000 
	 loss: 28.6003, MinusLogProbMetric: 28.6003, val_loss: 29.1512, val_MinusLogProbMetric: 29.1512

Epoch 902: val_loss did not improve from 29.07488
196/196 - 29s - loss: 28.6003 - MinusLogProbMetric: 28.6003 - val_loss: 29.1512 - val_MinusLogProbMetric: 29.1512 - lr: 8.3333e-05 - 29s/epoch - 150ms/step
Epoch 903/1000
2023-09-29 20:43:54.155 
Epoch 903/1000 
	 loss: 28.5713, MinusLogProbMetric: 28.5713, val_loss: 29.1734, val_MinusLogProbMetric: 29.1734

Epoch 903: val_loss did not improve from 29.07488
196/196 - 31s - loss: 28.5713 - MinusLogProbMetric: 28.5713 - val_loss: 29.1734 - val_MinusLogProbMetric: 29.1734 - lr: 8.3333e-05 - 31s/epoch - 160ms/step
Epoch 904/1000
2023-09-29 20:44:24.328 
Epoch 904/1000 
	 loss: 28.5886, MinusLogProbMetric: 28.5886, val_loss: 29.1461, val_MinusLogProbMetric: 29.1461

Epoch 904: val_loss did not improve from 29.07488
196/196 - 30s - loss: 28.5886 - MinusLogProbMetric: 28.5886 - val_loss: 29.1461 - val_MinusLogProbMetric: 29.1461 - lr: 8.3333e-05 - 30s/epoch - 154ms/step
Epoch 905/1000
2023-09-29 20:44:54.595 
Epoch 905/1000 
	 loss: 28.5997, MinusLogProbMetric: 28.5997, val_loss: 29.1110, val_MinusLogProbMetric: 29.1110

Epoch 905: val_loss did not improve from 29.07488
196/196 - 30s - loss: 28.5997 - MinusLogProbMetric: 28.5997 - val_loss: 29.1110 - val_MinusLogProbMetric: 29.1110 - lr: 8.3333e-05 - 30s/epoch - 154ms/step
Epoch 906/1000
2023-09-29 20:45:24.499 
Epoch 906/1000 
	 loss: 28.5718, MinusLogProbMetric: 28.5718, val_loss: 29.2352, val_MinusLogProbMetric: 29.2352

Epoch 906: val_loss did not improve from 29.07488
196/196 - 30s - loss: 28.5718 - MinusLogProbMetric: 28.5718 - val_loss: 29.2352 - val_MinusLogProbMetric: 29.2352 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 907/1000
2023-09-29 20:45:55.733 
Epoch 907/1000 
	 loss: 28.5980, MinusLogProbMetric: 28.5980, val_loss: 29.0818, val_MinusLogProbMetric: 29.0818

Epoch 907: val_loss did not improve from 29.07488
196/196 - 31s - loss: 28.5980 - MinusLogProbMetric: 28.5980 - val_loss: 29.0818 - val_MinusLogProbMetric: 29.0818 - lr: 8.3333e-05 - 31s/epoch - 159ms/step
Epoch 908/1000
2023-09-29 20:46:25.800 
Epoch 908/1000 
	 loss: 28.5832, MinusLogProbMetric: 28.5832, val_loss: 29.3484, val_MinusLogProbMetric: 29.3484

Epoch 908: val_loss did not improve from 29.07488
196/196 - 30s - loss: 28.5832 - MinusLogProbMetric: 28.5832 - val_loss: 29.3484 - val_MinusLogProbMetric: 29.3484 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 909/1000
2023-09-29 20:46:58.061 
Epoch 909/1000 
	 loss: 28.5952, MinusLogProbMetric: 28.5952, val_loss: 29.1352, val_MinusLogProbMetric: 29.1352

Epoch 909: val_loss did not improve from 29.07488
196/196 - 32s - loss: 28.5952 - MinusLogProbMetric: 28.5952 - val_loss: 29.1352 - val_MinusLogProbMetric: 29.1352 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 910/1000
2023-09-29 20:47:30.386 
Epoch 910/1000 
	 loss: 28.5733, MinusLogProbMetric: 28.5733, val_loss: 29.1407, val_MinusLogProbMetric: 29.1407

Epoch 910: val_loss did not improve from 29.07488
196/196 - 32s - loss: 28.5733 - MinusLogProbMetric: 28.5733 - val_loss: 29.1407 - val_MinusLogProbMetric: 29.1407 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 911/1000
2023-09-29 20:48:03.366 
Epoch 911/1000 
	 loss: 28.5833, MinusLogProbMetric: 28.5833, val_loss: 29.1436, val_MinusLogProbMetric: 29.1436

Epoch 911: val_loss did not improve from 29.07488
196/196 - 33s - loss: 28.5833 - MinusLogProbMetric: 28.5833 - val_loss: 29.1436 - val_MinusLogProbMetric: 29.1436 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 912/1000
2023-09-29 20:48:34.169 
Epoch 912/1000 
	 loss: 28.5928, MinusLogProbMetric: 28.5928, val_loss: 29.0891, val_MinusLogProbMetric: 29.0891

Epoch 912: val_loss did not improve from 29.07488
196/196 - 31s - loss: 28.5928 - MinusLogProbMetric: 28.5928 - val_loss: 29.0891 - val_MinusLogProbMetric: 29.0891 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 913/1000
2023-09-29 20:49:04.939 
Epoch 913/1000 
	 loss: 28.5588, MinusLogProbMetric: 28.5588, val_loss: 29.1649, val_MinusLogProbMetric: 29.1649

Epoch 913: val_loss did not improve from 29.07488
196/196 - 31s - loss: 28.5588 - MinusLogProbMetric: 28.5588 - val_loss: 29.1649 - val_MinusLogProbMetric: 29.1649 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 914/1000
2023-09-29 20:49:36.598 
Epoch 914/1000 
	 loss: 28.5994, MinusLogProbMetric: 28.5994, val_loss: 29.2541, val_MinusLogProbMetric: 29.2541

Epoch 914: val_loss did not improve from 29.07488
196/196 - 32s - loss: 28.5994 - MinusLogProbMetric: 28.5994 - val_loss: 29.2541 - val_MinusLogProbMetric: 29.2541 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 915/1000
2023-09-29 20:50:08.856 
Epoch 915/1000 
	 loss: 28.5889, MinusLogProbMetric: 28.5889, val_loss: 29.1997, val_MinusLogProbMetric: 29.1997

Epoch 915: val_loss did not improve from 29.07488
196/196 - 32s - loss: 28.5889 - MinusLogProbMetric: 28.5889 - val_loss: 29.1997 - val_MinusLogProbMetric: 29.1997 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 916/1000
2023-09-29 20:50:40.106 
Epoch 916/1000 
	 loss: 28.5662, MinusLogProbMetric: 28.5662, val_loss: 29.1680, val_MinusLogProbMetric: 29.1680

Epoch 916: val_loss did not improve from 29.07488
196/196 - 31s - loss: 28.5662 - MinusLogProbMetric: 28.5662 - val_loss: 29.1680 - val_MinusLogProbMetric: 29.1680 - lr: 8.3333e-05 - 31s/epoch - 159ms/step
Epoch 917/1000
2023-09-29 20:51:09.318 
Epoch 917/1000 
	 loss: 28.5905, MinusLogProbMetric: 28.5905, val_loss: 29.0898, val_MinusLogProbMetric: 29.0898

Epoch 917: val_loss did not improve from 29.07488
196/196 - 29s - loss: 28.5905 - MinusLogProbMetric: 28.5905 - val_loss: 29.0898 - val_MinusLogProbMetric: 29.0898 - lr: 8.3333e-05 - 29s/epoch - 149ms/step
Epoch 918/1000
2023-09-29 20:51:40.124 
Epoch 918/1000 
	 loss: 28.5740, MinusLogProbMetric: 28.5740, val_loss: 29.0406, val_MinusLogProbMetric: 29.0406

Epoch 918: val_loss improved from 29.07488 to 29.04061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 32s - loss: 28.5740 - MinusLogProbMetric: 28.5740 - val_loss: 29.0406 - val_MinusLogProbMetric: 29.0406 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 919/1000
2023-09-29 20:52:13.277 
Epoch 919/1000 
	 loss: 28.5636, MinusLogProbMetric: 28.5636, val_loss: 29.1369, val_MinusLogProbMetric: 29.1369

Epoch 919: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5636 - MinusLogProbMetric: 28.5636 - val_loss: 29.1369 - val_MinusLogProbMetric: 29.1369 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 920/1000
2023-09-29 20:52:45.245 
Epoch 920/1000 
	 loss: 28.5703, MinusLogProbMetric: 28.5703, val_loss: 29.2261, val_MinusLogProbMetric: 29.2261

Epoch 920: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5703 - MinusLogProbMetric: 28.5703 - val_loss: 29.2261 - val_MinusLogProbMetric: 29.2261 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 921/1000
2023-09-29 20:53:17.000 
Epoch 921/1000 
	 loss: 28.5640, MinusLogProbMetric: 28.5640, val_loss: 29.1247, val_MinusLogProbMetric: 29.1247

Epoch 921: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5640 - MinusLogProbMetric: 28.5640 - val_loss: 29.1247 - val_MinusLogProbMetric: 29.1247 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 922/1000
2023-09-29 20:53:46.787 
Epoch 922/1000 
	 loss: 28.5733, MinusLogProbMetric: 28.5733, val_loss: 29.1335, val_MinusLogProbMetric: 29.1335

Epoch 922: val_loss did not improve from 29.04061
196/196 - 30s - loss: 28.5733 - MinusLogProbMetric: 28.5733 - val_loss: 29.1335 - val_MinusLogProbMetric: 29.1335 - lr: 8.3333e-05 - 30s/epoch - 152ms/step
Epoch 923/1000
2023-09-29 20:54:17.254 
Epoch 923/1000 
	 loss: 28.5645, MinusLogProbMetric: 28.5645, val_loss: 29.1252, val_MinusLogProbMetric: 29.1252

Epoch 923: val_loss did not improve from 29.04061
196/196 - 30s - loss: 28.5645 - MinusLogProbMetric: 28.5645 - val_loss: 29.1252 - val_MinusLogProbMetric: 29.1252 - lr: 8.3333e-05 - 30s/epoch - 155ms/step
Epoch 924/1000
2023-09-29 20:54:48.878 
Epoch 924/1000 
	 loss: 28.5774, MinusLogProbMetric: 28.5774, val_loss: 29.0970, val_MinusLogProbMetric: 29.0970

Epoch 924: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5774 - MinusLogProbMetric: 28.5774 - val_loss: 29.0970 - val_MinusLogProbMetric: 29.0970 - lr: 8.3333e-05 - 32s/epoch - 161ms/step
Epoch 925/1000
2023-09-29 20:55:20.027 
Epoch 925/1000 
	 loss: 28.5589, MinusLogProbMetric: 28.5589, val_loss: 29.2137, val_MinusLogProbMetric: 29.2137

Epoch 925: val_loss did not improve from 29.04061
196/196 - 31s - loss: 28.5589 - MinusLogProbMetric: 28.5589 - val_loss: 29.2137 - val_MinusLogProbMetric: 29.2137 - lr: 8.3333e-05 - 31s/epoch - 159ms/step
Epoch 926/1000
2023-09-29 20:55:51.217 
Epoch 926/1000 
	 loss: 28.6072, MinusLogProbMetric: 28.6072, val_loss: 29.1376, val_MinusLogProbMetric: 29.1376

Epoch 926: val_loss did not improve from 29.04061
196/196 - 31s - loss: 28.6072 - MinusLogProbMetric: 28.6072 - val_loss: 29.1376 - val_MinusLogProbMetric: 29.1376 - lr: 8.3333e-05 - 31s/epoch - 159ms/step
Epoch 927/1000
2023-09-29 20:56:23.088 
Epoch 927/1000 
	 loss: 28.5649, MinusLogProbMetric: 28.5649, val_loss: 29.1815, val_MinusLogProbMetric: 29.1815

Epoch 927: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5649 - MinusLogProbMetric: 28.5649 - val_loss: 29.1815 - val_MinusLogProbMetric: 29.1815 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 928/1000
2023-09-29 20:56:54.963 
Epoch 928/1000 
	 loss: 28.5613, MinusLogProbMetric: 28.5613, val_loss: 29.1449, val_MinusLogProbMetric: 29.1449

Epoch 928: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5613 - MinusLogProbMetric: 28.5613 - val_loss: 29.1449 - val_MinusLogProbMetric: 29.1449 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 929/1000
2023-09-29 20:57:29.624 
Epoch 929/1000 
	 loss: 28.5769, MinusLogProbMetric: 28.5769, val_loss: 29.1806, val_MinusLogProbMetric: 29.1806

Epoch 929: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5769 - MinusLogProbMetric: 28.5769 - val_loss: 29.1806 - val_MinusLogProbMetric: 29.1806 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 930/1000
2023-09-29 20:58:02.714 
Epoch 930/1000 
	 loss: 28.5523, MinusLogProbMetric: 28.5523, val_loss: 29.1947, val_MinusLogProbMetric: 29.1947

Epoch 930: val_loss did not improve from 29.04061
196/196 - 33s - loss: 28.5523 - MinusLogProbMetric: 28.5523 - val_loss: 29.1947 - val_MinusLogProbMetric: 29.1947 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 931/1000
2023-09-29 20:58:36.823 
Epoch 931/1000 
	 loss: 28.5758, MinusLogProbMetric: 28.5758, val_loss: 29.1312, val_MinusLogProbMetric: 29.1312

Epoch 931: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5758 - MinusLogProbMetric: 28.5758 - val_loss: 29.1312 - val_MinusLogProbMetric: 29.1312 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 932/1000
2023-09-29 20:59:11.066 
Epoch 932/1000 
	 loss: 28.5694, MinusLogProbMetric: 28.5694, val_loss: 29.1311, val_MinusLogProbMetric: 29.1311

Epoch 932: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5694 - MinusLogProbMetric: 28.5694 - val_loss: 29.1311 - val_MinusLogProbMetric: 29.1311 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 933/1000
2023-09-29 20:59:45.898 
Epoch 933/1000 
	 loss: 28.5747, MinusLogProbMetric: 28.5747, val_loss: 29.1934, val_MinusLogProbMetric: 29.1934

Epoch 933: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5747 - MinusLogProbMetric: 28.5747 - val_loss: 29.1934 - val_MinusLogProbMetric: 29.1934 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 934/1000
2023-09-29 21:00:21.016 
Epoch 934/1000 
	 loss: 28.5791, MinusLogProbMetric: 28.5791, val_loss: 29.2391, val_MinusLogProbMetric: 29.2391

Epoch 934: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5791 - MinusLogProbMetric: 28.5791 - val_loss: 29.2391 - val_MinusLogProbMetric: 29.2391 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 935/1000
2023-09-29 21:00:55.817 
Epoch 935/1000 
	 loss: 28.5922, MinusLogProbMetric: 28.5922, val_loss: 29.1827, val_MinusLogProbMetric: 29.1827

Epoch 935: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5922 - MinusLogProbMetric: 28.5922 - val_loss: 29.1827 - val_MinusLogProbMetric: 29.1827 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 936/1000
2023-09-29 21:01:28.421 
Epoch 936/1000 
	 loss: 28.5790, MinusLogProbMetric: 28.5790, val_loss: 29.1886, val_MinusLogProbMetric: 29.1886

Epoch 936: val_loss did not improve from 29.04061
196/196 - 33s - loss: 28.5790 - MinusLogProbMetric: 28.5790 - val_loss: 29.1886 - val_MinusLogProbMetric: 29.1886 - lr: 8.3333e-05 - 33s/epoch - 166ms/step
Epoch 937/1000
2023-09-29 21:02:02.686 
Epoch 937/1000 
	 loss: 28.5783, MinusLogProbMetric: 28.5783, val_loss: 29.1572, val_MinusLogProbMetric: 29.1572

Epoch 937: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5783 - MinusLogProbMetric: 28.5783 - val_loss: 29.1572 - val_MinusLogProbMetric: 29.1572 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 938/1000
2023-09-29 21:02:37.299 
Epoch 938/1000 
	 loss: 28.5639, MinusLogProbMetric: 28.5639, val_loss: 29.1008, val_MinusLogProbMetric: 29.1008

Epoch 938: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5639 - MinusLogProbMetric: 28.5639 - val_loss: 29.1008 - val_MinusLogProbMetric: 29.1008 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 939/1000
2023-09-29 21:03:11.885 
Epoch 939/1000 
	 loss: 28.5596, MinusLogProbMetric: 28.5596, val_loss: 29.1041, val_MinusLogProbMetric: 29.1041

Epoch 939: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5596 - MinusLogProbMetric: 28.5596 - val_loss: 29.1041 - val_MinusLogProbMetric: 29.1041 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 940/1000
2023-09-29 21:03:46.202 
Epoch 940/1000 
	 loss: 28.5510, MinusLogProbMetric: 28.5510, val_loss: 29.0985, val_MinusLogProbMetric: 29.0985

Epoch 940: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5510 - MinusLogProbMetric: 28.5510 - val_loss: 29.0985 - val_MinusLogProbMetric: 29.0985 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 941/1000
2023-09-29 21:04:17.089 
Epoch 941/1000 
	 loss: 28.5708, MinusLogProbMetric: 28.5708, val_loss: 29.1149, val_MinusLogProbMetric: 29.1149

Epoch 941: val_loss did not improve from 29.04061
196/196 - 31s - loss: 28.5708 - MinusLogProbMetric: 28.5708 - val_loss: 29.1149 - val_MinusLogProbMetric: 29.1149 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 942/1000
2023-09-29 21:04:51.970 
Epoch 942/1000 
	 loss: 28.5755, MinusLogProbMetric: 28.5755, val_loss: 29.1102, val_MinusLogProbMetric: 29.1102

Epoch 942: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5755 - MinusLogProbMetric: 28.5755 - val_loss: 29.1102 - val_MinusLogProbMetric: 29.1102 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 943/1000
2023-09-29 21:05:26.113 
Epoch 943/1000 
	 loss: 28.5725, MinusLogProbMetric: 28.5725, val_loss: 29.2519, val_MinusLogProbMetric: 29.2519

Epoch 943: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5725 - MinusLogProbMetric: 28.5725 - val_loss: 29.2519 - val_MinusLogProbMetric: 29.2519 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 944/1000
2023-09-29 21:06:00.462 
Epoch 944/1000 
	 loss: 28.5857, MinusLogProbMetric: 28.5857, val_loss: 29.3781, val_MinusLogProbMetric: 29.3781

Epoch 944: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5857 - MinusLogProbMetric: 28.5857 - val_loss: 29.3781 - val_MinusLogProbMetric: 29.3781 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 945/1000
2023-09-29 21:06:31.361 
Epoch 945/1000 
	 loss: 28.5699, MinusLogProbMetric: 28.5699, val_loss: 29.1424, val_MinusLogProbMetric: 29.1424

Epoch 945: val_loss did not improve from 29.04061
196/196 - 31s - loss: 28.5699 - MinusLogProbMetric: 28.5699 - val_loss: 29.1424 - val_MinusLogProbMetric: 29.1424 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 946/1000
2023-09-29 21:07:04.590 
Epoch 946/1000 
	 loss: 28.5549, MinusLogProbMetric: 28.5549, val_loss: 29.0920, val_MinusLogProbMetric: 29.0920

Epoch 946: val_loss did not improve from 29.04061
196/196 - 33s - loss: 28.5549 - MinusLogProbMetric: 28.5549 - val_loss: 29.0920 - val_MinusLogProbMetric: 29.0920 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 947/1000
2023-09-29 21:07:39.200 
Epoch 947/1000 
	 loss: 28.5627, MinusLogProbMetric: 28.5627, val_loss: 29.1571, val_MinusLogProbMetric: 29.1571

Epoch 947: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5627 - MinusLogProbMetric: 28.5627 - val_loss: 29.1571 - val_MinusLogProbMetric: 29.1571 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 948/1000
2023-09-29 21:08:13.440 
Epoch 948/1000 
	 loss: 28.5798, MinusLogProbMetric: 28.5798, val_loss: 29.1646, val_MinusLogProbMetric: 29.1646

Epoch 948: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5798 - MinusLogProbMetric: 28.5798 - val_loss: 29.1646 - val_MinusLogProbMetric: 29.1646 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 949/1000
2023-09-29 21:08:45.553 
Epoch 949/1000 
	 loss: 28.5794, MinusLogProbMetric: 28.5794, val_loss: 29.2017, val_MinusLogProbMetric: 29.2017

Epoch 949: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5794 - MinusLogProbMetric: 28.5794 - val_loss: 29.2017 - val_MinusLogProbMetric: 29.2017 - lr: 8.3333e-05 - 32s/epoch - 164ms/step
Epoch 950/1000
2023-09-29 21:09:18.984 
Epoch 950/1000 
	 loss: 28.5557, MinusLogProbMetric: 28.5557, val_loss: 29.2551, val_MinusLogProbMetric: 29.2551

Epoch 950: val_loss did not improve from 29.04061
196/196 - 33s - loss: 28.5557 - MinusLogProbMetric: 28.5557 - val_loss: 29.2551 - val_MinusLogProbMetric: 29.2551 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 951/1000
2023-09-29 21:09:49.667 
Epoch 951/1000 
	 loss: 28.5759, MinusLogProbMetric: 28.5759, val_loss: 29.1917, val_MinusLogProbMetric: 29.1917

Epoch 951: val_loss did not improve from 29.04061
196/196 - 31s - loss: 28.5759 - MinusLogProbMetric: 28.5759 - val_loss: 29.1917 - val_MinusLogProbMetric: 29.1917 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 952/1000
2023-09-29 21:10:19.578 
Epoch 952/1000 
	 loss: 28.5514, MinusLogProbMetric: 28.5514, val_loss: 29.0805, val_MinusLogProbMetric: 29.0805

Epoch 952: val_loss did not improve from 29.04061
196/196 - 30s - loss: 28.5514 - MinusLogProbMetric: 28.5514 - val_loss: 29.0805 - val_MinusLogProbMetric: 29.0805 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 953/1000
2023-09-29 21:10:49.223 
Epoch 953/1000 
	 loss: 28.5594, MinusLogProbMetric: 28.5594, val_loss: 29.2456, val_MinusLogProbMetric: 29.2456

Epoch 953: val_loss did not improve from 29.04061
196/196 - 30s - loss: 28.5594 - MinusLogProbMetric: 28.5594 - val_loss: 29.2456 - val_MinusLogProbMetric: 29.2456 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 954/1000
2023-09-29 21:11:21.724 
Epoch 954/1000 
	 loss: 28.5600, MinusLogProbMetric: 28.5600, val_loss: 29.1274, val_MinusLogProbMetric: 29.1274

Epoch 954: val_loss did not improve from 29.04061
196/196 - 32s - loss: 28.5600 - MinusLogProbMetric: 28.5600 - val_loss: 29.1274 - val_MinusLogProbMetric: 29.1274 - lr: 8.3333e-05 - 32s/epoch - 166ms/step
Epoch 955/1000
2023-09-29 21:11:56.888 
Epoch 955/1000 
	 loss: 28.5444, MinusLogProbMetric: 28.5444, val_loss: 29.2647, val_MinusLogProbMetric: 29.2647

Epoch 955: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5444 - MinusLogProbMetric: 28.5444 - val_loss: 29.2647 - val_MinusLogProbMetric: 29.2647 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 956/1000
2023-09-29 21:12:31.908 
Epoch 956/1000 
	 loss: 28.5666, MinusLogProbMetric: 28.5666, val_loss: 29.1114, val_MinusLogProbMetric: 29.1114

Epoch 956: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5666 - MinusLogProbMetric: 28.5666 - val_loss: 29.1114 - val_MinusLogProbMetric: 29.1114 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 957/1000
2023-09-29 21:13:05.851 
Epoch 957/1000 
	 loss: 28.5548, MinusLogProbMetric: 28.5548, val_loss: 29.1955, val_MinusLogProbMetric: 29.1955

Epoch 957: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5548 - MinusLogProbMetric: 28.5548 - val_loss: 29.1955 - val_MinusLogProbMetric: 29.1955 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 958/1000
2023-09-29 21:13:40.850 
Epoch 958/1000 
	 loss: 28.5508, MinusLogProbMetric: 28.5508, val_loss: 29.1495, val_MinusLogProbMetric: 29.1495

Epoch 958: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5508 - MinusLogProbMetric: 28.5508 - val_loss: 29.1495 - val_MinusLogProbMetric: 29.1495 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 959/1000
2023-09-29 21:14:15.963 
Epoch 959/1000 
	 loss: 28.5634, MinusLogProbMetric: 28.5634, val_loss: 29.1152, val_MinusLogProbMetric: 29.1152

Epoch 959: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5634 - MinusLogProbMetric: 28.5634 - val_loss: 29.1152 - val_MinusLogProbMetric: 29.1152 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 960/1000
2023-09-29 21:14:50.675 
Epoch 960/1000 
	 loss: 28.5808, MinusLogProbMetric: 28.5808, val_loss: 29.1523, val_MinusLogProbMetric: 29.1523

Epoch 960: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5808 - MinusLogProbMetric: 28.5808 - val_loss: 29.1523 - val_MinusLogProbMetric: 29.1523 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 961/1000
2023-09-29 21:15:25.722 
Epoch 961/1000 
	 loss: 28.5505, MinusLogProbMetric: 28.5505, val_loss: 29.1429, val_MinusLogProbMetric: 29.1429

Epoch 961: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5505 - MinusLogProbMetric: 28.5505 - val_loss: 29.1429 - val_MinusLogProbMetric: 29.1429 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 962/1000
2023-09-29 21:16:00.734 
Epoch 962/1000 
	 loss: 28.5752, MinusLogProbMetric: 28.5752, val_loss: 29.1861, val_MinusLogProbMetric: 29.1861

Epoch 962: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5752 - MinusLogProbMetric: 28.5752 - val_loss: 29.1861 - val_MinusLogProbMetric: 29.1861 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 963/1000
2023-09-29 21:16:34.946 
Epoch 963/1000 
	 loss: 28.5455, MinusLogProbMetric: 28.5455, val_loss: 29.2839, val_MinusLogProbMetric: 29.2839

Epoch 963: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5455 - MinusLogProbMetric: 28.5455 - val_loss: 29.2839 - val_MinusLogProbMetric: 29.2839 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 964/1000
2023-09-29 21:17:09.416 
Epoch 964/1000 
	 loss: 28.5501, MinusLogProbMetric: 28.5501, val_loss: 29.1971, val_MinusLogProbMetric: 29.1971

Epoch 964: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5501 - MinusLogProbMetric: 28.5501 - val_loss: 29.1971 - val_MinusLogProbMetric: 29.1971 - lr: 8.3333e-05 - 34s/epoch - 176ms/step
Epoch 965/1000
2023-09-29 21:17:44.193 
Epoch 965/1000 
	 loss: 28.5605, MinusLogProbMetric: 28.5605, val_loss: 29.2486, val_MinusLogProbMetric: 29.2486

Epoch 965: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5605 - MinusLogProbMetric: 28.5605 - val_loss: 29.2486 - val_MinusLogProbMetric: 29.2486 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 966/1000
2023-09-29 21:18:19.044 
Epoch 966/1000 
	 loss: 28.5678, MinusLogProbMetric: 28.5678, val_loss: 29.2472, val_MinusLogProbMetric: 29.2472

Epoch 966: val_loss did not improve from 29.04061
196/196 - 35s - loss: 28.5678 - MinusLogProbMetric: 28.5678 - val_loss: 29.2472 - val_MinusLogProbMetric: 29.2472 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 967/1000
2023-09-29 21:18:53.349 
Epoch 967/1000 
	 loss: 28.5763, MinusLogProbMetric: 28.5763, val_loss: 29.1498, val_MinusLogProbMetric: 29.1498

Epoch 967: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5763 - MinusLogProbMetric: 28.5763 - val_loss: 29.1498 - val_MinusLogProbMetric: 29.1498 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 968/1000
2023-09-29 21:19:27.589 
Epoch 968/1000 
	 loss: 28.5441, MinusLogProbMetric: 28.5441, val_loss: 29.1558, val_MinusLogProbMetric: 29.1558

Epoch 968: val_loss did not improve from 29.04061
196/196 - 34s - loss: 28.5441 - MinusLogProbMetric: 28.5441 - val_loss: 29.1558 - val_MinusLogProbMetric: 29.1558 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 969/1000
2023-09-29 21:20:02.369 
Epoch 969/1000 
	 loss: 28.4780, MinusLogProbMetric: 28.4780, val_loss: 29.0346, val_MinusLogProbMetric: 29.0346

Epoch 969: val_loss improved from 29.04061 to 29.03455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 36s - loss: 28.4780 - MinusLogProbMetric: 28.4780 - val_loss: 29.0346 - val_MinusLogProbMetric: 29.0346 - lr: 4.1667e-05 - 36s/epoch - 182ms/step
Epoch 970/1000
2023-09-29 21:20:37.607 
Epoch 970/1000 
	 loss: 28.4703, MinusLogProbMetric: 28.4703, val_loss: 29.0501, val_MinusLogProbMetric: 29.0501

Epoch 970: val_loss did not improve from 29.03455
196/196 - 34s - loss: 28.4703 - MinusLogProbMetric: 28.4703 - val_loss: 29.0501 - val_MinusLogProbMetric: 29.0501 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 971/1000
2023-09-29 21:21:12.206 
Epoch 971/1000 
	 loss: 28.4660, MinusLogProbMetric: 28.4660, val_loss: 29.0426, val_MinusLogProbMetric: 29.0426

Epoch 971: val_loss did not improve from 29.03455
196/196 - 35s - loss: 28.4660 - MinusLogProbMetric: 28.4660 - val_loss: 29.0426 - val_MinusLogProbMetric: 29.0426 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 972/1000
2023-09-29 21:21:46.779 
Epoch 972/1000 
	 loss: 28.4679, MinusLogProbMetric: 28.4679, val_loss: 29.0416, val_MinusLogProbMetric: 29.0416

Epoch 972: val_loss did not improve from 29.03455
196/196 - 35s - loss: 28.4679 - MinusLogProbMetric: 28.4679 - val_loss: 29.0416 - val_MinusLogProbMetric: 29.0416 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 973/1000
2023-09-29 21:22:20.970 
Epoch 973/1000 
	 loss: 28.4644, MinusLogProbMetric: 28.4644, val_loss: 29.0525, val_MinusLogProbMetric: 29.0525

Epoch 973: val_loss did not improve from 29.03455
196/196 - 34s - loss: 28.4644 - MinusLogProbMetric: 28.4644 - val_loss: 29.0525 - val_MinusLogProbMetric: 29.0525 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 974/1000
2023-09-29 21:22:54.579 
Epoch 974/1000 
	 loss: 28.4590, MinusLogProbMetric: 28.4590, val_loss: 29.0750, val_MinusLogProbMetric: 29.0750

Epoch 974: val_loss did not improve from 29.03455
196/196 - 34s - loss: 28.4590 - MinusLogProbMetric: 28.4590 - val_loss: 29.0750 - val_MinusLogProbMetric: 29.0750 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 975/1000
2023-09-29 21:23:29.286 
Epoch 975/1000 
	 loss: 28.4646, MinusLogProbMetric: 28.4646, val_loss: 29.0571, val_MinusLogProbMetric: 29.0571

Epoch 975: val_loss did not improve from 29.03455
196/196 - 35s - loss: 28.4646 - MinusLogProbMetric: 28.4646 - val_loss: 29.0571 - val_MinusLogProbMetric: 29.0571 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 976/1000
2023-09-29 21:24:03.710 
Epoch 976/1000 
	 loss: 28.4708, MinusLogProbMetric: 28.4708, val_loss: 29.0442, val_MinusLogProbMetric: 29.0442

Epoch 976: val_loss did not improve from 29.03455
196/196 - 34s - loss: 28.4708 - MinusLogProbMetric: 28.4708 - val_loss: 29.0442 - val_MinusLogProbMetric: 29.0442 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 977/1000
2023-09-29 21:24:38.286 
Epoch 977/1000 
	 loss: 28.4666, MinusLogProbMetric: 28.4666, val_loss: 29.0670, val_MinusLogProbMetric: 29.0670

Epoch 977: val_loss did not improve from 29.03455
196/196 - 35s - loss: 28.4666 - MinusLogProbMetric: 28.4666 - val_loss: 29.0670 - val_MinusLogProbMetric: 29.0670 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 978/1000
2023-09-29 21:25:10.951 
Epoch 978/1000 
	 loss: 28.4643, MinusLogProbMetric: 28.4643, val_loss: 29.0756, val_MinusLogProbMetric: 29.0756

Epoch 978: val_loss did not improve from 29.03455
196/196 - 33s - loss: 28.4643 - MinusLogProbMetric: 28.4643 - val_loss: 29.0756 - val_MinusLogProbMetric: 29.0756 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 979/1000
2023-09-29 21:25:45.142 
Epoch 979/1000 
	 loss: 28.4583, MinusLogProbMetric: 28.4583, val_loss: 29.0191, val_MinusLogProbMetric: 29.0191

Epoch 979: val_loss improved from 29.03455 to 29.01910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_337/weights/best_weights.h5
196/196 - 35s - loss: 28.4583 - MinusLogProbMetric: 28.4583 - val_loss: 29.0191 - val_MinusLogProbMetric: 29.0191 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 980/1000
2023-09-29 21:26:16.842 
Epoch 980/1000 
	 loss: 28.4681, MinusLogProbMetric: 28.4681, val_loss: 29.0411, val_MinusLogProbMetric: 29.0411

Epoch 980: val_loss did not improve from 29.01910
196/196 - 31s - loss: 28.4681 - MinusLogProbMetric: 28.4681 - val_loss: 29.0411 - val_MinusLogProbMetric: 29.0411 - lr: 4.1667e-05 - 31s/epoch - 158ms/step
Epoch 981/1000
2023-09-29 21:26:49.610 
Epoch 981/1000 
	 loss: 28.4637, MinusLogProbMetric: 28.4637, val_loss: 29.0355, val_MinusLogProbMetric: 29.0355

Epoch 981: val_loss did not improve from 29.01910
196/196 - 33s - loss: 28.4637 - MinusLogProbMetric: 28.4637 - val_loss: 29.0355 - val_MinusLogProbMetric: 29.0355 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 982/1000
2023-09-29 21:27:22.937 
Epoch 982/1000 
	 loss: 28.4637, MinusLogProbMetric: 28.4637, val_loss: 29.0702, val_MinusLogProbMetric: 29.0702

Epoch 982: val_loss did not improve from 29.01910
196/196 - 33s - loss: 28.4637 - MinusLogProbMetric: 28.4637 - val_loss: 29.0702 - val_MinusLogProbMetric: 29.0702 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 983/1000
2023-09-29 21:27:56.873 
Epoch 983/1000 
	 loss: 28.4539, MinusLogProbMetric: 28.4539, val_loss: 29.0228, val_MinusLogProbMetric: 29.0228

Epoch 983: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4539 - MinusLogProbMetric: 28.4539 - val_loss: 29.0228 - val_MinusLogProbMetric: 29.0228 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 984/1000
2023-09-29 21:28:30.440 
Epoch 984/1000 
	 loss: 28.4650, MinusLogProbMetric: 28.4650, val_loss: 29.0269, val_MinusLogProbMetric: 29.0269

Epoch 984: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4650 - MinusLogProbMetric: 28.4650 - val_loss: 29.0269 - val_MinusLogProbMetric: 29.0269 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 985/1000
2023-09-29 21:29:04.516 
Epoch 985/1000 
	 loss: 28.4604, MinusLogProbMetric: 28.4604, val_loss: 29.1231, val_MinusLogProbMetric: 29.1231

Epoch 985: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4604 - MinusLogProbMetric: 28.4604 - val_loss: 29.1231 - val_MinusLogProbMetric: 29.1231 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 986/1000
2023-09-29 21:29:39.529 
Epoch 986/1000 
	 loss: 28.4661, MinusLogProbMetric: 28.4661, val_loss: 29.0695, val_MinusLogProbMetric: 29.0695

Epoch 986: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4661 - MinusLogProbMetric: 28.4661 - val_loss: 29.0695 - val_MinusLogProbMetric: 29.0695 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 987/1000
2023-09-29 21:30:14.607 
Epoch 987/1000 
	 loss: 28.4555, MinusLogProbMetric: 28.4555, val_loss: 29.0827, val_MinusLogProbMetric: 29.0827

Epoch 987: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4555 - MinusLogProbMetric: 28.4555 - val_loss: 29.0827 - val_MinusLogProbMetric: 29.0827 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 988/1000
2023-09-29 21:30:48.327 
Epoch 988/1000 
	 loss: 28.4627, MinusLogProbMetric: 28.4627, val_loss: 29.0836, val_MinusLogProbMetric: 29.0836

Epoch 988: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4627 - MinusLogProbMetric: 28.4627 - val_loss: 29.0836 - val_MinusLogProbMetric: 29.0836 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 989/1000
2023-09-29 21:31:22.244 
Epoch 989/1000 
	 loss: 28.4592, MinusLogProbMetric: 28.4592, val_loss: 29.0426, val_MinusLogProbMetric: 29.0426

Epoch 989: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4592 - MinusLogProbMetric: 28.4592 - val_loss: 29.0426 - val_MinusLogProbMetric: 29.0426 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 990/1000
2023-09-29 21:31:56.839 
Epoch 990/1000 
	 loss: 28.4654, MinusLogProbMetric: 28.4654, val_loss: 29.0803, val_MinusLogProbMetric: 29.0803

Epoch 990: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4654 - MinusLogProbMetric: 28.4654 - val_loss: 29.0803 - val_MinusLogProbMetric: 29.0803 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 991/1000
2023-09-29 21:32:31.598 
Epoch 991/1000 
	 loss: 28.4628, MinusLogProbMetric: 28.4628, val_loss: 29.0526, val_MinusLogProbMetric: 29.0526

Epoch 991: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4628 - MinusLogProbMetric: 28.4628 - val_loss: 29.0526 - val_MinusLogProbMetric: 29.0526 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 992/1000
2023-09-29 21:33:06.021 
Epoch 992/1000 
	 loss: 28.4601, MinusLogProbMetric: 28.4601, val_loss: 29.0338, val_MinusLogProbMetric: 29.0338

Epoch 992: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4601 - MinusLogProbMetric: 28.4601 - val_loss: 29.0338 - val_MinusLogProbMetric: 29.0338 - lr: 4.1667e-05 - 34s/epoch - 176ms/step
Epoch 993/1000
2023-09-29 21:33:40.872 
Epoch 993/1000 
	 loss: 28.4524, MinusLogProbMetric: 28.4524, val_loss: 29.0254, val_MinusLogProbMetric: 29.0254

Epoch 993: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4524 - MinusLogProbMetric: 28.4524 - val_loss: 29.0254 - val_MinusLogProbMetric: 29.0254 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 994/1000
2023-09-29 21:34:15.455 
Epoch 994/1000 
	 loss: 28.4535, MinusLogProbMetric: 28.4535, val_loss: 29.1030, val_MinusLogProbMetric: 29.1030

Epoch 994: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4535 - MinusLogProbMetric: 28.4535 - val_loss: 29.1030 - val_MinusLogProbMetric: 29.1030 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 995/1000
2023-09-29 21:34:50.126 
Epoch 995/1000 
	 loss: 28.4636, MinusLogProbMetric: 28.4636, val_loss: 29.0312, val_MinusLogProbMetric: 29.0312

Epoch 995: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4636 - MinusLogProbMetric: 28.4636 - val_loss: 29.0312 - val_MinusLogProbMetric: 29.0312 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 996/1000
2023-09-29 21:35:24.048 
Epoch 996/1000 
	 loss: 28.4578, MinusLogProbMetric: 28.4578, val_loss: 29.0825, val_MinusLogProbMetric: 29.0825

Epoch 996: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4578 - MinusLogProbMetric: 28.4578 - val_loss: 29.0825 - val_MinusLogProbMetric: 29.0825 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 997/1000
2023-09-29 21:35:59.137 
Epoch 997/1000 
	 loss: 28.4514, MinusLogProbMetric: 28.4514, val_loss: 29.0575, val_MinusLogProbMetric: 29.0575

Epoch 997: val_loss did not improve from 29.01910
196/196 - 35s - loss: 28.4514 - MinusLogProbMetric: 28.4514 - val_loss: 29.0575 - val_MinusLogProbMetric: 29.0575 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 998/1000
2023-09-29 21:36:32.862 
Epoch 998/1000 
	 loss: 28.4645, MinusLogProbMetric: 28.4645, val_loss: 29.0487, val_MinusLogProbMetric: 29.0487

Epoch 998: val_loss did not improve from 29.01910
196/196 - 34s - loss: 28.4645 - MinusLogProbMetric: 28.4645 - val_loss: 29.0487 - val_MinusLogProbMetric: 29.0487 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 999/1000
2023-09-29 21:37:05.736 
Epoch 999/1000 
	 loss: 28.4700, MinusLogProbMetric: 28.4700, val_loss: 29.0408, val_MinusLogProbMetric: 29.0408

Epoch 999: val_loss did not improve from 29.01910
196/196 - 33s - loss: 28.4700 - MinusLogProbMetric: 28.4700 - val_loss: 29.0408 - val_MinusLogProbMetric: 29.0408 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 1000/1000
2023-09-29 21:37:37.787 
Epoch 1000/1000 
	 loss: 28.4589, MinusLogProbMetric: 28.4589, val_loss: 29.0270, val_MinusLogProbMetric: 29.0270

Epoch 1000: val_loss did not improve from 29.01910
196/196 - 32s - loss: 28.4589 - MinusLogProbMetric: 28.4589 - val_loss: 29.0270 - val_MinusLogProbMetric: 29.0270 - lr: 4.1667e-05 - 32s/epoch - 164ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 14.551384844002314 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 8.78836152504664 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 5.8920883340761065 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 7.487356869969517 seconds.
Training succeeded with seed 377.
Model trained in 32619.19 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 37.89 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 38.19 s.
===========
Run 337/720 done in 32731.74 s.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

===========
Generating train data for run 345.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_345/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_345/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_345/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_345
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_167"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_168 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7fc238663460>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc1b043af80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc1b043af80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc39825c460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc22b280850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc22b280dc0>, <keras.callbacks.ModelCheckpoint object at 0x7fc22b280e80>, <keras.callbacks.EarlyStopping object at 0x7fc22b2810f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc22b281120>, <keras.callbacks.TerminateOnNaN object at 0x7fc22b280d60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_345/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 345/720 with hyperparameters:
timestamp = 2023-09-29 21:38:20.880067
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-09-29 21:39:51.642 
Epoch 1/1000 
	 loss: 984.9517, MinusLogProbMetric: 984.9517, val_loss: 339.1322, val_MinusLogProbMetric: 339.1322

Epoch 1: val_loss improved from inf to 339.13217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 91s - loss: 984.9517 - MinusLogProbMetric: 984.9517 - val_loss: 339.1322 - val_MinusLogProbMetric: 339.1322 - lr: 0.0010 - 91s/epoch - 465ms/step
Epoch 2/1000
2023-09-29 21:40:25.218 
Epoch 2/1000 
	 loss: 221.7954, MinusLogProbMetric: 221.7954, val_loss: 165.2215, val_MinusLogProbMetric: 165.2215

Epoch 2: val_loss improved from 339.13217 to 165.22145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 221.7954 - MinusLogProbMetric: 221.7954 - val_loss: 165.2215 - val_MinusLogProbMetric: 165.2215 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 3/1000
2023-09-29 21:41:00.340 
Epoch 3/1000 
	 loss: 145.7611, MinusLogProbMetric: 145.7611, val_loss: 126.0587, val_MinusLogProbMetric: 126.0587

Epoch 3: val_loss improved from 165.22145 to 126.05875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 145.7611 - MinusLogProbMetric: 145.7611 - val_loss: 126.0587 - val_MinusLogProbMetric: 126.0587 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 4/1000
2023-09-29 21:41:34.601 
Epoch 4/1000 
	 loss: 148.5956, MinusLogProbMetric: 148.5956, val_loss: 173.7824, val_MinusLogProbMetric: 173.7824

Epoch 4: val_loss did not improve from 126.05875
196/196 - 34s - loss: 148.5956 - MinusLogProbMetric: 148.5956 - val_loss: 173.7824 - val_MinusLogProbMetric: 173.7824 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 5/1000
2023-09-29 21:42:07.874 
Epoch 5/1000 
	 loss: 129.7373, MinusLogProbMetric: 129.7373, val_loss: 107.5786, val_MinusLogProbMetric: 107.5786

Epoch 5: val_loss improved from 126.05875 to 107.57859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 129.7373 - MinusLogProbMetric: 129.7373 - val_loss: 107.5786 - val_MinusLogProbMetric: 107.5786 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 6/1000
2023-09-29 21:42:43.103 
Epoch 6/1000 
	 loss: 97.0045, MinusLogProbMetric: 97.0045, val_loss: 89.6732, val_MinusLogProbMetric: 89.6732

Epoch 6: val_loss improved from 107.57859 to 89.67323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 97.0045 - MinusLogProbMetric: 97.0045 - val_loss: 89.6732 - val_MinusLogProbMetric: 89.6732 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 7/1000
2023-09-29 21:43:14.687 
Epoch 7/1000 
	 loss: 83.8090, MinusLogProbMetric: 83.8090, val_loss: 79.4097, val_MinusLogProbMetric: 79.4097

Epoch 7: val_loss improved from 89.67323 to 79.40974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 31s - loss: 83.8090 - MinusLogProbMetric: 83.8090 - val_loss: 79.4097 - val_MinusLogProbMetric: 79.4097 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 8/1000
2023-09-29 21:43:48.456 
Epoch 8/1000 
	 loss: 75.6276, MinusLogProbMetric: 75.6276, val_loss: 72.3560, val_MinusLogProbMetric: 72.3560

Epoch 8: val_loss improved from 79.40974 to 72.35600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 75.6276 - MinusLogProbMetric: 75.6276 - val_loss: 72.3560 - val_MinusLogProbMetric: 72.3560 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 9/1000
2023-09-29 21:44:22.236 
Epoch 9/1000 
	 loss: 69.3661, MinusLogProbMetric: 69.3661, val_loss: 67.4119, val_MinusLogProbMetric: 67.4119

Epoch 9: val_loss improved from 72.35600 to 67.41191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 69.3661 - MinusLogProbMetric: 69.3661 - val_loss: 67.4119 - val_MinusLogProbMetric: 67.4119 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 10/1000
2023-09-29 21:44:55.545 
Epoch 10/1000 
	 loss: 64.6763, MinusLogProbMetric: 64.6763, val_loss: 63.5129, val_MinusLogProbMetric: 63.5129

Epoch 10: val_loss improved from 67.41191 to 63.51292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 64.6763 - MinusLogProbMetric: 64.6763 - val_loss: 63.5129 - val_MinusLogProbMetric: 63.5129 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 11/1000
2023-09-29 21:45:29.303 
Epoch 11/1000 
	 loss: 61.4670, MinusLogProbMetric: 61.4670, val_loss: 66.4848, val_MinusLogProbMetric: 66.4848

Epoch 11: val_loss did not improve from 63.51292
196/196 - 33s - loss: 61.4670 - MinusLogProbMetric: 61.4670 - val_loss: 66.4848 - val_MinusLogProbMetric: 66.4848 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 12/1000
2023-09-29 21:46:04.037 
Epoch 12/1000 
	 loss: 58.9309, MinusLogProbMetric: 58.9309, val_loss: 57.0721, val_MinusLogProbMetric: 57.0721

Epoch 12: val_loss improved from 63.51292 to 57.07212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 58.9309 - MinusLogProbMetric: 58.9309 - val_loss: 57.0721 - val_MinusLogProbMetric: 57.0721 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 13/1000
2023-09-29 21:46:36.955 
Epoch 13/1000 
	 loss: 62.7289, MinusLogProbMetric: 62.7289, val_loss: 56.8089, val_MinusLogProbMetric: 56.8089

Epoch 13: val_loss improved from 57.07212 to 56.80894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 62.7289 - MinusLogProbMetric: 62.7289 - val_loss: 56.8089 - val_MinusLogProbMetric: 56.8089 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 14/1000
2023-09-29 21:47:10.371 
Epoch 14/1000 
	 loss: 54.2969, MinusLogProbMetric: 54.2969, val_loss: 53.0840, val_MinusLogProbMetric: 53.0840

Epoch 14: val_loss improved from 56.80894 to 53.08395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 54.2969 - MinusLogProbMetric: 54.2969 - val_loss: 53.0840 - val_MinusLogProbMetric: 53.0840 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 15/1000
2023-09-29 21:47:42.488 
Epoch 15/1000 
	 loss: 52.8153, MinusLogProbMetric: 52.8153, val_loss: 53.5278, val_MinusLogProbMetric: 53.5278

Epoch 15: val_loss did not improve from 53.08395
196/196 - 32s - loss: 52.8153 - MinusLogProbMetric: 52.8153 - val_loss: 53.5278 - val_MinusLogProbMetric: 53.5278 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 16/1000
2023-09-29 21:48:15.973 
Epoch 16/1000 
	 loss: 69.1788, MinusLogProbMetric: 69.1788, val_loss: 85.1583, val_MinusLogProbMetric: 85.1583

Epoch 16: val_loss did not improve from 53.08395
196/196 - 33s - loss: 69.1788 - MinusLogProbMetric: 69.1788 - val_loss: 85.1583 - val_MinusLogProbMetric: 85.1583 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 17/1000
2023-09-29 21:48:48.709 
Epoch 17/1000 
	 loss: 61.0939, MinusLogProbMetric: 61.0939, val_loss: 55.3801, val_MinusLogProbMetric: 55.3801

Epoch 17: val_loss did not improve from 53.08395
196/196 - 33s - loss: 61.0939 - MinusLogProbMetric: 61.0939 - val_loss: 55.3801 - val_MinusLogProbMetric: 55.3801 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 18/1000
2023-09-29 21:49:22.705 
Epoch 18/1000 
	 loss: 53.6203, MinusLogProbMetric: 53.6203, val_loss: 54.2586, val_MinusLogProbMetric: 54.2586

Epoch 18: val_loss did not improve from 53.08395
196/196 - 34s - loss: 53.6203 - MinusLogProbMetric: 53.6203 - val_loss: 54.2586 - val_MinusLogProbMetric: 54.2586 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 19/1000
2023-09-29 21:49:55.927 
Epoch 19/1000 
	 loss: 51.7121, MinusLogProbMetric: 51.7121, val_loss: 52.0407, val_MinusLogProbMetric: 52.0407

Epoch 19: val_loss improved from 53.08395 to 52.04068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 51.7121 - MinusLogProbMetric: 51.7121 - val_loss: 52.0407 - val_MinusLogProbMetric: 52.0407 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 20/1000
2023-09-29 21:50:30.053 
Epoch 20/1000 
	 loss: 49.1027, MinusLogProbMetric: 49.1027, val_loss: 48.5425, val_MinusLogProbMetric: 48.5425

Epoch 20: val_loss improved from 52.04068 to 48.54249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 49.1027 - MinusLogProbMetric: 49.1027 - val_loss: 48.5425 - val_MinusLogProbMetric: 48.5425 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 21/1000
2023-09-29 21:51:03.057 
Epoch 21/1000 
	 loss: 47.8218, MinusLogProbMetric: 47.8218, val_loss: 46.5179, val_MinusLogProbMetric: 46.5179

Epoch 21: val_loss improved from 48.54249 to 46.51788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 47.8218 - MinusLogProbMetric: 47.8218 - val_loss: 46.5179 - val_MinusLogProbMetric: 46.5179 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 22/1000
2023-09-29 21:51:37.595 
Epoch 22/1000 
	 loss: 45.5459, MinusLogProbMetric: 45.5459, val_loss: 44.4473, val_MinusLogProbMetric: 44.4473

Epoch 22: val_loss improved from 46.51788 to 44.44731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 45.5459 - MinusLogProbMetric: 45.5459 - val_loss: 44.4473 - val_MinusLogProbMetric: 44.4473 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 23/1000
2023-09-29 21:52:11.876 
Epoch 23/1000 
	 loss: 44.7188, MinusLogProbMetric: 44.7188, val_loss: 45.0372, val_MinusLogProbMetric: 45.0372

Epoch 23: val_loss did not improve from 44.44731
196/196 - 34s - loss: 44.7188 - MinusLogProbMetric: 44.7188 - val_loss: 45.0372 - val_MinusLogProbMetric: 45.0372 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 24/1000
2023-09-29 21:52:46.383 
Epoch 24/1000 
	 loss: 43.8077, MinusLogProbMetric: 43.8077, val_loss: 44.6183, val_MinusLogProbMetric: 44.6183

Epoch 24: val_loss did not improve from 44.44731
196/196 - 35s - loss: 43.8077 - MinusLogProbMetric: 43.8077 - val_loss: 44.6183 - val_MinusLogProbMetric: 44.6183 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 25/1000
2023-09-29 21:53:18.976 
Epoch 25/1000 
	 loss: 43.0363, MinusLogProbMetric: 43.0363, val_loss: 44.3319, val_MinusLogProbMetric: 44.3319

Epoch 25: val_loss improved from 44.44731 to 44.33190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 43.0363 - MinusLogProbMetric: 43.0363 - val_loss: 44.3319 - val_MinusLogProbMetric: 44.3319 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 26/1000
2023-09-29 21:53:53.452 
Epoch 26/1000 
	 loss: 43.1130, MinusLogProbMetric: 43.1130, val_loss: 43.3734, val_MinusLogProbMetric: 43.3734

Epoch 26: val_loss improved from 44.33190 to 43.37343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 43.1130 - MinusLogProbMetric: 43.1130 - val_loss: 43.3734 - val_MinusLogProbMetric: 43.3734 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 27/1000
2023-09-29 21:54:27.343 
Epoch 27/1000 
	 loss: 42.3985, MinusLogProbMetric: 42.3985, val_loss: 41.5057, val_MinusLogProbMetric: 41.5057

Epoch 27: val_loss improved from 43.37343 to 41.50571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 42.3985 - MinusLogProbMetric: 42.3985 - val_loss: 41.5057 - val_MinusLogProbMetric: 41.5057 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 28/1000
2023-09-29 21:55:02.541 
Epoch 28/1000 
	 loss: 41.8789, MinusLogProbMetric: 41.8789, val_loss: 43.6421, val_MinusLogProbMetric: 43.6421

Epoch 28: val_loss did not improve from 41.50571
196/196 - 35s - loss: 41.8789 - MinusLogProbMetric: 41.8789 - val_loss: 43.6421 - val_MinusLogProbMetric: 43.6421 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 29/1000
2023-09-29 21:55:36.269 
Epoch 29/1000 
	 loss: 41.2659, MinusLogProbMetric: 41.2659, val_loss: 41.5915, val_MinusLogProbMetric: 41.5915

Epoch 29: val_loss did not improve from 41.50571
196/196 - 34s - loss: 41.2659 - MinusLogProbMetric: 41.2659 - val_loss: 41.5915 - val_MinusLogProbMetric: 41.5915 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 30/1000
2023-09-29 21:56:10.876 
Epoch 30/1000 
	 loss: 40.7060, MinusLogProbMetric: 40.7060, val_loss: 40.8425, val_MinusLogProbMetric: 40.8425

Epoch 30: val_loss improved from 41.50571 to 40.84253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 40.7060 - MinusLogProbMetric: 40.7060 - val_loss: 40.8425 - val_MinusLogProbMetric: 40.8425 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 31/1000
2023-09-29 21:56:45.345 
Epoch 31/1000 
	 loss: 50.5671, MinusLogProbMetric: 50.5671, val_loss: 60.9613, val_MinusLogProbMetric: 60.9613

Epoch 31: val_loss did not improve from 40.84253
196/196 - 34s - loss: 50.5671 - MinusLogProbMetric: 50.5671 - val_loss: 60.9613 - val_MinusLogProbMetric: 60.9613 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 32/1000
2023-09-29 21:57:18.950 
Epoch 32/1000 
	 loss: 45.2034, MinusLogProbMetric: 45.2034, val_loss: 41.0383, val_MinusLogProbMetric: 41.0383

Epoch 32: val_loss did not improve from 40.84253
196/196 - 34s - loss: 45.2034 - MinusLogProbMetric: 45.2034 - val_loss: 41.0383 - val_MinusLogProbMetric: 41.0383 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 33/1000
2023-09-29 21:57:51.813 
Epoch 33/1000 
	 loss: 40.1183, MinusLogProbMetric: 40.1183, val_loss: 40.6038, val_MinusLogProbMetric: 40.6038

Epoch 33: val_loss improved from 40.84253 to 40.60380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 40.1183 - MinusLogProbMetric: 40.1183 - val_loss: 40.6038 - val_MinusLogProbMetric: 40.6038 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 34/1000
2023-09-29 21:58:26.995 
Epoch 34/1000 
	 loss: 39.4040, MinusLogProbMetric: 39.4040, val_loss: 39.2181, val_MinusLogProbMetric: 39.2181

Epoch 34: val_loss improved from 40.60380 to 39.21807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 39.4040 - MinusLogProbMetric: 39.4040 - val_loss: 39.2181 - val_MinusLogProbMetric: 39.2181 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 35/1000
2023-09-29 21:59:01.699 
Epoch 35/1000 
	 loss: 39.1633, MinusLogProbMetric: 39.1633, val_loss: 40.1442, val_MinusLogProbMetric: 40.1442

Epoch 35: val_loss did not improve from 39.21807
196/196 - 34s - loss: 39.1633 - MinusLogProbMetric: 39.1633 - val_loss: 40.1442 - val_MinusLogProbMetric: 40.1442 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 36/1000
2023-09-29 21:59:35.358 
Epoch 36/1000 
	 loss: 38.8925, MinusLogProbMetric: 38.8925, val_loss: 38.0550, val_MinusLogProbMetric: 38.0550

Epoch 36: val_loss improved from 39.21807 to 38.05497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 38.8925 - MinusLogProbMetric: 38.8925 - val_loss: 38.0550 - val_MinusLogProbMetric: 38.0550 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 37/1000
2023-09-29 22:00:09.989 
Epoch 37/1000 
	 loss: 38.3672, MinusLogProbMetric: 38.3672, val_loss: 39.0078, val_MinusLogProbMetric: 39.0078

Epoch 37: val_loss did not improve from 38.05497
196/196 - 34s - loss: 38.3672 - MinusLogProbMetric: 38.3672 - val_loss: 39.0078 - val_MinusLogProbMetric: 39.0078 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 38/1000
2023-09-29 22:00:42.209 
Epoch 38/1000 
	 loss: 38.0803, MinusLogProbMetric: 38.0803, val_loss: 38.0877, val_MinusLogProbMetric: 38.0877

Epoch 38: val_loss did not improve from 38.05497
196/196 - 32s - loss: 38.0803 - MinusLogProbMetric: 38.0803 - val_loss: 38.0877 - val_MinusLogProbMetric: 38.0877 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 39/1000
2023-09-29 22:01:15.668 
Epoch 39/1000 
	 loss: 37.7392, MinusLogProbMetric: 37.7392, val_loss: 38.0638, val_MinusLogProbMetric: 38.0638

Epoch 39: val_loss did not improve from 38.05497
196/196 - 33s - loss: 37.7392 - MinusLogProbMetric: 37.7392 - val_loss: 38.0638 - val_MinusLogProbMetric: 38.0638 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 40/1000
2023-09-29 22:01:48.752 
Epoch 40/1000 
	 loss: 53.2901, MinusLogProbMetric: 53.2901, val_loss: 40.0912, val_MinusLogProbMetric: 40.0912

Epoch 40: val_loss did not improve from 38.05497
196/196 - 33s - loss: 53.2901 - MinusLogProbMetric: 53.2901 - val_loss: 40.0912 - val_MinusLogProbMetric: 40.0912 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 41/1000
2023-09-29 22:02:23.207 
Epoch 41/1000 
	 loss: 50.3765, MinusLogProbMetric: 50.3765, val_loss: 44.1077, val_MinusLogProbMetric: 44.1077

Epoch 41: val_loss did not improve from 38.05497
196/196 - 34s - loss: 50.3765 - MinusLogProbMetric: 50.3765 - val_loss: 44.1077 - val_MinusLogProbMetric: 44.1077 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 42/1000
2023-09-29 22:02:57.261 
Epoch 42/1000 
	 loss: 40.5079, MinusLogProbMetric: 40.5079, val_loss: 38.8376, val_MinusLogProbMetric: 38.8376

Epoch 42: val_loss did not improve from 38.05497
196/196 - 34s - loss: 40.5079 - MinusLogProbMetric: 40.5079 - val_loss: 38.8376 - val_MinusLogProbMetric: 38.8376 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 43/1000
2023-09-29 22:03:31.081 
Epoch 43/1000 
	 loss: 37.4250, MinusLogProbMetric: 37.4250, val_loss: 37.7520, val_MinusLogProbMetric: 37.7520

Epoch 43: val_loss improved from 38.05497 to 37.75202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 37.4250 - MinusLogProbMetric: 37.4250 - val_loss: 37.7520 - val_MinusLogProbMetric: 37.7520 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 44/1000
2023-09-29 22:04:04.466 
Epoch 44/1000 
	 loss: 36.9307, MinusLogProbMetric: 36.9307, val_loss: 37.6185, val_MinusLogProbMetric: 37.6185

Epoch 44: val_loss improved from 37.75202 to 37.61845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 36.9307 - MinusLogProbMetric: 36.9307 - val_loss: 37.6185 - val_MinusLogProbMetric: 37.6185 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 45/1000
2023-09-29 22:04:38.449 
Epoch 45/1000 
	 loss: 36.6340, MinusLogProbMetric: 36.6340, val_loss: 37.2661, val_MinusLogProbMetric: 37.2661

Epoch 45: val_loss improved from 37.61845 to 37.26609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 36.6340 - MinusLogProbMetric: 36.6340 - val_loss: 37.2661 - val_MinusLogProbMetric: 37.2661 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 46/1000
2023-09-29 22:05:12.747 
Epoch 46/1000 
	 loss: 36.4389, MinusLogProbMetric: 36.4389, val_loss: 36.1610, val_MinusLogProbMetric: 36.1610

Epoch 46: val_loss improved from 37.26609 to 36.16101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 36.4389 - MinusLogProbMetric: 36.4389 - val_loss: 36.1610 - val_MinusLogProbMetric: 36.1610 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 47/1000
2023-09-29 22:05:48.201 
Epoch 47/1000 
	 loss: 36.1509, MinusLogProbMetric: 36.1509, val_loss: 35.7565, val_MinusLogProbMetric: 35.7565

Epoch 47: val_loss improved from 36.16101 to 35.75647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 36.1509 - MinusLogProbMetric: 36.1509 - val_loss: 35.7565 - val_MinusLogProbMetric: 35.7565 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 48/1000
2023-09-29 22:06:21.363 
Epoch 48/1000 
	 loss: 35.9038, MinusLogProbMetric: 35.9038, val_loss: 36.3905, val_MinusLogProbMetric: 36.3905

Epoch 48: val_loss did not improve from 35.75647
196/196 - 33s - loss: 35.9038 - MinusLogProbMetric: 35.9038 - val_loss: 36.3905 - val_MinusLogProbMetric: 36.3905 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 49/1000
2023-09-29 22:06:55.989 
Epoch 49/1000 
	 loss: 35.8711, MinusLogProbMetric: 35.8711, val_loss: 36.9447, val_MinusLogProbMetric: 36.9447

Epoch 49: val_loss did not improve from 35.75647
196/196 - 35s - loss: 35.8711 - MinusLogProbMetric: 35.8711 - val_loss: 36.9447 - val_MinusLogProbMetric: 36.9447 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 50/1000
2023-09-29 22:07:28.462 
Epoch 50/1000 
	 loss: 35.6901, MinusLogProbMetric: 35.6901, val_loss: 35.4852, val_MinusLogProbMetric: 35.4852

Epoch 50: val_loss improved from 35.75647 to 35.48524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 35.6901 - MinusLogProbMetric: 35.6901 - val_loss: 35.4852 - val_MinusLogProbMetric: 35.4852 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 51/1000
2023-09-29 22:08:01.799 
Epoch 51/1000 
	 loss: 35.6441, MinusLogProbMetric: 35.6441, val_loss: 35.5889, val_MinusLogProbMetric: 35.5889

Epoch 51: val_loss did not improve from 35.48524
196/196 - 33s - loss: 35.6441 - MinusLogProbMetric: 35.6441 - val_loss: 35.5889 - val_MinusLogProbMetric: 35.5889 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 52/1000
2023-09-29 22:08:35.325 
Epoch 52/1000 
	 loss: 35.4312, MinusLogProbMetric: 35.4312, val_loss: 35.4930, val_MinusLogProbMetric: 35.4930

Epoch 52: val_loss did not improve from 35.48524
196/196 - 34s - loss: 35.4312 - MinusLogProbMetric: 35.4312 - val_loss: 35.4930 - val_MinusLogProbMetric: 35.4930 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 53/1000
2023-09-29 22:09:06.832 
Epoch 53/1000 
	 loss: 35.2612, MinusLogProbMetric: 35.2612, val_loss: 36.1826, val_MinusLogProbMetric: 36.1826

Epoch 53: val_loss did not improve from 35.48524
196/196 - 32s - loss: 35.2612 - MinusLogProbMetric: 35.2612 - val_loss: 36.1826 - val_MinusLogProbMetric: 36.1826 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 54/1000
2023-09-29 22:09:40.953 
Epoch 54/1000 
	 loss: 35.1787, MinusLogProbMetric: 35.1787, val_loss: 36.3462, val_MinusLogProbMetric: 36.3462

Epoch 54: val_loss did not improve from 35.48524
196/196 - 34s - loss: 35.1787 - MinusLogProbMetric: 35.1787 - val_loss: 36.3462 - val_MinusLogProbMetric: 36.3462 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 55/1000
2023-09-29 22:10:14.977 
Epoch 55/1000 
	 loss: 35.0278, MinusLogProbMetric: 35.0278, val_loss: 35.1953, val_MinusLogProbMetric: 35.1953

Epoch 55: val_loss improved from 35.48524 to 35.19529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 35.0278 - MinusLogProbMetric: 35.0278 - val_loss: 35.1953 - val_MinusLogProbMetric: 35.1953 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 56/1000
2023-09-29 22:10:46.335 
Epoch 56/1000 
	 loss: 34.9897, MinusLogProbMetric: 34.9897, val_loss: 35.3392, val_MinusLogProbMetric: 35.3392

Epoch 56: val_loss did not improve from 35.19529
196/196 - 31s - loss: 34.9897 - MinusLogProbMetric: 34.9897 - val_loss: 35.3392 - val_MinusLogProbMetric: 35.3392 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 57/1000
2023-09-29 22:11:19.787 
Epoch 57/1000 
	 loss: 34.8191, MinusLogProbMetric: 34.8191, val_loss: 34.4784, val_MinusLogProbMetric: 34.4784

Epoch 57: val_loss improved from 35.19529 to 34.47839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 34.8191 - MinusLogProbMetric: 34.8191 - val_loss: 34.4784 - val_MinusLogProbMetric: 34.4784 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 58/1000
2023-09-29 22:11:53.980 
Epoch 58/1000 
	 loss: 34.6365, MinusLogProbMetric: 34.6365, val_loss: 34.7136, val_MinusLogProbMetric: 34.7136

Epoch 58: val_loss did not improve from 34.47839
196/196 - 34s - loss: 34.6365 - MinusLogProbMetric: 34.6365 - val_loss: 34.7136 - val_MinusLogProbMetric: 34.7136 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 59/1000
2023-09-29 22:12:26.649 
Epoch 59/1000 
	 loss: 34.6820, MinusLogProbMetric: 34.6820, val_loss: 34.8106, val_MinusLogProbMetric: 34.8106

Epoch 59: val_loss did not improve from 34.47839
196/196 - 33s - loss: 34.6820 - MinusLogProbMetric: 34.6820 - val_loss: 34.8106 - val_MinusLogProbMetric: 34.8106 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 60/1000
2023-09-29 22:12:59.816 
Epoch 60/1000 
	 loss: 34.4934, MinusLogProbMetric: 34.4934, val_loss: 35.5459, val_MinusLogProbMetric: 35.5459

Epoch 60: val_loss did not improve from 34.47839
196/196 - 33s - loss: 34.4934 - MinusLogProbMetric: 34.4934 - val_loss: 35.5459 - val_MinusLogProbMetric: 35.5459 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 61/1000
2023-09-29 22:13:34.523 
Epoch 61/1000 
	 loss: 34.7004, MinusLogProbMetric: 34.7004, val_loss: 34.0923, val_MinusLogProbMetric: 34.0923

Epoch 61: val_loss improved from 34.47839 to 34.09227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 34.7004 - MinusLogProbMetric: 34.7004 - val_loss: 34.0923 - val_MinusLogProbMetric: 34.0923 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 62/1000
2023-09-29 22:14:09.496 
Epoch 62/1000 
	 loss: 34.4557, MinusLogProbMetric: 34.4557, val_loss: 35.5731, val_MinusLogProbMetric: 35.5731

Epoch 62: val_loss did not improve from 34.09227
196/196 - 34s - loss: 34.4557 - MinusLogProbMetric: 34.4557 - val_loss: 35.5731 - val_MinusLogProbMetric: 35.5731 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 63/1000
2023-09-29 22:14:42.653 
Epoch 63/1000 
	 loss: 34.2114, MinusLogProbMetric: 34.2114, val_loss: 34.2349, val_MinusLogProbMetric: 34.2349

Epoch 63: val_loss did not improve from 34.09227
196/196 - 33s - loss: 34.2114 - MinusLogProbMetric: 34.2114 - val_loss: 34.2349 - val_MinusLogProbMetric: 34.2349 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 64/1000
2023-09-29 22:15:17.168 
Epoch 64/1000 
	 loss: 34.1701, MinusLogProbMetric: 34.1701, val_loss: 34.5193, val_MinusLogProbMetric: 34.5193

Epoch 64: val_loss did not improve from 34.09227
196/196 - 35s - loss: 34.1701 - MinusLogProbMetric: 34.1701 - val_loss: 34.5193 - val_MinusLogProbMetric: 34.5193 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 65/1000
2023-09-29 22:15:51.061 
Epoch 65/1000 
	 loss: 34.0461, MinusLogProbMetric: 34.0461, val_loss: 33.8380, val_MinusLogProbMetric: 33.8380

Epoch 65: val_loss improved from 34.09227 to 33.83799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 34.0461 - MinusLogProbMetric: 34.0461 - val_loss: 33.8380 - val_MinusLogProbMetric: 33.8380 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 66/1000
2023-09-29 22:16:25.200 
Epoch 66/1000 
	 loss: 33.9151, MinusLogProbMetric: 33.9151, val_loss: 33.6866, val_MinusLogProbMetric: 33.6866

Epoch 66: val_loss improved from 33.83799 to 33.68662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 33.9151 - MinusLogProbMetric: 33.9151 - val_loss: 33.6866 - val_MinusLogProbMetric: 33.6866 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 67/1000
2023-09-29 22:16:59.651 
Epoch 67/1000 
	 loss: 33.9587, MinusLogProbMetric: 33.9587, val_loss: 34.0271, val_MinusLogProbMetric: 34.0271

Epoch 67: val_loss did not improve from 33.68662
196/196 - 34s - loss: 33.9587 - MinusLogProbMetric: 33.9587 - val_loss: 34.0271 - val_MinusLogProbMetric: 34.0271 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 68/1000
2023-09-29 22:17:32.556 
Epoch 68/1000 
	 loss: 34.1168, MinusLogProbMetric: 34.1168, val_loss: 34.5104, val_MinusLogProbMetric: 34.5104

Epoch 68: val_loss did not improve from 33.68662
196/196 - 33s - loss: 34.1168 - MinusLogProbMetric: 34.1168 - val_loss: 34.5104 - val_MinusLogProbMetric: 34.5104 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 69/1000
2023-09-29 22:18:05.733 
Epoch 69/1000 
	 loss: 33.9068, MinusLogProbMetric: 33.9068, val_loss: 34.5988, val_MinusLogProbMetric: 34.5988

Epoch 69: val_loss did not improve from 33.68662
196/196 - 33s - loss: 33.9068 - MinusLogProbMetric: 33.9068 - val_loss: 34.5988 - val_MinusLogProbMetric: 34.5988 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 70/1000
2023-09-29 22:18:38.965 
Epoch 70/1000 
	 loss: 33.6692, MinusLogProbMetric: 33.6692, val_loss: 34.1224, val_MinusLogProbMetric: 34.1224

Epoch 70: val_loss did not improve from 33.68662
196/196 - 33s - loss: 33.6692 - MinusLogProbMetric: 33.6692 - val_loss: 34.1224 - val_MinusLogProbMetric: 34.1224 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 71/1000
2023-09-29 22:19:13.478 
Epoch 71/1000 
	 loss: 33.6404, MinusLogProbMetric: 33.6404, val_loss: 33.5506, val_MinusLogProbMetric: 33.5506

Epoch 71: val_loss improved from 33.68662 to 33.55061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 33.6404 - MinusLogProbMetric: 33.6404 - val_loss: 33.5506 - val_MinusLogProbMetric: 33.5506 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 72/1000
2023-09-29 22:19:48.962 
Epoch 72/1000 
	 loss: 33.4945, MinusLogProbMetric: 33.4945, val_loss: 33.8968, val_MinusLogProbMetric: 33.8968

Epoch 72: val_loss did not improve from 33.55061
196/196 - 35s - loss: 33.4945 - MinusLogProbMetric: 33.4945 - val_loss: 33.8968 - val_MinusLogProbMetric: 33.8968 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 73/1000
2023-09-29 22:20:22.100 
Epoch 73/1000 
	 loss: 33.5140, MinusLogProbMetric: 33.5140, val_loss: 33.4617, val_MinusLogProbMetric: 33.4617

Epoch 73: val_loss improved from 33.55061 to 33.46167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 33.5140 - MinusLogProbMetric: 33.5140 - val_loss: 33.4617 - val_MinusLogProbMetric: 33.4617 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 74/1000
2023-09-29 22:20:55.590 
Epoch 74/1000 
	 loss: 33.5564, MinusLogProbMetric: 33.5564, val_loss: 34.0913, val_MinusLogProbMetric: 34.0913

Epoch 74: val_loss did not improve from 33.46167
196/196 - 33s - loss: 33.5564 - MinusLogProbMetric: 33.5564 - val_loss: 34.0913 - val_MinusLogProbMetric: 34.0913 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 75/1000
2023-09-29 22:21:30.115 
Epoch 75/1000 
	 loss: 33.3777, MinusLogProbMetric: 33.3777, val_loss: 33.3549, val_MinusLogProbMetric: 33.3549

Epoch 75: val_loss improved from 33.46167 to 33.35493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 33.3777 - MinusLogProbMetric: 33.3777 - val_loss: 33.3549 - val_MinusLogProbMetric: 33.3549 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 76/1000
2023-09-29 22:22:05.336 
Epoch 76/1000 
	 loss: 33.4068, MinusLogProbMetric: 33.4068, val_loss: 32.8296, val_MinusLogProbMetric: 32.8296

Epoch 76: val_loss improved from 33.35493 to 32.82957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 33.4068 - MinusLogProbMetric: 33.4068 - val_loss: 32.8296 - val_MinusLogProbMetric: 32.8296 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 77/1000
2023-09-29 22:22:38.779 
Epoch 77/1000 
	 loss: 33.2906, MinusLogProbMetric: 33.2906, val_loss: 35.0075, val_MinusLogProbMetric: 35.0075

Epoch 77: val_loss did not improve from 32.82957
196/196 - 33s - loss: 33.2906 - MinusLogProbMetric: 33.2906 - val_loss: 35.0075 - val_MinusLogProbMetric: 35.0075 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 78/1000
2023-09-29 22:23:12.098 
Epoch 78/1000 
	 loss: 33.3773, MinusLogProbMetric: 33.3773, val_loss: 33.5691, val_MinusLogProbMetric: 33.5691

Epoch 78: val_loss did not improve from 32.82957
196/196 - 33s - loss: 33.3773 - MinusLogProbMetric: 33.3773 - val_loss: 33.5691 - val_MinusLogProbMetric: 33.5691 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 79/1000
2023-09-29 22:23:46.435 
Epoch 79/1000 
	 loss: 33.2049, MinusLogProbMetric: 33.2049, val_loss: 33.3905, val_MinusLogProbMetric: 33.3905

Epoch 79: val_loss did not improve from 32.82957
196/196 - 34s - loss: 33.2049 - MinusLogProbMetric: 33.2049 - val_loss: 33.3905 - val_MinusLogProbMetric: 33.3905 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 80/1000
2023-09-29 22:24:20.674 
Epoch 80/1000 
	 loss: 33.1728, MinusLogProbMetric: 33.1728, val_loss: 33.5303, val_MinusLogProbMetric: 33.5303

Epoch 80: val_loss did not improve from 32.82957
196/196 - 34s - loss: 33.1728 - MinusLogProbMetric: 33.1728 - val_loss: 33.5303 - val_MinusLogProbMetric: 33.5303 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 81/1000
2023-09-29 22:24:55.269 
Epoch 81/1000 
	 loss: 33.1671, MinusLogProbMetric: 33.1671, val_loss: 33.8072, val_MinusLogProbMetric: 33.8072

Epoch 81: val_loss did not improve from 32.82957
196/196 - 35s - loss: 33.1671 - MinusLogProbMetric: 33.1671 - val_loss: 33.8072 - val_MinusLogProbMetric: 33.8072 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 82/1000
2023-09-29 22:25:29.598 
Epoch 82/1000 
	 loss: 33.1731, MinusLogProbMetric: 33.1731, val_loss: 32.7742, val_MinusLogProbMetric: 32.7742

Epoch 82: val_loss improved from 32.82957 to 32.77419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 33.1731 - MinusLogProbMetric: 33.1731 - val_loss: 32.7742 - val_MinusLogProbMetric: 32.7742 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 83/1000
2023-09-29 22:26:01.474 
Epoch 83/1000 
	 loss: 33.0924, MinusLogProbMetric: 33.0924, val_loss: 33.4131, val_MinusLogProbMetric: 33.4131

Epoch 83: val_loss did not improve from 32.77419
196/196 - 31s - loss: 33.0924 - MinusLogProbMetric: 33.0924 - val_loss: 33.4131 - val_MinusLogProbMetric: 33.4131 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 84/1000
2023-09-29 22:26:35.987 
Epoch 84/1000 
	 loss: 32.8867, MinusLogProbMetric: 32.8867, val_loss: 32.4506, val_MinusLogProbMetric: 32.4506

Epoch 84: val_loss improved from 32.77419 to 32.45055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 32.8867 - MinusLogProbMetric: 32.8867 - val_loss: 32.4506 - val_MinusLogProbMetric: 32.4506 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 85/1000
2023-09-29 22:27:07.800 
Epoch 85/1000 
	 loss: 34.6633, MinusLogProbMetric: 34.6633, val_loss: 32.5280, val_MinusLogProbMetric: 32.5280

Epoch 85: val_loss did not improve from 32.45055
196/196 - 31s - loss: 34.6633 - MinusLogProbMetric: 34.6633 - val_loss: 32.5280 - val_MinusLogProbMetric: 32.5280 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 86/1000
2023-09-29 22:27:40.456 
Epoch 86/1000 
	 loss: 32.7657, MinusLogProbMetric: 32.7657, val_loss: 34.2468, val_MinusLogProbMetric: 34.2468

Epoch 86: val_loss did not improve from 32.45055
196/196 - 33s - loss: 32.7657 - MinusLogProbMetric: 32.7657 - val_loss: 34.2468 - val_MinusLogProbMetric: 34.2468 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 87/1000
2023-09-29 22:28:15.050 
Epoch 87/1000 
	 loss: 32.7474, MinusLogProbMetric: 32.7474, val_loss: 33.8210, val_MinusLogProbMetric: 33.8210

Epoch 87: val_loss did not improve from 32.45055
196/196 - 35s - loss: 32.7474 - MinusLogProbMetric: 32.7474 - val_loss: 33.8210 - val_MinusLogProbMetric: 33.8210 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 88/1000
2023-09-29 22:28:47.113 
Epoch 88/1000 
	 loss: 32.7464, MinusLogProbMetric: 32.7464, val_loss: 32.6328, val_MinusLogProbMetric: 32.6328

Epoch 88: val_loss did not improve from 32.45055
196/196 - 32s - loss: 32.7464 - MinusLogProbMetric: 32.7464 - val_loss: 32.6328 - val_MinusLogProbMetric: 32.6328 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 89/1000
2023-09-29 22:29:17.618 
Epoch 89/1000 
	 loss: 32.8803, MinusLogProbMetric: 32.8803, val_loss: 33.1181, val_MinusLogProbMetric: 33.1181

Epoch 89: val_loss did not improve from 32.45055
196/196 - 31s - loss: 32.8803 - MinusLogProbMetric: 32.8803 - val_loss: 33.1181 - val_MinusLogProbMetric: 33.1181 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 90/1000
2023-09-29 22:29:52.216 
Epoch 90/1000 
	 loss: 32.6615, MinusLogProbMetric: 32.6615, val_loss: 33.1412, val_MinusLogProbMetric: 33.1412

Epoch 90: val_loss did not improve from 32.45055
196/196 - 35s - loss: 32.6615 - MinusLogProbMetric: 32.6615 - val_loss: 33.1412 - val_MinusLogProbMetric: 33.1412 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 91/1000
2023-09-29 22:30:24.610 
Epoch 91/1000 
	 loss: 32.8301, MinusLogProbMetric: 32.8301, val_loss: 33.1404, val_MinusLogProbMetric: 33.1404

Epoch 91: val_loss did not improve from 32.45055
196/196 - 32s - loss: 32.8301 - MinusLogProbMetric: 32.8301 - val_loss: 33.1404 - val_MinusLogProbMetric: 33.1404 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 92/1000
2023-09-29 22:30:57.696 
Epoch 92/1000 
	 loss: 32.5355, MinusLogProbMetric: 32.5355, val_loss: 33.4306, val_MinusLogProbMetric: 33.4306

Epoch 92: val_loss did not improve from 32.45055
196/196 - 33s - loss: 32.5355 - MinusLogProbMetric: 32.5355 - val_loss: 33.4306 - val_MinusLogProbMetric: 33.4306 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 93/1000
2023-09-29 22:31:29.656 
Epoch 93/1000 
	 loss: 32.7496, MinusLogProbMetric: 32.7496, val_loss: 32.1889, val_MinusLogProbMetric: 32.1889

Epoch 93: val_loss improved from 32.45055 to 32.18893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 32s - loss: 32.7496 - MinusLogProbMetric: 32.7496 - val_loss: 32.1889 - val_MinusLogProbMetric: 32.1889 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 94/1000
2023-09-29 22:32:02.596 
Epoch 94/1000 
	 loss: 32.5545, MinusLogProbMetric: 32.5545, val_loss: 33.3133, val_MinusLogProbMetric: 33.3133

Epoch 94: val_loss did not improve from 32.18893
196/196 - 33s - loss: 32.5545 - MinusLogProbMetric: 32.5545 - val_loss: 33.3133 - val_MinusLogProbMetric: 33.3133 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 95/1000
2023-09-29 22:32:35.221 
Epoch 95/1000 
	 loss: 32.4242, MinusLogProbMetric: 32.4242, val_loss: 35.3517, val_MinusLogProbMetric: 35.3517

Epoch 95: val_loss did not improve from 32.18893
196/196 - 33s - loss: 32.4242 - MinusLogProbMetric: 32.4242 - val_loss: 35.3517 - val_MinusLogProbMetric: 35.3517 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 96/1000
2023-09-29 22:33:06.612 
Epoch 96/1000 
	 loss: 32.7914, MinusLogProbMetric: 32.7914, val_loss: 32.7245, val_MinusLogProbMetric: 32.7245

Epoch 96: val_loss did not improve from 32.18893
196/196 - 31s - loss: 32.7914 - MinusLogProbMetric: 32.7914 - val_loss: 32.7245 - val_MinusLogProbMetric: 32.7245 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 97/1000
2023-09-29 22:33:38.431 
Epoch 97/1000 
	 loss: 32.4899, MinusLogProbMetric: 32.4899, val_loss: 31.7689, val_MinusLogProbMetric: 31.7689

Epoch 97: val_loss improved from 32.18893 to 31.76888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 32s - loss: 32.4899 - MinusLogProbMetric: 32.4899 - val_loss: 31.7689 - val_MinusLogProbMetric: 31.7689 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 98/1000
2023-09-29 22:34:12.048 
Epoch 98/1000 
	 loss: 32.3444, MinusLogProbMetric: 32.3444, val_loss: 33.2317, val_MinusLogProbMetric: 33.2317

Epoch 98: val_loss did not improve from 31.76888
196/196 - 33s - loss: 32.3444 - MinusLogProbMetric: 32.3444 - val_loss: 33.2317 - val_MinusLogProbMetric: 33.2317 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 99/1000
2023-09-29 22:34:45.629 
Epoch 99/1000 
	 loss: 32.5615, MinusLogProbMetric: 32.5615, val_loss: 32.7843, val_MinusLogProbMetric: 32.7843

Epoch 99: val_loss did not improve from 31.76888
196/196 - 34s - loss: 32.5615 - MinusLogProbMetric: 32.5615 - val_loss: 32.7843 - val_MinusLogProbMetric: 32.7843 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 100/1000
2023-09-29 22:35:20.047 
Epoch 100/1000 
	 loss: 32.2402, MinusLogProbMetric: 32.2402, val_loss: 33.7536, val_MinusLogProbMetric: 33.7536

Epoch 100: val_loss did not improve from 31.76888
196/196 - 34s - loss: 32.2402 - MinusLogProbMetric: 32.2402 - val_loss: 33.7536 - val_MinusLogProbMetric: 33.7536 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 101/1000
2023-09-29 22:35:54.310 
Epoch 101/1000 
	 loss: 32.3012, MinusLogProbMetric: 32.3012, val_loss: 32.2854, val_MinusLogProbMetric: 32.2854

Epoch 101: val_loss did not improve from 31.76888
196/196 - 34s - loss: 32.3012 - MinusLogProbMetric: 32.3012 - val_loss: 32.2854 - val_MinusLogProbMetric: 32.2854 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 102/1000
2023-09-29 22:36:26.653 
Epoch 102/1000 
	 loss: 32.4629, MinusLogProbMetric: 32.4629, val_loss: 31.7676, val_MinusLogProbMetric: 31.7676

Epoch 102: val_loss improved from 31.76888 to 31.76760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 32.4629 - MinusLogProbMetric: 32.4629 - val_loss: 31.7676 - val_MinusLogProbMetric: 31.7676 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 103/1000
2023-09-29 22:37:01.546 
Epoch 103/1000 
	 loss: 32.3337, MinusLogProbMetric: 32.3337, val_loss: 32.6509, val_MinusLogProbMetric: 32.6509

Epoch 103: val_loss did not improve from 31.76760
196/196 - 34s - loss: 32.3337 - MinusLogProbMetric: 32.3337 - val_loss: 32.6509 - val_MinusLogProbMetric: 32.6509 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 104/1000
2023-09-29 22:37:34.643 
Epoch 104/1000 
	 loss: 32.3430, MinusLogProbMetric: 32.3430, val_loss: 32.2883, val_MinusLogProbMetric: 32.2883

Epoch 104: val_loss did not improve from 31.76760
196/196 - 33s - loss: 32.3430 - MinusLogProbMetric: 32.3430 - val_loss: 32.2883 - val_MinusLogProbMetric: 32.2883 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 105/1000
2023-09-29 22:38:08.372 
Epoch 105/1000 
	 loss: 32.2628, MinusLogProbMetric: 32.2628, val_loss: 35.0234, val_MinusLogProbMetric: 35.0234

Epoch 105: val_loss did not improve from 31.76760
196/196 - 34s - loss: 32.2628 - MinusLogProbMetric: 32.2628 - val_loss: 35.0234 - val_MinusLogProbMetric: 35.0234 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 106/1000
2023-09-29 22:38:41.496 
Epoch 106/1000 
	 loss: 32.2830, MinusLogProbMetric: 32.2830, val_loss: 34.4884, val_MinusLogProbMetric: 34.4884

Epoch 106: val_loss did not improve from 31.76760
196/196 - 33s - loss: 32.2830 - MinusLogProbMetric: 32.2830 - val_loss: 34.4884 - val_MinusLogProbMetric: 34.4884 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 107/1000
2023-09-29 22:39:16.409 
Epoch 107/1000 
	 loss: 32.3424, MinusLogProbMetric: 32.3424, val_loss: 32.4203, val_MinusLogProbMetric: 32.4203

Epoch 107: val_loss did not improve from 31.76760
196/196 - 35s - loss: 32.3424 - MinusLogProbMetric: 32.3424 - val_loss: 32.4203 - val_MinusLogProbMetric: 32.4203 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 108/1000
2023-09-29 22:39:50.131 
Epoch 108/1000 
	 loss: 32.4331, MinusLogProbMetric: 32.4331, val_loss: 32.9615, val_MinusLogProbMetric: 32.9615

Epoch 108: val_loss did not improve from 31.76760
196/196 - 34s - loss: 32.4331 - MinusLogProbMetric: 32.4331 - val_loss: 32.9615 - val_MinusLogProbMetric: 32.9615 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 109/1000
2023-09-29 22:40:23.644 
Epoch 109/1000 
	 loss: 32.0728, MinusLogProbMetric: 32.0728, val_loss: 31.7954, val_MinusLogProbMetric: 31.7954

Epoch 109: val_loss did not improve from 31.76760
196/196 - 34s - loss: 32.0728 - MinusLogProbMetric: 32.0728 - val_loss: 31.7954 - val_MinusLogProbMetric: 31.7954 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 110/1000
2023-09-29 22:40:57.421 
Epoch 110/1000 
	 loss: 32.1570, MinusLogProbMetric: 32.1570, val_loss: 32.5671, val_MinusLogProbMetric: 32.5671

Epoch 110: val_loss did not improve from 31.76760
196/196 - 34s - loss: 32.1570 - MinusLogProbMetric: 32.1570 - val_loss: 32.5671 - val_MinusLogProbMetric: 32.5671 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 111/1000
2023-09-29 22:41:29.747 
Epoch 111/1000 
	 loss: 32.1478, MinusLogProbMetric: 32.1478, val_loss: 32.3162, val_MinusLogProbMetric: 32.3162

Epoch 111: val_loss did not improve from 31.76760
196/196 - 32s - loss: 32.1478 - MinusLogProbMetric: 32.1478 - val_loss: 32.3162 - val_MinusLogProbMetric: 32.3162 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 112/1000
2023-09-29 22:42:00.491 
Epoch 112/1000 
	 loss: 32.1436, MinusLogProbMetric: 32.1436, val_loss: 33.0333, val_MinusLogProbMetric: 33.0333

Epoch 112: val_loss did not improve from 31.76760
196/196 - 31s - loss: 32.1436 - MinusLogProbMetric: 32.1436 - val_loss: 33.0333 - val_MinusLogProbMetric: 33.0333 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 113/1000
2023-09-29 22:42:34.068 
Epoch 113/1000 
	 loss: 32.1552, MinusLogProbMetric: 32.1552, val_loss: 33.7358, val_MinusLogProbMetric: 33.7358

Epoch 113: val_loss did not improve from 31.76760
196/196 - 34s - loss: 32.1552 - MinusLogProbMetric: 32.1552 - val_loss: 33.7358 - val_MinusLogProbMetric: 33.7358 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 114/1000
2023-09-29 22:43:05.717 
Epoch 114/1000 
	 loss: 32.2613, MinusLogProbMetric: 32.2613, val_loss: 32.6448, val_MinusLogProbMetric: 32.6448

Epoch 114: val_loss did not improve from 31.76760
196/196 - 32s - loss: 32.2613 - MinusLogProbMetric: 32.2613 - val_loss: 32.6448 - val_MinusLogProbMetric: 32.6448 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 115/1000
2023-09-29 22:43:39.624 
Epoch 115/1000 
	 loss: 31.8319, MinusLogProbMetric: 31.8319, val_loss: 32.4163, val_MinusLogProbMetric: 32.4163

Epoch 115: val_loss did not improve from 31.76760
196/196 - 34s - loss: 31.8319 - MinusLogProbMetric: 31.8319 - val_loss: 32.4163 - val_MinusLogProbMetric: 32.4163 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 116/1000
2023-09-29 22:44:13.706 
Epoch 116/1000 
	 loss: 32.0557, MinusLogProbMetric: 32.0557, val_loss: 32.4135, val_MinusLogProbMetric: 32.4135

Epoch 116: val_loss did not improve from 31.76760
196/196 - 34s - loss: 32.0557 - MinusLogProbMetric: 32.0557 - val_loss: 32.4135 - val_MinusLogProbMetric: 32.4135 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 117/1000
2023-09-29 22:44:46.965 
Epoch 117/1000 
	 loss: 31.8570, MinusLogProbMetric: 31.8570, val_loss: 32.0367, val_MinusLogProbMetric: 32.0367

Epoch 117: val_loss did not improve from 31.76760
196/196 - 33s - loss: 31.8570 - MinusLogProbMetric: 31.8570 - val_loss: 32.0367 - val_MinusLogProbMetric: 32.0367 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 118/1000
2023-09-29 22:45:20.906 
Epoch 118/1000 
	 loss: 31.8748, MinusLogProbMetric: 31.8748, val_loss: 32.1678, val_MinusLogProbMetric: 32.1678

Epoch 118: val_loss did not improve from 31.76760
196/196 - 34s - loss: 31.8748 - MinusLogProbMetric: 31.8748 - val_loss: 32.1678 - val_MinusLogProbMetric: 32.1678 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 119/1000
2023-09-29 22:45:55.274 
Epoch 119/1000 
	 loss: 31.8372, MinusLogProbMetric: 31.8372, val_loss: 32.0542, val_MinusLogProbMetric: 32.0542

Epoch 119: val_loss did not improve from 31.76760
196/196 - 34s - loss: 31.8372 - MinusLogProbMetric: 31.8372 - val_loss: 32.0542 - val_MinusLogProbMetric: 32.0542 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 120/1000
2023-09-29 22:46:28.453 
Epoch 120/1000 
	 loss: 32.0759, MinusLogProbMetric: 32.0759, val_loss: 31.6509, val_MinusLogProbMetric: 31.6509

Epoch 120: val_loss improved from 31.76760 to 31.65093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 32.0759 - MinusLogProbMetric: 32.0759 - val_loss: 31.6509 - val_MinusLogProbMetric: 31.6509 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 121/1000
2023-09-29 22:47:02.133 
Epoch 121/1000 
	 loss: 31.8620, MinusLogProbMetric: 31.8620, val_loss: 33.0366, val_MinusLogProbMetric: 33.0366

Epoch 121: val_loss did not improve from 31.65093
196/196 - 33s - loss: 31.8620 - MinusLogProbMetric: 31.8620 - val_loss: 33.0366 - val_MinusLogProbMetric: 33.0366 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 122/1000
2023-09-29 22:47:35.101 
Epoch 122/1000 
	 loss: 32.1927, MinusLogProbMetric: 32.1927, val_loss: 34.0814, val_MinusLogProbMetric: 34.0814

Epoch 122: val_loss did not improve from 31.65093
196/196 - 33s - loss: 32.1927 - MinusLogProbMetric: 32.1927 - val_loss: 34.0814 - val_MinusLogProbMetric: 34.0814 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 123/1000
2023-09-29 22:48:09.498 
Epoch 123/1000 
	 loss: 31.7085, MinusLogProbMetric: 31.7085, val_loss: 32.6463, val_MinusLogProbMetric: 32.6463

Epoch 123: val_loss did not improve from 31.65093
196/196 - 34s - loss: 31.7085 - MinusLogProbMetric: 31.7085 - val_loss: 32.6463 - val_MinusLogProbMetric: 32.6463 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 124/1000
2023-09-29 22:48:44.149 
Epoch 124/1000 
	 loss: 31.7447, MinusLogProbMetric: 31.7447, val_loss: 32.0891, val_MinusLogProbMetric: 32.0891

Epoch 124: val_loss did not improve from 31.65093
196/196 - 35s - loss: 31.7447 - MinusLogProbMetric: 31.7447 - val_loss: 32.0891 - val_MinusLogProbMetric: 32.0891 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 125/1000
2023-09-29 22:49:15.618 
Epoch 125/1000 
	 loss: 32.0034, MinusLogProbMetric: 32.0034, val_loss: 32.5300, val_MinusLogProbMetric: 32.5300

Epoch 125: val_loss did not improve from 31.65093
196/196 - 31s - loss: 32.0034 - MinusLogProbMetric: 32.0034 - val_loss: 32.5300 - val_MinusLogProbMetric: 32.5300 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 126/1000
2023-09-29 22:49:48.100 
Epoch 126/1000 
	 loss: 31.7928, MinusLogProbMetric: 31.7928, val_loss: 31.3623, val_MinusLogProbMetric: 31.3623

Epoch 126: val_loss improved from 31.65093 to 31.36234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 31.7928 - MinusLogProbMetric: 31.7928 - val_loss: 31.3623 - val_MinusLogProbMetric: 31.3623 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 127/1000
2023-09-29 22:50:20.845 
Epoch 127/1000 
	 loss: 31.7336, MinusLogProbMetric: 31.7336, val_loss: 32.0224, val_MinusLogProbMetric: 32.0224

Epoch 127: val_loss did not improve from 31.36234
196/196 - 32s - loss: 31.7336 - MinusLogProbMetric: 31.7336 - val_loss: 32.0224 - val_MinusLogProbMetric: 32.0224 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 128/1000
2023-09-29 22:50:55.165 
Epoch 128/1000 
	 loss: 31.6773, MinusLogProbMetric: 31.6773, val_loss: 32.2801, val_MinusLogProbMetric: 32.2801

Epoch 128: val_loss did not improve from 31.36234
196/196 - 34s - loss: 31.6773 - MinusLogProbMetric: 31.6773 - val_loss: 32.2801 - val_MinusLogProbMetric: 32.2801 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 129/1000
2023-09-29 22:51:28.769 
Epoch 129/1000 
	 loss: 31.8235, MinusLogProbMetric: 31.8235, val_loss: 32.5029, val_MinusLogProbMetric: 32.5029

Epoch 129: val_loss did not improve from 31.36234
196/196 - 34s - loss: 31.8235 - MinusLogProbMetric: 31.8235 - val_loss: 32.5029 - val_MinusLogProbMetric: 32.5029 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 130/1000
2023-09-29 22:52:02.913 
Epoch 130/1000 
	 loss: 31.7942, MinusLogProbMetric: 31.7942, val_loss: 32.2470, val_MinusLogProbMetric: 32.2470

Epoch 130: val_loss did not improve from 31.36234
196/196 - 34s - loss: 31.7942 - MinusLogProbMetric: 31.7942 - val_loss: 32.2470 - val_MinusLogProbMetric: 32.2470 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 131/1000
2023-09-29 22:52:36.908 
Epoch 131/1000 
	 loss: 31.6179, MinusLogProbMetric: 31.6179, val_loss: 31.5349, val_MinusLogProbMetric: 31.5349

Epoch 131: val_loss did not improve from 31.36234
196/196 - 34s - loss: 31.6179 - MinusLogProbMetric: 31.6179 - val_loss: 31.5349 - val_MinusLogProbMetric: 31.5349 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 132/1000
2023-09-29 22:53:09.672 
Epoch 132/1000 
	 loss: 31.8511, MinusLogProbMetric: 31.8511, val_loss: 31.8218, val_MinusLogProbMetric: 31.8218

Epoch 132: val_loss did not improve from 31.36234
196/196 - 33s - loss: 31.8511 - MinusLogProbMetric: 31.8511 - val_loss: 31.8218 - val_MinusLogProbMetric: 31.8218 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 133/1000
2023-09-29 22:53:43.352 
Epoch 133/1000 
	 loss: 31.5951, MinusLogProbMetric: 31.5951, val_loss: 31.9602, val_MinusLogProbMetric: 31.9602

Epoch 133: val_loss did not improve from 31.36234
196/196 - 34s - loss: 31.5951 - MinusLogProbMetric: 31.5951 - val_loss: 31.9602 - val_MinusLogProbMetric: 31.9602 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 134/1000
2023-09-29 22:54:16.716 
Epoch 134/1000 
	 loss: 31.5690, MinusLogProbMetric: 31.5690, val_loss: 32.1279, val_MinusLogProbMetric: 32.1279

Epoch 134: val_loss did not improve from 31.36234
196/196 - 33s - loss: 31.5690 - MinusLogProbMetric: 31.5690 - val_loss: 32.1279 - val_MinusLogProbMetric: 32.1279 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 135/1000
2023-09-29 22:54:51.048 
Epoch 135/1000 
	 loss: 31.7187, MinusLogProbMetric: 31.7187, val_loss: 31.3896, val_MinusLogProbMetric: 31.3896

Epoch 135: val_loss did not improve from 31.36234
196/196 - 34s - loss: 31.7187 - MinusLogProbMetric: 31.7187 - val_loss: 31.3896 - val_MinusLogProbMetric: 31.3896 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 136/1000
2023-09-29 22:55:25.118 
Epoch 136/1000 
	 loss: 31.5192, MinusLogProbMetric: 31.5192, val_loss: 31.3352, val_MinusLogProbMetric: 31.3352

Epoch 136: val_loss improved from 31.36234 to 31.33517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 31.5192 - MinusLogProbMetric: 31.5192 - val_loss: 31.3352 - val_MinusLogProbMetric: 31.3352 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 137/1000
2023-09-29 22:56:00.313 
Epoch 137/1000 
	 loss: 31.8091, MinusLogProbMetric: 31.8091, val_loss: 32.5118, val_MinusLogProbMetric: 32.5118

Epoch 137: val_loss did not improve from 31.33517
196/196 - 35s - loss: 31.8091 - MinusLogProbMetric: 31.8091 - val_loss: 32.5118 - val_MinusLogProbMetric: 32.5118 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 138/1000
2023-09-29 22:56:33.028 
Epoch 138/1000 
	 loss: 31.3962, MinusLogProbMetric: 31.3962, val_loss: 31.7667, val_MinusLogProbMetric: 31.7667

Epoch 138: val_loss did not improve from 31.33517
196/196 - 33s - loss: 31.3962 - MinusLogProbMetric: 31.3962 - val_loss: 31.7667 - val_MinusLogProbMetric: 31.7667 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 139/1000
2023-09-29 22:57:07.345 
Epoch 139/1000 
	 loss: 31.6196, MinusLogProbMetric: 31.6196, val_loss: 31.4026, val_MinusLogProbMetric: 31.4026

Epoch 139: val_loss did not improve from 31.33517
196/196 - 34s - loss: 31.6196 - MinusLogProbMetric: 31.6196 - val_loss: 31.4026 - val_MinusLogProbMetric: 31.4026 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 140/1000
2023-09-29 22:57:41.433 
Epoch 140/1000 
	 loss: 31.6241, MinusLogProbMetric: 31.6241, val_loss: 32.1194, val_MinusLogProbMetric: 32.1194

Epoch 140: val_loss did not improve from 31.33517
196/196 - 34s - loss: 31.6241 - MinusLogProbMetric: 31.6241 - val_loss: 32.1194 - val_MinusLogProbMetric: 32.1194 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 141/1000
2023-09-29 22:58:16.012 
Epoch 141/1000 
	 loss: 31.4851, MinusLogProbMetric: 31.4851, val_loss: 31.1845, val_MinusLogProbMetric: 31.1845

Epoch 141: val_loss improved from 31.33517 to 31.18454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 31.4851 - MinusLogProbMetric: 31.4851 - val_loss: 31.1845 - val_MinusLogProbMetric: 31.1845 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 142/1000
2023-09-29 22:58:50.064 
Epoch 142/1000 
	 loss: 31.6948, MinusLogProbMetric: 31.6948, val_loss: 31.4175, val_MinusLogProbMetric: 31.4175

Epoch 142: val_loss did not improve from 31.18454
196/196 - 34s - loss: 31.6948 - MinusLogProbMetric: 31.6948 - val_loss: 31.4175 - val_MinusLogProbMetric: 31.4175 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 143/1000
2023-09-29 22:59:24.290 
Epoch 143/1000 
	 loss: 31.7399, MinusLogProbMetric: 31.7399, val_loss: 31.5149, val_MinusLogProbMetric: 31.5149

Epoch 143: val_loss did not improve from 31.18454
196/196 - 34s - loss: 31.7399 - MinusLogProbMetric: 31.7399 - val_loss: 31.5149 - val_MinusLogProbMetric: 31.5149 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 144/1000
2023-09-29 22:59:59.141 
Epoch 144/1000 
	 loss: 31.3967, MinusLogProbMetric: 31.3967, val_loss: 31.7226, val_MinusLogProbMetric: 31.7226

Epoch 144: val_loss did not improve from 31.18454
196/196 - 35s - loss: 31.3967 - MinusLogProbMetric: 31.3967 - val_loss: 31.7226 - val_MinusLogProbMetric: 31.7226 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 145/1000
2023-09-29 23:00:32.833 
Epoch 145/1000 
	 loss: 31.5456, MinusLogProbMetric: 31.5456, val_loss: 31.4811, val_MinusLogProbMetric: 31.4811

Epoch 145: val_loss did not improve from 31.18454
196/196 - 34s - loss: 31.5456 - MinusLogProbMetric: 31.5456 - val_loss: 31.4811 - val_MinusLogProbMetric: 31.4811 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 146/1000
2023-09-29 23:01:07.478 
Epoch 146/1000 
	 loss: 31.3185, MinusLogProbMetric: 31.3185, val_loss: 31.4563, val_MinusLogProbMetric: 31.4563

Epoch 146: val_loss did not improve from 31.18454
196/196 - 35s - loss: 31.3185 - MinusLogProbMetric: 31.3185 - val_loss: 31.4563 - val_MinusLogProbMetric: 31.4563 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 147/1000
2023-09-29 23:01:42.236 
Epoch 147/1000 
	 loss: 31.6521, MinusLogProbMetric: 31.6521, val_loss: 31.7779, val_MinusLogProbMetric: 31.7779

Epoch 147: val_loss did not improve from 31.18454
196/196 - 35s - loss: 31.6521 - MinusLogProbMetric: 31.6521 - val_loss: 31.7779 - val_MinusLogProbMetric: 31.7779 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 148/1000
2023-09-29 23:02:16.696 
Epoch 148/1000 
	 loss: 31.5617, MinusLogProbMetric: 31.5617, val_loss: 32.9663, val_MinusLogProbMetric: 32.9663

Epoch 148: val_loss did not improve from 31.18454
196/196 - 34s - loss: 31.5617 - MinusLogProbMetric: 31.5617 - val_loss: 32.9663 - val_MinusLogProbMetric: 32.9663 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 149/1000
2023-09-29 23:02:50.750 
Epoch 149/1000 
	 loss: 31.3901, MinusLogProbMetric: 31.3901, val_loss: 31.4161, val_MinusLogProbMetric: 31.4161

Epoch 149: val_loss did not improve from 31.18454
196/196 - 34s - loss: 31.3901 - MinusLogProbMetric: 31.3901 - val_loss: 31.4161 - val_MinusLogProbMetric: 31.4161 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 150/1000
2023-09-29 23:03:24.817 
Epoch 150/1000 
	 loss: 31.3940, MinusLogProbMetric: 31.3940, val_loss: 31.7867, val_MinusLogProbMetric: 31.7867

Epoch 150: val_loss did not improve from 31.18454
196/196 - 34s - loss: 31.3940 - MinusLogProbMetric: 31.3940 - val_loss: 31.7867 - val_MinusLogProbMetric: 31.7867 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 151/1000
2023-09-29 23:03:58.363 
Epoch 151/1000 
	 loss: 31.2874, MinusLogProbMetric: 31.2874, val_loss: 34.1881, val_MinusLogProbMetric: 34.1881

Epoch 151: val_loss did not improve from 31.18454
196/196 - 34s - loss: 31.2874 - MinusLogProbMetric: 31.2874 - val_loss: 34.1881 - val_MinusLogProbMetric: 34.1881 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 152/1000
2023-09-29 23:04:32.200 
Epoch 152/1000 
	 loss: 31.3389, MinusLogProbMetric: 31.3389, val_loss: 31.0294, val_MinusLogProbMetric: 31.0294

Epoch 152: val_loss improved from 31.18454 to 31.02943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 31.3389 - MinusLogProbMetric: 31.3389 - val_loss: 31.0294 - val_MinusLogProbMetric: 31.0294 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 153/1000
2023-09-29 23:05:06.861 
Epoch 153/1000 
	 loss: 31.3034, MinusLogProbMetric: 31.3034, val_loss: 31.7628, val_MinusLogProbMetric: 31.7628

Epoch 153: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.3034 - MinusLogProbMetric: 31.3034 - val_loss: 31.7628 - val_MinusLogProbMetric: 31.7628 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 154/1000
2023-09-29 23:05:40.699 
Epoch 154/1000 
	 loss: 31.3815, MinusLogProbMetric: 31.3815, val_loss: 31.4680, val_MinusLogProbMetric: 31.4680

Epoch 154: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.3815 - MinusLogProbMetric: 31.3815 - val_loss: 31.4680 - val_MinusLogProbMetric: 31.4680 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 155/1000
2023-09-29 23:06:14.420 
Epoch 155/1000 
	 loss: 31.3379, MinusLogProbMetric: 31.3379, val_loss: 31.8476, val_MinusLogProbMetric: 31.8476

Epoch 155: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.3379 - MinusLogProbMetric: 31.3379 - val_loss: 31.8476 - val_MinusLogProbMetric: 31.8476 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 156/1000
2023-09-29 23:06:48.458 
Epoch 156/1000 
	 loss: 31.3944, MinusLogProbMetric: 31.3944, val_loss: 31.8115, val_MinusLogProbMetric: 31.8115

Epoch 156: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.3944 - MinusLogProbMetric: 31.3944 - val_loss: 31.8115 - val_MinusLogProbMetric: 31.8115 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 157/1000
2023-09-29 23:07:23.449 
Epoch 157/1000 
	 loss: 31.3137, MinusLogProbMetric: 31.3137, val_loss: 31.3687, val_MinusLogProbMetric: 31.3687

Epoch 157: val_loss did not improve from 31.02943
196/196 - 35s - loss: 31.3137 - MinusLogProbMetric: 31.3137 - val_loss: 31.3687 - val_MinusLogProbMetric: 31.3687 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 158/1000
2023-09-29 23:07:56.810 
Epoch 158/1000 
	 loss: 31.1018, MinusLogProbMetric: 31.1018, val_loss: 32.0077, val_MinusLogProbMetric: 32.0077

Epoch 158: val_loss did not improve from 31.02943
196/196 - 33s - loss: 31.1018 - MinusLogProbMetric: 31.1018 - val_loss: 32.0077 - val_MinusLogProbMetric: 32.0077 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 159/1000
2023-09-29 23:08:30.108 
Epoch 159/1000 
	 loss: 31.0988, MinusLogProbMetric: 31.0988, val_loss: 32.0479, val_MinusLogProbMetric: 32.0479

Epoch 159: val_loss did not improve from 31.02943
196/196 - 33s - loss: 31.0988 - MinusLogProbMetric: 31.0988 - val_loss: 32.0479 - val_MinusLogProbMetric: 32.0479 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 160/1000
2023-09-29 23:09:04.610 
Epoch 160/1000 
	 loss: 31.3114, MinusLogProbMetric: 31.3114, val_loss: 31.4343, val_MinusLogProbMetric: 31.4343

Epoch 160: val_loss did not improve from 31.02943
196/196 - 35s - loss: 31.3114 - MinusLogProbMetric: 31.3114 - val_loss: 31.4343 - val_MinusLogProbMetric: 31.4343 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 161/1000
2023-09-29 23:09:37.809 
Epoch 161/1000 
	 loss: 31.1885, MinusLogProbMetric: 31.1885, val_loss: 31.1086, val_MinusLogProbMetric: 31.1086

Epoch 161: val_loss did not improve from 31.02943
196/196 - 33s - loss: 31.1885 - MinusLogProbMetric: 31.1885 - val_loss: 31.1086 - val_MinusLogProbMetric: 31.1086 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 162/1000
2023-09-29 23:10:12.120 
Epoch 162/1000 
	 loss: 31.2750, MinusLogProbMetric: 31.2750, val_loss: 31.3365, val_MinusLogProbMetric: 31.3365

Epoch 162: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.2750 - MinusLogProbMetric: 31.2750 - val_loss: 31.3365 - val_MinusLogProbMetric: 31.3365 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 163/1000
2023-09-29 23:10:45.512 
Epoch 163/1000 
	 loss: 31.1299, MinusLogProbMetric: 31.1299, val_loss: 31.6643, val_MinusLogProbMetric: 31.6643

Epoch 163: val_loss did not improve from 31.02943
196/196 - 33s - loss: 31.1299 - MinusLogProbMetric: 31.1299 - val_loss: 31.6643 - val_MinusLogProbMetric: 31.6643 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 164/1000
2023-09-29 23:11:19.435 
Epoch 164/1000 
	 loss: 31.1220, MinusLogProbMetric: 31.1220, val_loss: 32.2713, val_MinusLogProbMetric: 32.2713

Epoch 164: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.1220 - MinusLogProbMetric: 31.1220 - val_loss: 32.2713 - val_MinusLogProbMetric: 32.2713 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 165/1000
2023-09-29 23:11:53.411 
Epoch 165/1000 
	 loss: 31.2520, MinusLogProbMetric: 31.2520, val_loss: 31.0338, val_MinusLogProbMetric: 31.0338

Epoch 165: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.2520 - MinusLogProbMetric: 31.2520 - val_loss: 31.0338 - val_MinusLogProbMetric: 31.0338 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 166/1000
2023-09-29 23:12:28.169 
Epoch 166/1000 
	 loss: 31.4163, MinusLogProbMetric: 31.4163, val_loss: 31.1372, val_MinusLogProbMetric: 31.1372

Epoch 166: val_loss did not improve from 31.02943
196/196 - 35s - loss: 31.4163 - MinusLogProbMetric: 31.4163 - val_loss: 31.1372 - val_MinusLogProbMetric: 31.1372 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 167/1000
2023-09-29 23:13:02.579 
Epoch 167/1000 
	 loss: 31.0122, MinusLogProbMetric: 31.0122, val_loss: 32.0706, val_MinusLogProbMetric: 32.0706

Epoch 167: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.0122 - MinusLogProbMetric: 31.0122 - val_loss: 32.0706 - val_MinusLogProbMetric: 32.0706 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 168/1000
2023-09-29 23:13:36.390 
Epoch 168/1000 
	 loss: 31.1979, MinusLogProbMetric: 31.1979, val_loss: 31.4050, val_MinusLogProbMetric: 31.4050

Epoch 168: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.1979 - MinusLogProbMetric: 31.1979 - val_loss: 31.4050 - val_MinusLogProbMetric: 31.4050 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 169/1000
2023-09-29 23:14:10.841 
Epoch 169/1000 
	 loss: 31.1827, MinusLogProbMetric: 31.1827, val_loss: 31.1389, val_MinusLogProbMetric: 31.1389

Epoch 169: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.1827 - MinusLogProbMetric: 31.1827 - val_loss: 31.1389 - val_MinusLogProbMetric: 31.1389 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 170/1000
2023-09-29 23:14:45.569 
Epoch 170/1000 
	 loss: 31.1848, MinusLogProbMetric: 31.1848, val_loss: 31.1145, val_MinusLogProbMetric: 31.1145

Epoch 170: val_loss did not improve from 31.02943
196/196 - 35s - loss: 31.1848 - MinusLogProbMetric: 31.1848 - val_loss: 31.1145 - val_MinusLogProbMetric: 31.1145 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 171/1000
2023-09-29 23:15:19.900 
Epoch 171/1000 
	 loss: 30.9046, MinusLogProbMetric: 30.9046, val_loss: 33.7611, val_MinusLogProbMetric: 33.7611

Epoch 171: val_loss did not improve from 31.02943
196/196 - 34s - loss: 30.9046 - MinusLogProbMetric: 30.9046 - val_loss: 33.7611 - val_MinusLogProbMetric: 33.7611 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 172/1000
2023-09-29 23:15:53.950 
Epoch 172/1000 
	 loss: 31.2083, MinusLogProbMetric: 31.2083, val_loss: 31.4168, val_MinusLogProbMetric: 31.4168

Epoch 172: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.2083 - MinusLogProbMetric: 31.2083 - val_loss: 31.4168 - val_MinusLogProbMetric: 31.4168 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 173/1000
2023-09-29 23:16:27.468 
Epoch 173/1000 
	 loss: 31.1470, MinusLogProbMetric: 31.1470, val_loss: 32.0498, val_MinusLogProbMetric: 32.0498

Epoch 173: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.1470 - MinusLogProbMetric: 31.1470 - val_loss: 32.0498 - val_MinusLogProbMetric: 32.0498 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 174/1000
2023-09-29 23:17:01.769 
Epoch 174/1000 
	 loss: 31.0972, MinusLogProbMetric: 31.0972, val_loss: 31.6030, val_MinusLogProbMetric: 31.6030

Epoch 174: val_loss did not improve from 31.02943
196/196 - 34s - loss: 31.0972 - MinusLogProbMetric: 31.0972 - val_loss: 31.6030 - val_MinusLogProbMetric: 31.6030 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 175/1000
2023-09-29 23:17:36.454 
Epoch 175/1000 
	 loss: 31.1764, MinusLogProbMetric: 31.1764, val_loss: 30.7917, val_MinusLogProbMetric: 30.7917

Epoch 175: val_loss improved from 31.02943 to 30.79166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 31.1764 - MinusLogProbMetric: 31.1764 - val_loss: 30.7917 - val_MinusLogProbMetric: 30.7917 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 176/1000
2023-09-29 23:18:11.645 
Epoch 176/1000 
	 loss: 31.1496, MinusLogProbMetric: 31.1496, val_loss: 31.7612, val_MinusLogProbMetric: 31.7612

Epoch 176: val_loss did not improve from 30.79166
196/196 - 35s - loss: 31.1496 - MinusLogProbMetric: 31.1496 - val_loss: 31.7612 - val_MinusLogProbMetric: 31.7612 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 177/1000
2023-09-29 23:18:44.740 
Epoch 177/1000 
	 loss: 31.0343, MinusLogProbMetric: 31.0343, val_loss: 31.6702, val_MinusLogProbMetric: 31.6702

Epoch 177: val_loss did not improve from 30.79166
196/196 - 33s - loss: 31.0343 - MinusLogProbMetric: 31.0343 - val_loss: 31.6702 - val_MinusLogProbMetric: 31.6702 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 178/1000
2023-09-29 23:19:19.369 
Epoch 178/1000 
	 loss: 30.9540, MinusLogProbMetric: 30.9540, val_loss: 30.7379, val_MinusLogProbMetric: 30.7379

Epoch 178: val_loss improved from 30.79166 to 30.73787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 30.9540 - MinusLogProbMetric: 30.9540 - val_loss: 30.7379 - val_MinusLogProbMetric: 30.7379 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 179/1000
2023-09-29 23:19:54.534 
Epoch 179/1000 
	 loss: 30.8414, MinusLogProbMetric: 30.8414, val_loss: 32.5928, val_MinusLogProbMetric: 32.5928

Epoch 179: val_loss did not improve from 30.73787
196/196 - 35s - loss: 30.8414 - MinusLogProbMetric: 30.8414 - val_loss: 32.5928 - val_MinusLogProbMetric: 32.5928 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 180/1000
2023-09-29 23:20:28.205 
Epoch 180/1000 
	 loss: 31.0801, MinusLogProbMetric: 31.0801, val_loss: 30.9936, val_MinusLogProbMetric: 30.9936

Epoch 180: val_loss did not improve from 30.73787
196/196 - 34s - loss: 31.0801 - MinusLogProbMetric: 31.0801 - val_loss: 30.9936 - val_MinusLogProbMetric: 30.9936 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 181/1000
2023-09-29 23:21:01.872 
Epoch 181/1000 
	 loss: 31.0200, MinusLogProbMetric: 31.0200, val_loss: 31.2000, val_MinusLogProbMetric: 31.2000

Epoch 181: val_loss did not improve from 30.73787
196/196 - 34s - loss: 31.0200 - MinusLogProbMetric: 31.0200 - val_loss: 31.2000 - val_MinusLogProbMetric: 31.2000 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 182/1000
2023-09-29 23:21:36.076 
Epoch 182/1000 
	 loss: 31.3080, MinusLogProbMetric: 31.3080, val_loss: 30.9861, val_MinusLogProbMetric: 30.9861

Epoch 182: val_loss did not improve from 30.73787
196/196 - 34s - loss: 31.3080 - MinusLogProbMetric: 31.3080 - val_loss: 30.9861 - val_MinusLogProbMetric: 30.9861 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 183/1000
2023-09-29 23:22:08.669 
Epoch 183/1000 
	 loss: 30.9503, MinusLogProbMetric: 30.9503, val_loss: 31.3710, val_MinusLogProbMetric: 31.3710

Epoch 183: val_loss did not improve from 30.73787
196/196 - 33s - loss: 30.9503 - MinusLogProbMetric: 30.9503 - val_loss: 31.3710 - val_MinusLogProbMetric: 31.3710 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 184/1000
2023-09-29 23:22:41.224 
Epoch 184/1000 
	 loss: 30.9441, MinusLogProbMetric: 30.9441, val_loss: 32.0363, val_MinusLogProbMetric: 32.0363

Epoch 184: val_loss did not improve from 30.73787
196/196 - 33s - loss: 30.9441 - MinusLogProbMetric: 30.9441 - val_loss: 32.0363 - val_MinusLogProbMetric: 32.0363 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 185/1000
2023-09-29 23:23:15.986 
Epoch 185/1000 
	 loss: 30.7987, MinusLogProbMetric: 30.7987, val_loss: 31.2938, val_MinusLogProbMetric: 31.2938

Epoch 185: val_loss did not improve from 30.73787
196/196 - 35s - loss: 30.7987 - MinusLogProbMetric: 30.7987 - val_loss: 31.2938 - val_MinusLogProbMetric: 31.2938 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 186/1000
2023-09-29 23:23:49.348 
Epoch 186/1000 
	 loss: 30.9800, MinusLogProbMetric: 30.9800, val_loss: 31.0160, val_MinusLogProbMetric: 31.0160

Epoch 186: val_loss did not improve from 30.73787
196/196 - 33s - loss: 30.9800 - MinusLogProbMetric: 30.9800 - val_loss: 31.0160 - val_MinusLogProbMetric: 31.0160 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 187/1000
2023-09-29 23:24:23.364 
Epoch 187/1000 
	 loss: 30.8224, MinusLogProbMetric: 30.8224, val_loss: 30.8417, val_MinusLogProbMetric: 30.8417

Epoch 187: val_loss did not improve from 30.73787
196/196 - 34s - loss: 30.8224 - MinusLogProbMetric: 30.8224 - val_loss: 30.8417 - val_MinusLogProbMetric: 30.8417 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 188/1000
2023-09-29 23:24:57.204 
Epoch 188/1000 
	 loss: 30.7919, MinusLogProbMetric: 30.7919, val_loss: 31.5086, val_MinusLogProbMetric: 31.5086

Epoch 188: val_loss did not improve from 30.73787
196/196 - 34s - loss: 30.7919 - MinusLogProbMetric: 30.7919 - val_loss: 31.5086 - val_MinusLogProbMetric: 31.5086 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 189/1000
2023-09-29 23:25:31.958 
Epoch 189/1000 
	 loss: 30.9019, MinusLogProbMetric: 30.9019, val_loss: 31.0618, val_MinusLogProbMetric: 31.0618

Epoch 189: val_loss did not improve from 30.73787
196/196 - 35s - loss: 30.9019 - MinusLogProbMetric: 30.9019 - val_loss: 31.0618 - val_MinusLogProbMetric: 31.0618 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 190/1000
2023-09-29 23:26:06.561 
Epoch 190/1000 
	 loss: 30.8642, MinusLogProbMetric: 30.8642, val_loss: 30.4394, val_MinusLogProbMetric: 30.4394

Epoch 190: val_loss improved from 30.73787 to 30.43944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 30.8642 - MinusLogProbMetric: 30.8642 - val_loss: 30.4394 - val_MinusLogProbMetric: 30.4394 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 191/1000
2023-09-29 23:26:42.171 
Epoch 191/1000 
	 loss: 30.9325, MinusLogProbMetric: 30.9325, val_loss: 31.4258, val_MinusLogProbMetric: 31.4258

Epoch 191: val_loss did not improve from 30.43944
196/196 - 35s - loss: 30.9325 - MinusLogProbMetric: 30.9325 - val_loss: 31.4258 - val_MinusLogProbMetric: 31.4258 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 192/1000
2023-09-29 23:27:16.952 
Epoch 192/1000 
	 loss: 30.8515, MinusLogProbMetric: 30.8515, val_loss: 31.6547, val_MinusLogProbMetric: 31.6547

Epoch 192: val_loss did not improve from 30.43944
196/196 - 35s - loss: 30.8515 - MinusLogProbMetric: 30.8515 - val_loss: 31.6547 - val_MinusLogProbMetric: 31.6547 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 193/1000
2023-09-29 23:27:51.246 
Epoch 193/1000 
	 loss: 30.8304, MinusLogProbMetric: 30.8304, val_loss: 31.9990, val_MinusLogProbMetric: 31.9990

Epoch 193: val_loss did not improve from 30.43944
196/196 - 34s - loss: 30.8304 - MinusLogProbMetric: 30.8304 - val_loss: 31.9990 - val_MinusLogProbMetric: 31.9990 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 194/1000
2023-09-29 23:28:25.841 
Epoch 194/1000 
	 loss: 30.7558, MinusLogProbMetric: 30.7558, val_loss: 31.0177, val_MinusLogProbMetric: 31.0177

Epoch 194: val_loss did not improve from 30.43944
196/196 - 35s - loss: 30.7558 - MinusLogProbMetric: 30.7558 - val_loss: 31.0177 - val_MinusLogProbMetric: 31.0177 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 195/1000
2023-09-29 23:29:00.856 
Epoch 195/1000 
	 loss: 30.7810, MinusLogProbMetric: 30.7810, val_loss: 30.4189, val_MinusLogProbMetric: 30.4189

Epoch 195: val_loss improved from 30.43944 to 30.41893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 30.7810 - MinusLogProbMetric: 30.7810 - val_loss: 30.4189 - val_MinusLogProbMetric: 30.4189 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 196/1000
2023-09-29 23:29:36.357 
Epoch 196/1000 
	 loss: 30.9059, MinusLogProbMetric: 30.9059, val_loss: 31.9922, val_MinusLogProbMetric: 31.9922

Epoch 196: val_loss did not improve from 30.41893
196/196 - 35s - loss: 30.9059 - MinusLogProbMetric: 30.9059 - val_loss: 31.9922 - val_MinusLogProbMetric: 31.9922 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 197/1000
2023-09-29 23:30:10.779 
Epoch 197/1000 
	 loss: 30.8449, MinusLogProbMetric: 30.8449, val_loss: 31.2810, val_MinusLogProbMetric: 31.2810

Epoch 197: val_loss did not improve from 30.41893
196/196 - 34s - loss: 30.8449 - MinusLogProbMetric: 30.8449 - val_loss: 31.2810 - val_MinusLogProbMetric: 31.2810 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 198/1000
2023-09-29 23:30:45.606 
Epoch 198/1000 
	 loss: 30.8216, MinusLogProbMetric: 30.8216, val_loss: 31.8758, val_MinusLogProbMetric: 31.8758

Epoch 198: val_loss did not improve from 30.41893
196/196 - 35s - loss: 30.8216 - MinusLogProbMetric: 30.8216 - val_loss: 31.8758 - val_MinusLogProbMetric: 31.8758 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 199/1000
2023-09-29 23:31:20.272 
Epoch 199/1000 
	 loss: 30.6823, MinusLogProbMetric: 30.6823, val_loss: 30.4492, val_MinusLogProbMetric: 30.4492

Epoch 199: val_loss did not improve from 30.41893
196/196 - 35s - loss: 30.6823 - MinusLogProbMetric: 30.6823 - val_loss: 30.4492 - val_MinusLogProbMetric: 30.4492 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 200/1000
2023-09-29 23:31:54.729 
Epoch 200/1000 
	 loss: 30.8667, MinusLogProbMetric: 30.8667, val_loss: 31.1387, val_MinusLogProbMetric: 31.1387

Epoch 200: val_loss did not improve from 30.41893
196/196 - 34s - loss: 30.8667 - MinusLogProbMetric: 30.8667 - val_loss: 31.1387 - val_MinusLogProbMetric: 31.1387 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 201/1000
2023-09-29 23:32:29.443 
Epoch 201/1000 
	 loss: 30.6773, MinusLogProbMetric: 30.6773, val_loss: 30.8894, val_MinusLogProbMetric: 30.8894

Epoch 201: val_loss did not improve from 30.41893
196/196 - 35s - loss: 30.6773 - MinusLogProbMetric: 30.6773 - val_loss: 30.8894 - val_MinusLogProbMetric: 30.8894 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 202/1000
2023-09-29 23:33:02.331 
Epoch 202/1000 
	 loss: 30.8447, MinusLogProbMetric: 30.8447, val_loss: 33.5455, val_MinusLogProbMetric: 33.5455

Epoch 202: val_loss did not improve from 30.41893
196/196 - 33s - loss: 30.8447 - MinusLogProbMetric: 30.8447 - val_loss: 33.5455 - val_MinusLogProbMetric: 33.5455 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 203/1000
2023-09-29 23:33:36.824 
Epoch 203/1000 
	 loss: 30.7764, MinusLogProbMetric: 30.7764, val_loss: 30.8360, val_MinusLogProbMetric: 30.8360

Epoch 203: val_loss did not improve from 30.41893
196/196 - 34s - loss: 30.7764 - MinusLogProbMetric: 30.7764 - val_loss: 30.8360 - val_MinusLogProbMetric: 30.8360 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 204/1000
2023-09-29 23:34:11.320 
Epoch 204/1000 
	 loss: 30.6202, MinusLogProbMetric: 30.6202, val_loss: 30.7001, val_MinusLogProbMetric: 30.7001

Epoch 204: val_loss did not improve from 30.41893
196/196 - 34s - loss: 30.6202 - MinusLogProbMetric: 30.6202 - val_loss: 30.7001 - val_MinusLogProbMetric: 30.7001 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 205/1000
2023-09-29 23:34:46.197 
Epoch 205/1000 
	 loss: 30.6828, MinusLogProbMetric: 30.6828, val_loss: 30.6573, val_MinusLogProbMetric: 30.6573

Epoch 205: val_loss did not improve from 30.41893
196/196 - 35s - loss: 30.6828 - MinusLogProbMetric: 30.6828 - val_loss: 30.6573 - val_MinusLogProbMetric: 30.6573 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 206/1000
2023-09-29 23:35:20.678 
Epoch 206/1000 
	 loss: 30.4962, MinusLogProbMetric: 30.4962, val_loss: 30.6382, val_MinusLogProbMetric: 30.6382

Epoch 206: val_loss did not improve from 30.41893
196/196 - 34s - loss: 30.4962 - MinusLogProbMetric: 30.4962 - val_loss: 30.6382 - val_MinusLogProbMetric: 30.6382 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 207/1000
2023-09-29 23:35:55.381 
Epoch 207/1000 
	 loss: 30.6616, MinusLogProbMetric: 30.6616, val_loss: 31.8120, val_MinusLogProbMetric: 31.8120

Epoch 207: val_loss did not improve from 30.41893
196/196 - 35s - loss: 30.6616 - MinusLogProbMetric: 30.6616 - val_loss: 31.8120 - val_MinusLogProbMetric: 31.8120 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 208/1000
2023-09-29 23:36:30.114 
Epoch 208/1000 
	 loss: 30.6708, MinusLogProbMetric: 30.6708, val_loss: 31.6125, val_MinusLogProbMetric: 31.6125

Epoch 208: val_loss did not improve from 30.41893
196/196 - 35s - loss: 30.6708 - MinusLogProbMetric: 30.6708 - val_loss: 31.6125 - val_MinusLogProbMetric: 31.6125 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 209/1000
2023-09-29 23:37:01.461 
Epoch 209/1000 
	 loss: 30.5781, MinusLogProbMetric: 30.5781, val_loss: 31.0116, val_MinusLogProbMetric: 31.0116

Epoch 209: val_loss did not improve from 30.41893
196/196 - 31s - loss: 30.5781 - MinusLogProbMetric: 30.5781 - val_loss: 31.0116 - val_MinusLogProbMetric: 31.0116 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 210/1000
2023-09-29 23:37:35.902 
Epoch 210/1000 
	 loss: 30.6004, MinusLogProbMetric: 30.6004, val_loss: 30.2516, val_MinusLogProbMetric: 30.2516

Epoch 210: val_loss improved from 30.41893 to 30.25160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 30.6004 - MinusLogProbMetric: 30.6004 - val_loss: 30.2516 - val_MinusLogProbMetric: 30.2516 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 211/1000
2023-09-29 23:38:11.474 
Epoch 211/1000 
	 loss: 30.8515, MinusLogProbMetric: 30.8515, val_loss: 30.8189, val_MinusLogProbMetric: 30.8189

Epoch 211: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.8515 - MinusLogProbMetric: 30.8515 - val_loss: 30.8189 - val_MinusLogProbMetric: 30.8189 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 212/1000
2023-09-29 23:38:46.404 
Epoch 212/1000 
	 loss: 30.5530, MinusLogProbMetric: 30.5530, val_loss: 30.4710, val_MinusLogProbMetric: 30.4710

Epoch 212: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.5530 - MinusLogProbMetric: 30.5530 - val_loss: 30.4710 - val_MinusLogProbMetric: 30.4710 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 213/1000
2023-09-29 23:39:21.229 
Epoch 213/1000 
	 loss: 30.8249, MinusLogProbMetric: 30.8249, val_loss: 30.3667, val_MinusLogProbMetric: 30.3667

Epoch 213: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.8249 - MinusLogProbMetric: 30.8249 - val_loss: 30.3667 - val_MinusLogProbMetric: 30.3667 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 214/1000
2023-09-29 23:39:56.126 
Epoch 214/1000 
	 loss: 30.7039, MinusLogProbMetric: 30.7039, val_loss: 30.8861, val_MinusLogProbMetric: 30.8861

Epoch 214: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.7039 - MinusLogProbMetric: 30.7039 - val_loss: 30.8861 - val_MinusLogProbMetric: 30.8861 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 215/1000
2023-09-29 23:40:31.014 
Epoch 215/1000 
	 loss: 30.7515, MinusLogProbMetric: 30.7515, val_loss: 31.9525, val_MinusLogProbMetric: 31.9525

Epoch 215: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.7515 - MinusLogProbMetric: 30.7515 - val_loss: 31.9525 - val_MinusLogProbMetric: 31.9525 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 216/1000
2023-09-29 23:41:04.034 
Epoch 216/1000 
	 loss: 30.7935, MinusLogProbMetric: 30.7935, val_loss: 30.6489, val_MinusLogProbMetric: 30.6489

Epoch 216: val_loss did not improve from 30.25160
196/196 - 33s - loss: 30.7935 - MinusLogProbMetric: 30.7935 - val_loss: 30.6489 - val_MinusLogProbMetric: 30.6489 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 217/1000
2023-09-29 23:41:39.036 
Epoch 217/1000 
	 loss: 30.6473, MinusLogProbMetric: 30.6473, val_loss: 30.9400, val_MinusLogProbMetric: 30.9400

Epoch 217: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.6473 - MinusLogProbMetric: 30.6473 - val_loss: 30.9400 - val_MinusLogProbMetric: 30.9400 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 218/1000
2023-09-29 23:42:14.081 
Epoch 218/1000 
	 loss: 30.5694, MinusLogProbMetric: 30.5694, val_loss: 30.6290, val_MinusLogProbMetric: 30.6290

Epoch 218: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.5694 - MinusLogProbMetric: 30.5694 - val_loss: 30.6290 - val_MinusLogProbMetric: 30.6290 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 219/1000
2023-09-29 23:42:48.944 
Epoch 219/1000 
	 loss: 30.7532, MinusLogProbMetric: 30.7532, val_loss: 31.1618, val_MinusLogProbMetric: 31.1618

Epoch 219: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.7532 - MinusLogProbMetric: 30.7532 - val_loss: 31.1618 - val_MinusLogProbMetric: 31.1618 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 220/1000
2023-09-29 23:43:23.886 
Epoch 220/1000 
	 loss: 30.6020, MinusLogProbMetric: 30.6020, val_loss: 30.6406, val_MinusLogProbMetric: 30.6406

Epoch 220: val_loss did not improve from 30.25160
196/196 - 35s - loss: 30.6020 - MinusLogProbMetric: 30.6020 - val_loss: 30.6406 - val_MinusLogProbMetric: 30.6406 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 221/1000
2023-09-29 23:43:58.353 
Epoch 221/1000 
	 loss: 30.5939, MinusLogProbMetric: 30.5939, val_loss: 31.1464, val_MinusLogProbMetric: 31.1464

Epoch 221: val_loss did not improve from 30.25160
196/196 - 34s - loss: 30.5939 - MinusLogProbMetric: 30.5939 - val_loss: 31.1464 - val_MinusLogProbMetric: 31.1464 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 222/1000
2023-09-29 23:44:32.498 
Epoch 222/1000 
	 loss: 30.4357, MinusLogProbMetric: 30.4357, val_loss: 30.8446, val_MinusLogProbMetric: 30.8446

Epoch 222: val_loss did not improve from 30.25160
196/196 - 34s - loss: 30.4357 - MinusLogProbMetric: 30.4357 - val_loss: 30.8446 - val_MinusLogProbMetric: 30.8446 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 223/1000
2023-09-29 23:45:06.853 
Epoch 223/1000 
	 loss: 30.5449, MinusLogProbMetric: 30.5449, val_loss: 30.1610, val_MinusLogProbMetric: 30.1610

Epoch 223: val_loss improved from 30.25160 to 30.16103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 30.5449 - MinusLogProbMetric: 30.5449 - val_loss: 30.1610 - val_MinusLogProbMetric: 30.1610 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 224/1000
2023-09-29 23:45:40.832 
Epoch 224/1000 
	 loss: 30.5678, MinusLogProbMetric: 30.5678, val_loss: 30.7312, val_MinusLogProbMetric: 30.7312

Epoch 224: val_loss did not improve from 30.16103
196/196 - 33s - loss: 30.5678 - MinusLogProbMetric: 30.5678 - val_loss: 30.7312 - val_MinusLogProbMetric: 30.7312 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 225/1000
2023-09-29 23:46:14.967 
Epoch 225/1000 
	 loss: 30.5507, MinusLogProbMetric: 30.5507, val_loss: 30.6984, val_MinusLogProbMetric: 30.6984

Epoch 225: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.5507 - MinusLogProbMetric: 30.5507 - val_loss: 30.6984 - val_MinusLogProbMetric: 30.6984 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 226/1000
2023-09-29 23:46:49.102 
Epoch 226/1000 
	 loss: 30.3470, MinusLogProbMetric: 30.3470, val_loss: 32.0748, val_MinusLogProbMetric: 32.0748

Epoch 226: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.3470 - MinusLogProbMetric: 30.3470 - val_loss: 32.0748 - val_MinusLogProbMetric: 32.0748 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 227/1000
2023-09-29 23:47:23.320 
Epoch 227/1000 
	 loss: 30.5358, MinusLogProbMetric: 30.5358, val_loss: 30.7409, val_MinusLogProbMetric: 30.7409

Epoch 227: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.5358 - MinusLogProbMetric: 30.5358 - val_loss: 30.7409 - val_MinusLogProbMetric: 30.7409 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 228/1000
2023-09-29 23:47:57.623 
Epoch 228/1000 
	 loss: 30.6178, MinusLogProbMetric: 30.6178, val_loss: 30.8415, val_MinusLogProbMetric: 30.8415

Epoch 228: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.6178 - MinusLogProbMetric: 30.6178 - val_loss: 30.8415 - val_MinusLogProbMetric: 30.8415 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 229/1000
2023-09-29 23:48:31.781 
Epoch 229/1000 
	 loss: 30.4902, MinusLogProbMetric: 30.4902, val_loss: 31.2908, val_MinusLogProbMetric: 31.2908

Epoch 229: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.4902 - MinusLogProbMetric: 30.4902 - val_loss: 31.2908 - val_MinusLogProbMetric: 31.2908 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 230/1000
2023-09-29 23:49:06.320 
Epoch 230/1000 
	 loss: 30.5482, MinusLogProbMetric: 30.5482, val_loss: 31.6780, val_MinusLogProbMetric: 31.6780

Epoch 230: val_loss did not improve from 30.16103
196/196 - 35s - loss: 30.5482 - MinusLogProbMetric: 30.5482 - val_loss: 31.6780 - val_MinusLogProbMetric: 31.6780 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 231/1000
2023-09-29 23:49:40.790 
Epoch 231/1000 
	 loss: 30.5687, MinusLogProbMetric: 30.5687, val_loss: 30.3006, val_MinusLogProbMetric: 30.3006

Epoch 231: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.5687 - MinusLogProbMetric: 30.5687 - val_loss: 30.3006 - val_MinusLogProbMetric: 30.3006 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 232/1000
2023-09-29 23:50:15.063 
Epoch 232/1000 
	 loss: 30.4950, MinusLogProbMetric: 30.4950, val_loss: 30.4525, val_MinusLogProbMetric: 30.4525

Epoch 232: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.4950 - MinusLogProbMetric: 30.4950 - val_loss: 30.4525 - val_MinusLogProbMetric: 30.4525 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 233/1000
2023-09-29 23:50:49.337 
Epoch 233/1000 
	 loss: 30.6536, MinusLogProbMetric: 30.6536, val_loss: 30.2879, val_MinusLogProbMetric: 30.2879

Epoch 233: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.6536 - MinusLogProbMetric: 30.6536 - val_loss: 30.2879 - val_MinusLogProbMetric: 30.2879 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 234/1000
2023-09-29 23:51:22.422 
Epoch 234/1000 
	 loss: 30.5220, MinusLogProbMetric: 30.5220, val_loss: 30.8033, val_MinusLogProbMetric: 30.8033

Epoch 234: val_loss did not improve from 30.16103
196/196 - 33s - loss: 30.5220 - MinusLogProbMetric: 30.5220 - val_loss: 30.8033 - val_MinusLogProbMetric: 30.8033 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 235/1000
2023-09-29 23:51:56.626 
Epoch 235/1000 
	 loss: 30.4773, MinusLogProbMetric: 30.4773, val_loss: 30.1844, val_MinusLogProbMetric: 30.1844

Epoch 235: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.4773 - MinusLogProbMetric: 30.4773 - val_loss: 30.1844 - val_MinusLogProbMetric: 30.1844 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 236/1000
2023-09-29 23:52:31.334 
Epoch 236/1000 
	 loss: 30.3091, MinusLogProbMetric: 30.3091, val_loss: 30.3659, val_MinusLogProbMetric: 30.3659

Epoch 236: val_loss did not improve from 30.16103
196/196 - 35s - loss: 30.3091 - MinusLogProbMetric: 30.3091 - val_loss: 30.3659 - val_MinusLogProbMetric: 30.3659 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 237/1000
2023-09-29 23:53:06.101 
Epoch 237/1000 
	 loss: 30.5450, MinusLogProbMetric: 30.5450, val_loss: 31.9080, val_MinusLogProbMetric: 31.9080

Epoch 237: val_loss did not improve from 30.16103
196/196 - 35s - loss: 30.5450 - MinusLogProbMetric: 30.5450 - val_loss: 31.9080 - val_MinusLogProbMetric: 31.9080 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 238/1000
2023-09-29 23:53:39.785 
Epoch 238/1000 
	 loss: 30.3185, MinusLogProbMetric: 30.3185, val_loss: 30.7104, val_MinusLogProbMetric: 30.7104

Epoch 238: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.3185 - MinusLogProbMetric: 30.3185 - val_loss: 30.7104 - val_MinusLogProbMetric: 30.7104 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 239/1000
2023-09-29 23:54:14.415 
Epoch 239/1000 
	 loss: 30.4178, MinusLogProbMetric: 30.4178, val_loss: 30.3576, val_MinusLogProbMetric: 30.3576

Epoch 239: val_loss did not improve from 30.16103
196/196 - 35s - loss: 30.4178 - MinusLogProbMetric: 30.4178 - val_loss: 30.3576 - val_MinusLogProbMetric: 30.3576 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 240/1000
2023-09-29 23:54:48.800 
Epoch 240/1000 
	 loss: 30.3974, MinusLogProbMetric: 30.3974, val_loss: 30.2734, val_MinusLogProbMetric: 30.2734

Epoch 240: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.3974 - MinusLogProbMetric: 30.3974 - val_loss: 30.2734 - val_MinusLogProbMetric: 30.2734 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 241/1000
2023-09-29 23:55:23.021 
Epoch 241/1000 
	 loss: 30.3627, MinusLogProbMetric: 30.3627, val_loss: 32.5939, val_MinusLogProbMetric: 32.5939

Epoch 241: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.3627 - MinusLogProbMetric: 30.3627 - val_loss: 32.5939 - val_MinusLogProbMetric: 32.5939 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 242/1000
2023-09-29 23:55:56.523 
Epoch 242/1000 
	 loss: 30.3881, MinusLogProbMetric: 30.3881, val_loss: 30.2906, val_MinusLogProbMetric: 30.2906

Epoch 242: val_loss did not improve from 30.16103
196/196 - 34s - loss: 30.3881 - MinusLogProbMetric: 30.3881 - val_loss: 30.2906 - val_MinusLogProbMetric: 30.2906 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 243/1000
2023-09-29 23:56:26.990 
Epoch 243/1000 
	 loss: 30.4219, MinusLogProbMetric: 30.4219, val_loss: 30.0699, val_MinusLogProbMetric: 30.0699

Epoch 243: val_loss improved from 30.16103 to 30.06992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 31s - loss: 30.4219 - MinusLogProbMetric: 30.4219 - val_loss: 30.0699 - val_MinusLogProbMetric: 30.0699 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 244/1000
2023-09-29 23:57:00.954 
Epoch 244/1000 
	 loss: 30.4149, MinusLogProbMetric: 30.4149, val_loss: 30.2219, val_MinusLogProbMetric: 30.2219

Epoch 244: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.4149 - MinusLogProbMetric: 30.4149 - val_loss: 30.2219 - val_MinusLogProbMetric: 30.2219 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 245/1000
2023-09-29 23:57:34.522 
Epoch 245/1000 
	 loss: 30.3949, MinusLogProbMetric: 30.3949, val_loss: 31.0022, val_MinusLogProbMetric: 31.0022

Epoch 245: val_loss did not improve from 30.06992
196/196 - 34s - loss: 30.3949 - MinusLogProbMetric: 30.3949 - val_loss: 31.0022 - val_MinusLogProbMetric: 31.0022 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 246/1000
2023-09-29 23:58:08.156 
Epoch 246/1000 
	 loss: 30.3036, MinusLogProbMetric: 30.3036, val_loss: 31.4614, val_MinusLogProbMetric: 31.4614

Epoch 246: val_loss did not improve from 30.06992
196/196 - 34s - loss: 30.3036 - MinusLogProbMetric: 30.3036 - val_loss: 31.4614 - val_MinusLogProbMetric: 31.4614 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 247/1000
2023-09-29 23:58:40.519 
Epoch 247/1000 
	 loss: 30.4806, MinusLogProbMetric: 30.4806, val_loss: 31.1075, val_MinusLogProbMetric: 31.1075

Epoch 247: val_loss did not improve from 30.06992
196/196 - 32s - loss: 30.4806 - MinusLogProbMetric: 30.4806 - val_loss: 31.1075 - val_MinusLogProbMetric: 31.1075 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 248/1000
2023-09-29 23:59:13.700 
Epoch 248/1000 
	 loss: 30.2801, MinusLogProbMetric: 30.2801, val_loss: 31.6025, val_MinusLogProbMetric: 31.6025

Epoch 248: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.2801 - MinusLogProbMetric: 30.2801 - val_loss: 31.6025 - val_MinusLogProbMetric: 31.6025 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 249/1000
2023-09-29 23:59:47.250 
Epoch 249/1000 
	 loss: 30.4065, MinusLogProbMetric: 30.4065, val_loss: 30.4674, val_MinusLogProbMetric: 30.4674

Epoch 249: val_loss did not improve from 30.06992
196/196 - 34s - loss: 30.4065 - MinusLogProbMetric: 30.4065 - val_loss: 30.4674 - val_MinusLogProbMetric: 30.4674 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 250/1000
2023-09-30 00:00:19.815 
Epoch 250/1000 
	 loss: 30.3638, MinusLogProbMetric: 30.3638, val_loss: 31.0877, val_MinusLogProbMetric: 31.0877

Epoch 250: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.3638 - MinusLogProbMetric: 30.3638 - val_loss: 31.0877 - val_MinusLogProbMetric: 31.0877 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 251/1000
2023-09-30 00:00:53.048 
Epoch 251/1000 
	 loss: 30.3346, MinusLogProbMetric: 30.3346, val_loss: 30.9078, val_MinusLogProbMetric: 30.9078

Epoch 251: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.3346 - MinusLogProbMetric: 30.3346 - val_loss: 30.9078 - val_MinusLogProbMetric: 30.9078 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 252/1000
2023-09-30 00:01:24.321 
Epoch 252/1000 
	 loss: 30.3180, MinusLogProbMetric: 30.3180, val_loss: 30.4147, val_MinusLogProbMetric: 30.4147

Epoch 252: val_loss did not improve from 30.06992
196/196 - 31s - loss: 30.3180 - MinusLogProbMetric: 30.3180 - val_loss: 30.4147 - val_MinusLogProbMetric: 30.4147 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 253/1000
2023-09-30 00:01:52.403 
Epoch 253/1000 
	 loss: 30.2760, MinusLogProbMetric: 30.2760, val_loss: 30.0922, val_MinusLogProbMetric: 30.0922

Epoch 253: val_loss did not improve from 30.06992
196/196 - 28s - loss: 30.2760 - MinusLogProbMetric: 30.2760 - val_loss: 30.0922 - val_MinusLogProbMetric: 30.0922 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 254/1000
2023-09-30 00:02:22.877 
Epoch 254/1000 
	 loss: 30.4426, MinusLogProbMetric: 30.4426, val_loss: 30.5524, val_MinusLogProbMetric: 30.5524

Epoch 254: val_loss did not improve from 30.06992
196/196 - 30s - loss: 30.4426 - MinusLogProbMetric: 30.4426 - val_loss: 30.5524 - val_MinusLogProbMetric: 30.5524 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 255/1000
2023-09-30 00:02:51.695 
Epoch 255/1000 
	 loss: 30.4920, MinusLogProbMetric: 30.4920, val_loss: 30.9131, val_MinusLogProbMetric: 30.9131

Epoch 255: val_loss did not improve from 30.06992
196/196 - 29s - loss: 30.4920 - MinusLogProbMetric: 30.4920 - val_loss: 30.9131 - val_MinusLogProbMetric: 30.9131 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 256/1000
2023-09-30 00:03:22.344 
Epoch 256/1000 
	 loss: 30.3943, MinusLogProbMetric: 30.3943, val_loss: 30.3047, val_MinusLogProbMetric: 30.3047

Epoch 256: val_loss did not improve from 30.06992
196/196 - 31s - loss: 30.3943 - MinusLogProbMetric: 30.3943 - val_loss: 30.3047 - val_MinusLogProbMetric: 30.3047 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 257/1000
2023-09-30 00:03:50.189 
Epoch 257/1000 
	 loss: 30.2939, MinusLogProbMetric: 30.2939, val_loss: 30.7172, val_MinusLogProbMetric: 30.7172

Epoch 257: val_loss did not improve from 30.06992
196/196 - 28s - loss: 30.2939 - MinusLogProbMetric: 30.2939 - val_loss: 30.7172 - val_MinusLogProbMetric: 30.7172 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 258/1000
2023-09-30 00:04:19.757 
Epoch 258/1000 
	 loss: 30.4029, MinusLogProbMetric: 30.4029, val_loss: 30.2789, val_MinusLogProbMetric: 30.2789

Epoch 258: val_loss did not improve from 30.06992
196/196 - 30s - loss: 30.4029 - MinusLogProbMetric: 30.4029 - val_loss: 30.2789 - val_MinusLogProbMetric: 30.2789 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 259/1000
2023-09-30 00:04:47.474 
Epoch 259/1000 
	 loss: 30.2648, MinusLogProbMetric: 30.2648, val_loss: 30.4780, val_MinusLogProbMetric: 30.4780

Epoch 259: val_loss did not improve from 30.06992
196/196 - 28s - loss: 30.2648 - MinusLogProbMetric: 30.2648 - val_loss: 30.4780 - val_MinusLogProbMetric: 30.4780 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 260/1000
2023-09-30 00:05:15.767 
Epoch 260/1000 
	 loss: 30.3396, MinusLogProbMetric: 30.3396, val_loss: 33.6939, val_MinusLogProbMetric: 33.6939

Epoch 260: val_loss did not improve from 30.06992
196/196 - 28s - loss: 30.3396 - MinusLogProbMetric: 30.3396 - val_loss: 33.6939 - val_MinusLogProbMetric: 33.6939 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 261/1000
2023-09-30 00:05:45.469 
Epoch 261/1000 
	 loss: 30.3444, MinusLogProbMetric: 30.3444, val_loss: 30.2353, val_MinusLogProbMetric: 30.2353

Epoch 261: val_loss did not improve from 30.06992
196/196 - 30s - loss: 30.3444 - MinusLogProbMetric: 30.3444 - val_loss: 30.2353 - val_MinusLogProbMetric: 30.2353 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 262/1000
2023-09-30 00:06:17.326 
Epoch 262/1000 
	 loss: 30.1479, MinusLogProbMetric: 30.1479, val_loss: 30.3832, val_MinusLogProbMetric: 30.3832

Epoch 262: val_loss did not improve from 30.06992
196/196 - 32s - loss: 30.1479 - MinusLogProbMetric: 30.1479 - val_loss: 30.3832 - val_MinusLogProbMetric: 30.3832 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 263/1000
2023-09-30 00:06:50.821 
Epoch 263/1000 
	 loss: 30.3275, MinusLogProbMetric: 30.3275, val_loss: 30.4326, val_MinusLogProbMetric: 30.4326

Epoch 263: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.3275 - MinusLogProbMetric: 30.3275 - val_loss: 30.4326 - val_MinusLogProbMetric: 30.4326 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 264/1000
2023-09-30 00:07:24.347 
Epoch 264/1000 
	 loss: 30.2519, MinusLogProbMetric: 30.2519, val_loss: 30.1784, val_MinusLogProbMetric: 30.1784

Epoch 264: val_loss did not improve from 30.06992
196/196 - 34s - loss: 30.2519 - MinusLogProbMetric: 30.2519 - val_loss: 30.1784 - val_MinusLogProbMetric: 30.1784 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 265/1000
2023-09-30 00:07:57.055 
Epoch 265/1000 
	 loss: 30.2640, MinusLogProbMetric: 30.2640, val_loss: 30.1292, val_MinusLogProbMetric: 30.1292

Epoch 265: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.2640 - MinusLogProbMetric: 30.2640 - val_loss: 30.1292 - val_MinusLogProbMetric: 30.1292 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 266/1000
2023-09-30 00:08:30.685 
Epoch 266/1000 
	 loss: 30.2957, MinusLogProbMetric: 30.2957, val_loss: 31.3995, val_MinusLogProbMetric: 31.3995

Epoch 266: val_loss did not improve from 30.06992
196/196 - 34s - loss: 30.2957 - MinusLogProbMetric: 30.2957 - val_loss: 31.3995 - val_MinusLogProbMetric: 31.3995 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 267/1000
2023-09-30 00:08:59.785 
Epoch 267/1000 
	 loss: 30.3023, MinusLogProbMetric: 30.3023, val_loss: 31.2856, val_MinusLogProbMetric: 31.2856

Epoch 267: val_loss did not improve from 30.06992
196/196 - 29s - loss: 30.3023 - MinusLogProbMetric: 30.3023 - val_loss: 31.2856 - val_MinusLogProbMetric: 31.2856 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 268/1000
2023-09-30 00:09:31.980 
Epoch 268/1000 
	 loss: 30.2710, MinusLogProbMetric: 30.2710, val_loss: 30.2924, val_MinusLogProbMetric: 30.2924

Epoch 268: val_loss did not improve from 30.06992
196/196 - 32s - loss: 30.2710 - MinusLogProbMetric: 30.2710 - val_loss: 30.2924 - val_MinusLogProbMetric: 30.2924 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 269/1000
2023-09-30 00:10:04.832 
Epoch 269/1000 
	 loss: 30.2760, MinusLogProbMetric: 30.2760, val_loss: 30.7022, val_MinusLogProbMetric: 30.7022

Epoch 269: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.2760 - MinusLogProbMetric: 30.2760 - val_loss: 30.7022 - val_MinusLogProbMetric: 30.7022 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 270/1000
2023-09-30 00:10:38.204 
Epoch 270/1000 
	 loss: 30.2277, MinusLogProbMetric: 30.2277, val_loss: 30.5706, val_MinusLogProbMetric: 30.5706

Epoch 270: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.2277 - MinusLogProbMetric: 30.2277 - val_loss: 30.5706 - val_MinusLogProbMetric: 30.5706 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 271/1000
2023-09-30 00:11:11.249 
Epoch 271/1000 
	 loss: 30.1927, MinusLogProbMetric: 30.1927, val_loss: 30.2471, val_MinusLogProbMetric: 30.2471

Epoch 271: val_loss did not improve from 30.06992
196/196 - 33s - loss: 30.1927 - MinusLogProbMetric: 30.1927 - val_loss: 30.2471 - val_MinusLogProbMetric: 30.2471 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 272/1000
2023-09-30 00:11:40.924 
Epoch 272/1000 
	 loss: 30.1943, MinusLogProbMetric: 30.1943, val_loss: 30.3273, val_MinusLogProbMetric: 30.3273

Epoch 272: val_loss did not improve from 30.06992
196/196 - 30s - loss: 30.1943 - MinusLogProbMetric: 30.1943 - val_loss: 30.3273 - val_MinusLogProbMetric: 30.3273 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 273/1000
2023-09-30 00:12:12.449 
Epoch 273/1000 
	 loss: 30.1039, MinusLogProbMetric: 30.1039, val_loss: 31.4765, val_MinusLogProbMetric: 31.4765

Epoch 273: val_loss did not improve from 30.06992
196/196 - 32s - loss: 30.1039 - MinusLogProbMetric: 30.1039 - val_loss: 31.4765 - val_MinusLogProbMetric: 31.4765 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 274/1000
2023-09-30 00:12:41.268 
Epoch 274/1000 
	 loss: 30.3197, MinusLogProbMetric: 30.3197, val_loss: 30.6874, val_MinusLogProbMetric: 30.6874

Epoch 274: val_loss did not improve from 30.06992
196/196 - 29s - loss: 30.3197 - MinusLogProbMetric: 30.3197 - val_loss: 30.6874 - val_MinusLogProbMetric: 30.6874 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 275/1000
2023-09-30 00:13:10.013 
Epoch 275/1000 
	 loss: 30.1405, MinusLogProbMetric: 30.1405, val_loss: 31.0686, val_MinusLogProbMetric: 31.0686

Epoch 275: val_loss did not improve from 30.06992
196/196 - 29s - loss: 30.1405 - MinusLogProbMetric: 30.1405 - val_loss: 31.0686 - val_MinusLogProbMetric: 31.0686 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 276/1000
2023-09-30 00:13:40.985 
Epoch 276/1000 
	 loss: 30.2927, MinusLogProbMetric: 30.2927, val_loss: 31.1590, val_MinusLogProbMetric: 31.1590

Epoch 276: val_loss did not improve from 30.06992
196/196 - 31s - loss: 30.2927 - MinusLogProbMetric: 30.2927 - val_loss: 31.1590 - val_MinusLogProbMetric: 31.1590 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 277/1000
2023-09-30 00:14:10.949 
Epoch 277/1000 
	 loss: 30.1069, MinusLogProbMetric: 30.1069, val_loss: 29.9718, val_MinusLogProbMetric: 29.9718

Epoch 277: val_loss improved from 30.06992 to 29.97180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 30s - loss: 30.1069 - MinusLogProbMetric: 30.1069 - val_loss: 29.9718 - val_MinusLogProbMetric: 29.9718 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 278/1000
2023-09-30 00:14:42.393 
Epoch 278/1000 
	 loss: 30.0553, MinusLogProbMetric: 30.0553, val_loss: 30.3769, val_MinusLogProbMetric: 30.3769

Epoch 278: val_loss did not improve from 29.97180
196/196 - 31s - loss: 30.0553 - MinusLogProbMetric: 30.0553 - val_loss: 30.3769 - val_MinusLogProbMetric: 30.3769 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 279/1000
2023-09-30 00:15:13.289 
Epoch 279/1000 
	 loss: 30.3147, MinusLogProbMetric: 30.3147, val_loss: 30.6152, val_MinusLogProbMetric: 30.6152

Epoch 279: val_loss did not improve from 29.97180
196/196 - 31s - loss: 30.3147 - MinusLogProbMetric: 30.3147 - val_loss: 30.6152 - val_MinusLogProbMetric: 30.6152 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 280/1000
2023-09-30 00:15:40.597 
Epoch 280/1000 
	 loss: 30.2177, MinusLogProbMetric: 30.2177, val_loss: 30.2378, val_MinusLogProbMetric: 30.2378

Epoch 280: val_loss did not improve from 29.97180
196/196 - 27s - loss: 30.2177 - MinusLogProbMetric: 30.2177 - val_loss: 30.2378 - val_MinusLogProbMetric: 30.2378 - lr: 0.0010 - 27s/epoch - 139ms/step
Epoch 281/1000
2023-09-30 00:16:11.014 
Epoch 281/1000 
	 loss: 30.0672, MinusLogProbMetric: 30.0672, val_loss: 30.6177, val_MinusLogProbMetric: 30.6177

Epoch 281: val_loss did not improve from 29.97180
196/196 - 30s - loss: 30.0672 - MinusLogProbMetric: 30.0672 - val_loss: 30.6177 - val_MinusLogProbMetric: 30.6177 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 282/1000
2023-09-30 00:16:43.240 
Epoch 282/1000 
	 loss: 30.1891, MinusLogProbMetric: 30.1891, val_loss: 30.7026, val_MinusLogProbMetric: 30.7026

Epoch 282: val_loss did not improve from 29.97180
196/196 - 32s - loss: 30.1891 - MinusLogProbMetric: 30.1891 - val_loss: 30.7026 - val_MinusLogProbMetric: 30.7026 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 283/1000
2023-09-30 00:17:16.809 
Epoch 283/1000 
	 loss: 30.1649, MinusLogProbMetric: 30.1649, val_loss: 30.4621, val_MinusLogProbMetric: 30.4621

Epoch 283: val_loss did not improve from 29.97180
196/196 - 34s - loss: 30.1649 - MinusLogProbMetric: 30.1649 - val_loss: 30.4621 - val_MinusLogProbMetric: 30.4621 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 284/1000
2023-09-30 00:17:49.635 
Epoch 284/1000 
	 loss: 30.1910, MinusLogProbMetric: 30.1910, val_loss: 30.6453, val_MinusLogProbMetric: 30.6453

Epoch 284: val_loss did not improve from 29.97180
196/196 - 33s - loss: 30.1910 - MinusLogProbMetric: 30.1910 - val_loss: 30.6453 - val_MinusLogProbMetric: 30.6453 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 285/1000
2023-09-30 00:18:20.123 
Epoch 285/1000 
	 loss: 30.2098, MinusLogProbMetric: 30.2098, val_loss: 30.9281, val_MinusLogProbMetric: 30.9281

Epoch 285: val_loss did not improve from 29.97180
196/196 - 30s - loss: 30.2098 - MinusLogProbMetric: 30.2098 - val_loss: 30.9281 - val_MinusLogProbMetric: 30.9281 - lr: 0.0010 - 30s/epoch - 156ms/step
Epoch 286/1000
2023-09-30 00:18:53.396 
Epoch 286/1000 
	 loss: 30.0788, MinusLogProbMetric: 30.0788, val_loss: 30.8617, val_MinusLogProbMetric: 30.8617

Epoch 286: val_loss did not improve from 29.97180
196/196 - 33s - loss: 30.0788 - MinusLogProbMetric: 30.0788 - val_loss: 30.8617 - val_MinusLogProbMetric: 30.8617 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 287/1000
2023-09-30 00:19:26.617 
Epoch 287/1000 
	 loss: 30.3052, MinusLogProbMetric: 30.3052, val_loss: 30.1814, val_MinusLogProbMetric: 30.1814

Epoch 287: val_loss did not improve from 29.97180
196/196 - 33s - loss: 30.3052 - MinusLogProbMetric: 30.3052 - val_loss: 30.1814 - val_MinusLogProbMetric: 30.1814 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 288/1000
2023-09-30 00:19:59.775 
Epoch 288/1000 
	 loss: 30.0953, MinusLogProbMetric: 30.0953, val_loss: 30.4808, val_MinusLogProbMetric: 30.4808

Epoch 288: val_loss did not improve from 29.97180
196/196 - 33s - loss: 30.0953 - MinusLogProbMetric: 30.0953 - val_loss: 30.4808 - val_MinusLogProbMetric: 30.4808 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 289/1000
2023-09-30 00:20:31.953 
Epoch 289/1000 
	 loss: 30.0951, MinusLogProbMetric: 30.0951, val_loss: 30.2196, val_MinusLogProbMetric: 30.2196

Epoch 289: val_loss did not improve from 29.97180
196/196 - 32s - loss: 30.0951 - MinusLogProbMetric: 30.0951 - val_loss: 30.2196 - val_MinusLogProbMetric: 30.2196 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 290/1000
2023-09-30 00:21:05.317 
Epoch 290/1000 
	 loss: 30.1413, MinusLogProbMetric: 30.1413, val_loss: 30.1631, val_MinusLogProbMetric: 30.1631

Epoch 290: val_loss did not improve from 29.97180
196/196 - 33s - loss: 30.1413 - MinusLogProbMetric: 30.1413 - val_loss: 30.1631 - val_MinusLogProbMetric: 30.1631 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 291/1000
2023-09-30 00:21:37.360 
Epoch 291/1000 
	 loss: 30.1008, MinusLogProbMetric: 30.1008, val_loss: 30.0020, val_MinusLogProbMetric: 30.0020

Epoch 291: val_loss did not improve from 29.97180
196/196 - 32s - loss: 30.1008 - MinusLogProbMetric: 30.1008 - val_loss: 30.0020 - val_MinusLogProbMetric: 30.0020 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 292/1000
2023-09-30 00:22:07.268 
Epoch 292/1000 
	 loss: 30.1277, MinusLogProbMetric: 30.1277, val_loss: 30.1447, val_MinusLogProbMetric: 30.1447

Epoch 292: val_loss did not improve from 29.97180
196/196 - 30s - loss: 30.1277 - MinusLogProbMetric: 30.1277 - val_loss: 30.1447 - val_MinusLogProbMetric: 30.1447 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 293/1000
2023-09-30 00:22:39.551 
Epoch 293/1000 
	 loss: 29.9679, MinusLogProbMetric: 29.9679, val_loss: 30.5803, val_MinusLogProbMetric: 30.5803

Epoch 293: val_loss did not improve from 29.97180
196/196 - 32s - loss: 29.9679 - MinusLogProbMetric: 29.9679 - val_loss: 30.5803 - val_MinusLogProbMetric: 30.5803 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 294/1000
2023-09-30 00:23:10.556 
Epoch 294/1000 
	 loss: 30.2831, MinusLogProbMetric: 30.2831, val_loss: 30.7145, val_MinusLogProbMetric: 30.7145

Epoch 294: val_loss did not improve from 29.97180
196/196 - 31s - loss: 30.2831 - MinusLogProbMetric: 30.2831 - val_loss: 30.7145 - val_MinusLogProbMetric: 30.7145 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 295/1000
2023-09-30 00:23:38.170 
Epoch 295/1000 
	 loss: 30.0718, MinusLogProbMetric: 30.0718, val_loss: 30.4462, val_MinusLogProbMetric: 30.4462

Epoch 295: val_loss did not improve from 29.97180
196/196 - 28s - loss: 30.0718 - MinusLogProbMetric: 30.0718 - val_loss: 30.4462 - val_MinusLogProbMetric: 30.4462 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 296/1000
2023-09-30 00:24:08.341 
Epoch 296/1000 
	 loss: 30.1121, MinusLogProbMetric: 30.1121, val_loss: 29.9568, val_MinusLogProbMetric: 29.9568

Epoch 296: val_loss improved from 29.97180 to 29.95677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 31s - loss: 30.1121 - MinusLogProbMetric: 30.1121 - val_loss: 29.9568 - val_MinusLogProbMetric: 29.9568 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 297/1000
2023-09-30 00:24:40.828 
Epoch 297/1000 
	 loss: 30.1416, MinusLogProbMetric: 30.1416, val_loss: 29.9907, val_MinusLogProbMetric: 29.9907

Epoch 297: val_loss did not improve from 29.95677
196/196 - 32s - loss: 30.1416 - MinusLogProbMetric: 30.1416 - val_loss: 29.9907 - val_MinusLogProbMetric: 29.9907 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 298/1000
2023-09-30 00:25:11.349 
Epoch 298/1000 
	 loss: 29.9912, MinusLogProbMetric: 29.9912, val_loss: 30.5625, val_MinusLogProbMetric: 30.5625

Epoch 298: val_loss did not improve from 29.95677
196/196 - 31s - loss: 29.9912 - MinusLogProbMetric: 29.9912 - val_loss: 30.5625 - val_MinusLogProbMetric: 30.5625 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 299/1000
2023-09-30 00:25:40.522 
Epoch 299/1000 
	 loss: 30.1084, MinusLogProbMetric: 30.1084, val_loss: 30.2277, val_MinusLogProbMetric: 30.2277

Epoch 299: val_loss did not improve from 29.95677
196/196 - 29s - loss: 30.1084 - MinusLogProbMetric: 30.1084 - val_loss: 30.2277 - val_MinusLogProbMetric: 30.2277 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 300/1000
2023-09-30 00:26:11.111 
Epoch 300/1000 
	 loss: 29.8561, MinusLogProbMetric: 29.8561, val_loss: 30.1351, val_MinusLogProbMetric: 30.1351

Epoch 300: val_loss did not improve from 29.95677
196/196 - 31s - loss: 29.8561 - MinusLogProbMetric: 29.8561 - val_loss: 30.1351 - val_MinusLogProbMetric: 30.1351 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 301/1000
2023-09-30 00:26:43.755 
Epoch 301/1000 
	 loss: 30.1816, MinusLogProbMetric: 30.1816, val_loss: 29.9204, val_MinusLogProbMetric: 29.9204

Epoch 301: val_loss improved from 29.95677 to 29.92040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 30.1816 - MinusLogProbMetric: 30.1816 - val_loss: 29.9204 - val_MinusLogProbMetric: 29.9204 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 302/1000
2023-09-30 00:27:17.557 
Epoch 302/1000 
	 loss: 29.9948, MinusLogProbMetric: 29.9948, val_loss: 29.9984, val_MinusLogProbMetric: 29.9984

Epoch 302: val_loss did not improve from 29.92040
196/196 - 33s - loss: 29.9948 - MinusLogProbMetric: 29.9948 - val_loss: 29.9984 - val_MinusLogProbMetric: 29.9984 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 303/1000
2023-09-30 00:27:51.069 
Epoch 303/1000 
	 loss: 30.1138, MinusLogProbMetric: 30.1138, val_loss: 30.6802, val_MinusLogProbMetric: 30.6802

Epoch 303: val_loss did not improve from 29.92040
196/196 - 34s - loss: 30.1138 - MinusLogProbMetric: 30.1138 - val_loss: 30.6802 - val_MinusLogProbMetric: 30.6802 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 304/1000
2023-09-30 00:28:24.136 
Epoch 304/1000 
	 loss: 30.0277, MinusLogProbMetric: 30.0277, val_loss: 30.4054, val_MinusLogProbMetric: 30.4054

Epoch 304: val_loss did not improve from 29.92040
196/196 - 33s - loss: 30.0277 - MinusLogProbMetric: 30.0277 - val_loss: 30.4054 - val_MinusLogProbMetric: 30.4054 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 305/1000
2023-09-30 00:28:55.905 
Epoch 305/1000 
	 loss: 29.9779, MinusLogProbMetric: 29.9779, val_loss: 30.3691, val_MinusLogProbMetric: 30.3691

Epoch 305: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.9779 - MinusLogProbMetric: 29.9779 - val_loss: 30.3691 - val_MinusLogProbMetric: 30.3691 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 306/1000
2023-09-30 00:29:29.248 
Epoch 306/1000 
	 loss: 29.9495, MinusLogProbMetric: 29.9495, val_loss: 30.3922, val_MinusLogProbMetric: 30.3922

Epoch 306: val_loss did not improve from 29.92040
196/196 - 33s - loss: 29.9495 - MinusLogProbMetric: 29.9495 - val_loss: 30.3922 - val_MinusLogProbMetric: 30.3922 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 307/1000
2023-09-30 00:30:02.812 
Epoch 307/1000 
	 loss: 29.9928, MinusLogProbMetric: 29.9928, val_loss: 30.7906, val_MinusLogProbMetric: 30.7906

Epoch 307: val_loss did not improve from 29.92040
196/196 - 34s - loss: 29.9928 - MinusLogProbMetric: 29.9928 - val_loss: 30.7906 - val_MinusLogProbMetric: 30.7906 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 308/1000
2023-09-30 00:30:36.080 
Epoch 308/1000 
	 loss: 30.0315, MinusLogProbMetric: 30.0315, val_loss: 30.1498, val_MinusLogProbMetric: 30.1498

Epoch 308: val_loss did not improve from 29.92040
196/196 - 33s - loss: 30.0315 - MinusLogProbMetric: 30.0315 - val_loss: 30.1498 - val_MinusLogProbMetric: 30.1498 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 309/1000
2023-09-30 00:31:09.094 
Epoch 309/1000 
	 loss: 30.1076, MinusLogProbMetric: 30.1076, val_loss: 30.3325, val_MinusLogProbMetric: 30.3325

Epoch 309: val_loss did not improve from 29.92040
196/196 - 33s - loss: 30.1076 - MinusLogProbMetric: 30.1076 - val_loss: 30.3325 - val_MinusLogProbMetric: 30.3325 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 310/1000
2023-09-30 00:31:41.328 
Epoch 310/1000 
	 loss: 29.8973, MinusLogProbMetric: 29.8973, val_loss: 30.1592, val_MinusLogProbMetric: 30.1592

Epoch 310: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.8973 - MinusLogProbMetric: 29.8973 - val_loss: 30.1592 - val_MinusLogProbMetric: 30.1592 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 311/1000
2023-09-30 00:32:11.265 
Epoch 311/1000 
	 loss: 29.8882, MinusLogProbMetric: 29.8882, val_loss: 31.1760, val_MinusLogProbMetric: 31.1760

Epoch 311: val_loss did not improve from 29.92040
196/196 - 30s - loss: 29.8882 - MinusLogProbMetric: 29.8882 - val_loss: 31.1760 - val_MinusLogProbMetric: 31.1760 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 312/1000
2023-09-30 00:32:40.888 
Epoch 312/1000 
	 loss: 30.0283, MinusLogProbMetric: 30.0283, val_loss: 30.6014, val_MinusLogProbMetric: 30.6014

Epoch 312: val_loss did not improve from 29.92040
196/196 - 30s - loss: 30.0283 - MinusLogProbMetric: 30.0283 - val_loss: 30.6014 - val_MinusLogProbMetric: 30.6014 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 313/1000
2023-09-30 00:33:10.991 
Epoch 313/1000 
	 loss: 29.9293, MinusLogProbMetric: 29.9293, val_loss: 30.0635, val_MinusLogProbMetric: 30.0635

Epoch 313: val_loss did not improve from 29.92040
196/196 - 30s - loss: 29.9293 - MinusLogProbMetric: 29.9293 - val_loss: 30.0635 - val_MinusLogProbMetric: 30.0635 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 314/1000
2023-09-30 00:33:41.589 
Epoch 314/1000 
	 loss: 29.9886, MinusLogProbMetric: 29.9886, val_loss: 31.0612, val_MinusLogProbMetric: 31.0612

Epoch 314: val_loss did not improve from 29.92040
196/196 - 31s - loss: 29.9886 - MinusLogProbMetric: 29.9886 - val_loss: 31.0612 - val_MinusLogProbMetric: 31.0612 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 315/1000
2023-09-30 00:34:13.109 
Epoch 315/1000 
	 loss: 29.9168, MinusLogProbMetric: 29.9168, val_loss: 30.2643, val_MinusLogProbMetric: 30.2643

Epoch 315: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.9168 - MinusLogProbMetric: 29.9168 - val_loss: 30.2643 - val_MinusLogProbMetric: 30.2643 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 316/1000
2023-09-30 00:34:46.269 
Epoch 316/1000 
	 loss: 30.0781, MinusLogProbMetric: 30.0781, val_loss: 30.6291, val_MinusLogProbMetric: 30.6291

Epoch 316: val_loss did not improve from 29.92040
196/196 - 33s - loss: 30.0781 - MinusLogProbMetric: 30.0781 - val_loss: 30.6291 - val_MinusLogProbMetric: 30.6291 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 317/1000
2023-09-30 00:35:15.704 
Epoch 317/1000 
	 loss: 29.9626, MinusLogProbMetric: 29.9626, val_loss: 30.4947, val_MinusLogProbMetric: 30.4947

Epoch 317: val_loss did not improve from 29.92040
196/196 - 29s - loss: 29.9626 - MinusLogProbMetric: 29.9626 - val_loss: 30.4947 - val_MinusLogProbMetric: 30.4947 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 318/1000
2023-09-30 00:35:45.956 
Epoch 318/1000 
	 loss: 30.0396, MinusLogProbMetric: 30.0396, val_loss: 30.8492, val_MinusLogProbMetric: 30.8492

Epoch 318: val_loss did not improve from 29.92040
196/196 - 30s - loss: 30.0396 - MinusLogProbMetric: 30.0396 - val_loss: 30.8492 - val_MinusLogProbMetric: 30.8492 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 319/1000
2023-09-30 00:36:18.097 
Epoch 319/1000 
	 loss: 30.1143, MinusLogProbMetric: 30.1143, val_loss: 30.1724, val_MinusLogProbMetric: 30.1724

Epoch 319: val_loss did not improve from 29.92040
196/196 - 32s - loss: 30.1143 - MinusLogProbMetric: 30.1143 - val_loss: 30.1724 - val_MinusLogProbMetric: 30.1724 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 320/1000
2023-09-30 00:36:47.751 
Epoch 320/1000 
	 loss: 29.8620, MinusLogProbMetric: 29.8620, val_loss: 30.2209, val_MinusLogProbMetric: 30.2209

Epoch 320: val_loss did not improve from 29.92040
196/196 - 30s - loss: 29.8620 - MinusLogProbMetric: 29.8620 - val_loss: 30.2209 - val_MinusLogProbMetric: 30.2209 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 321/1000
2023-09-30 00:37:18.474 
Epoch 321/1000 
	 loss: 29.9787, MinusLogProbMetric: 29.9787, val_loss: 29.9998, val_MinusLogProbMetric: 29.9998

Epoch 321: val_loss did not improve from 29.92040
196/196 - 31s - loss: 29.9787 - MinusLogProbMetric: 29.9787 - val_loss: 29.9998 - val_MinusLogProbMetric: 29.9998 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 322/1000
2023-09-30 00:37:50.840 
Epoch 322/1000 
	 loss: 30.1138, MinusLogProbMetric: 30.1138, val_loss: 30.2800, val_MinusLogProbMetric: 30.2800

Epoch 322: val_loss did not improve from 29.92040
196/196 - 32s - loss: 30.1138 - MinusLogProbMetric: 30.1138 - val_loss: 30.2800 - val_MinusLogProbMetric: 30.2800 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 323/1000
2023-09-30 00:38:20.241 
Epoch 323/1000 
	 loss: 30.1627, MinusLogProbMetric: 30.1627, val_loss: 30.5275, val_MinusLogProbMetric: 30.5275

Epoch 323: val_loss did not improve from 29.92040
196/196 - 29s - loss: 30.1627 - MinusLogProbMetric: 30.1627 - val_loss: 30.5275 - val_MinusLogProbMetric: 30.5275 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 324/1000
2023-09-30 00:38:52.212 
Epoch 324/1000 
	 loss: 29.8456, MinusLogProbMetric: 29.8456, val_loss: 30.0845, val_MinusLogProbMetric: 30.0845

Epoch 324: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.8456 - MinusLogProbMetric: 29.8456 - val_loss: 30.0845 - val_MinusLogProbMetric: 30.0845 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 325/1000
2023-09-30 00:39:24.090 
Epoch 325/1000 
	 loss: 29.8223, MinusLogProbMetric: 29.8223, val_loss: 30.1719, val_MinusLogProbMetric: 30.1719

Epoch 325: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.8223 - MinusLogProbMetric: 29.8223 - val_loss: 30.1719 - val_MinusLogProbMetric: 30.1719 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 326/1000
2023-09-30 00:39:57.021 
Epoch 326/1000 
	 loss: 29.7941, MinusLogProbMetric: 29.7941, val_loss: 30.3787, val_MinusLogProbMetric: 30.3787

Epoch 326: val_loss did not improve from 29.92040
196/196 - 33s - loss: 29.7941 - MinusLogProbMetric: 29.7941 - val_loss: 30.3787 - val_MinusLogProbMetric: 30.3787 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 327/1000
2023-09-30 00:40:28.982 
Epoch 327/1000 
	 loss: 29.8418, MinusLogProbMetric: 29.8418, val_loss: 30.0303, val_MinusLogProbMetric: 30.0303

Epoch 327: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.8418 - MinusLogProbMetric: 29.8418 - val_loss: 30.0303 - val_MinusLogProbMetric: 30.0303 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 328/1000
2023-09-30 00:41:01.674 
Epoch 328/1000 
	 loss: 29.9532, MinusLogProbMetric: 29.9532, val_loss: 31.0157, val_MinusLogProbMetric: 31.0157

Epoch 328: val_loss did not improve from 29.92040
196/196 - 33s - loss: 29.9532 - MinusLogProbMetric: 29.9532 - val_loss: 31.0157 - val_MinusLogProbMetric: 31.0157 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 329/1000
2023-09-30 00:41:33.931 
Epoch 329/1000 
	 loss: 29.8306, MinusLogProbMetric: 29.8306, val_loss: 29.9522, val_MinusLogProbMetric: 29.9522

Epoch 329: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.8306 - MinusLogProbMetric: 29.8306 - val_loss: 29.9522 - val_MinusLogProbMetric: 29.9522 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 330/1000
2023-09-30 00:42:06.357 
Epoch 330/1000 
	 loss: 29.8538, MinusLogProbMetric: 29.8538, val_loss: 30.5045, val_MinusLogProbMetric: 30.5045

Epoch 330: val_loss did not improve from 29.92040
196/196 - 32s - loss: 29.8538 - MinusLogProbMetric: 29.8538 - val_loss: 30.5045 - val_MinusLogProbMetric: 30.5045 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 331/1000
2023-09-30 00:42:33.076 
Epoch 331/1000 
	 loss: 29.9609, MinusLogProbMetric: 29.9609, val_loss: 30.3560, val_MinusLogProbMetric: 30.3560

Epoch 331: val_loss did not improve from 29.92040
196/196 - 27s - loss: 29.9609 - MinusLogProbMetric: 29.9609 - val_loss: 30.3560 - val_MinusLogProbMetric: 30.3560 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 332/1000
2023-09-30 00:43:03.508 
Epoch 332/1000 
	 loss: 29.9031, MinusLogProbMetric: 29.9031, val_loss: 31.3510, val_MinusLogProbMetric: 31.3510

Epoch 332: val_loss did not improve from 29.92040
196/196 - 30s - loss: 29.9031 - MinusLogProbMetric: 29.9031 - val_loss: 31.3510 - val_MinusLogProbMetric: 31.3510 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 333/1000
2023-09-30 00:43:33.757 
Epoch 333/1000 
	 loss: 29.9366, MinusLogProbMetric: 29.9366, val_loss: 30.4184, val_MinusLogProbMetric: 30.4184

Epoch 333: val_loss did not improve from 29.92040
196/196 - 30s - loss: 29.9366 - MinusLogProbMetric: 29.9366 - val_loss: 30.4184 - val_MinusLogProbMetric: 30.4184 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 334/1000
2023-09-30 00:44:05.115 
Epoch 334/1000 
	 loss: 29.8351, MinusLogProbMetric: 29.8351, val_loss: 29.8413, val_MinusLogProbMetric: 29.8413

Epoch 334: val_loss improved from 29.92040 to 29.84133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 32s - loss: 29.8351 - MinusLogProbMetric: 29.8351 - val_loss: 29.8413 - val_MinusLogProbMetric: 29.8413 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 335/1000
2023-09-30 00:44:35.814 
Epoch 335/1000 
	 loss: 30.0516, MinusLogProbMetric: 30.0516, val_loss: 30.5027, val_MinusLogProbMetric: 30.5027

Epoch 335: val_loss did not improve from 29.84133
196/196 - 30s - loss: 30.0516 - MinusLogProbMetric: 30.0516 - val_loss: 30.5027 - val_MinusLogProbMetric: 30.5027 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 336/1000
2023-09-30 00:45:07.951 
Epoch 336/1000 
	 loss: 29.9930, MinusLogProbMetric: 29.9930, val_loss: 30.3969, val_MinusLogProbMetric: 30.3969

Epoch 336: val_loss did not improve from 29.84133
196/196 - 32s - loss: 29.9930 - MinusLogProbMetric: 29.9930 - val_loss: 30.3969 - val_MinusLogProbMetric: 30.3969 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 337/1000
2023-09-30 00:45:36.140 
Epoch 337/1000 
	 loss: 29.8551, MinusLogProbMetric: 29.8551, val_loss: 29.9051, val_MinusLogProbMetric: 29.9051

Epoch 337: val_loss did not improve from 29.84133
196/196 - 28s - loss: 29.8551 - MinusLogProbMetric: 29.8551 - val_loss: 29.9051 - val_MinusLogProbMetric: 29.9051 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 338/1000
2023-09-30 00:46:07.011 
Epoch 338/1000 
	 loss: 29.8540, MinusLogProbMetric: 29.8540, val_loss: 30.0238, val_MinusLogProbMetric: 30.0238

Epoch 338: val_loss did not improve from 29.84133
196/196 - 31s - loss: 29.8540 - MinusLogProbMetric: 29.8540 - val_loss: 30.0238 - val_MinusLogProbMetric: 30.0238 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 339/1000
2023-09-30 00:46:36.376 
Epoch 339/1000 
	 loss: 29.6932, MinusLogProbMetric: 29.6932, val_loss: 30.5630, val_MinusLogProbMetric: 30.5630

Epoch 339: val_loss did not improve from 29.84133
196/196 - 29s - loss: 29.6932 - MinusLogProbMetric: 29.6932 - val_loss: 30.5630 - val_MinusLogProbMetric: 30.5630 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 340/1000
2023-09-30 00:47:06.976 
Epoch 340/1000 
	 loss: 29.9402, MinusLogProbMetric: 29.9402, val_loss: 30.6047, val_MinusLogProbMetric: 30.6047

Epoch 340: val_loss did not improve from 29.84133
196/196 - 31s - loss: 29.9402 - MinusLogProbMetric: 29.9402 - val_loss: 30.6047 - val_MinusLogProbMetric: 30.6047 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 341/1000
2023-09-30 00:47:40.100 
Epoch 341/1000 
	 loss: 29.9027, MinusLogProbMetric: 29.9027, val_loss: 30.2156, val_MinusLogProbMetric: 30.2156

Epoch 341: val_loss did not improve from 29.84133
196/196 - 33s - loss: 29.9027 - MinusLogProbMetric: 29.9027 - val_loss: 30.2156 - val_MinusLogProbMetric: 30.2156 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 342/1000
2023-09-30 00:48:10.134 
Epoch 342/1000 
	 loss: 29.7639, MinusLogProbMetric: 29.7639, val_loss: 30.5106, val_MinusLogProbMetric: 30.5106

Epoch 342: val_loss did not improve from 29.84133
196/196 - 30s - loss: 29.7639 - MinusLogProbMetric: 29.7639 - val_loss: 30.5106 - val_MinusLogProbMetric: 30.5106 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 343/1000
2023-09-30 00:48:39.170 
Epoch 343/1000 
	 loss: 29.7565, MinusLogProbMetric: 29.7565, val_loss: 29.9931, val_MinusLogProbMetric: 29.9931

Epoch 343: val_loss did not improve from 29.84133
196/196 - 29s - loss: 29.7565 - MinusLogProbMetric: 29.7565 - val_loss: 29.9931 - val_MinusLogProbMetric: 29.9931 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 344/1000
2023-09-30 00:49:09.014 
Epoch 344/1000 
	 loss: 29.8424, MinusLogProbMetric: 29.8424, val_loss: 29.9283, val_MinusLogProbMetric: 29.9283

Epoch 344: val_loss did not improve from 29.84133
196/196 - 30s - loss: 29.8424 - MinusLogProbMetric: 29.8424 - val_loss: 29.9283 - val_MinusLogProbMetric: 29.9283 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 345/1000
2023-09-30 00:49:38.714 
Epoch 345/1000 
	 loss: 29.7713, MinusLogProbMetric: 29.7713, val_loss: 30.4366, val_MinusLogProbMetric: 30.4366

Epoch 345: val_loss did not improve from 29.84133
196/196 - 30s - loss: 29.7713 - MinusLogProbMetric: 29.7713 - val_loss: 30.4366 - val_MinusLogProbMetric: 30.4366 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 346/1000
2023-09-30 00:50:08.562 
Epoch 346/1000 
	 loss: 29.8767, MinusLogProbMetric: 29.8767, val_loss: 29.8851, val_MinusLogProbMetric: 29.8851

Epoch 346: val_loss did not improve from 29.84133
196/196 - 30s - loss: 29.8767 - MinusLogProbMetric: 29.8767 - val_loss: 29.8851 - val_MinusLogProbMetric: 29.8851 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 347/1000
2023-09-30 00:50:38.672 
Epoch 347/1000 
	 loss: 30.0004, MinusLogProbMetric: 30.0004, val_loss: 30.3479, val_MinusLogProbMetric: 30.3479

Epoch 347: val_loss did not improve from 29.84133
196/196 - 30s - loss: 30.0004 - MinusLogProbMetric: 30.0004 - val_loss: 30.3479 - val_MinusLogProbMetric: 30.3479 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 348/1000
2023-09-30 00:51:06.495 
Epoch 348/1000 
	 loss: 29.8375, MinusLogProbMetric: 29.8375, val_loss: 30.1807, val_MinusLogProbMetric: 30.1807

Epoch 348: val_loss did not improve from 29.84133
196/196 - 28s - loss: 29.8375 - MinusLogProbMetric: 29.8375 - val_loss: 30.1807 - val_MinusLogProbMetric: 30.1807 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 349/1000
2023-09-30 00:51:35.865 
Epoch 349/1000 
	 loss: 29.9599, MinusLogProbMetric: 29.9599, val_loss: 29.8231, val_MinusLogProbMetric: 29.8231

Epoch 349: val_loss improved from 29.84133 to 29.82311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 30s - loss: 29.9599 - MinusLogProbMetric: 29.9599 - val_loss: 29.8231 - val_MinusLogProbMetric: 29.8231 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 350/1000
2023-09-30 00:52:04.210 
Epoch 350/1000 
	 loss: 29.7061, MinusLogProbMetric: 29.7061, val_loss: 30.0891, val_MinusLogProbMetric: 30.0891

Epoch 350: val_loss did not improve from 29.82311
196/196 - 28s - loss: 29.7061 - MinusLogProbMetric: 29.7061 - val_loss: 30.0891 - val_MinusLogProbMetric: 30.0891 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 351/1000
2023-09-30 00:52:33.321 
Epoch 351/1000 
	 loss: 29.8735, MinusLogProbMetric: 29.8735, val_loss: 30.1080, val_MinusLogProbMetric: 30.1080

Epoch 351: val_loss did not improve from 29.82311
196/196 - 29s - loss: 29.8735 - MinusLogProbMetric: 29.8735 - val_loss: 30.1080 - val_MinusLogProbMetric: 30.1080 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 352/1000
2023-09-30 00:53:01.241 
Epoch 352/1000 
	 loss: 29.8443, MinusLogProbMetric: 29.8443, val_loss: 30.2871, val_MinusLogProbMetric: 30.2871

Epoch 352: val_loss did not improve from 29.82311
196/196 - 28s - loss: 29.8443 - MinusLogProbMetric: 29.8443 - val_loss: 30.2871 - val_MinusLogProbMetric: 30.2871 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 353/1000
2023-09-30 00:53:28.799 
Epoch 353/1000 
	 loss: 29.6906, MinusLogProbMetric: 29.6906, val_loss: 31.3604, val_MinusLogProbMetric: 31.3604

Epoch 353: val_loss did not improve from 29.82311
196/196 - 28s - loss: 29.6906 - MinusLogProbMetric: 29.6906 - val_loss: 31.3604 - val_MinusLogProbMetric: 31.3604 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 354/1000
2023-09-30 00:53:56.147 
Epoch 354/1000 
	 loss: 29.8082, MinusLogProbMetric: 29.8082, val_loss: 30.4747, val_MinusLogProbMetric: 30.4747

Epoch 354: val_loss did not improve from 29.82311
196/196 - 27s - loss: 29.8082 - MinusLogProbMetric: 29.8082 - val_loss: 30.4747 - val_MinusLogProbMetric: 30.4747 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 355/1000
2023-09-30 00:54:27.275 
Epoch 355/1000 
	 loss: 29.6570, MinusLogProbMetric: 29.6570, val_loss: 30.5205, val_MinusLogProbMetric: 30.5205

Epoch 355: val_loss did not improve from 29.82311
196/196 - 31s - loss: 29.6570 - MinusLogProbMetric: 29.6570 - val_loss: 30.5205 - val_MinusLogProbMetric: 30.5205 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 356/1000
2023-09-30 00:54:57.888 
Epoch 356/1000 
	 loss: 29.9162, MinusLogProbMetric: 29.9162, val_loss: 30.6522, val_MinusLogProbMetric: 30.6522

Epoch 356: val_loss did not improve from 29.82311
196/196 - 31s - loss: 29.9162 - MinusLogProbMetric: 29.9162 - val_loss: 30.6522 - val_MinusLogProbMetric: 30.6522 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 357/1000
2023-09-30 00:55:25.895 
Epoch 357/1000 
	 loss: 29.7747, MinusLogProbMetric: 29.7747, val_loss: 30.3931, val_MinusLogProbMetric: 30.3931

Epoch 357: val_loss did not improve from 29.82311
196/196 - 28s - loss: 29.7747 - MinusLogProbMetric: 29.7747 - val_loss: 30.3931 - val_MinusLogProbMetric: 30.3931 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 358/1000
2023-09-30 00:55:53.941 
Epoch 358/1000 
	 loss: 29.8099, MinusLogProbMetric: 29.8099, val_loss: 30.3075, val_MinusLogProbMetric: 30.3075

Epoch 358: val_loss did not improve from 29.82311
196/196 - 28s - loss: 29.8099 - MinusLogProbMetric: 29.8099 - val_loss: 30.3075 - val_MinusLogProbMetric: 30.3075 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 359/1000
2023-09-30 00:56:20.735 
Epoch 359/1000 
	 loss: 29.6931, MinusLogProbMetric: 29.6931, val_loss: 29.8576, val_MinusLogProbMetric: 29.8576

Epoch 359: val_loss did not improve from 29.82311
196/196 - 27s - loss: 29.6931 - MinusLogProbMetric: 29.6931 - val_loss: 29.8576 - val_MinusLogProbMetric: 29.8576 - lr: 0.0010 - 27s/epoch - 137ms/step
Epoch 360/1000
2023-09-30 00:56:49.959 
Epoch 360/1000 
	 loss: 29.7168, MinusLogProbMetric: 29.7168, val_loss: 30.2426, val_MinusLogProbMetric: 30.2426

Epoch 360: val_loss did not improve from 29.82311
196/196 - 29s - loss: 29.7168 - MinusLogProbMetric: 29.7168 - val_loss: 30.2426 - val_MinusLogProbMetric: 30.2426 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 361/1000
2023-09-30 00:57:22.799 
Epoch 361/1000 
	 loss: 29.7147, MinusLogProbMetric: 29.7147, val_loss: 30.0654, val_MinusLogProbMetric: 30.0654

Epoch 361: val_loss did not improve from 29.82311
196/196 - 33s - loss: 29.7147 - MinusLogProbMetric: 29.7147 - val_loss: 30.0654 - val_MinusLogProbMetric: 30.0654 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 362/1000
2023-09-30 00:57:50.494 
Epoch 362/1000 
	 loss: 29.7300, MinusLogProbMetric: 29.7300, val_loss: 30.1576, val_MinusLogProbMetric: 30.1576

Epoch 362: val_loss did not improve from 29.82311
196/196 - 28s - loss: 29.7300 - MinusLogProbMetric: 29.7300 - val_loss: 30.1576 - val_MinusLogProbMetric: 30.1576 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 363/1000
2023-09-30 00:58:18.370 
Epoch 363/1000 
	 loss: 29.6623, MinusLogProbMetric: 29.6623, val_loss: 29.8592, val_MinusLogProbMetric: 29.8592

Epoch 363: val_loss did not improve from 29.82311
196/196 - 28s - loss: 29.6623 - MinusLogProbMetric: 29.6623 - val_loss: 29.8592 - val_MinusLogProbMetric: 29.8592 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 364/1000
2023-09-30 00:58:47.259 
Epoch 364/1000 
	 loss: 29.7423, MinusLogProbMetric: 29.7423, val_loss: 31.6306, val_MinusLogProbMetric: 31.6306

Epoch 364: val_loss did not improve from 29.82311
196/196 - 29s - loss: 29.7423 - MinusLogProbMetric: 29.7423 - val_loss: 31.6306 - val_MinusLogProbMetric: 31.6306 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 365/1000
2023-09-30 00:59:14.243 
Epoch 365/1000 
	 loss: 29.7466, MinusLogProbMetric: 29.7466, val_loss: 29.8052, val_MinusLogProbMetric: 29.8052

Epoch 365: val_loss improved from 29.82311 to 29.80517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 28s - loss: 29.7466 - MinusLogProbMetric: 29.7466 - val_loss: 29.8052 - val_MinusLogProbMetric: 29.8052 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 366/1000
2023-09-30 00:59:42.652 
Epoch 366/1000 
	 loss: 29.6672, MinusLogProbMetric: 29.6672, val_loss: 29.9639, val_MinusLogProbMetric: 29.9639

Epoch 366: val_loss did not improve from 29.80517
196/196 - 28s - loss: 29.6672 - MinusLogProbMetric: 29.6672 - val_loss: 29.9639 - val_MinusLogProbMetric: 29.9639 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 367/1000
2023-09-30 01:00:10.820 
Epoch 367/1000 
	 loss: 29.7857, MinusLogProbMetric: 29.7857, val_loss: 30.0342, val_MinusLogProbMetric: 30.0342

Epoch 367: val_loss did not improve from 29.80517
196/196 - 28s - loss: 29.7857 - MinusLogProbMetric: 29.7857 - val_loss: 30.0342 - val_MinusLogProbMetric: 30.0342 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 368/1000
2023-09-30 01:00:39.111 
Epoch 368/1000 
	 loss: 29.8422, MinusLogProbMetric: 29.8422, val_loss: 30.3139, val_MinusLogProbMetric: 30.3139

Epoch 368: val_loss did not improve from 29.80517
196/196 - 28s - loss: 29.8422 - MinusLogProbMetric: 29.8422 - val_loss: 30.3139 - val_MinusLogProbMetric: 30.3139 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 369/1000
2023-09-30 01:01:08.605 
Epoch 369/1000 
	 loss: 29.6200, MinusLogProbMetric: 29.6200, val_loss: 29.9259, val_MinusLogProbMetric: 29.9259

Epoch 369: val_loss did not improve from 29.80517
196/196 - 29s - loss: 29.6200 - MinusLogProbMetric: 29.6200 - val_loss: 29.9259 - val_MinusLogProbMetric: 29.9259 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 370/1000
2023-09-30 01:01:36.097 
Epoch 370/1000 
	 loss: 29.8353, MinusLogProbMetric: 29.8353, val_loss: 30.6569, val_MinusLogProbMetric: 30.6569

Epoch 370: val_loss did not improve from 29.80517
196/196 - 27s - loss: 29.8353 - MinusLogProbMetric: 29.8353 - val_loss: 30.6569 - val_MinusLogProbMetric: 30.6569 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 371/1000
2023-09-30 01:02:03.748 
Epoch 371/1000 
	 loss: 29.7054, MinusLogProbMetric: 29.7054, val_loss: 30.4677, val_MinusLogProbMetric: 30.4677

Epoch 371: val_loss did not improve from 29.80517
196/196 - 28s - loss: 29.7054 - MinusLogProbMetric: 29.7054 - val_loss: 30.4677 - val_MinusLogProbMetric: 30.4677 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 372/1000
2023-09-30 01:02:34.259 
Epoch 372/1000 
	 loss: 29.8781, MinusLogProbMetric: 29.8781, val_loss: 29.6444, val_MinusLogProbMetric: 29.6444

Epoch 372: val_loss improved from 29.80517 to 29.64441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 31s - loss: 29.8781 - MinusLogProbMetric: 29.8781 - val_loss: 29.6444 - val_MinusLogProbMetric: 29.6444 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 373/1000
2023-09-30 01:03:04.995 
Epoch 373/1000 
	 loss: 29.5728, MinusLogProbMetric: 29.5728, val_loss: 30.9198, val_MinusLogProbMetric: 30.9198

Epoch 373: val_loss did not improve from 29.64441
196/196 - 30s - loss: 29.5728 - MinusLogProbMetric: 29.5728 - val_loss: 30.9198 - val_MinusLogProbMetric: 30.9198 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 374/1000
2023-09-30 01:03:33.825 
Epoch 374/1000 
	 loss: 29.6486, MinusLogProbMetric: 29.6486, val_loss: 30.5837, val_MinusLogProbMetric: 30.5837

Epoch 374: val_loss did not improve from 29.64441
196/196 - 29s - loss: 29.6486 - MinusLogProbMetric: 29.6486 - val_loss: 30.5837 - val_MinusLogProbMetric: 30.5837 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 375/1000
2023-09-30 01:04:02.700 
Epoch 375/1000 
	 loss: 29.6981, MinusLogProbMetric: 29.6981, val_loss: 29.6087, val_MinusLogProbMetric: 29.6087

Epoch 375: val_loss improved from 29.64441 to 29.60870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 29s - loss: 29.6981 - MinusLogProbMetric: 29.6981 - val_loss: 29.6087 - val_MinusLogProbMetric: 29.6087 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 376/1000
2023-09-30 01:04:31.807 
Epoch 376/1000 
	 loss: 29.5419, MinusLogProbMetric: 29.5419, val_loss: 29.9827, val_MinusLogProbMetric: 29.9827

Epoch 376: val_loss did not improve from 29.60870
196/196 - 29s - loss: 29.5419 - MinusLogProbMetric: 29.5419 - val_loss: 29.9827 - val_MinusLogProbMetric: 29.9827 - lr: 0.0010 - 29s/epoch - 145ms/step
Epoch 377/1000
2023-09-30 01:04:59.169 
Epoch 377/1000 
	 loss: 29.7343, MinusLogProbMetric: 29.7343, val_loss: 29.7911, val_MinusLogProbMetric: 29.7911

Epoch 377: val_loss did not improve from 29.60870
196/196 - 27s - loss: 29.7343 - MinusLogProbMetric: 29.7343 - val_loss: 29.7911 - val_MinusLogProbMetric: 29.7911 - lr: 0.0010 - 27s/epoch - 140ms/step
Epoch 378/1000
2023-09-30 01:05:27.016 
Epoch 378/1000 
	 loss: 29.7202, MinusLogProbMetric: 29.7202, val_loss: 29.8776, val_MinusLogProbMetric: 29.8776

Epoch 378: val_loss did not improve from 29.60870
196/196 - 28s - loss: 29.7202 - MinusLogProbMetric: 29.7202 - val_loss: 29.8776 - val_MinusLogProbMetric: 29.8776 - lr: 0.0010 - 28s/epoch - 142ms/step
Epoch 379/1000
2023-09-30 01:05:58.088 
Epoch 379/1000 
	 loss: 29.6231, MinusLogProbMetric: 29.6231, val_loss: 30.0966, val_MinusLogProbMetric: 30.0966

Epoch 379: val_loss did not improve from 29.60870
196/196 - 31s - loss: 29.6231 - MinusLogProbMetric: 29.6231 - val_loss: 30.0966 - val_MinusLogProbMetric: 30.0966 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 380/1000
2023-09-30 01:06:27.794 
Epoch 380/1000 
	 loss: 29.8490, MinusLogProbMetric: 29.8490, val_loss: 29.8024, val_MinusLogProbMetric: 29.8024

Epoch 380: val_loss did not improve from 29.60870
196/196 - 30s - loss: 29.8490 - MinusLogProbMetric: 29.8490 - val_loss: 29.8024 - val_MinusLogProbMetric: 29.8024 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 381/1000
2023-09-30 01:06:55.493 
Epoch 381/1000 
	 loss: 29.7157, MinusLogProbMetric: 29.7157, val_loss: 30.5700, val_MinusLogProbMetric: 30.5700

Epoch 381: val_loss did not improve from 29.60870
196/196 - 28s - loss: 29.7157 - MinusLogProbMetric: 29.7157 - val_loss: 30.5700 - val_MinusLogProbMetric: 30.5700 - lr: 0.0010 - 28s/epoch - 141ms/step
Epoch 382/1000
2023-09-30 01:07:26.027 
Epoch 382/1000 
	 loss: 29.6718, MinusLogProbMetric: 29.6718, val_loss: 30.1419, val_MinusLogProbMetric: 30.1419

Epoch 382: val_loss did not improve from 29.60870
196/196 - 31s - loss: 29.6718 - MinusLogProbMetric: 29.6718 - val_loss: 30.1419 - val_MinusLogProbMetric: 30.1419 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 383/1000
2023-09-30 01:08:00.663 
Epoch 383/1000 
	 loss: 29.6174, MinusLogProbMetric: 29.6174, val_loss: 30.5955, val_MinusLogProbMetric: 30.5955

Epoch 383: val_loss did not improve from 29.60870
196/196 - 35s - loss: 29.6174 - MinusLogProbMetric: 29.6174 - val_loss: 30.5955 - val_MinusLogProbMetric: 30.5955 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 384/1000
2023-09-30 01:08:35.104 
Epoch 384/1000 
	 loss: 29.7090, MinusLogProbMetric: 29.7090, val_loss: 30.2622, val_MinusLogProbMetric: 30.2622

Epoch 384: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.7090 - MinusLogProbMetric: 29.7090 - val_loss: 30.2622 - val_MinusLogProbMetric: 30.2622 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 385/1000
2023-09-30 01:09:08.948 
Epoch 385/1000 
	 loss: 29.5603, MinusLogProbMetric: 29.5603, val_loss: 30.1331, val_MinusLogProbMetric: 30.1331

Epoch 385: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.5603 - MinusLogProbMetric: 29.5603 - val_loss: 30.1331 - val_MinusLogProbMetric: 30.1331 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 386/1000
2023-09-30 01:09:43.564 
Epoch 386/1000 
	 loss: 29.5133, MinusLogProbMetric: 29.5133, val_loss: 29.7747, val_MinusLogProbMetric: 29.7747

Epoch 386: val_loss did not improve from 29.60870
196/196 - 35s - loss: 29.5133 - MinusLogProbMetric: 29.5133 - val_loss: 29.7747 - val_MinusLogProbMetric: 29.7747 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 387/1000
2023-09-30 01:10:14.859 
Epoch 387/1000 
	 loss: 29.6076, MinusLogProbMetric: 29.6076, val_loss: 30.2540, val_MinusLogProbMetric: 30.2540

Epoch 387: val_loss did not improve from 29.60870
196/196 - 31s - loss: 29.6076 - MinusLogProbMetric: 29.6076 - val_loss: 30.2540 - val_MinusLogProbMetric: 30.2540 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 388/1000
2023-09-30 01:10:47.573 
Epoch 388/1000 
	 loss: 29.7936, MinusLogProbMetric: 29.7936, val_loss: 30.4819, val_MinusLogProbMetric: 30.4819

Epoch 388: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.7936 - MinusLogProbMetric: 29.7936 - val_loss: 30.4819 - val_MinusLogProbMetric: 30.4819 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 389/1000
2023-09-30 01:11:20.540 
Epoch 389/1000 
	 loss: 29.6831, MinusLogProbMetric: 29.6831, val_loss: 29.8388, val_MinusLogProbMetric: 29.8388

Epoch 389: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.6831 - MinusLogProbMetric: 29.6831 - val_loss: 29.8388 - val_MinusLogProbMetric: 29.8388 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 390/1000
2023-09-30 01:11:54.364 
Epoch 390/1000 
	 loss: 29.6267, MinusLogProbMetric: 29.6267, val_loss: 30.7581, val_MinusLogProbMetric: 30.7581

Epoch 390: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.6267 - MinusLogProbMetric: 29.6267 - val_loss: 30.7581 - val_MinusLogProbMetric: 30.7581 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 391/1000
2023-09-30 01:12:27.140 
Epoch 391/1000 
	 loss: 29.6582, MinusLogProbMetric: 29.6582, val_loss: 29.8972, val_MinusLogProbMetric: 29.8972

Epoch 391: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.6582 - MinusLogProbMetric: 29.6582 - val_loss: 29.8972 - val_MinusLogProbMetric: 29.8972 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 392/1000
2023-09-30 01:12:58.452 
Epoch 392/1000 
	 loss: 29.7743, MinusLogProbMetric: 29.7743, val_loss: 30.3240, val_MinusLogProbMetric: 30.3240

Epoch 392: val_loss did not improve from 29.60870
196/196 - 31s - loss: 29.7743 - MinusLogProbMetric: 29.7743 - val_loss: 30.3240 - val_MinusLogProbMetric: 30.3240 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 393/1000
2023-09-30 01:13:31.698 
Epoch 393/1000 
	 loss: 29.6201, MinusLogProbMetric: 29.6201, val_loss: 29.7691, val_MinusLogProbMetric: 29.7691

Epoch 393: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.6201 - MinusLogProbMetric: 29.6201 - val_loss: 29.7691 - val_MinusLogProbMetric: 29.7691 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 394/1000
2023-09-30 01:14:04.385 
Epoch 394/1000 
	 loss: 29.6406, MinusLogProbMetric: 29.6406, val_loss: 33.1503, val_MinusLogProbMetric: 33.1503

Epoch 394: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.6406 - MinusLogProbMetric: 29.6406 - val_loss: 33.1503 - val_MinusLogProbMetric: 33.1503 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 395/1000
2023-09-30 01:14:39.012 
Epoch 395/1000 
	 loss: 29.7134, MinusLogProbMetric: 29.7134, val_loss: 30.0086, val_MinusLogProbMetric: 30.0086

Epoch 395: val_loss did not improve from 29.60870
196/196 - 35s - loss: 29.7134 - MinusLogProbMetric: 29.7134 - val_loss: 30.0086 - val_MinusLogProbMetric: 30.0086 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 396/1000
2023-09-30 01:15:13.347 
Epoch 396/1000 
	 loss: 29.6272, MinusLogProbMetric: 29.6272, val_loss: 29.9388, val_MinusLogProbMetric: 29.9388

Epoch 396: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.6272 - MinusLogProbMetric: 29.6272 - val_loss: 29.9388 - val_MinusLogProbMetric: 29.9388 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 397/1000
2023-09-30 01:15:48.092 
Epoch 397/1000 
	 loss: 29.6436, MinusLogProbMetric: 29.6436, val_loss: 29.8789, val_MinusLogProbMetric: 29.8789

Epoch 397: val_loss did not improve from 29.60870
196/196 - 35s - loss: 29.6436 - MinusLogProbMetric: 29.6436 - val_loss: 29.8789 - val_MinusLogProbMetric: 29.8789 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 398/1000
2023-09-30 01:16:22.530 
Epoch 398/1000 
	 loss: 29.5611, MinusLogProbMetric: 29.5611, val_loss: 30.0948, val_MinusLogProbMetric: 30.0948

Epoch 398: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.5611 - MinusLogProbMetric: 29.5611 - val_loss: 30.0948 - val_MinusLogProbMetric: 30.0948 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 399/1000
2023-09-30 01:16:55.023 
Epoch 399/1000 
	 loss: 29.6622, MinusLogProbMetric: 29.6622, val_loss: 30.1008, val_MinusLogProbMetric: 30.1008

Epoch 399: val_loss did not improve from 29.60870
196/196 - 32s - loss: 29.6622 - MinusLogProbMetric: 29.6622 - val_loss: 30.1008 - val_MinusLogProbMetric: 30.1008 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 400/1000
2023-09-30 01:17:28.706 
Epoch 400/1000 
	 loss: 29.5203, MinusLogProbMetric: 29.5203, val_loss: 29.6859, val_MinusLogProbMetric: 29.6859

Epoch 400: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.5203 - MinusLogProbMetric: 29.5203 - val_loss: 29.6859 - val_MinusLogProbMetric: 29.6859 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 401/1000
2023-09-30 01:18:03.304 
Epoch 401/1000 
	 loss: 30.0097, MinusLogProbMetric: 30.0097, val_loss: 29.8438, val_MinusLogProbMetric: 29.8438

Epoch 401: val_loss did not improve from 29.60870
196/196 - 35s - loss: 30.0097 - MinusLogProbMetric: 30.0097 - val_loss: 29.8438 - val_MinusLogProbMetric: 29.8438 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 402/1000
2023-09-30 01:18:36.833 
Epoch 402/1000 
	 loss: 29.6513, MinusLogProbMetric: 29.6513, val_loss: 31.1294, val_MinusLogProbMetric: 31.1294

Epoch 402: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.6513 - MinusLogProbMetric: 29.6513 - val_loss: 31.1294 - val_MinusLogProbMetric: 31.1294 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 403/1000
2023-09-30 01:19:09.757 
Epoch 403/1000 
	 loss: 29.6680, MinusLogProbMetric: 29.6680, val_loss: 30.7213, val_MinusLogProbMetric: 30.7213

Epoch 403: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.6680 - MinusLogProbMetric: 29.6680 - val_loss: 30.7213 - val_MinusLogProbMetric: 30.7213 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 404/1000
2023-09-30 01:19:43.714 
Epoch 404/1000 
	 loss: 29.5909, MinusLogProbMetric: 29.5909, val_loss: 30.3317, val_MinusLogProbMetric: 30.3317

Epoch 404: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.5909 - MinusLogProbMetric: 29.5909 - val_loss: 30.3317 - val_MinusLogProbMetric: 30.3317 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 405/1000
2023-09-30 01:20:18.305 
Epoch 405/1000 
	 loss: 29.6183, MinusLogProbMetric: 29.6183, val_loss: 30.0403, val_MinusLogProbMetric: 30.0403

Epoch 405: val_loss did not improve from 29.60870
196/196 - 35s - loss: 29.6183 - MinusLogProbMetric: 29.6183 - val_loss: 30.0403 - val_MinusLogProbMetric: 30.0403 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 406/1000
2023-09-30 01:20:52.953 
Epoch 406/1000 
	 loss: 29.7463, MinusLogProbMetric: 29.7463, val_loss: 30.2526, val_MinusLogProbMetric: 30.2526

Epoch 406: val_loss did not improve from 29.60870
196/196 - 35s - loss: 29.7463 - MinusLogProbMetric: 29.7463 - val_loss: 30.2526 - val_MinusLogProbMetric: 30.2526 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 407/1000
2023-09-30 01:21:26.634 
Epoch 407/1000 
	 loss: 29.5319, MinusLogProbMetric: 29.5319, val_loss: 30.2052, val_MinusLogProbMetric: 30.2052

Epoch 407: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.5319 - MinusLogProbMetric: 29.5319 - val_loss: 30.2052 - val_MinusLogProbMetric: 30.2052 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 408/1000
2023-09-30 01:22:01.016 
Epoch 408/1000 
	 loss: 29.4486, MinusLogProbMetric: 29.4486, val_loss: 29.6163, val_MinusLogProbMetric: 29.6163

Epoch 408: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.4486 - MinusLogProbMetric: 29.4486 - val_loss: 29.6163 - val_MinusLogProbMetric: 29.6163 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 409/1000
2023-09-30 01:22:34.916 
Epoch 409/1000 
	 loss: 29.4507, MinusLogProbMetric: 29.4507, val_loss: 29.9825, val_MinusLogProbMetric: 29.9825

Epoch 409: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.4507 - MinusLogProbMetric: 29.4507 - val_loss: 29.9825 - val_MinusLogProbMetric: 29.9825 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 410/1000
2023-09-30 01:23:07.421 
Epoch 410/1000 
	 loss: 29.8767, MinusLogProbMetric: 29.8767, val_loss: 29.6927, val_MinusLogProbMetric: 29.6927

Epoch 410: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.8767 - MinusLogProbMetric: 29.8767 - val_loss: 29.6927 - val_MinusLogProbMetric: 29.6927 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 411/1000
2023-09-30 01:23:40.499 
Epoch 411/1000 
	 loss: 29.5205, MinusLogProbMetric: 29.5205, val_loss: 29.9495, val_MinusLogProbMetric: 29.9495

Epoch 411: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.5205 - MinusLogProbMetric: 29.5205 - val_loss: 29.9495 - val_MinusLogProbMetric: 29.9495 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 412/1000
2023-09-30 01:24:13.054 
Epoch 412/1000 
	 loss: 29.7523, MinusLogProbMetric: 29.7523, val_loss: 29.8414, val_MinusLogProbMetric: 29.8414

Epoch 412: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.7523 - MinusLogProbMetric: 29.7523 - val_loss: 29.8414 - val_MinusLogProbMetric: 29.8414 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 413/1000
2023-09-30 01:24:45.442 
Epoch 413/1000 
	 loss: 29.7288, MinusLogProbMetric: 29.7288, val_loss: 30.4330, val_MinusLogProbMetric: 30.4330

Epoch 413: val_loss did not improve from 29.60870
196/196 - 32s - loss: 29.7288 - MinusLogProbMetric: 29.7288 - val_loss: 30.4330 - val_MinusLogProbMetric: 30.4330 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 414/1000
2023-09-30 01:25:19.038 
Epoch 414/1000 
	 loss: 29.4933, MinusLogProbMetric: 29.4933, val_loss: 29.8351, val_MinusLogProbMetric: 29.8351

Epoch 414: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.4933 - MinusLogProbMetric: 29.4933 - val_loss: 29.8351 - val_MinusLogProbMetric: 29.8351 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 415/1000
2023-09-30 01:25:52.350 
Epoch 415/1000 
	 loss: 29.6113, MinusLogProbMetric: 29.6113, val_loss: 29.7717, val_MinusLogProbMetric: 29.7717

Epoch 415: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.6113 - MinusLogProbMetric: 29.6113 - val_loss: 29.7717 - val_MinusLogProbMetric: 29.7717 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 416/1000
2023-09-30 01:26:25.908 
Epoch 416/1000 
	 loss: 29.5821, MinusLogProbMetric: 29.5821, val_loss: 29.6911, val_MinusLogProbMetric: 29.6911

Epoch 416: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.5821 - MinusLogProbMetric: 29.5821 - val_loss: 29.6911 - val_MinusLogProbMetric: 29.6911 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 417/1000
2023-09-30 01:26:59.258 
Epoch 417/1000 
	 loss: 29.5335, MinusLogProbMetric: 29.5335, val_loss: 30.6902, val_MinusLogProbMetric: 30.6902

Epoch 417: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.5335 - MinusLogProbMetric: 29.5335 - val_loss: 30.6902 - val_MinusLogProbMetric: 30.6902 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 418/1000
2023-09-30 01:27:33.517 
Epoch 418/1000 
	 loss: 29.5250, MinusLogProbMetric: 29.5250, val_loss: 30.3965, val_MinusLogProbMetric: 30.3965

Epoch 418: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.5250 - MinusLogProbMetric: 29.5250 - val_loss: 30.3965 - val_MinusLogProbMetric: 30.3965 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 419/1000
2023-09-30 01:28:08.761 
Epoch 419/1000 
	 loss: 29.5930, MinusLogProbMetric: 29.5930, val_loss: 30.2652, val_MinusLogProbMetric: 30.2652

Epoch 419: val_loss did not improve from 29.60870
196/196 - 35s - loss: 29.5930 - MinusLogProbMetric: 29.5930 - val_loss: 30.2652 - val_MinusLogProbMetric: 30.2652 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 420/1000
2023-09-30 01:28:41.321 
Epoch 420/1000 
	 loss: 29.5115, MinusLogProbMetric: 29.5115, val_loss: 30.8491, val_MinusLogProbMetric: 30.8491

Epoch 420: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.5115 - MinusLogProbMetric: 29.5115 - val_loss: 30.8491 - val_MinusLogProbMetric: 30.8491 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 421/1000
2023-09-30 01:29:15.083 
Epoch 421/1000 
	 loss: 29.4646, MinusLogProbMetric: 29.4646, val_loss: 29.7130, val_MinusLogProbMetric: 29.7130

Epoch 421: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.4646 - MinusLogProbMetric: 29.4646 - val_loss: 29.7130 - val_MinusLogProbMetric: 29.7130 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 422/1000
2023-09-30 01:29:46.592 
Epoch 422/1000 
	 loss: 29.6967, MinusLogProbMetric: 29.6967, val_loss: 30.4529, val_MinusLogProbMetric: 30.4529

Epoch 422: val_loss did not improve from 29.60870
196/196 - 32s - loss: 29.6967 - MinusLogProbMetric: 29.6967 - val_loss: 30.4529 - val_MinusLogProbMetric: 30.4529 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 423/1000
2023-09-30 01:30:19.658 
Epoch 423/1000 
	 loss: 29.5182, MinusLogProbMetric: 29.5182, val_loss: 30.5371, val_MinusLogProbMetric: 30.5371

Epoch 423: val_loss did not improve from 29.60870
196/196 - 33s - loss: 29.5182 - MinusLogProbMetric: 29.5182 - val_loss: 30.5371 - val_MinusLogProbMetric: 30.5371 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 424/1000
2023-09-30 01:30:53.899 
Epoch 424/1000 
	 loss: 29.6350, MinusLogProbMetric: 29.6350, val_loss: 29.8401, val_MinusLogProbMetric: 29.8401

Epoch 424: val_loss did not improve from 29.60870
196/196 - 34s - loss: 29.6350 - MinusLogProbMetric: 29.6350 - val_loss: 29.8401 - val_MinusLogProbMetric: 29.8401 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 425/1000
2023-09-30 01:31:25.257 
Epoch 425/1000 
	 loss: 29.4100, MinusLogProbMetric: 29.4100, val_loss: 29.6816, val_MinusLogProbMetric: 29.6816

Epoch 425: val_loss did not improve from 29.60870
196/196 - 31s - loss: 29.4100 - MinusLogProbMetric: 29.4100 - val_loss: 29.6816 - val_MinusLogProbMetric: 29.6816 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 426/1000
2023-09-30 01:31:56.963 
Epoch 426/1000 
	 loss: 28.8095, MinusLogProbMetric: 28.8095, val_loss: 29.2782, val_MinusLogProbMetric: 29.2782

Epoch 426: val_loss improved from 29.60870 to 29.27818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 32s - loss: 28.8095 - MinusLogProbMetric: 28.8095 - val_loss: 29.2782 - val_MinusLogProbMetric: 29.2782 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 427/1000
2023-09-30 01:32:32.645 
Epoch 427/1000 
	 loss: 28.9444, MinusLogProbMetric: 28.9444, val_loss: 29.3790, val_MinusLogProbMetric: 29.3790

Epoch 427: val_loss did not improve from 29.27818
196/196 - 35s - loss: 28.9444 - MinusLogProbMetric: 28.9444 - val_loss: 29.3790 - val_MinusLogProbMetric: 29.3790 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 428/1000
2023-09-30 01:33:07.516 
Epoch 428/1000 
	 loss: 28.8270, MinusLogProbMetric: 28.8270, val_loss: 29.2298, val_MinusLogProbMetric: 29.2298

Epoch 428: val_loss improved from 29.27818 to 29.22980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 28.8270 - MinusLogProbMetric: 28.8270 - val_loss: 29.2298 - val_MinusLogProbMetric: 29.2298 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 429/1000
2023-09-30 01:33:43.140 
Epoch 429/1000 
	 loss: 28.8535, MinusLogProbMetric: 28.8535, val_loss: 29.3210, val_MinusLogProbMetric: 29.3210

Epoch 429: val_loss did not improve from 29.22980
196/196 - 35s - loss: 28.8535 - MinusLogProbMetric: 28.8535 - val_loss: 29.3210 - val_MinusLogProbMetric: 29.3210 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 430/1000
2023-09-30 01:34:17.872 
Epoch 430/1000 
	 loss: 28.8473, MinusLogProbMetric: 28.8473, val_loss: 29.3370, val_MinusLogProbMetric: 29.3370

Epoch 430: val_loss did not improve from 29.22980
196/196 - 35s - loss: 28.8473 - MinusLogProbMetric: 28.8473 - val_loss: 29.3370 - val_MinusLogProbMetric: 29.3370 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 431/1000
2023-09-30 01:34:52.607 
Epoch 431/1000 
	 loss: 28.8560, MinusLogProbMetric: 28.8560, val_loss: 29.7461, val_MinusLogProbMetric: 29.7461

Epoch 431: val_loss did not improve from 29.22980
196/196 - 35s - loss: 28.8560 - MinusLogProbMetric: 28.8560 - val_loss: 29.7461 - val_MinusLogProbMetric: 29.7461 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 432/1000
2023-09-30 01:35:27.480 
Epoch 432/1000 
	 loss: 28.8750, MinusLogProbMetric: 28.8750, val_loss: 29.2079, val_MinusLogProbMetric: 29.2079

Epoch 432: val_loss improved from 29.22980 to 29.20795, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.8750 - MinusLogProbMetric: 28.8750 - val_loss: 29.2079 - val_MinusLogProbMetric: 29.2079 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 433/1000
2023-09-30 01:36:03.217 
Epoch 433/1000 
	 loss: 29.0078, MinusLogProbMetric: 29.0078, val_loss: 29.1326, val_MinusLogProbMetric: 29.1326

Epoch 433: val_loss improved from 29.20795 to 29.13261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 29.0078 - MinusLogProbMetric: 29.0078 - val_loss: 29.1326 - val_MinusLogProbMetric: 29.1326 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 434/1000
2023-09-30 01:36:38.577 
Epoch 434/1000 
	 loss: 28.8143, MinusLogProbMetric: 28.8143, val_loss: 29.3670, val_MinusLogProbMetric: 29.3670

Epoch 434: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.8143 - MinusLogProbMetric: 28.8143 - val_loss: 29.3670 - val_MinusLogProbMetric: 29.3670 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 435/1000
2023-09-30 01:37:13.444 
Epoch 435/1000 
	 loss: 28.7899, MinusLogProbMetric: 28.7899, val_loss: 29.3296, val_MinusLogProbMetric: 29.3296

Epoch 435: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.7899 - MinusLogProbMetric: 28.7899 - val_loss: 29.3296 - val_MinusLogProbMetric: 29.3296 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 436/1000
2023-09-30 01:37:48.606 
Epoch 436/1000 
	 loss: 28.9647, MinusLogProbMetric: 28.9647, val_loss: 29.2014, val_MinusLogProbMetric: 29.2014

Epoch 436: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.9647 - MinusLogProbMetric: 28.9647 - val_loss: 29.2014 - val_MinusLogProbMetric: 29.2014 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 437/1000
2023-09-30 01:38:23.213 
Epoch 437/1000 
	 loss: 28.8172, MinusLogProbMetric: 28.8172, val_loss: 29.1659, val_MinusLogProbMetric: 29.1659

Epoch 437: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.8172 - MinusLogProbMetric: 28.8172 - val_loss: 29.1659 - val_MinusLogProbMetric: 29.1659 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 438/1000
2023-09-30 01:38:58.356 
Epoch 438/1000 
	 loss: 28.8280, MinusLogProbMetric: 28.8280, val_loss: 29.2443, val_MinusLogProbMetric: 29.2443

Epoch 438: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.8280 - MinusLogProbMetric: 28.8280 - val_loss: 29.2443 - val_MinusLogProbMetric: 29.2443 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 439/1000
2023-09-30 01:39:33.480 
Epoch 439/1000 
	 loss: 28.9013, MinusLogProbMetric: 28.9013, val_loss: 29.3413, val_MinusLogProbMetric: 29.3413

Epoch 439: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.9013 - MinusLogProbMetric: 28.9013 - val_loss: 29.3413 - val_MinusLogProbMetric: 29.3413 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 440/1000
2023-09-30 01:40:08.407 
Epoch 440/1000 
	 loss: 28.8128, MinusLogProbMetric: 28.8128, val_loss: 29.2090, val_MinusLogProbMetric: 29.2090

Epoch 440: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.8128 - MinusLogProbMetric: 28.8128 - val_loss: 29.2090 - val_MinusLogProbMetric: 29.2090 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 441/1000
2023-09-30 01:40:43.514 
Epoch 441/1000 
	 loss: 28.9308, MinusLogProbMetric: 28.9308, val_loss: 29.5101, val_MinusLogProbMetric: 29.5101

Epoch 441: val_loss did not improve from 29.13261
196/196 - 35s - loss: 28.9308 - MinusLogProbMetric: 28.9308 - val_loss: 29.5101 - val_MinusLogProbMetric: 29.5101 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 442/1000
2023-09-30 01:41:18.530 
Epoch 442/1000 
	 loss: 29.0855, MinusLogProbMetric: 29.0855, val_loss: 29.3572, val_MinusLogProbMetric: 29.3572

Epoch 442: val_loss did not improve from 29.13261
196/196 - 35s - loss: 29.0855 - MinusLogProbMetric: 29.0855 - val_loss: 29.3572 - val_MinusLogProbMetric: 29.3572 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 443/1000
2023-09-30 01:41:53.571 
Epoch 443/1000 
	 loss: 28.8120, MinusLogProbMetric: 28.8120, val_loss: 29.0665, val_MinusLogProbMetric: 29.0665

Epoch 443: val_loss improved from 29.13261 to 29.06647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 37s - loss: 28.8120 - MinusLogProbMetric: 28.8120 - val_loss: 29.0665 - val_MinusLogProbMetric: 29.0665 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 444/1000
2023-09-30 01:42:29.936 
Epoch 444/1000 
	 loss: 29.0200, MinusLogProbMetric: 29.0200, val_loss: 29.5241, val_MinusLogProbMetric: 29.5241

Epoch 444: val_loss did not improve from 29.06647
196/196 - 35s - loss: 29.0200 - MinusLogProbMetric: 29.0200 - val_loss: 29.5241 - val_MinusLogProbMetric: 29.5241 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 445/1000
2023-09-30 01:43:04.982 
Epoch 445/1000 
	 loss: 28.8129, MinusLogProbMetric: 28.8129, val_loss: 29.0911, val_MinusLogProbMetric: 29.0911

Epoch 445: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8129 - MinusLogProbMetric: 28.8129 - val_loss: 29.0911 - val_MinusLogProbMetric: 29.0911 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 446/1000
2023-09-30 01:43:40.205 
Epoch 446/1000 
	 loss: 28.7817, MinusLogProbMetric: 28.7817, val_loss: 29.2838, val_MinusLogProbMetric: 29.2838

Epoch 446: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.7817 - MinusLogProbMetric: 28.7817 - val_loss: 29.2838 - val_MinusLogProbMetric: 29.2838 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 447/1000
2023-09-30 01:44:14.783 
Epoch 447/1000 
	 loss: 28.7987, MinusLogProbMetric: 28.7987, val_loss: 29.2926, val_MinusLogProbMetric: 29.2926

Epoch 447: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.7987 - MinusLogProbMetric: 28.7987 - val_loss: 29.2926 - val_MinusLogProbMetric: 29.2926 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 448/1000
2023-09-30 01:44:49.777 
Epoch 448/1000 
	 loss: 28.8075, MinusLogProbMetric: 28.8075, val_loss: 29.3041, val_MinusLogProbMetric: 29.3041

Epoch 448: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8075 - MinusLogProbMetric: 28.8075 - val_loss: 29.3041 - val_MinusLogProbMetric: 29.3041 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 449/1000
2023-09-30 01:45:24.792 
Epoch 449/1000 
	 loss: 28.9284, MinusLogProbMetric: 28.9284, val_loss: 29.3182, val_MinusLogProbMetric: 29.3182

Epoch 449: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.9284 - MinusLogProbMetric: 28.9284 - val_loss: 29.3182 - val_MinusLogProbMetric: 29.3182 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 450/1000
2023-09-30 01:45:59.708 
Epoch 450/1000 
	 loss: 28.9184, MinusLogProbMetric: 28.9184, val_loss: 30.1017, val_MinusLogProbMetric: 30.1017

Epoch 450: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.9184 - MinusLogProbMetric: 28.9184 - val_loss: 30.1017 - val_MinusLogProbMetric: 30.1017 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 451/1000
2023-09-30 01:46:34.582 
Epoch 451/1000 
	 loss: 28.8276, MinusLogProbMetric: 28.8276, val_loss: 29.2550, val_MinusLogProbMetric: 29.2550

Epoch 451: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8276 - MinusLogProbMetric: 28.8276 - val_loss: 29.2550 - val_MinusLogProbMetric: 29.2550 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 452/1000
2023-09-30 01:47:09.622 
Epoch 452/1000 
	 loss: 28.8432, MinusLogProbMetric: 28.8432, val_loss: 29.1740, val_MinusLogProbMetric: 29.1740

Epoch 452: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8432 - MinusLogProbMetric: 28.8432 - val_loss: 29.1740 - val_MinusLogProbMetric: 29.1740 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 453/1000
2023-09-30 01:47:44.328 
Epoch 453/1000 
	 loss: 28.8112, MinusLogProbMetric: 28.8112, val_loss: 29.1967, val_MinusLogProbMetric: 29.1967

Epoch 453: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8112 - MinusLogProbMetric: 28.8112 - val_loss: 29.1967 - val_MinusLogProbMetric: 29.1967 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 454/1000
2023-09-30 01:48:19.416 
Epoch 454/1000 
	 loss: 28.8613, MinusLogProbMetric: 28.8613, val_loss: 29.1761, val_MinusLogProbMetric: 29.1761

Epoch 454: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8613 - MinusLogProbMetric: 28.8613 - val_loss: 29.1761 - val_MinusLogProbMetric: 29.1761 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 455/1000
2023-09-30 01:48:54.641 
Epoch 455/1000 
	 loss: 28.8623, MinusLogProbMetric: 28.8623, val_loss: 29.1313, val_MinusLogProbMetric: 29.1313

Epoch 455: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8623 - MinusLogProbMetric: 28.8623 - val_loss: 29.1313 - val_MinusLogProbMetric: 29.1313 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 456/1000
2023-09-30 01:49:29.226 
Epoch 456/1000 
	 loss: 28.7672, MinusLogProbMetric: 28.7672, val_loss: 29.1588, val_MinusLogProbMetric: 29.1588

Epoch 456: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.7672 - MinusLogProbMetric: 28.7672 - val_loss: 29.1588 - val_MinusLogProbMetric: 29.1588 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 457/1000
2023-09-30 01:50:04.137 
Epoch 457/1000 
	 loss: 28.8281, MinusLogProbMetric: 28.8281, val_loss: 29.2416, val_MinusLogProbMetric: 29.2416

Epoch 457: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8281 - MinusLogProbMetric: 28.8281 - val_loss: 29.2416 - val_MinusLogProbMetric: 29.2416 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 458/1000
2023-09-30 01:50:38.951 
Epoch 458/1000 
	 loss: 28.8305, MinusLogProbMetric: 28.8305, val_loss: 29.2455, val_MinusLogProbMetric: 29.2455

Epoch 458: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8305 - MinusLogProbMetric: 28.8305 - val_loss: 29.2455 - val_MinusLogProbMetric: 29.2455 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 459/1000
2023-09-30 01:51:14.080 
Epoch 459/1000 
	 loss: 28.7961, MinusLogProbMetric: 28.7961, val_loss: 29.1098, val_MinusLogProbMetric: 29.1098

Epoch 459: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.7961 - MinusLogProbMetric: 28.7961 - val_loss: 29.1098 - val_MinusLogProbMetric: 29.1098 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 460/1000
2023-09-30 01:51:48.831 
Epoch 460/1000 
	 loss: 28.8985, MinusLogProbMetric: 28.8985, val_loss: 29.0712, val_MinusLogProbMetric: 29.0712

Epoch 460: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8985 - MinusLogProbMetric: 28.8985 - val_loss: 29.0712 - val_MinusLogProbMetric: 29.0712 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 461/1000
2023-09-30 01:52:23.767 
Epoch 461/1000 
	 loss: 28.8786, MinusLogProbMetric: 28.8786, val_loss: 29.1306, val_MinusLogProbMetric: 29.1306

Epoch 461: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8786 - MinusLogProbMetric: 28.8786 - val_loss: 29.1306 - val_MinusLogProbMetric: 29.1306 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 462/1000
2023-09-30 01:52:58.857 
Epoch 462/1000 
	 loss: 28.8272, MinusLogProbMetric: 28.8272, val_loss: 29.3345, val_MinusLogProbMetric: 29.3345

Epoch 462: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8272 - MinusLogProbMetric: 28.8272 - val_loss: 29.3345 - val_MinusLogProbMetric: 29.3345 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 463/1000
2023-09-30 01:53:33.826 
Epoch 463/1000 
	 loss: 28.8235, MinusLogProbMetric: 28.8235, val_loss: 29.6070, val_MinusLogProbMetric: 29.6070

Epoch 463: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8235 - MinusLogProbMetric: 28.8235 - val_loss: 29.6070 - val_MinusLogProbMetric: 29.6070 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 464/1000
2023-09-30 01:54:08.961 
Epoch 464/1000 
	 loss: 28.8886, MinusLogProbMetric: 28.8886, val_loss: 29.2252, val_MinusLogProbMetric: 29.2252

Epoch 464: val_loss did not improve from 29.06647
196/196 - 35s - loss: 28.8886 - MinusLogProbMetric: 28.8886 - val_loss: 29.2252 - val_MinusLogProbMetric: 29.2252 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 465/1000
2023-09-30 01:54:43.849 
Epoch 465/1000 
	 loss: 28.7953, MinusLogProbMetric: 28.7953, val_loss: 29.0647, val_MinusLogProbMetric: 29.0647

Epoch 465: val_loss improved from 29.06647 to 29.06470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 28.7953 - MinusLogProbMetric: 28.7953 - val_loss: 29.0647 - val_MinusLogProbMetric: 29.0647 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 466/1000
2023-09-30 01:55:19.297 
Epoch 466/1000 
	 loss: 28.8134, MinusLogProbMetric: 28.8134, val_loss: 29.4006, val_MinusLogProbMetric: 29.4006

Epoch 466: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.8134 - MinusLogProbMetric: 28.8134 - val_loss: 29.4006 - val_MinusLogProbMetric: 29.4006 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 467/1000
2023-09-30 01:55:54.147 
Epoch 467/1000 
	 loss: 28.9029, MinusLogProbMetric: 28.9029, val_loss: 29.2141, val_MinusLogProbMetric: 29.2141

Epoch 467: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.9029 - MinusLogProbMetric: 28.9029 - val_loss: 29.2141 - val_MinusLogProbMetric: 29.2141 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 468/1000
2023-09-30 01:56:29.291 
Epoch 468/1000 
	 loss: 28.7608, MinusLogProbMetric: 28.7608, val_loss: 29.1602, val_MinusLogProbMetric: 29.1602

Epoch 468: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.7608 - MinusLogProbMetric: 28.7608 - val_loss: 29.1602 - val_MinusLogProbMetric: 29.1602 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 469/1000
2023-09-30 01:57:04.069 
Epoch 469/1000 
	 loss: 28.8274, MinusLogProbMetric: 28.8274, val_loss: 29.1721, val_MinusLogProbMetric: 29.1721

Epoch 469: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.8274 - MinusLogProbMetric: 28.8274 - val_loss: 29.1721 - val_MinusLogProbMetric: 29.1721 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 470/1000
2023-09-30 01:57:39.150 
Epoch 470/1000 
	 loss: 28.8294, MinusLogProbMetric: 28.8294, val_loss: 29.6930, val_MinusLogProbMetric: 29.6930

Epoch 470: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.8294 - MinusLogProbMetric: 28.8294 - val_loss: 29.6930 - val_MinusLogProbMetric: 29.6930 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 471/1000
2023-09-30 01:58:14.260 
Epoch 471/1000 
	 loss: 28.7438, MinusLogProbMetric: 28.7438, val_loss: 30.2276, val_MinusLogProbMetric: 30.2276

Epoch 471: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.7438 - MinusLogProbMetric: 28.7438 - val_loss: 30.2276 - val_MinusLogProbMetric: 30.2276 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 472/1000
2023-09-30 01:58:48.890 
Epoch 472/1000 
	 loss: 28.8577, MinusLogProbMetric: 28.8577, val_loss: 29.1777, val_MinusLogProbMetric: 29.1777

Epoch 472: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.8577 - MinusLogProbMetric: 28.8577 - val_loss: 29.1777 - val_MinusLogProbMetric: 29.1777 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 473/1000
2023-09-30 01:59:23.658 
Epoch 473/1000 
	 loss: 28.7938, MinusLogProbMetric: 28.7938, val_loss: 29.3956, val_MinusLogProbMetric: 29.3956

Epoch 473: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.7938 - MinusLogProbMetric: 28.7938 - val_loss: 29.3956 - val_MinusLogProbMetric: 29.3956 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 474/1000
2023-09-30 01:59:58.644 
Epoch 474/1000 
	 loss: 28.7865, MinusLogProbMetric: 28.7865, val_loss: 29.2247, val_MinusLogProbMetric: 29.2247

Epoch 474: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.7865 - MinusLogProbMetric: 28.7865 - val_loss: 29.2247 - val_MinusLogProbMetric: 29.2247 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 475/1000
2023-09-30 02:00:33.530 
Epoch 475/1000 
	 loss: 28.7473, MinusLogProbMetric: 28.7473, val_loss: 32.0092, val_MinusLogProbMetric: 32.0092

Epoch 475: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.7473 - MinusLogProbMetric: 28.7473 - val_loss: 32.0092 - val_MinusLogProbMetric: 32.0092 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 476/1000
2023-09-30 02:01:08.158 
Epoch 476/1000 
	 loss: 28.8858, MinusLogProbMetric: 28.8858, val_loss: 29.2023, val_MinusLogProbMetric: 29.2023

Epoch 476: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.8858 - MinusLogProbMetric: 28.8858 - val_loss: 29.2023 - val_MinusLogProbMetric: 29.2023 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 477/1000
2023-09-30 02:01:43.162 
Epoch 477/1000 
	 loss: 28.7727, MinusLogProbMetric: 28.7727, val_loss: 29.3276, val_MinusLogProbMetric: 29.3276

Epoch 477: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.7727 - MinusLogProbMetric: 28.7727 - val_loss: 29.3276 - val_MinusLogProbMetric: 29.3276 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 478/1000
2023-09-30 02:02:18.287 
Epoch 478/1000 
	 loss: 28.8052, MinusLogProbMetric: 28.8052, val_loss: 29.1805, val_MinusLogProbMetric: 29.1805

Epoch 478: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.8052 - MinusLogProbMetric: 28.8052 - val_loss: 29.1805 - val_MinusLogProbMetric: 29.1805 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 479/1000
2023-09-30 02:02:53.214 
Epoch 479/1000 
	 loss: 28.7813, MinusLogProbMetric: 28.7813, val_loss: 29.1397, val_MinusLogProbMetric: 29.1397

Epoch 479: val_loss did not improve from 29.06470
196/196 - 35s - loss: 28.7813 - MinusLogProbMetric: 28.7813 - val_loss: 29.1397 - val_MinusLogProbMetric: 29.1397 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 480/1000
2023-09-30 02:03:26.573 
Epoch 480/1000 
	 loss: 28.8579, MinusLogProbMetric: 28.8579, val_loss: 29.3962, val_MinusLogProbMetric: 29.3962

Epoch 480: val_loss did not improve from 29.06470
196/196 - 33s - loss: 28.8579 - MinusLogProbMetric: 28.8579 - val_loss: 29.3962 - val_MinusLogProbMetric: 29.3962 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 481/1000
2023-09-30 02:03:57.055 
Epoch 481/1000 
	 loss: 28.7866, MinusLogProbMetric: 28.7866, val_loss: 29.1125, val_MinusLogProbMetric: 29.1125

Epoch 481: val_loss did not improve from 29.06470
196/196 - 30s - loss: 28.7866 - MinusLogProbMetric: 28.7866 - val_loss: 29.1125 - val_MinusLogProbMetric: 29.1125 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 482/1000
2023-09-30 02:04:31.105 
Epoch 482/1000 
	 loss: 28.8573, MinusLogProbMetric: 28.8573, val_loss: 29.3289, val_MinusLogProbMetric: 29.3289

Epoch 482: val_loss did not improve from 29.06470
196/196 - 34s - loss: 28.8573 - MinusLogProbMetric: 28.8573 - val_loss: 29.3289 - val_MinusLogProbMetric: 29.3289 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 483/1000
2023-09-30 02:05:04.720 
Epoch 483/1000 
	 loss: 28.8296, MinusLogProbMetric: 28.8296, val_loss: 29.1335, val_MinusLogProbMetric: 29.1335

Epoch 483: val_loss did not improve from 29.06470
196/196 - 34s - loss: 28.8296 - MinusLogProbMetric: 28.8296 - val_loss: 29.1335 - val_MinusLogProbMetric: 29.1335 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 484/1000
2023-09-30 02:05:37.236 
Epoch 484/1000 
	 loss: 28.8803, MinusLogProbMetric: 28.8803, val_loss: 28.9867, val_MinusLogProbMetric: 28.9867

Epoch 484: val_loss improved from 29.06470 to 28.98674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 28.8803 - MinusLogProbMetric: 28.8803 - val_loss: 28.9867 - val_MinusLogProbMetric: 28.9867 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 485/1000
2023-09-30 02:06:09.912 
Epoch 485/1000 
	 loss: 28.7158, MinusLogProbMetric: 28.7158, val_loss: 29.0395, val_MinusLogProbMetric: 29.0395

Epoch 485: val_loss did not improve from 28.98674
196/196 - 32s - loss: 28.7158 - MinusLogProbMetric: 28.7158 - val_loss: 29.0395 - val_MinusLogProbMetric: 29.0395 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 486/1000
2023-09-30 02:06:42.292 
Epoch 486/1000 
	 loss: 28.7386, MinusLogProbMetric: 28.7386, val_loss: 29.4893, val_MinusLogProbMetric: 29.4893

Epoch 486: val_loss did not improve from 28.98674
196/196 - 32s - loss: 28.7386 - MinusLogProbMetric: 28.7386 - val_loss: 29.4893 - val_MinusLogProbMetric: 29.4893 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 487/1000
2023-09-30 02:07:16.283 
Epoch 487/1000 
	 loss: 28.8061, MinusLogProbMetric: 28.8061, val_loss: 29.0288, val_MinusLogProbMetric: 29.0288

Epoch 487: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.8061 - MinusLogProbMetric: 28.8061 - val_loss: 29.0288 - val_MinusLogProbMetric: 29.0288 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 488/1000
2023-09-30 02:07:50.661 
Epoch 488/1000 
	 loss: 28.8624, MinusLogProbMetric: 28.8624, val_loss: 29.1281, val_MinusLogProbMetric: 29.1281

Epoch 488: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.8624 - MinusLogProbMetric: 28.8624 - val_loss: 29.1281 - val_MinusLogProbMetric: 29.1281 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 489/1000
2023-09-30 02:08:23.148 
Epoch 489/1000 
	 loss: 28.7921, MinusLogProbMetric: 28.7921, val_loss: 29.2148, val_MinusLogProbMetric: 29.2148

Epoch 489: val_loss did not improve from 28.98674
196/196 - 32s - loss: 28.7921 - MinusLogProbMetric: 28.7921 - val_loss: 29.2148 - val_MinusLogProbMetric: 29.2148 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 490/1000
2023-09-30 02:08:56.888 
Epoch 490/1000 
	 loss: 28.7567, MinusLogProbMetric: 28.7567, val_loss: 29.7084, val_MinusLogProbMetric: 29.7084

Epoch 490: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.7567 - MinusLogProbMetric: 28.7567 - val_loss: 29.7084 - val_MinusLogProbMetric: 29.7084 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 491/1000
2023-09-30 02:09:31.718 
Epoch 491/1000 
	 loss: 28.8297, MinusLogProbMetric: 28.8297, val_loss: 29.2314, val_MinusLogProbMetric: 29.2314

Epoch 491: val_loss did not improve from 28.98674
196/196 - 35s - loss: 28.8297 - MinusLogProbMetric: 28.8297 - val_loss: 29.2314 - val_MinusLogProbMetric: 29.2314 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 492/1000
2023-09-30 02:10:05.868 
Epoch 492/1000 
	 loss: 28.7618, MinusLogProbMetric: 28.7618, val_loss: 29.8209, val_MinusLogProbMetric: 29.8209

Epoch 492: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.7618 - MinusLogProbMetric: 28.7618 - val_loss: 29.8209 - val_MinusLogProbMetric: 29.8209 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 493/1000
2023-09-30 02:10:39.403 
Epoch 493/1000 
	 loss: 28.7886, MinusLogProbMetric: 28.7886, val_loss: 29.2256, val_MinusLogProbMetric: 29.2256

Epoch 493: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.7886 - MinusLogProbMetric: 28.7886 - val_loss: 29.2256 - val_MinusLogProbMetric: 29.2256 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 494/1000
2023-09-30 02:11:13.860 
Epoch 494/1000 
	 loss: 28.7495, MinusLogProbMetric: 28.7495, val_loss: 29.0720, val_MinusLogProbMetric: 29.0720

Epoch 494: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.7495 - MinusLogProbMetric: 28.7495 - val_loss: 29.0720 - val_MinusLogProbMetric: 29.0720 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 495/1000
2023-09-30 02:11:45.443 
Epoch 495/1000 
	 loss: 28.8043, MinusLogProbMetric: 28.8043, val_loss: 29.2652, val_MinusLogProbMetric: 29.2652

Epoch 495: val_loss did not improve from 28.98674
196/196 - 32s - loss: 28.8043 - MinusLogProbMetric: 28.8043 - val_loss: 29.2652 - val_MinusLogProbMetric: 29.2652 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 496/1000
2023-09-30 02:12:18.859 
Epoch 496/1000 
	 loss: 28.7524, MinusLogProbMetric: 28.7524, val_loss: 29.2517, val_MinusLogProbMetric: 29.2517

Epoch 496: val_loss did not improve from 28.98674
196/196 - 33s - loss: 28.7524 - MinusLogProbMetric: 28.7524 - val_loss: 29.2517 - val_MinusLogProbMetric: 29.2517 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 497/1000
2023-09-30 02:12:51.559 
Epoch 497/1000 
	 loss: 28.8580, MinusLogProbMetric: 28.8580, val_loss: 29.1494, val_MinusLogProbMetric: 29.1494

Epoch 497: val_loss did not improve from 28.98674
196/196 - 33s - loss: 28.8580 - MinusLogProbMetric: 28.8580 - val_loss: 29.1494 - val_MinusLogProbMetric: 29.1494 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 498/1000
2023-09-30 02:13:23.734 
Epoch 498/1000 
	 loss: 28.7074, MinusLogProbMetric: 28.7074, val_loss: 29.0668, val_MinusLogProbMetric: 29.0668

Epoch 498: val_loss did not improve from 28.98674
196/196 - 32s - loss: 28.7074 - MinusLogProbMetric: 28.7074 - val_loss: 29.0668 - val_MinusLogProbMetric: 29.0668 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 499/1000
2023-09-30 02:13:57.360 
Epoch 499/1000 
	 loss: 28.7031, MinusLogProbMetric: 28.7031, val_loss: 29.3984, val_MinusLogProbMetric: 29.3984

Epoch 499: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.7031 - MinusLogProbMetric: 28.7031 - val_loss: 29.3984 - val_MinusLogProbMetric: 29.3984 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 500/1000
2023-09-30 02:14:31.733 
Epoch 500/1000 
	 loss: 28.8120, MinusLogProbMetric: 28.8120, val_loss: 29.2655, val_MinusLogProbMetric: 29.2655

Epoch 500: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.8120 - MinusLogProbMetric: 28.8120 - val_loss: 29.2655 - val_MinusLogProbMetric: 29.2655 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 501/1000
2023-09-30 02:15:02.306 
Epoch 501/1000 
	 loss: 28.6999, MinusLogProbMetric: 28.6999, val_loss: 29.3878, val_MinusLogProbMetric: 29.3878

Epoch 501: val_loss did not improve from 28.98674
196/196 - 31s - loss: 28.6999 - MinusLogProbMetric: 28.6999 - val_loss: 29.3878 - val_MinusLogProbMetric: 29.3878 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 502/1000
2023-09-30 02:15:36.643 
Epoch 502/1000 
	 loss: 28.7512, MinusLogProbMetric: 28.7512, val_loss: 29.1399, val_MinusLogProbMetric: 29.1399

Epoch 502: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.7512 - MinusLogProbMetric: 28.7512 - val_loss: 29.1399 - val_MinusLogProbMetric: 29.1399 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 503/1000
2023-09-30 02:16:09.540 
Epoch 503/1000 
	 loss: 28.7615, MinusLogProbMetric: 28.7615, val_loss: 29.4701, val_MinusLogProbMetric: 29.4701

Epoch 503: val_loss did not improve from 28.98674
196/196 - 33s - loss: 28.7615 - MinusLogProbMetric: 28.7615 - val_loss: 29.4701 - val_MinusLogProbMetric: 29.4701 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 504/1000
2023-09-30 02:16:43.670 
Epoch 504/1000 
	 loss: 28.8154, MinusLogProbMetric: 28.8154, val_loss: 29.1914, val_MinusLogProbMetric: 29.1914

Epoch 504: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.8154 - MinusLogProbMetric: 28.8154 - val_loss: 29.1914 - val_MinusLogProbMetric: 29.1914 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 505/1000
2023-09-30 02:17:17.561 
Epoch 505/1000 
	 loss: 28.7570, MinusLogProbMetric: 28.7570, val_loss: 28.9972, val_MinusLogProbMetric: 28.9972

Epoch 505: val_loss did not improve from 28.98674
196/196 - 34s - loss: 28.7570 - MinusLogProbMetric: 28.7570 - val_loss: 28.9972 - val_MinusLogProbMetric: 28.9972 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 506/1000
2023-09-30 02:17:50.488 
Epoch 506/1000 
	 loss: 28.8243, MinusLogProbMetric: 28.8243, val_loss: 28.9288, val_MinusLogProbMetric: 28.9288

Epoch 506: val_loss improved from 28.98674 to 28.92877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 28.8243 - MinusLogProbMetric: 28.8243 - val_loss: 28.9288 - val_MinusLogProbMetric: 28.9288 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 507/1000
2023-09-30 02:18:23.787 
Epoch 507/1000 
	 loss: 28.7867, MinusLogProbMetric: 28.7867, val_loss: 29.1627, val_MinusLogProbMetric: 29.1627

Epoch 507: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.7867 - MinusLogProbMetric: 28.7867 - val_loss: 29.1627 - val_MinusLogProbMetric: 29.1627 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 508/1000
2023-09-30 02:18:57.695 
Epoch 508/1000 
	 loss: 28.7022, MinusLogProbMetric: 28.7022, val_loss: 29.1825, val_MinusLogProbMetric: 29.1825

Epoch 508: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7022 - MinusLogProbMetric: 28.7022 - val_loss: 29.1825 - val_MinusLogProbMetric: 29.1825 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 509/1000
2023-09-30 02:19:30.943 
Epoch 509/1000 
	 loss: 28.8531, MinusLogProbMetric: 28.8531, val_loss: 29.1427, val_MinusLogProbMetric: 29.1427

Epoch 509: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.8531 - MinusLogProbMetric: 28.8531 - val_loss: 29.1427 - val_MinusLogProbMetric: 29.1427 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 510/1000
2023-09-30 02:20:02.369 
Epoch 510/1000 
	 loss: 28.7617, MinusLogProbMetric: 28.7617, val_loss: 29.5448, val_MinusLogProbMetric: 29.5448

Epoch 510: val_loss did not improve from 28.92877
196/196 - 31s - loss: 28.7617 - MinusLogProbMetric: 28.7617 - val_loss: 29.5448 - val_MinusLogProbMetric: 29.5448 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 511/1000
2023-09-30 02:20:35.276 
Epoch 511/1000 
	 loss: 28.7639, MinusLogProbMetric: 28.7639, val_loss: 29.0901, val_MinusLogProbMetric: 29.0901

Epoch 511: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.7639 - MinusLogProbMetric: 28.7639 - val_loss: 29.0901 - val_MinusLogProbMetric: 29.0901 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 512/1000
2023-09-30 02:21:09.802 
Epoch 512/1000 
	 loss: 28.7069, MinusLogProbMetric: 28.7069, val_loss: 29.0677, val_MinusLogProbMetric: 29.0677

Epoch 512: val_loss did not improve from 28.92877
196/196 - 35s - loss: 28.7069 - MinusLogProbMetric: 28.7069 - val_loss: 29.0677 - val_MinusLogProbMetric: 29.0677 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 513/1000
2023-09-30 02:21:43.671 
Epoch 513/1000 
	 loss: 28.7890, MinusLogProbMetric: 28.7890, val_loss: 29.0647, val_MinusLogProbMetric: 29.0647

Epoch 513: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7890 - MinusLogProbMetric: 28.7890 - val_loss: 29.0647 - val_MinusLogProbMetric: 29.0647 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 514/1000
2023-09-30 02:22:15.391 
Epoch 514/1000 
	 loss: 28.7793, MinusLogProbMetric: 28.7793, val_loss: 29.1710, val_MinusLogProbMetric: 29.1710

Epoch 514: val_loss did not improve from 28.92877
196/196 - 32s - loss: 28.7793 - MinusLogProbMetric: 28.7793 - val_loss: 29.1710 - val_MinusLogProbMetric: 29.1710 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 515/1000
2023-09-30 02:22:48.836 
Epoch 515/1000 
	 loss: 28.7442, MinusLogProbMetric: 28.7442, val_loss: 29.3943, val_MinusLogProbMetric: 29.3943

Epoch 515: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.7442 - MinusLogProbMetric: 28.7442 - val_loss: 29.3943 - val_MinusLogProbMetric: 29.3943 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 516/1000
2023-09-30 02:23:22.810 
Epoch 516/1000 
	 loss: 28.7685, MinusLogProbMetric: 28.7685, val_loss: 29.3125, val_MinusLogProbMetric: 29.3125

Epoch 516: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7685 - MinusLogProbMetric: 28.7685 - val_loss: 29.3125 - val_MinusLogProbMetric: 29.3125 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 517/1000
2023-09-30 02:23:55.285 
Epoch 517/1000 
	 loss: 28.7986, MinusLogProbMetric: 28.7986, val_loss: 29.1149, val_MinusLogProbMetric: 29.1149

Epoch 517: val_loss did not improve from 28.92877
196/196 - 32s - loss: 28.7986 - MinusLogProbMetric: 28.7986 - val_loss: 29.1149 - val_MinusLogProbMetric: 29.1149 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 518/1000
2023-09-30 02:24:28.289 
Epoch 518/1000 
	 loss: 28.7834, MinusLogProbMetric: 28.7834, val_loss: 29.3624, val_MinusLogProbMetric: 29.3624

Epoch 518: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.7834 - MinusLogProbMetric: 28.7834 - val_loss: 29.3624 - val_MinusLogProbMetric: 29.3624 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 519/1000
2023-09-30 02:25:02.376 
Epoch 519/1000 
	 loss: 28.6957, MinusLogProbMetric: 28.6957, val_loss: 28.9905, val_MinusLogProbMetric: 28.9905

Epoch 519: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.6957 - MinusLogProbMetric: 28.6957 - val_loss: 28.9905 - val_MinusLogProbMetric: 28.9905 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 520/1000
2023-09-30 02:25:36.252 
Epoch 520/1000 
	 loss: 28.7849, MinusLogProbMetric: 28.7849, val_loss: 29.0624, val_MinusLogProbMetric: 29.0624

Epoch 520: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7849 - MinusLogProbMetric: 28.7849 - val_loss: 29.0624 - val_MinusLogProbMetric: 29.0624 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 521/1000
2023-09-30 02:26:10.033 
Epoch 521/1000 
	 loss: 28.7637, MinusLogProbMetric: 28.7637, val_loss: 29.2033, val_MinusLogProbMetric: 29.2033

Epoch 521: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7637 - MinusLogProbMetric: 28.7637 - val_loss: 29.2033 - val_MinusLogProbMetric: 29.2033 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 522/1000
2023-09-30 02:26:44.109 
Epoch 522/1000 
	 loss: 28.7680, MinusLogProbMetric: 28.7680, val_loss: 29.2716, val_MinusLogProbMetric: 29.2716

Epoch 522: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7680 - MinusLogProbMetric: 28.7680 - val_loss: 29.2716 - val_MinusLogProbMetric: 29.2716 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 523/1000
2023-09-30 02:27:18.102 
Epoch 523/1000 
	 loss: 28.7788, MinusLogProbMetric: 28.7788, val_loss: 29.0374, val_MinusLogProbMetric: 29.0374

Epoch 523: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7788 - MinusLogProbMetric: 28.7788 - val_loss: 29.0374 - val_MinusLogProbMetric: 29.0374 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 524/1000
2023-09-30 02:27:51.885 
Epoch 524/1000 
	 loss: 28.7362, MinusLogProbMetric: 28.7362, val_loss: 29.8789, val_MinusLogProbMetric: 29.8789

Epoch 524: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7362 - MinusLogProbMetric: 28.7362 - val_loss: 29.8789 - val_MinusLogProbMetric: 29.8789 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 525/1000
2023-09-30 02:28:25.894 
Epoch 525/1000 
	 loss: 28.8667, MinusLogProbMetric: 28.8667, val_loss: 29.2818, val_MinusLogProbMetric: 29.2818

Epoch 525: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.8667 - MinusLogProbMetric: 28.8667 - val_loss: 29.2818 - val_MinusLogProbMetric: 29.2818 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 526/1000
2023-09-30 02:29:00.012 
Epoch 526/1000 
	 loss: 28.7066, MinusLogProbMetric: 28.7066, val_loss: 29.4877, val_MinusLogProbMetric: 29.4877

Epoch 526: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7066 - MinusLogProbMetric: 28.7066 - val_loss: 29.4877 - val_MinusLogProbMetric: 29.4877 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 527/1000
2023-09-30 02:29:34.033 
Epoch 527/1000 
	 loss: 28.6910, MinusLogProbMetric: 28.6910, val_loss: 29.1382, val_MinusLogProbMetric: 29.1382

Epoch 527: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.6910 - MinusLogProbMetric: 28.6910 - val_loss: 29.1382 - val_MinusLogProbMetric: 29.1382 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 528/1000
2023-09-30 02:30:07.827 
Epoch 528/1000 
	 loss: 28.6987, MinusLogProbMetric: 28.6987, val_loss: 29.1333, val_MinusLogProbMetric: 29.1333

Epoch 528: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.6987 - MinusLogProbMetric: 28.6987 - val_loss: 29.1333 - val_MinusLogProbMetric: 29.1333 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 529/1000
2023-09-30 02:30:41.696 
Epoch 529/1000 
	 loss: 28.7381, MinusLogProbMetric: 28.7381, val_loss: 29.0786, val_MinusLogProbMetric: 29.0786

Epoch 529: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7381 - MinusLogProbMetric: 28.7381 - val_loss: 29.0786 - val_MinusLogProbMetric: 29.0786 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 530/1000
2023-09-30 02:31:15.891 
Epoch 530/1000 
	 loss: 28.6901, MinusLogProbMetric: 28.6901, val_loss: 29.0838, val_MinusLogProbMetric: 29.0838

Epoch 530: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.6901 - MinusLogProbMetric: 28.6901 - val_loss: 29.0838 - val_MinusLogProbMetric: 29.0838 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 531/1000
2023-09-30 02:31:49.268 
Epoch 531/1000 
	 loss: 28.8750, MinusLogProbMetric: 28.8750, val_loss: 29.0953, val_MinusLogProbMetric: 29.0953

Epoch 531: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.8750 - MinusLogProbMetric: 28.8750 - val_loss: 29.0953 - val_MinusLogProbMetric: 29.0953 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 532/1000
2023-09-30 02:32:23.335 
Epoch 532/1000 
	 loss: 28.7623, MinusLogProbMetric: 28.7623, val_loss: 29.1894, val_MinusLogProbMetric: 29.1894

Epoch 532: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7623 - MinusLogProbMetric: 28.7623 - val_loss: 29.1894 - val_MinusLogProbMetric: 29.1894 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 533/1000
2023-09-30 02:32:56.598 
Epoch 533/1000 
	 loss: 28.7776, MinusLogProbMetric: 28.7776, val_loss: 28.9889, val_MinusLogProbMetric: 28.9889

Epoch 533: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.7776 - MinusLogProbMetric: 28.7776 - val_loss: 28.9889 - val_MinusLogProbMetric: 28.9889 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 534/1000
2023-09-30 02:33:29.864 
Epoch 534/1000 
	 loss: 28.7063, MinusLogProbMetric: 28.7063, val_loss: 28.9825, val_MinusLogProbMetric: 28.9825

Epoch 534: val_loss did not improve from 28.92877
196/196 - 33s - loss: 28.7063 - MinusLogProbMetric: 28.7063 - val_loss: 28.9825 - val_MinusLogProbMetric: 28.9825 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 535/1000
2023-09-30 02:34:03.756 
Epoch 535/1000 
	 loss: 28.7307, MinusLogProbMetric: 28.7307, val_loss: 29.3135, val_MinusLogProbMetric: 29.3135

Epoch 535: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7307 - MinusLogProbMetric: 28.7307 - val_loss: 29.3135 - val_MinusLogProbMetric: 29.3135 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 536/1000
2023-09-30 02:34:37.941 
Epoch 536/1000 
	 loss: 28.6725, MinusLogProbMetric: 28.6725, val_loss: 29.1019, val_MinusLogProbMetric: 29.1019

Epoch 536: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.6725 - MinusLogProbMetric: 28.6725 - val_loss: 29.1019 - val_MinusLogProbMetric: 29.1019 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 537/1000
2023-09-30 02:35:12.057 
Epoch 537/1000 
	 loss: 28.7406, MinusLogProbMetric: 28.7406, val_loss: 29.2169, val_MinusLogProbMetric: 29.2169

Epoch 537: val_loss did not improve from 28.92877
196/196 - 34s - loss: 28.7406 - MinusLogProbMetric: 28.7406 - val_loss: 29.2169 - val_MinusLogProbMetric: 29.2169 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 538/1000
2023-09-30 02:35:46.279 
Epoch 538/1000 
	 loss: 28.6635, MinusLogProbMetric: 28.6635, val_loss: 28.9122, val_MinusLogProbMetric: 28.9122

Epoch 538: val_loss improved from 28.92877 to 28.91223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.6635 - MinusLogProbMetric: 28.6635 - val_loss: 28.9122 - val_MinusLogProbMetric: 28.9122 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 539/1000
2023-09-30 02:36:20.974 
Epoch 539/1000 
	 loss: 28.7597, MinusLogProbMetric: 28.7597, val_loss: 29.2770, val_MinusLogProbMetric: 29.2770

Epoch 539: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7597 - MinusLogProbMetric: 28.7597 - val_loss: 29.2770 - val_MinusLogProbMetric: 29.2770 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 540/1000
2023-09-30 02:36:55.244 
Epoch 540/1000 
	 loss: 28.7703, MinusLogProbMetric: 28.7703, val_loss: 29.2551, val_MinusLogProbMetric: 29.2551

Epoch 540: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7703 - MinusLogProbMetric: 28.7703 - val_loss: 29.2551 - val_MinusLogProbMetric: 29.2551 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 541/1000
2023-09-30 02:37:29.334 
Epoch 541/1000 
	 loss: 28.7183, MinusLogProbMetric: 28.7183, val_loss: 29.3467, val_MinusLogProbMetric: 29.3467

Epoch 541: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7183 - MinusLogProbMetric: 28.7183 - val_loss: 29.3467 - val_MinusLogProbMetric: 29.3467 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 542/1000
2023-09-30 02:38:03.540 
Epoch 542/1000 
	 loss: 28.8536, MinusLogProbMetric: 28.8536, val_loss: 29.3339, val_MinusLogProbMetric: 29.3339

Epoch 542: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.8536 - MinusLogProbMetric: 28.8536 - val_loss: 29.3339 - val_MinusLogProbMetric: 29.3339 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 543/1000
2023-09-30 02:38:36.015 
Epoch 543/1000 
	 loss: 28.7179, MinusLogProbMetric: 28.7179, val_loss: 29.2651, val_MinusLogProbMetric: 29.2651

Epoch 543: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.7179 - MinusLogProbMetric: 28.7179 - val_loss: 29.2651 - val_MinusLogProbMetric: 29.2651 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 544/1000
2023-09-30 02:39:08.912 
Epoch 544/1000 
	 loss: 28.7694, MinusLogProbMetric: 28.7694, val_loss: 29.3368, val_MinusLogProbMetric: 29.3368

Epoch 544: val_loss did not improve from 28.91223
196/196 - 33s - loss: 28.7694 - MinusLogProbMetric: 28.7694 - val_loss: 29.3368 - val_MinusLogProbMetric: 29.3368 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 545/1000
2023-09-30 02:39:43.286 
Epoch 545/1000 
	 loss: 28.7541, MinusLogProbMetric: 28.7541, val_loss: 29.1161, val_MinusLogProbMetric: 29.1161

Epoch 545: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7541 - MinusLogProbMetric: 28.7541 - val_loss: 29.1161 - val_MinusLogProbMetric: 29.1161 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 546/1000
2023-09-30 02:40:17.514 
Epoch 546/1000 
	 loss: 28.7772, MinusLogProbMetric: 28.7772, val_loss: 29.0739, val_MinusLogProbMetric: 29.0739

Epoch 546: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7772 - MinusLogProbMetric: 28.7772 - val_loss: 29.0739 - val_MinusLogProbMetric: 29.0739 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 547/1000
2023-09-30 02:40:50.920 
Epoch 547/1000 
	 loss: 28.7631, MinusLogProbMetric: 28.7631, val_loss: 29.5811, val_MinusLogProbMetric: 29.5811

Epoch 547: val_loss did not improve from 28.91223
196/196 - 33s - loss: 28.7631 - MinusLogProbMetric: 28.7631 - val_loss: 29.5811 - val_MinusLogProbMetric: 29.5811 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 548/1000
2023-09-30 02:41:24.539 
Epoch 548/1000 
	 loss: 28.7071, MinusLogProbMetric: 28.7071, val_loss: 29.1376, val_MinusLogProbMetric: 29.1376

Epoch 548: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7071 - MinusLogProbMetric: 28.7071 - val_loss: 29.1376 - val_MinusLogProbMetric: 29.1376 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 549/1000
2023-09-30 02:41:58.291 
Epoch 549/1000 
	 loss: 28.6539, MinusLogProbMetric: 28.6539, val_loss: 29.2262, val_MinusLogProbMetric: 29.2262

Epoch 549: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.6539 - MinusLogProbMetric: 28.6539 - val_loss: 29.2262 - val_MinusLogProbMetric: 29.2262 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 550/1000
2023-09-30 02:42:32.162 
Epoch 550/1000 
	 loss: 28.7321, MinusLogProbMetric: 28.7321, val_loss: 29.0170, val_MinusLogProbMetric: 29.0170

Epoch 550: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7321 - MinusLogProbMetric: 28.7321 - val_loss: 29.0170 - val_MinusLogProbMetric: 29.0170 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 551/1000
2023-09-30 02:43:05.758 
Epoch 551/1000 
	 loss: 28.7383, MinusLogProbMetric: 28.7383, val_loss: 29.4998, val_MinusLogProbMetric: 29.4998

Epoch 551: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7383 - MinusLogProbMetric: 28.7383 - val_loss: 29.4998 - val_MinusLogProbMetric: 29.4998 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 552/1000
2023-09-30 02:43:39.219 
Epoch 552/1000 
	 loss: 28.7192, MinusLogProbMetric: 28.7192, val_loss: 29.2787, val_MinusLogProbMetric: 29.2787

Epoch 552: val_loss did not improve from 28.91223
196/196 - 33s - loss: 28.7192 - MinusLogProbMetric: 28.7192 - val_loss: 29.2787 - val_MinusLogProbMetric: 29.2787 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 553/1000
2023-09-30 02:44:11.881 
Epoch 553/1000 
	 loss: 28.6899, MinusLogProbMetric: 28.6899, val_loss: 29.2584, val_MinusLogProbMetric: 29.2584

Epoch 553: val_loss did not improve from 28.91223
196/196 - 33s - loss: 28.6899 - MinusLogProbMetric: 28.6899 - val_loss: 29.2584 - val_MinusLogProbMetric: 29.2584 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 554/1000
2023-09-30 02:44:45.831 
Epoch 554/1000 
	 loss: 28.7151, MinusLogProbMetric: 28.7151, val_loss: 28.9839, val_MinusLogProbMetric: 28.9839

Epoch 554: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7151 - MinusLogProbMetric: 28.7151 - val_loss: 28.9839 - val_MinusLogProbMetric: 28.9839 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 555/1000
2023-09-30 02:45:19.500 
Epoch 555/1000 
	 loss: 28.7084, MinusLogProbMetric: 28.7084, val_loss: 29.0368, val_MinusLogProbMetric: 29.0368

Epoch 555: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7084 - MinusLogProbMetric: 28.7084 - val_loss: 29.0368 - val_MinusLogProbMetric: 29.0368 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 556/1000
2023-09-30 02:45:53.701 
Epoch 556/1000 
	 loss: 28.7155, MinusLogProbMetric: 28.7155, val_loss: 29.0598, val_MinusLogProbMetric: 29.0598

Epoch 556: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7155 - MinusLogProbMetric: 28.7155 - val_loss: 29.0598 - val_MinusLogProbMetric: 29.0598 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 557/1000
2023-09-30 02:46:27.574 
Epoch 557/1000 
	 loss: 28.7181, MinusLogProbMetric: 28.7181, val_loss: 29.2753, val_MinusLogProbMetric: 29.2753

Epoch 557: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.7181 - MinusLogProbMetric: 28.7181 - val_loss: 29.2753 - val_MinusLogProbMetric: 29.2753 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 558/1000
2023-09-30 02:47:01.647 
Epoch 558/1000 
	 loss: 28.6843, MinusLogProbMetric: 28.6843, val_loss: 29.5330, val_MinusLogProbMetric: 29.5330

Epoch 558: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.6843 - MinusLogProbMetric: 28.6843 - val_loss: 29.5330 - val_MinusLogProbMetric: 29.5330 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 559/1000
2023-09-30 02:47:34.958 
Epoch 559/1000 
	 loss: 28.6897, MinusLogProbMetric: 28.6897, val_loss: 29.0951, val_MinusLogProbMetric: 29.0951

Epoch 559: val_loss did not improve from 28.91223
196/196 - 33s - loss: 28.6897 - MinusLogProbMetric: 28.6897 - val_loss: 29.0951 - val_MinusLogProbMetric: 29.0951 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 560/1000
2023-09-30 02:48:08.918 
Epoch 560/1000 
	 loss: 28.6797, MinusLogProbMetric: 28.6797, val_loss: 28.9610, val_MinusLogProbMetric: 28.9610

Epoch 560: val_loss did not improve from 28.91223
196/196 - 34s - loss: 28.6797 - MinusLogProbMetric: 28.6797 - val_loss: 28.9610 - val_MinusLogProbMetric: 28.9610 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 561/1000
2023-09-30 02:48:40.464 
Epoch 561/1000 
	 loss: 28.6862, MinusLogProbMetric: 28.6862, val_loss: 29.1112, val_MinusLogProbMetric: 29.1112

Epoch 561: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.6862 - MinusLogProbMetric: 28.6862 - val_loss: 29.1112 - val_MinusLogProbMetric: 29.1112 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 562/1000
2023-09-30 02:49:10.864 
Epoch 562/1000 
	 loss: 28.7041, MinusLogProbMetric: 28.7041, val_loss: 29.1502, val_MinusLogProbMetric: 29.1502

Epoch 562: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.7041 - MinusLogProbMetric: 28.7041 - val_loss: 29.1502 - val_MinusLogProbMetric: 29.1502 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 563/1000
2023-09-30 02:49:42.444 
Epoch 563/1000 
	 loss: 28.6969, MinusLogProbMetric: 28.6969, val_loss: 28.9924, val_MinusLogProbMetric: 28.9924

Epoch 563: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.6969 - MinusLogProbMetric: 28.6969 - val_loss: 28.9924 - val_MinusLogProbMetric: 28.9924 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 564/1000
2023-09-30 02:50:12.899 
Epoch 564/1000 
	 loss: 28.7585, MinusLogProbMetric: 28.7585, val_loss: 29.8853, val_MinusLogProbMetric: 29.8853

Epoch 564: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.7585 - MinusLogProbMetric: 28.7585 - val_loss: 29.8853 - val_MinusLogProbMetric: 29.8853 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 565/1000
2023-09-30 02:50:44.946 
Epoch 565/1000 
	 loss: 28.7016, MinusLogProbMetric: 28.7016, val_loss: 29.0877, val_MinusLogProbMetric: 29.0877

Epoch 565: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.7016 - MinusLogProbMetric: 28.7016 - val_loss: 29.0877 - val_MinusLogProbMetric: 29.0877 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 566/1000
2023-09-30 02:51:17.416 
Epoch 566/1000 
	 loss: 28.6533, MinusLogProbMetric: 28.6533, val_loss: 28.9883, val_MinusLogProbMetric: 28.9883

Epoch 566: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.6533 - MinusLogProbMetric: 28.6533 - val_loss: 28.9883 - val_MinusLogProbMetric: 28.9883 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 567/1000
2023-09-30 02:51:47.189 
Epoch 567/1000 
	 loss: 28.6937, MinusLogProbMetric: 28.6937, val_loss: 29.0794, val_MinusLogProbMetric: 29.0794

Epoch 567: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.6937 - MinusLogProbMetric: 28.6937 - val_loss: 29.0794 - val_MinusLogProbMetric: 29.0794 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 568/1000
2023-09-30 02:52:18.132 
Epoch 568/1000 
	 loss: 28.7000, MinusLogProbMetric: 28.7000, val_loss: 28.9647, val_MinusLogProbMetric: 28.9647

Epoch 568: val_loss did not improve from 28.91223
196/196 - 31s - loss: 28.7000 - MinusLogProbMetric: 28.7000 - val_loss: 28.9647 - val_MinusLogProbMetric: 28.9647 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 569/1000
2023-09-30 02:52:50.435 
Epoch 569/1000 
	 loss: 28.6788, MinusLogProbMetric: 28.6788, val_loss: 29.1754, val_MinusLogProbMetric: 29.1754

Epoch 569: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.6788 - MinusLogProbMetric: 28.6788 - val_loss: 29.1754 - val_MinusLogProbMetric: 29.1754 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 570/1000
2023-09-30 02:53:20.573 
Epoch 570/1000 
	 loss: 28.7260, MinusLogProbMetric: 28.7260, val_loss: 29.3238, val_MinusLogProbMetric: 29.3238

Epoch 570: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.7260 - MinusLogProbMetric: 28.7260 - val_loss: 29.3238 - val_MinusLogProbMetric: 29.3238 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 571/1000
2023-09-30 02:53:52.277 
Epoch 571/1000 
	 loss: 28.7028, MinusLogProbMetric: 28.7028, val_loss: 29.6641, val_MinusLogProbMetric: 29.6641

Epoch 571: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.7028 - MinusLogProbMetric: 28.7028 - val_loss: 29.6641 - val_MinusLogProbMetric: 29.6641 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 572/1000
2023-09-30 02:54:21.998 
Epoch 572/1000 
	 loss: 28.6598, MinusLogProbMetric: 28.6598, val_loss: 29.0430, val_MinusLogProbMetric: 29.0430

Epoch 572: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.6598 - MinusLogProbMetric: 28.6598 - val_loss: 29.0430 - val_MinusLogProbMetric: 29.0430 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 573/1000
2023-09-30 02:54:51.075 
Epoch 573/1000 
	 loss: 28.6666, MinusLogProbMetric: 28.6666, val_loss: 29.0979, val_MinusLogProbMetric: 29.0979

Epoch 573: val_loss did not improve from 28.91223
196/196 - 29s - loss: 28.6666 - MinusLogProbMetric: 28.6666 - val_loss: 29.0979 - val_MinusLogProbMetric: 29.0979 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 574/1000
2023-09-30 02:55:22.635 
Epoch 574/1000 
	 loss: 28.6125, MinusLogProbMetric: 28.6125, val_loss: 29.1436, val_MinusLogProbMetric: 29.1436

Epoch 574: val_loss did not improve from 28.91223
196/196 - 32s - loss: 28.6125 - MinusLogProbMetric: 28.6125 - val_loss: 29.1436 - val_MinusLogProbMetric: 29.1436 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 575/1000
2023-09-30 02:55:53.673 
Epoch 575/1000 
	 loss: 28.7161, MinusLogProbMetric: 28.7161, val_loss: 29.3208, val_MinusLogProbMetric: 29.3208

Epoch 575: val_loss did not improve from 28.91223
196/196 - 31s - loss: 28.7161 - MinusLogProbMetric: 28.7161 - val_loss: 29.3208 - val_MinusLogProbMetric: 29.3208 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 576/1000
2023-09-30 02:56:24.723 
Epoch 576/1000 
	 loss: 28.6940, MinusLogProbMetric: 28.6940, val_loss: 29.2942, val_MinusLogProbMetric: 29.2942

Epoch 576: val_loss did not improve from 28.91223
196/196 - 31s - loss: 28.6940 - MinusLogProbMetric: 28.6940 - val_loss: 29.2942 - val_MinusLogProbMetric: 29.2942 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 577/1000
2023-09-30 02:56:53.455 
Epoch 577/1000 
	 loss: 28.7350, MinusLogProbMetric: 28.7350, val_loss: 29.2649, val_MinusLogProbMetric: 29.2649

Epoch 577: val_loss did not improve from 28.91223
196/196 - 29s - loss: 28.7350 - MinusLogProbMetric: 28.7350 - val_loss: 29.2649 - val_MinusLogProbMetric: 29.2649 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 578/1000
2023-09-30 02:57:23.403 
Epoch 578/1000 
	 loss: 28.7680, MinusLogProbMetric: 28.7680, val_loss: 29.0499, val_MinusLogProbMetric: 29.0499

Epoch 578: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.7680 - MinusLogProbMetric: 28.7680 - val_loss: 29.0499 - val_MinusLogProbMetric: 29.0499 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 579/1000
2023-09-30 02:57:54.185 
Epoch 579/1000 
	 loss: 28.6172, MinusLogProbMetric: 28.6172, val_loss: 29.0593, val_MinusLogProbMetric: 29.0593

Epoch 579: val_loss did not improve from 28.91223
196/196 - 31s - loss: 28.6172 - MinusLogProbMetric: 28.6172 - val_loss: 29.0593 - val_MinusLogProbMetric: 29.0593 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 580/1000
2023-09-30 02:58:24.935 
Epoch 580/1000 
	 loss: 28.6111, MinusLogProbMetric: 28.6111, val_loss: 28.9573, val_MinusLogProbMetric: 28.9573

Epoch 580: val_loss did not improve from 28.91223
196/196 - 31s - loss: 28.6111 - MinusLogProbMetric: 28.6111 - val_loss: 28.9573 - val_MinusLogProbMetric: 28.9573 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 581/1000
2023-09-30 02:58:52.667 
Epoch 581/1000 
	 loss: 28.6760, MinusLogProbMetric: 28.6760, val_loss: 29.2011, val_MinusLogProbMetric: 29.2011

Epoch 581: val_loss did not improve from 28.91223
196/196 - 28s - loss: 28.6760 - MinusLogProbMetric: 28.6760 - val_loss: 29.2011 - val_MinusLogProbMetric: 29.2011 - lr: 5.0000e-04 - 28s/epoch - 141ms/step
Epoch 582/1000
2023-09-30 02:59:23.654 
Epoch 582/1000 
	 loss: 28.6642, MinusLogProbMetric: 28.6642, val_loss: 29.2203, val_MinusLogProbMetric: 29.2203

Epoch 582: val_loss did not improve from 28.91223
196/196 - 31s - loss: 28.6642 - MinusLogProbMetric: 28.6642 - val_loss: 29.2203 - val_MinusLogProbMetric: 29.2203 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 583/1000
2023-09-30 02:59:52.812 
Epoch 583/1000 
	 loss: 28.6775, MinusLogProbMetric: 28.6775, val_loss: 29.2255, val_MinusLogProbMetric: 29.2255

Epoch 583: val_loss did not improve from 28.91223
196/196 - 29s - loss: 28.6775 - MinusLogProbMetric: 28.6775 - val_loss: 29.2255 - val_MinusLogProbMetric: 29.2255 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 584/1000
2023-09-30 03:00:23.221 
Epoch 584/1000 
	 loss: 28.6590, MinusLogProbMetric: 28.6590, val_loss: 29.2235, val_MinusLogProbMetric: 29.2235

Epoch 584: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.6590 - MinusLogProbMetric: 28.6590 - val_loss: 29.2235 - val_MinusLogProbMetric: 29.2235 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 585/1000
2023-09-30 03:00:53.232 
Epoch 585/1000 
	 loss: 28.7344, MinusLogProbMetric: 28.7344, val_loss: 29.2400, val_MinusLogProbMetric: 29.2400

Epoch 585: val_loss did not improve from 28.91223
196/196 - 30s - loss: 28.7344 - MinusLogProbMetric: 28.7344 - val_loss: 29.2400 - val_MinusLogProbMetric: 29.2400 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 586/1000
2023-09-30 03:01:23.984 
Epoch 586/1000 
	 loss: 28.6017, MinusLogProbMetric: 28.6017, val_loss: 29.0363, val_MinusLogProbMetric: 29.0363

Epoch 586: val_loss did not improve from 28.91223
196/196 - 31s - loss: 28.6017 - MinusLogProbMetric: 28.6017 - val_loss: 29.0363 - val_MinusLogProbMetric: 29.0363 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 587/1000
2023-09-30 03:01:51.516 
Epoch 587/1000 
	 loss: 28.6140, MinusLogProbMetric: 28.6140, val_loss: 28.8701, val_MinusLogProbMetric: 28.8701

Epoch 587: val_loss improved from 28.91223 to 28.87007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 28s - loss: 28.6140 - MinusLogProbMetric: 28.6140 - val_loss: 28.8701 - val_MinusLogProbMetric: 28.8701 - lr: 5.0000e-04 - 28s/epoch - 143ms/step
Epoch 588/1000
2023-09-30 03:02:25.373 
Epoch 588/1000 
	 loss: 28.6387, MinusLogProbMetric: 28.6387, val_loss: 29.2520, val_MinusLogProbMetric: 29.2520

Epoch 588: val_loss did not improve from 28.87007
196/196 - 33s - loss: 28.6387 - MinusLogProbMetric: 28.6387 - val_loss: 29.2520 - val_MinusLogProbMetric: 29.2520 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 589/1000
2023-09-30 03:02:55.932 
Epoch 589/1000 
	 loss: 28.6690, MinusLogProbMetric: 28.6690, val_loss: 29.2094, val_MinusLogProbMetric: 29.2094

Epoch 589: val_loss did not improve from 28.87007
196/196 - 31s - loss: 28.6690 - MinusLogProbMetric: 28.6690 - val_loss: 29.2094 - val_MinusLogProbMetric: 29.2094 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 590/1000
2023-09-30 03:03:25.401 
Epoch 590/1000 
	 loss: 28.6539, MinusLogProbMetric: 28.6539, val_loss: 29.0396, val_MinusLogProbMetric: 29.0396

Epoch 590: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6539 - MinusLogProbMetric: 28.6539 - val_loss: 29.0396 - val_MinusLogProbMetric: 29.0396 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 591/1000
2023-09-30 03:03:55.932 
Epoch 591/1000 
	 loss: 28.7300, MinusLogProbMetric: 28.7300, val_loss: 29.2450, val_MinusLogProbMetric: 29.2450

Epoch 591: val_loss did not improve from 28.87007
196/196 - 31s - loss: 28.7300 - MinusLogProbMetric: 28.7300 - val_loss: 29.2450 - val_MinusLogProbMetric: 29.2450 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 592/1000
2023-09-30 03:04:24.585 
Epoch 592/1000 
	 loss: 28.6680, MinusLogProbMetric: 28.6680, val_loss: 28.9761, val_MinusLogProbMetric: 28.9761

Epoch 592: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6680 - MinusLogProbMetric: 28.6680 - val_loss: 28.9761 - val_MinusLogProbMetric: 28.9761 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 593/1000
2023-09-30 03:04:55.268 
Epoch 593/1000 
	 loss: 28.6920, MinusLogProbMetric: 28.6920, val_loss: 29.1266, val_MinusLogProbMetric: 29.1266

Epoch 593: val_loss did not improve from 28.87007
196/196 - 31s - loss: 28.6920 - MinusLogProbMetric: 28.6920 - val_loss: 29.1266 - val_MinusLogProbMetric: 29.1266 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 594/1000
2023-09-30 03:05:22.637 
Epoch 594/1000 
	 loss: 28.7048, MinusLogProbMetric: 28.7048, val_loss: 29.1242, val_MinusLogProbMetric: 29.1242

Epoch 594: val_loss did not improve from 28.87007
196/196 - 27s - loss: 28.7048 - MinusLogProbMetric: 28.7048 - val_loss: 29.1242 - val_MinusLogProbMetric: 29.1242 - lr: 5.0000e-04 - 27s/epoch - 140ms/step
Epoch 595/1000
2023-09-30 03:05:51.857 
Epoch 595/1000 
	 loss: 28.6600, MinusLogProbMetric: 28.6600, val_loss: 29.2188, val_MinusLogProbMetric: 29.2188

Epoch 595: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6600 - MinusLogProbMetric: 28.6600 - val_loss: 29.2188 - val_MinusLogProbMetric: 29.2188 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 596/1000
2023-09-30 03:06:21.037 
Epoch 596/1000 
	 loss: 28.5878, MinusLogProbMetric: 28.5878, val_loss: 28.9574, val_MinusLogProbMetric: 28.9574

Epoch 596: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.5878 - MinusLogProbMetric: 28.5878 - val_loss: 28.9574 - val_MinusLogProbMetric: 28.9574 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 597/1000
2023-09-30 03:06:51.330 
Epoch 597/1000 
	 loss: 28.7401, MinusLogProbMetric: 28.7401, val_loss: 29.2940, val_MinusLogProbMetric: 29.2940

Epoch 597: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.7401 - MinusLogProbMetric: 28.7401 - val_loss: 29.2940 - val_MinusLogProbMetric: 29.2940 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 598/1000
2023-09-30 03:07:20.299 
Epoch 598/1000 
	 loss: 28.6250, MinusLogProbMetric: 28.6250, val_loss: 29.4141, val_MinusLogProbMetric: 29.4141

Epoch 598: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6250 - MinusLogProbMetric: 28.6250 - val_loss: 29.4141 - val_MinusLogProbMetric: 29.4141 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 599/1000
2023-09-30 03:07:50.728 
Epoch 599/1000 
	 loss: 28.7156, MinusLogProbMetric: 28.7156, val_loss: 29.1437, val_MinusLogProbMetric: 29.1437

Epoch 599: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.7156 - MinusLogProbMetric: 28.7156 - val_loss: 29.1437 - val_MinusLogProbMetric: 29.1437 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 600/1000
2023-09-30 03:08:20.102 
Epoch 600/1000 
	 loss: 28.7044, MinusLogProbMetric: 28.7044, val_loss: 29.2043, val_MinusLogProbMetric: 29.2043

Epoch 600: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.7044 - MinusLogProbMetric: 28.7044 - val_loss: 29.2043 - val_MinusLogProbMetric: 29.2043 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 601/1000
2023-09-30 03:08:52.246 
Epoch 601/1000 
	 loss: 28.6434, MinusLogProbMetric: 28.6434, val_loss: 29.0616, val_MinusLogProbMetric: 29.0616

Epoch 601: val_loss did not improve from 28.87007
196/196 - 32s - loss: 28.6434 - MinusLogProbMetric: 28.6434 - val_loss: 29.0616 - val_MinusLogProbMetric: 29.0616 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 602/1000
2023-09-30 03:09:21.589 
Epoch 602/1000 
	 loss: 28.6104, MinusLogProbMetric: 28.6104, val_loss: 28.9018, val_MinusLogProbMetric: 28.9018

Epoch 602: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6104 - MinusLogProbMetric: 28.6104 - val_loss: 28.9018 - val_MinusLogProbMetric: 28.9018 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 603/1000
2023-09-30 03:09:51.858 
Epoch 603/1000 
	 loss: 28.6482, MinusLogProbMetric: 28.6482, val_loss: 28.8960, val_MinusLogProbMetric: 28.8960

Epoch 603: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6482 - MinusLogProbMetric: 28.6482 - val_loss: 28.8960 - val_MinusLogProbMetric: 28.8960 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 604/1000
2023-09-30 03:10:20.934 
Epoch 604/1000 
	 loss: 28.6387, MinusLogProbMetric: 28.6387, val_loss: 29.0416, val_MinusLogProbMetric: 29.0416

Epoch 604: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6387 - MinusLogProbMetric: 28.6387 - val_loss: 29.0416 - val_MinusLogProbMetric: 29.0416 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 605/1000
2023-09-30 03:10:50.497 
Epoch 605/1000 
	 loss: 28.6874, MinusLogProbMetric: 28.6874, val_loss: 29.0046, val_MinusLogProbMetric: 29.0046

Epoch 605: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6874 - MinusLogProbMetric: 28.6874 - val_loss: 29.0046 - val_MinusLogProbMetric: 29.0046 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 606/1000
2023-09-30 03:11:18.654 
Epoch 606/1000 
	 loss: 28.6176, MinusLogProbMetric: 28.6176, val_loss: 29.1149, val_MinusLogProbMetric: 29.1149

Epoch 606: val_loss did not improve from 28.87007
196/196 - 28s - loss: 28.6176 - MinusLogProbMetric: 28.6176 - val_loss: 29.1149 - val_MinusLogProbMetric: 29.1149 - lr: 5.0000e-04 - 28s/epoch - 144ms/step
Epoch 607/1000
2023-09-30 03:11:47.894 
Epoch 607/1000 
	 loss: 28.6236, MinusLogProbMetric: 28.6236, val_loss: 29.6772, val_MinusLogProbMetric: 29.6772

Epoch 607: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6236 - MinusLogProbMetric: 28.6236 - val_loss: 29.6772 - val_MinusLogProbMetric: 29.6772 - lr: 5.0000e-04 - 29s/epoch - 149ms/step
Epoch 608/1000
2023-09-30 03:12:15.050 
Epoch 608/1000 
	 loss: 28.7275, MinusLogProbMetric: 28.7275, val_loss: 28.8982, val_MinusLogProbMetric: 28.8982

Epoch 608: val_loss did not improve from 28.87007
196/196 - 27s - loss: 28.7275 - MinusLogProbMetric: 28.7275 - val_loss: 28.8982 - val_MinusLogProbMetric: 28.8982 - lr: 5.0000e-04 - 27s/epoch - 139ms/step
Epoch 609/1000
2023-09-30 03:12:45.459 
Epoch 609/1000 
	 loss: 28.6551, MinusLogProbMetric: 28.6551, val_loss: 28.9532, val_MinusLogProbMetric: 28.9532

Epoch 609: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6551 - MinusLogProbMetric: 28.6551 - val_loss: 28.9532 - val_MinusLogProbMetric: 28.9532 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 610/1000
2023-09-30 03:13:13.458 
Epoch 610/1000 
	 loss: 28.8010, MinusLogProbMetric: 28.8010, val_loss: 29.1875, val_MinusLogProbMetric: 29.1875

Epoch 610: val_loss did not improve from 28.87007
196/196 - 28s - loss: 28.8010 - MinusLogProbMetric: 28.8010 - val_loss: 29.1875 - val_MinusLogProbMetric: 29.1875 - lr: 5.0000e-04 - 28s/epoch - 143ms/step
Epoch 611/1000
2023-09-30 03:13:43.312 
Epoch 611/1000 
	 loss: 28.6033, MinusLogProbMetric: 28.6033, val_loss: 28.9262, val_MinusLogProbMetric: 28.9262

Epoch 611: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6033 - MinusLogProbMetric: 28.6033 - val_loss: 28.9262 - val_MinusLogProbMetric: 28.9262 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 612/1000
2023-09-30 03:14:13.285 
Epoch 612/1000 
	 loss: 28.6454, MinusLogProbMetric: 28.6454, val_loss: 28.9785, val_MinusLogProbMetric: 28.9785

Epoch 612: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6454 - MinusLogProbMetric: 28.6454 - val_loss: 28.9785 - val_MinusLogProbMetric: 28.9785 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 613/1000
2023-09-30 03:14:45.341 
Epoch 613/1000 
	 loss: 28.5836, MinusLogProbMetric: 28.5836, val_loss: 29.1780, val_MinusLogProbMetric: 29.1780

Epoch 613: val_loss did not improve from 28.87007
196/196 - 32s - loss: 28.5836 - MinusLogProbMetric: 28.5836 - val_loss: 29.1780 - val_MinusLogProbMetric: 29.1780 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 614/1000
2023-09-30 03:15:15.406 
Epoch 614/1000 
	 loss: 28.5937, MinusLogProbMetric: 28.5937, val_loss: 28.8947, val_MinusLogProbMetric: 28.8947

Epoch 614: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.5937 - MinusLogProbMetric: 28.5937 - val_loss: 28.8947 - val_MinusLogProbMetric: 28.8947 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 615/1000
2023-09-30 03:15:43.861 
Epoch 615/1000 
	 loss: 28.6658, MinusLogProbMetric: 28.6658, val_loss: 29.1475, val_MinusLogProbMetric: 29.1475

Epoch 615: val_loss did not improve from 28.87007
196/196 - 28s - loss: 28.6658 - MinusLogProbMetric: 28.6658 - val_loss: 29.1475 - val_MinusLogProbMetric: 29.1475 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 616/1000
2023-09-30 03:16:10.829 
Epoch 616/1000 
	 loss: 28.6000, MinusLogProbMetric: 28.6000, val_loss: 28.9021, val_MinusLogProbMetric: 28.9021

Epoch 616: val_loss did not improve from 28.87007
196/196 - 27s - loss: 28.6000 - MinusLogProbMetric: 28.6000 - val_loss: 28.9021 - val_MinusLogProbMetric: 28.9021 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 617/1000
2023-09-30 03:16:40.621 
Epoch 617/1000 
	 loss: 28.6833, MinusLogProbMetric: 28.6833, val_loss: 29.0556, val_MinusLogProbMetric: 29.0556

Epoch 617: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6833 - MinusLogProbMetric: 28.6833 - val_loss: 29.0556 - val_MinusLogProbMetric: 29.0556 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 618/1000
2023-09-30 03:17:10.055 
Epoch 618/1000 
	 loss: 28.7221, MinusLogProbMetric: 28.7221, val_loss: 28.9362, val_MinusLogProbMetric: 28.9362

Epoch 618: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.7221 - MinusLogProbMetric: 28.7221 - val_loss: 28.9362 - val_MinusLogProbMetric: 28.9362 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 619/1000
2023-09-30 03:17:40.149 
Epoch 619/1000 
	 loss: 28.6215, MinusLogProbMetric: 28.6215, val_loss: 29.1488, val_MinusLogProbMetric: 29.1488

Epoch 619: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6215 - MinusLogProbMetric: 28.6215 - val_loss: 29.1488 - val_MinusLogProbMetric: 29.1488 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 620/1000
2023-09-30 03:18:08.233 
Epoch 620/1000 
	 loss: 28.6249, MinusLogProbMetric: 28.6249, val_loss: 29.0527, val_MinusLogProbMetric: 29.0527

Epoch 620: val_loss did not improve from 28.87007
196/196 - 28s - loss: 28.6249 - MinusLogProbMetric: 28.6249 - val_loss: 29.0527 - val_MinusLogProbMetric: 29.0527 - lr: 5.0000e-04 - 28s/epoch - 143ms/step
Epoch 621/1000
2023-09-30 03:18:37.770 
Epoch 621/1000 
	 loss: 28.6485, MinusLogProbMetric: 28.6485, val_loss: 28.9872, val_MinusLogProbMetric: 28.9872

Epoch 621: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6485 - MinusLogProbMetric: 28.6485 - val_loss: 28.9872 - val_MinusLogProbMetric: 28.9872 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 622/1000
2023-09-30 03:19:04.753 
Epoch 622/1000 
	 loss: 28.6318, MinusLogProbMetric: 28.6318, val_loss: 29.1081, val_MinusLogProbMetric: 29.1081

Epoch 622: val_loss did not improve from 28.87007
196/196 - 27s - loss: 28.6318 - MinusLogProbMetric: 28.6318 - val_loss: 29.1081 - val_MinusLogProbMetric: 29.1081 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 623/1000
2023-09-30 03:19:35.462 
Epoch 623/1000 
	 loss: 28.6611, MinusLogProbMetric: 28.6611, val_loss: 29.0863, val_MinusLogProbMetric: 29.0863

Epoch 623: val_loss did not improve from 28.87007
196/196 - 31s - loss: 28.6611 - MinusLogProbMetric: 28.6611 - val_loss: 29.0863 - val_MinusLogProbMetric: 29.0863 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 624/1000
2023-09-30 03:20:06.188 
Epoch 624/1000 
	 loss: 28.6063, MinusLogProbMetric: 28.6063, val_loss: 29.7799, val_MinusLogProbMetric: 29.7799

Epoch 624: val_loss did not improve from 28.87007
196/196 - 31s - loss: 28.6063 - MinusLogProbMetric: 28.6063 - val_loss: 29.7799 - val_MinusLogProbMetric: 29.7799 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 625/1000
2023-09-30 03:20:33.621 
Epoch 625/1000 
	 loss: 28.6122, MinusLogProbMetric: 28.6122, val_loss: 29.1132, val_MinusLogProbMetric: 29.1132

Epoch 625: val_loss did not improve from 28.87007
196/196 - 27s - loss: 28.6122 - MinusLogProbMetric: 28.6122 - val_loss: 29.1132 - val_MinusLogProbMetric: 29.1132 - lr: 5.0000e-04 - 27s/epoch - 140ms/step
Epoch 626/1000
2023-09-30 03:21:02.475 
Epoch 626/1000 
	 loss: 28.5818, MinusLogProbMetric: 28.5818, val_loss: 29.6893, val_MinusLogProbMetric: 29.6893

Epoch 626: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.5818 - MinusLogProbMetric: 28.5818 - val_loss: 29.6893 - val_MinusLogProbMetric: 29.6893 - lr: 5.0000e-04 - 29s/epoch - 147ms/step
Epoch 627/1000
2023-09-30 03:21:30.824 
Epoch 627/1000 
	 loss: 28.6916, MinusLogProbMetric: 28.6916, val_loss: 29.6898, val_MinusLogProbMetric: 29.6898

Epoch 627: val_loss did not improve from 28.87007
196/196 - 28s - loss: 28.6916 - MinusLogProbMetric: 28.6916 - val_loss: 29.6898 - val_MinusLogProbMetric: 29.6898 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 628/1000
2023-09-30 03:21:59.533 
Epoch 628/1000 
	 loss: 28.6231, MinusLogProbMetric: 28.6231, val_loss: 29.0822, val_MinusLogProbMetric: 29.0822

Epoch 628: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6231 - MinusLogProbMetric: 28.6231 - val_loss: 29.0822 - val_MinusLogProbMetric: 29.0822 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 629/1000
2023-09-30 03:22:28.105 
Epoch 629/1000 
	 loss: 28.6106, MinusLogProbMetric: 28.6106, val_loss: 28.8708, val_MinusLogProbMetric: 28.8708

Epoch 629: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6106 - MinusLogProbMetric: 28.6106 - val_loss: 28.8708 - val_MinusLogProbMetric: 28.8708 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 630/1000
2023-09-30 03:22:58.526 
Epoch 630/1000 
	 loss: 28.6506, MinusLogProbMetric: 28.6506, val_loss: 29.2726, val_MinusLogProbMetric: 29.2726

Epoch 630: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6506 - MinusLogProbMetric: 28.6506 - val_loss: 29.2726 - val_MinusLogProbMetric: 29.2726 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 631/1000
2023-09-30 03:23:28.405 
Epoch 631/1000 
	 loss: 28.6338, MinusLogProbMetric: 28.6338, val_loss: 29.4399, val_MinusLogProbMetric: 29.4399

Epoch 631: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.6338 - MinusLogProbMetric: 28.6338 - val_loss: 29.4399 - val_MinusLogProbMetric: 29.4399 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 632/1000
2023-09-30 03:23:57.819 
Epoch 632/1000 
	 loss: 28.6435, MinusLogProbMetric: 28.6435, val_loss: 29.2749, val_MinusLogProbMetric: 29.2749

Epoch 632: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6435 - MinusLogProbMetric: 28.6435 - val_loss: 29.2749 - val_MinusLogProbMetric: 29.2749 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 633/1000
2023-09-30 03:24:26.029 
Epoch 633/1000 
	 loss: 28.7051, MinusLogProbMetric: 28.7051, val_loss: 29.0577, val_MinusLogProbMetric: 29.0577

Epoch 633: val_loss did not improve from 28.87007
196/196 - 28s - loss: 28.7051 - MinusLogProbMetric: 28.7051 - val_loss: 29.0577 - val_MinusLogProbMetric: 29.0577 - lr: 5.0000e-04 - 28s/epoch - 144ms/step
Epoch 634/1000
2023-09-30 03:24:56.007 
Epoch 634/1000 
	 loss: 28.5971, MinusLogProbMetric: 28.5971, val_loss: 29.0119, val_MinusLogProbMetric: 29.0119

Epoch 634: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.5971 - MinusLogProbMetric: 28.5971 - val_loss: 29.0119 - val_MinusLogProbMetric: 29.0119 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 635/1000
2023-09-30 03:25:25.504 
Epoch 635/1000 
	 loss: 28.6827, MinusLogProbMetric: 28.6827, val_loss: 29.0756, val_MinusLogProbMetric: 29.0756

Epoch 635: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6827 - MinusLogProbMetric: 28.6827 - val_loss: 29.0756 - val_MinusLogProbMetric: 29.0756 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 636/1000
2023-09-30 03:25:54.996 
Epoch 636/1000 
	 loss: 28.6900, MinusLogProbMetric: 28.6900, val_loss: 29.0551, val_MinusLogProbMetric: 29.0551

Epoch 636: val_loss did not improve from 28.87007
196/196 - 29s - loss: 28.6900 - MinusLogProbMetric: 28.6900 - val_loss: 29.0551 - val_MinusLogProbMetric: 29.0551 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 637/1000
2023-09-30 03:26:25.216 
Epoch 637/1000 
	 loss: 28.5976, MinusLogProbMetric: 28.5976, val_loss: 28.9669, val_MinusLogProbMetric: 28.9669

Epoch 637: val_loss did not improve from 28.87007
196/196 - 30s - loss: 28.5976 - MinusLogProbMetric: 28.5976 - val_loss: 28.9669 - val_MinusLogProbMetric: 28.9669 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 638/1000
2023-09-30 03:26:55.517 
Epoch 638/1000 
	 loss: 28.3089, MinusLogProbMetric: 28.3089, val_loss: 28.7740, val_MinusLogProbMetric: 28.7740

Epoch 638: val_loss improved from 28.87007 to 28.77400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 31s - loss: 28.3089 - MinusLogProbMetric: 28.3089 - val_loss: 28.7740 - val_MinusLogProbMetric: 28.7740 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 639/1000
2023-09-30 03:27:24.128 
Epoch 639/1000 
	 loss: 28.3154, MinusLogProbMetric: 28.3154, val_loss: 28.7449, val_MinusLogProbMetric: 28.7449

Epoch 639: val_loss improved from 28.77400 to 28.74485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 29s - loss: 28.3154 - MinusLogProbMetric: 28.3154 - val_loss: 28.7449 - val_MinusLogProbMetric: 28.7449 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 640/1000
2023-09-30 03:27:56.362 
Epoch 640/1000 
	 loss: 28.3465, MinusLogProbMetric: 28.3465, val_loss: 28.8209, val_MinusLogProbMetric: 28.8209

Epoch 640: val_loss did not improve from 28.74485
196/196 - 31s - loss: 28.3465 - MinusLogProbMetric: 28.3465 - val_loss: 28.8209 - val_MinusLogProbMetric: 28.8209 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 641/1000
2023-09-30 03:28:27.495 
Epoch 641/1000 
	 loss: 28.3098, MinusLogProbMetric: 28.3098, val_loss: 28.8392, val_MinusLogProbMetric: 28.8392

Epoch 641: val_loss did not improve from 28.74485
196/196 - 31s - loss: 28.3098 - MinusLogProbMetric: 28.3098 - val_loss: 28.8392 - val_MinusLogProbMetric: 28.8392 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 642/1000
2023-09-30 03:28:57.480 
Epoch 642/1000 
	 loss: 28.3313, MinusLogProbMetric: 28.3313, val_loss: 28.8567, val_MinusLogProbMetric: 28.8567

Epoch 642: val_loss did not improve from 28.74485
196/196 - 30s - loss: 28.3313 - MinusLogProbMetric: 28.3313 - val_loss: 28.8567 - val_MinusLogProbMetric: 28.8567 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 643/1000
2023-09-30 03:29:27.024 
Epoch 643/1000 
	 loss: 28.3407, MinusLogProbMetric: 28.3407, val_loss: 28.7563, val_MinusLogProbMetric: 28.7563

Epoch 643: val_loss did not improve from 28.74485
196/196 - 30s - loss: 28.3407 - MinusLogProbMetric: 28.3407 - val_loss: 28.7563 - val_MinusLogProbMetric: 28.7563 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 644/1000
2023-09-30 03:29:58.669 
Epoch 644/1000 
	 loss: 28.3048, MinusLogProbMetric: 28.3048, val_loss: 28.8487, val_MinusLogProbMetric: 28.8487

Epoch 644: val_loss did not improve from 28.74485
196/196 - 32s - loss: 28.3048 - MinusLogProbMetric: 28.3048 - val_loss: 28.8487 - val_MinusLogProbMetric: 28.8487 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 645/1000
2023-09-30 03:30:29.450 
Epoch 645/1000 
	 loss: 28.2956, MinusLogProbMetric: 28.2956, val_loss: 28.7564, val_MinusLogProbMetric: 28.7564

Epoch 645: val_loss did not improve from 28.74485
196/196 - 31s - loss: 28.2956 - MinusLogProbMetric: 28.2956 - val_loss: 28.7564 - val_MinusLogProbMetric: 28.7564 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 646/1000
2023-09-30 03:30:58.399 
Epoch 646/1000 
	 loss: 28.3371, MinusLogProbMetric: 28.3371, val_loss: 28.8060, val_MinusLogProbMetric: 28.8060

Epoch 646: val_loss did not improve from 28.74485
196/196 - 29s - loss: 28.3371 - MinusLogProbMetric: 28.3371 - val_loss: 28.8060 - val_MinusLogProbMetric: 28.8060 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 647/1000
2023-09-30 03:31:30.324 
Epoch 647/1000 
	 loss: 28.2984, MinusLogProbMetric: 28.2984, val_loss: 29.1616, val_MinusLogProbMetric: 29.1616

Epoch 647: val_loss did not improve from 28.74485
196/196 - 32s - loss: 28.2984 - MinusLogProbMetric: 28.2984 - val_loss: 29.1616 - val_MinusLogProbMetric: 29.1616 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 648/1000
2023-09-30 03:32:00.181 
Epoch 648/1000 
	 loss: 28.3022, MinusLogProbMetric: 28.3022, val_loss: 28.8233, val_MinusLogProbMetric: 28.8233

Epoch 648: val_loss did not improve from 28.74485
196/196 - 30s - loss: 28.3022 - MinusLogProbMetric: 28.3022 - val_loss: 28.8233 - val_MinusLogProbMetric: 28.8233 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 649/1000
2023-09-30 03:32:28.399 
Epoch 649/1000 
	 loss: 28.3309, MinusLogProbMetric: 28.3309, val_loss: 28.7559, val_MinusLogProbMetric: 28.7559

Epoch 649: val_loss did not improve from 28.74485
196/196 - 28s - loss: 28.3309 - MinusLogProbMetric: 28.3309 - val_loss: 28.7559 - val_MinusLogProbMetric: 28.7559 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 650/1000
2023-09-30 03:32:56.829 
Epoch 650/1000 
	 loss: 28.3030, MinusLogProbMetric: 28.3030, val_loss: 28.7915, val_MinusLogProbMetric: 28.7915

Epoch 650: val_loss did not improve from 28.74485
196/196 - 28s - loss: 28.3030 - MinusLogProbMetric: 28.3030 - val_loss: 28.7915 - val_MinusLogProbMetric: 28.7915 - lr: 2.5000e-04 - 28s/epoch - 145ms/step
Epoch 651/1000
2023-09-30 03:33:28.205 
Epoch 651/1000 
	 loss: 28.3161, MinusLogProbMetric: 28.3161, val_loss: 28.7571, val_MinusLogProbMetric: 28.7571

Epoch 651: val_loss did not improve from 28.74485
196/196 - 31s - loss: 28.3161 - MinusLogProbMetric: 28.3161 - val_loss: 28.7571 - val_MinusLogProbMetric: 28.7571 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 652/1000
2023-09-30 03:33:58.218 
Epoch 652/1000 
	 loss: 28.3132, MinusLogProbMetric: 28.3132, val_loss: 29.0896, val_MinusLogProbMetric: 29.0896

Epoch 652: val_loss did not improve from 28.74485
196/196 - 30s - loss: 28.3132 - MinusLogProbMetric: 28.3132 - val_loss: 29.0896 - val_MinusLogProbMetric: 29.0896 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 653/1000
2023-09-30 03:34:27.395 
Epoch 653/1000 
	 loss: 28.3048, MinusLogProbMetric: 28.3048, val_loss: 28.8947, val_MinusLogProbMetric: 28.8947

Epoch 653: val_loss did not improve from 28.74485
196/196 - 29s - loss: 28.3048 - MinusLogProbMetric: 28.3048 - val_loss: 28.8947 - val_MinusLogProbMetric: 28.8947 - lr: 2.5000e-04 - 29s/epoch - 149ms/step
Epoch 654/1000
2023-09-30 03:34:57.886 
Epoch 654/1000 
	 loss: 28.3165, MinusLogProbMetric: 28.3165, val_loss: 28.9287, val_MinusLogProbMetric: 28.9287

Epoch 654: val_loss did not improve from 28.74485
196/196 - 30s - loss: 28.3165 - MinusLogProbMetric: 28.3165 - val_loss: 28.9287 - val_MinusLogProbMetric: 28.9287 - lr: 2.5000e-04 - 30s/epoch - 156ms/step
Epoch 655/1000
2023-09-30 03:35:25.764 
Epoch 655/1000 
	 loss: 28.2712, MinusLogProbMetric: 28.2712, val_loss: 28.7832, val_MinusLogProbMetric: 28.7832

Epoch 655: val_loss did not improve from 28.74485
196/196 - 28s - loss: 28.2712 - MinusLogProbMetric: 28.2712 - val_loss: 28.7832 - val_MinusLogProbMetric: 28.7832 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 656/1000
2023-09-30 03:35:56.099 
Epoch 656/1000 
	 loss: 28.3426, MinusLogProbMetric: 28.3426, val_loss: 28.7579, val_MinusLogProbMetric: 28.7579

Epoch 656: val_loss did not improve from 28.74485
196/196 - 30s - loss: 28.3426 - MinusLogProbMetric: 28.3426 - val_loss: 28.7579 - val_MinusLogProbMetric: 28.7579 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 657/1000
2023-09-30 03:36:28.677 
Epoch 657/1000 
	 loss: 28.2974, MinusLogProbMetric: 28.2974, val_loss: 28.9209, val_MinusLogProbMetric: 28.9209

Epoch 657: val_loss did not improve from 28.74485
196/196 - 33s - loss: 28.2974 - MinusLogProbMetric: 28.2974 - val_loss: 28.9209 - val_MinusLogProbMetric: 28.9209 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 658/1000
2023-09-30 03:36:58.418 
Epoch 658/1000 
	 loss: 28.2891, MinusLogProbMetric: 28.2891, val_loss: 28.8108, val_MinusLogProbMetric: 28.8108

Epoch 658: val_loss did not improve from 28.74485
196/196 - 30s - loss: 28.2891 - MinusLogProbMetric: 28.2891 - val_loss: 28.8108 - val_MinusLogProbMetric: 28.8108 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 659/1000
2023-09-30 03:37:25.651 
Epoch 659/1000 
	 loss: 28.3000, MinusLogProbMetric: 28.3000, val_loss: 28.7627, val_MinusLogProbMetric: 28.7627

Epoch 659: val_loss did not improve from 28.74485
196/196 - 27s - loss: 28.3000 - MinusLogProbMetric: 28.3000 - val_loss: 28.7627 - val_MinusLogProbMetric: 28.7627 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 660/1000
2023-09-30 03:37:54.202 
Epoch 660/1000 
	 loss: 28.3074, MinusLogProbMetric: 28.3074, val_loss: 29.3294, val_MinusLogProbMetric: 29.3294

Epoch 660: val_loss did not improve from 28.74485
196/196 - 29s - loss: 28.3074 - MinusLogProbMetric: 28.3074 - val_loss: 29.3294 - val_MinusLogProbMetric: 29.3294 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 661/1000
2023-09-30 03:38:22.261 
Epoch 661/1000 
	 loss: 28.3068, MinusLogProbMetric: 28.3068, val_loss: 28.8947, val_MinusLogProbMetric: 28.8947

Epoch 661: val_loss did not improve from 28.74485
196/196 - 28s - loss: 28.3068 - MinusLogProbMetric: 28.3068 - val_loss: 28.8947 - val_MinusLogProbMetric: 28.8947 - lr: 2.5000e-04 - 28s/epoch - 143ms/step
Epoch 662/1000
2023-09-30 03:38:52.797 
Epoch 662/1000 
	 loss: 28.3307, MinusLogProbMetric: 28.3307, val_loss: 28.7298, val_MinusLogProbMetric: 28.7298

Epoch 662: val_loss improved from 28.74485 to 28.72975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 31s - loss: 28.3307 - MinusLogProbMetric: 28.3307 - val_loss: 28.7298 - val_MinusLogProbMetric: 28.7298 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 663/1000
2023-09-30 03:39:23.182 
Epoch 663/1000 
	 loss: 28.3376, MinusLogProbMetric: 28.3376, val_loss: 28.8950, val_MinusLogProbMetric: 28.8950

Epoch 663: val_loss did not improve from 28.72975
196/196 - 30s - loss: 28.3376 - MinusLogProbMetric: 28.3376 - val_loss: 28.8950 - val_MinusLogProbMetric: 28.8950 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 664/1000
2023-09-30 03:39:53.934 
Epoch 664/1000 
	 loss: 28.3167, MinusLogProbMetric: 28.3167, val_loss: 28.7394, val_MinusLogProbMetric: 28.7394

Epoch 664: val_loss did not improve from 28.72975
196/196 - 31s - loss: 28.3167 - MinusLogProbMetric: 28.3167 - val_loss: 28.7394 - val_MinusLogProbMetric: 28.7394 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 665/1000
2023-09-30 03:40:23.580 
Epoch 665/1000 
	 loss: 28.2828, MinusLogProbMetric: 28.2828, val_loss: 28.8963, val_MinusLogProbMetric: 28.8963

Epoch 665: val_loss did not improve from 28.72975
196/196 - 30s - loss: 28.2828 - MinusLogProbMetric: 28.2828 - val_loss: 28.8963 - val_MinusLogProbMetric: 28.8963 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 666/1000
2023-09-30 03:40:55.345 
Epoch 666/1000 
	 loss: 28.3168, MinusLogProbMetric: 28.3168, val_loss: 28.9240, val_MinusLogProbMetric: 28.9240

Epoch 666: val_loss did not improve from 28.72975
196/196 - 32s - loss: 28.3168 - MinusLogProbMetric: 28.3168 - val_loss: 28.9240 - val_MinusLogProbMetric: 28.9240 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 667/1000
2023-09-30 03:41:23.601 
Epoch 667/1000 
	 loss: 28.3096, MinusLogProbMetric: 28.3096, val_loss: 28.8947, val_MinusLogProbMetric: 28.8947

Epoch 667: val_loss did not improve from 28.72975
196/196 - 28s - loss: 28.3096 - MinusLogProbMetric: 28.3096 - val_loss: 28.8947 - val_MinusLogProbMetric: 28.8947 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 668/1000
2023-09-30 03:41:52.693 
Epoch 668/1000 
	 loss: 28.3213, MinusLogProbMetric: 28.3213, val_loss: 28.8039, val_MinusLogProbMetric: 28.8039

Epoch 668: val_loss did not improve from 28.72975
196/196 - 29s - loss: 28.3213 - MinusLogProbMetric: 28.3213 - val_loss: 28.8039 - val_MinusLogProbMetric: 28.8039 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 669/1000
2023-09-30 03:42:21.847 
Epoch 669/1000 
	 loss: 28.3211, MinusLogProbMetric: 28.3211, val_loss: 28.7155, val_MinusLogProbMetric: 28.7155

Epoch 669: val_loss improved from 28.72975 to 28.71547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 30s - loss: 28.3211 - MinusLogProbMetric: 28.3211 - val_loss: 28.7155 - val_MinusLogProbMetric: 28.7155 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 670/1000
2023-09-30 03:42:56.904 
Epoch 670/1000 
	 loss: 28.3448, MinusLogProbMetric: 28.3448, val_loss: 28.7345, val_MinusLogProbMetric: 28.7345

Epoch 670: val_loss did not improve from 28.71547
196/196 - 34s - loss: 28.3448 - MinusLogProbMetric: 28.3448 - val_loss: 28.7345 - val_MinusLogProbMetric: 28.7345 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 671/1000
2023-09-30 03:43:31.878 
Epoch 671/1000 
	 loss: 28.3121, MinusLogProbMetric: 28.3121, val_loss: 28.9574, val_MinusLogProbMetric: 28.9574

Epoch 671: val_loss did not improve from 28.71547
196/196 - 35s - loss: 28.3121 - MinusLogProbMetric: 28.3121 - val_loss: 28.9574 - val_MinusLogProbMetric: 28.9574 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 672/1000
2023-09-30 03:44:06.405 
Epoch 672/1000 
	 loss: 28.3494, MinusLogProbMetric: 28.3494, val_loss: 29.1970, val_MinusLogProbMetric: 29.1970

Epoch 672: val_loss did not improve from 28.71547
196/196 - 35s - loss: 28.3494 - MinusLogProbMetric: 28.3494 - val_loss: 29.1970 - val_MinusLogProbMetric: 29.1970 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 673/1000
2023-09-30 03:44:41.002 
Epoch 673/1000 
	 loss: 28.3014, MinusLogProbMetric: 28.3014, val_loss: 28.8170, val_MinusLogProbMetric: 28.8170

Epoch 673: val_loss did not improve from 28.71547
196/196 - 35s - loss: 28.3014 - MinusLogProbMetric: 28.3014 - val_loss: 28.8170 - val_MinusLogProbMetric: 28.8170 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 674/1000
2023-09-30 03:45:15.120 
Epoch 674/1000 
	 loss: 28.3297, MinusLogProbMetric: 28.3297, val_loss: 28.8612, val_MinusLogProbMetric: 28.8612

Epoch 674: val_loss did not improve from 28.71547
196/196 - 34s - loss: 28.3297 - MinusLogProbMetric: 28.3297 - val_loss: 28.8612 - val_MinusLogProbMetric: 28.8612 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 675/1000
2023-09-30 03:45:47.822 
Epoch 675/1000 
	 loss: 28.2862, MinusLogProbMetric: 28.2862, val_loss: 28.7253, val_MinusLogProbMetric: 28.7253

Epoch 675: val_loss did not improve from 28.71547
196/196 - 33s - loss: 28.2862 - MinusLogProbMetric: 28.2862 - val_loss: 28.7253 - val_MinusLogProbMetric: 28.7253 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 676/1000
2023-09-30 03:46:22.449 
Epoch 676/1000 
	 loss: 28.3540, MinusLogProbMetric: 28.3540, val_loss: 28.7284, val_MinusLogProbMetric: 28.7284

Epoch 676: val_loss did not improve from 28.71547
196/196 - 35s - loss: 28.3540 - MinusLogProbMetric: 28.3540 - val_loss: 28.7284 - val_MinusLogProbMetric: 28.7284 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 677/1000
2023-09-30 03:46:57.456 
Epoch 677/1000 
	 loss: 28.2822, MinusLogProbMetric: 28.2822, val_loss: 28.8128, val_MinusLogProbMetric: 28.8128

Epoch 677: val_loss did not improve from 28.71547
196/196 - 35s - loss: 28.2822 - MinusLogProbMetric: 28.2822 - val_loss: 28.8128 - val_MinusLogProbMetric: 28.8128 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 678/1000
2023-09-30 03:47:29.250 
Epoch 678/1000 
	 loss: 28.2826, MinusLogProbMetric: 28.2826, val_loss: 28.7146, val_MinusLogProbMetric: 28.7146

Epoch 678: val_loss improved from 28.71547 to 28.71455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 32s - loss: 28.2826 - MinusLogProbMetric: 28.2826 - val_loss: 28.7146 - val_MinusLogProbMetric: 28.7146 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 679/1000
2023-09-30 03:48:03.605 
Epoch 679/1000 
	 loss: 28.2966, MinusLogProbMetric: 28.2966, val_loss: 28.9707, val_MinusLogProbMetric: 28.9707

Epoch 679: val_loss did not improve from 28.71455
196/196 - 34s - loss: 28.2966 - MinusLogProbMetric: 28.2966 - val_loss: 28.9707 - val_MinusLogProbMetric: 28.9707 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 680/1000
2023-09-30 03:48:35.791 
Epoch 680/1000 
	 loss: 28.2680, MinusLogProbMetric: 28.2680, val_loss: 28.8578, val_MinusLogProbMetric: 28.8578

Epoch 680: val_loss did not improve from 28.71455
196/196 - 32s - loss: 28.2680 - MinusLogProbMetric: 28.2680 - val_loss: 28.8578 - val_MinusLogProbMetric: 28.8578 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 681/1000
2023-09-30 03:49:07.540 
Epoch 681/1000 
	 loss: 28.2844, MinusLogProbMetric: 28.2844, val_loss: 28.7684, val_MinusLogProbMetric: 28.7684

Epoch 681: val_loss did not improve from 28.71455
196/196 - 32s - loss: 28.2844 - MinusLogProbMetric: 28.2844 - val_loss: 28.7684 - val_MinusLogProbMetric: 28.7684 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 682/1000
2023-09-30 03:49:42.047 
Epoch 682/1000 
	 loss: 28.2942, MinusLogProbMetric: 28.2942, val_loss: 28.7613, val_MinusLogProbMetric: 28.7613

Epoch 682: val_loss did not improve from 28.71455
196/196 - 35s - loss: 28.2942 - MinusLogProbMetric: 28.2942 - val_loss: 28.7613 - val_MinusLogProbMetric: 28.7613 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 683/1000
2023-09-30 03:50:16.580 
Epoch 683/1000 
	 loss: 28.2968, MinusLogProbMetric: 28.2968, val_loss: 28.7663, val_MinusLogProbMetric: 28.7663

Epoch 683: val_loss did not improve from 28.71455
196/196 - 35s - loss: 28.2968 - MinusLogProbMetric: 28.2968 - val_loss: 28.7663 - val_MinusLogProbMetric: 28.7663 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 684/1000
2023-09-30 03:50:51.317 
Epoch 684/1000 
	 loss: 28.2792, MinusLogProbMetric: 28.2792, val_loss: 28.8117, val_MinusLogProbMetric: 28.8117

Epoch 684: val_loss did not improve from 28.71455
196/196 - 35s - loss: 28.2792 - MinusLogProbMetric: 28.2792 - val_loss: 28.8117 - val_MinusLogProbMetric: 28.8117 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 685/1000
2023-09-30 03:51:22.724 
Epoch 685/1000 
	 loss: 28.3157, MinusLogProbMetric: 28.3157, val_loss: 28.7124, val_MinusLogProbMetric: 28.7124

Epoch 685: val_loss improved from 28.71455 to 28.71242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 32s - loss: 28.3157 - MinusLogProbMetric: 28.3157 - val_loss: 28.7124 - val_MinusLogProbMetric: 28.7124 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 686/1000
2023-09-30 03:51:57.872 
Epoch 686/1000 
	 loss: 28.2950, MinusLogProbMetric: 28.2950, val_loss: 28.6641, val_MinusLogProbMetric: 28.6641

Epoch 686: val_loss improved from 28.71242 to 28.66409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.2950 - MinusLogProbMetric: 28.2950 - val_loss: 28.6641 - val_MinusLogProbMetric: 28.6641 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 687/1000
2023-09-30 03:52:32.978 
Epoch 687/1000 
	 loss: 28.2786, MinusLogProbMetric: 28.2786, val_loss: 28.6956, val_MinusLogProbMetric: 28.6956

Epoch 687: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2786 - MinusLogProbMetric: 28.2786 - val_loss: 28.6956 - val_MinusLogProbMetric: 28.6956 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 688/1000
2023-09-30 03:53:07.568 
Epoch 688/1000 
	 loss: 28.2843, MinusLogProbMetric: 28.2843, val_loss: 28.7228, val_MinusLogProbMetric: 28.7228

Epoch 688: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2843 - MinusLogProbMetric: 28.2843 - val_loss: 28.7228 - val_MinusLogProbMetric: 28.7228 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 689/1000
2023-09-30 03:53:41.863 
Epoch 689/1000 
	 loss: 28.3021, MinusLogProbMetric: 28.3021, val_loss: 28.8018, val_MinusLogProbMetric: 28.8018

Epoch 689: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.3021 - MinusLogProbMetric: 28.3021 - val_loss: 28.8018 - val_MinusLogProbMetric: 28.8018 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 690/1000
2023-09-30 03:54:15.224 
Epoch 690/1000 
	 loss: 28.2817, MinusLogProbMetric: 28.2817, val_loss: 28.9429, val_MinusLogProbMetric: 28.9429

Epoch 690: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2817 - MinusLogProbMetric: 28.2817 - val_loss: 28.9429 - val_MinusLogProbMetric: 28.9429 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 691/1000
2023-09-30 03:54:48.105 
Epoch 691/1000 
	 loss: 28.3096, MinusLogProbMetric: 28.3096, val_loss: 29.6929, val_MinusLogProbMetric: 29.6929

Epoch 691: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.3096 - MinusLogProbMetric: 28.3096 - val_loss: 29.6929 - val_MinusLogProbMetric: 29.6929 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 692/1000
2023-09-30 03:55:19.397 
Epoch 692/1000 
	 loss: 28.3362, MinusLogProbMetric: 28.3362, val_loss: 28.8378, val_MinusLogProbMetric: 28.8378

Epoch 692: val_loss did not improve from 28.66409
196/196 - 31s - loss: 28.3362 - MinusLogProbMetric: 28.3362 - val_loss: 28.8378 - val_MinusLogProbMetric: 28.8378 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 693/1000
2023-09-30 03:55:51.926 
Epoch 693/1000 
	 loss: 28.2839, MinusLogProbMetric: 28.2839, val_loss: 28.9324, val_MinusLogProbMetric: 28.9324

Epoch 693: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2839 - MinusLogProbMetric: 28.2839 - val_loss: 28.9324 - val_MinusLogProbMetric: 28.9324 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 694/1000
2023-09-30 03:56:25.222 
Epoch 694/1000 
	 loss: 28.3052, MinusLogProbMetric: 28.3052, val_loss: 28.8788, val_MinusLogProbMetric: 28.8788

Epoch 694: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.3052 - MinusLogProbMetric: 28.3052 - val_loss: 28.8788 - val_MinusLogProbMetric: 28.8788 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 695/1000
2023-09-30 03:56:59.336 
Epoch 695/1000 
	 loss: 28.2910, MinusLogProbMetric: 28.2910, val_loss: 28.7651, val_MinusLogProbMetric: 28.7651

Epoch 695: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2910 - MinusLogProbMetric: 28.2910 - val_loss: 28.7651 - val_MinusLogProbMetric: 28.7651 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 696/1000
2023-09-30 03:57:33.829 
Epoch 696/1000 
	 loss: 28.3223, MinusLogProbMetric: 28.3223, val_loss: 28.7537, val_MinusLogProbMetric: 28.7537

Epoch 696: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.3223 - MinusLogProbMetric: 28.3223 - val_loss: 28.7537 - val_MinusLogProbMetric: 28.7537 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 697/1000
2023-09-30 03:58:08.239 
Epoch 697/1000 
	 loss: 28.2962, MinusLogProbMetric: 28.2962, val_loss: 28.8204, val_MinusLogProbMetric: 28.8204

Epoch 697: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2962 - MinusLogProbMetric: 28.2962 - val_loss: 28.8204 - val_MinusLogProbMetric: 28.8204 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 698/1000
2023-09-30 03:58:42.575 
Epoch 698/1000 
	 loss: 28.2680, MinusLogProbMetric: 28.2680, val_loss: 28.8348, val_MinusLogProbMetric: 28.8348

Epoch 698: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2680 - MinusLogProbMetric: 28.2680 - val_loss: 28.8348 - val_MinusLogProbMetric: 28.8348 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 699/1000
2023-09-30 03:59:16.478 
Epoch 699/1000 
	 loss: 28.3377, MinusLogProbMetric: 28.3377, val_loss: 28.8454, val_MinusLogProbMetric: 28.8454

Epoch 699: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.3377 - MinusLogProbMetric: 28.3377 - val_loss: 28.8454 - val_MinusLogProbMetric: 28.8454 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 700/1000
2023-09-30 03:59:51.431 
Epoch 700/1000 
	 loss: 28.3141, MinusLogProbMetric: 28.3141, val_loss: 28.7618, val_MinusLogProbMetric: 28.7618

Epoch 700: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.3141 - MinusLogProbMetric: 28.3141 - val_loss: 28.7618 - val_MinusLogProbMetric: 28.7618 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 701/1000
2023-09-30 04:00:26.145 
Epoch 701/1000 
	 loss: 28.3188, MinusLogProbMetric: 28.3188, val_loss: 28.7859, val_MinusLogProbMetric: 28.7859

Epoch 701: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.3188 - MinusLogProbMetric: 28.3188 - val_loss: 28.7859 - val_MinusLogProbMetric: 28.7859 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 702/1000
2023-09-30 04:01:01.216 
Epoch 702/1000 
	 loss: 28.3452, MinusLogProbMetric: 28.3452, val_loss: 28.7352, val_MinusLogProbMetric: 28.7352

Epoch 702: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.3452 - MinusLogProbMetric: 28.3452 - val_loss: 28.7352 - val_MinusLogProbMetric: 28.7352 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 703/1000
2023-09-30 04:01:31.918 
Epoch 703/1000 
	 loss: 28.3033, MinusLogProbMetric: 28.3033, val_loss: 28.8770, val_MinusLogProbMetric: 28.8770

Epoch 703: val_loss did not improve from 28.66409
196/196 - 31s - loss: 28.3033 - MinusLogProbMetric: 28.3033 - val_loss: 28.8770 - val_MinusLogProbMetric: 28.8770 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 704/1000
2023-09-30 04:02:06.920 
Epoch 704/1000 
	 loss: 28.2906, MinusLogProbMetric: 28.2906, val_loss: 28.7814, val_MinusLogProbMetric: 28.7814

Epoch 704: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2906 - MinusLogProbMetric: 28.2906 - val_loss: 28.7814 - val_MinusLogProbMetric: 28.7814 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 705/1000
2023-09-30 04:02:41.126 
Epoch 705/1000 
	 loss: 28.3161, MinusLogProbMetric: 28.3161, val_loss: 28.7093, val_MinusLogProbMetric: 28.7093

Epoch 705: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.3161 - MinusLogProbMetric: 28.3161 - val_loss: 28.7093 - val_MinusLogProbMetric: 28.7093 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 706/1000
2023-09-30 04:03:15.820 
Epoch 706/1000 
	 loss: 28.3135, MinusLogProbMetric: 28.3135, val_loss: 29.0364, val_MinusLogProbMetric: 29.0364

Epoch 706: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.3135 - MinusLogProbMetric: 28.3135 - val_loss: 29.0364 - val_MinusLogProbMetric: 29.0364 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 707/1000
2023-09-30 04:03:49.204 
Epoch 707/1000 
	 loss: 28.3008, MinusLogProbMetric: 28.3008, val_loss: 28.8107, val_MinusLogProbMetric: 28.8107

Epoch 707: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.3008 - MinusLogProbMetric: 28.3008 - val_loss: 28.8107 - val_MinusLogProbMetric: 28.8107 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 708/1000
2023-09-30 04:04:21.330 
Epoch 708/1000 
	 loss: 28.3399, MinusLogProbMetric: 28.3399, val_loss: 28.8556, val_MinusLogProbMetric: 28.8556

Epoch 708: val_loss did not improve from 28.66409
196/196 - 32s - loss: 28.3399 - MinusLogProbMetric: 28.3399 - val_loss: 28.8556 - val_MinusLogProbMetric: 28.8556 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 709/1000
2023-09-30 04:04:54.378 
Epoch 709/1000 
	 loss: 28.2930, MinusLogProbMetric: 28.2930, val_loss: 28.7584, val_MinusLogProbMetric: 28.7584

Epoch 709: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2930 - MinusLogProbMetric: 28.2930 - val_loss: 28.7584 - val_MinusLogProbMetric: 28.7584 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 710/1000
2023-09-30 04:05:28.186 
Epoch 710/1000 
	 loss: 28.3176, MinusLogProbMetric: 28.3176, val_loss: 28.8533, val_MinusLogProbMetric: 28.8533

Epoch 710: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.3176 - MinusLogProbMetric: 28.3176 - val_loss: 28.8533 - val_MinusLogProbMetric: 28.8533 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 711/1000
2023-09-30 04:06:02.645 
Epoch 711/1000 
	 loss: 28.2859, MinusLogProbMetric: 28.2859, val_loss: 28.8823, val_MinusLogProbMetric: 28.8823

Epoch 711: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2859 - MinusLogProbMetric: 28.2859 - val_loss: 28.8823 - val_MinusLogProbMetric: 28.8823 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 712/1000
2023-09-30 04:06:36.215 
Epoch 712/1000 
	 loss: 28.2971, MinusLogProbMetric: 28.2971, val_loss: 28.7346, val_MinusLogProbMetric: 28.7346

Epoch 712: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2971 - MinusLogProbMetric: 28.2971 - val_loss: 28.7346 - val_MinusLogProbMetric: 28.7346 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 713/1000
2023-09-30 04:07:10.783 
Epoch 713/1000 
	 loss: 28.2602, MinusLogProbMetric: 28.2602, val_loss: 28.7294, val_MinusLogProbMetric: 28.7294

Epoch 713: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2602 - MinusLogProbMetric: 28.2602 - val_loss: 28.7294 - val_MinusLogProbMetric: 28.7294 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 714/1000
2023-09-30 04:07:44.561 
Epoch 714/1000 
	 loss: 28.2888, MinusLogProbMetric: 28.2888, val_loss: 28.7744, val_MinusLogProbMetric: 28.7744

Epoch 714: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2888 - MinusLogProbMetric: 28.2888 - val_loss: 28.7744 - val_MinusLogProbMetric: 28.7744 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 715/1000
2023-09-30 04:08:17.584 
Epoch 715/1000 
	 loss: 28.2869, MinusLogProbMetric: 28.2869, val_loss: 28.8493, val_MinusLogProbMetric: 28.8493

Epoch 715: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2869 - MinusLogProbMetric: 28.2869 - val_loss: 28.8493 - val_MinusLogProbMetric: 28.8493 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 716/1000
2023-09-30 04:08:51.538 
Epoch 716/1000 
	 loss: 28.2655, MinusLogProbMetric: 28.2655, val_loss: 28.7463, val_MinusLogProbMetric: 28.7463

Epoch 716: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2655 - MinusLogProbMetric: 28.2655 - val_loss: 28.7463 - val_MinusLogProbMetric: 28.7463 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 717/1000
2023-09-30 04:09:26.187 
Epoch 717/1000 
	 loss: 28.2656, MinusLogProbMetric: 28.2656, val_loss: 28.8595, val_MinusLogProbMetric: 28.8595

Epoch 717: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2656 - MinusLogProbMetric: 28.2656 - val_loss: 28.8595 - val_MinusLogProbMetric: 28.8595 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 718/1000
2023-09-30 04:10:00.560 
Epoch 718/1000 
	 loss: 28.3048, MinusLogProbMetric: 28.3048, val_loss: 29.1316, val_MinusLogProbMetric: 29.1316

Epoch 718: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.3048 - MinusLogProbMetric: 28.3048 - val_loss: 29.1316 - val_MinusLogProbMetric: 29.1316 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 719/1000
2023-09-30 04:10:35.150 
Epoch 719/1000 
	 loss: 28.2893, MinusLogProbMetric: 28.2893, val_loss: 28.7691, val_MinusLogProbMetric: 28.7691

Epoch 719: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2893 - MinusLogProbMetric: 28.2893 - val_loss: 28.7691 - val_MinusLogProbMetric: 28.7691 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 720/1000
2023-09-30 04:11:09.744 
Epoch 720/1000 
	 loss: 28.2705, MinusLogProbMetric: 28.2705, val_loss: 28.8371, val_MinusLogProbMetric: 28.8371

Epoch 720: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2705 - MinusLogProbMetric: 28.2705 - val_loss: 28.8371 - val_MinusLogProbMetric: 28.8371 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 721/1000
2023-09-30 04:11:44.212 
Epoch 721/1000 
	 loss: 28.2955, MinusLogProbMetric: 28.2955, val_loss: 29.0289, val_MinusLogProbMetric: 29.0289

Epoch 721: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2955 - MinusLogProbMetric: 28.2955 - val_loss: 29.0289 - val_MinusLogProbMetric: 29.0289 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 722/1000
2023-09-30 04:12:17.386 
Epoch 722/1000 
	 loss: 28.2966, MinusLogProbMetric: 28.2966, val_loss: 28.8331, val_MinusLogProbMetric: 28.8331

Epoch 722: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2966 - MinusLogProbMetric: 28.2966 - val_loss: 28.8331 - val_MinusLogProbMetric: 28.8331 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 723/1000
2023-09-30 04:12:50.710 
Epoch 723/1000 
	 loss: 28.2855, MinusLogProbMetric: 28.2855, val_loss: 28.7761, val_MinusLogProbMetric: 28.7761

Epoch 723: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2855 - MinusLogProbMetric: 28.2855 - val_loss: 28.7761 - val_MinusLogProbMetric: 28.7761 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 724/1000
2023-09-30 04:13:23.956 
Epoch 724/1000 
	 loss: 28.2758, MinusLogProbMetric: 28.2758, val_loss: 28.8913, val_MinusLogProbMetric: 28.8913

Epoch 724: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2758 - MinusLogProbMetric: 28.2758 - val_loss: 28.8913 - val_MinusLogProbMetric: 28.8913 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 725/1000
2023-09-30 04:13:58.935 
Epoch 725/1000 
	 loss: 28.2708, MinusLogProbMetric: 28.2708, val_loss: 28.7284, val_MinusLogProbMetric: 28.7284

Epoch 725: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2708 - MinusLogProbMetric: 28.2708 - val_loss: 28.7284 - val_MinusLogProbMetric: 28.7284 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 726/1000
2023-09-30 04:14:33.405 
Epoch 726/1000 
	 loss: 28.2805, MinusLogProbMetric: 28.2805, val_loss: 28.7753, val_MinusLogProbMetric: 28.7753

Epoch 726: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2805 - MinusLogProbMetric: 28.2805 - val_loss: 28.7753 - val_MinusLogProbMetric: 28.7753 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 727/1000
2023-09-30 04:15:07.395 
Epoch 727/1000 
	 loss: 28.2665, MinusLogProbMetric: 28.2665, val_loss: 28.9257, val_MinusLogProbMetric: 28.9257

Epoch 727: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2665 - MinusLogProbMetric: 28.2665 - val_loss: 28.9257 - val_MinusLogProbMetric: 28.9257 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 728/1000
2023-09-30 04:15:40.028 
Epoch 728/1000 
	 loss: 28.2981, MinusLogProbMetric: 28.2981, val_loss: 28.6836, val_MinusLogProbMetric: 28.6836

Epoch 728: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2981 - MinusLogProbMetric: 28.2981 - val_loss: 28.6836 - val_MinusLogProbMetric: 28.6836 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 729/1000
2023-09-30 04:16:13.400 
Epoch 729/1000 
	 loss: 28.2701, MinusLogProbMetric: 28.2701, val_loss: 28.7526, val_MinusLogProbMetric: 28.7526

Epoch 729: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2701 - MinusLogProbMetric: 28.2701 - val_loss: 28.7526 - val_MinusLogProbMetric: 28.7526 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 730/1000
2023-09-30 04:16:48.158 
Epoch 730/1000 
	 loss: 28.3149, MinusLogProbMetric: 28.3149, val_loss: 28.7450, val_MinusLogProbMetric: 28.7450

Epoch 730: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.3149 - MinusLogProbMetric: 28.3149 - val_loss: 28.7450 - val_MinusLogProbMetric: 28.7450 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 731/1000
2023-09-30 04:17:22.383 
Epoch 731/1000 
	 loss: 28.2854, MinusLogProbMetric: 28.2854, val_loss: 28.7396, val_MinusLogProbMetric: 28.7396

Epoch 731: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2854 - MinusLogProbMetric: 28.2854 - val_loss: 28.7396 - val_MinusLogProbMetric: 28.7396 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 732/1000
2023-09-30 04:17:55.479 
Epoch 732/1000 
	 loss: 28.2654, MinusLogProbMetric: 28.2654, val_loss: 28.7482, val_MinusLogProbMetric: 28.7482

Epoch 732: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2654 - MinusLogProbMetric: 28.2654 - val_loss: 28.7482 - val_MinusLogProbMetric: 28.7482 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 733/1000
2023-09-30 04:18:28.635 
Epoch 733/1000 
	 loss: 28.2921, MinusLogProbMetric: 28.2921, val_loss: 28.7925, val_MinusLogProbMetric: 28.7925

Epoch 733: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.2921 - MinusLogProbMetric: 28.2921 - val_loss: 28.7925 - val_MinusLogProbMetric: 28.7925 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 734/1000
2023-09-30 04:19:01.052 
Epoch 734/1000 
	 loss: 28.2924, MinusLogProbMetric: 28.2924, val_loss: 28.7744, val_MinusLogProbMetric: 28.7744

Epoch 734: val_loss did not improve from 28.66409
196/196 - 32s - loss: 28.2924 - MinusLogProbMetric: 28.2924 - val_loss: 28.7744 - val_MinusLogProbMetric: 28.7744 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 735/1000
2023-09-30 04:19:34.570 
Epoch 735/1000 
	 loss: 28.2957, MinusLogProbMetric: 28.2957, val_loss: 28.8512, val_MinusLogProbMetric: 28.8512

Epoch 735: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.2957 - MinusLogProbMetric: 28.2957 - val_loss: 28.8512 - val_MinusLogProbMetric: 28.8512 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 736/1000
2023-09-30 04:20:09.306 
Epoch 736/1000 
	 loss: 28.2757, MinusLogProbMetric: 28.2757, val_loss: 28.7124, val_MinusLogProbMetric: 28.7124

Epoch 736: val_loss did not improve from 28.66409
196/196 - 35s - loss: 28.2757 - MinusLogProbMetric: 28.2757 - val_loss: 28.7124 - val_MinusLogProbMetric: 28.7124 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 737/1000
2023-09-30 04:20:43.285 
Epoch 737/1000 
	 loss: 28.1611, MinusLogProbMetric: 28.1611, val_loss: 28.7006, val_MinusLogProbMetric: 28.7006

Epoch 737: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.1611 - MinusLogProbMetric: 28.1611 - val_loss: 28.7006 - val_MinusLogProbMetric: 28.7006 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 738/1000
2023-09-30 04:21:17.336 
Epoch 738/1000 
	 loss: 28.1723, MinusLogProbMetric: 28.1723, val_loss: 28.6768, val_MinusLogProbMetric: 28.6768

Epoch 738: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.1723 - MinusLogProbMetric: 28.1723 - val_loss: 28.6768 - val_MinusLogProbMetric: 28.6768 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 739/1000
2023-09-30 04:21:51.356 
Epoch 739/1000 
	 loss: 28.1636, MinusLogProbMetric: 28.1636, val_loss: 28.6753, val_MinusLogProbMetric: 28.6753

Epoch 739: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.1636 - MinusLogProbMetric: 28.1636 - val_loss: 28.6753 - val_MinusLogProbMetric: 28.6753 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 740/1000
2023-09-30 04:22:25.741 
Epoch 740/1000 
	 loss: 28.1493, MinusLogProbMetric: 28.1493, val_loss: 28.6734, val_MinusLogProbMetric: 28.6734

Epoch 740: val_loss did not improve from 28.66409
196/196 - 34s - loss: 28.1493 - MinusLogProbMetric: 28.1493 - val_loss: 28.6734 - val_MinusLogProbMetric: 28.6734 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 741/1000
2023-09-30 04:22:58.849 
Epoch 741/1000 
	 loss: 28.1631, MinusLogProbMetric: 28.1631, val_loss: 28.8447, val_MinusLogProbMetric: 28.8447

Epoch 741: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.1631 - MinusLogProbMetric: 28.1631 - val_loss: 28.8447 - val_MinusLogProbMetric: 28.8447 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 742/1000
2023-09-30 04:23:31.397 
Epoch 742/1000 
	 loss: 28.1546, MinusLogProbMetric: 28.1546, val_loss: 28.7973, val_MinusLogProbMetric: 28.7973

Epoch 742: val_loss did not improve from 28.66409
196/196 - 33s - loss: 28.1546 - MinusLogProbMetric: 28.1546 - val_loss: 28.7973 - val_MinusLogProbMetric: 28.7973 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 743/1000
2023-09-30 04:24:04.981 
Epoch 743/1000 
	 loss: 28.1535, MinusLogProbMetric: 28.1535, val_loss: 28.6571, val_MinusLogProbMetric: 28.6571

Epoch 743: val_loss improved from 28.66409 to 28.65709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 28.1535 - MinusLogProbMetric: 28.1535 - val_loss: 28.6571 - val_MinusLogProbMetric: 28.6571 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 744/1000
2023-09-30 04:24:40.824 
Epoch 744/1000 
	 loss: 28.1562, MinusLogProbMetric: 28.1562, val_loss: 28.6494, val_MinusLogProbMetric: 28.6494

Epoch 744: val_loss improved from 28.65709 to 28.64943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 28.1562 - MinusLogProbMetric: 28.1562 - val_loss: 28.6494 - val_MinusLogProbMetric: 28.6494 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 745/1000
2023-09-30 04:25:16.335 
Epoch 745/1000 
	 loss: 28.1573, MinusLogProbMetric: 28.1573, val_loss: 28.6958, val_MinusLogProbMetric: 28.6958

Epoch 745: val_loss did not improve from 28.64943
196/196 - 35s - loss: 28.1573 - MinusLogProbMetric: 28.1573 - val_loss: 28.6958 - val_MinusLogProbMetric: 28.6958 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 746/1000
2023-09-30 04:25:51.034 
Epoch 746/1000 
	 loss: 28.1501, MinusLogProbMetric: 28.1501, val_loss: 28.6905, val_MinusLogProbMetric: 28.6905

Epoch 746: val_loss did not improve from 28.64943
196/196 - 35s - loss: 28.1501 - MinusLogProbMetric: 28.1501 - val_loss: 28.6905 - val_MinusLogProbMetric: 28.6905 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 747/1000
2023-09-30 04:26:24.717 
Epoch 747/1000 
	 loss: 28.1455, MinusLogProbMetric: 28.1455, val_loss: 28.6840, val_MinusLogProbMetric: 28.6840

Epoch 747: val_loss did not improve from 28.64943
196/196 - 34s - loss: 28.1455 - MinusLogProbMetric: 28.1455 - val_loss: 28.6840 - val_MinusLogProbMetric: 28.6840 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 748/1000
2023-09-30 04:26:58.943 
Epoch 748/1000 
	 loss: 28.1374, MinusLogProbMetric: 28.1374, val_loss: 28.7232, val_MinusLogProbMetric: 28.7232

Epoch 748: val_loss did not improve from 28.64943
196/196 - 34s - loss: 28.1374 - MinusLogProbMetric: 28.1374 - val_loss: 28.7232 - val_MinusLogProbMetric: 28.7232 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 749/1000
2023-09-30 04:27:33.583 
Epoch 749/1000 
	 loss: 28.1524, MinusLogProbMetric: 28.1524, val_loss: 28.6720, val_MinusLogProbMetric: 28.6720

Epoch 749: val_loss did not improve from 28.64943
196/196 - 35s - loss: 28.1524 - MinusLogProbMetric: 28.1524 - val_loss: 28.6720 - val_MinusLogProbMetric: 28.6720 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 750/1000
2023-09-30 04:28:08.171 
Epoch 750/1000 
	 loss: 28.1387, MinusLogProbMetric: 28.1387, val_loss: 28.6689, val_MinusLogProbMetric: 28.6689

Epoch 750: val_loss did not improve from 28.64943
196/196 - 35s - loss: 28.1387 - MinusLogProbMetric: 28.1387 - val_loss: 28.6689 - val_MinusLogProbMetric: 28.6689 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 751/1000
2023-09-30 04:28:42.855 
Epoch 751/1000 
	 loss: 28.1404, MinusLogProbMetric: 28.1404, val_loss: 28.7539, val_MinusLogProbMetric: 28.7539

Epoch 751: val_loss did not improve from 28.64943
196/196 - 35s - loss: 28.1404 - MinusLogProbMetric: 28.1404 - val_loss: 28.7539 - val_MinusLogProbMetric: 28.7539 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 752/1000
2023-09-30 04:29:17.451 
Epoch 752/1000 
	 loss: 28.1350, MinusLogProbMetric: 28.1350, val_loss: 28.7062, val_MinusLogProbMetric: 28.7062

Epoch 752: val_loss did not improve from 28.64943
196/196 - 35s - loss: 28.1350 - MinusLogProbMetric: 28.1350 - val_loss: 28.7062 - val_MinusLogProbMetric: 28.7062 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 753/1000
2023-09-30 04:29:51.991 
Epoch 753/1000 
	 loss: 28.1451, MinusLogProbMetric: 28.1451, val_loss: 28.8931, val_MinusLogProbMetric: 28.8931

Epoch 753: val_loss did not improve from 28.64943
196/196 - 35s - loss: 28.1451 - MinusLogProbMetric: 28.1451 - val_loss: 28.8931 - val_MinusLogProbMetric: 28.8931 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 754/1000
2023-09-30 04:30:24.675 
Epoch 754/1000 
	 loss: 28.1548, MinusLogProbMetric: 28.1548, val_loss: 28.7505, val_MinusLogProbMetric: 28.7505

Epoch 754: val_loss did not improve from 28.64943
196/196 - 33s - loss: 28.1548 - MinusLogProbMetric: 28.1548 - val_loss: 28.7505 - val_MinusLogProbMetric: 28.7505 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 755/1000
2023-09-30 04:30:57.881 
Epoch 755/1000 
	 loss: 28.1576, MinusLogProbMetric: 28.1576, val_loss: 28.6122, val_MinusLogProbMetric: 28.6122

Epoch 755: val_loss improved from 28.64943 to 28.61222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 28.1576 - MinusLogProbMetric: 28.1576 - val_loss: 28.6122 - val_MinusLogProbMetric: 28.6122 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 756/1000
2023-09-30 04:31:33.370 
Epoch 756/1000 
	 loss: 28.1674, MinusLogProbMetric: 28.1674, val_loss: 28.7378, val_MinusLogProbMetric: 28.7378

Epoch 756: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1674 - MinusLogProbMetric: 28.1674 - val_loss: 28.7378 - val_MinusLogProbMetric: 28.7378 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 757/1000
2023-09-30 04:32:07.289 
Epoch 757/1000 
	 loss: 28.1533, MinusLogProbMetric: 28.1533, val_loss: 28.6478, val_MinusLogProbMetric: 28.6478

Epoch 757: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1533 - MinusLogProbMetric: 28.1533 - val_loss: 28.6478 - val_MinusLogProbMetric: 28.6478 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 758/1000
2023-09-30 04:32:42.049 
Epoch 758/1000 
	 loss: 28.1479, MinusLogProbMetric: 28.1479, val_loss: 28.6876, val_MinusLogProbMetric: 28.6876

Epoch 758: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1479 - MinusLogProbMetric: 28.1479 - val_loss: 28.6876 - val_MinusLogProbMetric: 28.6876 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 759/1000
2023-09-30 04:33:15.226 
Epoch 759/1000 
	 loss: 28.1424, MinusLogProbMetric: 28.1424, val_loss: 28.6858, val_MinusLogProbMetric: 28.6858

Epoch 759: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1424 - MinusLogProbMetric: 28.1424 - val_loss: 28.6858 - val_MinusLogProbMetric: 28.6858 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 760/1000
2023-09-30 04:33:49.676 
Epoch 760/1000 
	 loss: 28.1581, MinusLogProbMetric: 28.1581, val_loss: 28.7970, val_MinusLogProbMetric: 28.7970

Epoch 760: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1581 - MinusLogProbMetric: 28.1581 - val_loss: 28.7970 - val_MinusLogProbMetric: 28.7970 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 761/1000
2023-09-30 04:34:23.324 
Epoch 761/1000 
	 loss: 28.1330, MinusLogProbMetric: 28.1330, val_loss: 28.6310, val_MinusLogProbMetric: 28.6310

Epoch 761: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1330 - MinusLogProbMetric: 28.1330 - val_loss: 28.6310 - val_MinusLogProbMetric: 28.6310 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 762/1000
2023-09-30 04:34:57.817 
Epoch 762/1000 
	 loss: 28.1306, MinusLogProbMetric: 28.1306, val_loss: 28.6624, val_MinusLogProbMetric: 28.6624

Epoch 762: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1306 - MinusLogProbMetric: 28.1306 - val_loss: 28.6624 - val_MinusLogProbMetric: 28.6624 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 763/1000
2023-09-30 04:35:31.841 
Epoch 763/1000 
	 loss: 28.1481, MinusLogProbMetric: 28.1481, val_loss: 28.6664, val_MinusLogProbMetric: 28.6664

Epoch 763: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1481 - MinusLogProbMetric: 28.1481 - val_loss: 28.6664 - val_MinusLogProbMetric: 28.6664 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 764/1000
2023-09-30 04:36:06.533 
Epoch 764/1000 
	 loss: 28.1437, MinusLogProbMetric: 28.1437, val_loss: 28.8250, val_MinusLogProbMetric: 28.8250

Epoch 764: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1437 - MinusLogProbMetric: 28.1437 - val_loss: 28.8250 - val_MinusLogProbMetric: 28.8250 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 765/1000
2023-09-30 04:36:40.615 
Epoch 765/1000 
	 loss: 28.1590, MinusLogProbMetric: 28.1590, val_loss: 28.7273, val_MinusLogProbMetric: 28.7273

Epoch 765: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1590 - MinusLogProbMetric: 28.1590 - val_loss: 28.7273 - val_MinusLogProbMetric: 28.7273 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 766/1000
2023-09-30 04:37:15.032 
Epoch 766/1000 
	 loss: 28.1440, MinusLogProbMetric: 28.1440, val_loss: 28.6685, val_MinusLogProbMetric: 28.6685

Epoch 766: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1440 - MinusLogProbMetric: 28.1440 - val_loss: 28.6685 - val_MinusLogProbMetric: 28.6685 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 767/1000
2023-09-30 04:37:48.959 
Epoch 767/1000 
	 loss: 28.1417, MinusLogProbMetric: 28.1417, val_loss: 28.6389, val_MinusLogProbMetric: 28.6389

Epoch 767: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1417 - MinusLogProbMetric: 28.1417 - val_loss: 28.6389 - val_MinusLogProbMetric: 28.6389 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 768/1000
2023-09-30 04:38:22.606 
Epoch 768/1000 
	 loss: 28.1376, MinusLogProbMetric: 28.1376, val_loss: 28.7609, val_MinusLogProbMetric: 28.7609

Epoch 768: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1376 - MinusLogProbMetric: 28.1376 - val_loss: 28.7609 - val_MinusLogProbMetric: 28.7609 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 769/1000
2023-09-30 04:38:56.505 
Epoch 769/1000 
	 loss: 28.1549, MinusLogProbMetric: 28.1549, val_loss: 28.6475, val_MinusLogProbMetric: 28.6475

Epoch 769: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1549 - MinusLogProbMetric: 28.1549 - val_loss: 28.6475 - val_MinusLogProbMetric: 28.6475 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 770/1000
2023-09-30 04:39:29.390 
Epoch 770/1000 
	 loss: 28.1505, MinusLogProbMetric: 28.1505, val_loss: 28.8817, val_MinusLogProbMetric: 28.8817

Epoch 770: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1505 - MinusLogProbMetric: 28.1505 - val_loss: 28.8817 - val_MinusLogProbMetric: 28.8817 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 771/1000
2023-09-30 04:40:03.931 
Epoch 771/1000 
	 loss: 28.1393, MinusLogProbMetric: 28.1393, val_loss: 28.6560, val_MinusLogProbMetric: 28.6560

Epoch 771: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1393 - MinusLogProbMetric: 28.1393 - val_loss: 28.6560 - val_MinusLogProbMetric: 28.6560 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 772/1000
2023-09-30 04:40:37.888 
Epoch 772/1000 
	 loss: 28.1456, MinusLogProbMetric: 28.1456, val_loss: 28.6672, val_MinusLogProbMetric: 28.6672

Epoch 772: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1456 - MinusLogProbMetric: 28.1456 - val_loss: 28.6672 - val_MinusLogProbMetric: 28.6672 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 773/1000
2023-09-30 04:41:12.540 
Epoch 773/1000 
	 loss: 28.1401, MinusLogProbMetric: 28.1401, val_loss: 28.6736, val_MinusLogProbMetric: 28.6736

Epoch 773: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1401 - MinusLogProbMetric: 28.1401 - val_loss: 28.6736 - val_MinusLogProbMetric: 28.6736 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 774/1000
2023-09-30 04:41:46.483 
Epoch 774/1000 
	 loss: 28.1497, MinusLogProbMetric: 28.1497, val_loss: 28.7149, val_MinusLogProbMetric: 28.7149

Epoch 774: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1497 - MinusLogProbMetric: 28.1497 - val_loss: 28.7149 - val_MinusLogProbMetric: 28.7149 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 775/1000
2023-09-30 04:42:20.525 
Epoch 775/1000 
	 loss: 28.1478, MinusLogProbMetric: 28.1478, val_loss: 28.6661, val_MinusLogProbMetric: 28.6661

Epoch 775: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1478 - MinusLogProbMetric: 28.1478 - val_loss: 28.6661 - val_MinusLogProbMetric: 28.6661 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 776/1000
2023-09-30 04:42:51.609 
Epoch 776/1000 
	 loss: 28.1410, MinusLogProbMetric: 28.1410, val_loss: 28.7027, val_MinusLogProbMetric: 28.7027

Epoch 776: val_loss did not improve from 28.61222
196/196 - 31s - loss: 28.1410 - MinusLogProbMetric: 28.1410 - val_loss: 28.7027 - val_MinusLogProbMetric: 28.7027 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 777/1000
2023-09-30 04:43:25.200 
Epoch 777/1000 
	 loss: 28.1435, MinusLogProbMetric: 28.1435, val_loss: 28.6657, val_MinusLogProbMetric: 28.6657

Epoch 777: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1435 - MinusLogProbMetric: 28.1435 - val_loss: 28.6657 - val_MinusLogProbMetric: 28.6657 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 778/1000
2023-09-30 04:43:59.884 
Epoch 778/1000 
	 loss: 28.1168, MinusLogProbMetric: 28.1168, val_loss: 28.6981, val_MinusLogProbMetric: 28.6981

Epoch 778: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1168 - MinusLogProbMetric: 28.1168 - val_loss: 28.6981 - val_MinusLogProbMetric: 28.6981 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 779/1000
2023-09-30 04:44:32.834 
Epoch 779/1000 
	 loss: 28.1272, MinusLogProbMetric: 28.1272, val_loss: 28.7865, val_MinusLogProbMetric: 28.7865

Epoch 779: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1272 - MinusLogProbMetric: 28.1272 - val_loss: 28.7865 - val_MinusLogProbMetric: 28.7865 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 780/1000
2023-09-30 04:45:04.773 
Epoch 780/1000 
	 loss: 28.1382, MinusLogProbMetric: 28.1382, val_loss: 28.7359, val_MinusLogProbMetric: 28.7359

Epoch 780: val_loss did not improve from 28.61222
196/196 - 32s - loss: 28.1382 - MinusLogProbMetric: 28.1382 - val_loss: 28.7359 - val_MinusLogProbMetric: 28.7359 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 781/1000
2023-09-30 04:45:38.595 
Epoch 781/1000 
	 loss: 28.1321, MinusLogProbMetric: 28.1321, val_loss: 28.6617, val_MinusLogProbMetric: 28.6617

Epoch 781: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1321 - MinusLogProbMetric: 28.1321 - val_loss: 28.6617 - val_MinusLogProbMetric: 28.6617 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 782/1000
2023-09-30 04:46:11.353 
Epoch 782/1000 
	 loss: 28.1235, MinusLogProbMetric: 28.1235, val_loss: 28.7148, val_MinusLogProbMetric: 28.7148

Epoch 782: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1235 - MinusLogProbMetric: 28.1235 - val_loss: 28.7148 - val_MinusLogProbMetric: 28.7148 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 783/1000
2023-09-30 04:46:46.117 
Epoch 783/1000 
	 loss: 28.1371, MinusLogProbMetric: 28.1371, val_loss: 28.7236, val_MinusLogProbMetric: 28.7236

Epoch 783: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1371 - MinusLogProbMetric: 28.1371 - val_loss: 28.7236 - val_MinusLogProbMetric: 28.7236 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 784/1000
2023-09-30 04:47:20.096 
Epoch 784/1000 
	 loss: 28.1319, MinusLogProbMetric: 28.1319, val_loss: 28.8424, val_MinusLogProbMetric: 28.8424

Epoch 784: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1319 - MinusLogProbMetric: 28.1319 - val_loss: 28.8424 - val_MinusLogProbMetric: 28.8424 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 785/1000
2023-09-30 04:47:52.936 
Epoch 785/1000 
	 loss: 28.1385, MinusLogProbMetric: 28.1385, val_loss: 28.8184, val_MinusLogProbMetric: 28.8184

Epoch 785: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1385 - MinusLogProbMetric: 28.1385 - val_loss: 28.8184 - val_MinusLogProbMetric: 28.8184 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 786/1000
2023-09-30 04:48:27.520 
Epoch 786/1000 
	 loss: 28.1296, MinusLogProbMetric: 28.1296, val_loss: 28.8358, val_MinusLogProbMetric: 28.8358

Epoch 786: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1296 - MinusLogProbMetric: 28.1296 - val_loss: 28.8358 - val_MinusLogProbMetric: 28.8358 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 787/1000
2023-09-30 04:49:02.062 
Epoch 787/1000 
	 loss: 28.1638, MinusLogProbMetric: 28.1638, val_loss: 28.6378, val_MinusLogProbMetric: 28.6378

Epoch 787: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1638 - MinusLogProbMetric: 28.1638 - val_loss: 28.6378 - val_MinusLogProbMetric: 28.6378 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 788/1000
2023-09-30 04:49:34.009 
Epoch 788/1000 
	 loss: 28.1427, MinusLogProbMetric: 28.1427, val_loss: 28.6859, val_MinusLogProbMetric: 28.6859

Epoch 788: val_loss did not improve from 28.61222
196/196 - 32s - loss: 28.1427 - MinusLogProbMetric: 28.1427 - val_loss: 28.6859 - val_MinusLogProbMetric: 28.6859 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 789/1000
2023-09-30 04:50:06.968 
Epoch 789/1000 
	 loss: 28.1397, MinusLogProbMetric: 28.1397, val_loss: 28.8017, val_MinusLogProbMetric: 28.8017

Epoch 789: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1397 - MinusLogProbMetric: 28.1397 - val_loss: 28.8017 - val_MinusLogProbMetric: 28.8017 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 790/1000
2023-09-30 04:50:40.225 
Epoch 790/1000 
	 loss: 28.1388, MinusLogProbMetric: 28.1388, val_loss: 28.6594, val_MinusLogProbMetric: 28.6594

Epoch 790: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1388 - MinusLogProbMetric: 28.1388 - val_loss: 28.6594 - val_MinusLogProbMetric: 28.6594 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 791/1000
2023-09-30 04:51:12.582 
Epoch 791/1000 
	 loss: 28.1374, MinusLogProbMetric: 28.1374, val_loss: 28.6177, val_MinusLogProbMetric: 28.6177

Epoch 791: val_loss did not improve from 28.61222
196/196 - 32s - loss: 28.1374 - MinusLogProbMetric: 28.1374 - val_loss: 28.6177 - val_MinusLogProbMetric: 28.6177 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 792/1000
2023-09-30 04:51:45.655 
Epoch 792/1000 
	 loss: 28.1515, MinusLogProbMetric: 28.1515, val_loss: 28.6941, val_MinusLogProbMetric: 28.6941

Epoch 792: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1515 - MinusLogProbMetric: 28.1515 - val_loss: 28.6941 - val_MinusLogProbMetric: 28.6941 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 793/1000
2023-09-30 04:52:18.073 
Epoch 793/1000 
	 loss: 28.1526, MinusLogProbMetric: 28.1526, val_loss: 28.6616, val_MinusLogProbMetric: 28.6616

Epoch 793: val_loss did not improve from 28.61222
196/196 - 32s - loss: 28.1526 - MinusLogProbMetric: 28.1526 - val_loss: 28.6616 - val_MinusLogProbMetric: 28.6616 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 794/1000
2023-09-30 04:52:50.451 
Epoch 794/1000 
	 loss: 28.1259, MinusLogProbMetric: 28.1259, val_loss: 28.6574, val_MinusLogProbMetric: 28.6574

Epoch 794: val_loss did not improve from 28.61222
196/196 - 32s - loss: 28.1259 - MinusLogProbMetric: 28.1259 - val_loss: 28.6574 - val_MinusLogProbMetric: 28.6574 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 795/1000
2023-09-30 04:53:24.705 
Epoch 795/1000 
	 loss: 28.1375, MinusLogProbMetric: 28.1375, val_loss: 28.6652, val_MinusLogProbMetric: 28.6652

Epoch 795: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1375 - MinusLogProbMetric: 28.1375 - val_loss: 28.6652 - val_MinusLogProbMetric: 28.6652 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 796/1000
2023-09-30 04:53:59.274 
Epoch 796/1000 
	 loss: 28.1666, MinusLogProbMetric: 28.1666, val_loss: 28.8380, val_MinusLogProbMetric: 28.8380

Epoch 796: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1666 - MinusLogProbMetric: 28.1666 - val_loss: 28.8380 - val_MinusLogProbMetric: 28.8380 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 797/1000
2023-09-30 04:54:31.104 
Epoch 797/1000 
	 loss: 28.1548, MinusLogProbMetric: 28.1548, val_loss: 28.6700, val_MinusLogProbMetric: 28.6700

Epoch 797: val_loss did not improve from 28.61222
196/196 - 32s - loss: 28.1548 - MinusLogProbMetric: 28.1548 - val_loss: 28.6700 - val_MinusLogProbMetric: 28.6700 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 798/1000
2023-09-30 04:55:03.672 
Epoch 798/1000 
	 loss: 28.1679, MinusLogProbMetric: 28.1679, val_loss: 28.8702, val_MinusLogProbMetric: 28.8702

Epoch 798: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1679 - MinusLogProbMetric: 28.1679 - val_loss: 28.8702 - val_MinusLogProbMetric: 28.8702 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 799/1000
2023-09-30 04:55:37.074 
Epoch 799/1000 
	 loss: 28.1492, MinusLogProbMetric: 28.1492, val_loss: 28.9257, val_MinusLogProbMetric: 28.9257

Epoch 799: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1492 - MinusLogProbMetric: 28.1492 - val_loss: 28.9257 - val_MinusLogProbMetric: 28.9257 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 800/1000
2023-09-30 04:56:10.610 
Epoch 800/1000 
	 loss: 28.1473, MinusLogProbMetric: 28.1473, val_loss: 28.6570, val_MinusLogProbMetric: 28.6570

Epoch 800: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1473 - MinusLogProbMetric: 28.1473 - val_loss: 28.6570 - val_MinusLogProbMetric: 28.6570 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 801/1000
2023-09-30 04:56:45.091 
Epoch 801/1000 
	 loss: 28.1488, MinusLogProbMetric: 28.1488, val_loss: 28.8026, val_MinusLogProbMetric: 28.8026

Epoch 801: val_loss did not improve from 28.61222
196/196 - 34s - loss: 28.1488 - MinusLogProbMetric: 28.1488 - val_loss: 28.8026 - val_MinusLogProbMetric: 28.8026 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 802/1000
2023-09-30 04:57:18.286 
Epoch 802/1000 
	 loss: 28.1498, MinusLogProbMetric: 28.1498, val_loss: 28.7542, val_MinusLogProbMetric: 28.7542

Epoch 802: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1498 - MinusLogProbMetric: 28.1498 - val_loss: 28.7542 - val_MinusLogProbMetric: 28.7542 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 803/1000
2023-09-30 04:57:53.029 
Epoch 803/1000 
	 loss: 28.1357, MinusLogProbMetric: 28.1357, val_loss: 28.6931, val_MinusLogProbMetric: 28.6931

Epoch 803: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1357 - MinusLogProbMetric: 28.1357 - val_loss: 28.6931 - val_MinusLogProbMetric: 28.6931 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 804/1000
2023-09-30 04:58:27.546 
Epoch 804/1000 
	 loss: 28.1398, MinusLogProbMetric: 28.1398, val_loss: 28.6283, val_MinusLogProbMetric: 28.6283

Epoch 804: val_loss did not improve from 28.61222
196/196 - 35s - loss: 28.1398 - MinusLogProbMetric: 28.1398 - val_loss: 28.6283 - val_MinusLogProbMetric: 28.6283 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 805/1000
2023-09-30 04:59:00.578 
Epoch 805/1000 
	 loss: 28.1331, MinusLogProbMetric: 28.1331, val_loss: 28.7123, val_MinusLogProbMetric: 28.7123

Epoch 805: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.1331 - MinusLogProbMetric: 28.1331 - val_loss: 28.7123 - val_MinusLogProbMetric: 28.7123 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 806/1000
2023-09-30 04:59:33.892 
Epoch 806/1000 
	 loss: 28.0851, MinusLogProbMetric: 28.0851, val_loss: 28.6235, val_MinusLogProbMetric: 28.6235

Epoch 806: val_loss did not improve from 28.61222
196/196 - 33s - loss: 28.0851 - MinusLogProbMetric: 28.0851 - val_loss: 28.6235 - val_MinusLogProbMetric: 28.6235 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 807/1000
2023-09-30 05:00:06.362 
Epoch 807/1000 
	 loss: 28.0828, MinusLogProbMetric: 28.0828, val_loss: 28.6423, val_MinusLogProbMetric: 28.6423

Epoch 807: val_loss did not improve from 28.61222
196/196 - 32s - loss: 28.0828 - MinusLogProbMetric: 28.0828 - val_loss: 28.6423 - val_MinusLogProbMetric: 28.6423 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 808/1000
2023-09-30 05:00:40.401 
Epoch 808/1000 
	 loss: 28.0834, MinusLogProbMetric: 28.0834, val_loss: 28.5999, val_MinusLogProbMetric: 28.5999

Epoch 808: val_loss improved from 28.61222 to 28.59987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.0834 - MinusLogProbMetric: 28.0834 - val_loss: 28.5999 - val_MinusLogProbMetric: 28.5999 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 809/1000
2023-09-30 05:01:15.928 
Epoch 809/1000 
	 loss: 28.0833, MinusLogProbMetric: 28.0833, val_loss: 28.6317, val_MinusLogProbMetric: 28.6317

Epoch 809: val_loss did not improve from 28.59987
196/196 - 35s - loss: 28.0833 - MinusLogProbMetric: 28.0833 - val_loss: 28.6317 - val_MinusLogProbMetric: 28.6317 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 810/1000
2023-09-30 05:01:48.208 
Epoch 810/1000 
	 loss: 28.0831, MinusLogProbMetric: 28.0831, val_loss: 28.6187, val_MinusLogProbMetric: 28.6187

Epoch 810: val_loss did not improve from 28.59987
196/196 - 32s - loss: 28.0831 - MinusLogProbMetric: 28.0831 - val_loss: 28.6187 - val_MinusLogProbMetric: 28.6187 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 811/1000
2023-09-30 05:02:20.966 
Epoch 811/1000 
	 loss: 28.0839, MinusLogProbMetric: 28.0839, val_loss: 28.6295, val_MinusLogProbMetric: 28.6295

Epoch 811: val_loss did not improve from 28.59987
196/196 - 33s - loss: 28.0839 - MinusLogProbMetric: 28.0839 - val_loss: 28.6295 - val_MinusLogProbMetric: 28.6295 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 812/1000
2023-09-30 05:02:53.667 
Epoch 812/1000 
	 loss: 28.0828, MinusLogProbMetric: 28.0828, val_loss: 28.6104, val_MinusLogProbMetric: 28.6104

Epoch 812: val_loss did not improve from 28.59987
196/196 - 33s - loss: 28.0828 - MinusLogProbMetric: 28.0828 - val_loss: 28.6104 - val_MinusLogProbMetric: 28.6104 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 813/1000
2023-09-30 05:03:25.982 
Epoch 813/1000 
	 loss: 28.0813, MinusLogProbMetric: 28.0813, val_loss: 28.6304, val_MinusLogProbMetric: 28.6304

Epoch 813: val_loss did not improve from 28.59987
196/196 - 32s - loss: 28.0813 - MinusLogProbMetric: 28.0813 - val_loss: 28.6304 - val_MinusLogProbMetric: 28.6304 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 814/1000
2023-09-30 05:03:57.320 
Epoch 814/1000 
	 loss: 28.0842, MinusLogProbMetric: 28.0842, val_loss: 28.6124, val_MinusLogProbMetric: 28.6124

Epoch 814: val_loss did not improve from 28.59987
196/196 - 31s - loss: 28.0842 - MinusLogProbMetric: 28.0842 - val_loss: 28.6124 - val_MinusLogProbMetric: 28.6124 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 815/1000
2023-09-30 05:04:31.094 
Epoch 815/1000 
	 loss: 28.0863, MinusLogProbMetric: 28.0863, val_loss: 28.6448, val_MinusLogProbMetric: 28.6448

Epoch 815: val_loss did not improve from 28.59987
196/196 - 34s - loss: 28.0863 - MinusLogProbMetric: 28.0863 - val_loss: 28.6448 - val_MinusLogProbMetric: 28.6448 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 816/1000
2023-09-30 05:05:04.789 
Epoch 816/1000 
	 loss: 28.0802, MinusLogProbMetric: 28.0802, val_loss: 28.6273, val_MinusLogProbMetric: 28.6273

Epoch 816: val_loss did not improve from 28.59987
196/196 - 34s - loss: 28.0802 - MinusLogProbMetric: 28.0802 - val_loss: 28.6273 - val_MinusLogProbMetric: 28.6273 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 817/1000
2023-09-30 05:05:38.378 
Epoch 817/1000 
	 loss: 28.0854, MinusLogProbMetric: 28.0854, val_loss: 28.7140, val_MinusLogProbMetric: 28.7140

Epoch 817: val_loss did not improve from 28.59987
196/196 - 34s - loss: 28.0854 - MinusLogProbMetric: 28.0854 - val_loss: 28.7140 - val_MinusLogProbMetric: 28.7140 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 818/1000
2023-09-30 05:06:11.150 
Epoch 818/1000 
	 loss: 28.0850, MinusLogProbMetric: 28.0850, val_loss: 28.7077, val_MinusLogProbMetric: 28.7077

Epoch 818: val_loss did not improve from 28.59987
196/196 - 33s - loss: 28.0850 - MinusLogProbMetric: 28.0850 - val_loss: 28.7077 - val_MinusLogProbMetric: 28.7077 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 819/1000
2023-09-30 05:06:44.683 
Epoch 819/1000 
	 loss: 28.0836, MinusLogProbMetric: 28.0836, val_loss: 28.6084, val_MinusLogProbMetric: 28.6084

Epoch 819: val_loss did not improve from 28.59987
196/196 - 34s - loss: 28.0836 - MinusLogProbMetric: 28.0836 - val_loss: 28.6084 - val_MinusLogProbMetric: 28.6084 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 820/1000
2023-09-30 05:07:17.297 
Epoch 820/1000 
	 loss: 28.0855, MinusLogProbMetric: 28.0855, val_loss: 28.5962, val_MinusLogProbMetric: 28.5962

Epoch 820: val_loss improved from 28.59987 to 28.59625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.0855 - MinusLogProbMetric: 28.0855 - val_loss: 28.5962 - val_MinusLogProbMetric: 28.5962 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 821/1000
2023-09-30 05:07:50.681 
Epoch 821/1000 
	 loss: 28.0854, MinusLogProbMetric: 28.0854, val_loss: 28.6216, val_MinusLogProbMetric: 28.6216

Epoch 821: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0854 - MinusLogProbMetric: 28.0854 - val_loss: 28.6216 - val_MinusLogProbMetric: 28.6216 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 822/1000
2023-09-30 05:08:21.300 
Epoch 822/1000 
	 loss: 28.0823, MinusLogProbMetric: 28.0823, val_loss: 28.6238, val_MinusLogProbMetric: 28.6238

Epoch 822: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0823 - MinusLogProbMetric: 28.0823 - val_loss: 28.6238 - val_MinusLogProbMetric: 28.6238 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 823/1000
2023-09-30 05:08:54.446 
Epoch 823/1000 
	 loss: 28.0790, MinusLogProbMetric: 28.0790, val_loss: 28.6547, val_MinusLogProbMetric: 28.6547

Epoch 823: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0790 - MinusLogProbMetric: 28.0790 - val_loss: 28.6547 - val_MinusLogProbMetric: 28.6547 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 824/1000
2023-09-30 05:09:28.920 
Epoch 824/1000 
	 loss: 28.0812, MinusLogProbMetric: 28.0812, val_loss: 28.6206, val_MinusLogProbMetric: 28.6206

Epoch 824: val_loss did not improve from 28.59625
196/196 - 34s - loss: 28.0812 - MinusLogProbMetric: 28.0812 - val_loss: 28.6206 - val_MinusLogProbMetric: 28.6206 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 825/1000
2023-09-30 05:10:01.364 
Epoch 825/1000 
	 loss: 28.0802, MinusLogProbMetric: 28.0802, val_loss: 28.6268, val_MinusLogProbMetric: 28.6268

Epoch 825: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0802 - MinusLogProbMetric: 28.0802 - val_loss: 28.6268 - val_MinusLogProbMetric: 28.6268 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 826/1000
2023-09-30 05:10:33.577 
Epoch 826/1000 
	 loss: 28.0824, MinusLogProbMetric: 28.0824, val_loss: 28.6183, val_MinusLogProbMetric: 28.6183

Epoch 826: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0824 - MinusLogProbMetric: 28.0824 - val_loss: 28.6183 - val_MinusLogProbMetric: 28.6183 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 827/1000
2023-09-30 05:11:07.915 
Epoch 827/1000 
	 loss: 28.0818, MinusLogProbMetric: 28.0818, val_loss: 28.6346, val_MinusLogProbMetric: 28.6346

Epoch 827: val_loss did not improve from 28.59625
196/196 - 34s - loss: 28.0818 - MinusLogProbMetric: 28.0818 - val_loss: 28.6346 - val_MinusLogProbMetric: 28.6346 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 828/1000
2023-09-30 05:11:40.850 
Epoch 828/1000 
	 loss: 28.0822, MinusLogProbMetric: 28.0822, val_loss: 28.6817, val_MinusLogProbMetric: 28.6817

Epoch 828: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0822 - MinusLogProbMetric: 28.0822 - val_loss: 28.6817 - val_MinusLogProbMetric: 28.6817 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 829/1000
2023-09-30 05:12:14.460 
Epoch 829/1000 
	 loss: 28.0831, MinusLogProbMetric: 28.0831, val_loss: 28.6123, val_MinusLogProbMetric: 28.6123

Epoch 829: val_loss did not improve from 28.59625
196/196 - 34s - loss: 28.0831 - MinusLogProbMetric: 28.0831 - val_loss: 28.6123 - val_MinusLogProbMetric: 28.6123 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 830/1000
2023-09-30 05:12:47.759 
Epoch 830/1000 
	 loss: 28.0815, MinusLogProbMetric: 28.0815, val_loss: 28.6176, val_MinusLogProbMetric: 28.6176

Epoch 830: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0815 - MinusLogProbMetric: 28.0815 - val_loss: 28.6176 - val_MinusLogProbMetric: 28.6176 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 831/1000
2023-09-30 05:13:19.808 
Epoch 831/1000 
	 loss: 28.0822, MinusLogProbMetric: 28.0822, val_loss: 28.6518, val_MinusLogProbMetric: 28.6518

Epoch 831: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0822 - MinusLogProbMetric: 28.0822 - val_loss: 28.6518 - val_MinusLogProbMetric: 28.6518 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 832/1000
2023-09-30 05:13:54.542 
Epoch 832/1000 
	 loss: 28.0852, MinusLogProbMetric: 28.0852, val_loss: 28.6662, val_MinusLogProbMetric: 28.6662

Epoch 832: val_loss did not improve from 28.59625
196/196 - 35s - loss: 28.0852 - MinusLogProbMetric: 28.0852 - val_loss: 28.6662 - val_MinusLogProbMetric: 28.6662 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 833/1000
2023-09-30 05:14:26.472 
Epoch 833/1000 
	 loss: 28.0817, MinusLogProbMetric: 28.0817, val_loss: 28.6146, val_MinusLogProbMetric: 28.6146

Epoch 833: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0817 - MinusLogProbMetric: 28.0817 - val_loss: 28.6146 - val_MinusLogProbMetric: 28.6146 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 834/1000
2023-09-30 05:14:57.718 
Epoch 834/1000 
	 loss: 28.0787, MinusLogProbMetric: 28.0787, val_loss: 28.6527, val_MinusLogProbMetric: 28.6527

Epoch 834: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0787 - MinusLogProbMetric: 28.0787 - val_loss: 28.6527 - val_MinusLogProbMetric: 28.6527 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 835/1000
2023-09-30 05:15:31.473 
Epoch 835/1000 
	 loss: 28.0801, MinusLogProbMetric: 28.0801, val_loss: 28.6221, val_MinusLogProbMetric: 28.6221

Epoch 835: val_loss did not improve from 28.59625
196/196 - 34s - loss: 28.0801 - MinusLogProbMetric: 28.0801 - val_loss: 28.6221 - val_MinusLogProbMetric: 28.6221 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 836/1000
2023-09-30 05:16:06.324 
Epoch 836/1000 
	 loss: 28.0777, MinusLogProbMetric: 28.0777, val_loss: 28.6296, val_MinusLogProbMetric: 28.6296

Epoch 836: val_loss did not improve from 28.59625
196/196 - 35s - loss: 28.0777 - MinusLogProbMetric: 28.0777 - val_loss: 28.6296 - val_MinusLogProbMetric: 28.6296 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 837/1000
2023-09-30 05:16:40.983 
Epoch 837/1000 
	 loss: 28.0788, MinusLogProbMetric: 28.0788, val_loss: 28.6317, val_MinusLogProbMetric: 28.6317

Epoch 837: val_loss did not improve from 28.59625
196/196 - 35s - loss: 28.0788 - MinusLogProbMetric: 28.0788 - val_loss: 28.6317 - val_MinusLogProbMetric: 28.6317 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 838/1000
2023-09-30 05:17:13.169 
Epoch 838/1000 
	 loss: 28.0809, MinusLogProbMetric: 28.0809, val_loss: 28.6646, val_MinusLogProbMetric: 28.6646

Epoch 838: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0809 - MinusLogProbMetric: 28.0809 - val_loss: 28.6646 - val_MinusLogProbMetric: 28.6646 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 839/1000
2023-09-30 05:17:47.437 
Epoch 839/1000 
	 loss: 28.0789, MinusLogProbMetric: 28.0789, val_loss: 28.6489, val_MinusLogProbMetric: 28.6489

Epoch 839: val_loss did not improve from 28.59625
196/196 - 34s - loss: 28.0789 - MinusLogProbMetric: 28.0789 - val_loss: 28.6489 - val_MinusLogProbMetric: 28.6489 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 840/1000
2023-09-30 05:18:20.386 
Epoch 840/1000 
	 loss: 28.0806, MinusLogProbMetric: 28.0806, val_loss: 28.6124, val_MinusLogProbMetric: 28.6124

Epoch 840: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0806 - MinusLogProbMetric: 28.0806 - val_loss: 28.6124 - val_MinusLogProbMetric: 28.6124 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 841/1000
2023-09-30 05:18:53.274 
Epoch 841/1000 
	 loss: 28.0851, MinusLogProbMetric: 28.0851, val_loss: 28.6515, val_MinusLogProbMetric: 28.6515

Epoch 841: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0851 - MinusLogProbMetric: 28.0851 - val_loss: 28.6515 - val_MinusLogProbMetric: 28.6515 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 842/1000
2023-09-30 05:19:25.710 
Epoch 842/1000 
	 loss: 28.0817, MinusLogProbMetric: 28.0817, val_loss: 28.6382, val_MinusLogProbMetric: 28.6382

Epoch 842: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0817 - MinusLogProbMetric: 28.0817 - val_loss: 28.6382 - val_MinusLogProbMetric: 28.6382 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 843/1000
2023-09-30 05:20:00.373 
Epoch 843/1000 
	 loss: 28.0755, MinusLogProbMetric: 28.0755, val_loss: 28.6227, val_MinusLogProbMetric: 28.6227

Epoch 843: val_loss did not improve from 28.59625
196/196 - 35s - loss: 28.0755 - MinusLogProbMetric: 28.0755 - val_loss: 28.6227 - val_MinusLogProbMetric: 28.6227 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 844/1000
2023-09-30 05:20:31.940 
Epoch 844/1000 
	 loss: 28.0873, MinusLogProbMetric: 28.0873, val_loss: 28.6256, val_MinusLogProbMetric: 28.6256

Epoch 844: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0873 - MinusLogProbMetric: 28.0873 - val_loss: 28.6256 - val_MinusLogProbMetric: 28.6256 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 845/1000
2023-09-30 05:21:04.564 
Epoch 845/1000 
	 loss: 28.0880, MinusLogProbMetric: 28.0880, val_loss: 28.6356, val_MinusLogProbMetric: 28.6356

Epoch 845: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0880 - MinusLogProbMetric: 28.0880 - val_loss: 28.6356 - val_MinusLogProbMetric: 28.6356 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 846/1000
2023-09-30 05:21:38.006 
Epoch 846/1000 
	 loss: 28.0858, MinusLogProbMetric: 28.0858, val_loss: 28.6301, val_MinusLogProbMetric: 28.6301

Epoch 846: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0858 - MinusLogProbMetric: 28.0858 - val_loss: 28.6301 - val_MinusLogProbMetric: 28.6301 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 847/1000
2023-09-30 05:22:12.977 
Epoch 847/1000 
	 loss: 28.0862, MinusLogProbMetric: 28.0862, val_loss: 28.6507, val_MinusLogProbMetric: 28.6507

Epoch 847: val_loss did not improve from 28.59625
196/196 - 35s - loss: 28.0862 - MinusLogProbMetric: 28.0862 - val_loss: 28.6507 - val_MinusLogProbMetric: 28.6507 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 848/1000
2023-09-30 05:22:43.956 
Epoch 848/1000 
	 loss: 28.0882, MinusLogProbMetric: 28.0882, val_loss: 28.6473, val_MinusLogProbMetric: 28.6473

Epoch 848: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0882 - MinusLogProbMetric: 28.0882 - val_loss: 28.6473 - val_MinusLogProbMetric: 28.6473 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 849/1000
2023-09-30 05:23:15.071 
Epoch 849/1000 
	 loss: 28.0830, MinusLogProbMetric: 28.0830, val_loss: 28.6078, val_MinusLogProbMetric: 28.6078

Epoch 849: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0830 - MinusLogProbMetric: 28.0830 - val_loss: 28.6078 - val_MinusLogProbMetric: 28.6078 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 850/1000
2023-09-30 05:23:46.314 
Epoch 850/1000 
	 loss: 28.0761, MinusLogProbMetric: 28.0761, val_loss: 28.6206, val_MinusLogProbMetric: 28.6206

Epoch 850: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0761 - MinusLogProbMetric: 28.0761 - val_loss: 28.6206 - val_MinusLogProbMetric: 28.6206 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 851/1000
2023-09-30 05:24:17.207 
Epoch 851/1000 
	 loss: 28.0777, MinusLogProbMetric: 28.0777, val_loss: 28.6299, val_MinusLogProbMetric: 28.6299

Epoch 851: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0777 - MinusLogProbMetric: 28.0777 - val_loss: 28.6299 - val_MinusLogProbMetric: 28.6299 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 852/1000
2023-09-30 05:24:45.173 
Epoch 852/1000 
	 loss: 28.0795, MinusLogProbMetric: 28.0795, val_loss: 28.6292, val_MinusLogProbMetric: 28.6292

Epoch 852: val_loss did not improve from 28.59625
196/196 - 28s - loss: 28.0795 - MinusLogProbMetric: 28.0795 - val_loss: 28.6292 - val_MinusLogProbMetric: 28.6292 - lr: 6.2500e-05 - 28s/epoch - 143ms/step
Epoch 853/1000
2023-09-30 05:25:13.753 
Epoch 853/1000 
	 loss: 28.0757, MinusLogProbMetric: 28.0757, val_loss: 28.6112, val_MinusLogProbMetric: 28.6112

Epoch 853: val_loss did not improve from 28.59625
196/196 - 29s - loss: 28.0757 - MinusLogProbMetric: 28.0757 - val_loss: 28.6112 - val_MinusLogProbMetric: 28.6112 - lr: 6.2500e-05 - 29s/epoch - 146ms/step
Epoch 854/1000
2023-09-30 05:25:44.487 
Epoch 854/1000 
	 loss: 28.0764, MinusLogProbMetric: 28.0764, val_loss: 28.6184, val_MinusLogProbMetric: 28.6184

Epoch 854: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0764 - MinusLogProbMetric: 28.0764 - val_loss: 28.6184 - val_MinusLogProbMetric: 28.6184 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 855/1000
2023-09-30 05:26:17.294 
Epoch 855/1000 
	 loss: 28.0712, MinusLogProbMetric: 28.0712, val_loss: 28.6230, val_MinusLogProbMetric: 28.6230

Epoch 855: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0712 - MinusLogProbMetric: 28.0712 - val_loss: 28.6230 - val_MinusLogProbMetric: 28.6230 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 856/1000
2023-09-30 05:26:46.871 
Epoch 856/1000 
	 loss: 28.0779, MinusLogProbMetric: 28.0779, val_loss: 28.6168, val_MinusLogProbMetric: 28.6168

Epoch 856: val_loss did not improve from 28.59625
196/196 - 30s - loss: 28.0779 - MinusLogProbMetric: 28.0779 - val_loss: 28.6168 - val_MinusLogProbMetric: 28.6168 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 857/1000
2023-09-30 05:27:17.560 
Epoch 857/1000 
	 loss: 28.0786, MinusLogProbMetric: 28.0786, val_loss: 28.6086, val_MinusLogProbMetric: 28.6086

Epoch 857: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0786 - MinusLogProbMetric: 28.0786 - val_loss: 28.6086 - val_MinusLogProbMetric: 28.6086 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 858/1000
2023-09-30 05:27:50.984 
Epoch 858/1000 
	 loss: 28.0807, MinusLogProbMetric: 28.0807, val_loss: 28.6203, val_MinusLogProbMetric: 28.6203

Epoch 858: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0807 - MinusLogProbMetric: 28.0807 - val_loss: 28.6203 - val_MinusLogProbMetric: 28.6203 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 859/1000
2023-09-30 05:28:20.063 
Epoch 859/1000 
	 loss: 28.0822, MinusLogProbMetric: 28.0822, val_loss: 28.6184, val_MinusLogProbMetric: 28.6184

Epoch 859: val_loss did not improve from 28.59625
196/196 - 29s - loss: 28.0822 - MinusLogProbMetric: 28.0822 - val_loss: 28.6184 - val_MinusLogProbMetric: 28.6184 - lr: 6.2500e-05 - 29s/epoch - 148ms/step
Epoch 860/1000
2023-09-30 05:28:50.856 
Epoch 860/1000 
	 loss: 28.0729, MinusLogProbMetric: 28.0729, val_loss: 28.6305, val_MinusLogProbMetric: 28.6305

Epoch 860: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0729 - MinusLogProbMetric: 28.0729 - val_loss: 28.6305 - val_MinusLogProbMetric: 28.6305 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 861/1000
2023-09-30 05:29:20.816 
Epoch 861/1000 
	 loss: 28.0791, MinusLogProbMetric: 28.0791, val_loss: 28.6260, val_MinusLogProbMetric: 28.6260

Epoch 861: val_loss did not improve from 28.59625
196/196 - 30s - loss: 28.0791 - MinusLogProbMetric: 28.0791 - val_loss: 28.6260 - val_MinusLogProbMetric: 28.6260 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 862/1000
2023-09-30 05:29:51.072 
Epoch 862/1000 
	 loss: 28.0771, MinusLogProbMetric: 28.0771, val_loss: 28.6248, val_MinusLogProbMetric: 28.6248

Epoch 862: val_loss did not improve from 28.59625
196/196 - 30s - loss: 28.0771 - MinusLogProbMetric: 28.0771 - val_loss: 28.6248 - val_MinusLogProbMetric: 28.6248 - lr: 6.2500e-05 - 30s/epoch - 154ms/step
Epoch 863/1000
2023-09-30 05:30:22.113 
Epoch 863/1000 
	 loss: 28.0753, MinusLogProbMetric: 28.0753, val_loss: 28.6343, val_MinusLogProbMetric: 28.6343

Epoch 863: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0753 - MinusLogProbMetric: 28.0753 - val_loss: 28.6343 - val_MinusLogProbMetric: 28.6343 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 864/1000
2023-09-30 05:30:54.033 
Epoch 864/1000 
	 loss: 28.0800, MinusLogProbMetric: 28.0800, val_loss: 28.6236, val_MinusLogProbMetric: 28.6236

Epoch 864: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0800 - MinusLogProbMetric: 28.0800 - val_loss: 28.6236 - val_MinusLogProbMetric: 28.6236 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 865/1000
2023-09-30 05:31:23.853 
Epoch 865/1000 
	 loss: 28.0788, MinusLogProbMetric: 28.0788, val_loss: 28.6254, val_MinusLogProbMetric: 28.6254

Epoch 865: val_loss did not improve from 28.59625
196/196 - 30s - loss: 28.0788 - MinusLogProbMetric: 28.0788 - val_loss: 28.6254 - val_MinusLogProbMetric: 28.6254 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 866/1000
2023-09-30 05:31:52.740 
Epoch 866/1000 
	 loss: 28.0775, MinusLogProbMetric: 28.0775, val_loss: 28.6275, val_MinusLogProbMetric: 28.6275

Epoch 866: val_loss did not improve from 28.59625
196/196 - 29s - loss: 28.0775 - MinusLogProbMetric: 28.0775 - val_loss: 28.6275 - val_MinusLogProbMetric: 28.6275 - lr: 6.2500e-05 - 29s/epoch - 147ms/step
Epoch 867/1000
2023-09-30 05:32:21.820 
Epoch 867/1000 
	 loss: 28.0728, MinusLogProbMetric: 28.0728, val_loss: 28.6250, val_MinusLogProbMetric: 28.6250

Epoch 867: val_loss did not improve from 28.59625
196/196 - 29s - loss: 28.0728 - MinusLogProbMetric: 28.0728 - val_loss: 28.6250 - val_MinusLogProbMetric: 28.6250 - lr: 6.2500e-05 - 29s/epoch - 148ms/step
Epoch 868/1000
2023-09-30 05:32:50.829 
Epoch 868/1000 
	 loss: 28.0776, MinusLogProbMetric: 28.0776, val_loss: 28.6162, val_MinusLogProbMetric: 28.6162

Epoch 868: val_loss did not improve from 28.59625
196/196 - 29s - loss: 28.0776 - MinusLogProbMetric: 28.0776 - val_loss: 28.6162 - val_MinusLogProbMetric: 28.6162 - lr: 6.2500e-05 - 29s/epoch - 148ms/step
Epoch 869/1000
2023-09-30 05:33:20.781 
Epoch 869/1000 
	 loss: 28.0811, MinusLogProbMetric: 28.0811, val_loss: 28.6015, val_MinusLogProbMetric: 28.6015

Epoch 869: val_loss did not improve from 28.59625
196/196 - 30s - loss: 28.0811 - MinusLogProbMetric: 28.0811 - val_loss: 28.6015 - val_MinusLogProbMetric: 28.6015 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 870/1000
2023-09-30 05:33:50.748 
Epoch 870/1000 
	 loss: 28.0739, MinusLogProbMetric: 28.0739, val_loss: 28.6091, val_MinusLogProbMetric: 28.6091

Epoch 870: val_loss did not improve from 28.59625
196/196 - 30s - loss: 28.0739 - MinusLogProbMetric: 28.0739 - val_loss: 28.6091 - val_MinusLogProbMetric: 28.6091 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 871/1000
2023-09-30 05:34:18.940 
Epoch 871/1000 
	 loss: 28.0600, MinusLogProbMetric: 28.0600, val_loss: 28.6218, val_MinusLogProbMetric: 28.6218

Epoch 871: val_loss did not improve from 28.59625
196/196 - 28s - loss: 28.0600 - MinusLogProbMetric: 28.0600 - val_loss: 28.6218 - val_MinusLogProbMetric: 28.6218 - lr: 3.1250e-05 - 28s/epoch - 144ms/step
Epoch 872/1000
2023-09-30 05:34:51.619 
Epoch 872/1000 
	 loss: 28.0613, MinusLogProbMetric: 28.0613, val_loss: 28.6126, val_MinusLogProbMetric: 28.6126

Epoch 872: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0613 - MinusLogProbMetric: 28.0613 - val_loss: 28.6126 - val_MinusLogProbMetric: 28.6126 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 873/1000
2023-09-30 05:35:24.466 
Epoch 873/1000 
	 loss: 28.0585, MinusLogProbMetric: 28.0585, val_loss: 28.6156, val_MinusLogProbMetric: 28.6156

Epoch 873: val_loss did not improve from 28.59625
196/196 - 33s - loss: 28.0585 - MinusLogProbMetric: 28.0585 - val_loss: 28.6156 - val_MinusLogProbMetric: 28.6156 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 874/1000
2023-09-30 05:35:55.739 
Epoch 874/1000 
	 loss: 28.0589, MinusLogProbMetric: 28.0589, val_loss: 28.6258, val_MinusLogProbMetric: 28.6258

Epoch 874: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0589 - MinusLogProbMetric: 28.0589 - val_loss: 28.6258 - val_MinusLogProbMetric: 28.6258 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 875/1000
2023-09-30 05:36:25.565 
Epoch 875/1000 
	 loss: 28.0550, MinusLogProbMetric: 28.0550, val_loss: 28.6067, val_MinusLogProbMetric: 28.6067

Epoch 875: val_loss did not improve from 28.59625
196/196 - 30s - loss: 28.0550 - MinusLogProbMetric: 28.0550 - val_loss: 28.6067 - val_MinusLogProbMetric: 28.6067 - lr: 3.1250e-05 - 30s/epoch - 152ms/step
Epoch 876/1000
2023-09-30 05:36:59.569 
Epoch 876/1000 
	 loss: 28.0563, MinusLogProbMetric: 28.0563, val_loss: 28.6055, val_MinusLogProbMetric: 28.6055

Epoch 876: val_loss did not improve from 28.59625
196/196 - 34s - loss: 28.0563 - MinusLogProbMetric: 28.0563 - val_loss: 28.6055 - val_MinusLogProbMetric: 28.6055 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 877/1000
2023-09-30 05:37:31.008 
Epoch 877/1000 
	 loss: 28.0531, MinusLogProbMetric: 28.0531, val_loss: 28.6115, val_MinusLogProbMetric: 28.6115

Epoch 877: val_loss did not improve from 28.59625
196/196 - 31s - loss: 28.0531 - MinusLogProbMetric: 28.0531 - val_loss: 28.6115 - val_MinusLogProbMetric: 28.6115 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 878/1000
2023-09-30 05:38:03.288 
Epoch 878/1000 
	 loss: 28.0530, MinusLogProbMetric: 28.0530, val_loss: 28.6118, val_MinusLogProbMetric: 28.6118

Epoch 878: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0530 - MinusLogProbMetric: 28.0530 - val_loss: 28.6118 - val_MinusLogProbMetric: 28.6118 - lr: 3.1250e-05 - 32s/epoch - 165ms/step
Epoch 879/1000
2023-09-30 05:38:35.245 
Epoch 879/1000 
	 loss: 28.0534, MinusLogProbMetric: 28.0534, val_loss: 28.6134, val_MinusLogProbMetric: 28.6134

Epoch 879: val_loss did not improve from 28.59625
196/196 - 32s - loss: 28.0534 - MinusLogProbMetric: 28.0534 - val_loss: 28.6134 - val_MinusLogProbMetric: 28.6134 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 880/1000
2023-09-30 05:39:04.549 
Epoch 880/1000 
	 loss: 28.0555, MinusLogProbMetric: 28.0555, val_loss: 28.5878, val_MinusLogProbMetric: 28.5878

Epoch 880: val_loss improved from 28.59625 to 28.58780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 30s - loss: 28.0555 - MinusLogProbMetric: 28.0555 - val_loss: 28.5878 - val_MinusLogProbMetric: 28.5878 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 881/1000
2023-09-30 05:39:34.572 
Epoch 881/1000 
	 loss: 28.0552, MinusLogProbMetric: 28.0552, val_loss: 28.6110, val_MinusLogProbMetric: 28.6110

Epoch 881: val_loss did not improve from 28.58780
196/196 - 29s - loss: 28.0552 - MinusLogProbMetric: 28.0552 - val_loss: 28.6110 - val_MinusLogProbMetric: 28.6110 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 882/1000
2023-09-30 05:40:05.437 
Epoch 882/1000 
	 loss: 28.0523, MinusLogProbMetric: 28.0523, val_loss: 28.6123, val_MinusLogProbMetric: 28.6123

Epoch 882: val_loss did not improve from 28.58780
196/196 - 31s - loss: 28.0523 - MinusLogProbMetric: 28.0523 - val_loss: 28.6123 - val_MinusLogProbMetric: 28.6123 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 883/1000
2023-09-30 05:40:36.475 
Epoch 883/1000 
	 loss: 28.0584, MinusLogProbMetric: 28.0584, val_loss: 28.6058, val_MinusLogProbMetric: 28.6058

Epoch 883: val_loss did not improve from 28.58780
196/196 - 31s - loss: 28.0584 - MinusLogProbMetric: 28.0584 - val_loss: 28.6058 - val_MinusLogProbMetric: 28.6058 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 884/1000
2023-09-30 05:41:08.159 
Epoch 884/1000 
	 loss: 28.0531, MinusLogProbMetric: 28.0531, val_loss: 28.6152, val_MinusLogProbMetric: 28.6152

Epoch 884: val_loss did not improve from 28.58780
196/196 - 32s - loss: 28.0531 - MinusLogProbMetric: 28.0531 - val_loss: 28.6152 - val_MinusLogProbMetric: 28.6152 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 885/1000
2023-09-30 05:41:37.810 
Epoch 885/1000 
	 loss: 28.0585, MinusLogProbMetric: 28.0585, val_loss: 28.6072, val_MinusLogProbMetric: 28.6072

Epoch 885: val_loss did not improve from 28.58780
196/196 - 30s - loss: 28.0585 - MinusLogProbMetric: 28.0585 - val_loss: 28.6072 - val_MinusLogProbMetric: 28.6072 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 886/1000
2023-09-30 05:42:08.362 
Epoch 886/1000 
	 loss: 28.0518, MinusLogProbMetric: 28.0518, val_loss: 28.6003, val_MinusLogProbMetric: 28.6003

Epoch 886: val_loss did not improve from 28.58780
196/196 - 31s - loss: 28.0518 - MinusLogProbMetric: 28.0518 - val_loss: 28.6003 - val_MinusLogProbMetric: 28.6003 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 887/1000
2023-09-30 05:42:40.148 
Epoch 887/1000 
	 loss: 28.0542, MinusLogProbMetric: 28.0542, val_loss: 28.6153, val_MinusLogProbMetric: 28.6153

Epoch 887: val_loss did not improve from 28.58780
196/196 - 32s - loss: 28.0542 - MinusLogProbMetric: 28.0542 - val_loss: 28.6153 - val_MinusLogProbMetric: 28.6153 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 888/1000
2023-09-30 05:43:08.066 
Epoch 888/1000 
	 loss: 28.0537, MinusLogProbMetric: 28.0537, val_loss: 28.6120, val_MinusLogProbMetric: 28.6120

Epoch 888: val_loss did not improve from 28.58780
196/196 - 28s - loss: 28.0537 - MinusLogProbMetric: 28.0537 - val_loss: 28.6120 - val_MinusLogProbMetric: 28.6120 - lr: 3.1250e-05 - 28s/epoch - 142ms/step
Epoch 889/1000
2023-09-30 05:43:38.736 
Epoch 889/1000 
	 loss: 28.0579, MinusLogProbMetric: 28.0579, val_loss: 28.6316, val_MinusLogProbMetric: 28.6316

Epoch 889: val_loss did not improve from 28.58780
196/196 - 31s - loss: 28.0579 - MinusLogProbMetric: 28.0579 - val_loss: 28.6316 - val_MinusLogProbMetric: 28.6316 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 890/1000
2023-09-30 05:44:11.515 
Epoch 890/1000 
	 loss: 28.0544, MinusLogProbMetric: 28.0544, val_loss: 28.6082, val_MinusLogProbMetric: 28.6082

Epoch 890: val_loss did not improve from 28.58780
196/196 - 33s - loss: 28.0544 - MinusLogProbMetric: 28.0544 - val_loss: 28.6082 - val_MinusLogProbMetric: 28.6082 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 891/1000
2023-09-30 05:44:41.693 
Epoch 891/1000 
	 loss: 28.0554, MinusLogProbMetric: 28.0554, val_loss: 28.6017, val_MinusLogProbMetric: 28.6017

Epoch 891: val_loss did not improve from 28.58780
196/196 - 30s - loss: 28.0554 - MinusLogProbMetric: 28.0554 - val_loss: 28.6017 - val_MinusLogProbMetric: 28.6017 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 892/1000
2023-09-30 05:45:11.020 
Epoch 892/1000 
	 loss: 28.0548, MinusLogProbMetric: 28.0548, val_loss: 28.6186, val_MinusLogProbMetric: 28.6186

Epoch 892: val_loss did not improve from 28.58780
196/196 - 29s - loss: 28.0548 - MinusLogProbMetric: 28.0548 - val_loss: 28.6186 - val_MinusLogProbMetric: 28.6186 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 893/1000
2023-09-30 05:45:39.646 
Epoch 893/1000 
	 loss: 28.0552, MinusLogProbMetric: 28.0552, val_loss: 28.6102, val_MinusLogProbMetric: 28.6102

Epoch 893: val_loss did not improve from 28.58780
196/196 - 29s - loss: 28.0552 - MinusLogProbMetric: 28.0552 - val_loss: 28.6102 - val_MinusLogProbMetric: 28.6102 - lr: 3.1250e-05 - 29s/epoch - 146ms/step
Epoch 894/1000
2023-09-30 05:46:08.104 
Epoch 894/1000 
	 loss: 28.0585, MinusLogProbMetric: 28.0585, val_loss: 28.6387, val_MinusLogProbMetric: 28.6387

Epoch 894: val_loss did not improve from 28.58780
196/196 - 28s - loss: 28.0585 - MinusLogProbMetric: 28.0585 - val_loss: 28.6387 - val_MinusLogProbMetric: 28.6387 - lr: 3.1250e-05 - 28s/epoch - 145ms/step
Epoch 895/1000
2023-09-30 05:46:37.655 
Epoch 895/1000 
	 loss: 28.0572, MinusLogProbMetric: 28.0572, val_loss: 28.5967, val_MinusLogProbMetric: 28.5967

Epoch 895: val_loss did not improve from 28.58780
196/196 - 30s - loss: 28.0572 - MinusLogProbMetric: 28.0572 - val_loss: 28.5967 - val_MinusLogProbMetric: 28.5967 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 896/1000
2023-09-30 05:47:09.827 
Epoch 896/1000 
	 loss: 28.0577, MinusLogProbMetric: 28.0577, val_loss: 28.6081, val_MinusLogProbMetric: 28.6081

Epoch 896: val_loss did not improve from 28.58780
196/196 - 32s - loss: 28.0577 - MinusLogProbMetric: 28.0577 - val_loss: 28.6081 - val_MinusLogProbMetric: 28.6081 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 897/1000
2023-09-30 05:47:41.774 
Epoch 897/1000 
	 loss: 28.0564, MinusLogProbMetric: 28.0564, val_loss: 28.5994, val_MinusLogProbMetric: 28.5994

Epoch 897: val_loss did not improve from 28.58780
196/196 - 32s - loss: 28.0564 - MinusLogProbMetric: 28.0564 - val_loss: 28.5994 - val_MinusLogProbMetric: 28.5994 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 898/1000
2023-09-30 05:48:13.016 
Epoch 898/1000 
	 loss: 28.0562, MinusLogProbMetric: 28.0562, val_loss: 28.6185, val_MinusLogProbMetric: 28.6185

Epoch 898: val_loss did not improve from 28.58780
196/196 - 31s - loss: 28.0562 - MinusLogProbMetric: 28.0562 - val_loss: 28.6185 - val_MinusLogProbMetric: 28.6185 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 899/1000
2023-09-30 05:48:42.574 
Epoch 899/1000 
	 loss: 28.0549, MinusLogProbMetric: 28.0549, val_loss: 28.5835, val_MinusLogProbMetric: 28.5835

Epoch 899: val_loss improved from 28.58780 to 28.58354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 30s - loss: 28.0549 - MinusLogProbMetric: 28.0549 - val_loss: 28.5835 - val_MinusLogProbMetric: 28.5835 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 900/1000
2023-09-30 05:49:14.245 
Epoch 900/1000 
	 loss: 28.0540, MinusLogProbMetric: 28.0540, val_loss: 28.6099, val_MinusLogProbMetric: 28.6099

Epoch 900: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0540 - MinusLogProbMetric: 28.0540 - val_loss: 28.6099 - val_MinusLogProbMetric: 28.6099 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 901/1000
2023-09-30 05:49:43.510 
Epoch 901/1000 
	 loss: 28.0553, MinusLogProbMetric: 28.0553, val_loss: 28.6092, val_MinusLogProbMetric: 28.6092

Epoch 901: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0553 - MinusLogProbMetric: 28.0553 - val_loss: 28.6092 - val_MinusLogProbMetric: 28.6092 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 902/1000
2023-09-30 05:50:17.407 
Epoch 902/1000 
	 loss: 28.0548, MinusLogProbMetric: 28.0548, val_loss: 28.6182, val_MinusLogProbMetric: 28.6182

Epoch 902: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0548 - MinusLogProbMetric: 28.0548 - val_loss: 28.6182 - val_MinusLogProbMetric: 28.6182 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 903/1000
2023-09-30 05:50:46.333 
Epoch 903/1000 
	 loss: 28.0573, MinusLogProbMetric: 28.0573, val_loss: 28.6045, val_MinusLogProbMetric: 28.6045

Epoch 903: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0573 - MinusLogProbMetric: 28.0573 - val_loss: 28.6045 - val_MinusLogProbMetric: 28.6045 - lr: 3.1250e-05 - 29s/epoch - 148ms/step
Epoch 904/1000
2023-09-30 05:51:15.588 
Epoch 904/1000 
	 loss: 28.0570, MinusLogProbMetric: 28.0570, val_loss: 28.6159, val_MinusLogProbMetric: 28.6159

Epoch 904: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0570 - MinusLogProbMetric: 28.0570 - val_loss: 28.6159 - val_MinusLogProbMetric: 28.6159 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 905/1000
2023-09-30 05:51:48.314 
Epoch 905/1000 
	 loss: 28.0540, MinusLogProbMetric: 28.0540, val_loss: 28.6147, val_MinusLogProbMetric: 28.6147

Epoch 905: val_loss did not improve from 28.58354
196/196 - 33s - loss: 28.0540 - MinusLogProbMetric: 28.0540 - val_loss: 28.6147 - val_MinusLogProbMetric: 28.6147 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 906/1000
2023-09-30 05:52:18.892 
Epoch 906/1000 
	 loss: 28.0541, MinusLogProbMetric: 28.0541, val_loss: 28.5952, val_MinusLogProbMetric: 28.5952

Epoch 906: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0541 - MinusLogProbMetric: 28.0541 - val_loss: 28.5952 - val_MinusLogProbMetric: 28.5952 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 907/1000
2023-09-30 05:52:47.873 
Epoch 907/1000 
	 loss: 28.0551, MinusLogProbMetric: 28.0551, val_loss: 28.6204, val_MinusLogProbMetric: 28.6204

Epoch 907: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0551 - MinusLogProbMetric: 28.0551 - val_loss: 28.6204 - val_MinusLogProbMetric: 28.6204 - lr: 3.1250e-05 - 29s/epoch - 148ms/step
Epoch 908/1000
2023-09-30 05:53:17.851 
Epoch 908/1000 
	 loss: 28.0542, MinusLogProbMetric: 28.0542, val_loss: 28.6111, val_MinusLogProbMetric: 28.6111

Epoch 908: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0542 - MinusLogProbMetric: 28.0542 - val_loss: 28.6111 - val_MinusLogProbMetric: 28.6111 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 909/1000
2023-09-30 05:53:47.760 
Epoch 909/1000 
	 loss: 28.0539, MinusLogProbMetric: 28.0539, val_loss: 28.6230, val_MinusLogProbMetric: 28.6230

Epoch 909: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0539 - MinusLogProbMetric: 28.0539 - val_loss: 28.6230 - val_MinusLogProbMetric: 28.6230 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 910/1000
2023-09-30 05:54:16.518 
Epoch 910/1000 
	 loss: 28.0582, MinusLogProbMetric: 28.0582, val_loss: 28.5992, val_MinusLogProbMetric: 28.5992

Epoch 910: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0582 - MinusLogProbMetric: 28.0582 - val_loss: 28.5992 - val_MinusLogProbMetric: 28.5992 - lr: 3.1250e-05 - 29s/epoch - 147ms/step
Epoch 911/1000
2023-09-30 05:54:44.357 
Epoch 911/1000 
	 loss: 28.0547, MinusLogProbMetric: 28.0547, val_loss: 28.5987, val_MinusLogProbMetric: 28.5987

Epoch 911: val_loss did not improve from 28.58354
196/196 - 28s - loss: 28.0547 - MinusLogProbMetric: 28.0547 - val_loss: 28.5987 - val_MinusLogProbMetric: 28.5987 - lr: 3.1250e-05 - 28s/epoch - 142ms/step
Epoch 912/1000
2023-09-30 05:55:13.531 
Epoch 912/1000 
	 loss: 28.0572, MinusLogProbMetric: 28.0572, val_loss: 28.6072, val_MinusLogProbMetric: 28.6072

Epoch 912: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0572 - MinusLogProbMetric: 28.0572 - val_loss: 28.6072 - val_MinusLogProbMetric: 28.6072 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 913/1000
2023-09-30 05:55:43.583 
Epoch 913/1000 
	 loss: 28.0542, MinusLogProbMetric: 28.0542, val_loss: 28.6083, val_MinusLogProbMetric: 28.6083

Epoch 913: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0542 - MinusLogProbMetric: 28.0542 - val_loss: 28.6083 - val_MinusLogProbMetric: 28.6083 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 914/1000
2023-09-30 05:56:12.125 
Epoch 914/1000 
	 loss: 28.0555, MinusLogProbMetric: 28.0555, val_loss: 28.6106, val_MinusLogProbMetric: 28.6106

Epoch 914: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0555 - MinusLogProbMetric: 28.0555 - val_loss: 28.6106 - val_MinusLogProbMetric: 28.6106 - lr: 3.1250e-05 - 29s/epoch - 146ms/step
Epoch 915/1000
2023-09-30 05:56:40.401 
Epoch 915/1000 
	 loss: 28.0534, MinusLogProbMetric: 28.0534, val_loss: 28.6231, val_MinusLogProbMetric: 28.6231

Epoch 915: val_loss did not improve from 28.58354
196/196 - 28s - loss: 28.0534 - MinusLogProbMetric: 28.0534 - val_loss: 28.6231 - val_MinusLogProbMetric: 28.6231 - lr: 3.1250e-05 - 28s/epoch - 144ms/step
Epoch 916/1000
2023-09-30 05:57:11.095 
Epoch 916/1000 
	 loss: 28.0532, MinusLogProbMetric: 28.0532, val_loss: 28.6243, val_MinusLogProbMetric: 28.6243

Epoch 916: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0532 - MinusLogProbMetric: 28.0532 - val_loss: 28.6243 - val_MinusLogProbMetric: 28.6243 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 917/1000
2023-09-30 05:57:41.526 
Epoch 917/1000 
	 loss: 28.0525, MinusLogProbMetric: 28.0525, val_loss: 28.6121, val_MinusLogProbMetric: 28.6121

Epoch 917: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0525 - MinusLogProbMetric: 28.0525 - val_loss: 28.6121 - val_MinusLogProbMetric: 28.6121 - lr: 3.1250e-05 - 30s/epoch - 155ms/step
Epoch 918/1000
2023-09-30 05:58:12.250 
Epoch 918/1000 
	 loss: 28.0526, MinusLogProbMetric: 28.0526, val_loss: 28.6153, val_MinusLogProbMetric: 28.6153

Epoch 918: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0526 - MinusLogProbMetric: 28.0526 - val_loss: 28.6153 - val_MinusLogProbMetric: 28.6153 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 919/1000
2023-09-30 05:58:42.202 
Epoch 919/1000 
	 loss: 28.0525, MinusLogProbMetric: 28.0525, val_loss: 28.6061, val_MinusLogProbMetric: 28.6061

Epoch 919: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0525 - MinusLogProbMetric: 28.0525 - val_loss: 28.6061 - val_MinusLogProbMetric: 28.6061 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 920/1000
2023-09-30 05:59:12.411 
Epoch 920/1000 
	 loss: 28.0514, MinusLogProbMetric: 28.0514, val_loss: 28.6288, val_MinusLogProbMetric: 28.6288

Epoch 920: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0514 - MinusLogProbMetric: 28.0514 - val_loss: 28.6288 - val_MinusLogProbMetric: 28.6288 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 921/1000
2023-09-30 05:59:41.656 
Epoch 921/1000 
	 loss: 28.0505, MinusLogProbMetric: 28.0505, val_loss: 28.6079, val_MinusLogProbMetric: 28.6079

Epoch 921: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0505 - MinusLogProbMetric: 28.0505 - val_loss: 28.6079 - val_MinusLogProbMetric: 28.6079 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 922/1000
2023-09-30 06:00:11.268 
Epoch 922/1000 
	 loss: 28.0542, MinusLogProbMetric: 28.0542, val_loss: 28.6056, val_MinusLogProbMetric: 28.6056

Epoch 922: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0542 - MinusLogProbMetric: 28.0542 - val_loss: 28.6056 - val_MinusLogProbMetric: 28.6056 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 923/1000
2023-09-30 06:00:42.737 
Epoch 923/1000 
	 loss: 28.0525, MinusLogProbMetric: 28.0525, val_loss: 28.6222, val_MinusLogProbMetric: 28.6222

Epoch 923: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0525 - MinusLogProbMetric: 28.0525 - val_loss: 28.6222 - val_MinusLogProbMetric: 28.6222 - lr: 3.1250e-05 - 31s/epoch - 161ms/step
Epoch 924/1000
2023-09-30 06:01:11.552 
Epoch 924/1000 
	 loss: 28.0531, MinusLogProbMetric: 28.0531, val_loss: 28.6236, val_MinusLogProbMetric: 28.6236

Epoch 924: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0531 - MinusLogProbMetric: 28.0531 - val_loss: 28.6236 - val_MinusLogProbMetric: 28.6236 - lr: 3.1250e-05 - 29s/epoch - 147ms/step
Epoch 925/1000
2023-09-30 06:01:41.580 
Epoch 925/1000 
	 loss: 28.0533, MinusLogProbMetric: 28.0533, val_loss: 28.6100, val_MinusLogProbMetric: 28.6100

Epoch 925: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0533 - MinusLogProbMetric: 28.0533 - val_loss: 28.6100 - val_MinusLogProbMetric: 28.6100 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 926/1000
2023-09-30 06:02:13.452 
Epoch 926/1000 
	 loss: 28.0541, MinusLogProbMetric: 28.0541, val_loss: 28.6109, val_MinusLogProbMetric: 28.6109

Epoch 926: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0541 - MinusLogProbMetric: 28.0541 - val_loss: 28.6109 - val_MinusLogProbMetric: 28.6109 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 927/1000
2023-09-30 06:02:43.568 
Epoch 927/1000 
	 loss: 28.0528, MinusLogProbMetric: 28.0528, val_loss: 28.6211, val_MinusLogProbMetric: 28.6211

Epoch 927: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0528 - MinusLogProbMetric: 28.0528 - val_loss: 28.6211 - val_MinusLogProbMetric: 28.6211 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 928/1000
2023-09-30 06:03:13.438 
Epoch 928/1000 
	 loss: 28.0515, MinusLogProbMetric: 28.0515, val_loss: 28.6085, val_MinusLogProbMetric: 28.6085

Epoch 928: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0515 - MinusLogProbMetric: 28.0515 - val_loss: 28.6085 - val_MinusLogProbMetric: 28.6085 - lr: 3.1250e-05 - 30s/epoch - 152ms/step
Epoch 929/1000
2023-09-30 06:03:42.834 
Epoch 929/1000 
	 loss: 28.0541, MinusLogProbMetric: 28.0541, val_loss: 28.6131, val_MinusLogProbMetric: 28.6131

Epoch 929: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0541 - MinusLogProbMetric: 28.0541 - val_loss: 28.6131 - val_MinusLogProbMetric: 28.6131 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 930/1000
2023-09-30 06:04:13.298 
Epoch 930/1000 
	 loss: 28.0538, MinusLogProbMetric: 28.0538, val_loss: 28.6047, val_MinusLogProbMetric: 28.6047

Epoch 930: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0538 - MinusLogProbMetric: 28.0538 - val_loss: 28.6047 - val_MinusLogProbMetric: 28.6047 - lr: 3.1250e-05 - 30s/epoch - 155ms/step
Epoch 931/1000
2023-09-30 06:04:44.231 
Epoch 931/1000 
	 loss: 28.0521, MinusLogProbMetric: 28.0521, val_loss: 28.6162, val_MinusLogProbMetric: 28.6162

Epoch 931: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0521 - MinusLogProbMetric: 28.0521 - val_loss: 28.6162 - val_MinusLogProbMetric: 28.6162 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 932/1000
2023-09-30 06:05:13.741 
Epoch 932/1000 
	 loss: 28.0495, MinusLogProbMetric: 28.0495, val_loss: 28.6096, val_MinusLogProbMetric: 28.6096

Epoch 932: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0495 - MinusLogProbMetric: 28.0495 - val_loss: 28.6096 - val_MinusLogProbMetric: 28.6096 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 933/1000
2023-09-30 06:05:45.237 
Epoch 933/1000 
	 loss: 28.0526, MinusLogProbMetric: 28.0526, val_loss: 28.6215, val_MinusLogProbMetric: 28.6215

Epoch 933: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0526 - MinusLogProbMetric: 28.0526 - val_loss: 28.6215 - val_MinusLogProbMetric: 28.6215 - lr: 3.1250e-05 - 31s/epoch - 161ms/step
Epoch 934/1000
2023-09-30 06:06:16.512 
Epoch 934/1000 
	 loss: 28.0536, MinusLogProbMetric: 28.0536, val_loss: 28.6129, val_MinusLogProbMetric: 28.6129

Epoch 934: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0536 - MinusLogProbMetric: 28.0536 - val_loss: 28.6129 - val_MinusLogProbMetric: 28.6129 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 935/1000
2023-09-30 06:06:46.475 
Epoch 935/1000 
	 loss: 28.0532, MinusLogProbMetric: 28.0532, val_loss: 28.6440, val_MinusLogProbMetric: 28.6440

Epoch 935: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0532 - MinusLogProbMetric: 28.0532 - val_loss: 28.6440 - val_MinusLogProbMetric: 28.6440 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 936/1000
2023-09-30 06:07:16.099 
Epoch 936/1000 
	 loss: 28.0539, MinusLogProbMetric: 28.0539, val_loss: 28.5982, val_MinusLogProbMetric: 28.5982

Epoch 936: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0539 - MinusLogProbMetric: 28.0539 - val_loss: 28.5982 - val_MinusLogProbMetric: 28.5982 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 937/1000
2023-09-30 06:07:45.234 
Epoch 937/1000 
	 loss: 28.0527, MinusLogProbMetric: 28.0527, val_loss: 28.6006, val_MinusLogProbMetric: 28.6006

Epoch 937: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0527 - MinusLogProbMetric: 28.0527 - val_loss: 28.6006 - val_MinusLogProbMetric: 28.6006 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 938/1000
2023-09-30 06:08:14.070 
Epoch 938/1000 
	 loss: 28.0518, MinusLogProbMetric: 28.0518, val_loss: 28.6073, val_MinusLogProbMetric: 28.6073

Epoch 938: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0518 - MinusLogProbMetric: 28.0518 - val_loss: 28.6073 - val_MinusLogProbMetric: 28.6073 - lr: 3.1250e-05 - 29s/epoch - 147ms/step
Epoch 939/1000
2023-09-30 06:08:43.478 
Epoch 939/1000 
	 loss: 28.0547, MinusLogProbMetric: 28.0547, val_loss: 28.6185, val_MinusLogProbMetric: 28.6185

Epoch 939: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0547 - MinusLogProbMetric: 28.0547 - val_loss: 28.6185 - val_MinusLogProbMetric: 28.6185 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 940/1000
2023-09-30 06:09:15.524 
Epoch 940/1000 
	 loss: 28.0542, MinusLogProbMetric: 28.0542, val_loss: 28.5949, val_MinusLogProbMetric: 28.5949

Epoch 940: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0542 - MinusLogProbMetric: 28.0542 - val_loss: 28.5949 - val_MinusLogProbMetric: 28.5949 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 941/1000
2023-09-30 06:09:45.134 
Epoch 941/1000 
	 loss: 28.0530, MinusLogProbMetric: 28.0530, val_loss: 28.6075, val_MinusLogProbMetric: 28.6075

Epoch 941: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0530 - MinusLogProbMetric: 28.0530 - val_loss: 28.6075 - val_MinusLogProbMetric: 28.6075 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 942/1000
2023-09-30 06:10:13.818 
Epoch 942/1000 
	 loss: 28.0482, MinusLogProbMetric: 28.0482, val_loss: 28.5927, val_MinusLogProbMetric: 28.5927

Epoch 942: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0482 - MinusLogProbMetric: 28.0482 - val_loss: 28.5927 - val_MinusLogProbMetric: 28.5927 - lr: 3.1250e-05 - 29s/epoch - 146ms/step
Epoch 943/1000
2023-09-30 06:10:44.086 
Epoch 943/1000 
	 loss: 28.0506, MinusLogProbMetric: 28.0506, val_loss: 28.6166, val_MinusLogProbMetric: 28.6166

Epoch 943: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0506 - MinusLogProbMetric: 28.0506 - val_loss: 28.6166 - val_MinusLogProbMetric: 28.6166 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 944/1000
2023-09-30 06:11:14.649 
Epoch 944/1000 
	 loss: 28.0494, MinusLogProbMetric: 28.0494, val_loss: 28.5998, val_MinusLogProbMetric: 28.5998

Epoch 944: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0494 - MinusLogProbMetric: 28.0494 - val_loss: 28.5998 - val_MinusLogProbMetric: 28.5998 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 945/1000
2023-09-30 06:11:44.348 
Epoch 945/1000 
	 loss: 28.0475, MinusLogProbMetric: 28.0475, val_loss: 28.6048, val_MinusLogProbMetric: 28.6048

Epoch 945: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0475 - MinusLogProbMetric: 28.0475 - val_loss: 28.6048 - val_MinusLogProbMetric: 28.6048 - lr: 3.1250e-05 - 30s/epoch - 152ms/step
Epoch 946/1000
2023-09-30 06:12:13.686 
Epoch 946/1000 
	 loss: 28.0520, MinusLogProbMetric: 28.0520, val_loss: 28.6044, val_MinusLogProbMetric: 28.6044

Epoch 946: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0520 - MinusLogProbMetric: 28.0520 - val_loss: 28.6044 - val_MinusLogProbMetric: 28.6044 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 947/1000
2023-09-30 06:12:42.978 
Epoch 947/1000 
	 loss: 28.0514, MinusLogProbMetric: 28.0514, val_loss: 28.6175, val_MinusLogProbMetric: 28.6175

Epoch 947: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0514 - MinusLogProbMetric: 28.0514 - val_loss: 28.6175 - val_MinusLogProbMetric: 28.6175 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 948/1000
2023-09-30 06:13:12.002 
Epoch 948/1000 
	 loss: 28.0510, MinusLogProbMetric: 28.0510, val_loss: 28.6067, val_MinusLogProbMetric: 28.6067

Epoch 948: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0510 - MinusLogProbMetric: 28.0510 - val_loss: 28.6067 - val_MinusLogProbMetric: 28.6067 - lr: 3.1250e-05 - 29s/epoch - 148ms/step
Epoch 949/1000
2023-09-30 06:13:39.483 
Epoch 949/1000 
	 loss: 28.0498, MinusLogProbMetric: 28.0498, val_loss: 28.6200, val_MinusLogProbMetric: 28.6200

Epoch 949: val_loss did not improve from 28.58354
196/196 - 27s - loss: 28.0498 - MinusLogProbMetric: 28.0498 - val_loss: 28.6200 - val_MinusLogProbMetric: 28.6200 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 950/1000
2023-09-30 06:14:07.001 
Epoch 950/1000 
	 loss: 28.0412, MinusLogProbMetric: 28.0412, val_loss: 28.6220, val_MinusLogProbMetric: 28.6220

Epoch 950: val_loss did not improve from 28.58354
196/196 - 28s - loss: 28.0412 - MinusLogProbMetric: 28.0412 - val_loss: 28.6220 - val_MinusLogProbMetric: 28.6220 - lr: 1.5625e-05 - 28s/epoch - 140ms/step
Epoch 951/1000
2023-09-30 06:14:36.363 
Epoch 951/1000 
	 loss: 28.0406, MinusLogProbMetric: 28.0406, val_loss: 28.6155, val_MinusLogProbMetric: 28.6155

Epoch 951: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0406 - MinusLogProbMetric: 28.0406 - val_loss: 28.6155 - val_MinusLogProbMetric: 28.6155 - lr: 1.5625e-05 - 29s/epoch - 150ms/step
Epoch 952/1000
2023-09-30 06:15:06.276 
Epoch 952/1000 
	 loss: 28.0414, MinusLogProbMetric: 28.0414, val_loss: 28.6067, val_MinusLogProbMetric: 28.6067

Epoch 952: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0414 - MinusLogProbMetric: 28.0414 - val_loss: 28.6067 - val_MinusLogProbMetric: 28.6067 - lr: 1.5625e-05 - 30s/epoch - 153ms/step
Epoch 953/1000
2023-09-30 06:15:34.227 
Epoch 953/1000 
	 loss: 28.0395, MinusLogProbMetric: 28.0395, val_loss: 28.6030, val_MinusLogProbMetric: 28.6030

Epoch 953: val_loss did not improve from 28.58354
196/196 - 28s - loss: 28.0395 - MinusLogProbMetric: 28.0395 - val_loss: 28.6030 - val_MinusLogProbMetric: 28.6030 - lr: 1.5625e-05 - 28s/epoch - 143ms/step
Epoch 954/1000
2023-09-30 06:16:04.103 
Epoch 954/1000 
	 loss: 28.0433, MinusLogProbMetric: 28.0433, val_loss: 28.6062, val_MinusLogProbMetric: 28.6062

Epoch 954: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0433 - MinusLogProbMetric: 28.0433 - val_loss: 28.6062 - val_MinusLogProbMetric: 28.6062 - lr: 1.5625e-05 - 30s/epoch - 152ms/step
Epoch 955/1000
2023-09-30 06:16:33.605 
Epoch 955/1000 
	 loss: 28.0422, MinusLogProbMetric: 28.0422, val_loss: 28.6110, val_MinusLogProbMetric: 28.6110

Epoch 955: val_loss did not improve from 28.58354
196/196 - 29s - loss: 28.0422 - MinusLogProbMetric: 28.0422 - val_loss: 28.6110 - val_MinusLogProbMetric: 28.6110 - lr: 1.5625e-05 - 29s/epoch - 151ms/step
Epoch 956/1000
2023-09-30 06:17:04.058 
Epoch 956/1000 
	 loss: 28.0418, MinusLogProbMetric: 28.0418, val_loss: 28.6077, val_MinusLogProbMetric: 28.6077

Epoch 956: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0418 - MinusLogProbMetric: 28.0418 - val_loss: 28.6077 - val_MinusLogProbMetric: 28.6077 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 957/1000
2023-09-30 06:17:34.463 
Epoch 957/1000 
	 loss: 28.0436, MinusLogProbMetric: 28.0436, val_loss: 28.6123, val_MinusLogProbMetric: 28.6123

Epoch 957: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0436 - MinusLogProbMetric: 28.0436 - val_loss: 28.6123 - val_MinusLogProbMetric: 28.6123 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 958/1000
2023-09-30 06:18:02.565 
Epoch 958/1000 
	 loss: 28.0396, MinusLogProbMetric: 28.0396, val_loss: 28.5980, val_MinusLogProbMetric: 28.5980

Epoch 958: val_loss did not improve from 28.58354
196/196 - 28s - loss: 28.0396 - MinusLogProbMetric: 28.0396 - val_loss: 28.5980 - val_MinusLogProbMetric: 28.5980 - lr: 1.5625e-05 - 28s/epoch - 143ms/step
Epoch 959/1000
2023-09-30 06:18:29.894 
Epoch 959/1000 
	 loss: 28.0411, MinusLogProbMetric: 28.0411, val_loss: 28.6065, val_MinusLogProbMetric: 28.6065

Epoch 959: val_loss did not improve from 28.58354
196/196 - 27s - loss: 28.0411 - MinusLogProbMetric: 28.0411 - val_loss: 28.6065 - val_MinusLogProbMetric: 28.6065 - lr: 1.5625e-05 - 27s/epoch - 139ms/step
Epoch 960/1000
2023-09-30 06:19:03.177 
Epoch 960/1000 
	 loss: 28.0425, MinusLogProbMetric: 28.0425, val_loss: 28.5967, val_MinusLogProbMetric: 28.5967

Epoch 960: val_loss did not improve from 28.58354
196/196 - 33s - loss: 28.0425 - MinusLogProbMetric: 28.0425 - val_loss: 28.5967 - val_MinusLogProbMetric: 28.5967 - lr: 1.5625e-05 - 33s/epoch - 170ms/step
Epoch 961/1000
2023-09-30 06:19:35.072 
Epoch 961/1000 
	 loss: 28.0403, MinusLogProbMetric: 28.0403, val_loss: 28.6024, val_MinusLogProbMetric: 28.6024

Epoch 961: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0403 - MinusLogProbMetric: 28.0403 - val_loss: 28.6024 - val_MinusLogProbMetric: 28.6024 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 962/1000
2023-09-30 06:20:05.960 
Epoch 962/1000 
	 loss: 28.0429, MinusLogProbMetric: 28.0429, val_loss: 28.6007, val_MinusLogProbMetric: 28.6007

Epoch 962: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0429 - MinusLogProbMetric: 28.0429 - val_loss: 28.6007 - val_MinusLogProbMetric: 28.6007 - lr: 1.5625e-05 - 31s/epoch - 158ms/step
Epoch 963/1000
2023-09-30 06:20:35.739 
Epoch 963/1000 
	 loss: 28.0414, MinusLogProbMetric: 28.0414, val_loss: 28.6059, val_MinusLogProbMetric: 28.6059

Epoch 963: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0414 - MinusLogProbMetric: 28.0414 - val_loss: 28.6059 - val_MinusLogProbMetric: 28.6059 - lr: 1.5625e-05 - 30s/epoch - 152ms/step
Epoch 964/1000
2023-09-30 06:21:05.430 
Epoch 964/1000 
	 loss: 28.0411, MinusLogProbMetric: 28.0411, val_loss: 28.6045, val_MinusLogProbMetric: 28.6045

Epoch 964: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0411 - MinusLogProbMetric: 28.0411 - val_loss: 28.6045 - val_MinusLogProbMetric: 28.6045 - lr: 1.5625e-05 - 30s/epoch - 151ms/step
Epoch 965/1000
2023-09-30 06:21:36.993 
Epoch 965/1000 
	 loss: 28.0408, MinusLogProbMetric: 28.0408, val_loss: 28.6087, val_MinusLogProbMetric: 28.6087

Epoch 965: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0408 - MinusLogProbMetric: 28.0408 - val_loss: 28.6087 - val_MinusLogProbMetric: 28.6087 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 966/1000
2023-09-30 06:22:06.693 
Epoch 966/1000 
	 loss: 28.0412, MinusLogProbMetric: 28.0412, val_loss: 28.6031, val_MinusLogProbMetric: 28.6031

Epoch 966: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0412 - MinusLogProbMetric: 28.0412 - val_loss: 28.6031 - val_MinusLogProbMetric: 28.6031 - lr: 1.5625e-05 - 30s/epoch - 152ms/step
Epoch 967/1000
2023-09-30 06:22:38.920 
Epoch 967/1000 
	 loss: 28.0427, MinusLogProbMetric: 28.0427, val_loss: 28.6109, val_MinusLogProbMetric: 28.6109

Epoch 967: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0427 - MinusLogProbMetric: 28.0427 - val_loss: 28.6109 - val_MinusLogProbMetric: 28.6109 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 968/1000
2023-09-30 06:23:08.887 
Epoch 968/1000 
	 loss: 28.0402, MinusLogProbMetric: 28.0402, val_loss: 28.6016, val_MinusLogProbMetric: 28.6016

Epoch 968: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0402 - MinusLogProbMetric: 28.0402 - val_loss: 28.6016 - val_MinusLogProbMetric: 28.6016 - lr: 1.5625e-05 - 30s/epoch - 153ms/step
Epoch 969/1000
2023-09-30 06:23:41.774 
Epoch 969/1000 
	 loss: 28.0412, MinusLogProbMetric: 28.0412, val_loss: 28.6116, val_MinusLogProbMetric: 28.6116

Epoch 969: val_loss did not improve from 28.58354
196/196 - 33s - loss: 28.0412 - MinusLogProbMetric: 28.0412 - val_loss: 28.6116 - val_MinusLogProbMetric: 28.6116 - lr: 1.5625e-05 - 33s/epoch - 168ms/step
Epoch 970/1000
2023-09-30 06:24:13.807 
Epoch 970/1000 
	 loss: 28.0420, MinusLogProbMetric: 28.0420, val_loss: 28.6113, val_MinusLogProbMetric: 28.6113

Epoch 970: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0420 - MinusLogProbMetric: 28.0420 - val_loss: 28.6113 - val_MinusLogProbMetric: 28.6113 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 971/1000
2023-09-30 06:24:44.166 
Epoch 971/1000 
	 loss: 28.0413, MinusLogProbMetric: 28.0413, val_loss: 28.6031, val_MinusLogProbMetric: 28.6031

Epoch 971: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0413 - MinusLogProbMetric: 28.0413 - val_loss: 28.6031 - val_MinusLogProbMetric: 28.6031 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 972/1000
2023-09-30 06:25:14.653 
Epoch 972/1000 
	 loss: 28.0420, MinusLogProbMetric: 28.0420, val_loss: 28.6139, val_MinusLogProbMetric: 28.6139

Epoch 972: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0420 - MinusLogProbMetric: 28.0420 - val_loss: 28.6139 - val_MinusLogProbMetric: 28.6139 - lr: 1.5625e-05 - 30s/epoch - 156ms/step
Epoch 973/1000
2023-09-30 06:25:45.726 
Epoch 973/1000 
	 loss: 28.0400, MinusLogProbMetric: 28.0400, val_loss: 28.6016, val_MinusLogProbMetric: 28.6016

Epoch 973: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0400 - MinusLogProbMetric: 28.0400 - val_loss: 28.6016 - val_MinusLogProbMetric: 28.6016 - lr: 1.5625e-05 - 31s/epoch - 159ms/step
Epoch 974/1000
2023-09-30 06:26:17.119 
Epoch 974/1000 
	 loss: 28.0433, MinusLogProbMetric: 28.0433, val_loss: 28.6024, val_MinusLogProbMetric: 28.6024

Epoch 974: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0433 - MinusLogProbMetric: 28.0433 - val_loss: 28.6024 - val_MinusLogProbMetric: 28.6024 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 975/1000
2023-09-30 06:26:47.213 
Epoch 975/1000 
	 loss: 28.0408, MinusLogProbMetric: 28.0408, val_loss: 28.5966, val_MinusLogProbMetric: 28.5966

Epoch 975: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0408 - MinusLogProbMetric: 28.0408 - val_loss: 28.5966 - val_MinusLogProbMetric: 28.5966 - lr: 1.5625e-05 - 30s/epoch - 154ms/step
Epoch 976/1000
2023-09-30 06:27:18.144 
Epoch 976/1000 
	 loss: 28.0404, MinusLogProbMetric: 28.0404, val_loss: 28.6037, val_MinusLogProbMetric: 28.6037

Epoch 976: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0404 - MinusLogProbMetric: 28.0404 - val_loss: 28.6037 - val_MinusLogProbMetric: 28.6037 - lr: 1.5625e-05 - 31s/epoch - 158ms/step
Epoch 977/1000
2023-09-30 06:27:48.660 
Epoch 977/1000 
	 loss: 28.0394, MinusLogProbMetric: 28.0394, val_loss: 28.6105, val_MinusLogProbMetric: 28.6105

Epoch 977: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0394 - MinusLogProbMetric: 28.0394 - val_loss: 28.6105 - val_MinusLogProbMetric: 28.6105 - lr: 1.5625e-05 - 31s/epoch - 156ms/step
Epoch 978/1000
2023-09-30 06:28:18.894 
Epoch 978/1000 
	 loss: 28.0398, MinusLogProbMetric: 28.0398, val_loss: 28.6091, val_MinusLogProbMetric: 28.6091

Epoch 978: val_loss did not improve from 28.58354
196/196 - 30s - loss: 28.0398 - MinusLogProbMetric: 28.0398 - val_loss: 28.6091 - val_MinusLogProbMetric: 28.6091 - lr: 1.5625e-05 - 30s/epoch - 154ms/step
Epoch 979/1000
2023-09-30 06:28:52.662 
Epoch 979/1000 
	 loss: 28.0408, MinusLogProbMetric: 28.0408, val_loss: 28.6063, val_MinusLogProbMetric: 28.6063

Epoch 979: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0408 - MinusLogProbMetric: 28.0408 - val_loss: 28.6063 - val_MinusLogProbMetric: 28.6063 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 980/1000
2023-09-30 06:29:26.397 
Epoch 980/1000 
	 loss: 28.0411, MinusLogProbMetric: 28.0411, val_loss: 28.6065, val_MinusLogProbMetric: 28.6065

Epoch 980: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0411 - MinusLogProbMetric: 28.0411 - val_loss: 28.6065 - val_MinusLogProbMetric: 28.6065 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 981/1000
2023-09-30 06:30:00.129 
Epoch 981/1000 
	 loss: 28.0412, MinusLogProbMetric: 28.0412, val_loss: 28.6129, val_MinusLogProbMetric: 28.6129

Epoch 981: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0412 - MinusLogProbMetric: 28.0412 - val_loss: 28.6129 - val_MinusLogProbMetric: 28.6129 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 982/1000
2023-09-30 06:30:34.082 
Epoch 982/1000 
	 loss: 28.0424, MinusLogProbMetric: 28.0424, val_loss: 28.5978, val_MinusLogProbMetric: 28.5978

Epoch 982: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0424 - MinusLogProbMetric: 28.0424 - val_loss: 28.5978 - val_MinusLogProbMetric: 28.5978 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 983/1000
2023-09-30 06:31:08.269 
Epoch 983/1000 
	 loss: 28.0415, MinusLogProbMetric: 28.0415, val_loss: 28.6090, val_MinusLogProbMetric: 28.6090

Epoch 983: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0415 - MinusLogProbMetric: 28.0415 - val_loss: 28.6090 - val_MinusLogProbMetric: 28.6090 - lr: 1.5625e-05 - 34s/epoch - 174ms/step
Epoch 984/1000
2023-09-30 06:31:41.963 
Epoch 984/1000 
	 loss: 28.0417, MinusLogProbMetric: 28.0417, val_loss: 28.5942, val_MinusLogProbMetric: 28.5942

Epoch 984: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0417 - MinusLogProbMetric: 28.0417 - val_loss: 28.5942 - val_MinusLogProbMetric: 28.5942 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 985/1000
2023-09-30 06:32:14.652 
Epoch 985/1000 
	 loss: 28.0408, MinusLogProbMetric: 28.0408, val_loss: 28.6113, val_MinusLogProbMetric: 28.6113

Epoch 985: val_loss did not improve from 28.58354
196/196 - 33s - loss: 28.0408 - MinusLogProbMetric: 28.0408 - val_loss: 28.6113 - val_MinusLogProbMetric: 28.6113 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 986/1000
2023-09-30 06:32:45.959 
Epoch 986/1000 
	 loss: 28.0404, MinusLogProbMetric: 28.0404, val_loss: 28.6128, val_MinusLogProbMetric: 28.6128

Epoch 986: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0404 - MinusLogProbMetric: 28.0404 - val_loss: 28.6128 - val_MinusLogProbMetric: 28.6128 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 987/1000
2023-09-30 06:33:19.755 
Epoch 987/1000 
	 loss: 28.0390, MinusLogProbMetric: 28.0390, val_loss: 28.6117, val_MinusLogProbMetric: 28.6117

Epoch 987: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0390 - MinusLogProbMetric: 28.0390 - val_loss: 28.6117 - val_MinusLogProbMetric: 28.6117 - lr: 1.5625e-05 - 34s/epoch - 172ms/step
Epoch 988/1000
2023-09-30 06:33:53.282 
Epoch 988/1000 
	 loss: 28.0410, MinusLogProbMetric: 28.0410, val_loss: 28.6110, val_MinusLogProbMetric: 28.6110

Epoch 988: val_loss did not improve from 28.58354
196/196 - 34s - loss: 28.0410 - MinusLogProbMetric: 28.0410 - val_loss: 28.6110 - val_MinusLogProbMetric: 28.6110 - lr: 1.5625e-05 - 34s/epoch - 171ms/step
Epoch 989/1000
2023-09-30 06:34:25.696 
Epoch 989/1000 
	 loss: 28.0405, MinusLogProbMetric: 28.0405, val_loss: 28.6226, val_MinusLogProbMetric: 28.6226

Epoch 989: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0405 - MinusLogProbMetric: 28.0405 - val_loss: 28.6226 - val_MinusLogProbMetric: 28.6226 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 990/1000
2023-09-30 06:34:59.146 
Epoch 990/1000 
	 loss: 28.0398, MinusLogProbMetric: 28.0398, val_loss: 28.5969, val_MinusLogProbMetric: 28.5969

Epoch 990: val_loss did not improve from 28.58354
196/196 - 33s - loss: 28.0398 - MinusLogProbMetric: 28.0398 - val_loss: 28.5969 - val_MinusLogProbMetric: 28.5969 - lr: 1.5625e-05 - 33s/epoch - 171ms/step
Epoch 991/1000
2023-09-30 06:35:30.441 
Epoch 991/1000 
	 loss: 28.0408, MinusLogProbMetric: 28.0408, val_loss: 28.6072, val_MinusLogProbMetric: 28.6072

Epoch 991: val_loss did not improve from 28.58354
196/196 - 31s - loss: 28.0408 - MinusLogProbMetric: 28.0408 - val_loss: 28.6072 - val_MinusLogProbMetric: 28.6072 - lr: 1.5625e-05 - 31s/epoch - 159ms/step
Epoch 992/1000
2023-09-30 06:36:02.730 
Epoch 992/1000 
	 loss: 28.0402, MinusLogProbMetric: 28.0402, val_loss: 28.5936, val_MinusLogProbMetric: 28.5936

Epoch 992: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0402 - MinusLogProbMetric: 28.0402 - val_loss: 28.5936 - val_MinusLogProbMetric: 28.5936 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 993/1000
2023-09-30 06:36:34.895 
Epoch 993/1000 
	 loss: 28.0386, MinusLogProbMetric: 28.0386, val_loss: 28.5998, val_MinusLogProbMetric: 28.5998

Epoch 993: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0386 - MinusLogProbMetric: 28.0386 - val_loss: 28.5998 - val_MinusLogProbMetric: 28.5998 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 994/1000
2023-09-30 06:37:06.515 
Epoch 994/1000 
	 loss: 28.0399, MinusLogProbMetric: 28.0399, val_loss: 28.6091, val_MinusLogProbMetric: 28.6091

Epoch 994: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0399 - MinusLogProbMetric: 28.0399 - val_loss: 28.6091 - val_MinusLogProbMetric: 28.6091 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 995/1000
2023-09-30 06:37:39.562 
Epoch 995/1000 
	 loss: 28.0405, MinusLogProbMetric: 28.0405, val_loss: 28.5939, val_MinusLogProbMetric: 28.5939

Epoch 995: val_loss did not improve from 28.58354
196/196 - 33s - loss: 28.0405 - MinusLogProbMetric: 28.0405 - val_loss: 28.5939 - val_MinusLogProbMetric: 28.5939 - lr: 1.5625e-05 - 33s/epoch - 169ms/step
Epoch 996/1000
2023-09-30 06:38:11.902 
Epoch 996/1000 
	 loss: 28.0417, MinusLogProbMetric: 28.0417, val_loss: 28.6037, val_MinusLogProbMetric: 28.6037

Epoch 996: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0417 - MinusLogProbMetric: 28.0417 - val_loss: 28.6037 - val_MinusLogProbMetric: 28.6037 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 997/1000
2023-09-30 06:38:43.612 
Epoch 997/1000 
	 loss: 28.0434, MinusLogProbMetric: 28.0434, val_loss: 28.6010, val_MinusLogProbMetric: 28.6010

Epoch 997: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0434 - MinusLogProbMetric: 28.0434 - val_loss: 28.6010 - val_MinusLogProbMetric: 28.6010 - lr: 1.5625e-05 - 32s/epoch - 162ms/step
Epoch 998/1000
2023-09-30 06:39:15.127 
Epoch 998/1000 
	 loss: 28.0382, MinusLogProbMetric: 28.0382, val_loss: 28.6096, val_MinusLogProbMetric: 28.6096

Epoch 998: val_loss did not improve from 28.58354
196/196 - 32s - loss: 28.0382 - MinusLogProbMetric: 28.0382 - val_loss: 28.6096 - val_MinusLogProbMetric: 28.6096 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 999/1000
2023-09-30 06:39:47.276 
Epoch 999/1000 
	 loss: 28.0422, MinusLogProbMetric: 28.0422, val_loss: 28.6175, val_MinusLogProbMetric: 28.6175

Epoch 999: val_loss did not improve from 28.58354
Restoring model weights from the end of the best epoch: 899.
196/196 - 32s - loss: 28.0422 - MinusLogProbMetric: 28.0422 - val_loss: 28.6175 - val_MinusLogProbMetric: 28.6175 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 999: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 20.395037205074914 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 12.834649408934638 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 9.05846502899658 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 9.988756333943456 seconds.
Training succeeded with seed 440.
Model trained in 32486.57 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 53.73 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 54.38 s.
===========
Run 345/720 done in 32545.77 s.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

===========
Generating train data for run 347.
===========
Train data generated in 0.51 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_347
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_173"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_174 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7fc192dbbf40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc192aac460>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc192aac460>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc1934f7ee0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc19281e140>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc19281e6b0>, <keras.callbacks.ModelCheckpoint object at 0x7fc19281e770>, <keras.callbacks.EarlyStopping object at 0x7fc19281e9e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc19281ea10>, <keras.callbacks.TerminateOnNaN object at 0x7fc19281e650>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_347/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 347/720 with hyperparameters:
timestamp = 2023-09-30 06:40:50.473948
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 37: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-30 06:43:43.355 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3610.2000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 3610.2000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 172s/epoch - 880ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 347.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_347
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_179"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_180 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7fc42c44ec20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb98a35e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb98a35e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbfb379eef0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbfb36779d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbfb3677f40>, <keras.callbacks.ModelCheckpoint object at 0x7fbfb3677ee0>, <keras.callbacks.EarlyStopping object at 0x7fbfb3677f10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbfa02e40d0>, <keras.callbacks.TerminateOnNaN object at 0x7fbfa02e42b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_347/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 347/720 with hyperparameters:
timestamp = 2023-09-30 06:43:51.400470
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-09-30 06:47:08.063 
Epoch 1/1000 
	 loss: 1680.9509, MinusLogProbMetric: 1680.9509, val_loss: 454.3510, val_MinusLogProbMetric: 454.3510

Epoch 1: val_loss improved from inf to 454.35101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 197s - loss: 1680.9509 - MinusLogProbMetric: 1680.9509 - val_loss: 454.3510 - val_MinusLogProbMetric: 454.3510 - lr: 3.3333e-04 - 197s/epoch - 1s/step
Epoch 2/1000
2023-09-30 06:47:51.112 
Epoch 2/1000 
	 loss: 340.7473, MinusLogProbMetric: 340.7473, val_loss: 285.4436, val_MinusLogProbMetric: 285.4436

Epoch 2: val_loss improved from 454.35101 to 285.44357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 340.7473 - MinusLogProbMetric: 340.7473 - val_loss: 285.4436 - val_MinusLogProbMetric: 285.4436 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 3/1000
2023-09-30 06:48:34.991 
Epoch 3/1000 
	 loss: 285.4004, MinusLogProbMetric: 285.4004, val_loss: 242.5879, val_MinusLogProbMetric: 242.5879

Epoch 3: val_loss improved from 285.44357 to 242.58792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 285.4004 - MinusLogProbMetric: 285.4004 - val_loss: 242.5879 - val_MinusLogProbMetric: 242.5879 - lr: 3.3333e-04 - 45s/epoch - 227ms/step
Epoch 4/1000
2023-09-30 06:49:20.297 
Epoch 4/1000 
	 loss: 230.9718, MinusLogProbMetric: 230.9718, val_loss: 212.1033, val_MinusLogProbMetric: 212.1033

Epoch 4: val_loss improved from 242.58792 to 212.10327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 230.9718 - MinusLogProbMetric: 230.9718 - val_loss: 212.1033 - val_MinusLogProbMetric: 212.1033 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 5/1000
2023-09-30 06:50:02.904 
Epoch 5/1000 
	 loss: 202.6098, MinusLogProbMetric: 202.6098, val_loss: 192.4890, val_MinusLogProbMetric: 192.4890

Epoch 5: val_loss improved from 212.10327 to 192.48897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 202.6098 - MinusLogProbMetric: 202.6098 - val_loss: 192.4890 - val_MinusLogProbMetric: 192.4890 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 6/1000
2023-09-30 06:50:45.966 
Epoch 6/1000 
	 loss: 186.5633, MinusLogProbMetric: 186.5633, val_loss: 197.3990, val_MinusLogProbMetric: 197.3990

Epoch 6: val_loss did not improve from 192.48897
196/196 - 42s - loss: 186.5633 - MinusLogProbMetric: 186.5633 - val_loss: 197.3990 - val_MinusLogProbMetric: 197.3990 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 7/1000
2023-09-30 06:51:28.113 
Epoch 7/1000 
	 loss: 179.8291, MinusLogProbMetric: 179.8291, val_loss: 166.8907, val_MinusLogProbMetric: 166.8907

Epoch 7: val_loss improved from 192.48897 to 166.89067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 179.8291 - MinusLogProbMetric: 179.8291 - val_loss: 166.8907 - val_MinusLogProbMetric: 166.8907 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 8/1000
2023-09-30 06:52:10.506 
Epoch 8/1000 
	 loss: 152.6823, MinusLogProbMetric: 152.6823, val_loss: 139.1583, val_MinusLogProbMetric: 139.1583

Epoch 8: val_loss improved from 166.89067 to 139.15831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 152.6823 - MinusLogProbMetric: 152.6823 - val_loss: 139.1583 - val_MinusLogProbMetric: 139.1583 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 9/1000
2023-09-30 06:52:54.154 
Epoch 9/1000 
	 loss: 150.5604, MinusLogProbMetric: 150.5604, val_loss: 137.6774, val_MinusLogProbMetric: 137.6774

Epoch 9: val_loss improved from 139.15831 to 137.67741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 150.5604 - MinusLogProbMetric: 150.5604 - val_loss: 137.6774 - val_MinusLogProbMetric: 137.6774 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 10/1000
2023-09-30 06:53:37.928 
Epoch 10/1000 
	 loss: 207.7900, MinusLogProbMetric: 207.7900, val_loss: 175.0171, val_MinusLogProbMetric: 175.0171

Epoch 10: val_loss did not improve from 137.67741
196/196 - 43s - loss: 207.7900 - MinusLogProbMetric: 207.7900 - val_loss: 175.0171 - val_MinusLogProbMetric: 175.0171 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 11/1000
2023-09-30 06:54:19.513 
Epoch 11/1000 
	 loss: 155.7274, MinusLogProbMetric: 155.7274, val_loss: 142.7383, val_MinusLogProbMetric: 142.7383

Epoch 11: val_loss did not improve from 137.67741
196/196 - 42s - loss: 155.7274 - MinusLogProbMetric: 155.7274 - val_loss: 142.7383 - val_MinusLogProbMetric: 142.7383 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 12/1000
2023-09-30 06:55:01.134 
Epoch 12/1000 
	 loss: 136.8686, MinusLogProbMetric: 136.8686, val_loss: 129.7909, val_MinusLogProbMetric: 129.7909

Epoch 12: val_loss improved from 137.67741 to 129.79091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 136.8686 - MinusLogProbMetric: 136.8686 - val_loss: 129.7909 - val_MinusLogProbMetric: 129.7909 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 13/1000
2023-09-30 06:55:42.734 
Epoch 13/1000 
	 loss: 126.1184, MinusLogProbMetric: 126.1184, val_loss: 121.4603, val_MinusLogProbMetric: 121.4603

Epoch 13: val_loss improved from 129.79091 to 121.46033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 126.1184 - MinusLogProbMetric: 126.1184 - val_loss: 121.4603 - val_MinusLogProbMetric: 121.4603 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 14/1000
2023-09-30 06:56:26.951 
Epoch 14/1000 
	 loss: 119.0297, MinusLogProbMetric: 119.0297, val_loss: 118.4167, val_MinusLogProbMetric: 118.4167

Epoch 14: val_loss improved from 121.46033 to 118.41674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 119.0297 - MinusLogProbMetric: 119.0297 - val_loss: 118.4167 - val_MinusLogProbMetric: 118.4167 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 15/1000
2023-09-30 06:57:09.218 
Epoch 15/1000 
	 loss: 115.0929, MinusLogProbMetric: 115.0929, val_loss: 111.4828, val_MinusLogProbMetric: 111.4828

Epoch 15: val_loss improved from 118.41674 to 111.48280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 115.0929 - MinusLogProbMetric: 115.0929 - val_loss: 111.4828 - val_MinusLogProbMetric: 111.4828 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 16/1000
2023-09-30 06:57:51.575 
Epoch 16/1000 
	 loss: 110.2722, MinusLogProbMetric: 110.2722, val_loss: 108.3295, val_MinusLogProbMetric: 108.3295

Epoch 16: val_loss improved from 111.48280 to 108.32945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 110.2722 - MinusLogProbMetric: 110.2722 - val_loss: 108.3295 - val_MinusLogProbMetric: 108.3295 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 17/1000
2023-09-30 06:58:33.474 
Epoch 17/1000 
	 loss: 103.5344, MinusLogProbMetric: 103.5344, val_loss: 98.4135, val_MinusLogProbMetric: 98.4135

Epoch 17: val_loss improved from 108.32945 to 98.41347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 103.5344 - MinusLogProbMetric: 103.5344 - val_loss: 98.4135 - val_MinusLogProbMetric: 98.4135 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 18/1000
2023-09-30 06:59:19.139 
Epoch 18/1000 
	 loss: 93.6661, MinusLogProbMetric: 93.6661, val_loss: 89.3629, val_MinusLogProbMetric: 89.3629

Epoch 18: val_loss improved from 98.41347 to 89.36288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 93.6661 - MinusLogProbMetric: 93.6661 - val_loss: 89.3629 - val_MinusLogProbMetric: 89.3629 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 19/1000
2023-09-30 07:00:01.077 
Epoch 19/1000 
	 loss: 87.9419, MinusLogProbMetric: 87.9419, val_loss: 86.8791, val_MinusLogProbMetric: 86.8791

Epoch 19: val_loss improved from 89.36288 to 86.87907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 87.9419 - MinusLogProbMetric: 87.9419 - val_loss: 86.8791 - val_MinusLogProbMetric: 86.8791 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 20/1000
2023-09-30 07:00:44.306 
Epoch 20/1000 
	 loss: 85.2565, MinusLogProbMetric: 85.2565, val_loss: 84.3607, val_MinusLogProbMetric: 84.3607

Epoch 20: val_loss improved from 86.87907 to 84.36075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 85.2565 - MinusLogProbMetric: 85.2565 - val_loss: 84.3607 - val_MinusLogProbMetric: 84.3607 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 21/1000
2023-09-30 07:01:28.242 
Epoch 21/1000 
	 loss: 88.4536, MinusLogProbMetric: 88.4536, val_loss: 83.4639, val_MinusLogProbMetric: 83.4639

Epoch 21: val_loss improved from 84.36075 to 83.46391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 88.4536 - MinusLogProbMetric: 88.4536 - val_loss: 83.4639 - val_MinusLogProbMetric: 83.4639 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 22/1000
2023-09-30 07:02:11.553 
Epoch 22/1000 
	 loss: 81.8817, MinusLogProbMetric: 81.8817, val_loss: 80.7741, val_MinusLogProbMetric: 80.7741

Epoch 22: val_loss improved from 83.46391 to 80.77406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 81.8817 - MinusLogProbMetric: 81.8817 - val_loss: 80.7741 - val_MinusLogProbMetric: 80.7741 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 23/1000
2023-09-30 07:02:54.208 
Epoch 23/1000 
	 loss: 79.8084, MinusLogProbMetric: 79.8084, val_loss: 78.9137, val_MinusLogProbMetric: 78.9137

Epoch 23: val_loss improved from 80.77406 to 78.91367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 79.8084 - MinusLogProbMetric: 79.8084 - val_loss: 78.9137 - val_MinusLogProbMetric: 78.9137 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 24/1000
2023-09-30 07:03:39.030 
Epoch 24/1000 
	 loss: 78.7043, MinusLogProbMetric: 78.7043, val_loss: 77.2773, val_MinusLogProbMetric: 77.2773

Epoch 24: val_loss improved from 78.91367 to 77.27726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 78.7043 - MinusLogProbMetric: 78.7043 - val_loss: 77.2773 - val_MinusLogProbMetric: 77.2773 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 25/1000
2023-09-30 07:04:22.827 
Epoch 25/1000 
	 loss: 94.7935, MinusLogProbMetric: 94.7935, val_loss: 84.9045, val_MinusLogProbMetric: 84.9045

Epoch 25: val_loss did not improve from 77.27726
196/196 - 43s - loss: 94.7935 - MinusLogProbMetric: 94.7935 - val_loss: 84.9045 - val_MinusLogProbMetric: 84.9045 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 26/1000
2023-09-30 07:05:04.920 
Epoch 26/1000 
	 loss: 81.4706, MinusLogProbMetric: 81.4706, val_loss: 79.1278, val_MinusLogProbMetric: 79.1278

Epoch 26: val_loss did not improve from 77.27726
196/196 - 42s - loss: 81.4706 - MinusLogProbMetric: 81.4706 - val_loss: 79.1278 - val_MinusLogProbMetric: 79.1278 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 27/1000
2023-09-30 07:05:46.996 
Epoch 27/1000 
	 loss: 78.3645, MinusLogProbMetric: 78.3645, val_loss: 77.2121, val_MinusLogProbMetric: 77.2121

Epoch 27: val_loss improved from 77.27726 to 77.21205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 78.3645 - MinusLogProbMetric: 78.3645 - val_loss: 77.2121 - val_MinusLogProbMetric: 77.2121 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 28/1000
2023-09-30 07:06:29.625 
Epoch 28/1000 
	 loss: 76.0533, MinusLogProbMetric: 76.0533, val_loss: 77.0543, val_MinusLogProbMetric: 77.0543

Epoch 28: val_loss improved from 77.21205 to 77.05431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 76.0533 - MinusLogProbMetric: 76.0533 - val_loss: 77.0543 - val_MinusLogProbMetric: 77.0543 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 29/1000
2023-09-30 07:07:13.824 
Epoch 29/1000 
	 loss: 74.2098, MinusLogProbMetric: 74.2098, val_loss: 73.4808, val_MinusLogProbMetric: 73.4808

Epoch 29: val_loss improved from 77.05431 to 73.48078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 49s - loss: 74.2098 - MinusLogProbMetric: 74.2098 - val_loss: 73.4808 - val_MinusLogProbMetric: 73.4808 - lr: 3.3333e-04 - 49s/epoch - 251ms/step
Epoch 30/1000
2023-09-30 07:08:02.171 
Epoch 30/1000 
	 loss: 72.6439, MinusLogProbMetric: 72.6439, val_loss: 72.1839, val_MinusLogProbMetric: 72.1839

Epoch 30: val_loss improved from 73.48078 to 72.18387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 72.6439 - MinusLogProbMetric: 72.6439 - val_loss: 72.1839 - val_MinusLogProbMetric: 72.1839 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 31/1000
2023-09-30 07:08:45.721 
Epoch 31/1000 
	 loss: 82.5467, MinusLogProbMetric: 82.5467, val_loss: 74.6462, val_MinusLogProbMetric: 74.6462

Epoch 31: val_loss did not improve from 72.18387
196/196 - 42s - loss: 82.5467 - MinusLogProbMetric: 82.5467 - val_loss: 74.6462 - val_MinusLogProbMetric: 74.6462 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 32/1000
2023-09-30 07:09:27.219 
Epoch 32/1000 
	 loss: 73.0251, MinusLogProbMetric: 73.0251, val_loss: 72.2006, val_MinusLogProbMetric: 72.2006

Epoch 32: val_loss did not improve from 72.18387
196/196 - 41s - loss: 73.0251 - MinusLogProbMetric: 73.0251 - val_loss: 72.2006 - val_MinusLogProbMetric: 72.2006 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 33/1000
2023-09-30 07:10:08.398 
Epoch 33/1000 
	 loss: 71.3176, MinusLogProbMetric: 71.3176, val_loss: 70.6833, val_MinusLogProbMetric: 70.6833

Epoch 33: val_loss improved from 72.18387 to 70.68334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 71.3176 - MinusLogProbMetric: 71.3176 - val_loss: 70.6833 - val_MinusLogProbMetric: 70.6833 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 34/1000
2023-09-30 07:10:51.368 
Epoch 34/1000 
	 loss: 75.2843, MinusLogProbMetric: 75.2843, val_loss: 72.9354, val_MinusLogProbMetric: 72.9354

Epoch 34: val_loss did not improve from 70.68334
196/196 - 42s - loss: 75.2843 - MinusLogProbMetric: 75.2843 - val_loss: 72.9354 - val_MinusLogProbMetric: 72.9354 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 35/1000
2023-09-30 07:11:32.489 
Epoch 35/1000 
	 loss: 70.9612, MinusLogProbMetric: 70.9612, val_loss: 70.2815, val_MinusLogProbMetric: 70.2815

Epoch 35: val_loss improved from 70.68334 to 70.28152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 70.9612 - MinusLogProbMetric: 70.9612 - val_loss: 70.2815 - val_MinusLogProbMetric: 70.2815 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 36/1000
2023-09-30 07:12:15.789 
Epoch 36/1000 
	 loss: 69.6377, MinusLogProbMetric: 69.6377, val_loss: 69.1721, val_MinusLogProbMetric: 69.1721

Epoch 36: val_loss improved from 70.28152 to 69.17209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 69.6377 - MinusLogProbMetric: 69.6377 - val_loss: 69.1721 - val_MinusLogProbMetric: 69.1721 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 37/1000
2023-09-30 07:13:00.618 
Epoch 37/1000 
	 loss: 68.5070, MinusLogProbMetric: 68.5070, val_loss: 68.4273, val_MinusLogProbMetric: 68.4273

Epoch 37: val_loss improved from 69.17209 to 68.42731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 68.5070 - MinusLogProbMetric: 68.5070 - val_loss: 68.4273 - val_MinusLogProbMetric: 68.4273 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 38/1000
2023-09-30 07:13:43.752 
Epoch 38/1000 
	 loss: 67.8877, MinusLogProbMetric: 67.8877, val_loss: 67.5467, val_MinusLogProbMetric: 67.5467

Epoch 38: val_loss improved from 68.42731 to 67.54668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 67.8877 - MinusLogProbMetric: 67.8877 - val_loss: 67.5467 - val_MinusLogProbMetric: 67.5467 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 39/1000
2023-09-30 07:14:26.377 
Epoch 39/1000 
	 loss: 67.7699, MinusLogProbMetric: 67.7699, val_loss: 68.0417, val_MinusLogProbMetric: 68.0417

Epoch 39: val_loss did not improve from 67.54668
196/196 - 42s - loss: 67.7699 - MinusLogProbMetric: 67.7699 - val_loss: 68.0417 - val_MinusLogProbMetric: 68.0417 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 40/1000
2023-09-30 07:15:08.280 
Epoch 40/1000 
	 loss: 67.0755, MinusLogProbMetric: 67.0755, val_loss: 67.2167, val_MinusLogProbMetric: 67.2167

Epoch 40: val_loss improved from 67.54668 to 67.21667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 67.0755 - MinusLogProbMetric: 67.0755 - val_loss: 67.2167 - val_MinusLogProbMetric: 67.2167 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 41/1000
2023-09-30 07:15:51.446 
Epoch 41/1000 
	 loss: 66.3030, MinusLogProbMetric: 66.3030, val_loss: 66.3979, val_MinusLogProbMetric: 66.3979

Epoch 41: val_loss improved from 67.21667 to 66.39789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 66.3030 - MinusLogProbMetric: 66.3030 - val_loss: 66.3979 - val_MinusLogProbMetric: 66.3979 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 42/1000
2023-09-30 07:16:33.893 
Epoch 42/1000 
	 loss: 65.6273, MinusLogProbMetric: 65.6273, val_loss: 65.3796, val_MinusLogProbMetric: 65.3796

Epoch 42: val_loss improved from 66.39789 to 65.37962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 65.6273 - MinusLogProbMetric: 65.6273 - val_loss: 65.3796 - val_MinusLogProbMetric: 65.3796 - lr: 3.3333e-04 - 45s/epoch - 227ms/step
Epoch 43/1000
2023-09-30 07:17:16.441 
Epoch 43/1000 
	 loss: 65.0241, MinusLogProbMetric: 65.0241, val_loss: 66.5282, val_MinusLogProbMetric: 66.5282

Epoch 43: val_loss did not improve from 65.37962
196/196 - 40s - loss: 65.0241 - MinusLogProbMetric: 65.0241 - val_loss: 66.5282 - val_MinusLogProbMetric: 66.5282 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 44/1000
2023-09-30 07:17:58.677 
Epoch 44/1000 
	 loss: 65.7037, MinusLogProbMetric: 65.7037, val_loss: 66.8133, val_MinusLogProbMetric: 66.8133

Epoch 44: val_loss did not improve from 65.37962
196/196 - 42s - loss: 65.7037 - MinusLogProbMetric: 65.7037 - val_loss: 66.8133 - val_MinusLogProbMetric: 66.8133 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 45/1000
2023-09-30 07:18:39.947 
Epoch 45/1000 
	 loss: 65.2654, MinusLogProbMetric: 65.2654, val_loss: 67.2388, val_MinusLogProbMetric: 67.2388

Epoch 45: val_loss did not improve from 65.37962
196/196 - 41s - loss: 65.2654 - MinusLogProbMetric: 65.2654 - val_loss: 67.2388 - val_MinusLogProbMetric: 67.2388 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 46/1000
2023-09-30 07:19:22.326 
Epoch 46/1000 
	 loss: 64.6086, MinusLogProbMetric: 64.6086, val_loss: 64.1409, val_MinusLogProbMetric: 64.1409

Epoch 46: val_loss improved from 65.37962 to 64.14086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 64.6086 - MinusLogProbMetric: 64.6086 - val_loss: 64.1409 - val_MinusLogProbMetric: 64.1409 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 47/1000
2023-09-30 07:20:04.859 
Epoch 47/1000 
	 loss: 63.9630, MinusLogProbMetric: 63.9630, val_loss: 63.5746, val_MinusLogProbMetric: 63.5746

Epoch 47: val_loss improved from 64.14086 to 63.57457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 63.9630 - MinusLogProbMetric: 63.9630 - val_loss: 63.5746 - val_MinusLogProbMetric: 63.5746 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 48/1000
2023-09-30 07:20:47.455 
Epoch 48/1000 
	 loss: 63.1817, MinusLogProbMetric: 63.1817, val_loss: 63.1340, val_MinusLogProbMetric: 63.1340

Epoch 48: val_loss improved from 63.57457 to 63.13405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 63.1817 - MinusLogProbMetric: 63.1817 - val_loss: 63.1340 - val_MinusLogProbMetric: 63.1340 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 49/1000
2023-09-30 07:21:29.666 
Epoch 49/1000 
	 loss: 62.7439, MinusLogProbMetric: 62.7439, val_loss: 62.8876, val_MinusLogProbMetric: 62.8876

Epoch 49: val_loss improved from 63.13405 to 62.88757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 62.7439 - MinusLogProbMetric: 62.7439 - val_loss: 62.8876 - val_MinusLogProbMetric: 62.8876 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 50/1000
2023-09-30 07:22:12.167 
Epoch 50/1000 
	 loss: 66.8432, MinusLogProbMetric: 66.8432, val_loss: 85.8566, val_MinusLogProbMetric: 85.8566

Epoch 50: val_loss did not improve from 62.88757
196/196 - 41s - loss: 66.8432 - MinusLogProbMetric: 66.8432 - val_loss: 85.8566 - val_MinusLogProbMetric: 85.8566 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 51/1000
2023-09-30 07:22:53.375 
Epoch 51/1000 
	 loss: 69.2653, MinusLogProbMetric: 69.2653, val_loss: 65.1698, val_MinusLogProbMetric: 65.1698

Epoch 51: val_loss did not improve from 62.88757
196/196 - 41s - loss: 69.2653 - MinusLogProbMetric: 69.2653 - val_loss: 65.1698 - val_MinusLogProbMetric: 65.1698 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 52/1000
2023-09-30 07:23:34.574 
Epoch 52/1000 
	 loss: 64.0700, MinusLogProbMetric: 64.0700, val_loss: 63.6796, val_MinusLogProbMetric: 63.6796

Epoch 52: val_loss did not improve from 62.88757
196/196 - 41s - loss: 64.0700 - MinusLogProbMetric: 64.0700 - val_loss: 63.6796 - val_MinusLogProbMetric: 63.6796 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 53/1000
2023-09-30 07:24:15.387 
Epoch 53/1000 
	 loss: 69.4509, MinusLogProbMetric: 69.4509, val_loss: 68.2535, val_MinusLogProbMetric: 68.2535

Epoch 53: val_loss did not improve from 62.88757
196/196 - 41s - loss: 69.4509 - MinusLogProbMetric: 69.4509 - val_loss: 68.2535 - val_MinusLogProbMetric: 68.2535 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 54/1000
2023-09-30 07:24:56.894 
Epoch 54/1000 
	 loss: 66.0488, MinusLogProbMetric: 66.0488, val_loss: 64.8063, val_MinusLogProbMetric: 64.8063

Epoch 54: val_loss did not improve from 62.88757
196/196 - 42s - loss: 66.0488 - MinusLogProbMetric: 66.0488 - val_loss: 64.8063 - val_MinusLogProbMetric: 64.8063 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 55/1000
2023-09-30 07:25:40.169 
Epoch 55/1000 
	 loss: 64.0273, MinusLogProbMetric: 64.0273, val_loss: 63.5643, val_MinusLogProbMetric: 63.5643

Epoch 55: val_loss did not improve from 62.88757
196/196 - 43s - loss: 64.0273 - MinusLogProbMetric: 64.0273 - val_loss: 63.5643 - val_MinusLogProbMetric: 63.5643 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 56/1000
2023-09-30 07:26:23.849 
Epoch 56/1000 
	 loss: 62.9058, MinusLogProbMetric: 62.9058, val_loss: 62.5759, val_MinusLogProbMetric: 62.5759

Epoch 56: val_loss improved from 62.88757 to 62.57586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 62.9058 - MinusLogProbMetric: 62.9058 - val_loss: 62.5759 - val_MinusLogProbMetric: 62.5759 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 57/1000
2023-09-30 07:27:07.315 
Epoch 57/1000 
	 loss: 62.4163, MinusLogProbMetric: 62.4163, val_loss: 62.4838, val_MinusLogProbMetric: 62.4838

Epoch 57: val_loss improved from 62.57586 to 62.48380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 62.4163 - MinusLogProbMetric: 62.4163 - val_loss: 62.4838 - val_MinusLogProbMetric: 62.4838 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 58/1000
2023-09-30 07:27:50.943 
Epoch 58/1000 
	 loss: 61.9921, MinusLogProbMetric: 61.9921, val_loss: 61.9852, val_MinusLogProbMetric: 61.9852

Epoch 58: val_loss improved from 62.48380 to 61.98523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 61.9921 - MinusLogProbMetric: 61.9921 - val_loss: 61.9852 - val_MinusLogProbMetric: 61.9852 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 59/1000
2023-09-30 07:28:34.868 
Epoch 59/1000 
	 loss: 61.6592, MinusLogProbMetric: 61.6592, val_loss: 62.0470, val_MinusLogProbMetric: 62.0470

Epoch 59: val_loss did not improve from 61.98523
196/196 - 42s - loss: 61.6592 - MinusLogProbMetric: 61.6592 - val_loss: 62.0470 - val_MinusLogProbMetric: 62.0470 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 60/1000
2023-09-30 07:29:16.887 
Epoch 60/1000 
	 loss: 61.3872, MinusLogProbMetric: 61.3872, val_loss: 61.1911, val_MinusLogProbMetric: 61.1911

Epoch 60: val_loss improved from 61.98523 to 61.19107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 61.3872 - MinusLogProbMetric: 61.3872 - val_loss: 61.1911 - val_MinusLogProbMetric: 61.1911 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 61/1000
2023-09-30 07:30:01.213 
Epoch 61/1000 
	 loss: 64.1733, MinusLogProbMetric: 64.1733, val_loss: 61.6172, val_MinusLogProbMetric: 61.6172

Epoch 61: val_loss did not improve from 61.19107
196/196 - 42s - loss: 64.1733 - MinusLogProbMetric: 64.1733 - val_loss: 61.6172 - val_MinusLogProbMetric: 61.6172 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 62/1000
2023-09-30 07:30:42.678 
Epoch 62/1000 
	 loss: 60.8235, MinusLogProbMetric: 60.8235, val_loss: 62.3574, val_MinusLogProbMetric: 62.3574

Epoch 62: val_loss did not improve from 61.19107
196/196 - 41s - loss: 60.8235 - MinusLogProbMetric: 60.8235 - val_loss: 62.3574 - val_MinusLogProbMetric: 62.3574 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 63/1000
2023-09-30 07:31:25.347 
Epoch 63/1000 
	 loss: 60.5505, MinusLogProbMetric: 60.5505, val_loss: 60.6423, val_MinusLogProbMetric: 60.6423

Epoch 63: val_loss improved from 61.19107 to 60.64234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 60.5505 - MinusLogProbMetric: 60.5505 - val_loss: 60.6423 - val_MinusLogProbMetric: 60.6423 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 64/1000
2023-09-30 07:32:08.702 
Epoch 64/1000 
	 loss: 60.3232, MinusLogProbMetric: 60.3232, val_loss: 60.3055, val_MinusLogProbMetric: 60.3055

Epoch 64: val_loss improved from 60.64234 to 60.30551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 60.3232 - MinusLogProbMetric: 60.3232 - val_loss: 60.3055 - val_MinusLogProbMetric: 60.3055 - lr: 3.3333e-04 - 45s/epoch - 231ms/step
Epoch 65/1000
2023-09-30 07:32:54.811 
Epoch 65/1000 
	 loss: 60.0521, MinusLogProbMetric: 60.0521, val_loss: 60.1071, val_MinusLogProbMetric: 60.1071

Epoch 65: val_loss improved from 60.30551 to 60.10709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 60.0521 - MinusLogProbMetric: 60.0521 - val_loss: 60.1071 - val_MinusLogProbMetric: 60.1071 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 66/1000
2023-09-30 07:33:38.182 
Epoch 66/1000 
	 loss: 59.8344, MinusLogProbMetric: 59.8344, val_loss: 60.4246, val_MinusLogProbMetric: 60.4246

Epoch 66: val_loss did not improve from 60.10709
196/196 - 42s - loss: 59.8344 - MinusLogProbMetric: 59.8344 - val_loss: 60.4246 - val_MinusLogProbMetric: 60.4246 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 67/1000
2023-09-30 07:34:20.953 
Epoch 67/1000 
	 loss: 59.7210, MinusLogProbMetric: 59.7210, val_loss: 59.8658, val_MinusLogProbMetric: 59.8658

Epoch 67: val_loss improved from 60.10709 to 59.86583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 59.7210 - MinusLogProbMetric: 59.7210 - val_loss: 59.8658 - val_MinusLogProbMetric: 59.8658 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 68/1000
2023-09-30 07:35:04.384 
Epoch 68/1000 
	 loss: 59.2915, MinusLogProbMetric: 59.2915, val_loss: 59.3491, val_MinusLogProbMetric: 59.3491

Epoch 68: val_loss improved from 59.86583 to 59.34909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 59.2915 - MinusLogProbMetric: 59.2915 - val_loss: 59.3491 - val_MinusLogProbMetric: 59.3491 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 69/1000
2023-09-30 07:35:47.704 
Epoch 69/1000 
	 loss: 59.2550, MinusLogProbMetric: 59.2550, val_loss: 59.9338, val_MinusLogProbMetric: 59.9338

Epoch 69: val_loss did not improve from 59.34909
196/196 - 42s - loss: 59.2550 - MinusLogProbMetric: 59.2550 - val_loss: 59.9338 - val_MinusLogProbMetric: 59.9338 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 70/1000
2023-09-30 07:36:30.503 
Epoch 70/1000 
	 loss: 59.0103, MinusLogProbMetric: 59.0103, val_loss: 58.7878, val_MinusLogProbMetric: 58.7878

Epoch 70: val_loss improved from 59.34909 to 58.78777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 59.0103 - MinusLogProbMetric: 59.0103 - val_loss: 58.7878 - val_MinusLogProbMetric: 58.7878 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 71/1000
2023-09-30 07:37:13.818 
Epoch 71/1000 
	 loss: 58.8872, MinusLogProbMetric: 58.8872, val_loss: 58.8021, val_MinusLogProbMetric: 58.8021

Epoch 71: val_loss did not improve from 58.78777
196/196 - 42s - loss: 58.8872 - MinusLogProbMetric: 58.8872 - val_loss: 58.8021 - val_MinusLogProbMetric: 58.8021 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 72/1000
2023-09-30 07:37:56.313 
Epoch 72/1000 
	 loss: 58.6914, MinusLogProbMetric: 58.6914, val_loss: 58.9675, val_MinusLogProbMetric: 58.9675

Epoch 72: val_loss did not improve from 58.78777
196/196 - 43s - loss: 58.6914 - MinusLogProbMetric: 58.6914 - val_loss: 58.9675 - val_MinusLogProbMetric: 58.9675 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 73/1000
2023-09-30 07:38:37.230 
Epoch 73/1000 
	 loss: 58.7994, MinusLogProbMetric: 58.7994, val_loss: 58.8995, val_MinusLogProbMetric: 58.8995

Epoch 73: val_loss did not improve from 58.78777
196/196 - 41s - loss: 58.7994 - MinusLogProbMetric: 58.7994 - val_loss: 58.8995 - val_MinusLogProbMetric: 58.8995 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 74/1000
2023-09-30 07:39:19.643 
Epoch 74/1000 
	 loss: 59.8845, MinusLogProbMetric: 59.8845, val_loss: 59.2753, val_MinusLogProbMetric: 59.2753

Epoch 74: val_loss did not improve from 58.78777
196/196 - 42s - loss: 59.8845 - MinusLogProbMetric: 59.8845 - val_loss: 59.2753 - val_MinusLogProbMetric: 59.2753 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 75/1000
2023-09-30 07:40:01.498 
Epoch 75/1000 
	 loss: 58.8110, MinusLogProbMetric: 58.8110, val_loss: 60.5592, val_MinusLogProbMetric: 60.5592

Epoch 75: val_loss did not improve from 58.78777
196/196 - 42s - loss: 58.8110 - MinusLogProbMetric: 58.8110 - val_loss: 60.5592 - val_MinusLogProbMetric: 60.5592 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 76/1000
2023-09-30 07:40:43.703 
Epoch 76/1000 
	 loss: 58.7918, MinusLogProbMetric: 58.7918, val_loss: 58.9843, val_MinusLogProbMetric: 58.9843

Epoch 76: val_loss did not improve from 58.78777
196/196 - 42s - loss: 58.7918 - MinusLogProbMetric: 58.7918 - val_loss: 58.9843 - val_MinusLogProbMetric: 58.9843 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 77/1000
2023-09-30 07:41:25.835 
Epoch 77/1000 
	 loss: 58.2569, MinusLogProbMetric: 58.2569, val_loss: 58.7937, val_MinusLogProbMetric: 58.7937

Epoch 77: val_loss did not improve from 58.78777
196/196 - 42s - loss: 58.2569 - MinusLogProbMetric: 58.2569 - val_loss: 58.7937 - val_MinusLogProbMetric: 58.7937 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 78/1000
2023-09-30 07:42:08.252 
Epoch 78/1000 
	 loss: 58.9635, MinusLogProbMetric: 58.9635, val_loss: 58.4687, val_MinusLogProbMetric: 58.4687

Epoch 78: val_loss improved from 58.78777 to 58.46872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 58.9635 - MinusLogProbMetric: 58.9635 - val_loss: 58.4687 - val_MinusLogProbMetric: 58.4687 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 79/1000
2023-09-30 07:42:52.215 
Epoch 79/1000 
	 loss: 58.9385, MinusLogProbMetric: 58.9385, val_loss: 59.3661, val_MinusLogProbMetric: 59.3661

Epoch 79: val_loss did not improve from 58.46872
196/196 - 42s - loss: 58.9385 - MinusLogProbMetric: 58.9385 - val_loss: 59.3661 - val_MinusLogProbMetric: 59.3661 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 80/1000
2023-09-30 07:43:35.280 
Epoch 80/1000 
	 loss: 58.1733, MinusLogProbMetric: 58.1733, val_loss: 58.1423, val_MinusLogProbMetric: 58.1423

Epoch 80: val_loss improved from 58.46872 to 58.14231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 58.1733 - MinusLogProbMetric: 58.1733 - val_loss: 58.1423 - val_MinusLogProbMetric: 58.1423 - lr: 3.3333e-04 - 45s/epoch - 231ms/step
Epoch 81/1000
2023-09-30 07:44:18.736 
Epoch 81/1000 
	 loss: 57.5642, MinusLogProbMetric: 57.5642, val_loss: 57.2085, val_MinusLogProbMetric: 57.2085

Epoch 81: val_loss improved from 58.14231 to 57.20845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 57.5642 - MinusLogProbMetric: 57.5642 - val_loss: 57.2085 - val_MinusLogProbMetric: 57.2085 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 82/1000
2023-09-30 07:45:01.874 
Epoch 82/1000 
	 loss: 58.4609, MinusLogProbMetric: 58.4609, val_loss: 58.0902, val_MinusLogProbMetric: 58.0902

Epoch 82: val_loss did not improve from 57.20845
196/196 - 42s - loss: 58.4609 - MinusLogProbMetric: 58.4609 - val_loss: 58.0902 - val_MinusLogProbMetric: 58.0902 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 83/1000
2023-09-30 07:45:44.525 
Epoch 83/1000 
	 loss: 57.7475, MinusLogProbMetric: 57.7475, val_loss: 57.5849, val_MinusLogProbMetric: 57.5849

Epoch 83: val_loss did not improve from 57.20845
196/196 - 43s - loss: 57.7475 - MinusLogProbMetric: 57.7475 - val_loss: 57.5849 - val_MinusLogProbMetric: 57.5849 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 84/1000
2023-09-30 07:46:27.356 
Epoch 84/1000 
	 loss: 57.2709, MinusLogProbMetric: 57.2709, val_loss: 57.9221, val_MinusLogProbMetric: 57.9221

Epoch 84: val_loss did not improve from 57.20845
196/196 - 43s - loss: 57.2709 - MinusLogProbMetric: 57.2709 - val_loss: 57.9221 - val_MinusLogProbMetric: 57.9221 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 85/1000
2023-09-30 07:47:08.894 
Epoch 85/1000 
	 loss: 57.0474, MinusLogProbMetric: 57.0474, val_loss: 57.2751, val_MinusLogProbMetric: 57.2751

Epoch 85: val_loss did not improve from 57.20845
196/196 - 42s - loss: 57.0474 - MinusLogProbMetric: 57.0474 - val_loss: 57.2751 - val_MinusLogProbMetric: 57.2751 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 86/1000
2023-09-30 07:47:50.871 
Epoch 86/1000 
	 loss: 58.4095, MinusLogProbMetric: 58.4095, val_loss: 57.6082, val_MinusLogProbMetric: 57.6082

Epoch 86: val_loss did not improve from 57.20845
196/196 - 42s - loss: 58.4095 - MinusLogProbMetric: 58.4095 - val_loss: 57.6082 - val_MinusLogProbMetric: 57.6082 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 87/1000
2023-09-30 07:48:31.361 
Epoch 87/1000 
	 loss: 57.2668, MinusLogProbMetric: 57.2668, val_loss: 57.0332, val_MinusLogProbMetric: 57.0332

Epoch 87: val_loss improved from 57.20845 to 57.03323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 57.2668 - MinusLogProbMetric: 57.2668 - val_loss: 57.0332 - val_MinusLogProbMetric: 57.0332 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 88/1000
2023-09-30 07:49:14.054 
Epoch 88/1000 
	 loss: 56.8027, MinusLogProbMetric: 56.8027, val_loss: 57.2857, val_MinusLogProbMetric: 57.2857

Epoch 88: val_loss did not improve from 57.03323
196/196 - 41s - loss: 56.8027 - MinusLogProbMetric: 56.8027 - val_loss: 57.2857 - val_MinusLogProbMetric: 57.2857 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 89/1000
2023-09-30 07:49:56.808 
Epoch 89/1000 
	 loss: 57.1343, MinusLogProbMetric: 57.1343, val_loss: 57.3486, val_MinusLogProbMetric: 57.3486

Epoch 89: val_loss did not improve from 57.03323
196/196 - 43s - loss: 57.1343 - MinusLogProbMetric: 57.1343 - val_loss: 57.3486 - val_MinusLogProbMetric: 57.3486 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 90/1000
2023-09-30 07:50:39.078 
Epoch 90/1000 
	 loss: 56.5452, MinusLogProbMetric: 56.5452, val_loss: 57.0974, val_MinusLogProbMetric: 57.0974

Epoch 90: val_loss did not improve from 57.03323
196/196 - 42s - loss: 56.5452 - MinusLogProbMetric: 56.5452 - val_loss: 57.0974 - val_MinusLogProbMetric: 57.0974 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 91/1000
2023-09-30 07:51:20.853 
Epoch 91/1000 
	 loss: 57.2108, MinusLogProbMetric: 57.2108, val_loss: 57.0236, val_MinusLogProbMetric: 57.0236

Epoch 91: val_loss improved from 57.03323 to 57.02359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 57.2108 - MinusLogProbMetric: 57.2108 - val_loss: 57.0236 - val_MinusLogProbMetric: 57.0236 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 92/1000
2023-09-30 07:52:02.512 
Epoch 92/1000 
	 loss: 56.5428, MinusLogProbMetric: 56.5428, val_loss: 56.0899, val_MinusLogProbMetric: 56.0899

Epoch 92: val_loss improved from 57.02359 to 56.08987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 56.5428 - MinusLogProbMetric: 56.5428 - val_loss: 56.0899 - val_MinusLogProbMetric: 56.0899 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 93/1000
2023-09-30 07:52:43.709 
Epoch 93/1000 
	 loss: 56.2842, MinusLogProbMetric: 56.2842, val_loss: 56.3226, val_MinusLogProbMetric: 56.3226

Epoch 93: val_loss did not improve from 56.08987
196/196 - 40s - loss: 56.2842 - MinusLogProbMetric: 56.2842 - val_loss: 56.3226 - val_MinusLogProbMetric: 56.3226 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 94/1000
2023-09-30 07:53:23.615 
Epoch 94/1000 
	 loss: 56.0509, MinusLogProbMetric: 56.0509, val_loss: 56.4570, val_MinusLogProbMetric: 56.4570

Epoch 94: val_loss did not improve from 56.08987
196/196 - 40s - loss: 56.0509 - MinusLogProbMetric: 56.0509 - val_loss: 56.4570 - val_MinusLogProbMetric: 56.4570 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 95/1000
2023-09-30 07:54:03.834 
Epoch 95/1000 
	 loss: 55.8865, MinusLogProbMetric: 55.8865, val_loss: 57.1372, val_MinusLogProbMetric: 57.1372

Epoch 95: val_loss did not improve from 56.08987
196/196 - 40s - loss: 55.8865 - MinusLogProbMetric: 55.8865 - val_loss: 57.1372 - val_MinusLogProbMetric: 57.1372 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 96/1000
2023-09-30 07:54:43.567 
Epoch 96/1000 
	 loss: 55.7801, MinusLogProbMetric: 55.7801, val_loss: 57.7928, val_MinusLogProbMetric: 57.7928

Epoch 96: val_loss did not improve from 56.08987
196/196 - 40s - loss: 55.7801 - MinusLogProbMetric: 55.7801 - val_loss: 57.7928 - val_MinusLogProbMetric: 57.7928 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 97/1000
2023-09-30 07:55:22.769 
Epoch 97/1000 
	 loss: 54.8895, MinusLogProbMetric: 54.8895, val_loss: 53.6894, val_MinusLogProbMetric: 53.6894

Epoch 97: val_loss improved from 56.08987 to 53.68935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 54.8895 - MinusLogProbMetric: 54.8895 - val_loss: 53.6894 - val_MinusLogProbMetric: 53.6894 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 98/1000
2023-09-30 07:56:03.850 
Epoch 98/1000 
	 loss: 46.3642, MinusLogProbMetric: 46.3642, val_loss: 45.1400, val_MinusLogProbMetric: 45.1400

Epoch 98: val_loss improved from 53.68935 to 45.13998, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 46.3642 - MinusLogProbMetric: 46.3642 - val_loss: 45.1400 - val_MinusLogProbMetric: 45.1400 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 99/1000
2023-09-30 07:56:44.770 
Epoch 99/1000 
	 loss: 44.8947, MinusLogProbMetric: 44.8947, val_loss: 44.6451, val_MinusLogProbMetric: 44.6451

Epoch 99: val_loss improved from 45.13998 to 44.64505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 44.8947 - MinusLogProbMetric: 44.8947 - val_loss: 44.6451 - val_MinusLogProbMetric: 44.6451 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 100/1000
2023-09-30 07:57:26.059 
Epoch 100/1000 
	 loss: 44.7012, MinusLogProbMetric: 44.7012, val_loss: 44.3072, val_MinusLogProbMetric: 44.3072

Epoch 100: val_loss improved from 44.64505 to 44.30717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 40s - loss: 44.7012 - MinusLogProbMetric: 44.7012 - val_loss: 44.3072 - val_MinusLogProbMetric: 44.3072 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 101/1000
2023-09-30 07:58:06.691 
Epoch 101/1000 
	 loss: 44.3836, MinusLogProbMetric: 44.3836, val_loss: 44.7951, val_MinusLogProbMetric: 44.7951

Epoch 101: val_loss did not improve from 44.30717
196/196 - 40s - loss: 44.3836 - MinusLogProbMetric: 44.3836 - val_loss: 44.7951 - val_MinusLogProbMetric: 44.7951 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 102/1000
2023-09-30 07:58:46.591 
Epoch 102/1000 
	 loss: 44.4761, MinusLogProbMetric: 44.4761, val_loss: 45.2446, val_MinusLogProbMetric: 45.2446

Epoch 102: val_loss did not improve from 44.30717
196/196 - 40s - loss: 44.4761 - MinusLogProbMetric: 44.4761 - val_loss: 45.2446 - val_MinusLogProbMetric: 45.2446 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 103/1000
2023-09-30 07:59:25.962 
Epoch 103/1000 
	 loss: 44.3362, MinusLogProbMetric: 44.3362, val_loss: 44.6510, val_MinusLogProbMetric: 44.6510

Epoch 103: val_loss did not improve from 44.30717
196/196 - 39s - loss: 44.3362 - MinusLogProbMetric: 44.3362 - val_loss: 44.6510 - val_MinusLogProbMetric: 44.6510 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 104/1000
2023-09-30 08:00:06.174 
Epoch 104/1000 
	 loss: 163.2784, MinusLogProbMetric: 163.2784, val_loss: 116.5327, val_MinusLogProbMetric: 116.5327

Epoch 104: val_loss did not improve from 44.30717
196/196 - 40s - loss: 163.2784 - MinusLogProbMetric: 163.2784 - val_loss: 116.5327 - val_MinusLogProbMetric: 116.5327 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 105/1000
2023-09-30 08:00:45.116 
Epoch 105/1000 
	 loss: 100.3785, MinusLogProbMetric: 100.3785, val_loss: 89.6122, val_MinusLogProbMetric: 89.6122

Epoch 105: val_loss did not improve from 44.30717
196/196 - 39s - loss: 100.3785 - MinusLogProbMetric: 100.3785 - val_loss: 89.6122 - val_MinusLogProbMetric: 89.6122 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 106/1000
2023-09-30 08:01:25.238 
Epoch 106/1000 
	 loss: 85.5782, MinusLogProbMetric: 85.5782, val_loss: 81.6104, val_MinusLogProbMetric: 81.6104

Epoch 106: val_loss did not improve from 44.30717
196/196 - 40s - loss: 85.5782 - MinusLogProbMetric: 85.5782 - val_loss: 81.6104 - val_MinusLogProbMetric: 81.6104 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 107/1000
2023-09-30 08:02:04.362 
Epoch 107/1000 
	 loss: 81.5738, MinusLogProbMetric: 81.5738, val_loss: 76.0663, val_MinusLogProbMetric: 76.0663

Epoch 107: val_loss did not improve from 44.30717
196/196 - 39s - loss: 81.5738 - MinusLogProbMetric: 81.5738 - val_loss: 76.0663 - val_MinusLogProbMetric: 76.0663 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 108/1000
2023-09-30 08:02:44.462 
Epoch 108/1000 
	 loss: 74.9028, MinusLogProbMetric: 74.9028, val_loss: 72.8443, val_MinusLogProbMetric: 72.8443

Epoch 108: val_loss did not improve from 44.30717
196/196 - 40s - loss: 74.9028 - MinusLogProbMetric: 74.9028 - val_loss: 72.8443 - val_MinusLogProbMetric: 72.8443 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 109/1000
2023-09-30 08:03:23.934 
Epoch 109/1000 
	 loss: 71.4408, MinusLogProbMetric: 71.4408, val_loss: 70.4043, val_MinusLogProbMetric: 70.4043

Epoch 109: val_loss did not improve from 44.30717
196/196 - 39s - loss: 71.4408 - MinusLogProbMetric: 71.4408 - val_loss: 70.4043 - val_MinusLogProbMetric: 70.4043 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 110/1000
2023-09-30 08:04:03.160 
Epoch 110/1000 
	 loss: 69.2771, MinusLogProbMetric: 69.2771, val_loss: 68.7462, val_MinusLogProbMetric: 68.7462

Epoch 110: val_loss did not improve from 44.30717
196/196 - 39s - loss: 69.2771 - MinusLogProbMetric: 69.2771 - val_loss: 68.7462 - val_MinusLogProbMetric: 68.7462 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 111/1000
2023-09-30 08:04:44.050 
Epoch 111/1000 
	 loss: 67.8647, MinusLogProbMetric: 67.8647, val_loss: 67.0645, val_MinusLogProbMetric: 67.0645

Epoch 111: val_loss did not improve from 44.30717
196/196 - 41s - loss: 67.8647 - MinusLogProbMetric: 67.8647 - val_loss: 67.0645 - val_MinusLogProbMetric: 67.0645 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 112/1000
2023-09-30 08:05:23.289 
Epoch 112/1000 
	 loss: 66.5955, MinusLogProbMetric: 66.5955, val_loss: 66.2516, val_MinusLogProbMetric: 66.2516

Epoch 112: val_loss did not improve from 44.30717
196/196 - 39s - loss: 66.5955 - MinusLogProbMetric: 66.5955 - val_loss: 66.2516 - val_MinusLogProbMetric: 66.2516 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 113/1000
2023-09-30 08:06:03.610 
Epoch 113/1000 
	 loss: 65.5124, MinusLogProbMetric: 65.5124, val_loss: 65.4249, val_MinusLogProbMetric: 65.4249

Epoch 113: val_loss did not improve from 44.30717
196/196 - 40s - loss: 65.5124 - MinusLogProbMetric: 65.5124 - val_loss: 65.4249 - val_MinusLogProbMetric: 65.4249 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 114/1000
2023-09-30 08:06:43.547 
Epoch 114/1000 
	 loss: 64.5901, MinusLogProbMetric: 64.5901, val_loss: 64.2829, val_MinusLogProbMetric: 64.2829

Epoch 114: val_loss did not improve from 44.30717
196/196 - 40s - loss: 64.5901 - MinusLogProbMetric: 64.5901 - val_loss: 64.2829 - val_MinusLogProbMetric: 64.2829 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 115/1000
2023-09-30 08:07:22.410 
Epoch 115/1000 
	 loss: 63.8328, MinusLogProbMetric: 63.8328, val_loss: 63.8512, val_MinusLogProbMetric: 63.8512

Epoch 115: val_loss did not improve from 44.30717
196/196 - 39s - loss: 63.8328 - MinusLogProbMetric: 63.8328 - val_loss: 63.8512 - val_MinusLogProbMetric: 63.8512 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 116/1000
2023-09-30 08:08:01.983 
Epoch 116/1000 
	 loss: 63.1895, MinusLogProbMetric: 63.1895, val_loss: 63.2130, val_MinusLogProbMetric: 63.2130

Epoch 116: val_loss did not improve from 44.30717
196/196 - 40s - loss: 63.1895 - MinusLogProbMetric: 63.1895 - val_loss: 63.2130 - val_MinusLogProbMetric: 63.2130 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 117/1000
2023-09-30 08:08:41.483 
Epoch 117/1000 
	 loss: 62.7006, MinusLogProbMetric: 62.7006, val_loss: 62.8500, val_MinusLogProbMetric: 62.8500

Epoch 117: val_loss did not improve from 44.30717
196/196 - 39s - loss: 62.7006 - MinusLogProbMetric: 62.7006 - val_loss: 62.8500 - val_MinusLogProbMetric: 62.8500 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 118/1000
2023-09-30 08:09:19.607 
Epoch 118/1000 
	 loss: 62.1232, MinusLogProbMetric: 62.1232, val_loss: 62.5089, val_MinusLogProbMetric: 62.5089

Epoch 118: val_loss did not improve from 44.30717
196/196 - 38s - loss: 62.1232 - MinusLogProbMetric: 62.1232 - val_loss: 62.5089 - val_MinusLogProbMetric: 62.5089 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 119/1000
2023-09-30 08:09:59.245 
Epoch 119/1000 
	 loss: 61.8608, MinusLogProbMetric: 61.8608, val_loss: 61.9267, val_MinusLogProbMetric: 61.9267

Epoch 119: val_loss did not improve from 44.30717
196/196 - 40s - loss: 61.8608 - MinusLogProbMetric: 61.8608 - val_loss: 61.9267 - val_MinusLogProbMetric: 61.9267 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 120/1000
2023-09-30 08:10:38.218 
Epoch 120/1000 
	 loss: 61.5269, MinusLogProbMetric: 61.5269, val_loss: 61.7448, val_MinusLogProbMetric: 61.7448

Epoch 120: val_loss did not improve from 44.30717
196/196 - 39s - loss: 61.5269 - MinusLogProbMetric: 61.5269 - val_loss: 61.7448 - val_MinusLogProbMetric: 61.7448 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 121/1000
2023-09-30 08:11:17.472 
Epoch 121/1000 
	 loss: 61.1674, MinusLogProbMetric: 61.1674, val_loss: 60.8885, val_MinusLogProbMetric: 60.8885

Epoch 121: val_loss did not improve from 44.30717
196/196 - 39s - loss: 61.1674 - MinusLogProbMetric: 61.1674 - val_loss: 60.8885 - val_MinusLogProbMetric: 60.8885 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 122/1000
2023-09-30 08:11:56.970 
Epoch 122/1000 
	 loss: 60.8218, MinusLogProbMetric: 60.8218, val_loss: 60.7864, val_MinusLogProbMetric: 60.7864

Epoch 122: val_loss did not improve from 44.30717
196/196 - 39s - loss: 60.8218 - MinusLogProbMetric: 60.8218 - val_loss: 60.7864 - val_MinusLogProbMetric: 60.7864 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 123/1000
2023-09-30 08:12:36.331 
Epoch 123/1000 
	 loss: 60.6043, MinusLogProbMetric: 60.6043, val_loss: 60.9020, val_MinusLogProbMetric: 60.9020

Epoch 123: val_loss did not improve from 44.30717
196/196 - 39s - loss: 60.6043 - MinusLogProbMetric: 60.6043 - val_loss: 60.9020 - val_MinusLogProbMetric: 60.9020 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 124/1000
2023-09-30 08:13:15.465 
Epoch 124/1000 
	 loss: 60.4004, MinusLogProbMetric: 60.4004, val_loss: 60.2931, val_MinusLogProbMetric: 60.2931

Epoch 124: val_loss did not improve from 44.30717
196/196 - 39s - loss: 60.4004 - MinusLogProbMetric: 60.4004 - val_loss: 60.2931 - val_MinusLogProbMetric: 60.2931 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 125/1000
2023-09-30 08:13:53.796 
Epoch 125/1000 
	 loss: 60.1144, MinusLogProbMetric: 60.1144, val_loss: 60.2969, val_MinusLogProbMetric: 60.2969

Epoch 125: val_loss did not improve from 44.30717
196/196 - 38s - loss: 60.1144 - MinusLogProbMetric: 60.1144 - val_loss: 60.2969 - val_MinusLogProbMetric: 60.2969 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 126/1000
2023-09-30 08:14:32.316 
Epoch 126/1000 
	 loss: 60.3701, MinusLogProbMetric: 60.3701, val_loss: 60.2351, val_MinusLogProbMetric: 60.2351

Epoch 126: val_loss did not improve from 44.30717
196/196 - 39s - loss: 60.3701 - MinusLogProbMetric: 60.3701 - val_loss: 60.2351 - val_MinusLogProbMetric: 60.2351 - lr: 3.3333e-04 - 39s/epoch - 196ms/step
Epoch 127/1000
2023-09-30 08:15:12.002 
Epoch 127/1000 
	 loss: 59.9676, MinusLogProbMetric: 59.9676, val_loss: 60.0946, val_MinusLogProbMetric: 60.0946

Epoch 127: val_loss did not improve from 44.30717
196/196 - 40s - loss: 59.9676 - MinusLogProbMetric: 59.9676 - val_loss: 60.0946 - val_MinusLogProbMetric: 60.0946 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 128/1000
2023-09-30 08:15:50.684 
Epoch 128/1000 
	 loss: 59.6903, MinusLogProbMetric: 59.6903, val_loss: 59.9426, val_MinusLogProbMetric: 59.9426

Epoch 128: val_loss did not improve from 44.30717
196/196 - 39s - loss: 59.6903 - MinusLogProbMetric: 59.6903 - val_loss: 59.9426 - val_MinusLogProbMetric: 59.9426 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 129/1000
2023-09-30 08:16:31.369 
Epoch 129/1000 
	 loss: 59.3595, MinusLogProbMetric: 59.3595, val_loss: 59.1275, val_MinusLogProbMetric: 59.1275

Epoch 129: val_loss did not improve from 44.30717
196/196 - 41s - loss: 59.3595 - MinusLogProbMetric: 59.3595 - val_loss: 59.1275 - val_MinusLogProbMetric: 59.1275 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 130/1000
2023-09-30 08:17:16.152 
Epoch 130/1000 
	 loss: 59.2082, MinusLogProbMetric: 59.2082, val_loss: 59.0915, val_MinusLogProbMetric: 59.0915

Epoch 130: val_loss did not improve from 44.30717
196/196 - 45s - loss: 59.2082 - MinusLogProbMetric: 59.2082 - val_loss: 59.0915 - val_MinusLogProbMetric: 59.0915 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 131/1000
2023-09-30 08:17:57.267 
Epoch 131/1000 
	 loss: 59.1015, MinusLogProbMetric: 59.1015, val_loss: 59.1205, val_MinusLogProbMetric: 59.1205

Epoch 131: val_loss did not improve from 44.30717
196/196 - 41s - loss: 59.1015 - MinusLogProbMetric: 59.1015 - val_loss: 59.1205 - val_MinusLogProbMetric: 59.1205 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 132/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 165: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-30 08:18:32.772 
Epoch 132/1000 
	 loss: nan, MinusLogProbMetric: 58.7417, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 132: val_loss did not improve from 44.30717
196/196 - 36s - loss: nan - MinusLogProbMetric: 58.7417 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 36s/epoch - 181ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 347.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_347
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_185"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_186 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7fc0602cafe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb2dc43a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb2dc43a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbfb3a623e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc00d7a4e50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc00d7a53c0>, <keras.callbacks.ModelCheckpoint object at 0x7fc00d7a5480>, <keras.callbacks.EarlyStopping object at 0x7fc00d7a56f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc00d7a5720>, <keras.callbacks.TerminateOnNaN object at 0x7fc00d7a5360>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 347/720 with hyperparameters:
timestamp = 2023-09-30 08:18:42.410962
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-09-30 08:22:14.592 
Epoch 1/1000 
	 loss: 56.3497, MinusLogProbMetric: 56.3497, val_loss: 44.4539, val_MinusLogProbMetric: 44.4539

Epoch 1: val_loss improved from inf to 44.45388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 213s - loss: 56.3497 - MinusLogProbMetric: 56.3497 - val_loss: 44.4539 - val_MinusLogProbMetric: 44.4539 - lr: 1.1111e-04 - 213s/epoch - 1s/step
Epoch 2/1000
2023-09-30 08:23:00.580 
Epoch 2/1000 
	 loss: 44.3762, MinusLogProbMetric: 44.3762, val_loss: 44.3970, val_MinusLogProbMetric: 44.3970

Epoch 2: val_loss improved from 44.45388 to 44.39701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 44.3762 - MinusLogProbMetric: 44.3762 - val_loss: 44.3970 - val_MinusLogProbMetric: 44.3970 - lr: 1.1111e-04 - 45s/epoch - 230ms/step
Epoch 3/1000
2023-09-30 08:23:45.035 
Epoch 3/1000 
	 loss: 44.2880, MinusLogProbMetric: 44.2880, val_loss: 45.6339, val_MinusLogProbMetric: 45.6339

Epoch 3: val_loss did not improve from 44.39701
196/196 - 43s - loss: 44.2880 - MinusLogProbMetric: 44.2880 - val_loss: 45.6339 - val_MinusLogProbMetric: 45.6339 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 4/1000
2023-09-30 08:24:28.625 
Epoch 4/1000 
	 loss: 44.1282, MinusLogProbMetric: 44.1282, val_loss: 44.4152, val_MinusLogProbMetric: 44.4152

Epoch 4: val_loss did not improve from 44.39701
196/196 - 44s - loss: 44.1282 - MinusLogProbMetric: 44.1282 - val_loss: 44.4152 - val_MinusLogProbMetric: 44.4152 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 5/1000
2023-09-30 08:25:12.378 
Epoch 5/1000 
	 loss: 50.5257, MinusLogProbMetric: 50.5257, val_loss: 45.3719, val_MinusLogProbMetric: 45.3719

Epoch 5: val_loss did not improve from 44.39701
196/196 - 44s - loss: 50.5257 - MinusLogProbMetric: 50.5257 - val_loss: 45.3719 - val_MinusLogProbMetric: 45.3719 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 6/1000
2023-09-30 08:25:54.528 
Epoch 6/1000 
	 loss: 44.4978, MinusLogProbMetric: 44.4978, val_loss: 44.6158, val_MinusLogProbMetric: 44.6158

Epoch 6: val_loss did not improve from 44.39701
196/196 - 42s - loss: 44.4978 - MinusLogProbMetric: 44.4978 - val_loss: 44.6158 - val_MinusLogProbMetric: 44.6158 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 7/1000
2023-09-30 08:26:38.272 
Epoch 7/1000 
	 loss: 43.9081, MinusLogProbMetric: 43.9081, val_loss: 44.9879, val_MinusLogProbMetric: 44.9879

Epoch 7: val_loss did not improve from 44.39701
196/196 - 44s - loss: 43.9081 - MinusLogProbMetric: 43.9081 - val_loss: 44.9879 - val_MinusLogProbMetric: 44.9879 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 8/1000
2023-09-30 08:27:21.700 
Epoch 8/1000 
	 loss: 44.0047, MinusLogProbMetric: 44.0047, val_loss: 43.6157, val_MinusLogProbMetric: 43.6157

Epoch 8: val_loss improved from 44.39701 to 43.61566, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 44.0047 - MinusLogProbMetric: 44.0047 - val_loss: 43.6157 - val_MinusLogProbMetric: 43.6157 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 9/1000
2023-09-30 08:28:05.976 
Epoch 9/1000 
	 loss: 43.5223, MinusLogProbMetric: 43.5223, val_loss: 43.6606, val_MinusLogProbMetric: 43.6606

Epoch 9: val_loss did not improve from 43.61566
196/196 - 43s - loss: 43.5223 - MinusLogProbMetric: 43.5223 - val_loss: 43.6606 - val_MinusLogProbMetric: 43.6606 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 10/1000
2023-09-30 08:28:49.915 
Epoch 10/1000 
	 loss: 43.7728, MinusLogProbMetric: 43.7728, val_loss: 53.0682, val_MinusLogProbMetric: 53.0682

Epoch 10: val_loss did not improve from 43.61566
196/196 - 44s - loss: 43.7728 - MinusLogProbMetric: 43.7728 - val_loss: 53.0682 - val_MinusLogProbMetric: 53.0682 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 11/1000
2023-09-30 08:29:33.666 
Epoch 11/1000 
	 loss: 43.8210, MinusLogProbMetric: 43.8210, val_loss: 43.9793, val_MinusLogProbMetric: 43.9793

Epoch 11: val_loss did not improve from 43.61566
196/196 - 44s - loss: 43.8210 - MinusLogProbMetric: 43.8210 - val_loss: 43.9793 - val_MinusLogProbMetric: 43.9793 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 12/1000
2023-09-30 08:30:17.090 
Epoch 12/1000 
	 loss: 43.1427, MinusLogProbMetric: 43.1427, val_loss: 69.2834, val_MinusLogProbMetric: 69.2834

Epoch 12: val_loss did not improve from 43.61566
196/196 - 43s - loss: 43.1427 - MinusLogProbMetric: 43.1427 - val_loss: 69.2834 - val_MinusLogProbMetric: 69.2834 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 13/1000
2023-09-30 08:30:59.449 
Epoch 13/1000 
	 loss: 45.0566, MinusLogProbMetric: 45.0566, val_loss: 43.7586, val_MinusLogProbMetric: 43.7586

Epoch 13: val_loss did not improve from 43.61566
196/196 - 42s - loss: 45.0566 - MinusLogProbMetric: 45.0566 - val_loss: 43.7586 - val_MinusLogProbMetric: 43.7586 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 14/1000
2023-09-30 08:31:43.456 
Epoch 14/1000 
	 loss: 42.9759, MinusLogProbMetric: 42.9759, val_loss: 42.8117, val_MinusLogProbMetric: 42.8117

Epoch 14: val_loss improved from 43.61566 to 42.81170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 42.9759 - MinusLogProbMetric: 42.9759 - val_loss: 42.8117 - val_MinusLogProbMetric: 42.8117 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 15/1000
2023-09-30 08:32:27.810 
Epoch 15/1000 
	 loss: 42.5695, MinusLogProbMetric: 42.5695, val_loss: 42.8210, val_MinusLogProbMetric: 42.8210

Epoch 15: val_loss did not improve from 42.81170
196/196 - 44s - loss: 42.5695 - MinusLogProbMetric: 42.5695 - val_loss: 42.8210 - val_MinusLogProbMetric: 42.8210 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 16/1000
2023-09-30 08:33:10.512 
Epoch 16/1000 
	 loss: 41.4100, MinusLogProbMetric: 41.4100, val_loss: 41.1302, val_MinusLogProbMetric: 41.1302

Epoch 16: val_loss improved from 42.81170 to 41.13024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 41.4100 - MinusLogProbMetric: 41.4100 - val_loss: 41.1302 - val_MinusLogProbMetric: 41.1302 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 17/1000
2023-09-30 08:33:54.357 
Epoch 17/1000 
	 loss: 40.1159, MinusLogProbMetric: 40.1159, val_loss: 37.7325, val_MinusLogProbMetric: 37.7325

Epoch 17: val_loss improved from 41.13024 to 37.73249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 40.1159 - MinusLogProbMetric: 40.1159 - val_loss: 37.7325 - val_MinusLogProbMetric: 37.7325 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 18/1000
2023-09-30 08:34:39.010 
Epoch 18/1000 
	 loss: 37.1474, MinusLogProbMetric: 37.1474, val_loss: 37.5144, val_MinusLogProbMetric: 37.5144

Epoch 18: val_loss improved from 37.73249 to 37.51445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 37.1474 - MinusLogProbMetric: 37.1474 - val_loss: 37.5144 - val_MinusLogProbMetric: 37.5144 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 19/1000
2023-09-30 08:35:23.321 
Epoch 19/1000 
	 loss: 36.6973, MinusLogProbMetric: 36.6973, val_loss: 36.6408, val_MinusLogProbMetric: 36.6408

Epoch 19: val_loss improved from 37.51445 to 36.64081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 36.6973 - MinusLogProbMetric: 36.6973 - val_loss: 36.6408 - val_MinusLogProbMetric: 36.6408 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 20/1000
2023-09-30 08:36:07.211 
Epoch 20/1000 
	 loss: 36.3529, MinusLogProbMetric: 36.3529, val_loss: 36.6245, val_MinusLogProbMetric: 36.6245

Epoch 20: val_loss improved from 36.64081 to 36.62448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 45s - loss: 36.3529 - MinusLogProbMetric: 36.3529 - val_loss: 36.6245 - val_MinusLogProbMetric: 36.6245 - lr: 1.1111e-04 - 45s/epoch - 230ms/step
Epoch 21/1000
2023-09-30 08:37:02.384 
Epoch 21/1000 
	 loss: 36.1869, MinusLogProbMetric: 36.1869, val_loss: 36.3715, val_MinusLogProbMetric: 36.3715

Epoch 21: val_loss improved from 36.62448 to 36.37153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 55s - loss: 36.1869 - MinusLogProbMetric: 36.1869 - val_loss: 36.3715 - val_MinusLogProbMetric: 36.3715 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 22/1000
2023-09-30 08:38:06.748 
Epoch 22/1000 
	 loss: 35.8330, MinusLogProbMetric: 35.8330, val_loss: 36.0135, val_MinusLogProbMetric: 36.0135

Epoch 22: val_loss improved from 36.37153 to 36.01345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 67s - loss: 35.8330 - MinusLogProbMetric: 35.8330 - val_loss: 36.0135 - val_MinusLogProbMetric: 36.0135 - lr: 1.1111e-04 - 67s/epoch - 339ms/step
Epoch 23/1000
2023-09-30 08:39:08.146 
Epoch 23/1000 
	 loss: 35.7139, MinusLogProbMetric: 35.7139, val_loss: 35.8605, val_MinusLogProbMetric: 35.8605

Epoch 23: val_loss improved from 36.01345 to 35.86049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 35.7139 - MinusLogProbMetric: 35.7139 - val_loss: 35.8605 - val_MinusLogProbMetric: 35.8605 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 24/1000
2023-09-30 08:40:10.553 
Epoch 24/1000 
	 loss: 57.1669, MinusLogProbMetric: 57.1669, val_loss: 46.7750, val_MinusLogProbMetric: 46.7750

Epoch 24: val_loss did not improve from 35.86049
196/196 - 60s - loss: 57.1669 - MinusLogProbMetric: 57.1669 - val_loss: 46.7750 - val_MinusLogProbMetric: 46.7750 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 25/1000
2023-09-30 08:41:11.518 
Epoch 25/1000 
	 loss: 40.4332, MinusLogProbMetric: 40.4332, val_loss: 37.9628, val_MinusLogProbMetric: 37.9628

Epoch 25: val_loss did not improve from 35.86049
196/196 - 61s - loss: 40.4332 - MinusLogProbMetric: 40.4332 - val_loss: 37.9628 - val_MinusLogProbMetric: 37.9628 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 26/1000
2023-09-30 08:42:10.397 
Epoch 26/1000 
	 loss: 37.3077, MinusLogProbMetric: 37.3077, val_loss: 37.2712, val_MinusLogProbMetric: 37.2712

Epoch 26: val_loss did not improve from 35.86049
196/196 - 59s - loss: 37.3077 - MinusLogProbMetric: 37.3077 - val_loss: 37.2712 - val_MinusLogProbMetric: 37.2712 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 27/1000
2023-09-30 08:43:12.084 
Epoch 27/1000 
	 loss: 36.7017, MinusLogProbMetric: 36.7017, val_loss: 36.7243, val_MinusLogProbMetric: 36.7243

Epoch 27: val_loss did not improve from 35.86049
196/196 - 62s - loss: 36.7017 - MinusLogProbMetric: 36.7017 - val_loss: 36.7243 - val_MinusLogProbMetric: 36.7243 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 28/1000
2023-09-30 08:44:13.553 
Epoch 28/1000 
	 loss: 36.2768, MinusLogProbMetric: 36.2768, val_loss: 36.3297, val_MinusLogProbMetric: 36.3297

Epoch 28: val_loss did not improve from 35.86049
196/196 - 61s - loss: 36.2768 - MinusLogProbMetric: 36.2768 - val_loss: 36.3297 - val_MinusLogProbMetric: 36.3297 - lr: 1.1111e-04 - 61s/epoch - 314ms/step
Epoch 29/1000
2023-09-30 08:45:16.428 
Epoch 29/1000 
	 loss: 35.9158, MinusLogProbMetric: 35.9158, val_loss: 36.2236, val_MinusLogProbMetric: 36.2236

Epoch 29: val_loss did not improve from 35.86049
196/196 - 63s - loss: 35.9158 - MinusLogProbMetric: 35.9158 - val_loss: 36.2236 - val_MinusLogProbMetric: 36.2236 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 30/1000
2023-09-30 08:46:17.711 
Epoch 30/1000 
	 loss: 35.6224, MinusLogProbMetric: 35.6224, val_loss: 35.9644, val_MinusLogProbMetric: 35.9644

Epoch 30: val_loss did not improve from 35.86049
196/196 - 61s - loss: 35.6224 - MinusLogProbMetric: 35.6224 - val_loss: 35.9644 - val_MinusLogProbMetric: 35.9644 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 31/1000
2023-09-30 08:47:19.676 
Epoch 31/1000 
	 loss: 35.5594, MinusLogProbMetric: 35.5594, val_loss: 35.5479, val_MinusLogProbMetric: 35.5479

Epoch 31: val_loss improved from 35.86049 to 35.54792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 63s - loss: 35.5594 - MinusLogProbMetric: 35.5594 - val_loss: 35.5479 - val_MinusLogProbMetric: 35.5479 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 32/1000
2023-09-30 08:48:23.765 
Epoch 32/1000 
	 loss: 35.2167, MinusLogProbMetric: 35.2167, val_loss: 35.6117, val_MinusLogProbMetric: 35.6117

Epoch 32: val_loss did not improve from 35.54792
196/196 - 63s - loss: 35.2167 - MinusLogProbMetric: 35.2167 - val_loss: 35.6117 - val_MinusLogProbMetric: 35.6117 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 33/1000
2023-09-30 08:49:23.104 
Epoch 33/1000 
	 loss: 35.2142, MinusLogProbMetric: 35.2142, val_loss: 34.9343, val_MinusLogProbMetric: 34.9343

Epoch 33: val_loss improved from 35.54792 to 34.93428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 35.2142 - MinusLogProbMetric: 35.2142 - val_loss: 34.9343 - val_MinusLogProbMetric: 34.9343 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 34/1000
2023-09-30 08:50:25.746 
Epoch 34/1000 
	 loss: 35.0321, MinusLogProbMetric: 35.0321, val_loss: 34.7252, val_MinusLogProbMetric: 34.7252

Epoch 34: val_loss improved from 34.93428 to 34.72517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 64s - loss: 35.0321 - MinusLogProbMetric: 35.0321 - val_loss: 34.7252 - val_MinusLogProbMetric: 34.7252 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 35/1000
2023-09-30 08:51:28.774 
Epoch 35/1000 
	 loss: 35.0613, MinusLogProbMetric: 35.0613, val_loss: 34.9531, val_MinusLogProbMetric: 34.9531

Epoch 35: val_loss did not improve from 34.72517
196/196 - 61s - loss: 35.0613 - MinusLogProbMetric: 35.0613 - val_loss: 34.9531 - val_MinusLogProbMetric: 34.9531 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 36/1000
2023-09-30 08:52:28.783 
Epoch 36/1000 
	 loss: 34.9979, MinusLogProbMetric: 34.9979, val_loss: 35.1133, val_MinusLogProbMetric: 35.1133

Epoch 36: val_loss did not improve from 34.72517
196/196 - 60s - loss: 34.9979 - MinusLogProbMetric: 34.9979 - val_loss: 35.1133 - val_MinusLogProbMetric: 35.1133 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 37/1000
2023-09-30 08:53:28.307 
Epoch 37/1000 
	 loss: 34.8283, MinusLogProbMetric: 34.8283, val_loss: 34.5308, val_MinusLogProbMetric: 34.5308

Epoch 37: val_loss improved from 34.72517 to 34.53075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 34.8283 - MinusLogProbMetric: 34.8283 - val_loss: 34.5308 - val_MinusLogProbMetric: 34.5308 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 38/1000
2023-09-30 08:54:33.723 
Epoch 38/1000 
	 loss: 34.8579, MinusLogProbMetric: 34.8579, val_loss: 34.9856, val_MinusLogProbMetric: 34.9856

Epoch 38: val_loss did not improve from 34.53075
196/196 - 62s - loss: 34.8579 - MinusLogProbMetric: 34.8579 - val_loss: 34.9856 - val_MinusLogProbMetric: 34.9856 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 39/1000
2023-09-30 08:55:33.952 
Epoch 39/1000 
	 loss: 34.7102, MinusLogProbMetric: 34.7102, val_loss: 35.1669, val_MinusLogProbMetric: 35.1669

Epoch 39: val_loss did not improve from 34.53075
196/196 - 60s - loss: 34.7102 - MinusLogProbMetric: 34.7102 - val_loss: 35.1669 - val_MinusLogProbMetric: 35.1669 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 40/1000
2023-09-30 08:56:33.289 
Epoch 40/1000 
	 loss: 34.6799, MinusLogProbMetric: 34.6799, val_loss: 34.6746, val_MinusLogProbMetric: 34.6746

Epoch 40: val_loss did not improve from 34.53075
196/196 - 59s - loss: 34.6799 - MinusLogProbMetric: 34.6799 - val_loss: 34.6746 - val_MinusLogProbMetric: 34.6746 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 41/1000
2023-09-30 08:57:36.764 
Epoch 41/1000 
	 loss: 34.7041, MinusLogProbMetric: 34.7041, val_loss: 34.4064, val_MinusLogProbMetric: 34.4064

Epoch 41: val_loss improved from 34.53075 to 34.40644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 65s - loss: 34.7041 - MinusLogProbMetric: 34.7041 - val_loss: 34.4064 - val_MinusLogProbMetric: 34.4064 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 42/1000
2023-09-30 08:58:38.302 
Epoch 42/1000 
	 loss: 34.6013, MinusLogProbMetric: 34.6013, val_loss: 35.1521, val_MinusLogProbMetric: 35.1521

Epoch 42: val_loss did not improve from 34.40644
196/196 - 60s - loss: 34.6013 - MinusLogProbMetric: 34.6013 - val_loss: 35.1521 - val_MinusLogProbMetric: 35.1521 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 43/1000
2023-09-30 08:59:41.306 
Epoch 43/1000 
	 loss: 34.5047, MinusLogProbMetric: 34.5047, val_loss: 34.6157, val_MinusLogProbMetric: 34.6157

Epoch 43: val_loss did not improve from 34.40644
196/196 - 63s - loss: 34.5047 - MinusLogProbMetric: 34.5047 - val_loss: 34.6157 - val_MinusLogProbMetric: 34.6157 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 44/1000
2023-09-30 09:00:41.168 
Epoch 44/1000 
	 loss: 34.3811, MinusLogProbMetric: 34.3811, val_loss: 34.2765, val_MinusLogProbMetric: 34.2765

Epoch 44: val_loss improved from 34.40644 to 34.27654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 34.3811 - MinusLogProbMetric: 34.3811 - val_loss: 34.2765 - val_MinusLogProbMetric: 34.2765 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 45/1000
2023-09-30 09:01:43.127 
Epoch 45/1000 
	 loss: 34.3357, MinusLogProbMetric: 34.3357, val_loss: 34.6253, val_MinusLogProbMetric: 34.6253

Epoch 45: val_loss did not improve from 34.27654
196/196 - 60s - loss: 34.3357 - MinusLogProbMetric: 34.3357 - val_loss: 34.6253 - val_MinusLogProbMetric: 34.6253 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 46/1000
2023-09-30 09:02:41.362 
Epoch 46/1000 
	 loss: 34.2989, MinusLogProbMetric: 34.2989, val_loss: 34.5717, val_MinusLogProbMetric: 34.5717

Epoch 46: val_loss did not improve from 34.27654
196/196 - 58s - loss: 34.2989 - MinusLogProbMetric: 34.2989 - val_loss: 34.5717 - val_MinusLogProbMetric: 34.5717 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 47/1000
2023-09-30 09:03:44.430 
Epoch 47/1000 
	 loss: 34.3185, MinusLogProbMetric: 34.3185, val_loss: 35.0430, val_MinusLogProbMetric: 35.0430

Epoch 47: val_loss did not improve from 34.27654
196/196 - 63s - loss: 34.3185 - MinusLogProbMetric: 34.3185 - val_loss: 35.0430 - val_MinusLogProbMetric: 35.0430 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 48/1000
2023-09-30 09:04:44.045 
Epoch 48/1000 
	 loss: 34.2203, MinusLogProbMetric: 34.2203, val_loss: 34.4305, val_MinusLogProbMetric: 34.4305

Epoch 48: val_loss did not improve from 34.27654
196/196 - 60s - loss: 34.2203 - MinusLogProbMetric: 34.2203 - val_loss: 34.4305 - val_MinusLogProbMetric: 34.4305 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 49/1000
2023-09-30 09:05:44.542 
Epoch 49/1000 
	 loss: 34.2514, MinusLogProbMetric: 34.2514, val_loss: 35.4557, val_MinusLogProbMetric: 35.4557

Epoch 49: val_loss did not improve from 34.27654
196/196 - 60s - loss: 34.2514 - MinusLogProbMetric: 34.2514 - val_loss: 35.4557 - val_MinusLogProbMetric: 35.4557 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 50/1000
2023-09-30 09:06:45.075 
Epoch 50/1000 
	 loss: 34.2231, MinusLogProbMetric: 34.2231, val_loss: 34.3849, val_MinusLogProbMetric: 34.3849

Epoch 50: val_loss did not improve from 34.27654
196/196 - 61s - loss: 34.2231 - MinusLogProbMetric: 34.2231 - val_loss: 34.3849 - val_MinusLogProbMetric: 34.3849 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 51/1000
2023-09-30 09:07:47.279 
Epoch 51/1000 
	 loss: 34.3326, MinusLogProbMetric: 34.3326, val_loss: 34.3896, val_MinusLogProbMetric: 34.3896

Epoch 51: val_loss did not improve from 34.27654
196/196 - 62s - loss: 34.3326 - MinusLogProbMetric: 34.3326 - val_loss: 34.3896 - val_MinusLogProbMetric: 34.3896 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 52/1000
2023-09-30 09:08:45.813 
Epoch 52/1000 
	 loss: 33.9882, MinusLogProbMetric: 33.9882, val_loss: 35.2399, val_MinusLogProbMetric: 35.2399

Epoch 52: val_loss did not improve from 34.27654
196/196 - 59s - loss: 33.9882 - MinusLogProbMetric: 33.9882 - val_loss: 35.2399 - val_MinusLogProbMetric: 35.2399 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 53/1000
2023-09-30 09:09:46.683 
Epoch 53/1000 
	 loss: 34.0414, MinusLogProbMetric: 34.0414, val_loss: 34.4585, val_MinusLogProbMetric: 34.4585

Epoch 53: val_loss did not improve from 34.27654
196/196 - 61s - loss: 34.0414 - MinusLogProbMetric: 34.0414 - val_loss: 34.4585 - val_MinusLogProbMetric: 34.4585 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 54/1000
2023-09-30 09:10:46.834 
Epoch 54/1000 
	 loss: 34.0647, MinusLogProbMetric: 34.0647, val_loss: 33.9255, val_MinusLogProbMetric: 33.9255

Epoch 54: val_loss improved from 34.27654 to 33.92545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 61s - loss: 34.0647 - MinusLogProbMetric: 34.0647 - val_loss: 33.9255 - val_MinusLogProbMetric: 33.9255 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 55/1000
2023-09-30 09:11:48.553 
Epoch 55/1000 
	 loss: 34.0015, MinusLogProbMetric: 34.0015, val_loss: 33.7199, val_MinusLogProbMetric: 33.7199

Epoch 55: val_loss improved from 33.92545 to 33.71989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 34.0015 - MinusLogProbMetric: 34.0015 - val_loss: 33.7199 - val_MinusLogProbMetric: 33.7199 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 56/1000
2023-09-30 09:12:50.992 
Epoch 56/1000 
	 loss: 33.9804, MinusLogProbMetric: 33.9804, val_loss: 34.2317, val_MinusLogProbMetric: 34.2317

Epoch 56: val_loss did not improve from 33.71989
196/196 - 61s - loss: 33.9804 - MinusLogProbMetric: 33.9804 - val_loss: 34.2317 - val_MinusLogProbMetric: 34.2317 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 57/1000
2023-09-30 09:13:52.614 
Epoch 57/1000 
	 loss: 33.8744, MinusLogProbMetric: 33.8744, val_loss: 33.9121, val_MinusLogProbMetric: 33.9121

Epoch 57: val_loss did not improve from 33.71989
196/196 - 62s - loss: 33.8744 - MinusLogProbMetric: 33.8744 - val_loss: 33.9121 - val_MinusLogProbMetric: 33.9121 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 58/1000
2023-09-30 09:14:53.201 
Epoch 58/1000 
	 loss: 33.9281, MinusLogProbMetric: 33.9281, val_loss: 34.7883, val_MinusLogProbMetric: 34.7883

Epoch 58: val_loss did not improve from 33.71989
196/196 - 61s - loss: 33.9281 - MinusLogProbMetric: 33.9281 - val_loss: 34.7883 - val_MinusLogProbMetric: 34.7883 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 59/1000
2023-09-30 09:15:53.989 
Epoch 59/1000 
	 loss: 33.8285, MinusLogProbMetric: 33.8285, val_loss: 33.5644, val_MinusLogProbMetric: 33.5644

Epoch 59: val_loss improved from 33.71989 to 33.56444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 33.8285 - MinusLogProbMetric: 33.8285 - val_loss: 33.5644 - val_MinusLogProbMetric: 33.5644 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 60/1000
2023-09-30 09:16:56.797 
Epoch 60/1000 
	 loss: 33.8145, MinusLogProbMetric: 33.8145, val_loss: 34.1361, val_MinusLogProbMetric: 34.1361

Epoch 60: val_loss did not improve from 33.56444
196/196 - 62s - loss: 33.8145 - MinusLogProbMetric: 33.8145 - val_loss: 34.1361 - val_MinusLogProbMetric: 34.1361 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 61/1000
2023-09-30 09:17:57.957 
Epoch 61/1000 
	 loss: 33.9500, MinusLogProbMetric: 33.9500, val_loss: 34.7114, val_MinusLogProbMetric: 34.7114

Epoch 61: val_loss did not improve from 33.56444
196/196 - 61s - loss: 33.9500 - MinusLogProbMetric: 33.9500 - val_loss: 34.7114 - val_MinusLogProbMetric: 34.7114 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 62/1000
2023-09-30 09:18:58.884 
Epoch 62/1000 
	 loss: 33.7143, MinusLogProbMetric: 33.7143, val_loss: 34.0933, val_MinusLogProbMetric: 34.0933

Epoch 62: val_loss did not improve from 33.56444
196/196 - 61s - loss: 33.7143 - MinusLogProbMetric: 33.7143 - val_loss: 34.0933 - val_MinusLogProbMetric: 34.0933 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 63/1000
2023-09-30 09:19:58.730 
Epoch 63/1000 
	 loss: 33.7220, MinusLogProbMetric: 33.7220, val_loss: 33.8951, val_MinusLogProbMetric: 33.8951

Epoch 63: val_loss did not improve from 33.56444
196/196 - 60s - loss: 33.7220 - MinusLogProbMetric: 33.7220 - val_loss: 33.8951 - val_MinusLogProbMetric: 33.8951 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 64/1000
2023-09-30 09:20:58.565 
Epoch 64/1000 
	 loss: 33.7848, MinusLogProbMetric: 33.7848, val_loss: 33.6056, val_MinusLogProbMetric: 33.6056

Epoch 64: val_loss did not improve from 33.56444
196/196 - 60s - loss: 33.7848 - MinusLogProbMetric: 33.7848 - val_loss: 33.6056 - val_MinusLogProbMetric: 33.6056 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 65/1000
2023-09-30 09:21:57.887 
Epoch 65/1000 
	 loss: 33.7604, MinusLogProbMetric: 33.7604, val_loss: 33.8872, val_MinusLogProbMetric: 33.8872

Epoch 65: val_loss did not improve from 33.56444
196/196 - 59s - loss: 33.7604 - MinusLogProbMetric: 33.7604 - val_loss: 33.8872 - val_MinusLogProbMetric: 33.8872 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 66/1000
2023-09-30 09:22:58.723 
Epoch 66/1000 
	 loss: 33.6239, MinusLogProbMetric: 33.6239, val_loss: 33.5524, val_MinusLogProbMetric: 33.5524

Epoch 66: val_loss improved from 33.56444 to 33.55244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 33.6239 - MinusLogProbMetric: 33.6239 - val_loss: 33.5524 - val_MinusLogProbMetric: 33.5524 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 67/1000
2023-09-30 09:24:01.489 
Epoch 67/1000 
	 loss: 33.8138, MinusLogProbMetric: 33.8138, val_loss: 34.0450, val_MinusLogProbMetric: 34.0450

Epoch 67: val_loss did not improve from 33.55244
196/196 - 61s - loss: 33.8138 - MinusLogProbMetric: 33.8138 - val_loss: 34.0450 - val_MinusLogProbMetric: 34.0450 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 68/1000
2023-09-30 09:25:03.207 
Epoch 68/1000 
	 loss: 33.6330, MinusLogProbMetric: 33.6330, val_loss: 33.4977, val_MinusLogProbMetric: 33.4977

Epoch 68: val_loss improved from 33.55244 to 33.49767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 64s - loss: 33.6330 - MinusLogProbMetric: 33.6330 - val_loss: 33.4977 - val_MinusLogProbMetric: 33.4977 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 69/1000
2023-09-30 09:26:06.414 
Epoch 69/1000 
	 loss: 33.7643, MinusLogProbMetric: 33.7643, val_loss: 33.8140, val_MinusLogProbMetric: 33.8140

Epoch 69: val_loss did not improve from 33.49767
196/196 - 61s - loss: 33.7643 - MinusLogProbMetric: 33.7643 - val_loss: 33.8140 - val_MinusLogProbMetric: 33.8140 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 70/1000
2023-09-30 09:27:06.854 
Epoch 70/1000 
	 loss: 33.6189, MinusLogProbMetric: 33.6189, val_loss: 34.0102, val_MinusLogProbMetric: 34.0102

Epoch 70: val_loss did not improve from 33.49767
196/196 - 60s - loss: 33.6189 - MinusLogProbMetric: 33.6189 - val_loss: 34.0102 - val_MinusLogProbMetric: 34.0102 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 71/1000
2023-09-30 09:28:06.385 
Epoch 71/1000 
	 loss: 33.5729, MinusLogProbMetric: 33.5729, val_loss: 33.5376, val_MinusLogProbMetric: 33.5376

Epoch 71: val_loss did not improve from 33.49767
196/196 - 59s - loss: 33.5729 - MinusLogProbMetric: 33.5729 - val_loss: 33.5376 - val_MinusLogProbMetric: 33.5376 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 72/1000
2023-09-30 09:29:06.068 
Epoch 72/1000 
	 loss: 33.5899, MinusLogProbMetric: 33.5899, val_loss: 34.7105, val_MinusLogProbMetric: 34.7105

Epoch 72: val_loss did not improve from 33.49767
196/196 - 60s - loss: 33.5899 - MinusLogProbMetric: 33.5899 - val_loss: 34.7105 - val_MinusLogProbMetric: 34.7105 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 73/1000
2023-09-30 09:30:05.638 
Epoch 73/1000 
	 loss: 33.5211, MinusLogProbMetric: 33.5211, val_loss: 34.8048, val_MinusLogProbMetric: 34.8048

Epoch 73: val_loss did not improve from 33.49767
196/196 - 60s - loss: 33.5211 - MinusLogProbMetric: 33.5211 - val_loss: 34.8048 - val_MinusLogProbMetric: 34.8048 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 74/1000
2023-09-30 09:31:05.336 
Epoch 74/1000 
	 loss: 33.4365, MinusLogProbMetric: 33.4365, val_loss: 33.3689, val_MinusLogProbMetric: 33.3689

Epoch 74: val_loss improved from 33.49767 to 33.36892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 33.4365 - MinusLogProbMetric: 33.4365 - val_loss: 33.3689 - val_MinusLogProbMetric: 33.3689 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 75/1000
2023-09-30 09:32:09.219 
Epoch 75/1000 
	 loss: 33.4911, MinusLogProbMetric: 33.4911, val_loss: 34.1217, val_MinusLogProbMetric: 34.1217

Epoch 75: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.4911 - MinusLogProbMetric: 33.4911 - val_loss: 34.1217 - val_MinusLogProbMetric: 34.1217 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 76/1000
2023-09-30 09:33:09.371 
Epoch 76/1000 
	 loss: 33.4831, MinusLogProbMetric: 33.4831, val_loss: 33.8564, val_MinusLogProbMetric: 33.8564

Epoch 76: val_loss did not improve from 33.36892
196/196 - 60s - loss: 33.4831 - MinusLogProbMetric: 33.4831 - val_loss: 33.8564 - val_MinusLogProbMetric: 33.8564 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 77/1000
2023-09-30 09:34:13.295 
Epoch 77/1000 
	 loss: 33.3633, MinusLogProbMetric: 33.3633, val_loss: 33.4965, val_MinusLogProbMetric: 33.4965

Epoch 77: val_loss did not improve from 33.36892
196/196 - 64s - loss: 33.3633 - MinusLogProbMetric: 33.3633 - val_loss: 33.4965 - val_MinusLogProbMetric: 33.4965 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 78/1000
2023-09-30 09:35:14.721 
Epoch 78/1000 
	 loss: 33.4301, MinusLogProbMetric: 33.4301, val_loss: 34.4468, val_MinusLogProbMetric: 34.4468

Epoch 78: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.4301 - MinusLogProbMetric: 33.4301 - val_loss: 34.4468 - val_MinusLogProbMetric: 34.4468 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 79/1000
2023-09-30 09:36:16.387 
Epoch 79/1000 
	 loss: 33.4042, MinusLogProbMetric: 33.4042, val_loss: 33.6372, val_MinusLogProbMetric: 33.6372

Epoch 79: val_loss did not improve from 33.36892
196/196 - 62s - loss: 33.4042 - MinusLogProbMetric: 33.4042 - val_loss: 33.6372 - val_MinusLogProbMetric: 33.6372 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 80/1000
2023-09-30 09:37:17.743 
Epoch 80/1000 
	 loss: 33.3730, MinusLogProbMetric: 33.3730, val_loss: 33.4494, val_MinusLogProbMetric: 33.4494

Epoch 80: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.3730 - MinusLogProbMetric: 33.3730 - val_loss: 33.4494 - val_MinusLogProbMetric: 33.4494 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 81/1000
2023-09-30 09:38:18.697 
Epoch 81/1000 
	 loss: 33.3527, MinusLogProbMetric: 33.3527, val_loss: 33.4941, val_MinusLogProbMetric: 33.4941

Epoch 81: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.3527 - MinusLogProbMetric: 33.3527 - val_loss: 33.4941 - val_MinusLogProbMetric: 33.4941 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 82/1000
2023-09-30 09:39:19.741 
Epoch 82/1000 
	 loss: 33.4595, MinusLogProbMetric: 33.4595, val_loss: 34.9684, val_MinusLogProbMetric: 34.9684

Epoch 82: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.4595 - MinusLogProbMetric: 33.4595 - val_loss: 34.9684 - val_MinusLogProbMetric: 34.9684 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 83/1000
2023-09-30 09:40:23.459 
Epoch 83/1000 
	 loss: 33.2926, MinusLogProbMetric: 33.2926, val_loss: 33.4738, val_MinusLogProbMetric: 33.4738

Epoch 83: val_loss did not improve from 33.36892
196/196 - 64s - loss: 33.2926 - MinusLogProbMetric: 33.2926 - val_loss: 33.4738 - val_MinusLogProbMetric: 33.4738 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 84/1000
2023-09-30 09:41:24.365 
Epoch 84/1000 
	 loss: 33.3698, MinusLogProbMetric: 33.3698, val_loss: 33.5676, val_MinusLogProbMetric: 33.5676

Epoch 84: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.3698 - MinusLogProbMetric: 33.3698 - val_loss: 33.5676 - val_MinusLogProbMetric: 33.5676 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 85/1000
2023-09-30 09:42:27.193 
Epoch 85/1000 
	 loss: 33.2973, MinusLogProbMetric: 33.2973, val_loss: 34.6328, val_MinusLogProbMetric: 34.6328

Epoch 85: val_loss did not improve from 33.36892
196/196 - 63s - loss: 33.2973 - MinusLogProbMetric: 33.2973 - val_loss: 34.6328 - val_MinusLogProbMetric: 34.6328 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 86/1000
2023-09-30 09:43:28.563 
Epoch 86/1000 
	 loss: 33.2914, MinusLogProbMetric: 33.2914, val_loss: 33.4850, val_MinusLogProbMetric: 33.4850

Epoch 86: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.2914 - MinusLogProbMetric: 33.2914 - val_loss: 33.4850 - val_MinusLogProbMetric: 33.4850 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 87/1000
2023-09-30 09:44:29.827 
Epoch 87/1000 
	 loss: 33.3104, MinusLogProbMetric: 33.3104, val_loss: 33.5097, val_MinusLogProbMetric: 33.5097

Epoch 87: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.3104 - MinusLogProbMetric: 33.3104 - val_loss: 33.5097 - val_MinusLogProbMetric: 33.5097 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 88/1000
2023-09-30 09:45:30.103 
Epoch 88/1000 
	 loss: 33.2513, MinusLogProbMetric: 33.2513, val_loss: 34.0869, val_MinusLogProbMetric: 34.0869

Epoch 88: val_loss did not improve from 33.36892
196/196 - 60s - loss: 33.2513 - MinusLogProbMetric: 33.2513 - val_loss: 34.0869 - val_MinusLogProbMetric: 34.0869 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 89/1000
2023-09-30 09:46:31.657 
Epoch 89/1000 
	 loss: 33.2528, MinusLogProbMetric: 33.2528, val_loss: 33.6923, val_MinusLogProbMetric: 33.6923

Epoch 89: val_loss did not improve from 33.36892
196/196 - 62s - loss: 33.2528 - MinusLogProbMetric: 33.2528 - val_loss: 33.6923 - val_MinusLogProbMetric: 33.6923 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 90/1000
2023-09-30 09:47:32.962 
Epoch 90/1000 
	 loss: 33.2494, MinusLogProbMetric: 33.2494, val_loss: 33.5857, val_MinusLogProbMetric: 33.5857

Epoch 90: val_loss did not improve from 33.36892
196/196 - 61s - loss: 33.2494 - MinusLogProbMetric: 33.2494 - val_loss: 33.5857 - val_MinusLogProbMetric: 33.5857 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 91/1000
2023-09-30 09:48:31.731 
Epoch 91/1000 
	 loss: 33.1723, MinusLogProbMetric: 33.1723, val_loss: 34.1860, val_MinusLogProbMetric: 34.1860

Epoch 91: val_loss did not improve from 33.36892
196/196 - 59s - loss: 33.1723 - MinusLogProbMetric: 33.1723 - val_loss: 34.1860 - val_MinusLogProbMetric: 34.1860 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 92/1000
2023-09-30 09:49:32.573 
Epoch 92/1000 
	 loss: 32.9813, MinusLogProbMetric: 32.9813, val_loss: 33.5921, val_MinusLogProbMetric: 33.5921

Epoch 92: val_loss did not improve from 33.36892
196/196 - 61s - loss: 32.9813 - MinusLogProbMetric: 32.9813 - val_loss: 33.5921 - val_MinusLogProbMetric: 33.5921 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 93/1000
2023-09-30 09:50:32.952 
Epoch 93/1000 
	 loss: 33.0018, MinusLogProbMetric: 33.0018, val_loss: 32.9236, val_MinusLogProbMetric: 32.9236

Epoch 93: val_loss improved from 33.36892 to 32.92358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 61s - loss: 33.0018 - MinusLogProbMetric: 33.0018 - val_loss: 32.9236 - val_MinusLogProbMetric: 32.9236 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 94/1000
2023-09-30 09:51:35.918 
Epoch 94/1000 
	 loss: 32.8505, MinusLogProbMetric: 32.8505, val_loss: 34.0322, val_MinusLogProbMetric: 34.0322

Epoch 94: val_loss did not improve from 32.92358
196/196 - 62s - loss: 32.8505 - MinusLogProbMetric: 32.8505 - val_loss: 34.0322 - val_MinusLogProbMetric: 34.0322 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 95/1000
2023-09-30 09:52:34.664 
Epoch 95/1000 
	 loss: 32.8796, MinusLogProbMetric: 32.8796, val_loss: 33.1988, val_MinusLogProbMetric: 33.1988

Epoch 95: val_loss did not improve from 32.92358
196/196 - 59s - loss: 32.8796 - MinusLogProbMetric: 32.8796 - val_loss: 33.1988 - val_MinusLogProbMetric: 33.1988 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 96/1000
2023-09-30 09:53:34.311 
Epoch 96/1000 
	 loss: 32.8300, MinusLogProbMetric: 32.8300, val_loss: 33.4479, val_MinusLogProbMetric: 33.4479

Epoch 96: val_loss did not improve from 32.92358
196/196 - 60s - loss: 32.8300 - MinusLogProbMetric: 32.8300 - val_loss: 33.4479 - val_MinusLogProbMetric: 33.4479 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 97/1000
2023-09-30 09:54:35.675 
Epoch 97/1000 
	 loss: 32.6983, MinusLogProbMetric: 32.6983, val_loss: 33.6716, val_MinusLogProbMetric: 33.6716

Epoch 97: val_loss did not improve from 32.92358
196/196 - 61s - loss: 32.6983 - MinusLogProbMetric: 32.6983 - val_loss: 33.6716 - val_MinusLogProbMetric: 33.6716 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 98/1000
2023-09-30 09:55:36.539 
Epoch 98/1000 
	 loss: 32.6738, MinusLogProbMetric: 32.6738, val_loss: 32.9739, val_MinusLogProbMetric: 32.9739

Epoch 98: val_loss did not improve from 32.92358
196/196 - 61s - loss: 32.6738 - MinusLogProbMetric: 32.6738 - val_loss: 32.9739 - val_MinusLogProbMetric: 32.9739 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 99/1000
2023-09-30 09:56:38.009 
Epoch 99/1000 
	 loss: 32.5769, MinusLogProbMetric: 32.5769, val_loss: 33.1304, val_MinusLogProbMetric: 33.1304

Epoch 99: val_loss did not improve from 32.92358
196/196 - 61s - loss: 32.5769 - MinusLogProbMetric: 32.5769 - val_loss: 33.1304 - val_MinusLogProbMetric: 33.1304 - lr: 1.1111e-04 - 61s/epoch - 314ms/step
Epoch 100/1000
2023-09-30 09:57:39.321 
Epoch 100/1000 
	 loss: 32.8600, MinusLogProbMetric: 32.8600, val_loss: 32.7678, val_MinusLogProbMetric: 32.7678

Epoch 100: val_loss improved from 32.92358 to 32.76784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 32.8600 - MinusLogProbMetric: 32.8600 - val_loss: 32.7678 - val_MinusLogProbMetric: 32.7678 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 101/1000
2023-09-30 09:58:40.102 
Epoch 101/1000 
	 loss: 32.4420, MinusLogProbMetric: 32.4420, val_loss: 32.5854, val_MinusLogProbMetric: 32.5854

Epoch 101: val_loss improved from 32.76784 to 32.58538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 63s - loss: 32.4420 - MinusLogProbMetric: 32.4420 - val_loss: 32.5854 - val_MinusLogProbMetric: 32.5854 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 102/1000
2023-09-30 09:59:43.742 
Epoch 102/1000 
	 loss: 32.3599, MinusLogProbMetric: 32.3599, val_loss: 32.4394, val_MinusLogProbMetric: 32.4394

Epoch 102: val_loss improved from 32.58538 to 32.43942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 63s - loss: 32.3599 - MinusLogProbMetric: 32.3599 - val_loss: 32.4394 - val_MinusLogProbMetric: 32.4394 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 103/1000
2023-09-30 10:00:46.313 
Epoch 103/1000 
	 loss: 32.4341, MinusLogProbMetric: 32.4341, val_loss: 33.0239, val_MinusLogProbMetric: 33.0239

Epoch 103: val_loss did not improve from 32.43942
196/196 - 61s - loss: 32.4341 - MinusLogProbMetric: 32.4341 - val_loss: 33.0239 - val_MinusLogProbMetric: 33.0239 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 104/1000
2023-09-30 10:01:47.184 
Epoch 104/1000 
	 loss: 32.4888, MinusLogProbMetric: 32.4888, val_loss: 32.6448, val_MinusLogProbMetric: 32.6448

Epoch 104: val_loss did not improve from 32.43942
196/196 - 61s - loss: 32.4888 - MinusLogProbMetric: 32.4888 - val_loss: 32.6448 - val_MinusLogProbMetric: 32.6448 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 105/1000
2023-09-30 10:02:48.471 
Epoch 105/1000 
	 loss: 32.3322, MinusLogProbMetric: 32.3322, val_loss: 32.9056, val_MinusLogProbMetric: 32.9056

Epoch 105: val_loss did not improve from 32.43942
196/196 - 61s - loss: 32.3322 - MinusLogProbMetric: 32.3322 - val_loss: 32.9056 - val_MinusLogProbMetric: 32.9056 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 106/1000
2023-09-30 10:03:47.736 
Epoch 106/1000 
	 loss: 32.1808, MinusLogProbMetric: 32.1808, val_loss: 32.4385, val_MinusLogProbMetric: 32.4385

Epoch 106: val_loss improved from 32.43942 to 32.43851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 32.1808 - MinusLogProbMetric: 32.1808 - val_loss: 32.4385 - val_MinusLogProbMetric: 32.4385 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 107/1000
2023-09-30 10:04:49.349 
Epoch 107/1000 
	 loss: 32.1179, MinusLogProbMetric: 32.1179, val_loss: 33.6720, val_MinusLogProbMetric: 33.6720

Epoch 107: val_loss did not improve from 32.43851
196/196 - 60s - loss: 32.1179 - MinusLogProbMetric: 32.1179 - val_loss: 33.6720 - val_MinusLogProbMetric: 33.6720 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 108/1000
2023-09-30 10:05:48.593 
Epoch 108/1000 
	 loss: 32.1089, MinusLogProbMetric: 32.1089, val_loss: 32.7369, val_MinusLogProbMetric: 32.7369

Epoch 108: val_loss did not improve from 32.43851
196/196 - 59s - loss: 32.1089 - MinusLogProbMetric: 32.1089 - val_loss: 32.7369 - val_MinusLogProbMetric: 32.7369 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 109/1000
2023-09-30 10:06:42.292 
Epoch 109/1000 
	 loss: 32.0016, MinusLogProbMetric: 32.0016, val_loss: 32.5994, val_MinusLogProbMetric: 32.5994

Epoch 109: val_loss did not improve from 32.43851
196/196 - 54s - loss: 32.0016 - MinusLogProbMetric: 32.0016 - val_loss: 32.5994 - val_MinusLogProbMetric: 32.5994 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 110/1000
2023-09-30 10:07:40.815 
Epoch 110/1000 
	 loss: 32.1122, MinusLogProbMetric: 32.1122, val_loss: 32.3502, val_MinusLogProbMetric: 32.3502

Epoch 110: val_loss improved from 32.43851 to 32.35022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 61s - loss: 32.1122 - MinusLogProbMetric: 32.1122 - val_loss: 32.3502 - val_MinusLogProbMetric: 32.3502 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 111/1000
2023-09-30 10:08:41.813 
Epoch 111/1000 
	 loss: 32.0557, MinusLogProbMetric: 32.0557, val_loss: 31.9307, val_MinusLogProbMetric: 31.9307

Epoch 111: val_loss improved from 32.35022 to 31.93074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 32.0557 - MinusLogProbMetric: 32.0557 - val_loss: 31.9307 - val_MinusLogProbMetric: 31.9307 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 112/1000
2023-09-30 10:09:43.409 
Epoch 112/1000 
	 loss: 31.9049, MinusLogProbMetric: 31.9049, val_loss: 32.2101, val_MinusLogProbMetric: 32.2101

Epoch 112: val_loss did not improve from 31.93074
196/196 - 59s - loss: 31.9049 - MinusLogProbMetric: 31.9049 - val_loss: 32.2101 - val_MinusLogProbMetric: 32.2101 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 113/1000
2023-09-30 10:10:43.259 
Epoch 113/1000 
	 loss: 31.9024, MinusLogProbMetric: 31.9024, val_loss: 32.9080, val_MinusLogProbMetric: 32.9080

Epoch 113: val_loss did not improve from 31.93074
196/196 - 60s - loss: 31.9024 - MinusLogProbMetric: 31.9024 - val_loss: 32.9080 - val_MinusLogProbMetric: 32.9080 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 114/1000
2023-09-30 10:11:45.796 
Epoch 114/1000 
	 loss: 31.8575, MinusLogProbMetric: 31.8575, val_loss: 32.5090, val_MinusLogProbMetric: 32.5090

Epoch 114: val_loss did not improve from 31.93074
196/196 - 63s - loss: 31.8575 - MinusLogProbMetric: 31.8575 - val_loss: 32.5090 - val_MinusLogProbMetric: 32.5090 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 115/1000
2023-09-30 10:12:45.837 
Epoch 115/1000 
	 loss: 31.9403, MinusLogProbMetric: 31.9403, val_loss: 32.0336, val_MinusLogProbMetric: 32.0336

Epoch 115: val_loss did not improve from 31.93074
196/196 - 60s - loss: 31.9403 - MinusLogProbMetric: 31.9403 - val_loss: 32.0336 - val_MinusLogProbMetric: 32.0336 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 116/1000
2023-09-30 10:13:44.689 
Epoch 116/1000 
	 loss: 31.7799, MinusLogProbMetric: 31.7799, val_loss: 32.4230, val_MinusLogProbMetric: 32.4230

Epoch 116: val_loss did not improve from 31.93074
196/196 - 59s - loss: 31.7799 - MinusLogProbMetric: 31.7799 - val_loss: 32.4230 - val_MinusLogProbMetric: 32.4230 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 117/1000
2023-09-30 10:14:47.631 
Epoch 117/1000 
	 loss: 31.7385, MinusLogProbMetric: 31.7385, val_loss: 31.7020, val_MinusLogProbMetric: 31.7020

Epoch 117: val_loss improved from 31.93074 to 31.70204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 64s - loss: 31.7385 - MinusLogProbMetric: 31.7385 - val_loss: 31.7020 - val_MinusLogProbMetric: 31.7020 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 118/1000
2023-09-30 10:15:49.277 
Epoch 118/1000 
	 loss: 31.7016, MinusLogProbMetric: 31.7016, val_loss: 32.0742, val_MinusLogProbMetric: 32.0742

Epoch 118: val_loss did not improve from 31.70204
196/196 - 60s - loss: 31.7016 - MinusLogProbMetric: 31.7016 - val_loss: 32.0742 - val_MinusLogProbMetric: 32.0742 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 119/1000
2023-09-30 10:16:48.010 
Epoch 119/1000 
	 loss: 31.7108, MinusLogProbMetric: 31.7108, val_loss: 31.8416, val_MinusLogProbMetric: 31.8416

Epoch 119: val_loss did not improve from 31.70204
196/196 - 59s - loss: 31.7108 - MinusLogProbMetric: 31.7108 - val_loss: 31.8416 - val_MinusLogProbMetric: 31.8416 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 120/1000
2023-09-30 10:17:49.891 
Epoch 120/1000 
	 loss: 31.7152, MinusLogProbMetric: 31.7152, val_loss: 32.6144, val_MinusLogProbMetric: 32.6144

Epoch 120: val_loss did not improve from 31.70204
196/196 - 62s - loss: 31.7152 - MinusLogProbMetric: 31.7152 - val_loss: 32.6144 - val_MinusLogProbMetric: 32.6144 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 121/1000
2023-09-30 10:18:48.048 
Epoch 121/1000 
	 loss: 31.6898, MinusLogProbMetric: 31.6898, val_loss: 32.3333, val_MinusLogProbMetric: 32.3333

Epoch 121: val_loss did not improve from 31.70204
196/196 - 58s - loss: 31.6898 - MinusLogProbMetric: 31.6898 - val_loss: 32.3333 - val_MinusLogProbMetric: 32.3333 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 122/1000
2023-09-30 10:19:46.691 
Epoch 122/1000 
	 loss: 31.6903, MinusLogProbMetric: 31.6903, val_loss: 31.8073, val_MinusLogProbMetric: 31.8073

Epoch 122: val_loss did not improve from 31.70204
196/196 - 59s - loss: 31.6903 - MinusLogProbMetric: 31.6903 - val_loss: 31.8073 - val_MinusLogProbMetric: 31.8073 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 123/1000
2023-09-30 10:20:47.157 
Epoch 123/1000 
	 loss: 31.6117, MinusLogProbMetric: 31.6117, val_loss: 31.8645, val_MinusLogProbMetric: 31.8645

Epoch 123: val_loss did not improve from 31.70204
196/196 - 60s - loss: 31.6117 - MinusLogProbMetric: 31.6117 - val_loss: 31.8645 - val_MinusLogProbMetric: 31.8645 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 124/1000
2023-09-30 10:21:45.124 
Epoch 124/1000 
	 loss: 31.6073, MinusLogProbMetric: 31.6073, val_loss: 32.5396, val_MinusLogProbMetric: 32.5396

Epoch 124: val_loss did not improve from 31.70204
196/196 - 58s - loss: 31.6073 - MinusLogProbMetric: 31.6073 - val_loss: 32.5396 - val_MinusLogProbMetric: 32.5396 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 125/1000
2023-09-30 10:22:43.537 
Epoch 125/1000 
	 loss: 31.6242, MinusLogProbMetric: 31.6242, val_loss: 32.4209, val_MinusLogProbMetric: 32.4209

Epoch 125: val_loss did not improve from 31.70204
196/196 - 58s - loss: 31.6242 - MinusLogProbMetric: 31.6242 - val_loss: 32.4209 - val_MinusLogProbMetric: 32.4209 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 126/1000
2023-09-30 10:23:42.021 
Epoch 126/1000 
	 loss: 31.5370, MinusLogProbMetric: 31.5370, val_loss: 32.2612, val_MinusLogProbMetric: 32.2612

Epoch 126: val_loss did not improve from 31.70204
196/196 - 58s - loss: 31.5370 - MinusLogProbMetric: 31.5370 - val_loss: 32.2612 - val_MinusLogProbMetric: 32.2612 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 127/1000
2023-09-30 10:24:43.947 
Epoch 127/1000 
	 loss: 31.4621, MinusLogProbMetric: 31.4621, val_loss: 32.9780, val_MinusLogProbMetric: 32.9780

Epoch 127: val_loss did not improve from 31.70204
196/196 - 62s - loss: 31.4621 - MinusLogProbMetric: 31.4621 - val_loss: 32.9780 - val_MinusLogProbMetric: 32.9780 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 128/1000
2023-09-30 10:25:43.282 
Epoch 128/1000 
	 loss: 31.4747, MinusLogProbMetric: 31.4747, val_loss: 32.5765, val_MinusLogProbMetric: 32.5765

Epoch 128: val_loss did not improve from 31.70204
196/196 - 59s - loss: 31.4747 - MinusLogProbMetric: 31.4747 - val_loss: 32.5765 - val_MinusLogProbMetric: 32.5765 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 129/1000
2023-09-30 10:26:40.654 
Epoch 129/1000 
	 loss: 31.5406, MinusLogProbMetric: 31.5406, val_loss: 31.9167, val_MinusLogProbMetric: 31.9167

Epoch 129: val_loss did not improve from 31.70204
196/196 - 57s - loss: 31.5406 - MinusLogProbMetric: 31.5406 - val_loss: 31.9167 - val_MinusLogProbMetric: 31.9167 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 130/1000
2023-09-30 10:27:40.477 
Epoch 130/1000 
	 loss: 31.4163, MinusLogProbMetric: 31.4163, val_loss: 31.6712, val_MinusLogProbMetric: 31.6712

Epoch 130: val_loss improved from 31.70204 to 31.67117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 31.4163 - MinusLogProbMetric: 31.4163 - val_loss: 31.6712 - val_MinusLogProbMetric: 31.6712 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 131/1000
2023-09-30 10:28:41.724 
Epoch 131/1000 
	 loss: 31.3839, MinusLogProbMetric: 31.3839, val_loss: 31.5279, val_MinusLogProbMetric: 31.5279

Epoch 131: val_loss improved from 31.67117 to 31.52785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 31.3839 - MinusLogProbMetric: 31.3839 - val_loss: 31.5279 - val_MinusLogProbMetric: 31.5279 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 132/1000
2023-09-30 10:29:41.073 
Epoch 132/1000 
	 loss: 31.3840, MinusLogProbMetric: 31.3840, val_loss: 31.3928, val_MinusLogProbMetric: 31.3928

Epoch 132: val_loss improved from 31.52785 to 31.39279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 61s - loss: 31.3840 - MinusLogProbMetric: 31.3840 - val_loss: 31.3928 - val_MinusLogProbMetric: 31.3928 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 133/1000
2023-09-30 10:30:43.099 
Epoch 133/1000 
	 loss: 31.4152, MinusLogProbMetric: 31.4152, val_loss: 31.7454, val_MinusLogProbMetric: 31.7454

Epoch 133: val_loss did not improve from 31.39279
196/196 - 60s - loss: 31.4152 - MinusLogProbMetric: 31.4152 - val_loss: 31.7454 - val_MinusLogProbMetric: 31.7454 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 134/1000
2023-09-30 10:31:42.441 
Epoch 134/1000 
	 loss: 31.3839, MinusLogProbMetric: 31.3839, val_loss: 31.4549, val_MinusLogProbMetric: 31.4549

Epoch 134: val_loss did not improve from 31.39279
196/196 - 59s - loss: 31.3839 - MinusLogProbMetric: 31.3839 - val_loss: 31.4549 - val_MinusLogProbMetric: 31.4549 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 135/1000
2023-09-30 10:32:42.431 
Epoch 135/1000 
	 loss: 31.3277, MinusLogProbMetric: 31.3277, val_loss: 31.5435, val_MinusLogProbMetric: 31.5435

Epoch 135: val_loss did not improve from 31.39279
196/196 - 60s - loss: 31.3277 - MinusLogProbMetric: 31.3277 - val_loss: 31.5435 - val_MinusLogProbMetric: 31.5435 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 136/1000
2023-09-30 10:33:41.968 
Epoch 136/1000 
	 loss: 31.1927, MinusLogProbMetric: 31.1927, val_loss: 31.4389, val_MinusLogProbMetric: 31.4389

Epoch 136: val_loss did not improve from 31.39279
196/196 - 60s - loss: 31.1927 - MinusLogProbMetric: 31.1927 - val_loss: 31.4389 - val_MinusLogProbMetric: 31.4389 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 137/1000
2023-09-30 10:34:39.958 
Epoch 137/1000 
	 loss: 31.2647, MinusLogProbMetric: 31.2647, val_loss: 32.1170, val_MinusLogProbMetric: 32.1170

Epoch 137: val_loss did not improve from 31.39279
196/196 - 58s - loss: 31.2647 - MinusLogProbMetric: 31.2647 - val_loss: 32.1170 - val_MinusLogProbMetric: 32.1170 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 138/1000
2023-09-30 10:35:40.871 
Epoch 138/1000 
	 loss: 31.2605, MinusLogProbMetric: 31.2605, val_loss: 31.5748, val_MinusLogProbMetric: 31.5748

Epoch 138: val_loss did not improve from 31.39279
196/196 - 61s - loss: 31.2605 - MinusLogProbMetric: 31.2605 - val_loss: 31.5748 - val_MinusLogProbMetric: 31.5748 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 139/1000
2023-09-30 10:36:39.334 
Epoch 139/1000 
	 loss: 31.3008, MinusLogProbMetric: 31.3008, val_loss: 33.0741, val_MinusLogProbMetric: 33.0741

Epoch 139: val_loss did not improve from 31.39279
196/196 - 58s - loss: 31.3008 - MinusLogProbMetric: 31.3008 - val_loss: 33.0741 - val_MinusLogProbMetric: 33.0741 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 140/1000
2023-09-30 10:37:43.138 
Epoch 140/1000 
	 loss: 31.2800, MinusLogProbMetric: 31.2800, val_loss: 32.1900, val_MinusLogProbMetric: 32.1900

Epoch 140: val_loss did not improve from 31.39279
196/196 - 64s - loss: 31.2800 - MinusLogProbMetric: 31.2800 - val_loss: 32.1900 - val_MinusLogProbMetric: 32.1900 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 141/1000
2023-09-30 10:38:41.248 
Epoch 141/1000 
	 loss: 31.2035, MinusLogProbMetric: 31.2035, val_loss: 31.1441, val_MinusLogProbMetric: 31.1441

Epoch 141: val_loss improved from 31.39279 to 31.14414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 59s - loss: 31.2035 - MinusLogProbMetric: 31.2035 - val_loss: 31.1441 - val_MinusLogProbMetric: 31.1441 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 142/1000
2023-09-30 10:39:40.727 
Epoch 142/1000 
	 loss: 31.3488, MinusLogProbMetric: 31.3488, val_loss: 31.3783, val_MinusLogProbMetric: 31.3783

Epoch 142: val_loss did not improve from 31.14414
196/196 - 58s - loss: 31.3488 - MinusLogProbMetric: 31.3488 - val_loss: 31.3783 - val_MinusLogProbMetric: 31.3783 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 143/1000
2023-09-30 10:40:41.388 
Epoch 143/1000 
	 loss: 31.1710, MinusLogProbMetric: 31.1710, val_loss: 31.4193, val_MinusLogProbMetric: 31.4193

Epoch 143: val_loss did not improve from 31.14414
196/196 - 61s - loss: 31.1710 - MinusLogProbMetric: 31.1710 - val_loss: 31.4193 - val_MinusLogProbMetric: 31.4193 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 144/1000
2023-09-30 10:41:40.055 
Epoch 144/1000 
	 loss: 31.1626, MinusLogProbMetric: 31.1626, val_loss: 32.2282, val_MinusLogProbMetric: 32.2282

Epoch 144: val_loss did not improve from 31.14414
196/196 - 59s - loss: 31.1626 - MinusLogProbMetric: 31.1626 - val_loss: 32.2282 - val_MinusLogProbMetric: 32.2282 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 145/1000
2023-09-30 10:42:40.193 
Epoch 145/1000 
	 loss: 31.0808, MinusLogProbMetric: 31.0808, val_loss: 31.7347, val_MinusLogProbMetric: 31.7347

Epoch 145: val_loss did not improve from 31.14414
196/196 - 60s - loss: 31.0808 - MinusLogProbMetric: 31.0808 - val_loss: 31.7347 - val_MinusLogProbMetric: 31.7347 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 146/1000
2023-09-30 10:43:40.363 
Epoch 146/1000 
	 loss: 31.5314, MinusLogProbMetric: 31.5314, val_loss: 31.6148, val_MinusLogProbMetric: 31.6148

Epoch 146: val_loss did not improve from 31.14414
196/196 - 60s - loss: 31.5314 - MinusLogProbMetric: 31.5314 - val_loss: 31.6148 - val_MinusLogProbMetric: 31.6148 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 147/1000
2023-09-30 10:44:37.189 
Epoch 147/1000 
	 loss: 31.2279, MinusLogProbMetric: 31.2279, val_loss: 31.9495, val_MinusLogProbMetric: 31.9495

Epoch 147: val_loss did not improve from 31.14414
196/196 - 57s - loss: 31.2279 - MinusLogProbMetric: 31.2279 - val_loss: 31.9495 - val_MinusLogProbMetric: 31.9495 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 148/1000
2023-09-30 10:45:35.737 
Epoch 148/1000 
	 loss: 31.0755, MinusLogProbMetric: 31.0755, val_loss: 31.4535, val_MinusLogProbMetric: 31.4535

Epoch 148: val_loss did not improve from 31.14414
196/196 - 59s - loss: 31.0755 - MinusLogProbMetric: 31.0755 - val_loss: 31.4535 - val_MinusLogProbMetric: 31.4535 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 149/1000
2023-09-30 10:46:35.574 
Epoch 149/1000 
	 loss: 31.0178, MinusLogProbMetric: 31.0178, val_loss: 31.3787, val_MinusLogProbMetric: 31.3787

Epoch 149: val_loss did not improve from 31.14414
196/196 - 60s - loss: 31.0178 - MinusLogProbMetric: 31.0178 - val_loss: 31.3787 - val_MinusLogProbMetric: 31.3787 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 150/1000
2023-09-30 10:47:35.554 
Epoch 150/1000 
	 loss: 31.0388, MinusLogProbMetric: 31.0388, val_loss: 31.9525, val_MinusLogProbMetric: 31.9525

Epoch 150: val_loss did not improve from 31.14414
196/196 - 60s - loss: 31.0388 - MinusLogProbMetric: 31.0388 - val_loss: 31.9525 - val_MinusLogProbMetric: 31.9525 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 151/1000
2023-09-30 10:48:34.477 
Epoch 151/1000 
	 loss: 30.9817, MinusLogProbMetric: 30.9817, val_loss: 31.8025, val_MinusLogProbMetric: 31.8025

Epoch 151: val_loss did not improve from 31.14414
196/196 - 59s - loss: 30.9817 - MinusLogProbMetric: 30.9817 - val_loss: 31.8025 - val_MinusLogProbMetric: 31.8025 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 152/1000
2023-09-30 10:49:35.496 
Epoch 152/1000 
	 loss: 31.1377, MinusLogProbMetric: 31.1377, val_loss: 31.3440, val_MinusLogProbMetric: 31.3440

Epoch 152: val_loss did not improve from 31.14414
196/196 - 61s - loss: 31.1377 - MinusLogProbMetric: 31.1377 - val_loss: 31.3440 - val_MinusLogProbMetric: 31.3440 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 153/1000
2023-09-30 10:50:35.089 
Epoch 153/1000 
	 loss: 30.8996, MinusLogProbMetric: 30.8996, val_loss: 31.5491, val_MinusLogProbMetric: 31.5491

Epoch 153: val_loss did not improve from 31.14414
196/196 - 60s - loss: 30.8996 - MinusLogProbMetric: 30.8996 - val_loss: 31.5491 - val_MinusLogProbMetric: 31.5491 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 154/1000
2023-09-30 10:51:33.736 
Epoch 154/1000 
	 loss: 30.9438, MinusLogProbMetric: 30.9438, val_loss: 31.0472, val_MinusLogProbMetric: 31.0472

Epoch 154: val_loss improved from 31.14414 to 31.04719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 61s - loss: 30.9438 - MinusLogProbMetric: 30.9438 - val_loss: 31.0472 - val_MinusLogProbMetric: 31.0472 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 155/1000
2023-09-30 10:52:35.777 
Epoch 155/1000 
	 loss: 30.9816, MinusLogProbMetric: 30.9816, val_loss: 31.7028, val_MinusLogProbMetric: 31.7028

Epoch 155: val_loss did not improve from 31.04719
196/196 - 59s - loss: 30.9816 - MinusLogProbMetric: 30.9816 - val_loss: 31.7028 - val_MinusLogProbMetric: 31.7028 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 156/1000
2023-09-30 10:53:34.802 
Epoch 156/1000 
	 loss: 30.9123, MinusLogProbMetric: 30.9123, val_loss: 31.6300, val_MinusLogProbMetric: 31.6300

Epoch 156: val_loss did not improve from 31.04719
196/196 - 59s - loss: 30.9123 - MinusLogProbMetric: 30.9123 - val_loss: 31.6300 - val_MinusLogProbMetric: 31.6300 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 157/1000
2023-09-30 10:54:33.963 
Epoch 157/1000 
	 loss: 30.9544, MinusLogProbMetric: 30.9544, val_loss: 31.2197, val_MinusLogProbMetric: 31.2197

Epoch 157: val_loss did not improve from 31.04719
196/196 - 59s - loss: 30.9544 - MinusLogProbMetric: 30.9544 - val_loss: 31.2197 - val_MinusLogProbMetric: 31.2197 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 158/1000
2023-09-30 10:55:33.753 
Epoch 158/1000 
	 loss: 30.8708, MinusLogProbMetric: 30.8708, val_loss: 31.0422, val_MinusLogProbMetric: 31.0422

Epoch 158: val_loss improved from 31.04719 to 31.04218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 63s - loss: 30.8708 - MinusLogProbMetric: 30.8708 - val_loss: 31.0422 - val_MinusLogProbMetric: 31.0422 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 159/1000
2023-09-30 10:56:34.983 
Epoch 159/1000 
	 loss: 30.9277, MinusLogProbMetric: 30.9277, val_loss: 31.2532, val_MinusLogProbMetric: 31.2532

Epoch 159: val_loss did not improve from 31.04218
196/196 - 58s - loss: 30.9277 - MinusLogProbMetric: 30.9277 - val_loss: 31.2532 - val_MinusLogProbMetric: 31.2532 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 160/1000
2023-09-30 10:57:33.435 
Epoch 160/1000 
	 loss: 30.7790, MinusLogProbMetric: 30.7790, val_loss: 31.2679, val_MinusLogProbMetric: 31.2679

Epoch 160: val_loss did not improve from 31.04218
196/196 - 58s - loss: 30.7790 - MinusLogProbMetric: 30.7790 - val_loss: 31.2679 - val_MinusLogProbMetric: 31.2679 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 161/1000
2023-09-30 10:58:32.779 
Epoch 161/1000 
	 loss: 30.8251, MinusLogProbMetric: 30.8251, val_loss: 31.1925, val_MinusLogProbMetric: 31.1925

Epoch 161: val_loss did not improve from 31.04218
196/196 - 59s - loss: 30.8251 - MinusLogProbMetric: 30.8251 - val_loss: 31.1925 - val_MinusLogProbMetric: 31.1925 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 162/1000
2023-09-30 10:59:31.169 
Epoch 162/1000 
	 loss: 30.7758, MinusLogProbMetric: 30.7758, val_loss: 31.3504, val_MinusLogProbMetric: 31.3504

Epoch 162: val_loss did not improve from 31.04218
196/196 - 58s - loss: 30.7758 - MinusLogProbMetric: 30.7758 - val_loss: 31.3504 - val_MinusLogProbMetric: 31.3504 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 163/1000
2023-09-30 11:00:29.965 
Epoch 163/1000 
	 loss: 30.7138, MinusLogProbMetric: 30.7138, val_loss: 30.8290, val_MinusLogProbMetric: 30.8290

Epoch 163: val_loss improved from 31.04218 to 30.82901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 30.7138 - MinusLogProbMetric: 30.7138 - val_loss: 30.8290 - val_MinusLogProbMetric: 30.8290 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 164/1000
2023-09-30 11:01:34.928 
Epoch 164/1000 
	 loss: 30.7850, MinusLogProbMetric: 30.7850, val_loss: 30.9953, val_MinusLogProbMetric: 30.9953

Epoch 164: val_loss did not improve from 30.82901
196/196 - 61s - loss: 30.7850 - MinusLogProbMetric: 30.7850 - val_loss: 30.9953 - val_MinusLogProbMetric: 30.9953 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 165/1000
2023-09-30 11:02:33.135 
Epoch 165/1000 
	 loss: 30.7701, MinusLogProbMetric: 30.7701, val_loss: 31.0657, val_MinusLogProbMetric: 31.0657

Epoch 165: val_loss did not improve from 30.82901
196/196 - 58s - loss: 30.7701 - MinusLogProbMetric: 30.7701 - val_loss: 31.0657 - val_MinusLogProbMetric: 31.0657 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 166/1000
2023-09-30 11:03:30.901 
Epoch 166/1000 
	 loss: 30.8590, MinusLogProbMetric: 30.8590, val_loss: 31.0755, val_MinusLogProbMetric: 31.0755

Epoch 166: val_loss did not improve from 30.82901
196/196 - 58s - loss: 30.8590 - MinusLogProbMetric: 30.8590 - val_loss: 31.0755 - val_MinusLogProbMetric: 31.0755 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 167/1000
2023-09-30 11:04:29.656 
Epoch 167/1000 
	 loss: 30.6613, MinusLogProbMetric: 30.6613, val_loss: 31.4265, val_MinusLogProbMetric: 31.4265

Epoch 167: val_loss did not improve from 30.82901
196/196 - 59s - loss: 30.6613 - MinusLogProbMetric: 30.6613 - val_loss: 31.4265 - val_MinusLogProbMetric: 31.4265 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 168/1000
2023-09-30 11:05:28.690 
Epoch 168/1000 
	 loss: 30.7972, MinusLogProbMetric: 30.7972, val_loss: 31.1035, val_MinusLogProbMetric: 31.1035

Epoch 168: val_loss did not improve from 30.82901
196/196 - 59s - loss: 30.7972 - MinusLogProbMetric: 30.7972 - val_loss: 31.1035 - val_MinusLogProbMetric: 31.1035 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 169/1000
2023-09-30 11:06:28.809 
Epoch 169/1000 
	 loss: 30.6166, MinusLogProbMetric: 30.6166, val_loss: 30.6895, val_MinusLogProbMetric: 30.6895

Epoch 169: val_loss improved from 30.82901 to 30.68954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 30.6166 - MinusLogProbMetric: 30.6166 - val_loss: 30.6895 - val_MinusLogProbMetric: 30.6895 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 170/1000
2023-09-30 11:07:30.048 
Epoch 170/1000 
	 loss: 30.6948, MinusLogProbMetric: 30.6948, val_loss: 31.4199, val_MinusLogProbMetric: 31.4199

Epoch 170: val_loss did not improve from 30.68954
196/196 - 59s - loss: 30.6948 - MinusLogProbMetric: 30.6948 - val_loss: 31.4199 - val_MinusLogProbMetric: 31.4199 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 171/1000
2023-09-30 11:08:28.692 
Epoch 171/1000 
	 loss: 30.6069, MinusLogProbMetric: 30.6069, val_loss: 30.6819, val_MinusLogProbMetric: 30.6819

Epoch 171: val_loss improved from 30.68954 to 30.68189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 30.6069 - MinusLogProbMetric: 30.6069 - val_loss: 30.6819 - val_MinusLogProbMetric: 30.6819 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 172/1000
2023-09-30 11:09:30.601 
Epoch 172/1000 
	 loss: 30.6177, MinusLogProbMetric: 30.6177, val_loss: 30.7963, val_MinusLogProbMetric: 30.7963

Epoch 172: val_loss did not improve from 30.68189
196/196 - 61s - loss: 30.6177 - MinusLogProbMetric: 30.6177 - val_loss: 30.7963 - val_MinusLogProbMetric: 30.7963 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 173/1000
2023-09-30 11:10:25.642 
Epoch 173/1000 
	 loss: 30.6174, MinusLogProbMetric: 30.6174, val_loss: 30.8305, val_MinusLogProbMetric: 30.8305

Epoch 173: val_loss did not improve from 30.68189
196/196 - 55s - loss: 30.6174 - MinusLogProbMetric: 30.6174 - val_loss: 30.8305 - val_MinusLogProbMetric: 30.8305 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 174/1000
2023-09-30 11:11:23.506 
Epoch 174/1000 
	 loss: 30.6735, MinusLogProbMetric: 30.6735, val_loss: 30.7671, val_MinusLogProbMetric: 30.7671

Epoch 174: val_loss did not improve from 30.68189
196/196 - 58s - loss: 30.6735 - MinusLogProbMetric: 30.6735 - val_loss: 30.7671 - val_MinusLogProbMetric: 30.7671 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 175/1000
2023-09-30 11:12:22.234 
Epoch 175/1000 
	 loss: 30.5673, MinusLogProbMetric: 30.5673, val_loss: 31.8846, val_MinusLogProbMetric: 31.8846

Epoch 175: val_loss did not improve from 30.68189
196/196 - 59s - loss: 30.5673 - MinusLogProbMetric: 30.5673 - val_loss: 31.8846 - val_MinusLogProbMetric: 31.8846 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 176/1000
2023-09-30 11:13:22.273 
Epoch 176/1000 
	 loss: 30.5977, MinusLogProbMetric: 30.5977, val_loss: 31.3014, val_MinusLogProbMetric: 31.3014

Epoch 176: val_loss did not improve from 30.68189
196/196 - 60s - loss: 30.5977 - MinusLogProbMetric: 30.5977 - val_loss: 31.3014 - val_MinusLogProbMetric: 31.3014 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 177/1000
2023-09-30 11:14:21.829 
Epoch 177/1000 
	 loss: 30.6983, MinusLogProbMetric: 30.6983, val_loss: 32.2354, val_MinusLogProbMetric: 32.2354

Epoch 177: val_loss did not improve from 30.68189
196/196 - 60s - loss: 30.6983 - MinusLogProbMetric: 30.6983 - val_loss: 32.2354 - val_MinusLogProbMetric: 32.2354 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 178/1000
2023-09-30 11:15:20.312 
Epoch 178/1000 
	 loss: 30.6874, MinusLogProbMetric: 30.6874, val_loss: 30.6373, val_MinusLogProbMetric: 30.6373

Epoch 178: val_loss improved from 30.68189 to 30.63727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 30.6874 - MinusLogProbMetric: 30.6874 - val_loss: 30.6373 - val_MinusLogProbMetric: 30.6373 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 179/1000
2023-09-30 11:16:21.461 
Epoch 179/1000 
	 loss: 30.4981, MinusLogProbMetric: 30.4981, val_loss: 31.0897, val_MinusLogProbMetric: 31.0897

Epoch 179: val_loss did not improve from 30.63727
196/196 - 60s - loss: 30.4981 - MinusLogProbMetric: 30.4981 - val_loss: 31.0897 - val_MinusLogProbMetric: 31.0897 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 180/1000
2023-09-30 11:17:20.773 
Epoch 180/1000 
	 loss: 30.5704, MinusLogProbMetric: 30.5704, val_loss: 30.8489, val_MinusLogProbMetric: 30.8489

Epoch 180: val_loss did not improve from 30.63727
196/196 - 59s - loss: 30.5704 - MinusLogProbMetric: 30.5704 - val_loss: 30.8489 - val_MinusLogProbMetric: 30.8489 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 181/1000
2023-09-30 11:18:19.492 
Epoch 181/1000 
	 loss: 30.5129, MinusLogProbMetric: 30.5129, val_loss: 30.7953, val_MinusLogProbMetric: 30.7953

Epoch 181: val_loss did not improve from 30.63727
196/196 - 59s - loss: 30.5129 - MinusLogProbMetric: 30.5129 - val_loss: 30.7953 - val_MinusLogProbMetric: 30.7953 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 182/1000
2023-09-30 11:19:17.550 
Epoch 182/1000 
	 loss: 30.4545, MinusLogProbMetric: 30.4545, val_loss: 31.0545, val_MinusLogProbMetric: 31.0545

Epoch 182: val_loss did not improve from 30.63727
196/196 - 58s - loss: 30.4545 - MinusLogProbMetric: 30.4545 - val_loss: 31.0545 - val_MinusLogProbMetric: 31.0545 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 183/1000
2023-09-30 11:20:17.833 
Epoch 183/1000 
	 loss: 30.4749, MinusLogProbMetric: 30.4749, val_loss: 31.1108, val_MinusLogProbMetric: 31.1108

Epoch 183: val_loss did not improve from 30.63727
196/196 - 60s - loss: 30.4749 - MinusLogProbMetric: 30.4749 - val_loss: 31.1108 - val_MinusLogProbMetric: 31.1108 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 184/1000
2023-09-30 11:21:17.220 
Epoch 184/1000 
	 loss: 30.5018, MinusLogProbMetric: 30.5018, val_loss: 30.6963, val_MinusLogProbMetric: 30.6963

Epoch 184: val_loss did not improve from 30.63727
196/196 - 59s - loss: 30.5018 - MinusLogProbMetric: 30.5018 - val_loss: 30.6963 - val_MinusLogProbMetric: 30.6963 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 185/1000
2023-09-30 11:22:16.354 
Epoch 185/1000 
	 loss: 30.4343, MinusLogProbMetric: 30.4343, val_loss: 30.9074, val_MinusLogProbMetric: 30.9074

Epoch 185: val_loss did not improve from 30.63727
196/196 - 59s - loss: 30.4343 - MinusLogProbMetric: 30.4343 - val_loss: 30.9074 - val_MinusLogProbMetric: 30.9074 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 186/1000
2023-09-30 11:23:14.791 
Epoch 186/1000 
	 loss: 30.4214, MinusLogProbMetric: 30.4214, val_loss: 31.1147, val_MinusLogProbMetric: 31.1147

Epoch 186: val_loss did not improve from 30.63727
196/196 - 58s - loss: 30.4214 - MinusLogProbMetric: 30.4214 - val_loss: 31.1147 - val_MinusLogProbMetric: 31.1147 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 187/1000
2023-09-30 11:24:13.973 
Epoch 187/1000 
	 loss: 30.3583, MinusLogProbMetric: 30.3583, val_loss: 30.9967, val_MinusLogProbMetric: 30.9967

Epoch 187: val_loss did not improve from 30.63727
196/196 - 59s - loss: 30.3583 - MinusLogProbMetric: 30.3583 - val_loss: 30.9967 - val_MinusLogProbMetric: 30.9967 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 188/1000
2023-09-30 11:25:14.676 
Epoch 188/1000 
	 loss: 30.4392, MinusLogProbMetric: 30.4392, val_loss: 31.6585, val_MinusLogProbMetric: 31.6585

Epoch 188: val_loss did not improve from 30.63727
196/196 - 61s - loss: 30.4392 - MinusLogProbMetric: 30.4392 - val_loss: 31.6585 - val_MinusLogProbMetric: 31.6585 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 189/1000
2023-09-30 11:26:13.928 
Epoch 189/1000 
	 loss: 30.5199, MinusLogProbMetric: 30.5199, val_loss: 30.5674, val_MinusLogProbMetric: 30.5674

Epoch 189: val_loss improved from 30.63727 to 30.56736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 60s - loss: 30.5199 - MinusLogProbMetric: 30.5199 - val_loss: 30.5674 - val_MinusLogProbMetric: 30.5674 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 190/1000
2023-09-30 11:27:13.863 
Epoch 190/1000 
	 loss: 30.3597, MinusLogProbMetric: 30.3597, val_loss: 30.7444, val_MinusLogProbMetric: 30.7444

Epoch 190: val_loss did not improve from 30.56736
196/196 - 59s - loss: 30.3597 - MinusLogProbMetric: 30.3597 - val_loss: 30.7444 - val_MinusLogProbMetric: 30.7444 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 191/1000
2023-09-30 11:28:12.395 
Epoch 191/1000 
	 loss: 30.4176, MinusLogProbMetric: 30.4176, val_loss: 30.8052, val_MinusLogProbMetric: 30.8052

Epoch 191: val_loss did not improve from 30.56736
196/196 - 59s - loss: 30.4176 - MinusLogProbMetric: 30.4176 - val_loss: 30.8052 - val_MinusLogProbMetric: 30.8052 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 192/1000
2023-09-30 11:29:10.759 
Epoch 192/1000 
	 loss: 30.3926, MinusLogProbMetric: 30.3926, val_loss: 30.7045, val_MinusLogProbMetric: 30.7045

Epoch 192: val_loss did not improve from 30.56736
196/196 - 58s - loss: 30.3926 - MinusLogProbMetric: 30.3926 - val_loss: 30.7045 - val_MinusLogProbMetric: 30.7045 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 193/1000
2023-09-30 11:30:11.613 
Epoch 193/1000 
	 loss: 30.3629, MinusLogProbMetric: 30.3629, val_loss: 30.5855, val_MinusLogProbMetric: 30.5855

Epoch 193: val_loss did not improve from 30.56736
196/196 - 61s - loss: 30.3629 - MinusLogProbMetric: 30.3629 - val_loss: 30.5855 - val_MinusLogProbMetric: 30.5855 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 194/1000
2023-09-30 11:31:11.704 
Epoch 194/1000 
	 loss: 30.2704, MinusLogProbMetric: 30.2704, val_loss: 30.9786, val_MinusLogProbMetric: 30.9786

Epoch 194: val_loss did not improve from 30.56736
196/196 - 60s - loss: 30.2704 - MinusLogProbMetric: 30.2704 - val_loss: 30.9786 - val_MinusLogProbMetric: 30.9786 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 195/1000
2023-09-30 11:32:10.264 
Epoch 195/1000 
	 loss: 30.3646, MinusLogProbMetric: 30.3646, val_loss: 30.3509, val_MinusLogProbMetric: 30.3509

Epoch 195: val_loss improved from 30.56736 to 30.35089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 62s - loss: 30.3646 - MinusLogProbMetric: 30.3646 - val_loss: 30.3509 - val_MinusLogProbMetric: 30.3509 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 196/1000
2023-09-30 11:33:19.265 
Epoch 196/1000 
	 loss: 30.3511, MinusLogProbMetric: 30.3511, val_loss: 30.7669, val_MinusLogProbMetric: 30.7669

Epoch 196: val_loss did not improve from 30.35089
196/196 - 66s - loss: 30.3511 - MinusLogProbMetric: 30.3511 - val_loss: 30.7669 - val_MinusLogProbMetric: 30.7669 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 197/1000
2023-09-30 11:34:38.957 
Epoch 197/1000 
	 loss: 30.2964, MinusLogProbMetric: 30.2964, val_loss: 30.4525, val_MinusLogProbMetric: 30.4525

Epoch 197: val_loss did not improve from 30.35089
196/196 - 80s - loss: 30.2964 - MinusLogProbMetric: 30.2964 - val_loss: 30.4525 - val_MinusLogProbMetric: 30.4525 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 198/1000
2023-09-30 11:36:03.055 
Epoch 198/1000 
	 loss: 30.2802, MinusLogProbMetric: 30.2802, val_loss: 30.9143, val_MinusLogProbMetric: 30.9143

Epoch 198: val_loss did not improve from 30.35089
196/196 - 84s - loss: 30.2802 - MinusLogProbMetric: 30.2802 - val_loss: 30.9143 - val_MinusLogProbMetric: 30.9143 - lr: 1.1111e-04 - 84s/epoch - 429ms/step
Epoch 199/1000
2023-09-30 11:37:37.258 
Epoch 199/1000 
	 loss: 30.3338, MinusLogProbMetric: 30.3338, val_loss: 30.4254, val_MinusLogProbMetric: 30.4254

Epoch 199: val_loss did not improve from 30.35089
196/196 - 94s - loss: 30.3338 - MinusLogProbMetric: 30.3338 - val_loss: 30.4254 - val_MinusLogProbMetric: 30.4254 - lr: 1.1111e-04 - 94s/epoch - 481ms/step
Epoch 200/1000
2023-09-30 11:39:12.995 
Epoch 200/1000 
	 loss: 30.2965, MinusLogProbMetric: 30.2965, val_loss: 30.6130, val_MinusLogProbMetric: 30.6130

Epoch 200: val_loss did not improve from 30.35089
196/196 - 96s - loss: 30.2965 - MinusLogProbMetric: 30.2965 - val_loss: 30.6130 - val_MinusLogProbMetric: 30.6130 - lr: 1.1111e-04 - 96s/epoch - 488ms/step
Epoch 201/1000
2023-09-30 11:40:50.382 
Epoch 201/1000 
	 loss: 30.1913, MinusLogProbMetric: 30.1913, val_loss: 31.0641, val_MinusLogProbMetric: 31.0641

Epoch 201: val_loss did not improve from 30.35089
196/196 - 97s - loss: 30.1913 - MinusLogProbMetric: 30.1913 - val_loss: 31.0641 - val_MinusLogProbMetric: 31.0641 - lr: 1.1111e-04 - 97s/epoch - 497ms/step
Epoch 202/1000
2023-09-30 11:42:22.982 
Epoch 202/1000 
	 loss: 30.3156, MinusLogProbMetric: 30.3156, val_loss: 31.3001, val_MinusLogProbMetric: 31.3001

Epoch 202: val_loss did not improve from 30.35089
196/196 - 93s - loss: 30.3156 - MinusLogProbMetric: 30.3156 - val_loss: 31.3001 - val_MinusLogProbMetric: 31.3001 - lr: 1.1111e-04 - 93s/epoch - 472ms/step
Epoch 203/1000
2023-09-30 11:43:52.605 
Epoch 203/1000 
	 loss: 30.2826, MinusLogProbMetric: 30.2826, val_loss: 31.0578, val_MinusLogProbMetric: 31.0578

Epoch 203: val_loss did not improve from 30.35089
196/196 - 90s - loss: 30.2826 - MinusLogProbMetric: 30.2826 - val_loss: 31.0578 - val_MinusLogProbMetric: 31.0578 - lr: 1.1111e-04 - 90s/epoch - 457ms/step
Epoch 204/1000
2023-09-30 11:45:19.912 
Epoch 204/1000 
	 loss: 30.3149, MinusLogProbMetric: 30.3149, val_loss: 30.3572, val_MinusLogProbMetric: 30.3572

Epoch 204: val_loss did not improve from 30.35089
196/196 - 87s - loss: 30.3149 - MinusLogProbMetric: 30.3149 - val_loss: 30.3572 - val_MinusLogProbMetric: 30.3572 - lr: 1.1111e-04 - 87s/epoch - 445ms/step
Epoch 205/1000
2023-09-30 11:46:50.037 
Epoch 205/1000 
	 loss: 30.1696, MinusLogProbMetric: 30.1696, val_loss: 30.7561, val_MinusLogProbMetric: 30.7561

Epoch 205: val_loss did not improve from 30.35089
196/196 - 90s - loss: 30.1696 - MinusLogProbMetric: 30.1696 - val_loss: 30.7561 - val_MinusLogProbMetric: 30.7561 - lr: 1.1111e-04 - 90s/epoch - 460ms/step
Epoch 206/1000
2023-09-30 11:48:18.893 
Epoch 206/1000 
	 loss: 30.2240, MinusLogProbMetric: 30.2240, val_loss: 30.5070, val_MinusLogProbMetric: 30.5070

Epoch 206: val_loss did not improve from 30.35089
196/196 - 89s - loss: 30.2240 - MinusLogProbMetric: 30.2240 - val_loss: 30.5070 - val_MinusLogProbMetric: 30.5070 - lr: 1.1111e-04 - 89s/epoch - 453ms/step
Epoch 207/1000
2023-09-30 11:49:42.213 
Epoch 207/1000 
	 loss: 30.2676, MinusLogProbMetric: 30.2676, val_loss: 30.6422, val_MinusLogProbMetric: 30.6422

Epoch 207: val_loss did not improve from 30.35089
196/196 - 83s - loss: 30.2676 - MinusLogProbMetric: 30.2676 - val_loss: 30.6422 - val_MinusLogProbMetric: 30.6422 - lr: 1.1111e-04 - 83s/epoch - 425ms/step
Epoch 208/1000
2023-09-30 11:51:00.576 
Epoch 208/1000 
	 loss: 30.2469, MinusLogProbMetric: 30.2469, val_loss: 30.6775, val_MinusLogProbMetric: 30.6775

Epoch 208: val_loss did not improve from 30.35089
196/196 - 78s - loss: 30.2469 - MinusLogProbMetric: 30.2469 - val_loss: 30.6775 - val_MinusLogProbMetric: 30.6775 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 209/1000
2023-09-30 11:52:22.446 
Epoch 209/1000 
	 loss: 30.1047, MinusLogProbMetric: 30.1047, val_loss: 30.6251, val_MinusLogProbMetric: 30.6251

Epoch 209: val_loss did not improve from 30.35089
196/196 - 82s - loss: 30.1047 - MinusLogProbMetric: 30.1047 - val_loss: 30.6251 - val_MinusLogProbMetric: 30.6251 - lr: 1.1111e-04 - 82s/epoch - 418ms/step
Epoch 210/1000
2023-09-30 11:53:47.087 
Epoch 210/1000 
	 loss: 30.1906, MinusLogProbMetric: 30.1906, val_loss: 30.6333, val_MinusLogProbMetric: 30.6333

Epoch 210: val_loss did not improve from 30.35089
196/196 - 85s - loss: 30.1906 - MinusLogProbMetric: 30.1906 - val_loss: 30.6333 - val_MinusLogProbMetric: 30.6333 - lr: 1.1111e-04 - 85s/epoch - 432ms/step
Epoch 211/1000
2023-09-30 11:55:16.460 
Epoch 211/1000 
	 loss: 30.2117, MinusLogProbMetric: 30.2117, val_loss: 30.6196, val_MinusLogProbMetric: 30.6196

Epoch 211: val_loss did not improve from 30.35089
196/196 - 89s - loss: 30.2117 - MinusLogProbMetric: 30.2117 - val_loss: 30.6196 - val_MinusLogProbMetric: 30.6196 - lr: 1.1111e-04 - 89s/epoch - 456ms/step
Epoch 212/1000
2023-09-30 11:56:40.062 
Epoch 212/1000 
	 loss: 30.1917, MinusLogProbMetric: 30.1917, val_loss: 30.6752, val_MinusLogProbMetric: 30.6752

Epoch 212: val_loss did not improve from 30.35089
196/196 - 84s - loss: 30.1917 - MinusLogProbMetric: 30.1917 - val_loss: 30.6752 - val_MinusLogProbMetric: 30.6752 - lr: 1.1111e-04 - 84s/epoch - 426ms/step
Epoch 213/1000
2023-09-30 11:58:05.025 
Epoch 213/1000 
	 loss: 30.2962, MinusLogProbMetric: 30.2962, val_loss: 30.4008, val_MinusLogProbMetric: 30.4008

Epoch 213: val_loss did not improve from 30.35089
196/196 - 85s - loss: 30.2962 - MinusLogProbMetric: 30.2962 - val_loss: 30.4008 - val_MinusLogProbMetric: 30.4008 - lr: 1.1111e-04 - 85s/epoch - 434ms/step
Epoch 214/1000
2023-09-30 11:59:30.390 
Epoch 214/1000 
	 loss: 30.1954, MinusLogProbMetric: 30.1954, val_loss: 31.1854, val_MinusLogProbMetric: 31.1854

Epoch 214: val_loss did not improve from 30.35089
196/196 - 85s - loss: 30.1954 - MinusLogProbMetric: 30.1954 - val_loss: 31.1854 - val_MinusLogProbMetric: 31.1854 - lr: 1.1111e-04 - 85s/epoch - 435ms/step
Epoch 215/1000
2023-09-30 12:00:55.155 
Epoch 215/1000 
	 loss: 30.1808, MinusLogProbMetric: 30.1808, val_loss: 30.3751, val_MinusLogProbMetric: 30.3751

Epoch 215: val_loss did not improve from 30.35089
196/196 - 85s - loss: 30.1808 - MinusLogProbMetric: 30.1808 - val_loss: 30.3751 - val_MinusLogProbMetric: 30.3751 - lr: 1.1111e-04 - 85s/epoch - 433ms/step
Epoch 216/1000
2023-09-30 12:02:15.643 
Epoch 216/1000 
	 loss: 30.2187, MinusLogProbMetric: 30.2187, val_loss: 31.1084, val_MinusLogProbMetric: 31.1084

Epoch 216: val_loss did not improve from 30.35089
196/196 - 80s - loss: 30.2187 - MinusLogProbMetric: 30.2187 - val_loss: 31.1084 - val_MinusLogProbMetric: 31.1084 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 217/1000
2023-09-30 12:03:40.083 
Epoch 217/1000 
	 loss: 30.1653, MinusLogProbMetric: 30.1653, val_loss: 31.2048, val_MinusLogProbMetric: 31.2048

Epoch 217: val_loss did not improve from 30.35089
196/196 - 84s - loss: 30.1653 - MinusLogProbMetric: 30.1653 - val_loss: 31.2048 - val_MinusLogProbMetric: 31.2048 - lr: 1.1111e-04 - 84s/epoch - 431ms/step
Epoch 218/1000
2023-09-30 12:05:02.451 
Epoch 218/1000 
	 loss: 30.1107, MinusLogProbMetric: 30.1107, val_loss: 30.7519, val_MinusLogProbMetric: 30.7519

Epoch 218: val_loss did not improve from 30.35089
196/196 - 82s - loss: 30.1107 - MinusLogProbMetric: 30.1107 - val_loss: 30.7519 - val_MinusLogProbMetric: 30.7519 - lr: 1.1111e-04 - 82s/epoch - 420ms/step
Epoch 219/1000
2023-09-30 12:06:29.225 
Epoch 219/1000 
	 loss: 30.1538, MinusLogProbMetric: 30.1538, val_loss: 30.1152, val_MinusLogProbMetric: 30.1152

Epoch 219: val_loss improved from 30.35089 to 30.11519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 89s - loss: 30.1538 - MinusLogProbMetric: 30.1538 - val_loss: 30.1152 - val_MinusLogProbMetric: 30.1152 - lr: 1.1111e-04 - 89s/epoch - 456ms/step
Epoch 220/1000
2023-09-30 12:07:55.143 
Epoch 220/1000 
	 loss: 30.2328, MinusLogProbMetric: 30.2328, val_loss: 31.3037, val_MinusLogProbMetric: 31.3037

Epoch 220: val_loss did not improve from 30.11519
196/196 - 83s - loss: 30.2328 - MinusLogProbMetric: 30.2328 - val_loss: 31.3037 - val_MinusLogProbMetric: 31.3037 - lr: 1.1111e-04 - 83s/epoch - 425ms/step
Epoch 221/1000
2023-09-30 12:09:19.630 
Epoch 221/1000 
	 loss: 30.0679, MinusLogProbMetric: 30.0679, val_loss: 30.4749, val_MinusLogProbMetric: 30.4749

Epoch 221: val_loss did not improve from 30.11519
196/196 - 84s - loss: 30.0679 - MinusLogProbMetric: 30.0679 - val_loss: 30.4749 - val_MinusLogProbMetric: 30.4749 - lr: 1.1111e-04 - 84s/epoch - 431ms/step
Epoch 222/1000
2023-09-30 12:10:35.455 
Epoch 222/1000 
	 loss: 30.0965, MinusLogProbMetric: 30.0965, val_loss: 30.5253, val_MinusLogProbMetric: 30.5253

Epoch 222: val_loss did not improve from 30.11519
196/196 - 76s - loss: 30.0965 - MinusLogProbMetric: 30.0965 - val_loss: 30.5253 - val_MinusLogProbMetric: 30.5253 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 223/1000
2023-09-30 12:11:52.286 
Epoch 223/1000 
	 loss: 30.1424, MinusLogProbMetric: 30.1424, val_loss: 31.1371, val_MinusLogProbMetric: 31.1371

Epoch 223: val_loss did not improve from 30.11519
196/196 - 77s - loss: 30.1424 - MinusLogProbMetric: 30.1424 - val_loss: 31.1371 - val_MinusLogProbMetric: 31.1371 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 224/1000
2023-09-30 12:13:06.407 
Epoch 224/1000 
	 loss: 30.0357, MinusLogProbMetric: 30.0357, val_loss: 30.7931, val_MinusLogProbMetric: 30.7931

Epoch 224: val_loss did not improve from 30.11519
196/196 - 74s - loss: 30.0357 - MinusLogProbMetric: 30.0357 - val_loss: 30.7931 - val_MinusLogProbMetric: 30.7931 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 225/1000
2023-09-30 12:14:18.925 
Epoch 225/1000 
	 loss: 30.1455, MinusLogProbMetric: 30.1455, val_loss: 30.3678, val_MinusLogProbMetric: 30.3678

Epoch 225: val_loss did not improve from 30.11519
196/196 - 72s - loss: 30.1455 - MinusLogProbMetric: 30.1455 - val_loss: 30.3678 - val_MinusLogProbMetric: 30.3678 - lr: 1.1111e-04 - 72s/epoch - 370ms/step
Epoch 226/1000
2023-09-30 12:15:39.085 
Epoch 226/1000 
	 loss: 30.1184, MinusLogProbMetric: 30.1184, val_loss: 30.1954, val_MinusLogProbMetric: 30.1954

Epoch 226: val_loss did not improve from 30.11519
196/196 - 80s - loss: 30.1184 - MinusLogProbMetric: 30.1184 - val_loss: 30.1954 - val_MinusLogProbMetric: 30.1954 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 227/1000
2023-09-30 12:16:54.956 
Epoch 227/1000 
	 loss: 30.0599, MinusLogProbMetric: 30.0599, val_loss: 30.5119, val_MinusLogProbMetric: 30.5119

Epoch 227: val_loss did not improve from 30.11519
196/196 - 76s - loss: 30.0599 - MinusLogProbMetric: 30.0599 - val_loss: 30.5119 - val_MinusLogProbMetric: 30.5119 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 228/1000
2023-09-30 12:18:08.257 
Epoch 228/1000 
	 loss: 30.1684, MinusLogProbMetric: 30.1684, val_loss: 30.5363, val_MinusLogProbMetric: 30.5363

Epoch 228: val_loss did not improve from 30.11519
196/196 - 73s - loss: 30.1684 - MinusLogProbMetric: 30.1684 - val_loss: 30.5363 - val_MinusLogProbMetric: 30.5363 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 229/1000
2023-09-30 12:19:22.045 
Epoch 229/1000 
	 loss: 30.0571, MinusLogProbMetric: 30.0571, val_loss: 30.3111, val_MinusLogProbMetric: 30.3111

Epoch 229: val_loss did not improve from 30.11519
196/196 - 74s - loss: 30.0571 - MinusLogProbMetric: 30.0571 - val_loss: 30.3111 - val_MinusLogProbMetric: 30.3111 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 230/1000
2023-09-30 12:20:33.334 
Epoch 230/1000 
	 loss: 30.0426, MinusLogProbMetric: 30.0426, val_loss: 30.2719, val_MinusLogProbMetric: 30.2719

Epoch 230: val_loss did not improve from 30.11519
196/196 - 71s - loss: 30.0426 - MinusLogProbMetric: 30.0426 - val_loss: 30.2719 - val_MinusLogProbMetric: 30.2719 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 231/1000
2023-09-30 12:21:46.392 
Epoch 231/1000 
	 loss: 30.1301, MinusLogProbMetric: 30.1301, val_loss: 30.3746, val_MinusLogProbMetric: 30.3746

Epoch 231: val_loss did not improve from 30.11519
196/196 - 73s - loss: 30.1301 - MinusLogProbMetric: 30.1301 - val_loss: 30.3746 - val_MinusLogProbMetric: 30.3746 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 232/1000
2023-09-30 12:23:00.741 
Epoch 232/1000 
	 loss: 30.0677, MinusLogProbMetric: 30.0677, val_loss: 30.2939, val_MinusLogProbMetric: 30.2939

Epoch 232: val_loss did not improve from 30.11519
196/196 - 74s - loss: 30.0677 - MinusLogProbMetric: 30.0677 - val_loss: 30.2939 - val_MinusLogProbMetric: 30.2939 - lr: 1.1111e-04 - 74s/epoch - 379ms/step
Epoch 233/1000
2023-09-30 12:24:11.263 
Epoch 233/1000 
	 loss: 30.1053, MinusLogProbMetric: 30.1053, val_loss: 30.5950, val_MinusLogProbMetric: 30.5950

Epoch 233: val_loss did not improve from 30.11519
196/196 - 71s - loss: 30.1053 - MinusLogProbMetric: 30.1053 - val_loss: 30.5950 - val_MinusLogProbMetric: 30.5950 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 234/1000
2023-09-30 12:25:24.343 
Epoch 234/1000 
	 loss: 29.9711, MinusLogProbMetric: 29.9711, val_loss: 30.1778, val_MinusLogProbMetric: 30.1778

Epoch 234: val_loss did not improve from 30.11519
196/196 - 73s - loss: 29.9711 - MinusLogProbMetric: 29.9711 - val_loss: 30.1778 - val_MinusLogProbMetric: 30.1778 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 235/1000
2023-09-30 12:26:36.052 
Epoch 235/1000 
	 loss: 30.0386, MinusLogProbMetric: 30.0386, val_loss: 30.9997, val_MinusLogProbMetric: 30.9997

Epoch 235: val_loss did not improve from 30.11519
196/196 - 72s - loss: 30.0386 - MinusLogProbMetric: 30.0386 - val_loss: 30.9997 - val_MinusLogProbMetric: 30.9997 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 236/1000
2023-09-30 12:27:52.052 
Epoch 236/1000 
	 loss: 30.0899, MinusLogProbMetric: 30.0899, val_loss: 30.7687, val_MinusLogProbMetric: 30.7687

Epoch 236: val_loss did not improve from 30.11519
196/196 - 76s - loss: 30.0899 - MinusLogProbMetric: 30.0899 - val_loss: 30.7687 - val_MinusLogProbMetric: 30.7687 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 237/1000
2023-09-30 12:29:04.622 
Epoch 237/1000 
	 loss: 30.0635, MinusLogProbMetric: 30.0635, val_loss: 30.6208, val_MinusLogProbMetric: 30.6208

Epoch 237: val_loss did not improve from 30.11519
196/196 - 73s - loss: 30.0635 - MinusLogProbMetric: 30.0635 - val_loss: 30.6208 - val_MinusLogProbMetric: 30.6208 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 238/1000
2023-09-30 12:30:14.930 
Epoch 238/1000 
	 loss: 30.0432, MinusLogProbMetric: 30.0432, val_loss: 30.4746, val_MinusLogProbMetric: 30.4746

Epoch 238: val_loss did not improve from 30.11519
196/196 - 70s - loss: 30.0432 - MinusLogProbMetric: 30.0432 - val_loss: 30.4746 - val_MinusLogProbMetric: 30.4746 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 239/1000
2023-09-30 12:31:24.788 
Epoch 239/1000 
	 loss: 30.0488, MinusLogProbMetric: 30.0488, val_loss: 30.3768, val_MinusLogProbMetric: 30.3768

Epoch 239: val_loss did not improve from 30.11519
196/196 - 70s - loss: 30.0488 - MinusLogProbMetric: 30.0488 - val_loss: 30.3768 - val_MinusLogProbMetric: 30.3768 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 240/1000
2023-09-30 12:32:37.758 
Epoch 240/1000 
	 loss: 29.9751, MinusLogProbMetric: 29.9751, val_loss: 30.7842, val_MinusLogProbMetric: 30.7842

Epoch 240: val_loss did not improve from 30.11519
196/196 - 73s - loss: 29.9751 - MinusLogProbMetric: 29.9751 - val_loss: 30.7842 - val_MinusLogProbMetric: 30.7842 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 241/1000
2023-09-30 12:33:48.468 
Epoch 241/1000 
	 loss: 29.9930, MinusLogProbMetric: 29.9930, val_loss: 31.2551, val_MinusLogProbMetric: 31.2551

Epoch 241: val_loss did not improve from 30.11519
196/196 - 71s - loss: 29.9930 - MinusLogProbMetric: 29.9930 - val_loss: 31.2551 - val_MinusLogProbMetric: 31.2551 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 242/1000
2023-09-30 12:34:57.268 
Epoch 242/1000 
	 loss: 30.0472, MinusLogProbMetric: 30.0472, val_loss: 30.3544, val_MinusLogProbMetric: 30.3544

Epoch 242: val_loss did not improve from 30.11519
196/196 - 69s - loss: 30.0472 - MinusLogProbMetric: 30.0472 - val_loss: 30.3544 - val_MinusLogProbMetric: 30.3544 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 243/1000
2023-09-30 12:36:07.291 
Epoch 243/1000 
	 loss: 29.9802, MinusLogProbMetric: 29.9802, val_loss: 30.2200, val_MinusLogProbMetric: 30.2200

Epoch 243: val_loss did not improve from 30.11519
196/196 - 70s - loss: 29.9802 - MinusLogProbMetric: 29.9802 - val_loss: 30.2200 - val_MinusLogProbMetric: 30.2200 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 244/1000
2023-09-30 12:37:17.786 
Epoch 244/1000 
	 loss: 29.9939, MinusLogProbMetric: 29.9939, val_loss: 30.5049, val_MinusLogProbMetric: 30.5049

Epoch 244: val_loss did not improve from 30.11519
196/196 - 71s - loss: 29.9939 - MinusLogProbMetric: 29.9939 - val_loss: 30.5049 - val_MinusLogProbMetric: 30.5049 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 245/1000
2023-09-30 12:38:29.364 
Epoch 245/1000 
	 loss: 30.0732, MinusLogProbMetric: 30.0732, val_loss: 30.5026, val_MinusLogProbMetric: 30.5026

Epoch 245: val_loss did not improve from 30.11519
196/196 - 72s - loss: 30.0732 - MinusLogProbMetric: 30.0732 - val_loss: 30.5026 - val_MinusLogProbMetric: 30.5026 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 246/1000
2023-09-30 12:39:38.544 
Epoch 246/1000 
	 loss: 29.9658, MinusLogProbMetric: 29.9658, val_loss: 30.3640, val_MinusLogProbMetric: 30.3640

Epoch 246: val_loss did not improve from 30.11519
196/196 - 69s - loss: 29.9658 - MinusLogProbMetric: 29.9658 - val_loss: 30.3640 - val_MinusLogProbMetric: 30.3640 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 247/1000
2023-09-30 12:40:47.777 
Epoch 247/1000 
	 loss: 30.0025, MinusLogProbMetric: 30.0025, val_loss: 30.6243, val_MinusLogProbMetric: 30.6243

Epoch 247: val_loss did not improve from 30.11519
196/196 - 69s - loss: 30.0025 - MinusLogProbMetric: 30.0025 - val_loss: 30.6243 - val_MinusLogProbMetric: 30.6243 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 248/1000
2023-09-30 12:41:55.615 
Epoch 248/1000 
	 loss: 30.0253, MinusLogProbMetric: 30.0253, val_loss: 30.6688, val_MinusLogProbMetric: 30.6688

Epoch 248: val_loss did not improve from 30.11519
196/196 - 68s - loss: 30.0253 - MinusLogProbMetric: 30.0253 - val_loss: 30.6688 - val_MinusLogProbMetric: 30.6688 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 249/1000
2023-09-30 12:43:05.654 
Epoch 249/1000 
	 loss: 29.9832, MinusLogProbMetric: 29.9832, val_loss: 30.7875, val_MinusLogProbMetric: 30.7875

Epoch 249: val_loss did not improve from 30.11519
196/196 - 70s - loss: 29.9832 - MinusLogProbMetric: 29.9832 - val_loss: 30.7875 - val_MinusLogProbMetric: 30.7875 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 250/1000
2023-09-30 12:44:15.669 
Epoch 250/1000 
	 loss: 29.9298, MinusLogProbMetric: 29.9298, val_loss: 30.2654, val_MinusLogProbMetric: 30.2654

Epoch 250: val_loss did not improve from 30.11519
196/196 - 70s - loss: 29.9298 - MinusLogProbMetric: 29.9298 - val_loss: 30.2654 - val_MinusLogProbMetric: 30.2654 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 251/1000
2023-09-30 12:45:23.721 
Epoch 251/1000 
	 loss: 29.9991, MinusLogProbMetric: 29.9991, val_loss: 30.3886, val_MinusLogProbMetric: 30.3886

Epoch 251: val_loss did not improve from 30.11519
196/196 - 68s - loss: 29.9991 - MinusLogProbMetric: 29.9991 - val_loss: 30.3886 - val_MinusLogProbMetric: 30.3886 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 252/1000
2023-09-30 12:46:31.013 
Epoch 252/1000 
	 loss: 29.9180, MinusLogProbMetric: 29.9180, val_loss: 30.1264, val_MinusLogProbMetric: 30.1264

Epoch 252: val_loss did not improve from 30.11519
196/196 - 67s - loss: 29.9180 - MinusLogProbMetric: 29.9180 - val_loss: 30.1264 - val_MinusLogProbMetric: 30.1264 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 253/1000
2023-09-30 12:47:41.896 
Epoch 253/1000 
	 loss: 29.9419, MinusLogProbMetric: 29.9419, val_loss: 30.0286, val_MinusLogProbMetric: 30.0286

Epoch 253: val_loss improved from 30.11519 to 30.02859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 73s - loss: 29.9419 - MinusLogProbMetric: 29.9419 - val_loss: 30.0286 - val_MinusLogProbMetric: 30.0286 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 254/1000
2023-09-30 12:48:52.529 
Epoch 254/1000 
	 loss: 29.9543, MinusLogProbMetric: 29.9543, val_loss: 30.2789, val_MinusLogProbMetric: 30.2789

Epoch 254: val_loss did not improve from 30.02859
196/196 - 68s - loss: 29.9543 - MinusLogProbMetric: 29.9543 - val_loss: 30.2789 - val_MinusLogProbMetric: 30.2789 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 255/1000
2023-09-30 12:50:05.162 
Epoch 255/1000 
	 loss: 29.9922, MinusLogProbMetric: 29.9922, val_loss: 30.5826, val_MinusLogProbMetric: 30.5826

Epoch 255: val_loss did not improve from 30.02859
196/196 - 73s - loss: 29.9922 - MinusLogProbMetric: 29.9922 - val_loss: 30.5826 - val_MinusLogProbMetric: 30.5826 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 256/1000
2023-09-30 12:51:18.263 
Epoch 256/1000 
	 loss: 29.9525, MinusLogProbMetric: 29.9525, val_loss: 30.3637, val_MinusLogProbMetric: 30.3637

Epoch 256: val_loss did not improve from 30.02859
196/196 - 73s - loss: 29.9525 - MinusLogProbMetric: 29.9525 - val_loss: 30.3637 - val_MinusLogProbMetric: 30.3637 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 257/1000
2023-09-30 12:52:27.298 
Epoch 257/1000 
	 loss: 29.9481, MinusLogProbMetric: 29.9481, val_loss: 31.2695, val_MinusLogProbMetric: 31.2695

Epoch 257: val_loss did not improve from 30.02859
196/196 - 69s - loss: 29.9481 - MinusLogProbMetric: 29.9481 - val_loss: 31.2695 - val_MinusLogProbMetric: 31.2695 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 258/1000
2023-09-30 12:53:42.252 
Epoch 258/1000 
	 loss: 29.9931, MinusLogProbMetric: 29.9931, val_loss: 30.1501, val_MinusLogProbMetric: 30.1501

Epoch 258: val_loss did not improve from 30.02859
196/196 - 75s - loss: 29.9931 - MinusLogProbMetric: 29.9931 - val_loss: 30.1501 - val_MinusLogProbMetric: 30.1501 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 259/1000
2023-09-30 12:54:57.887 
Epoch 259/1000 
	 loss: 29.9785, MinusLogProbMetric: 29.9785, val_loss: 30.0679, val_MinusLogProbMetric: 30.0679

Epoch 259: val_loss did not improve from 30.02859
196/196 - 76s - loss: 29.9785 - MinusLogProbMetric: 29.9785 - val_loss: 30.0679 - val_MinusLogProbMetric: 30.0679 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 260/1000
2023-09-30 12:56:12.946 
Epoch 260/1000 
	 loss: 29.9551, MinusLogProbMetric: 29.9551, val_loss: 30.0867, val_MinusLogProbMetric: 30.0867

Epoch 260: val_loss did not improve from 30.02859
196/196 - 75s - loss: 29.9551 - MinusLogProbMetric: 29.9551 - val_loss: 30.0867 - val_MinusLogProbMetric: 30.0867 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 261/1000
2023-09-30 12:57:28.673 
Epoch 261/1000 
	 loss: 29.8702, MinusLogProbMetric: 29.8702, val_loss: 30.7111, val_MinusLogProbMetric: 30.7111

Epoch 261: val_loss did not improve from 30.02859
196/196 - 76s - loss: 29.8702 - MinusLogProbMetric: 29.8702 - val_loss: 30.7111 - val_MinusLogProbMetric: 30.7111 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 262/1000
2023-09-30 12:58:43.475 
Epoch 262/1000 
	 loss: 29.8777, MinusLogProbMetric: 29.8777, val_loss: 30.2500, val_MinusLogProbMetric: 30.2500

Epoch 262: val_loss did not improve from 30.02859
196/196 - 75s - loss: 29.8777 - MinusLogProbMetric: 29.8777 - val_loss: 30.2500 - val_MinusLogProbMetric: 30.2500 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 263/1000
2023-09-30 12:59:57.509 
Epoch 263/1000 
	 loss: 29.9254, MinusLogProbMetric: 29.9254, val_loss: 30.1185, val_MinusLogProbMetric: 30.1185

Epoch 263: val_loss did not improve from 30.02859
196/196 - 74s - loss: 29.9254 - MinusLogProbMetric: 29.9254 - val_loss: 30.1185 - val_MinusLogProbMetric: 30.1185 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 264/1000
2023-09-30 13:01:08.611 
Epoch 264/1000 
	 loss: 30.0010, MinusLogProbMetric: 30.0010, val_loss: 30.8643, val_MinusLogProbMetric: 30.8643

Epoch 264: val_loss did not improve from 30.02859
196/196 - 71s - loss: 30.0010 - MinusLogProbMetric: 30.0010 - val_loss: 30.8643 - val_MinusLogProbMetric: 30.8643 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 265/1000
2023-09-30 13:02:19.742 
Epoch 265/1000 
	 loss: 29.9527, MinusLogProbMetric: 29.9527, val_loss: 30.3967, val_MinusLogProbMetric: 30.3967

Epoch 265: val_loss did not improve from 30.02859
196/196 - 71s - loss: 29.9527 - MinusLogProbMetric: 29.9527 - val_loss: 30.3967 - val_MinusLogProbMetric: 30.3967 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 266/1000
2023-09-30 13:03:29.517 
Epoch 266/1000 
	 loss: 29.9197, MinusLogProbMetric: 29.9197, val_loss: 30.1599, val_MinusLogProbMetric: 30.1599

Epoch 266: val_loss did not improve from 30.02859
196/196 - 70s - loss: 29.9197 - MinusLogProbMetric: 29.9197 - val_loss: 30.1599 - val_MinusLogProbMetric: 30.1599 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 267/1000
2023-09-30 13:04:43.012 
Epoch 267/1000 
	 loss: 29.9254, MinusLogProbMetric: 29.9254, val_loss: 30.1309, val_MinusLogProbMetric: 30.1309

Epoch 267: val_loss did not improve from 30.02859
196/196 - 73s - loss: 29.9254 - MinusLogProbMetric: 29.9254 - val_loss: 30.1309 - val_MinusLogProbMetric: 30.1309 - lr: 1.1111e-04 - 73s/epoch - 375ms/step
Epoch 268/1000
2023-09-30 13:05:55.431 
Epoch 268/1000 
	 loss: 29.8361, MinusLogProbMetric: 29.8361, val_loss: 30.1237, val_MinusLogProbMetric: 30.1237

Epoch 268: val_loss did not improve from 30.02859
196/196 - 72s - loss: 29.8361 - MinusLogProbMetric: 29.8361 - val_loss: 30.1237 - val_MinusLogProbMetric: 30.1237 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 269/1000
2023-09-30 13:07:07.956 
Epoch 269/1000 
	 loss: 29.8780, MinusLogProbMetric: 29.8780, val_loss: 30.1614, val_MinusLogProbMetric: 30.1614

Epoch 269: val_loss did not improve from 30.02859
196/196 - 73s - loss: 29.8780 - MinusLogProbMetric: 29.8780 - val_loss: 30.1614 - val_MinusLogProbMetric: 30.1614 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 270/1000
2023-09-30 13:08:19.219 
Epoch 270/1000 
	 loss: 29.8663, MinusLogProbMetric: 29.8663, val_loss: 30.1921, val_MinusLogProbMetric: 30.1921

Epoch 270: val_loss did not improve from 30.02859
196/196 - 71s - loss: 29.8663 - MinusLogProbMetric: 29.8663 - val_loss: 30.1921 - val_MinusLogProbMetric: 30.1921 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 271/1000
2023-09-30 13:09:38.246 
Epoch 271/1000 
	 loss: 29.9273, MinusLogProbMetric: 29.9273, val_loss: 30.7164, val_MinusLogProbMetric: 30.7164

Epoch 271: val_loss did not improve from 30.02859
196/196 - 79s - loss: 29.9273 - MinusLogProbMetric: 29.9273 - val_loss: 30.7164 - val_MinusLogProbMetric: 30.7164 - lr: 1.1111e-04 - 79s/epoch - 403ms/step
Epoch 272/1000
2023-09-30 13:10:48.293 
Epoch 272/1000 
	 loss: 29.7981, MinusLogProbMetric: 29.7981, val_loss: 30.1537, val_MinusLogProbMetric: 30.1537

Epoch 272: val_loss did not improve from 30.02859
196/196 - 70s - loss: 29.7981 - MinusLogProbMetric: 29.7981 - val_loss: 30.1537 - val_MinusLogProbMetric: 30.1537 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 273/1000
2023-09-30 13:11:59.933 
Epoch 273/1000 
	 loss: 29.8318, MinusLogProbMetric: 29.8318, val_loss: 30.4679, val_MinusLogProbMetric: 30.4679

Epoch 273: val_loss did not improve from 30.02859
196/196 - 72s - loss: 29.8318 - MinusLogProbMetric: 29.8318 - val_loss: 30.4679 - val_MinusLogProbMetric: 30.4679 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 274/1000
2023-09-30 13:13:11.940 
Epoch 274/1000 
	 loss: 29.8446, MinusLogProbMetric: 29.8446, val_loss: 30.6750, val_MinusLogProbMetric: 30.6750

Epoch 274: val_loss did not improve from 30.02859
196/196 - 72s - loss: 29.8446 - MinusLogProbMetric: 29.8446 - val_loss: 30.6750 - val_MinusLogProbMetric: 30.6750 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 275/1000
2023-09-30 13:14:25.693 
Epoch 275/1000 
	 loss: 29.9487, MinusLogProbMetric: 29.9487, val_loss: 31.2173, val_MinusLogProbMetric: 31.2173

Epoch 275: val_loss did not improve from 30.02859
196/196 - 74s - loss: 29.9487 - MinusLogProbMetric: 29.9487 - val_loss: 31.2173 - val_MinusLogProbMetric: 31.2173 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 276/1000
2023-09-30 13:15:35.401 
Epoch 276/1000 
	 loss: 29.8239, MinusLogProbMetric: 29.8239, val_loss: 30.1761, val_MinusLogProbMetric: 30.1761

Epoch 276: val_loss did not improve from 30.02859
196/196 - 70s - loss: 29.8239 - MinusLogProbMetric: 29.8239 - val_loss: 30.1761 - val_MinusLogProbMetric: 30.1761 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 277/1000
2023-09-30 13:16:50.325 
Epoch 277/1000 
	 loss: 29.9116, MinusLogProbMetric: 29.9116, val_loss: 30.1650, val_MinusLogProbMetric: 30.1650

Epoch 277: val_loss did not improve from 30.02859
196/196 - 75s - loss: 29.9116 - MinusLogProbMetric: 29.9116 - val_loss: 30.1650 - val_MinusLogProbMetric: 30.1650 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 278/1000
2023-09-30 13:18:02.226 
Epoch 278/1000 
	 loss: 29.8726, MinusLogProbMetric: 29.8726, val_loss: 30.6277, val_MinusLogProbMetric: 30.6277

Epoch 278: val_loss did not improve from 30.02859
196/196 - 72s - loss: 29.8726 - MinusLogProbMetric: 29.8726 - val_loss: 30.6277 - val_MinusLogProbMetric: 30.6277 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 279/1000
2023-09-30 13:19:14.034 
Epoch 279/1000 
	 loss: 29.8230, MinusLogProbMetric: 29.8230, val_loss: 30.2062, val_MinusLogProbMetric: 30.2062

Epoch 279: val_loss did not improve from 30.02859
196/196 - 72s - loss: 29.8230 - MinusLogProbMetric: 29.8230 - val_loss: 30.2062 - val_MinusLogProbMetric: 30.2062 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 280/1000
2023-09-30 13:20:25.292 
Epoch 280/1000 
	 loss: 29.9068, MinusLogProbMetric: 29.9068, val_loss: 30.1180, val_MinusLogProbMetric: 30.1180

Epoch 280: val_loss did not improve from 30.02859
196/196 - 71s - loss: 29.9068 - MinusLogProbMetric: 29.9068 - val_loss: 30.1180 - val_MinusLogProbMetric: 30.1180 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 281/1000
2023-09-30 13:21:36.382 
Epoch 281/1000 
	 loss: 29.7805, MinusLogProbMetric: 29.7805, val_loss: 30.0708, val_MinusLogProbMetric: 30.0708

Epoch 281: val_loss did not improve from 30.02859
196/196 - 71s - loss: 29.7805 - MinusLogProbMetric: 29.7805 - val_loss: 30.0708 - val_MinusLogProbMetric: 30.0708 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 282/1000
2023-09-30 13:22:48.641 
Epoch 282/1000 
	 loss: 29.8463, MinusLogProbMetric: 29.8463, val_loss: 30.0103, val_MinusLogProbMetric: 30.0103

Epoch 282: val_loss improved from 30.02859 to 30.01031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 74s - loss: 29.8463 - MinusLogProbMetric: 29.8463 - val_loss: 30.0103 - val_MinusLogProbMetric: 30.0103 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 283/1000
2023-09-30 13:24:00.691 
Epoch 283/1000 
	 loss: 29.8200, MinusLogProbMetric: 29.8200, val_loss: 30.4231, val_MinusLogProbMetric: 30.4231

Epoch 283: val_loss did not improve from 30.01031
196/196 - 70s - loss: 29.8200 - MinusLogProbMetric: 29.8200 - val_loss: 30.4231 - val_MinusLogProbMetric: 30.4231 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 284/1000
2023-09-30 13:25:16.917 
Epoch 284/1000 
	 loss: 29.7898, MinusLogProbMetric: 29.7898, val_loss: 30.0511, val_MinusLogProbMetric: 30.0511

Epoch 284: val_loss did not improve from 30.01031
196/196 - 76s - loss: 29.7898 - MinusLogProbMetric: 29.7898 - val_loss: 30.0511 - val_MinusLogProbMetric: 30.0511 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 285/1000
2023-09-30 13:26:31.677 
Epoch 285/1000 
	 loss: 29.8419, MinusLogProbMetric: 29.8419, val_loss: 30.0359, val_MinusLogProbMetric: 30.0359

Epoch 285: val_loss did not improve from 30.01031
196/196 - 75s - loss: 29.8419 - MinusLogProbMetric: 29.8419 - val_loss: 30.0359 - val_MinusLogProbMetric: 30.0359 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 286/1000
2023-09-30 13:27:44.686 
Epoch 286/1000 
	 loss: 29.7612, MinusLogProbMetric: 29.7612, val_loss: 31.5935, val_MinusLogProbMetric: 31.5935

Epoch 286: val_loss did not improve from 30.01031
196/196 - 73s - loss: 29.7612 - MinusLogProbMetric: 29.7612 - val_loss: 31.5935 - val_MinusLogProbMetric: 31.5935 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 287/1000
2023-09-30 13:28:54.710 
Epoch 287/1000 
	 loss: 29.8085, MinusLogProbMetric: 29.8085, val_loss: 30.0303, val_MinusLogProbMetric: 30.0303

Epoch 287: val_loss did not improve from 30.01031
196/196 - 70s - loss: 29.8085 - MinusLogProbMetric: 29.8085 - val_loss: 30.0303 - val_MinusLogProbMetric: 30.0303 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 288/1000
2023-09-30 13:30:14.160 
Epoch 288/1000 
	 loss: 29.7637, MinusLogProbMetric: 29.7637, val_loss: 30.1737, val_MinusLogProbMetric: 30.1737

Epoch 288: val_loss did not improve from 30.01031
196/196 - 79s - loss: 29.7637 - MinusLogProbMetric: 29.7637 - val_loss: 30.1737 - val_MinusLogProbMetric: 30.1737 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 289/1000
2023-09-30 13:31:27.207 
Epoch 289/1000 
	 loss: 29.9515, MinusLogProbMetric: 29.9515, val_loss: 29.9463, val_MinusLogProbMetric: 29.9463

Epoch 289: val_loss improved from 30.01031 to 29.94625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 75s - loss: 29.9515 - MinusLogProbMetric: 29.9515 - val_loss: 29.9463 - val_MinusLogProbMetric: 29.9463 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 290/1000
2023-09-30 13:32:38.261 
Epoch 290/1000 
	 loss: 29.8803, MinusLogProbMetric: 29.8803, val_loss: 30.0322, val_MinusLogProbMetric: 30.0322

Epoch 290: val_loss did not improve from 29.94625
196/196 - 69s - loss: 29.8803 - MinusLogProbMetric: 29.8803 - val_loss: 30.0322 - val_MinusLogProbMetric: 30.0322 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 291/1000
2023-09-30 13:33:49.263 
Epoch 291/1000 
	 loss: 29.7600, MinusLogProbMetric: 29.7600, val_loss: 29.9803, val_MinusLogProbMetric: 29.9803

Epoch 291: val_loss did not improve from 29.94625
196/196 - 71s - loss: 29.7600 - MinusLogProbMetric: 29.7600 - val_loss: 29.9803 - val_MinusLogProbMetric: 29.9803 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 292/1000
2023-09-30 13:35:03.506 
Epoch 292/1000 
	 loss: 29.8271, MinusLogProbMetric: 29.8271, val_loss: 30.3458, val_MinusLogProbMetric: 30.3458

Epoch 292: val_loss did not improve from 29.94625
196/196 - 74s - loss: 29.8271 - MinusLogProbMetric: 29.8271 - val_loss: 30.3458 - val_MinusLogProbMetric: 30.3458 - lr: 1.1111e-04 - 74s/epoch - 379ms/step
Epoch 293/1000
2023-09-30 13:36:14.823 
Epoch 293/1000 
	 loss: 29.8021, MinusLogProbMetric: 29.8021, val_loss: 29.9820, val_MinusLogProbMetric: 29.9820

Epoch 293: val_loss did not improve from 29.94625
196/196 - 71s - loss: 29.8021 - MinusLogProbMetric: 29.8021 - val_loss: 29.9820 - val_MinusLogProbMetric: 29.9820 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 294/1000
2023-09-30 13:37:28.747 
Epoch 294/1000 
	 loss: 29.7749, MinusLogProbMetric: 29.7749, val_loss: 30.4002, val_MinusLogProbMetric: 30.4002

Epoch 294: val_loss did not improve from 29.94625
196/196 - 74s - loss: 29.7749 - MinusLogProbMetric: 29.7749 - val_loss: 30.4002 - val_MinusLogProbMetric: 30.4002 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 295/1000
2023-09-30 13:38:36.900 
Epoch 295/1000 
	 loss: 29.8394, MinusLogProbMetric: 29.8394, val_loss: 30.1261, val_MinusLogProbMetric: 30.1261

Epoch 295: val_loss did not improve from 29.94625
196/196 - 68s - loss: 29.8394 - MinusLogProbMetric: 29.8394 - val_loss: 30.1261 - val_MinusLogProbMetric: 30.1261 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 296/1000
2023-09-30 13:39:46.373 
Epoch 296/1000 
	 loss: 29.7252, MinusLogProbMetric: 29.7252, val_loss: 30.2377, val_MinusLogProbMetric: 30.2377

Epoch 296: val_loss did not improve from 29.94625
196/196 - 69s - loss: 29.7252 - MinusLogProbMetric: 29.7252 - val_loss: 30.2377 - val_MinusLogProbMetric: 30.2377 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 297/1000
2023-09-30 13:40:52.766 
Epoch 297/1000 
	 loss: 29.7076, MinusLogProbMetric: 29.7076, val_loss: 29.9967, val_MinusLogProbMetric: 29.9967

Epoch 297: val_loss did not improve from 29.94625
196/196 - 66s - loss: 29.7076 - MinusLogProbMetric: 29.7076 - val_loss: 29.9967 - val_MinusLogProbMetric: 29.9967 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 298/1000
2023-09-30 13:42:03.793 
Epoch 298/1000 
	 loss: 29.7579, MinusLogProbMetric: 29.7579, val_loss: 30.9347, val_MinusLogProbMetric: 30.9347

Epoch 298: val_loss did not improve from 29.94625
196/196 - 71s - loss: 29.7579 - MinusLogProbMetric: 29.7579 - val_loss: 30.9347 - val_MinusLogProbMetric: 30.9347 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 299/1000
2023-09-30 13:43:17.384 
Epoch 299/1000 
	 loss: 29.8129, MinusLogProbMetric: 29.8129, val_loss: 30.4495, val_MinusLogProbMetric: 30.4495

Epoch 299: val_loss did not improve from 29.94625
196/196 - 74s - loss: 29.8129 - MinusLogProbMetric: 29.8129 - val_loss: 30.4495 - val_MinusLogProbMetric: 30.4495 - lr: 1.1111e-04 - 74s/epoch - 375ms/step
Epoch 300/1000
2023-09-30 13:44:30.363 
Epoch 300/1000 
	 loss: 29.7697, MinusLogProbMetric: 29.7697, val_loss: 30.0051, val_MinusLogProbMetric: 30.0051

Epoch 300: val_loss did not improve from 29.94625
196/196 - 73s - loss: 29.7697 - MinusLogProbMetric: 29.7697 - val_loss: 30.0051 - val_MinusLogProbMetric: 30.0051 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 301/1000
2023-09-30 13:45:38.038 
Epoch 301/1000 
	 loss: 29.7945, MinusLogProbMetric: 29.7945, val_loss: 29.9984, val_MinusLogProbMetric: 29.9984

Epoch 301: val_loss did not improve from 29.94625
196/196 - 68s - loss: 29.7945 - MinusLogProbMetric: 29.7945 - val_loss: 29.9984 - val_MinusLogProbMetric: 29.9984 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 302/1000
2023-09-30 13:46:50.611 
Epoch 302/1000 
	 loss: 29.6927, MinusLogProbMetric: 29.6927, val_loss: 30.3399, val_MinusLogProbMetric: 30.3399

Epoch 302: val_loss did not improve from 29.94625
196/196 - 73s - loss: 29.6927 - MinusLogProbMetric: 29.6927 - val_loss: 30.3399 - val_MinusLogProbMetric: 30.3399 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 303/1000
2023-09-30 13:48:02.826 
Epoch 303/1000 
	 loss: 29.7565, MinusLogProbMetric: 29.7565, val_loss: 30.0453, val_MinusLogProbMetric: 30.0453

Epoch 303: val_loss did not improve from 29.94625
196/196 - 72s - loss: 29.7565 - MinusLogProbMetric: 29.7565 - val_loss: 30.0453 - val_MinusLogProbMetric: 30.0453 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 304/1000
2023-09-30 13:49:14.132 
Epoch 304/1000 
	 loss: 29.7611, MinusLogProbMetric: 29.7611, val_loss: 30.1702, val_MinusLogProbMetric: 30.1702

Epoch 304: val_loss did not improve from 29.94625
196/196 - 71s - loss: 29.7611 - MinusLogProbMetric: 29.7611 - val_loss: 30.1702 - val_MinusLogProbMetric: 30.1702 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 305/1000
2023-09-30 13:50:21.034 
Epoch 305/1000 
	 loss: 29.8027, MinusLogProbMetric: 29.8027, val_loss: 30.0751, val_MinusLogProbMetric: 30.0751

Epoch 305: val_loss did not improve from 29.94625
196/196 - 67s - loss: 29.8027 - MinusLogProbMetric: 29.8027 - val_loss: 30.0751 - val_MinusLogProbMetric: 30.0751 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 306/1000
2023-09-30 13:51:28.165 
Epoch 306/1000 
	 loss: 29.7348, MinusLogProbMetric: 29.7348, val_loss: 30.0182, val_MinusLogProbMetric: 30.0182

Epoch 306: val_loss did not improve from 29.94625
196/196 - 67s - loss: 29.7348 - MinusLogProbMetric: 29.7348 - val_loss: 30.0182 - val_MinusLogProbMetric: 30.0182 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 307/1000
2023-09-30 13:52:35.165 
Epoch 307/1000 
	 loss: 29.7208, MinusLogProbMetric: 29.7208, val_loss: 30.5472, val_MinusLogProbMetric: 30.5472

Epoch 307: val_loss did not improve from 29.94625
196/196 - 67s - loss: 29.7208 - MinusLogProbMetric: 29.7208 - val_loss: 30.5472 - val_MinusLogProbMetric: 30.5472 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 308/1000
2023-09-30 13:53:47.715 
Epoch 308/1000 
	 loss: 29.7297, MinusLogProbMetric: 29.7297, val_loss: 30.0990, val_MinusLogProbMetric: 30.0990

Epoch 308: val_loss did not improve from 29.94625
196/196 - 73s - loss: 29.7297 - MinusLogProbMetric: 29.7297 - val_loss: 30.0990 - val_MinusLogProbMetric: 30.0990 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 309/1000
2023-09-30 13:54:58.063 
Epoch 309/1000 
	 loss: 29.7530, MinusLogProbMetric: 29.7530, val_loss: 29.8043, val_MinusLogProbMetric: 29.8043

Epoch 309: val_loss improved from 29.94625 to 29.80427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 71s - loss: 29.7530 - MinusLogProbMetric: 29.7530 - val_loss: 29.8043 - val_MinusLogProbMetric: 29.8043 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 310/1000
2023-09-30 13:56:06.653 
Epoch 310/1000 
	 loss: 29.7816, MinusLogProbMetric: 29.7816, val_loss: 30.0407, val_MinusLogProbMetric: 30.0407

Epoch 310: val_loss did not improve from 29.80427
196/196 - 68s - loss: 29.7816 - MinusLogProbMetric: 29.7816 - val_loss: 30.0407 - val_MinusLogProbMetric: 30.0407 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 311/1000
2023-09-30 13:57:15.680 
Epoch 311/1000 
	 loss: 29.7606, MinusLogProbMetric: 29.7606, val_loss: 30.3351, val_MinusLogProbMetric: 30.3351

Epoch 311: val_loss did not improve from 29.80427
196/196 - 69s - loss: 29.7606 - MinusLogProbMetric: 29.7606 - val_loss: 30.3351 - val_MinusLogProbMetric: 30.3351 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 312/1000
2023-09-30 13:58:23.119 
Epoch 312/1000 
	 loss: 29.6892, MinusLogProbMetric: 29.6892, val_loss: 30.1242, val_MinusLogProbMetric: 30.1242

Epoch 312: val_loss did not improve from 29.80427
196/196 - 67s - loss: 29.6892 - MinusLogProbMetric: 29.6892 - val_loss: 30.1242 - val_MinusLogProbMetric: 30.1242 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 313/1000
2023-09-30 13:59:34.733 
Epoch 313/1000 
	 loss: 29.7971, MinusLogProbMetric: 29.7971, val_loss: 30.5749, val_MinusLogProbMetric: 30.5749

Epoch 313: val_loss did not improve from 29.80427
196/196 - 72s - loss: 29.7971 - MinusLogProbMetric: 29.7971 - val_loss: 30.5749 - val_MinusLogProbMetric: 30.5749 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 314/1000
2023-09-30 14:00:48.752 
Epoch 314/1000 
	 loss: 29.8234, MinusLogProbMetric: 29.8234, val_loss: 30.7953, val_MinusLogProbMetric: 30.7953

Epoch 314: val_loss did not improve from 29.80427
196/196 - 74s - loss: 29.8234 - MinusLogProbMetric: 29.8234 - val_loss: 30.7953 - val_MinusLogProbMetric: 30.7953 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 315/1000
2023-09-30 14:01:58.723 
Epoch 315/1000 
	 loss: 29.7110, MinusLogProbMetric: 29.7110, val_loss: 30.4231, val_MinusLogProbMetric: 30.4231

Epoch 315: val_loss did not improve from 29.80427
196/196 - 70s - loss: 29.7110 - MinusLogProbMetric: 29.7110 - val_loss: 30.4231 - val_MinusLogProbMetric: 30.4231 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 316/1000
2023-09-30 14:03:14.924 
Epoch 316/1000 
	 loss: 29.7485, MinusLogProbMetric: 29.7485, val_loss: 30.2035, val_MinusLogProbMetric: 30.2035

Epoch 316: val_loss did not improve from 29.80427
196/196 - 76s - loss: 29.7485 - MinusLogProbMetric: 29.7485 - val_loss: 30.2035 - val_MinusLogProbMetric: 30.2035 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 317/1000
2023-09-30 14:04:22.139 
Epoch 317/1000 
	 loss: 29.7472, MinusLogProbMetric: 29.7472, val_loss: 30.0669, val_MinusLogProbMetric: 30.0669

Epoch 317: val_loss did not improve from 29.80427
196/196 - 67s - loss: 29.7472 - MinusLogProbMetric: 29.7472 - val_loss: 30.0669 - val_MinusLogProbMetric: 30.0669 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 318/1000
2023-09-30 14:05:27.752 
Epoch 318/1000 
	 loss: 29.7418, MinusLogProbMetric: 29.7418, val_loss: 31.9971, val_MinusLogProbMetric: 31.9971

Epoch 318: val_loss did not improve from 29.80427
196/196 - 66s - loss: 29.7418 - MinusLogProbMetric: 29.7418 - val_loss: 31.9971 - val_MinusLogProbMetric: 31.9971 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 319/1000
2023-09-30 14:06:35.446 
Epoch 319/1000 
	 loss: 29.6844, MinusLogProbMetric: 29.6844, val_loss: 30.2613, val_MinusLogProbMetric: 30.2613

Epoch 319: val_loss did not improve from 29.80427
196/196 - 68s - loss: 29.6844 - MinusLogProbMetric: 29.6844 - val_loss: 30.2613 - val_MinusLogProbMetric: 30.2613 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 320/1000
2023-09-30 14:07:42.253 
Epoch 320/1000 
	 loss: 29.6545, MinusLogProbMetric: 29.6545, val_loss: 30.0659, val_MinusLogProbMetric: 30.0659

Epoch 320: val_loss did not improve from 29.80427
196/196 - 67s - loss: 29.6545 - MinusLogProbMetric: 29.6545 - val_loss: 30.0659 - val_MinusLogProbMetric: 30.0659 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 321/1000
2023-09-30 14:08:53.006 
Epoch 321/1000 
	 loss: 29.6307, MinusLogProbMetric: 29.6307, val_loss: 30.0322, val_MinusLogProbMetric: 30.0322

Epoch 321: val_loss did not improve from 29.80427
196/196 - 71s - loss: 29.6307 - MinusLogProbMetric: 29.6307 - val_loss: 30.0322 - val_MinusLogProbMetric: 30.0322 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 322/1000
2023-09-30 14:10:01.085 
Epoch 322/1000 
	 loss: 29.6554, MinusLogProbMetric: 29.6554, val_loss: 30.2853, val_MinusLogProbMetric: 30.2853

Epoch 322: val_loss did not improve from 29.80427
196/196 - 68s - loss: 29.6554 - MinusLogProbMetric: 29.6554 - val_loss: 30.2853 - val_MinusLogProbMetric: 30.2853 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 323/1000
2023-09-30 14:11:06.901 
Epoch 323/1000 
	 loss: 29.6611, MinusLogProbMetric: 29.6611, val_loss: 29.9347, val_MinusLogProbMetric: 29.9347

Epoch 323: val_loss did not improve from 29.80427
196/196 - 66s - loss: 29.6611 - MinusLogProbMetric: 29.6611 - val_loss: 29.9347 - val_MinusLogProbMetric: 29.9347 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 324/1000
2023-09-30 14:12:14.929 
Epoch 324/1000 
	 loss: 29.7483, MinusLogProbMetric: 29.7483, val_loss: 30.4799, val_MinusLogProbMetric: 30.4799

Epoch 324: val_loss did not improve from 29.80427
196/196 - 68s - loss: 29.7483 - MinusLogProbMetric: 29.7483 - val_loss: 30.4799 - val_MinusLogProbMetric: 30.4799 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 325/1000
2023-09-30 14:13:21.872 
Epoch 325/1000 
	 loss: 29.6680, MinusLogProbMetric: 29.6680, val_loss: 30.5673, val_MinusLogProbMetric: 30.5673

Epoch 325: val_loss did not improve from 29.80427
196/196 - 67s - loss: 29.6680 - MinusLogProbMetric: 29.6680 - val_loss: 30.5673 - val_MinusLogProbMetric: 30.5673 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 326/1000
2023-09-30 14:14:25.110 
Epoch 326/1000 
	 loss: 29.7059, MinusLogProbMetric: 29.7059, val_loss: 29.8900, val_MinusLogProbMetric: 29.8900

Epoch 326: val_loss did not improve from 29.80427
196/196 - 63s - loss: 29.7059 - MinusLogProbMetric: 29.7059 - val_loss: 29.8900 - val_MinusLogProbMetric: 29.8900 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 327/1000
2023-09-30 14:15:33.875 
Epoch 327/1000 
	 loss: 29.6617, MinusLogProbMetric: 29.6617, val_loss: 30.1962, val_MinusLogProbMetric: 30.1962

Epoch 327: val_loss did not improve from 29.80427
196/196 - 69s - loss: 29.6617 - MinusLogProbMetric: 29.6617 - val_loss: 30.1962 - val_MinusLogProbMetric: 30.1962 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 328/1000
2023-09-30 14:16:42.721 
Epoch 328/1000 
	 loss: 29.7088, MinusLogProbMetric: 29.7088, val_loss: 29.7799, val_MinusLogProbMetric: 29.7799

Epoch 328: val_loss improved from 29.80427 to 29.77991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 70s - loss: 29.7088 - MinusLogProbMetric: 29.7088 - val_loss: 29.7799 - val_MinusLogProbMetric: 29.7799 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 329/1000
2023-09-30 14:17:51.584 
Epoch 329/1000 
	 loss: 29.6439, MinusLogProbMetric: 29.6439, val_loss: 29.8260, val_MinusLogProbMetric: 29.8260

Epoch 329: val_loss did not improve from 29.77991
196/196 - 67s - loss: 29.6439 - MinusLogProbMetric: 29.6439 - val_loss: 29.8260 - val_MinusLogProbMetric: 29.8260 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 330/1000
2023-09-30 14:18:57.608 
Epoch 330/1000 
	 loss: 29.6868, MinusLogProbMetric: 29.6868, val_loss: 30.1228, val_MinusLogProbMetric: 30.1228

Epoch 330: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.6868 - MinusLogProbMetric: 29.6868 - val_loss: 30.1228 - val_MinusLogProbMetric: 30.1228 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 331/1000
2023-09-30 14:20:06.332 
Epoch 331/1000 
	 loss: 29.6562, MinusLogProbMetric: 29.6562, val_loss: 30.1049, val_MinusLogProbMetric: 30.1049

Epoch 331: val_loss did not improve from 29.77991
196/196 - 69s - loss: 29.6562 - MinusLogProbMetric: 29.6562 - val_loss: 30.1049 - val_MinusLogProbMetric: 30.1049 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 332/1000
2023-09-30 14:21:15.490 
Epoch 332/1000 
	 loss: 29.6407, MinusLogProbMetric: 29.6407, val_loss: 29.9476, val_MinusLogProbMetric: 29.9476

Epoch 332: val_loss did not improve from 29.77991
196/196 - 69s - loss: 29.6407 - MinusLogProbMetric: 29.6407 - val_loss: 29.9476 - val_MinusLogProbMetric: 29.9476 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 333/1000
2023-09-30 14:22:21.329 
Epoch 333/1000 
	 loss: 29.6787, MinusLogProbMetric: 29.6787, val_loss: 30.2301, val_MinusLogProbMetric: 30.2301

Epoch 333: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.6787 - MinusLogProbMetric: 29.6787 - val_loss: 30.2301 - val_MinusLogProbMetric: 30.2301 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 334/1000
2023-09-30 14:23:26.182 
Epoch 334/1000 
	 loss: 29.6380, MinusLogProbMetric: 29.6380, val_loss: 29.8679, val_MinusLogProbMetric: 29.8679

Epoch 334: val_loss did not improve from 29.77991
196/196 - 65s - loss: 29.6380 - MinusLogProbMetric: 29.6380 - val_loss: 29.8679 - val_MinusLogProbMetric: 29.8679 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 335/1000
2023-09-30 14:24:33.579 
Epoch 335/1000 
	 loss: 29.6860, MinusLogProbMetric: 29.6860, val_loss: 30.2402, val_MinusLogProbMetric: 30.2402

Epoch 335: val_loss did not improve from 29.77991
196/196 - 67s - loss: 29.6860 - MinusLogProbMetric: 29.6860 - val_loss: 30.2402 - val_MinusLogProbMetric: 30.2402 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 336/1000
2023-09-30 14:25:38.613 
Epoch 336/1000 
	 loss: 29.6048, MinusLogProbMetric: 29.6048, val_loss: 30.9051, val_MinusLogProbMetric: 30.9051

Epoch 336: val_loss did not improve from 29.77991
196/196 - 65s - loss: 29.6048 - MinusLogProbMetric: 29.6048 - val_loss: 30.9051 - val_MinusLogProbMetric: 30.9051 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 337/1000
2023-09-30 14:26:44.429 
Epoch 337/1000 
	 loss: 29.7912, MinusLogProbMetric: 29.7912, val_loss: 30.1433, val_MinusLogProbMetric: 30.1433

Epoch 337: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.7912 - MinusLogProbMetric: 29.7912 - val_loss: 30.1433 - val_MinusLogProbMetric: 30.1433 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 338/1000
2023-09-30 14:27:48.298 
Epoch 338/1000 
	 loss: 29.6308, MinusLogProbMetric: 29.6308, val_loss: 30.0211, val_MinusLogProbMetric: 30.0211

Epoch 338: val_loss did not improve from 29.77991
196/196 - 64s - loss: 29.6308 - MinusLogProbMetric: 29.6308 - val_loss: 30.0211 - val_MinusLogProbMetric: 30.0211 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 339/1000
2023-09-30 14:28:55.061 
Epoch 339/1000 
	 loss: 29.6532, MinusLogProbMetric: 29.6532, val_loss: 30.1133, val_MinusLogProbMetric: 30.1133

Epoch 339: val_loss did not improve from 29.77991
196/196 - 67s - loss: 29.6532 - MinusLogProbMetric: 29.6532 - val_loss: 30.1133 - val_MinusLogProbMetric: 30.1133 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 340/1000
2023-09-30 14:30:00.685 
Epoch 340/1000 
	 loss: 29.7290, MinusLogProbMetric: 29.7290, val_loss: 30.4925, val_MinusLogProbMetric: 30.4925

Epoch 340: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.7290 - MinusLogProbMetric: 29.7290 - val_loss: 30.4925 - val_MinusLogProbMetric: 30.4925 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 341/1000
2023-09-30 14:31:02.529 
Epoch 341/1000 
	 loss: 29.6531, MinusLogProbMetric: 29.6531, val_loss: 30.0398, val_MinusLogProbMetric: 30.0398

Epoch 341: val_loss did not improve from 29.77991
196/196 - 62s - loss: 29.6531 - MinusLogProbMetric: 29.6531 - val_loss: 30.0398 - val_MinusLogProbMetric: 30.0398 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 342/1000
2023-09-30 14:32:07.269 
Epoch 342/1000 
	 loss: 29.6257, MinusLogProbMetric: 29.6257, val_loss: 30.1982, val_MinusLogProbMetric: 30.1982

Epoch 342: val_loss did not improve from 29.77991
196/196 - 65s - loss: 29.6257 - MinusLogProbMetric: 29.6257 - val_loss: 30.1982 - val_MinusLogProbMetric: 30.1982 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 343/1000
2023-09-30 14:33:16.438 
Epoch 343/1000 
	 loss: 29.5964, MinusLogProbMetric: 29.5964, val_loss: 29.9603, val_MinusLogProbMetric: 29.9603

Epoch 343: val_loss did not improve from 29.77991
196/196 - 69s - loss: 29.5964 - MinusLogProbMetric: 29.5964 - val_loss: 29.9603 - val_MinusLogProbMetric: 29.9603 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 344/1000
2023-09-30 14:34:22.555 
Epoch 344/1000 
	 loss: 29.5889, MinusLogProbMetric: 29.5889, val_loss: 29.8867, val_MinusLogProbMetric: 29.8867

Epoch 344: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.5889 - MinusLogProbMetric: 29.5889 - val_loss: 29.8867 - val_MinusLogProbMetric: 29.8867 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 345/1000
2023-09-30 14:35:27.255 
Epoch 345/1000 
	 loss: 29.6167, MinusLogProbMetric: 29.6167, val_loss: 29.8767, val_MinusLogProbMetric: 29.8767

Epoch 345: val_loss did not improve from 29.77991
196/196 - 65s - loss: 29.6167 - MinusLogProbMetric: 29.6167 - val_loss: 29.8767 - val_MinusLogProbMetric: 29.8767 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 346/1000
2023-09-30 14:36:33.449 
Epoch 346/1000 
	 loss: 29.5999, MinusLogProbMetric: 29.5999, val_loss: 30.0615, val_MinusLogProbMetric: 30.0615

Epoch 346: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.5999 - MinusLogProbMetric: 29.5999 - val_loss: 30.0615 - val_MinusLogProbMetric: 30.0615 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 347/1000
2023-09-30 14:37:37.682 
Epoch 347/1000 
	 loss: 29.6042, MinusLogProbMetric: 29.6042, val_loss: 29.8789, val_MinusLogProbMetric: 29.8789

Epoch 347: val_loss did not improve from 29.77991
196/196 - 64s - loss: 29.6042 - MinusLogProbMetric: 29.6042 - val_loss: 29.8789 - val_MinusLogProbMetric: 29.8789 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 348/1000
2023-09-30 14:38:44.780 
Epoch 348/1000 
	 loss: 29.6415, MinusLogProbMetric: 29.6415, val_loss: 30.2742, val_MinusLogProbMetric: 30.2742

Epoch 348: val_loss did not improve from 29.77991
196/196 - 67s - loss: 29.6415 - MinusLogProbMetric: 29.6415 - val_loss: 30.2742 - val_MinusLogProbMetric: 30.2742 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 349/1000
2023-09-30 14:39:50.576 
Epoch 349/1000 
	 loss: 29.5998, MinusLogProbMetric: 29.5998, val_loss: 30.0066, val_MinusLogProbMetric: 30.0066

Epoch 349: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.5998 - MinusLogProbMetric: 29.5998 - val_loss: 30.0066 - val_MinusLogProbMetric: 30.0066 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 350/1000
2023-09-30 14:40:54.548 
Epoch 350/1000 
	 loss: 29.6263, MinusLogProbMetric: 29.6263, val_loss: 29.9826, val_MinusLogProbMetric: 29.9826

Epoch 350: val_loss did not improve from 29.77991
196/196 - 64s - loss: 29.6263 - MinusLogProbMetric: 29.6263 - val_loss: 29.9826 - val_MinusLogProbMetric: 29.9826 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 351/1000
2023-09-30 14:41:57.352 
Epoch 351/1000 
	 loss: 29.5782, MinusLogProbMetric: 29.5782, val_loss: 30.7480, val_MinusLogProbMetric: 30.7480

Epoch 351: val_loss did not improve from 29.77991
196/196 - 63s - loss: 29.5782 - MinusLogProbMetric: 29.5782 - val_loss: 30.7480 - val_MinusLogProbMetric: 30.7480 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 352/1000
2023-09-30 14:43:03.400 
Epoch 352/1000 
	 loss: 29.5688, MinusLogProbMetric: 29.5688, val_loss: 30.7411, val_MinusLogProbMetric: 30.7411

Epoch 352: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.5688 - MinusLogProbMetric: 29.5688 - val_loss: 30.7411 - val_MinusLogProbMetric: 30.7411 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 353/1000
2023-09-30 14:44:09.408 
Epoch 353/1000 
	 loss: 29.6865, MinusLogProbMetric: 29.6865, val_loss: 29.8453, val_MinusLogProbMetric: 29.8453

Epoch 353: val_loss did not improve from 29.77991
196/196 - 66s - loss: 29.6865 - MinusLogProbMetric: 29.6865 - val_loss: 29.8453 - val_MinusLogProbMetric: 29.8453 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 354/1000
2023-09-30 14:45:18.373 
Epoch 354/1000 
	 loss: 29.5467, MinusLogProbMetric: 29.5467, val_loss: 29.7414, val_MinusLogProbMetric: 29.7414

Epoch 354: val_loss improved from 29.77991 to 29.74142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 70s - loss: 29.5467 - MinusLogProbMetric: 29.5467 - val_loss: 29.7414 - val_MinusLogProbMetric: 29.7414 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 355/1000
2023-09-30 14:46:24.619 
Epoch 355/1000 
	 loss: 29.6157, MinusLogProbMetric: 29.6157, val_loss: 30.1393, val_MinusLogProbMetric: 30.1393

Epoch 355: val_loss did not improve from 29.74142
196/196 - 65s - loss: 29.6157 - MinusLogProbMetric: 29.6157 - val_loss: 30.1393 - val_MinusLogProbMetric: 30.1393 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 356/1000
2023-09-30 14:47:28.449 
Epoch 356/1000 
	 loss: 29.5876, MinusLogProbMetric: 29.5876, val_loss: 30.3527, val_MinusLogProbMetric: 30.3527

Epoch 356: val_loss did not improve from 29.74142
196/196 - 64s - loss: 29.5876 - MinusLogProbMetric: 29.5876 - val_loss: 30.3527 - val_MinusLogProbMetric: 30.3527 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 357/1000
2023-09-30 14:48:30.564 
Epoch 357/1000 
	 loss: 29.6061, MinusLogProbMetric: 29.6061, val_loss: 30.0708, val_MinusLogProbMetric: 30.0708

Epoch 357: val_loss did not improve from 29.74142
196/196 - 62s - loss: 29.6061 - MinusLogProbMetric: 29.6061 - val_loss: 30.0708 - val_MinusLogProbMetric: 30.0708 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 358/1000
2023-09-30 14:49:33.066 
Epoch 358/1000 
	 loss: 29.6327, MinusLogProbMetric: 29.6327, val_loss: 30.0074, val_MinusLogProbMetric: 30.0074

Epoch 358: val_loss did not improve from 29.74142
196/196 - 62s - loss: 29.6327 - MinusLogProbMetric: 29.6327 - val_loss: 30.0074 - val_MinusLogProbMetric: 30.0074 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 359/1000
2023-09-30 14:50:35.075 
Epoch 359/1000 
	 loss: 29.5845, MinusLogProbMetric: 29.5845, val_loss: 30.0180, val_MinusLogProbMetric: 30.0180

Epoch 359: val_loss did not improve from 29.74142
196/196 - 62s - loss: 29.5845 - MinusLogProbMetric: 29.5845 - val_loss: 30.0180 - val_MinusLogProbMetric: 30.0180 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 360/1000
2023-09-30 14:51:37.821 
Epoch 360/1000 
	 loss: 29.6223, MinusLogProbMetric: 29.6223, val_loss: 29.9264, val_MinusLogProbMetric: 29.9264

Epoch 360: val_loss did not improve from 29.74142
196/196 - 63s - loss: 29.6223 - MinusLogProbMetric: 29.6223 - val_loss: 29.9264 - val_MinusLogProbMetric: 29.9264 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 361/1000
2023-09-30 14:52:38.142 
Epoch 361/1000 
	 loss: 29.6399, MinusLogProbMetric: 29.6399, val_loss: 29.8565, val_MinusLogProbMetric: 29.8565

Epoch 361: val_loss did not improve from 29.74142
196/196 - 60s - loss: 29.6399 - MinusLogProbMetric: 29.6399 - val_loss: 29.8565 - val_MinusLogProbMetric: 29.8565 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 362/1000
2023-09-30 14:53:39.099 
Epoch 362/1000 
	 loss: 29.5644, MinusLogProbMetric: 29.5644, val_loss: 29.7631, val_MinusLogProbMetric: 29.7631

Epoch 362: val_loss did not improve from 29.74142
196/196 - 61s - loss: 29.5644 - MinusLogProbMetric: 29.5644 - val_loss: 29.7631 - val_MinusLogProbMetric: 29.7631 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 363/1000
2023-09-30 14:54:42.666 
Epoch 363/1000 
	 loss: 29.6469, MinusLogProbMetric: 29.6469, val_loss: 29.8693, val_MinusLogProbMetric: 29.8693

Epoch 363: val_loss did not improve from 29.74142
196/196 - 64s - loss: 29.6469 - MinusLogProbMetric: 29.6469 - val_loss: 29.8693 - val_MinusLogProbMetric: 29.8693 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 364/1000
2023-09-30 14:55:43.928 
Epoch 364/1000 
	 loss: 29.5852, MinusLogProbMetric: 29.5852, val_loss: 30.2779, val_MinusLogProbMetric: 30.2779

Epoch 364: val_loss did not improve from 29.74142
196/196 - 61s - loss: 29.5852 - MinusLogProbMetric: 29.5852 - val_loss: 30.2779 - val_MinusLogProbMetric: 30.2779 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 365/1000
2023-09-30 14:56:46.345 
Epoch 365/1000 
	 loss: 29.5625, MinusLogProbMetric: 29.5625, val_loss: 30.3779, val_MinusLogProbMetric: 30.3779

Epoch 365: val_loss did not improve from 29.74142
196/196 - 62s - loss: 29.5625 - MinusLogProbMetric: 29.5625 - val_loss: 30.3779 - val_MinusLogProbMetric: 30.3779 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 366/1000
2023-09-30 14:57:49.600 
Epoch 366/1000 
	 loss: 29.6280, MinusLogProbMetric: 29.6280, val_loss: 30.4378, val_MinusLogProbMetric: 30.4378

Epoch 366: val_loss did not improve from 29.74142
196/196 - 63s - loss: 29.6280 - MinusLogProbMetric: 29.6280 - val_loss: 30.4378 - val_MinusLogProbMetric: 30.4378 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 367/1000
2023-09-30 14:58:51.486 
Epoch 367/1000 
	 loss: 29.6142, MinusLogProbMetric: 29.6142, val_loss: 30.6039, val_MinusLogProbMetric: 30.6039

Epoch 367: val_loss did not improve from 29.74142
196/196 - 62s - loss: 29.6142 - MinusLogProbMetric: 29.6142 - val_loss: 30.6039 - val_MinusLogProbMetric: 30.6039 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 368/1000
2023-09-30 14:59:54.223 
Epoch 368/1000 
	 loss: 29.5940, MinusLogProbMetric: 29.5940, val_loss: 30.6351, val_MinusLogProbMetric: 30.6351

Epoch 368: val_loss did not improve from 29.74142
196/196 - 63s - loss: 29.5940 - MinusLogProbMetric: 29.5940 - val_loss: 30.6351 - val_MinusLogProbMetric: 30.6351 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 369/1000
2023-09-30 15:00:54.969 
Epoch 369/1000 
	 loss: 29.5355, MinusLogProbMetric: 29.5355, val_loss: 29.7623, val_MinusLogProbMetric: 29.7623

Epoch 369: val_loss did not improve from 29.74142
196/196 - 61s - loss: 29.5355 - MinusLogProbMetric: 29.5355 - val_loss: 29.7623 - val_MinusLogProbMetric: 29.7623 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 370/1000
2023-09-30 15:01:56.304 
Epoch 370/1000 
	 loss: 29.5239, MinusLogProbMetric: 29.5239, val_loss: 30.0327, val_MinusLogProbMetric: 30.0327

Epoch 370: val_loss did not improve from 29.74142
196/196 - 61s - loss: 29.5239 - MinusLogProbMetric: 29.5239 - val_loss: 30.0327 - val_MinusLogProbMetric: 30.0327 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 371/1000
2023-09-30 15:02:58.684 
Epoch 371/1000 
	 loss: 29.5191, MinusLogProbMetric: 29.5191, val_loss: 29.8509, val_MinusLogProbMetric: 29.8509

Epoch 371: val_loss did not improve from 29.74142
196/196 - 62s - loss: 29.5191 - MinusLogProbMetric: 29.5191 - val_loss: 29.8509 - val_MinusLogProbMetric: 29.8509 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 372/1000
2023-09-30 15:04:02.815 
Epoch 372/1000 
	 loss: 29.5467, MinusLogProbMetric: 29.5467, val_loss: 29.7224, val_MinusLogProbMetric: 29.7224

Epoch 372: val_loss improved from 29.74142 to 29.72235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 66s - loss: 29.5467 - MinusLogProbMetric: 29.5467 - val_loss: 29.7224 - val_MinusLogProbMetric: 29.7224 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 373/1000
2023-09-30 15:05:07.456 
Epoch 373/1000 
	 loss: 29.5343, MinusLogProbMetric: 29.5343, val_loss: 30.2143, val_MinusLogProbMetric: 30.2143

Epoch 373: val_loss did not improve from 29.72235
196/196 - 63s - loss: 29.5343 - MinusLogProbMetric: 29.5343 - val_loss: 30.2143 - val_MinusLogProbMetric: 30.2143 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 374/1000
2023-09-30 15:06:08.385 
Epoch 374/1000 
	 loss: 29.5547, MinusLogProbMetric: 29.5547, val_loss: 30.0447, val_MinusLogProbMetric: 30.0447

Epoch 374: val_loss did not improve from 29.72235
196/196 - 61s - loss: 29.5547 - MinusLogProbMetric: 29.5547 - val_loss: 30.0447 - val_MinusLogProbMetric: 30.0447 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 375/1000
2023-09-30 15:07:11.124 
Epoch 375/1000 
	 loss: 29.5433, MinusLogProbMetric: 29.5433, val_loss: 30.5065, val_MinusLogProbMetric: 30.5065

Epoch 375: val_loss did not improve from 29.72235
196/196 - 63s - loss: 29.5433 - MinusLogProbMetric: 29.5433 - val_loss: 30.5065 - val_MinusLogProbMetric: 30.5065 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 376/1000
2023-09-30 15:08:17.839 
Epoch 376/1000 
	 loss: 29.4875, MinusLogProbMetric: 29.4875, val_loss: 29.8017, val_MinusLogProbMetric: 29.8017

Epoch 376: val_loss did not improve from 29.72235
196/196 - 67s - loss: 29.4875 - MinusLogProbMetric: 29.4875 - val_loss: 29.8017 - val_MinusLogProbMetric: 29.8017 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 377/1000
2023-09-30 15:09:22.526 
Epoch 377/1000 
	 loss: 29.5161, MinusLogProbMetric: 29.5161, val_loss: 30.1081, val_MinusLogProbMetric: 30.1081

Epoch 377: val_loss did not improve from 29.72235
196/196 - 65s - loss: 29.5161 - MinusLogProbMetric: 29.5161 - val_loss: 30.1081 - val_MinusLogProbMetric: 30.1081 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 378/1000
2023-09-30 15:10:31.473 
Epoch 378/1000 
	 loss: 29.5784, MinusLogProbMetric: 29.5784, val_loss: 29.8493, val_MinusLogProbMetric: 29.8493

Epoch 378: val_loss did not improve from 29.72235
196/196 - 69s - loss: 29.5784 - MinusLogProbMetric: 29.5784 - val_loss: 29.8493 - val_MinusLogProbMetric: 29.8493 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 379/1000
2023-09-30 15:11:34.583 
Epoch 379/1000 
	 loss: 29.5461, MinusLogProbMetric: 29.5461, val_loss: 30.3338, val_MinusLogProbMetric: 30.3338

Epoch 379: val_loss did not improve from 29.72235
196/196 - 63s - loss: 29.5461 - MinusLogProbMetric: 29.5461 - val_loss: 30.3338 - val_MinusLogProbMetric: 30.3338 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 380/1000
2023-09-30 15:12:38.822 
Epoch 380/1000 
	 loss: 29.5569, MinusLogProbMetric: 29.5569, val_loss: 29.9494, val_MinusLogProbMetric: 29.9494

Epoch 380: val_loss did not improve from 29.72235
196/196 - 64s - loss: 29.5569 - MinusLogProbMetric: 29.5569 - val_loss: 29.9494 - val_MinusLogProbMetric: 29.9494 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 381/1000
2023-09-30 15:13:44.493 
Epoch 381/1000 
	 loss: 29.5205, MinusLogProbMetric: 29.5205, val_loss: 30.1933, val_MinusLogProbMetric: 30.1933

Epoch 381: val_loss did not improve from 29.72235
196/196 - 66s - loss: 29.5205 - MinusLogProbMetric: 29.5205 - val_loss: 30.1933 - val_MinusLogProbMetric: 30.1933 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 382/1000
2023-09-30 15:14:50.534 
Epoch 382/1000 
	 loss: 29.5109, MinusLogProbMetric: 29.5109, val_loss: 29.8405, val_MinusLogProbMetric: 29.8405

Epoch 382: val_loss did not improve from 29.72235
196/196 - 66s - loss: 29.5109 - MinusLogProbMetric: 29.5109 - val_loss: 29.8405 - val_MinusLogProbMetric: 29.8405 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 383/1000
2023-09-30 15:15:53.393 
Epoch 383/1000 
	 loss: 29.5008, MinusLogProbMetric: 29.5008, val_loss: 29.7624, val_MinusLogProbMetric: 29.7624

Epoch 383: val_loss did not improve from 29.72235
196/196 - 63s - loss: 29.5008 - MinusLogProbMetric: 29.5008 - val_loss: 29.7624 - val_MinusLogProbMetric: 29.7624 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 384/1000
2023-09-30 15:16:53.524 
Epoch 384/1000 
	 loss: 29.5150, MinusLogProbMetric: 29.5150, val_loss: 29.9737, val_MinusLogProbMetric: 29.9737

Epoch 384: val_loss did not improve from 29.72235
196/196 - 60s - loss: 29.5150 - MinusLogProbMetric: 29.5150 - val_loss: 29.9737 - val_MinusLogProbMetric: 29.9737 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 385/1000
2023-09-30 15:17:54.731 
Epoch 385/1000 
	 loss: 29.5213, MinusLogProbMetric: 29.5213, val_loss: 29.8305, val_MinusLogProbMetric: 29.8305

Epoch 385: val_loss did not improve from 29.72235
196/196 - 61s - loss: 29.5213 - MinusLogProbMetric: 29.5213 - val_loss: 29.8305 - val_MinusLogProbMetric: 29.8305 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 386/1000
2023-09-30 15:18:59.440 
Epoch 386/1000 
	 loss: 29.5411, MinusLogProbMetric: 29.5411, val_loss: 30.1103, val_MinusLogProbMetric: 30.1103

Epoch 386: val_loss did not improve from 29.72235
196/196 - 65s - loss: 29.5411 - MinusLogProbMetric: 29.5411 - val_loss: 30.1103 - val_MinusLogProbMetric: 30.1103 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 387/1000
2023-09-30 15:20:02.534 
Epoch 387/1000 
	 loss: 29.5029, MinusLogProbMetric: 29.5029, val_loss: 30.1071, val_MinusLogProbMetric: 30.1071

Epoch 387: val_loss did not improve from 29.72235
196/196 - 63s - loss: 29.5029 - MinusLogProbMetric: 29.5029 - val_loss: 30.1071 - val_MinusLogProbMetric: 30.1071 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 388/1000
2023-09-30 15:21:06.372 
Epoch 388/1000 
	 loss: 29.5020, MinusLogProbMetric: 29.5020, val_loss: 29.9999, val_MinusLogProbMetric: 29.9999

Epoch 388: val_loss did not improve from 29.72235
196/196 - 64s - loss: 29.5020 - MinusLogProbMetric: 29.5020 - val_loss: 29.9999 - val_MinusLogProbMetric: 29.9999 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 389/1000
2023-09-30 15:22:08.506 
Epoch 389/1000 
	 loss: 29.5220, MinusLogProbMetric: 29.5220, val_loss: 30.2307, val_MinusLogProbMetric: 30.2307

Epoch 389: val_loss did not improve from 29.72235
196/196 - 62s - loss: 29.5220 - MinusLogProbMetric: 29.5220 - val_loss: 30.2307 - val_MinusLogProbMetric: 30.2307 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 390/1000
2023-09-30 15:23:09.644 
Epoch 390/1000 
	 loss: 29.5856, MinusLogProbMetric: 29.5856, val_loss: 29.7990, val_MinusLogProbMetric: 29.7990

Epoch 390: val_loss did not improve from 29.72235
196/196 - 61s - loss: 29.5856 - MinusLogProbMetric: 29.5856 - val_loss: 29.7990 - val_MinusLogProbMetric: 29.7990 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 391/1000
2023-09-30 15:24:14.625 
Epoch 391/1000 
	 loss: 29.4987, MinusLogProbMetric: 29.4987, val_loss: 30.0203, val_MinusLogProbMetric: 30.0203

Epoch 391: val_loss did not improve from 29.72235
196/196 - 65s - loss: 29.4987 - MinusLogProbMetric: 29.4987 - val_loss: 30.0203 - val_MinusLogProbMetric: 30.0203 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 392/1000
2023-09-30 15:25:18.638 
Epoch 392/1000 
	 loss: 29.5675, MinusLogProbMetric: 29.5675, val_loss: 30.3222, val_MinusLogProbMetric: 30.3222

Epoch 392: val_loss did not improve from 29.72235
196/196 - 64s - loss: 29.5675 - MinusLogProbMetric: 29.5675 - val_loss: 30.3222 - val_MinusLogProbMetric: 30.3222 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 393/1000
2023-09-30 15:26:18.374 
Epoch 393/1000 
	 loss: 29.4475, MinusLogProbMetric: 29.4475, val_loss: 30.2416, val_MinusLogProbMetric: 30.2416

Epoch 393: val_loss did not improve from 29.72235
196/196 - 60s - loss: 29.4475 - MinusLogProbMetric: 29.4475 - val_loss: 30.2416 - val_MinusLogProbMetric: 30.2416 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 394/1000
2023-09-30 15:27:20.803 
Epoch 394/1000 
	 loss: 29.5953, MinusLogProbMetric: 29.5953, val_loss: 30.4093, val_MinusLogProbMetric: 30.4093

Epoch 394: val_loss did not improve from 29.72235
196/196 - 62s - loss: 29.5953 - MinusLogProbMetric: 29.5953 - val_loss: 30.4093 - val_MinusLogProbMetric: 30.4093 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 395/1000
2023-09-30 15:28:30.743 
Epoch 395/1000 
	 loss: 29.5595, MinusLogProbMetric: 29.5595, val_loss: 29.9194, val_MinusLogProbMetric: 29.9194

Epoch 395: val_loss did not improve from 29.72235
196/196 - 70s - loss: 29.5595 - MinusLogProbMetric: 29.5595 - val_loss: 29.9194 - val_MinusLogProbMetric: 29.9194 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 396/1000
2023-09-30 15:29:34.574 
Epoch 396/1000 
	 loss: 29.5157, MinusLogProbMetric: 29.5157, val_loss: 30.0805, val_MinusLogProbMetric: 30.0805

Epoch 396: val_loss did not improve from 29.72235
196/196 - 64s - loss: 29.5157 - MinusLogProbMetric: 29.5157 - val_loss: 30.0805 - val_MinusLogProbMetric: 30.0805 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 397/1000
2023-09-30 15:30:35.488 
Epoch 397/1000 
	 loss: 29.5144, MinusLogProbMetric: 29.5144, val_loss: 29.8077, val_MinusLogProbMetric: 29.8077

Epoch 397: val_loss did not improve from 29.72235
196/196 - 61s - loss: 29.5144 - MinusLogProbMetric: 29.5144 - val_loss: 29.8077 - val_MinusLogProbMetric: 29.8077 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 398/1000
2023-09-30 15:31:34.874 
Epoch 398/1000 
	 loss: 29.5599, MinusLogProbMetric: 29.5599, val_loss: 29.8275, val_MinusLogProbMetric: 29.8275

Epoch 398: val_loss did not improve from 29.72235
196/196 - 59s - loss: 29.5599 - MinusLogProbMetric: 29.5599 - val_loss: 29.8275 - val_MinusLogProbMetric: 29.8275 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 399/1000
2023-09-30 15:32:37.386 
Epoch 399/1000 
	 loss: 29.4650, MinusLogProbMetric: 29.4650, val_loss: 29.7348, val_MinusLogProbMetric: 29.7348

Epoch 399: val_loss did not improve from 29.72235
196/196 - 63s - loss: 29.4650 - MinusLogProbMetric: 29.4650 - val_loss: 29.7348 - val_MinusLogProbMetric: 29.7348 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 400/1000
2023-09-30 15:33:39.052 
Epoch 400/1000 
	 loss: 29.4742, MinusLogProbMetric: 29.4742, val_loss: 29.8584, val_MinusLogProbMetric: 29.8584

Epoch 400: val_loss did not improve from 29.72235
196/196 - 62s - loss: 29.4742 - MinusLogProbMetric: 29.4742 - val_loss: 29.8584 - val_MinusLogProbMetric: 29.8584 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 401/1000
2023-09-30 15:34:39.615 
Epoch 401/1000 
	 loss: 29.5200, MinusLogProbMetric: 29.5200, val_loss: 29.8365, val_MinusLogProbMetric: 29.8365

Epoch 401: val_loss did not improve from 29.72235
196/196 - 61s - loss: 29.5200 - MinusLogProbMetric: 29.5200 - val_loss: 29.8365 - val_MinusLogProbMetric: 29.8365 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 402/1000
2023-09-30 15:35:41.268 
Epoch 402/1000 
	 loss: 29.4999, MinusLogProbMetric: 29.4999, val_loss: 30.1564, val_MinusLogProbMetric: 30.1564

Epoch 402: val_loss did not improve from 29.72235
196/196 - 62s - loss: 29.4999 - MinusLogProbMetric: 29.4999 - val_loss: 30.1564 - val_MinusLogProbMetric: 30.1564 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 403/1000
2023-09-30 15:36:43.979 
Epoch 403/1000 
	 loss: 29.4705, MinusLogProbMetric: 29.4705, val_loss: 30.8523, val_MinusLogProbMetric: 30.8523

Epoch 403: val_loss did not improve from 29.72235
196/196 - 63s - loss: 29.4705 - MinusLogProbMetric: 29.4705 - val_loss: 30.8523 - val_MinusLogProbMetric: 30.8523 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 404/1000
2023-09-30 15:37:49.281 
Epoch 404/1000 
	 loss: 29.5050, MinusLogProbMetric: 29.5050, val_loss: 29.7112, val_MinusLogProbMetric: 29.7112

Epoch 404: val_loss improved from 29.72235 to 29.71120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 67s - loss: 29.5050 - MinusLogProbMetric: 29.5050 - val_loss: 29.7112 - val_MinusLogProbMetric: 29.7112 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 405/1000
2023-09-30 15:38:51.488 
Epoch 405/1000 
	 loss: 29.4617, MinusLogProbMetric: 29.4617, val_loss: 30.3660, val_MinusLogProbMetric: 30.3660

Epoch 405: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.4617 - MinusLogProbMetric: 29.4617 - val_loss: 30.3660 - val_MinusLogProbMetric: 30.3660 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 406/1000
2023-09-30 15:39:54.053 
Epoch 406/1000 
	 loss: 29.4448, MinusLogProbMetric: 29.4448, val_loss: 29.8452, val_MinusLogProbMetric: 29.8452

Epoch 406: val_loss did not improve from 29.71120
196/196 - 63s - loss: 29.4448 - MinusLogProbMetric: 29.4448 - val_loss: 29.8452 - val_MinusLogProbMetric: 29.8452 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 407/1000
2023-09-30 15:40:55.438 
Epoch 407/1000 
	 loss: 29.5282, MinusLogProbMetric: 29.5282, val_loss: 29.7719, val_MinusLogProbMetric: 29.7719

Epoch 407: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.5282 - MinusLogProbMetric: 29.5282 - val_loss: 29.7719 - val_MinusLogProbMetric: 29.7719 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 408/1000
2023-09-30 15:41:57.508 
Epoch 408/1000 
	 loss: 29.5016, MinusLogProbMetric: 29.5016, val_loss: 29.8978, val_MinusLogProbMetric: 29.8978

Epoch 408: val_loss did not improve from 29.71120
196/196 - 62s - loss: 29.5016 - MinusLogProbMetric: 29.5016 - val_loss: 29.8978 - val_MinusLogProbMetric: 29.8978 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 409/1000
2023-09-30 15:42:57.915 
Epoch 409/1000 
	 loss: 29.4887, MinusLogProbMetric: 29.4887, val_loss: 29.9341, val_MinusLogProbMetric: 29.9341

Epoch 409: val_loss did not improve from 29.71120
196/196 - 60s - loss: 29.4887 - MinusLogProbMetric: 29.4887 - val_loss: 29.9341 - val_MinusLogProbMetric: 29.9341 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 410/1000
2023-09-30 15:44:00.250 
Epoch 410/1000 
	 loss: 29.4635, MinusLogProbMetric: 29.4635, val_loss: 29.8553, val_MinusLogProbMetric: 29.8553

Epoch 410: val_loss did not improve from 29.71120
196/196 - 62s - loss: 29.4635 - MinusLogProbMetric: 29.4635 - val_loss: 29.8553 - val_MinusLogProbMetric: 29.8553 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 411/1000
2023-09-30 15:45:02.415 
Epoch 411/1000 
	 loss: 29.5180, MinusLogProbMetric: 29.5180, val_loss: 29.8074, val_MinusLogProbMetric: 29.8074

Epoch 411: val_loss did not improve from 29.71120
196/196 - 62s - loss: 29.5180 - MinusLogProbMetric: 29.5180 - val_loss: 29.8074 - val_MinusLogProbMetric: 29.8074 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 412/1000
2023-09-30 15:46:01.469 
Epoch 412/1000 
	 loss: 29.3857, MinusLogProbMetric: 29.3857, val_loss: 29.9412, val_MinusLogProbMetric: 29.9412

Epoch 412: val_loss did not improve from 29.71120
196/196 - 59s - loss: 29.3857 - MinusLogProbMetric: 29.3857 - val_loss: 29.9412 - val_MinusLogProbMetric: 29.9412 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 413/1000
2023-09-30 15:47:04.089 
Epoch 413/1000 
	 loss: 29.5411, MinusLogProbMetric: 29.5411, val_loss: 29.9402, val_MinusLogProbMetric: 29.9402

Epoch 413: val_loss did not improve from 29.71120
196/196 - 63s - loss: 29.5411 - MinusLogProbMetric: 29.5411 - val_loss: 29.9402 - val_MinusLogProbMetric: 29.9402 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 414/1000
2023-09-30 15:48:05.585 
Epoch 414/1000 
	 loss: 29.4430, MinusLogProbMetric: 29.4430, val_loss: 29.9522, val_MinusLogProbMetric: 29.9522

Epoch 414: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.4430 - MinusLogProbMetric: 29.4430 - val_loss: 29.9522 - val_MinusLogProbMetric: 29.9522 - lr: 1.1111e-04 - 61s/epoch - 314ms/step
Epoch 415/1000
2023-09-30 15:49:04.920 
Epoch 415/1000 
	 loss: 29.4533, MinusLogProbMetric: 29.4533, val_loss: 29.8397, val_MinusLogProbMetric: 29.8397

Epoch 415: val_loss did not improve from 29.71120
196/196 - 59s - loss: 29.4533 - MinusLogProbMetric: 29.4533 - val_loss: 29.8397 - val_MinusLogProbMetric: 29.8397 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 416/1000
2023-09-30 15:50:06.730 
Epoch 416/1000 
	 loss: 29.4068, MinusLogProbMetric: 29.4068, val_loss: 29.7198, val_MinusLogProbMetric: 29.7198

Epoch 416: val_loss did not improve from 29.71120
196/196 - 62s - loss: 29.4068 - MinusLogProbMetric: 29.4068 - val_loss: 29.7198 - val_MinusLogProbMetric: 29.7198 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 417/1000
2023-09-30 15:51:06.569 
Epoch 417/1000 
	 loss: 29.5313, MinusLogProbMetric: 29.5313, val_loss: 30.2572, val_MinusLogProbMetric: 30.2572

Epoch 417: val_loss did not improve from 29.71120
196/196 - 60s - loss: 29.5313 - MinusLogProbMetric: 29.5313 - val_loss: 30.2572 - val_MinusLogProbMetric: 30.2572 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 418/1000
2023-09-30 15:52:07.850 
Epoch 418/1000 
	 loss: 29.4263, MinusLogProbMetric: 29.4263, val_loss: 29.9912, val_MinusLogProbMetric: 29.9912

Epoch 418: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.4263 - MinusLogProbMetric: 29.4263 - val_loss: 29.9912 - val_MinusLogProbMetric: 29.9912 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 419/1000
2023-09-30 15:53:10.748 
Epoch 419/1000 
	 loss: 29.4833, MinusLogProbMetric: 29.4833, val_loss: 29.9775, val_MinusLogProbMetric: 29.9775

Epoch 419: val_loss did not improve from 29.71120
196/196 - 63s - loss: 29.4833 - MinusLogProbMetric: 29.4833 - val_loss: 29.9775 - val_MinusLogProbMetric: 29.9775 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 420/1000
2023-09-30 15:54:12.232 
Epoch 420/1000 
	 loss: 29.4293, MinusLogProbMetric: 29.4293, val_loss: 29.9111, val_MinusLogProbMetric: 29.9111

Epoch 420: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.4293 - MinusLogProbMetric: 29.4293 - val_loss: 29.9111 - val_MinusLogProbMetric: 29.9111 - lr: 1.1111e-04 - 61s/epoch - 314ms/step
Epoch 421/1000
2023-09-30 15:55:15.298 
Epoch 421/1000 
	 loss: 29.4614, MinusLogProbMetric: 29.4614, val_loss: 29.9182, val_MinusLogProbMetric: 29.9182

Epoch 421: val_loss did not improve from 29.71120
196/196 - 63s - loss: 29.4614 - MinusLogProbMetric: 29.4614 - val_loss: 29.9182 - val_MinusLogProbMetric: 29.9182 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 422/1000
2023-09-30 15:56:16.867 
Epoch 422/1000 
	 loss: 29.5603, MinusLogProbMetric: 29.5603, val_loss: 30.0309, val_MinusLogProbMetric: 30.0309

Epoch 422: val_loss did not improve from 29.71120
196/196 - 62s - loss: 29.5603 - MinusLogProbMetric: 29.5603 - val_loss: 30.0309 - val_MinusLogProbMetric: 30.0309 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 423/1000
2023-09-30 15:57:20.281 
Epoch 423/1000 
	 loss: 29.4255, MinusLogProbMetric: 29.4255, val_loss: 30.0205, val_MinusLogProbMetric: 30.0205

Epoch 423: val_loss did not improve from 29.71120
196/196 - 63s - loss: 29.4255 - MinusLogProbMetric: 29.4255 - val_loss: 30.0205 - val_MinusLogProbMetric: 30.0205 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 424/1000
2023-09-30 15:58:23.677 
Epoch 424/1000 
	 loss: 29.4768, MinusLogProbMetric: 29.4768, val_loss: 30.1713, val_MinusLogProbMetric: 30.1713

Epoch 424: val_loss did not improve from 29.71120
196/196 - 63s - loss: 29.4768 - MinusLogProbMetric: 29.4768 - val_loss: 30.1713 - val_MinusLogProbMetric: 30.1713 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 425/1000
2023-09-30 15:59:24.637 
Epoch 425/1000 
	 loss: 29.4097, MinusLogProbMetric: 29.4097, val_loss: 31.8309, val_MinusLogProbMetric: 31.8309

Epoch 425: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.4097 - MinusLogProbMetric: 29.4097 - val_loss: 31.8309 - val_MinusLogProbMetric: 31.8309 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 426/1000
2023-09-30 16:00:24.032 
Epoch 426/1000 
	 loss: 29.4142, MinusLogProbMetric: 29.4142, val_loss: 29.7949, val_MinusLogProbMetric: 29.7949

Epoch 426: val_loss did not improve from 29.71120
196/196 - 59s - loss: 29.4142 - MinusLogProbMetric: 29.4142 - val_loss: 29.7949 - val_MinusLogProbMetric: 29.7949 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 427/1000
2023-09-30 16:01:23.753 
Epoch 427/1000 
	 loss: 29.4520, MinusLogProbMetric: 29.4520, val_loss: 29.8321, val_MinusLogProbMetric: 29.8321

Epoch 427: val_loss did not improve from 29.71120
196/196 - 60s - loss: 29.4520 - MinusLogProbMetric: 29.4520 - val_loss: 29.8321 - val_MinusLogProbMetric: 29.8321 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 428/1000
2023-09-30 16:02:21.038 
Epoch 428/1000 
	 loss: 29.4273, MinusLogProbMetric: 29.4273, val_loss: 29.7532, val_MinusLogProbMetric: 29.7532

Epoch 428: val_loss did not improve from 29.71120
196/196 - 57s - loss: 29.4273 - MinusLogProbMetric: 29.4273 - val_loss: 29.7532 - val_MinusLogProbMetric: 29.7532 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 429/1000
2023-09-30 16:03:21.688 
Epoch 429/1000 
	 loss: 29.4898, MinusLogProbMetric: 29.4898, val_loss: 30.4659, val_MinusLogProbMetric: 30.4659

Epoch 429: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.4898 - MinusLogProbMetric: 29.4898 - val_loss: 30.4659 - val_MinusLogProbMetric: 30.4659 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 430/1000
2023-09-30 16:04:22.192 
Epoch 430/1000 
	 loss: 29.4149, MinusLogProbMetric: 29.4149, val_loss: 29.8717, val_MinusLogProbMetric: 29.8717

Epoch 430: val_loss did not improve from 29.71120
196/196 - 60s - loss: 29.4149 - MinusLogProbMetric: 29.4149 - val_loss: 29.8717 - val_MinusLogProbMetric: 29.8717 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 431/1000
2023-09-30 16:05:21.752 
Epoch 431/1000 
	 loss: 29.3814, MinusLogProbMetric: 29.3814, val_loss: 30.0020, val_MinusLogProbMetric: 30.0020

Epoch 431: val_loss did not improve from 29.71120
196/196 - 60s - loss: 29.3814 - MinusLogProbMetric: 29.3814 - val_loss: 30.0020 - val_MinusLogProbMetric: 30.0020 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 432/1000
2023-09-30 16:06:24.434 
Epoch 432/1000 
	 loss: 29.4692, MinusLogProbMetric: 29.4692, val_loss: 29.7754, val_MinusLogProbMetric: 29.7754

Epoch 432: val_loss did not improve from 29.71120
196/196 - 63s - loss: 29.4692 - MinusLogProbMetric: 29.4692 - val_loss: 29.7754 - val_MinusLogProbMetric: 29.7754 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 433/1000
2023-09-30 16:07:28.445 
Epoch 433/1000 
	 loss: 29.4325, MinusLogProbMetric: 29.4325, val_loss: 29.9366, val_MinusLogProbMetric: 29.9366

Epoch 433: val_loss did not improve from 29.71120
196/196 - 64s - loss: 29.4325 - MinusLogProbMetric: 29.4325 - val_loss: 29.9366 - val_MinusLogProbMetric: 29.9366 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 434/1000
2023-09-30 16:08:28.234 
Epoch 434/1000 
	 loss: 29.4073, MinusLogProbMetric: 29.4073, val_loss: 30.0498, val_MinusLogProbMetric: 30.0498

Epoch 434: val_loss did not improve from 29.71120
196/196 - 60s - loss: 29.4073 - MinusLogProbMetric: 29.4073 - val_loss: 30.0498 - val_MinusLogProbMetric: 30.0498 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 435/1000
2023-09-30 16:09:27.057 
Epoch 435/1000 
	 loss: 29.4268, MinusLogProbMetric: 29.4268, val_loss: 30.4396, val_MinusLogProbMetric: 30.4396

Epoch 435: val_loss did not improve from 29.71120
196/196 - 59s - loss: 29.4268 - MinusLogProbMetric: 29.4268 - val_loss: 30.4396 - val_MinusLogProbMetric: 30.4396 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 436/1000
2023-09-30 16:10:25.629 
Epoch 436/1000 
	 loss: 29.3977, MinusLogProbMetric: 29.3977, val_loss: 29.8460, val_MinusLogProbMetric: 29.8460

Epoch 436: val_loss did not improve from 29.71120
196/196 - 59s - loss: 29.3977 - MinusLogProbMetric: 29.3977 - val_loss: 29.8460 - val_MinusLogProbMetric: 29.8460 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 437/1000
2023-09-30 16:11:24.565 
Epoch 437/1000 
	 loss: 29.3939, MinusLogProbMetric: 29.3939, val_loss: 30.1532, val_MinusLogProbMetric: 30.1532

Epoch 437: val_loss did not improve from 29.71120
196/196 - 59s - loss: 29.3939 - MinusLogProbMetric: 29.3939 - val_loss: 30.1532 - val_MinusLogProbMetric: 30.1532 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 438/1000
2023-09-30 16:12:25.796 
Epoch 438/1000 
	 loss: 29.4298, MinusLogProbMetric: 29.4298, val_loss: 29.7696, val_MinusLogProbMetric: 29.7696

Epoch 438: val_loss did not improve from 29.71120
196/196 - 61s - loss: 29.4298 - MinusLogProbMetric: 29.4298 - val_loss: 29.7696 - val_MinusLogProbMetric: 29.7696 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 439/1000
2023-09-30 16:13:29.520 
Epoch 439/1000 
	 loss: 29.4008, MinusLogProbMetric: 29.4008, val_loss: 29.6654, val_MinusLogProbMetric: 29.6654

Epoch 439: val_loss improved from 29.71120 to 29.66544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 65s - loss: 29.4008 - MinusLogProbMetric: 29.4008 - val_loss: 29.6654 - val_MinusLogProbMetric: 29.6654 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 440/1000
2023-09-30 16:14:30.035 
Epoch 440/1000 
	 loss: 29.4203, MinusLogProbMetric: 29.4203, val_loss: 29.7611, val_MinusLogProbMetric: 29.7611

Epoch 440: val_loss did not improve from 29.66544
196/196 - 59s - loss: 29.4203 - MinusLogProbMetric: 29.4203 - val_loss: 29.7611 - val_MinusLogProbMetric: 29.7611 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 441/1000
2023-09-30 16:15:32.623 
Epoch 441/1000 
	 loss: 29.4691, MinusLogProbMetric: 29.4691, val_loss: 29.7971, val_MinusLogProbMetric: 29.7971

Epoch 441: val_loss did not improve from 29.66544
196/196 - 63s - loss: 29.4691 - MinusLogProbMetric: 29.4691 - val_loss: 29.7971 - val_MinusLogProbMetric: 29.7971 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 442/1000
2023-09-30 16:16:32.112 
Epoch 442/1000 
	 loss: 29.3784, MinusLogProbMetric: 29.3784, val_loss: 29.6248, val_MinusLogProbMetric: 29.6248

Epoch 442: val_loss improved from 29.66544 to 29.62484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 61s - loss: 29.3784 - MinusLogProbMetric: 29.3784 - val_loss: 29.6248 - val_MinusLogProbMetric: 29.6248 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 443/1000
2023-09-30 16:17:32.117 
Epoch 443/1000 
	 loss: 29.3958, MinusLogProbMetric: 29.3958, val_loss: 29.7692, val_MinusLogProbMetric: 29.7692

Epoch 443: val_loss did not improve from 29.62484
196/196 - 58s - loss: 29.3958 - MinusLogProbMetric: 29.3958 - val_loss: 29.7692 - val_MinusLogProbMetric: 29.7692 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 444/1000
2023-09-30 16:18:32.186 
Epoch 444/1000 
	 loss: 29.3486, MinusLogProbMetric: 29.3486, val_loss: 29.8797, val_MinusLogProbMetric: 29.8797

Epoch 444: val_loss did not improve from 29.62484
196/196 - 60s - loss: 29.3486 - MinusLogProbMetric: 29.3486 - val_loss: 29.8797 - val_MinusLogProbMetric: 29.8797 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 445/1000
2023-09-30 16:19:31.916 
Epoch 445/1000 
	 loss: 29.4769, MinusLogProbMetric: 29.4769, val_loss: 29.7430, val_MinusLogProbMetric: 29.7430

Epoch 445: val_loss did not improve from 29.62484
196/196 - 60s - loss: 29.4769 - MinusLogProbMetric: 29.4769 - val_loss: 29.7430 - val_MinusLogProbMetric: 29.7430 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 446/1000
2023-09-30 16:20:32.388 
Epoch 446/1000 
	 loss: 29.3730, MinusLogProbMetric: 29.3730, val_loss: 30.0814, val_MinusLogProbMetric: 30.0814

Epoch 446: val_loss did not improve from 29.62484
196/196 - 60s - loss: 29.3730 - MinusLogProbMetric: 29.3730 - val_loss: 30.0814 - val_MinusLogProbMetric: 30.0814 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 447/1000
2023-09-30 16:21:32.311 
Epoch 447/1000 
	 loss: 29.3906, MinusLogProbMetric: 29.3906, val_loss: 29.8464, val_MinusLogProbMetric: 29.8464

Epoch 447: val_loss did not improve from 29.62484
196/196 - 60s - loss: 29.3906 - MinusLogProbMetric: 29.3906 - val_loss: 29.8464 - val_MinusLogProbMetric: 29.8464 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 448/1000
2023-09-30 16:22:31.466 
Epoch 448/1000 
	 loss: 29.4146, MinusLogProbMetric: 29.4146, val_loss: 30.1356, val_MinusLogProbMetric: 30.1356

Epoch 448: val_loss did not improve from 29.62484
196/196 - 59s - loss: 29.4146 - MinusLogProbMetric: 29.4146 - val_loss: 30.1356 - val_MinusLogProbMetric: 30.1356 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 449/1000
2023-09-30 16:23:32.573 
Epoch 449/1000 
	 loss: 29.4682, MinusLogProbMetric: 29.4682, val_loss: 29.9970, val_MinusLogProbMetric: 29.9970

Epoch 449: val_loss did not improve from 29.62484
196/196 - 61s - loss: 29.4682 - MinusLogProbMetric: 29.4682 - val_loss: 29.9970 - val_MinusLogProbMetric: 29.9970 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 450/1000
2023-09-30 16:24:36.174 
Epoch 450/1000 
	 loss: 29.3398, MinusLogProbMetric: 29.3398, val_loss: 29.9060, val_MinusLogProbMetric: 29.9060

Epoch 450: val_loss did not improve from 29.62484
196/196 - 64s - loss: 29.3398 - MinusLogProbMetric: 29.3398 - val_loss: 29.9060 - val_MinusLogProbMetric: 29.9060 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 451/1000
2023-09-30 16:25:36.797 
Epoch 451/1000 
	 loss: 29.4572, MinusLogProbMetric: 29.4572, val_loss: 29.8499, val_MinusLogProbMetric: 29.8499

Epoch 451: val_loss did not improve from 29.62484
196/196 - 61s - loss: 29.4572 - MinusLogProbMetric: 29.4572 - val_loss: 29.8499 - val_MinusLogProbMetric: 29.8499 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 452/1000
2023-09-30 16:26:36.375 
Epoch 452/1000 
	 loss: 29.4114, MinusLogProbMetric: 29.4114, val_loss: 29.6563, val_MinusLogProbMetric: 29.6563

Epoch 452: val_loss did not improve from 29.62484
196/196 - 60s - loss: 29.4114 - MinusLogProbMetric: 29.4114 - val_loss: 29.6563 - val_MinusLogProbMetric: 29.6563 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 453/1000
2023-09-30 16:27:40.697 
Epoch 453/1000 
	 loss: 29.3934, MinusLogProbMetric: 29.3934, val_loss: 29.8290, val_MinusLogProbMetric: 29.8290

Epoch 453: val_loss did not improve from 29.62484
196/196 - 64s - loss: 29.3934 - MinusLogProbMetric: 29.3934 - val_loss: 29.8290 - val_MinusLogProbMetric: 29.8290 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 454/1000
2023-09-30 16:28:40.236 
Epoch 454/1000 
	 loss: 29.3696, MinusLogProbMetric: 29.3696, val_loss: 29.8586, val_MinusLogProbMetric: 29.8586

Epoch 454: val_loss did not improve from 29.62484
196/196 - 60s - loss: 29.3696 - MinusLogProbMetric: 29.3696 - val_loss: 29.8586 - val_MinusLogProbMetric: 29.8586 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 455/1000
2023-09-30 16:29:40.871 
Epoch 455/1000 
	 loss: 29.4231, MinusLogProbMetric: 29.4231, val_loss: 29.8425, val_MinusLogProbMetric: 29.8425

Epoch 455: val_loss did not improve from 29.62484
196/196 - 61s - loss: 29.4231 - MinusLogProbMetric: 29.4231 - val_loss: 29.8425 - val_MinusLogProbMetric: 29.8425 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 456/1000
2023-09-30 16:30:42.368 
Epoch 456/1000 
	 loss: 29.3657, MinusLogProbMetric: 29.3657, val_loss: 29.9591, val_MinusLogProbMetric: 29.9591

Epoch 456: val_loss did not improve from 29.62484
196/196 - 61s - loss: 29.3657 - MinusLogProbMetric: 29.3657 - val_loss: 29.9591 - val_MinusLogProbMetric: 29.9591 - lr: 1.1111e-04 - 61s/epoch - 314ms/step
Epoch 457/1000
2023-09-30 16:31:44.537 
Epoch 457/1000 
	 loss: 29.3638, MinusLogProbMetric: 29.3638, val_loss: 30.1283, val_MinusLogProbMetric: 30.1283

Epoch 457: val_loss did not improve from 29.62484
196/196 - 62s - loss: 29.3638 - MinusLogProbMetric: 29.3638 - val_loss: 30.1283 - val_MinusLogProbMetric: 30.1283 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 458/1000
2023-09-30 16:32:47.998 
Epoch 458/1000 
	 loss: 29.4379, MinusLogProbMetric: 29.4379, val_loss: 29.8485, val_MinusLogProbMetric: 29.8485

Epoch 458: val_loss did not improve from 29.62484
196/196 - 63s - loss: 29.4379 - MinusLogProbMetric: 29.4379 - val_loss: 29.8485 - val_MinusLogProbMetric: 29.8485 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 459/1000
2023-09-30 16:33:50.788 
Epoch 459/1000 
	 loss: 29.3748, MinusLogProbMetric: 29.3748, val_loss: 30.4742, val_MinusLogProbMetric: 30.4742

Epoch 459: val_loss did not improve from 29.62484
196/196 - 63s - loss: 29.3748 - MinusLogProbMetric: 29.3748 - val_loss: 30.4742 - val_MinusLogProbMetric: 30.4742 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 460/1000
2023-09-30 16:34:52.621 
Epoch 460/1000 
	 loss: 29.3761, MinusLogProbMetric: 29.3761, val_loss: 29.7236, val_MinusLogProbMetric: 29.7236

Epoch 460: val_loss did not improve from 29.62484
196/196 - 62s - loss: 29.3761 - MinusLogProbMetric: 29.3761 - val_loss: 29.7236 - val_MinusLogProbMetric: 29.7236 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 461/1000
2023-09-30 16:35:54.886 
Epoch 461/1000 
	 loss: 29.4109, MinusLogProbMetric: 29.4109, val_loss: 30.3567, val_MinusLogProbMetric: 30.3567

Epoch 461: val_loss did not improve from 29.62484
196/196 - 62s - loss: 29.4109 - MinusLogProbMetric: 29.4109 - val_loss: 30.3567 - val_MinusLogProbMetric: 30.3567 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 462/1000
2023-09-30 16:36:57.845 
Epoch 462/1000 
	 loss: 29.3964, MinusLogProbMetric: 29.3964, val_loss: 29.7862, val_MinusLogProbMetric: 29.7862

Epoch 462: val_loss did not improve from 29.62484
196/196 - 63s - loss: 29.3964 - MinusLogProbMetric: 29.3964 - val_loss: 29.7862 - val_MinusLogProbMetric: 29.7862 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 463/1000
2023-09-30 16:37:59.729 
Epoch 463/1000 
	 loss: 29.4209, MinusLogProbMetric: 29.4209, val_loss: 30.3670, val_MinusLogProbMetric: 30.3670

Epoch 463: val_loss did not improve from 29.62484
196/196 - 62s - loss: 29.4209 - MinusLogProbMetric: 29.4209 - val_loss: 30.3670 - val_MinusLogProbMetric: 30.3670 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 464/1000
2023-09-30 16:39:04.813 
Epoch 464/1000 
	 loss: 29.3510, MinusLogProbMetric: 29.3510, val_loss: 29.8539, val_MinusLogProbMetric: 29.8539

Epoch 464: val_loss did not improve from 29.62484
196/196 - 65s - loss: 29.3510 - MinusLogProbMetric: 29.3510 - val_loss: 29.8539 - val_MinusLogProbMetric: 29.8539 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 465/1000
2023-09-30 16:40:08.305 
Epoch 465/1000 
	 loss: 29.3486, MinusLogProbMetric: 29.3486, val_loss: 29.8523, val_MinusLogProbMetric: 29.8523

Epoch 465: val_loss did not improve from 29.62484
196/196 - 63s - loss: 29.3486 - MinusLogProbMetric: 29.3486 - val_loss: 29.8523 - val_MinusLogProbMetric: 29.8523 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 466/1000
2023-09-30 16:41:10.612 
Epoch 466/1000 
	 loss: 29.4136, MinusLogProbMetric: 29.4136, val_loss: 29.7589, val_MinusLogProbMetric: 29.7589

Epoch 466: val_loss did not improve from 29.62484
196/196 - 62s - loss: 29.4136 - MinusLogProbMetric: 29.4136 - val_loss: 29.7589 - val_MinusLogProbMetric: 29.7589 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 467/1000
2023-09-30 16:42:13.295 
Epoch 467/1000 
	 loss: 29.3628, MinusLogProbMetric: 29.3628, val_loss: 29.6519, val_MinusLogProbMetric: 29.6519

Epoch 467: val_loss did not improve from 29.62484
196/196 - 63s - loss: 29.3628 - MinusLogProbMetric: 29.3628 - val_loss: 29.6519 - val_MinusLogProbMetric: 29.6519 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 468/1000
2023-09-30 16:43:15.007 
Epoch 468/1000 
	 loss: 29.3383, MinusLogProbMetric: 29.3383, val_loss: 29.9221, val_MinusLogProbMetric: 29.9221

Epoch 468: val_loss did not improve from 29.62484
196/196 - 62s - loss: 29.3383 - MinusLogProbMetric: 29.3383 - val_loss: 29.9221 - val_MinusLogProbMetric: 29.9221 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 469/1000
2023-09-30 16:44:16.581 
Epoch 469/1000 
	 loss: 29.4087, MinusLogProbMetric: 29.4087, val_loss: 30.2490, val_MinusLogProbMetric: 30.2490

Epoch 469: val_loss did not improve from 29.62484
196/196 - 62s - loss: 29.4087 - MinusLogProbMetric: 29.4087 - val_loss: 30.2490 - val_MinusLogProbMetric: 30.2490 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 470/1000
2023-09-30 16:45:19.974 
Epoch 470/1000 
	 loss: 29.3471, MinusLogProbMetric: 29.3471, val_loss: 30.1276, val_MinusLogProbMetric: 30.1276

Epoch 470: val_loss did not improve from 29.62484
196/196 - 63s - loss: 29.3471 - MinusLogProbMetric: 29.3471 - val_loss: 30.1276 - val_MinusLogProbMetric: 30.1276 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 471/1000
2023-09-30 16:46:22.994 
Epoch 471/1000 
	 loss: 29.3146, MinusLogProbMetric: 29.3146, val_loss: 29.7139, val_MinusLogProbMetric: 29.7139

Epoch 471: val_loss did not improve from 29.62484
196/196 - 63s - loss: 29.3146 - MinusLogProbMetric: 29.3146 - val_loss: 29.7139 - val_MinusLogProbMetric: 29.7139 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 472/1000
2023-09-30 16:47:28.112 
Epoch 472/1000 
	 loss: 29.3355, MinusLogProbMetric: 29.3355, val_loss: 29.8807, val_MinusLogProbMetric: 29.8807

Epoch 472: val_loss did not improve from 29.62484
196/196 - 65s - loss: 29.3355 - MinusLogProbMetric: 29.3355 - val_loss: 29.8807 - val_MinusLogProbMetric: 29.8807 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 473/1000
2023-09-30 16:48:32.898 
Epoch 473/1000 
	 loss: 29.4061, MinusLogProbMetric: 29.4061, val_loss: 30.0071, val_MinusLogProbMetric: 30.0071

Epoch 473: val_loss did not improve from 29.62484
196/196 - 65s - loss: 29.4061 - MinusLogProbMetric: 29.4061 - val_loss: 30.0071 - val_MinusLogProbMetric: 30.0071 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 474/1000
2023-09-30 16:49:42.742 
Epoch 474/1000 
	 loss: 29.3677, MinusLogProbMetric: 29.3677, val_loss: 29.9059, val_MinusLogProbMetric: 29.9059

Epoch 474: val_loss did not improve from 29.62484
196/196 - 70s - loss: 29.3677 - MinusLogProbMetric: 29.3677 - val_loss: 29.9059 - val_MinusLogProbMetric: 29.9059 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 475/1000
2023-09-30 16:50:46.271 
Epoch 475/1000 
	 loss: 29.3817, MinusLogProbMetric: 29.3817, val_loss: 30.5715, val_MinusLogProbMetric: 30.5715

Epoch 475: val_loss did not improve from 29.62484
196/196 - 64s - loss: 29.3817 - MinusLogProbMetric: 29.3817 - val_loss: 30.5715 - val_MinusLogProbMetric: 30.5715 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 476/1000
2023-09-30 16:51:52.977 
Epoch 476/1000 
	 loss: 29.3424, MinusLogProbMetric: 29.3424, val_loss: 29.7144, val_MinusLogProbMetric: 29.7144

Epoch 476: val_loss did not improve from 29.62484
196/196 - 67s - loss: 29.3424 - MinusLogProbMetric: 29.3424 - val_loss: 29.7144 - val_MinusLogProbMetric: 29.7144 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 477/1000
2023-09-30 16:52:59.311 
Epoch 477/1000 
	 loss: 29.3372, MinusLogProbMetric: 29.3372, val_loss: 30.5157, val_MinusLogProbMetric: 30.5157

Epoch 477: val_loss did not improve from 29.62484
196/196 - 66s - loss: 29.3372 - MinusLogProbMetric: 29.3372 - val_loss: 30.5157 - val_MinusLogProbMetric: 30.5157 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 478/1000
2023-09-30 16:54:08.632 
Epoch 478/1000 
	 loss: 29.4347, MinusLogProbMetric: 29.4347, val_loss: 29.9788, val_MinusLogProbMetric: 29.9788

Epoch 478: val_loss did not improve from 29.62484
196/196 - 69s - loss: 29.4347 - MinusLogProbMetric: 29.4347 - val_loss: 29.9788 - val_MinusLogProbMetric: 29.9788 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 479/1000
2023-09-30 16:55:17.553 
Epoch 479/1000 
	 loss: 29.3392, MinusLogProbMetric: 29.3392, val_loss: 29.6191, val_MinusLogProbMetric: 29.6191

Epoch 479: val_loss improved from 29.62484 to 29.61912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 71s - loss: 29.3392 - MinusLogProbMetric: 29.3392 - val_loss: 29.6191 - val_MinusLogProbMetric: 29.6191 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 480/1000
2023-09-30 16:56:29.015 
Epoch 480/1000 
	 loss: 29.3332, MinusLogProbMetric: 29.3332, val_loss: 30.1928, val_MinusLogProbMetric: 30.1928

Epoch 480: val_loss did not improve from 29.61912
196/196 - 70s - loss: 29.3332 - MinusLogProbMetric: 29.3332 - val_loss: 30.1928 - val_MinusLogProbMetric: 30.1928 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 481/1000
2023-09-30 16:57:34.478 
Epoch 481/1000 
	 loss: 29.3977, MinusLogProbMetric: 29.3977, val_loss: 29.9202, val_MinusLogProbMetric: 29.9202

Epoch 481: val_loss did not improve from 29.61912
196/196 - 65s - loss: 29.3977 - MinusLogProbMetric: 29.3977 - val_loss: 29.9202 - val_MinusLogProbMetric: 29.9202 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 482/1000
2023-09-30 16:58:39.822 
Epoch 482/1000 
	 loss: 29.3207, MinusLogProbMetric: 29.3207, val_loss: 29.7643, val_MinusLogProbMetric: 29.7643

Epoch 482: val_loss did not improve from 29.61912
196/196 - 65s - loss: 29.3207 - MinusLogProbMetric: 29.3207 - val_loss: 29.7643 - val_MinusLogProbMetric: 29.7643 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 483/1000
2023-09-30 16:59:48.929 
Epoch 483/1000 
	 loss: 29.3367, MinusLogProbMetric: 29.3367, val_loss: 29.9546, val_MinusLogProbMetric: 29.9546

Epoch 483: val_loss did not improve from 29.61912
196/196 - 69s - loss: 29.3367 - MinusLogProbMetric: 29.3367 - val_loss: 29.9546 - val_MinusLogProbMetric: 29.9546 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 484/1000
2023-09-30 17:01:00.135 
Epoch 484/1000 
	 loss: 29.2792, MinusLogProbMetric: 29.2792, val_loss: 29.7526, val_MinusLogProbMetric: 29.7526

Epoch 484: val_loss did not improve from 29.61912
196/196 - 71s - loss: 29.2792 - MinusLogProbMetric: 29.2792 - val_loss: 29.7526 - val_MinusLogProbMetric: 29.7526 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 485/1000
2023-09-30 17:02:09.078 
Epoch 485/1000 
	 loss: 29.2691, MinusLogProbMetric: 29.2691, val_loss: 30.0848, val_MinusLogProbMetric: 30.0848

Epoch 485: val_loss did not improve from 29.61912
196/196 - 69s - loss: 29.2691 - MinusLogProbMetric: 29.2691 - val_loss: 30.0848 - val_MinusLogProbMetric: 30.0848 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 486/1000
2023-09-30 17:03:19.132 
Epoch 486/1000 
	 loss: 29.3662, MinusLogProbMetric: 29.3662, val_loss: 29.7456, val_MinusLogProbMetric: 29.7456

Epoch 486: val_loss did not improve from 29.61912
196/196 - 70s - loss: 29.3662 - MinusLogProbMetric: 29.3662 - val_loss: 29.7456 - val_MinusLogProbMetric: 29.7456 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 487/1000
2023-09-30 17:04:33.504 
Epoch 487/1000 
	 loss: 29.3459, MinusLogProbMetric: 29.3459, val_loss: 29.7833, val_MinusLogProbMetric: 29.7833

Epoch 487: val_loss did not improve from 29.61912
196/196 - 74s - loss: 29.3459 - MinusLogProbMetric: 29.3459 - val_loss: 29.7833 - val_MinusLogProbMetric: 29.7833 - lr: 1.1111e-04 - 74s/epoch - 379ms/step
Epoch 488/1000
2023-09-30 17:05:44.607 
Epoch 488/1000 
	 loss: 29.2879, MinusLogProbMetric: 29.2879, val_loss: 29.9167, val_MinusLogProbMetric: 29.9167

Epoch 488: val_loss did not improve from 29.61912
196/196 - 71s - loss: 29.2879 - MinusLogProbMetric: 29.2879 - val_loss: 29.9167 - val_MinusLogProbMetric: 29.9167 - lr: 1.1111e-04 - 71s/epoch - 363ms/step
Epoch 489/1000
2023-09-30 17:06:49.245 
Epoch 489/1000 
	 loss: 29.3170, MinusLogProbMetric: 29.3170, val_loss: 29.7732, val_MinusLogProbMetric: 29.7732

Epoch 489: val_loss did not improve from 29.61912
196/196 - 65s - loss: 29.3170 - MinusLogProbMetric: 29.3170 - val_loss: 29.7732 - val_MinusLogProbMetric: 29.7732 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 490/1000
2023-09-30 17:07:54.679 
Epoch 490/1000 
	 loss: 29.3106, MinusLogProbMetric: 29.3106, val_loss: 30.0499, val_MinusLogProbMetric: 30.0499

Epoch 490: val_loss did not improve from 29.61912
196/196 - 65s - loss: 29.3106 - MinusLogProbMetric: 29.3106 - val_loss: 30.0499 - val_MinusLogProbMetric: 30.0499 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 491/1000
2023-09-30 17:08:58.641 
Epoch 491/1000 
	 loss: 29.2757, MinusLogProbMetric: 29.2757, val_loss: 29.7398, val_MinusLogProbMetric: 29.7398

Epoch 491: val_loss did not improve from 29.61912
196/196 - 64s - loss: 29.2757 - MinusLogProbMetric: 29.2757 - val_loss: 29.7398 - val_MinusLogProbMetric: 29.7398 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 492/1000
2023-09-30 17:10:07.110 
Epoch 492/1000 
	 loss: 29.3679, MinusLogProbMetric: 29.3679, val_loss: 29.6339, val_MinusLogProbMetric: 29.6339

Epoch 492: val_loss did not improve from 29.61912
196/196 - 68s - loss: 29.3679 - MinusLogProbMetric: 29.3679 - val_loss: 29.6339 - val_MinusLogProbMetric: 29.6339 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 493/1000
2023-09-30 17:11:11.793 
Epoch 493/1000 
	 loss: 29.3438, MinusLogProbMetric: 29.3438, val_loss: 29.7995, val_MinusLogProbMetric: 29.7995

Epoch 493: val_loss did not improve from 29.61912
196/196 - 65s - loss: 29.3438 - MinusLogProbMetric: 29.3438 - val_loss: 29.7995 - val_MinusLogProbMetric: 29.7995 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 494/1000
2023-09-30 17:12:23.896 
Epoch 494/1000 
	 loss: 29.3621, MinusLogProbMetric: 29.3621, val_loss: 29.6266, val_MinusLogProbMetric: 29.6266

Epoch 494: val_loss did not improve from 29.61912
196/196 - 72s - loss: 29.3621 - MinusLogProbMetric: 29.3621 - val_loss: 29.6266 - val_MinusLogProbMetric: 29.6266 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 495/1000
2023-09-30 17:13:31.315 
Epoch 495/1000 
	 loss: 29.2900, MinusLogProbMetric: 29.2900, val_loss: 29.7299, val_MinusLogProbMetric: 29.7299

Epoch 495: val_loss did not improve from 29.61912
196/196 - 67s - loss: 29.2900 - MinusLogProbMetric: 29.2900 - val_loss: 29.7299 - val_MinusLogProbMetric: 29.7299 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 496/1000
2023-09-30 17:14:41.061 
Epoch 496/1000 
	 loss: 29.3219, MinusLogProbMetric: 29.3219, val_loss: 29.8892, val_MinusLogProbMetric: 29.8892

Epoch 496: val_loss did not improve from 29.61912
196/196 - 70s - loss: 29.3219 - MinusLogProbMetric: 29.3219 - val_loss: 29.8892 - val_MinusLogProbMetric: 29.8892 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 497/1000
2023-09-30 17:15:49.373 
Epoch 497/1000 
	 loss: 29.2915, MinusLogProbMetric: 29.2915, val_loss: 29.8037, val_MinusLogProbMetric: 29.8037

Epoch 497: val_loss did not improve from 29.61912
196/196 - 68s - loss: 29.2915 - MinusLogProbMetric: 29.2915 - val_loss: 29.8037 - val_MinusLogProbMetric: 29.8037 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 498/1000
2023-09-30 17:16:54.400 
Epoch 498/1000 
	 loss: 29.2536, MinusLogProbMetric: 29.2536, val_loss: 29.5622, val_MinusLogProbMetric: 29.5622

Epoch 498: val_loss improved from 29.61912 to 29.56222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 67s - loss: 29.2536 - MinusLogProbMetric: 29.2536 - val_loss: 29.5622 - val_MinusLogProbMetric: 29.5622 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 499/1000
2023-09-30 17:18:05.678 
Epoch 499/1000 
	 loss: 29.2502, MinusLogProbMetric: 29.2502, val_loss: 29.9572, val_MinusLogProbMetric: 29.9572

Epoch 499: val_loss did not improve from 29.56222
196/196 - 69s - loss: 29.2502 - MinusLogProbMetric: 29.2502 - val_loss: 29.9572 - val_MinusLogProbMetric: 29.9572 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 500/1000
2023-09-30 17:19:11.893 
Epoch 500/1000 
	 loss: 29.3123, MinusLogProbMetric: 29.3123, val_loss: 29.7871, val_MinusLogProbMetric: 29.7871

Epoch 500: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.3123 - MinusLogProbMetric: 29.3123 - val_loss: 29.7871 - val_MinusLogProbMetric: 29.7871 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 501/1000
2023-09-30 17:20:17.277 
Epoch 501/1000 
	 loss: 29.2671, MinusLogProbMetric: 29.2671, val_loss: 29.7439, val_MinusLogProbMetric: 29.7439

Epoch 501: val_loss did not improve from 29.56222
196/196 - 65s - loss: 29.2671 - MinusLogProbMetric: 29.2671 - val_loss: 29.7439 - val_MinusLogProbMetric: 29.7439 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 502/1000
2023-09-30 17:21:27.944 
Epoch 502/1000 
	 loss: 29.3376, MinusLogProbMetric: 29.3376, val_loss: 30.0569, val_MinusLogProbMetric: 30.0569

Epoch 502: val_loss did not improve from 29.56222
196/196 - 71s - loss: 29.3376 - MinusLogProbMetric: 29.3376 - val_loss: 30.0569 - val_MinusLogProbMetric: 30.0569 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 503/1000
2023-09-30 17:22:34.192 
Epoch 503/1000 
	 loss: 29.3223, MinusLogProbMetric: 29.3223, val_loss: 29.9237, val_MinusLogProbMetric: 29.9237

Epoch 503: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.3223 - MinusLogProbMetric: 29.3223 - val_loss: 29.9237 - val_MinusLogProbMetric: 29.9237 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 504/1000
2023-09-30 17:23:45.068 
Epoch 504/1000 
	 loss: 29.2925, MinusLogProbMetric: 29.2925, val_loss: 30.2729, val_MinusLogProbMetric: 30.2729

Epoch 504: val_loss did not improve from 29.56222
196/196 - 71s - loss: 29.2925 - MinusLogProbMetric: 29.2925 - val_loss: 30.2729 - val_MinusLogProbMetric: 30.2729 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 505/1000
2023-09-30 17:24:52.276 
Epoch 505/1000 
	 loss: 29.3285, MinusLogProbMetric: 29.3285, val_loss: 30.1131, val_MinusLogProbMetric: 30.1131

Epoch 505: val_loss did not improve from 29.56222
196/196 - 67s - loss: 29.3285 - MinusLogProbMetric: 29.3285 - val_loss: 30.1131 - val_MinusLogProbMetric: 30.1131 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 506/1000
2023-09-30 17:25:53.859 
Epoch 506/1000 
	 loss: 29.3099, MinusLogProbMetric: 29.3099, val_loss: 29.7499, val_MinusLogProbMetric: 29.7499

Epoch 506: val_loss did not improve from 29.56222
196/196 - 62s - loss: 29.3099 - MinusLogProbMetric: 29.3099 - val_loss: 29.7499 - val_MinusLogProbMetric: 29.7499 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 507/1000
2023-09-30 17:26:59.024 
Epoch 507/1000 
	 loss: 29.3364, MinusLogProbMetric: 29.3364, val_loss: 29.7440, val_MinusLogProbMetric: 29.7440

Epoch 507: val_loss did not improve from 29.56222
196/196 - 65s - loss: 29.3364 - MinusLogProbMetric: 29.3364 - val_loss: 29.7440 - val_MinusLogProbMetric: 29.7440 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 508/1000
2023-09-30 17:28:03.197 
Epoch 508/1000 
	 loss: 29.3248, MinusLogProbMetric: 29.3248, val_loss: 29.8420, val_MinusLogProbMetric: 29.8420

Epoch 508: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.3248 - MinusLogProbMetric: 29.3248 - val_loss: 29.8420 - val_MinusLogProbMetric: 29.8420 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 509/1000
2023-09-30 17:29:09.302 
Epoch 509/1000 
	 loss: 29.2895, MinusLogProbMetric: 29.2895, val_loss: 29.7034, val_MinusLogProbMetric: 29.7034

Epoch 509: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.2895 - MinusLogProbMetric: 29.2895 - val_loss: 29.7034 - val_MinusLogProbMetric: 29.7034 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 510/1000
2023-09-30 17:30:15.492 
Epoch 510/1000 
	 loss: 29.3218, MinusLogProbMetric: 29.3218, val_loss: 29.9992, val_MinusLogProbMetric: 29.9992

Epoch 510: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.3218 - MinusLogProbMetric: 29.3218 - val_loss: 29.9992 - val_MinusLogProbMetric: 29.9992 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 511/1000
2023-09-30 17:31:19.268 
Epoch 511/1000 
	 loss: 29.3727, MinusLogProbMetric: 29.3727, val_loss: 29.8830, val_MinusLogProbMetric: 29.8830

Epoch 511: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.3727 - MinusLogProbMetric: 29.3727 - val_loss: 29.8830 - val_MinusLogProbMetric: 29.8830 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 512/1000
2023-09-30 17:32:23.651 
Epoch 512/1000 
	 loss: 29.2846, MinusLogProbMetric: 29.2846, val_loss: 29.8797, val_MinusLogProbMetric: 29.8797

Epoch 512: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2846 - MinusLogProbMetric: 29.2846 - val_loss: 29.8797 - val_MinusLogProbMetric: 29.8797 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 513/1000
2023-09-30 17:33:27.234 
Epoch 513/1000 
	 loss: 29.3144, MinusLogProbMetric: 29.3144, val_loss: 29.7284, val_MinusLogProbMetric: 29.7284

Epoch 513: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.3144 - MinusLogProbMetric: 29.3144 - val_loss: 29.7284 - val_MinusLogProbMetric: 29.7284 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 514/1000
2023-09-30 17:34:31.104 
Epoch 514/1000 
	 loss: 29.2790, MinusLogProbMetric: 29.2790, val_loss: 30.2076, val_MinusLogProbMetric: 30.2076

Epoch 514: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2790 - MinusLogProbMetric: 29.2790 - val_loss: 30.2076 - val_MinusLogProbMetric: 30.2076 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 515/1000
2023-09-30 17:35:36.303 
Epoch 515/1000 
	 loss: 29.2754, MinusLogProbMetric: 29.2754, val_loss: 29.8327, val_MinusLogProbMetric: 29.8327

Epoch 515: val_loss did not improve from 29.56222
196/196 - 65s - loss: 29.2754 - MinusLogProbMetric: 29.2754 - val_loss: 29.8327 - val_MinusLogProbMetric: 29.8327 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 516/1000
2023-09-30 17:36:40.124 
Epoch 516/1000 
	 loss: 29.3076, MinusLogProbMetric: 29.3076, val_loss: 29.7633, val_MinusLogProbMetric: 29.7633

Epoch 516: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.3076 - MinusLogProbMetric: 29.3076 - val_loss: 29.7633 - val_MinusLogProbMetric: 29.7633 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 517/1000
2023-09-30 17:37:42.692 
Epoch 517/1000 
	 loss: 29.2368, MinusLogProbMetric: 29.2368, val_loss: 29.7457, val_MinusLogProbMetric: 29.7457

Epoch 517: val_loss did not improve from 29.56222
196/196 - 63s - loss: 29.2368 - MinusLogProbMetric: 29.2368 - val_loss: 29.7457 - val_MinusLogProbMetric: 29.7457 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 518/1000
2023-09-30 17:38:48.309 
Epoch 518/1000 
	 loss: 29.3668, MinusLogProbMetric: 29.3668, val_loss: 29.6800, val_MinusLogProbMetric: 29.6800

Epoch 518: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.3668 - MinusLogProbMetric: 29.3668 - val_loss: 29.6800 - val_MinusLogProbMetric: 29.6800 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 519/1000
2023-09-30 17:39:49.718 
Epoch 519/1000 
	 loss: 29.3307, MinusLogProbMetric: 29.3307, val_loss: 29.8078, val_MinusLogProbMetric: 29.8078

Epoch 519: val_loss did not improve from 29.56222
196/196 - 61s - loss: 29.3307 - MinusLogProbMetric: 29.3307 - val_loss: 29.8078 - val_MinusLogProbMetric: 29.8078 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 520/1000
2023-09-30 17:40:52.230 
Epoch 520/1000 
	 loss: 29.3388, MinusLogProbMetric: 29.3388, val_loss: 29.7372, val_MinusLogProbMetric: 29.7372

Epoch 520: val_loss did not improve from 29.56222
196/196 - 63s - loss: 29.3388 - MinusLogProbMetric: 29.3388 - val_loss: 29.7372 - val_MinusLogProbMetric: 29.7372 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 521/1000
2023-09-30 17:41:57.793 
Epoch 521/1000 
	 loss: 29.2562, MinusLogProbMetric: 29.2562, val_loss: 29.7394, val_MinusLogProbMetric: 29.7394

Epoch 521: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.2562 - MinusLogProbMetric: 29.2562 - val_loss: 29.7394 - val_MinusLogProbMetric: 29.7394 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 522/1000
2023-09-30 17:42:58.319 
Epoch 522/1000 
	 loss: 29.2994, MinusLogProbMetric: 29.2994, val_loss: 29.7351, val_MinusLogProbMetric: 29.7351

Epoch 522: val_loss did not improve from 29.56222
196/196 - 61s - loss: 29.2994 - MinusLogProbMetric: 29.2994 - val_loss: 29.7351 - val_MinusLogProbMetric: 29.7351 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 523/1000
2023-09-30 17:43:59.597 
Epoch 523/1000 
	 loss: 29.2706, MinusLogProbMetric: 29.2706, val_loss: 29.7442, val_MinusLogProbMetric: 29.7442

Epoch 523: val_loss did not improve from 29.56222
196/196 - 61s - loss: 29.2706 - MinusLogProbMetric: 29.2706 - val_loss: 29.7442 - val_MinusLogProbMetric: 29.7442 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 524/1000
2023-09-30 17:45:01.843 
Epoch 524/1000 
	 loss: 29.2648, MinusLogProbMetric: 29.2648, val_loss: 29.8801, val_MinusLogProbMetric: 29.8801

Epoch 524: val_loss did not improve from 29.56222
196/196 - 62s - loss: 29.2648 - MinusLogProbMetric: 29.2648 - val_loss: 29.8801 - val_MinusLogProbMetric: 29.8801 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 525/1000
2023-09-30 17:46:03.550 
Epoch 525/1000 
	 loss: 29.2941, MinusLogProbMetric: 29.2941, val_loss: 29.8566, val_MinusLogProbMetric: 29.8566

Epoch 525: val_loss did not improve from 29.56222
196/196 - 62s - loss: 29.2941 - MinusLogProbMetric: 29.2941 - val_loss: 29.8566 - val_MinusLogProbMetric: 29.8566 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 526/1000
2023-09-30 17:47:03.790 
Epoch 526/1000 
	 loss: 29.2919, MinusLogProbMetric: 29.2919, val_loss: 29.9278, val_MinusLogProbMetric: 29.9278

Epoch 526: val_loss did not improve from 29.56222
196/196 - 60s - loss: 29.2919 - MinusLogProbMetric: 29.2919 - val_loss: 29.9278 - val_MinusLogProbMetric: 29.9278 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 527/1000
2023-09-30 17:48:07.928 
Epoch 527/1000 
	 loss: 29.2668, MinusLogProbMetric: 29.2668, val_loss: 29.7816, val_MinusLogProbMetric: 29.7816

Epoch 527: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2668 - MinusLogProbMetric: 29.2668 - val_loss: 29.7816 - val_MinusLogProbMetric: 29.7816 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 528/1000
2023-09-30 17:49:12.790 
Epoch 528/1000 
	 loss: 29.2728, MinusLogProbMetric: 29.2728, val_loss: 29.6056, val_MinusLogProbMetric: 29.6056

Epoch 528: val_loss did not improve from 29.56222
196/196 - 65s - loss: 29.2728 - MinusLogProbMetric: 29.2728 - val_loss: 29.6056 - val_MinusLogProbMetric: 29.6056 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 529/1000
2023-09-30 17:50:19.488 
Epoch 529/1000 
	 loss: 29.2320, MinusLogProbMetric: 29.2320, val_loss: 29.6736, val_MinusLogProbMetric: 29.6736

Epoch 529: val_loss did not improve from 29.56222
196/196 - 67s - loss: 29.2320 - MinusLogProbMetric: 29.2320 - val_loss: 29.6736 - val_MinusLogProbMetric: 29.6736 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 530/1000
2023-09-30 17:51:21.272 
Epoch 530/1000 
	 loss: 29.2345, MinusLogProbMetric: 29.2345, val_loss: 29.7505, val_MinusLogProbMetric: 29.7505

Epoch 530: val_loss did not improve from 29.56222
196/196 - 62s - loss: 29.2345 - MinusLogProbMetric: 29.2345 - val_loss: 29.7505 - val_MinusLogProbMetric: 29.7505 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 531/1000
2023-09-30 17:52:26.758 
Epoch 531/1000 
	 loss: 29.2291, MinusLogProbMetric: 29.2291, val_loss: 29.7450, val_MinusLogProbMetric: 29.7450

Epoch 531: val_loss did not improve from 29.56222
196/196 - 65s - loss: 29.2291 - MinusLogProbMetric: 29.2291 - val_loss: 29.7450 - val_MinusLogProbMetric: 29.7450 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 532/1000
2023-09-30 17:53:30.334 
Epoch 532/1000 
	 loss: 29.3109, MinusLogProbMetric: 29.3109, val_loss: 29.7292, val_MinusLogProbMetric: 29.7292

Epoch 532: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.3109 - MinusLogProbMetric: 29.3109 - val_loss: 29.7292 - val_MinusLogProbMetric: 29.7292 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 533/1000
2023-09-30 17:54:35.454 
Epoch 533/1000 
	 loss: 29.2594, MinusLogProbMetric: 29.2594, val_loss: 29.7898, val_MinusLogProbMetric: 29.7898

Epoch 533: val_loss did not improve from 29.56222
196/196 - 65s - loss: 29.2594 - MinusLogProbMetric: 29.2594 - val_loss: 29.7898 - val_MinusLogProbMetric: 29.7898 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 534/1000
2023-09-30 17:55:40.832 
Epoch 534/1000 
	 loss: 29.3031, MinusLogProbMetric: 29.3031, val_loss: 30.0832, val_MinusLogProbMetric: 30.0832

Epoch 534: val_loss did not improve from 29.56222
196/196 - 65s - loss: 29.3031 - MinusLogProbMetric: 29.3031 - val_loss: 30.0832 - val_MinusLogProbMetric: 30.0832 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 535/1000
2023-09-30 17:56:52.221 
Epoch 535/1000 
	 loss: 29.2801, MinusLogProbMetric: 29.2801, val_loss: 29.8453, val_MinusLogProbMetric: 29.8453

Epoch 535: val_loss did not improve from 29.56222
196/196 - 71s - loss: 29.2801 - MinusLogProbMetric: 29.2801 - val_loss: 29.8453 - val_MinusLogProbMetric: 29.8453 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 536/1000
2023-09-30 17:57:56.456 
Epoch 536/1000 
	 loss: 29.2342, MinusLogProbMetric: 29.2342, val_loss: 29.7667, val_MinusLogProbMetric: 29.7667

Epoch 536: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2342 - MinusLogProbMetric: 29.2342 - val_loss: 29.7667 - val_MinusLogProbMetric: 29.7667 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 537/1000
2023-09-30 17:59:02.817 
Epoch 537/1000 
	 loss: 29.2318, MinusLogProbMetric: 29.2318, val_loss: 30.0229, val_MinusLogProbMetric: 30.0229

Epoch 537: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.2318 - MinusLogProbMetric: 29.2318 - val_loss: 30.0229 - val_MinusLogProbMetric: 30.0229 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 538/1000
2023-09-30 18:00:06.881 
Epoch 538/1000 
	 loss: 29.2500, MinusLogProbMetric: 29.2500, val_loss: 29.7486, val_MinusLogProbMetric: 29.7486

Epoch 538: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2500 - MinusLogProbMetric: 29.2500 - val_loss: 29.7486 - val_MinusLogProbMetric: 29.7486 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 539/1000
2023-09-30 18:01:10.126 
Epoch 539/1000 
	 loss: 29.2548, MinusLogProbMetric: 29.2548, val_loss: 30.0106, val_MinusLogProbMetric: 30.0106

Epoch 539: val_loss did not improve from 29.56222
196/196 - 63s - loss: 29.2548 - MinusLogProbMetric: 29.2548 - val_loss: 30.0106 - val_MinusLogProbMetric: 30.0106 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 540/1000
2023-09-30 18:02:14.144 
Epoch 540/1000 
	 loss: 29.2522, MinusLogProbMetric: 29.2522, val_loss: 29.8488, val_MinusLogProbMetric: 29.8488

Epoch 540: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2522 - MinusLogProbMetric: 29.2522 - val_loss: 29.8488 - val_MinusLogProbMetric: 29.8488 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 541/1000
2023-09-30 18:03:22.236 
Epoch 541/1000 
	 loss: 29.1951, MinusLogProbMetric: 29.1951, val_loss: 29.8741, val_MinusLogProbMetric: 29.8741

Epoch 541: val_loss did not improve from 29.56222
196/196 - 68s - loss: 29.1951 - MinusLogProbMetric: 29.1951 - val_loss: 29.8741 - val_MinusLogProbMetric: 29.8741 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 542/1000
2023-09-30 18:04:26.541 
Epoch 542/1000 
	 loss: 29.2290, MinusLogProbMetric: 29.2290, val_loss: 29.6805, val_MinusLogProbMetric: 29.6805

Epoch 542: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2290 - MinusLogProbMetric: 29.2290 - val_loss: 29.6805 - val_MinusLogProbMetric: 29.6805 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 543/1000
2023-09-30 18:05:29.699 
Epoch 543/1000 
	 loss: 29.1742, MinusLogProbMetric: 29.1742, val_loss: 30.1262, val_MinusLogProbMetric: 30.1262

Epoch 543: val_loss did not improve from 29.56222
196/196 - 63s - loss: 29.1742 - MinusLogProbMetric: 29.1742 - val_loss: 30.1262 - val_MinusLogProbMetric: 30.1262 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 544/1000
2023-09-30 18:06:32.872 
Epoch 544/1000 
	 loss: 29.2474, MinusLogProbMetric: 29.2474, val_loss: 29.6422, val_MinusLogProbMetric: 29.6422

Epoch 544: val_loss did not improve from 29.56222
196/196 - 63s - loss: 29.2474 - MinusLogProbMetric: 29.2474 - val_loss: 29.6422 - val_MinusLogProbMetric: 29.6422 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 545/1000
2023-09-30 18:07:39.137 
Epoch 545/1000 
	 loss: 29.2045, MinusLogProbMetric: 29.2045, val_loss: 29.6575, val_MinusLogProbMetric: 29.6575

Epoch 545: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.2045 - MinusLogProbMetric: 29.2045 - val_loss: 29.6575 - val_MinusLogProbMetric: 29.6575 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 546/1000
2023-09-30 18:08:43.192 
Epoch 546/1000 
	 loss: 29.2696, MinusLogProbMetric: 29.2696, val_loss: 29.6545, val_MinusLogProbMetric: 29.6545

Epoch 546: val_loss did not improve from 29.56222
196/196 - 64s - loss: 29.2696 - MinusLogProbMetric: 29.2696 - val_loss: 29.6545 - val_MinusLogProbMetric: 29.6545 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 547/1000
2023-09-30 18:09:49.213 
Epoch 547/1000 
	 loss: 29.2737, MinusLogProbMetric: 29.2737, val_loss: 30.1019, val_MinusLogProbMetric: 30.1019

Epoch 547: val_loss did not improve from 29.56222
196/196 - 66s - loss: 29.2737 - MinusLogProbMetric: 29.2737 - val_loss: 30.1019 - val_MinusLogProbMetric: 30.1019 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 548/1000
2023-09-30 18:10:55.795 
Epoch 548/1000 
	 loss: 29.2671, MinusLogProbMetric: 29.2671, val_loss: 29.8727, val_MinusLogProbMetric: 29.8727

Epoch 548: val_loss did not improve from 29.56222
196/196 - 67s - loss: 29.2671 - MinusLogProbMetric: 29.2671 - val_loss: 29.8727 - val_MinusLogProbMetric: 29.8727 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 549/1000
2023-09-30 18:12:02.487 
Epoch 549/1000 
	 loss: 28.8966, MinusLogProbMetric: 28.8966, val_loss: 29.3673, val_MinusLogProbMetric: 29.3673

Epoch 549: val_loss improved from 29.56222 to 29.36727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 68s - loss: 28.8966 - MinusLogProbMetric: 28.8966 - val_loss: 29.3673 - val_MinusLogProbMetric: 29.3673 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 550/1000
2023-09-30 18:13:09.845 
Epoch 550/1000 
	 loss: 28.9096, MinusLogProbMetric: 28.9096, val_loss: 29.4602, val_MinusLogProbMetric: 29.4602

Epoch 550: val_loss did not improve from 29.36727
196/196 - 66s - loss: 28.9096 - MinusLogProbMetric: 28.9096 - val_loss: 29.4602 - val_MinusLogProbMetric: 29.4602 - lr: 5.5556e-05 - 66s/epoch - 336ms/step
Epoch 551/1000
2023-09-30 18:14:17.533 
Epoch 551/1000 
	 loss: 28.8923, MinusLogProbMetric: 28.8923, val_loss: 29.5754, val_MinusLogProbMetric: 29.5754

Epoch 551: val_loss did not improve from 29.36727
196/196 - 68s - loss: 28.8923 - MinusLogProbMetric: 28.8923 - val_loss: 29.5754 - val_MinusLogProbMetric: 29.5754 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 552/1000
2023-09-30 18:15:26.837 
Epoch 552/1000 
	 loss: 28.8964, MinusLogProbMetric: 28.8964, val_loss: 29.3949, val_MinusLogProbMetric: 29.3949

Epoch 552: val_loss did not improve from 29.36727
196/196 - 69s - loss: 28.8964 - MinusLogProbMetric: 28.8964 - val_loss: 29.3949 - val_MinusLogProbMetric: 29.3949 - lr: 5.5556e-05 - 69s/epoch - 354ms/step
Epoch 553/1000
2023-09-30 18:16:32.086 
Epoch 553/1000 
	 loss: 28.8974, MinusLogProbMetric: 28.8974, val_loss: 29.5633, val_MinusLogProbMetric: 29.5633

Epoch 553: val_loss did not improve from 29.36727
196/196 - 65s - loss: 28.8974 - MinusLogProbMetric: 28.8974 - val_loss: 29.5633 - val_MinusLogProbMetric: 29.5633 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 554/1000
2023-09-30 18:17:39.696 
Epoch 554/1000 
	 loss: 28.8972, MinusLogProbMetric: 28.8972, val_loss: 29.5679, val_MinusLogProbMetric: 29.5679

Epoch 554: val_loss did not improve from 29.36727
196/196 - 68s - loss: 28.8972 - MinusLogProbMetric: 28.8972 - val_loss: 29.5679 - val_MinusLogProbMetric: 29.5679 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 555/1000
2023-09-30 18:18:49.280 
Epoch 555/1000 
	 loss: 28.9109, MinusLogProbMetric: 28.9109, val_loss: 29.5239, val_MinusLogProbMetric: 29.5239

Epoch 555: val_loss did not improve from 29.36727
196/196 - 70s - loss: 28.9109 - MinusLogProbMetric: 28.9109 - val_loss: 29.5239 - val_MinusLogProbMetric: 29.5239 - lr: 5.5556e-05 - 70s/epoch - 355ms/step
Epoch 556/1000
2023-09-30 18:19:54.062 
Epoch 556/1000 
	 loss: 28.9198, MinusLogProbMetric: 28.9198, val_loss: 29.4252, val_MinusLogProbMetric: 29.4252

Epoch 556: val_loss did not improve from 29.36727
196/196 - 65s - loss: 28.9198 - MinusLogProbMetric: 28.9198 - val_loss: 29.4252 - val_MinusLogProbMetric: 29.4252 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 557/1000
2023-09-30 18:20:59.451 
Epoch 557/1000 
	 loss: 28.8768, MinusLogProbMetric: 28.8768, val_loss: 29.4297, val_MinusLogProbMetric: 29.4297

Epoch 557: val_loss did not improve from 29.36727
196/196 - 65s - loss: 28.8768 - MinusLogProbMetric: 28.8768 - val_loss: 29.4297 - val_MinusLogProbMetric: 29.4297 - lr: 5.5556e-05 - 65s/epoch - 334ms/step
Epoch 558/1000
2023-09-30 18:22:08.142 
Epoch 558/1000 
	 loss: 28.8980, MinusLogProbMetric: 28.8980, val_loss: 29.4417, val_MinusLogProbMetric: 29.4417

Epoch 558: val_loss did not improve from 29.36727
196/196 - 69s - loss: 28.8980 - MinusLogProbMetric: 28.8980 - val_loss: 29.4417 - val_MinusLogProbMetric: 29.4417 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 559/1000
2023-09-30 18:23:14.219 
Epoch 559/1000 
	 loss: 28.8727, MinusLogProbMetric: 28.8727, val_loss: 29.9501, val_MinusLogProbMetric: 29.9501

Epoch 559: val_loss did not improve from 29.36727
196/196 - 66s - loss: 28.8727 - MinusLogProbMetric: 28.8727 - val_loss: 29.9501 - val_MinusLogProbMetric: 29.9501 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 560/1000
2023-09-30 18:24:21.112 
Epoch 560/1000 
	 loss: 28.9131, MinusLogProbMetric: 28.9131, val_loss: 29.3603, val_MinusLogProbMetric: 29.3603

Epoch 560: val_loss improved from 29.36727 to 29.36027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 69s - loss: 28.9131 - MinusLogProbMetric: 28.9131 - val_loss: 29.3603 - val_MinusLogProbMetric: 29.3603 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 561/1000
2023-09-30 18:25:26.299 
Epoch 561/1000 
	 loss: 28.8837, MinusLogProbMetric: 28.8837, val_loss: 29.4209, val_MinusLogProbMetric: 29.4209

Epoch 561: val_loss did not improve from 29.36027
196/196 - 64s - loss: 28.8837 - MinusLogProbMetric: 28.8837 - val_loss: 29.4209 - val_MinusLogProbMetric: 29.4209 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 562/1000
2023-09-30 18:26:31.760 
Epoch 562/1000 
	 loss: 28.8872, MinusLogProbMetric: 28.8872, val_loss: 29.5542, val_MinusLogProbMetric: 29.5542

Epoch 562: val_loss did not improve from 29.36027
196/196 - 65s - loss: 28.8872 - MinusLogProbMetric: 28.8872 - val_loss: 29.5542 - val_MinusLogProbMetric: 29.5542 - lr: 5.5556e-05 - 65s/epoch - 334ms/step
Epoch 563/1000
2023-09-30 18:27:34.795 
Epoch 563/1000 
	 loss: 28.8967, MinusLogProbMetric: 28.8967, val_loss: 29.4718, val_MinusLogProbMetric: 29.4718

Epoch 563: val_loss did not improve from 29.36027
196/196 - 63s - loss: 28.8967 - MinusLogProbMetric: 28.8967 - val_loss: 29.4718 - val_MinusLogProbMetric: 29.4718 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 564/1000
2023-09-30 18:28:38.936 
Epoch 564/1000 
	 loss: 28.8984, MinusLogProbMetric: 28.8984, val_loss: 29.6254, val_MinusLogProbMetric: 29.6254

Epoch 564: val_loss did not improve from 29.36027
196/196 - 64s - loss: 28.8984 - MinusLogProbMetric: 28.8984 - val_loss: 29.6254 - val_MinusLogProbMetric: 29.6254 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 565/1000
2023-09-30 18:29:44.917 
Epoch 565/1000 
	 loss: 28.8836, MinusLogProbMetric: 28.8836, val_loss: 29.6382, val_MinusLogProbMetric: 29.6382

Epoch 565: val_loss did not improve from 29.36027
196/196 - 66s - loss: 28.8836 - MinusLogProbMetric: 28.8836 - val_loss: 29.6382 - val_MinusLogProbMetric: 29.6382 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 566/1000
2023-09-30 18:30:51.150 
Epoch 566/1000 
	 loss: 28.8812, MinusLogProbMetric: 28.8812, val_loss: 29.6135, val_MinusLogProbMetric: 29.6135

Epoch 566: val_loss did not improve from 29.36027
196/196 - 66s - loss: 28.8812 - MinusLogProbMetric: 28.8812 - val_loss: 29.6135 - val_MinusLogProbMetric: 29.6135 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 567/1000
2023-09-30 18:31:58.946 
Epoch 567/1000 
	 loss: 28.8988, MinusLogProbMetric: 28.8988, val_loss: 29.4244, val_MinusLogProbMetric: 29.4244

Epoch 567: val_loss did not improve from 29.36027
196/196 - 68s - loss: 28.8988 - MinusLogProbMetric: 28.8988 - val_loss: 29.4244 - val_MinusLogProbMetric: 29.4244 - lr: 5.5556e-05 - 68s/epoch - 346ms/step
Epoch 568/1000
2023-09-30 18:33:06.960 
Epoch 568/1000 
	 loss: 28.8722, MinusLogProbMetric: 28.8722, val_loss: 29.5188, val_MinusLogProbMetric: 29.5188

Epoch 568: val_loss did not improve from 29.36027
196/196 - 68s - loss: 28.8722 - MinusLogProbMetric: 28.8722 - val_loss: 29.5188 - val_MinusLogProbMetric: 29.5188 - lr: 5.5556e-05 - 68s/epoch - 347ms/step
Epoch 569/1000
2023-09-30 18:34:13.226 
Epoch 569/1000 
	 loss: 28.9108, MinusLogProbMetric: 28.9108, val_loss: 29.4750, val_MinusLogProbMetric: 29.4750

Epoch 569: val_loss did not improve from 29.36027
196/196 - 66s - loss: 28.9108 - MinusLogProbMetric: 28.9108 - val_loss: 29.4750 - val_MinusLogProbMetric: 29.4750 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 570/1000
2023-09-30 18:35:20.064 
Epoch 570/1000 
	 loss: 28.9120, MinusLogProbMetric: 28.9120, val_loss: 29.4171, val_MinusLogProbMetric: 29.4171

Epoch 570: val_loss did not improve from 29.36027
196/196 - 67s - loss: 28.9120 - MinusLogProbMetric: 28.9120 - val_loss: 29.4171 - val_MinusLogProbMetric: 29.4171 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 571/1000
2023-09-30 18:36:27.521 
Epoch 571/1000 
	 loss: 28.8672, MinusLogProbMetric: 28.8672, val_loss: 29.4694, val_MinusLogProbMetric: 29.4694

Epoch 571: val_loss did not improve from 29.36027
196/196 - 67s - loss: 28.8672 - MinusLogProbMetric: 28.8672 - val_loss: 29.4694 - val_MinusLogProbMetric: 29.4694 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 572/1000
2023-09-30 18:37:31.057 
Epoch 572/1000 
	 loss: 28.8893, MinusLogProbMetric: 28.8893, val_loss: 29.4653, val_MinusLogProbMetric: 29.4653

Epoch 572: val_loss did not improve from 29.36027
196/196 - 64s - loss: 28.8893 - MinusLogProbMetric: 28.8893 - val_loss: 29.4653 - val_MinusLogProbMetric: 29.4653 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 573/1000
2023-09-30 18:38:33.558 
Epoch 573/1000 
	 loss: 28.8893, MinusLogProbMetric: 28.8893, val_loss: 29.3816, val_MinusLogProbMetric: 29.3816

Epoch 573: val_loss did not improve from 29.36027
196/196 - 62s - loss: 28.8893 - MinusLogProbMetric: 28.8893 - val_loss: 29.3816 - val_MinusLogProbMetric: 29.3816 - lr: 5.5556e-05 - 62s/epoch - 319ms/step
Epoch 574/1000
2023-09-30 18:39:39.765 
Epoch 574/1000 
	 loss: 28.8824, MinusLogProbMetric: 28.8824, val_loss: 29.4795, val_MinusLogProbMetric: 29.4795

Epoch 574: val_loss did not improve from 29.36027
196/196 - 66s - loss: 28.8824 - MinusLogProbMetric: 28.8824 - val_loss: 29.4795 - val_MinusLogProbMetric: 29.4795 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 575/1000
2023-09-30 18:41:08.633 
Epoch 575/1000 
	 loss: 28.8873, MinusLogProbMetric: 28.8873, val_loss: 29.7109, val_MinusLogProbMetric: 29.7109

Epoch 575: val_loss did not improve from 29.36027
196/196 - 89s - loss: 28.8873 - MinusLogProbMetric: 28.8873 - val_loss: 29.7109 - val_MinusLogProbMetric: 29.7109 - lr: 5.5556e-05 - 89s/epoch - 453ms/step
Epoch 576/1000
2023-09-30 18:42:52.767 
Epoch 576/1000 
	 loss: 28.9179, MinusLogProbMetric: 28.9179, val_loss: 29.4393, val_MinusLogProbMetric: 29.4393

Epoch 576: val_loss did not improve from 29.36027
196/196 - 104s - loss: 28.9179 - MinusLogProbMetric: 28.9179 - val_loss: 29.4393 - val_MinusLogProbMetric: 29.4393 - lr: 5.5556e-05 - 104s/epoch - 531ms/step
Epoch 577/1000
2023-09-30 18:44:32.896 
Epoch 577/1000 
	 loss: 28.8805, MinusLogProbMetric: 28.8805, val_loss: 29.3591, val_MinusLogProbMetric: 29.3591

Epoch 577: val_loss improved from 29.36027 to 29.35910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 103s - loss: 28.8805 - MinusLogProbMetric: 28.8805 - val_loss: 29.3591 - val_MinusLogProbMetric: 29.3591 - lr: 5.5556e-05 - 103s/epoch - 526ms/step
Epoch 578/1000
2023-09-30 18:46:16.091 
Epoch 578/1000 
	 loss: 28.9094, MinusLogProbMetric: 28.9094, val_loss: 29.5653, val_MinusLogProbMetric: 29.5653

Epoch 578: val_loss did not improve from 29.35910
196/196 - 100s - loss: 28.9094 - MinusLogProbMetric: 28.9094 - val_loss: 29.5653 - val_MinusLogProbMetric: 29.5653 - lr: 5.5556e-05 - 100s/epoch - 512ms/step
Epoch 579/1000
2023-09-30 18:48:01.270 
Epoch 579/1000 
	 loss: 28.8947, MinusLogProbMetric: 28.8947, val_loss: 29.3841, val_MinusLogProbMetric: 29.3841

Epoch 579: val_loss did not improve from 29.35910
196/196 - 105s - loss: 28.8947 - MinusLogProbMetric: 28.8947 - val_loss: 29.3841 - val_MinusLogProbMetric: 29.3841 - lr: 5.5556e-05 - 105s/epoch - 537ms/step
Epoch 580/1000
2023-09-30 18:49:36.878 
Epoch 580/1000 
	 loss: 28.8879, MinusLogProbMetric: 28.8879, val_loss: 29.3886, val_MinusLogProbMetric: 29.3886

Epoch 580: val_loss did not improve from 29.35910
196/196 - 96s - loss: 28.8879 - MinusLogProbMetric: 28.8879 - val_loss: 29.3886 - val_MinusLogProbMetric: 29.3886 - lr: 5.5556e-05 - 96s/epoch - 488ms/step
Epoch 581/1000
2023-09-30 18:51:14.794 
Epoch 581/1000 
	 loss: 28.8808, MinusLogProbMetric: 28.8808, val_loss: 29.5884, val_MinusLogProbMetric: 29.5884

Epoch 581: val_loss did not improve from 29.35910
196/196 - 98s - loss: 28.8808 - MinusLogProbMetric: 28.8808 - val_loss: 29.5884 - val_MinusLogProbMetric: 29.5884 - lr: 5.5556e-05 - 98s/epoch - 499ms/step
Epoch 582/1000
2023-09-30 18:52:56.737 
Epoch 582/1000 
	 loss: 28.8718, MinusLogProbMetric: 28.8718, val_loss: 29.5726, val_MinusLogProbMetric: 29.5726

Epoch 582: val_loss did not improve from 29.35910
196/196 - 102s - loss: 28.8718 - MinusLogProbMetric: 28.8718 - val_loss: 29.5726 - val_MinusLogProbMetric: 29.5726 - lr: 5.5556e-05 - 102s/epoch - 520ms/step
Epoch 583/1000
2023-09-30 18:54:33.212 
Epoch 583/1000 
	 loss: 28.8716, MinusLogProbMetric: 28.8716, val_loss: 29.3869, val_MinusLogProbMetric: 29.3869

Epoch 583: val_loss did not improve from 29.35910
196/196 - 96s - loss: 28.8716 - MinusLogProbMetric: 28.8716 - val_loss: 29.3869 - val_MinusLogProbMetric: 29.3869 - lr: 5.5556e-05 - 96s/epoch - 492ms/step
Epoch 584/1000
2023-09-30 18:56:16.746 
Epoch 584/1000 
	 loss: 28.8755, MinusLogProbMetric: 28.8755, val_loss: 29.4321, val_MinusLogProbMetric: 29.4321

Epoch 584: val_loss did not improve from 29.35910
196/196 - 104s - loss: 28.8755 - MinusLogProbMetric: 28.8755 - val_loss: 29.4321 - val_MinusLogProbMetric: 29.4321 - lr: 5.5556e-05 - 104s/epoch - 528ms/step
Epoch 585/1000
2023-09-30 18:57:53.648 
Epoch 585/1000 
	 loss: 28.8800, MinusLogProbMetric: 28.8800, val_loss: 29.5854, val_MinusLogProbMetric: 29.5854

Epoch 585: val_loss did not improve from 29.35910
196/196 - 97s - loss: 28.8800 - MinusLogProbMetric: 28.8800 - val_loss: 29.5854 - val_MinusLogProbMetric: 29.5854 - lr: 5.5556e-05 - 97s/epoch - 494ms/step
Epoch 586/1000
2023-09-30 18:59:41.637 
Epoch 586/1000 
	 loss: 28.8666, MinusLogProbMetric: 28.8666, val_loss: 29.3817, val_MinusLogProbMetric: 29.3817

Epoch 586: val_loss did not improve from 29.35910
196/196 - 108s - loss: 28.8666 - MinusLogProbMetric: 28.8666 - val_loss: 29.3817 - val_MinusLogProbMetric: 29.3817 - lr: 5.5556e-05 - 108s/epoch - 551ms/step
Epoch 587/1000
2023-09-30 19:01:19.592 
Epoch 587/1000 
	 loss: 28.8772, MinusLogProbMetric: 28.8772, val_loss: 29.5209, val_MinusLogProbMetric: 29.5209

Epoch 587: val_loss did not improve from 29.35910
196/196 - 98s - loss: 28.8772 - MinusLogProbMetric: 28.8772 - val_loss: 29.5209 - val_MinusLogProbMetric: 29.5209 - lr: 5.5556e-05 - 98s/epoch - 500ms/step
Epoch 588/1000
2023-09-30 19:02:53.557 
Epoch 588/1000 
	 loss: 28.8997, MinusLogProbMetric: 28.8997, val_loss: 29.4000, val_MinusLogProbMetric: 29.4000

Epoch 588: val_loss did not improve from 29.35910
196/196 - 94s - loss: 28.8997 - MinusLogProbMetric: 28.8997 - val_loss: 29.4000 - val_MinusLogProbMetric: 29.4000 - lr: 5.5556e-05 - 94s/epoch - 479ms/step
Epoch 589/1000
2023-09-30 19:04:27.384 
Epoch 589/1000 
	 loss: 28.8733, MinusLogProbMetric: 28.8733, val_loss: 29.6351, val_MinusLogProbMetric: 29.6351

Epoch 589: val_loss did not improve from 29.35910
196/196 - 94s - loss: 28.8733 - MinusLogProbMetric: 28.8733 - val_loss: 29.6351 - val_MinusLogProbMetric: 29.6351 - lr: 5.5556e-05 - 94s/epoch - 479ms/step
Epoch 590/1000
2023-09-30 19:06:04.900 
Epoch 590/1000 
	 loss: 28.8927, MinusLogProbMetric: 28.8927, val_loss: 29.4919, val_MinusLogProbMetric: 29.4919

Epoch 590: val_loss did not improve from 29.35910
196/196 - 97s - loss: 28.8927 - MinusLogProbMetric: 28.8927 - val_loss: 29.4919 - val_MinusLogProbMetric: 29.4919 - lr: 5.5556e-05 - 97s/epoch - 497ms/step
Epoch 591/1000
2023-09-30 19:07:44.446 
Epoch 591/1000 
	 loss: 28.8788, MinusLogProbMetric: 28.8788, val_loss: 29.6178, val_MinusLogProbMetric: 29.6178

Epoch 591: val_loss did not improve from 29.35910
196/196 - 100s - loss: 28.8788 - MinusLogProbMetric: 28.8788 - val_loss: 29.6178 - val_MinusLogProbMetric: 29.6178 - lr: 5.5556e-05 - 100s/epoch - 508ms/step
Epoch 592/1000
2023-09-30 19:09:13.283 
Epoch 592/1000 
	 loss: 28.8773, MinusLogProbMetric: 28.8773, val_loss: 29.4307, val_MinusLogProbMetric: 29.4307

Epoch 592: val_loss did not improve from 29.35910
196/196 - 89s - loss: 28.8773 - MinusLogProbMetric: 28.8773 - val_loss: 29.4307 - val_MinusLogProbMetric: 29.4307 - lr: 5.5556e-05 - 89s/epoch - 453ms/step
Epoch 593/1000
2023-09-30 19:10:50.664 
Epoch 593/1000 
	 loss: 28.8737, MinusLogProbMetric: 28.8737, val_loss: 29.4829, val_MinusLogProbMetric: 29.4829

Epoch 593: val_loss did not improve from 29.35910
196/196 - 97s - loss: 28.8737 - MinusLogProbMetric: 28.8737 - val_loss: 29.4829 - val_MinusLogProbMetric: 29.4829 - lr: 5.5556e-05 - 97s/epoch - 497ms/step
Epoch 594/1000
2023-09-30 19:12:26.268 
Epoch 594/1000 
	 loss: 28.8755, MinusLogProbMetric: 28.8755, val_loss: 29.3986, val_MinusLogProbMetric: 29.3986

Epoch 594: val_loss did not improve from 29.35910
196/196 - 96s - loss: 28.8755 - MinusLogProbMetric: 28.8755 - val_loss: 29.3986 - val_MinusLogProbMetric: 29.3986 - lr: 5.5556e-05 - 96s/epoch - 488ms/step
Epoch 595/1000
2023-09-30 19:14:11.868 
Epoch 595/1000 
	 loss: 28.8633, MinusLogProbMetric: 28.8633, val_loss: 29.5909, val_MinusLogProbMetric: 29.5909

Epoch 595: val_loss did not improve from 29.35910
196/196 - 106s - loss: 28.8633 - MinusLogProbMetric: 28.8633 - val_loss: 29.5909 - val_MinusLogProbMetric: 29.5909 - lr: 5.5556e-05 - 106s/epoch - 538ms/step
Epoch 596/1000
2023-09-30 19:15:53.655 
Epoch 596/1000 
	 loss: 28.8587, MinusLogProbMetric: 28.8587, val_loss: 29.3539, val_MinusLogProbMetric: 29.3539

Epoch 596: val_loss improved from 29.35910 to 29.35391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 103s - loss: 28.8587 - MinusLogProbMetric: 28.8587 - val_loss: 29.3539 - val_MinusLogProbMetric: 29.3539 - lr: 5.5556e-05 - 103s/epoch - 526ms/step
Epoch 597/1000
2023-09-30 19:17:29.479 
Epoch 597/1000 
	 loss: 28.8579, MinusLogProbMetric: 28.8579, val_loss: 29.4019, val_MinusLogProbMetric: 29.4019

Epoch 597: val_loss did not improve from 29.35391
196/196 - 94s - loss: 28.8579 - MinusLogProbMetric: 28.8579 - val_loss: 29.4019 - val_MinusLogProbMetric: 29.4019 - lr: 5.5556e-05 - 94s/epoch - 482ms/step
Epoch 598/1000
2023-09-30 19:19:10.568 
Epoch 598/1000 
	 loss: 28.8707, MinusLogProbMetric: 28.8707, val_loss: 29.4093, val_MinusLogProbMetric: 29.4093

Epoch 598: val_loss did not improve from 29.35391
196/196 - 101s - loss: 28.8707 - MinusLogProbMetric: 28.8707 - val_loss: 29.4093 - val_MinusLogProbMetric: 29.4093 - lr: 5.5556e-05 - 101s/epoch - 516ms/step
Epoch 599/1000
2023-09-30 19:20:54.691 
Epoch 599/1000 
	 loss: 28.8778, MinusLogProbMetric: 28.8778, val_loss: 29.4175, val_MinusLogProbMetric: 29.4175

Epoch 599: val_loss did not improve from 29.35391
196/196 - 104s - loss: 28.8778 - MinusLogProbMetric: 28.8778 - val_loss: 29.4175 - val_MinusLogProbMetric: 29.4175 - lr: 5.5556e-05 - 104s/epoch - 531ms/step
Epoch 600/1000
2023-09-30 19:22:41.806 
Epoch 600/1000 
	 loss: 28.8663, MinusLogProbMetric: 28.8663, val_loss: 29.4173, val_MinusLogProbMetric: 29.4173

Epoch 600: val_loss did not improve from 29.35391
196/196 - 107s - loss: 28.8663 - MinusLogProbMetric: 28.8663 - val_loss: 29.4173 - val_MinusLogProbMetric: 29.4173 - lr: 5.5556e-05 - 107s/epoch - 546ms/step
Epoch 601/1000
2023-09-30 19:24:19.652 
Epoch 601/1000 
	 loss: 28.8579, MinusLogProbMetric: 28.8579, val_loss: 29.3904, val_MinusLogProbMetric: 29.3904

Epoch 601: val_loss did not improve from 29.35391
196/196 - 98s - loss: 28.8579 - MinusLogProbMetric: 28.8579 - val_loss: 29.3904 - val_MinusLogProbMetric: 29.3904 - lr: 5.5556e-05 - 98s/epoch - 499ms/step
Epoch 602/1000
2023-09-30 19:25:55.651 
Epoch 602/1000 
	 loss: 28.8599, MinusLogProbMetric: 28.8599, val_loss: 29.4077, val_MinusLogProbMetric: 29.4077

Epoch 602: val_loss did not improve from 29.35391
196/196 - 96s - loss: 28.8599 - MinusLogProbMetric: 28.8599 - val_loss: 29.4077 - val_MinusLogProbMetric: 29.4077 - lr: 5.5556e-05 - 96s/epoch - 490ms/step
Epoch 603/1000
2023-09-30 19:27:28.101 
Epoch 603/1000 
	 loss: 28.8623, MinusLogProbMetric: 28.8623, val_loss: 29.4893, val_MinusLogProbMetric: 29.4893

Epoch 603: val_loss did not improve from 29.35391
196/196 - 92s - loss: 28.8623 - MinusLogProbMetric: 28.8623 - val_loss: 29.4893 - val_MinusLogProbMetric: 29.4893 - lr: 5.5556e-05 - 92s/epoch - 472ms/step
Epoch 604/1000
2023-09-30 19:28:58.773 
Epoch 604/1000 
	 loss: 28.8653, MinusLogProbMetric: 28.8653, val_loss: 29.4246, val_MinusLogProbMetric: 29.4246

Epoch 604: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8653 - MinusLogProbMetric: 28.8653 - val_loss: 29.4246 - val_MinusLogProbMetric: 29.4246 - lr: 5.5556e-05 - 91s/epoch - 463ms/step
Epoch 605/1000
2023-09-30 19:30:27.450 
Epoch 605/1000 
	 loss: 28.8721, MinusLogProbMetric: 28.8721, val_loss: 29.3926, val_MinusLogProbMetric: 29.3926

Epoch 605: val_loss did not improve from 29.35391
196/196 - 89s - loss: 28.8721 - MinusLogProbMetric: 28.8721 - val_loss: 29.3926 - val_MinusLogProbMetric: 29.3926 - lr: 5.5556e-05 - 89s/epoch - 452ms/step
Epoch 606/1000
2023-09-30 19:32:06.156 
Epoch 606/1000 
	 loss: 28.8651, MinusLogProbMetric: 28.8651, val_loss: 29.3747, val_MinusLogProbMetric: 29.3747

Epoch 606: val_loss did not improve from 29.35391
196/196 - 99s - loss: 28.8651 - MinusLogProbMetric: 28.8651 - val_loss: 29.3747 - val_MinusLogProbMetric: 29.3747 - lr: 5.5556e-05 - 99s/epoch - 503ms/step
Epoch 607/1000
2023-09-30 19:33:41.482 
Epoch 607/1000 
	 loss: 28.8554, MinusLogProbMetric: 28.8554, val_loss: 29.3587, val_MinusLogProbMetric: 29.3587

Epoch 607: val_loss did not improve from 29.35391
196/196 - 95s - loss: 28.8554 - MinusLogProbMetric: 28.8554 - val_loss: 29.3587 - val_MinusLogProbMetric: 29.3587 - lr: 5.5556e-05 - 95s/epoch - 486ms/step
Epoch 608/1000
2023-09-30 19:35:21.633 
Epoch 608/1000 
	 loss: 28.8766, MinusLogProbMetric: 28.8766, val_loss: 29.4287, val_MinusLogProbMetric: 29.4287

Epoch 608: val_loss did not improve from 29.35391
196/196 - 100s - loss: 28.8766 - MinusLogProbMetric: 28.8766 - val_loss: 29.4287 - val_MinusLogProbMetric: 29.4287 - lr: 5.5556e-05 - 100s/epoch - 511ms/step
Epoch 609/1000
2023-09-30 19:36:52.765 
Epoch 609/1000 
	 loss: 28.8921, MinusLogProbMetric: 28.8921, val_loss: 29.5549, val_MinusLogProbMetric: 29.5549

Epoch 609: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8921 - MinusLogProbMetric: 28.8921 - val_loss: 29.5549 - val_MinusLogProbMetric: 29.5549 - lr: 5.5556e-05 - 91s/epoch - 465ms/step
Epoch 610/1000
2023-09-30 19:38:31.476 
Epoch 610/1000 
	 loss: 28.8666, MinusLogProbMetric: 28.8666, val_loss: 29.4468, val_MinusLogProbMetric: 29.4468

Epoch 610: val_loss did not improve from 29.35391
196/196 - 99s - loss: 28.8666 - MinusLogProbMetric: 28.8666 - val_loss: 29.4468 - val_MinusLogProbMetric: 29.4468 - lr: 5.5556e-05 - 99s/epoch - 504ms/step
Epoch 611/1000
2023-09-30 19:40:02.845 
Epoch 611/1000 
	 loss: 28.8705, MinusLogProbMetric: 28.8705, val_loss: 29.4467, val_MinusLogProbMetric: 29.4467

Epoch 611: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8705 - MinusLogProbMetric: 28.8705 - val_loss: 29.4467 - val_MinusLogProbMetric: 29.4467 - lr: 5.5556e-05 - 91s/epoch - 466ms/step
Epoch 612/1000
2023-09-30 19:41:39.097 
Epoch 612/1000 
	 loss: 28.8696, MinusLogProbMetric: 28.8696, val_loss: 29.4393, val_MinusLogProbMetric: 29.4393

Epoch 612: val_loss did not improve from 29.35391
196/196 - 96s - loss: 28.8696 - MinusLogProbMetric: 28.8696 - val_loss: 29.4393 - val_MinusLogProbMetric: 29.4393 - lr: 5.5556e-05 - 96s/epoch - 491ms/step
Epoch 613/1000
2023-09-30 19:43:08.269 
Epoch 613/1000 
	 loss: 28.8550, MinusLogProbMetric: 28.8550, val_loss: 29.5162, val_MinusLogProbMetric: 29.5162

Epoch 613: val_loss did not improve from 29.35391
196/196 - 89s - loss: 28.8550 - MinusLogProbMetric: 28.8550 - val_loss: 29.5162 - val_MinusLogProbMetric: 29.5162 - lr: 5.5556e-05 - 89s/epoch - 455ms/step
Epoch 614/1000
2023-09-30 19:44:41.434 
Epoch 614/1000 
	 loss: 28.8656, MinusLogProbMetric: 28.8656, val_loss: 29.4872, val_MinusLogProbMetric: 29.4872

Epoch 614: val_loss did not improve from 29.35391
196/196 - 93s - loss: 28.8656 - MinusLogProbMetric: 28.8656 - val_loss: 29.4872 - val_MinusLogProbMetric: 29.4872 - lr: 5.5556e-05 - 93s/epoch - 475ms/step
Epoch 615/1000
2023-09-30 19:46:17.304 
Epoch 615/1000 
	 loss: 28.8520, MinusLogProbMetric: 28.8520, val_loss: 29.4819, val_MinusLogProbMetric: 29.4819

Epoch 615: val_loss did not improve from 29.35391
196/196 - 96s - loss: 28.8520 - MinusLogProbMetric: 28.8520 - val_loss: 29.4819 - val_MinusLogProbMetric: 29.4819 - lr: 5.5556e-05 - 96s/epoch - 489ms/step
Epoch 616/1000
2023-09-30 19:47:43.511 
Epoch 616/1000 
	 loss: 28.8613, MinusLogProbMetric: 28.8613, val_loss: 29.4136, val_MinusLogProbMetric: 29.4136

Epoch 616: val_loss did not improve from 29.35391
196/196 - 86s - loss: 28.8613 - MinusLogProbMetric: 28.8613 - val_loss: 29.4136 - val_MinusLogProbMetric: 29.4136 - lr: 5.5556e-05 - 86s/epoch - 440ms/step
Epoch 617/1000
2023-09-30 19:49:14.916 
Epoch 617/1000 
	 loss: 28.9124, MinusLogProbMetric: 28.9124, val_loss: 29.4464, val_MinusLogProbMetric: 29.4464

Epoch 617: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.9124 - MinusLogProbMetric: 28.9124 - val_loss: 29.4464 - val_MinusLogProbMetric: 29.4464 - lr: 5.5556e-05 - 91s/epoch - 466ms/step
Epoch 618/1000
2023-09-30 19:50:46.545 
Epoch 618/1000 
	 loss: 28.8662, MinusLogProbMetric: 28.8662, val_loss: 29.4139, val_MinusLogProbMetric: 29.4139

Epoch 618: val_loss did not improve from 29.35391
196/196 - 92s - loss: 28.8662 - MinusLogProbMetric: 28.8662 - val_loss: 29.4139 - val_MinusLogProbMetric: 29.4139 - lr: 5.5556e-05 - 92s/epoch - 468ms/step
Epoch 619/1000
2023-09-30 19:52:25.450 
Epoch 619/1000 
	 loss: 28.8867, MinusLogProbMetric: 28.8867, val_loss: 29.5640, val_MinusLogProbMetric: 29.5640

Epoch 619: val_loss did not improve from 29.35391
196/196 - 99s - loss: 28.8867 - MinusLogProbMetric: 28.8867 - val_loss: 29.5640 - val_MinusLogProbMetric: 29.5640 - lr: 5.5556e-05 - 99s/epoch - 505ms/step
Epoch 620/1000
2023-09-30 19:53:55.665 
Epoch 620/1000 
	 loss: 28.8700, MinusLogProbMetric: 28.8700, val_loss: 29.3723, val_MinusLogProbMetric: 29.3723

Epoch 620: val_loss did not improve from 29.35391
196/196 - 90s - loss: 28.8700 - MinusLogProbMetric: 28.8700 - val_loss: 29.3723 - val_MinusLogProbMetric: 29.3723 - lr: 5.5556e-05 - 90s/epoch - 460ms/step
Epoch 621/1000
2023-09-30 19:55:26.410 
Epoch 621/1000 
	 loss: 28.8630, MinusLogProbMetric: 28.8630, val_loss: 29.3782, val_MinusLogProbMetric: 29.3782

Epoch 621: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8630 - MinusLogProbMetric: 28.8630 - val_loss: 29.3782 - val_MinusLogProbMetric: 29.3782 - lr: 5.5556e-05 - 91s/epoch - 463ms/step
Epoch 622/1000
2023-09-30 19:56:57.571 
Epoch 622/1000 
	 loss: 28.8478, MinusLogProbMetric: 28.8478, val_loss: 29.4123, val_MinusLogProbMetric: 29.4123

Epoch 622: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8478 - MinusLogProbMetric: 28.8478 - val_loss: 29.4123 - val_MinusLogProbMetric: 29.4123 - lr: 5.5556e-05 - 91s/epoch - 465ms/step
Epoch 623/1000
2023-09-30 19:58:25.640 
Epoch 623/1000 
	 loss: 28.8762, MinusLogProbMetric: 28.8762, val_loss: 29.3953, val_MinusLogProbMetric: 29.3953

Epoch 623: val_loss did not improve from 29.35391
196/196 - 88s - loss: 28.8762 - MinusLogProbMetric: 28.8762 - val_loss: 29.3953 - val_MinusLogProbMetric: 29.3953 - lr: 5.5556e-05 - 88s/epoch - 449ms/step
Epoch 624/1000
2023-09-30 19:59:54.887 
Epoch 624/1000 
	 loss: 28.8711, MinusLogProbMetric: 28.8711, val_loss: 29.6045, val_MinusLogProbMetric: 29.6045

Epoch 624: val_loss did not improve from 29.35391
196/196 - 89s - loss: 28.8711 - MinusLogProbMetric: 28.8711 - val_loss: 29.6045 - val_MinusLogProbMetric: 29.6045 - lr: 5.5556e-05 - 89s/epoch - 455ms/step
Epoch 625/1000
2023-09-30 20:01:22.301 
Epoch 625/1000 
	 loss: 28.8557, MinusLogProbMetric: 28.8557, val_loss: 29.5383, val_MinusLogProbMetric: 29.5383

Epoch 625: val_loss did not improve from 29.35391
196/196 - 87s - loss: 28.8557 - MinusLogProbMetric: 28.8557 - val_loss: 29.5383 - val_MinusLogProbMetric: 29.5383 - lr: 5.5556e-05 - 87s/epoch - 446ms/step
Epoch 626/1000
2023-09-30 20:02:54.831 
Epoch 626/1000 
	 loss: 28.8520, MinusLogProbMetric: 28.8520, val_loss: 29.4668, val_MinusLogProbMetric: 29.4668

Epoch 626: val_loss did not improve from 29.35391
196/196 - 93s - loss: 28.8520 - MinusLogProbMetric: 28.8520 - val_loss: 29.4668 - val_MinusLogProbMetric: 29.4668 - lr: 5.5556e-05 - 93s/epoch - 472ms/step
Epoch 627/1000
2023-09-30 20:04:24.966 
Epoch 627/1000 
	 loss: 28.8499, MinusLogProbMetric: 28.8499, val_loss: 29.5334, val_MinusLogProbMetric: 29.5334

Epoch 627: val_loss did not improve from 29.35391
196/196 - 90s - loss: 28.8499 - MinusLogProbMetric: 28.8499 - val_loss: 29.5334 - val_MinusLogProbMetric: 29.5334 - lr: 5.5556e-05 - 90s/epoch - 460ms/step
Epoch 628/1000
2023-09-30 20:05:52.214 
Epoch 628/1000 
	 loss: 28.8923, MinusLogProbMetric: 28.8923, val_loss: 29.7287, val_MinusLogProbMetric: 29.7287

Epoch 628: val_loss did not improve from 29.35391
196/196 - 87s - loss: 28.8923 - MinusLogProbMetric: 28.8923 - val_loss: 29.7287 - val_MinusLogProbMetric: 29.7287 - lr: 5.5556e-05 - 87s/epoch - 445ms/step
Epoch 629/1000
2023-09-30 20:07:26.637 
Epoch 629/1000 
	 loss: 28.8930, MinusLogProbMetric: 28.8930, val_loss: 29.3548, val_MinusLogProbMetric: 29.3548

Epoch 629: val_loss did not improve from 29.35391
196/196 - 94s - loss: 28.8930 - MinusLogProbMetric: 28.8930 - val_loss: 29.3548 - val_MinusLogProbMetric: 29.3548 - lr: 5.5556e-05 - 94s/epoch - 482ms/step
Epoch 630/1000
2023-09-30 20:08:57.509 
Epoch 630/1000 
	 loss: 28.8521, MinusLogProbMetric: 28.8521, val_loss: 29.4375, val_MinusLogProbMetric: 29.4375

Epoch 630: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8521 - MinusLogProbMetric: 28.8521 - val_loss: 29.4375 - val_MinusLogProbMetric: 29.4375 - lr: 5.5556e-05 - 91s/epoch - 464ms/step
Epoch 631/1000
2023-09-30 20:10:24.809 
Epoch 631/1000 
	 loss: 28.8864, MinusLogProbMetric: 28.8864, val_loss: 29.5655, val_MinusLogProbMetric: 29.5655

Epoch 631: val_loss did not improve from 29.35391
196/196 - 87s - loss: 28.8864 - MinusLogProbMetric: 28.8864 - val_loss: 29.5655 - val_MinusLogProbMetric: 29.5655 - lr: 5.5556e-05 - 87s/epoch - 445ms/step
Epoch 632/1000
2023-09-30 20:11:54.766 
Epoch 632/1000 
	 loss: 28.8692, MinusLogProbMetric: 28.8692, val_loss: 29.3864, val_MinusLogProbMetric: 29.3864

Epoch 632: val_loss did not improve from 29.35391
196/196 - 90s - loss: 28.8692 - MinusLogProbMetric: 28.8692 - val_loss: 29.3864 - val_MinusLogProbMetric: 29.3864 - lr: 5.5556e-05 - 90s/epoch - 459ms/step
Epoch 633/1000
2023-09-30 20:13:26.807 
Epoch 633/1000 
	 loss: 28.8706, MinusLogProbMetric: 28.8706, val_loss: 29.4689, val_MinusLogProbMetric: 29.4689

Epoch 633: val_loss did not improve from 29.35391
196/196 - 92s - loss: 28.8706 - MinusLogProbMetric: 28.8706 - val_loss: 29.4689 - val_MinusLogProbMetric: 29.4689 - lr: 5.5556e-05 - 92s/epoch - 469ms/step
Epoch 634/1000
2023-09-30 20:14:55.444 
Epoch 634/1000 
	 loss: 28.8633, MinusLogProbMetric: 28.8633, val_loss: 29.5944, val_MinusLogProbMetric: 29.5944

Epoch 634: val_loss did not improve from 29.35391
196/196 - 89s - loss: 28.8633 - MinusLogProbMetric: 28.8633 - val_loss: 29.5944 - val_MinusLogProbMetric: 29.5944 - lr: 5.5556e-05 - 89s/epoch - 452ms/step
Epoch 635/1000
2023-09-30 20:16:29.127 
Epoch 635/1000 
	 loss: 28.8914, MinusLogProbMetric: 28.8914, val_loss: 29.4866, val_MinusLogProbMetric: 29.4866

Epoch 635: val_loss did not improve from 29.35391
196/196 - 94s - loss: 28.8914 - MinusLogProbMetric: 28.8914 - val_loss: 29.4866 - val_MinusLogProbMetric: 29.4866 - lr: 5.5556e-05 - 94s/epoch - 478ms/step
Epoch 636/1000
2023-09-30 20:17:59.741 
Epoch 636/1000 
	 loss: 28.8746, MinusLogProbMetric: 28.8746, val_loss: 29.4582, val_MinusLogProbMetric: 29.4582

Epoch 636: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8746 - MinusLogProbMetric: 28.8746 - val_loss: 29.4582 - val_MinusLogProbMetric: 29.4582 - lr: 5.5556e-05 - 91s/epoch - 462ms/step
Epoch 637/1000
2023-09-30 20:19:29.641 
Epoch 637/1000 
	 loss: 28.8505, MinusLogProbMetric: 28.8505, val_loss: 29.3987, val_MinusLogProbMetric: 29.3987

Epoch 637: val_loss did not improve from 29.35391
196/196 - 90s - loss: 28.8505 - MinusLogProbMetric: 28.8505 - val_loss: 29.3987 - val_MinusLogProbMetric: 29.3987 - lr: 5.5556e-05 - 90s/epoch - 459ms/step
Epoch 638/1000
2023-09-30 20:21:05.353 
Epoch 638/1000 
	 loss: 28.8456, MinusLogProbMetric: 28.8456, val_loss: 29.4480, val_MinusLogProbMetric: 29.4480

Epoch 638: val_loss did not improve from 29.35391
196/196 - 96s - loss: 28.8456 - MinusLogProbMetric: 28.8456 - val_loss: 29.4480 - val_MinusLogProbMetric: 29.4480 - lr: 5.5556e-05 - 96s/epoch - 488ms/step
Epoch 639/1000
2023-09-30 20:22:35.512 
Epoch 639/1000 
	 loss: 28.8655, MinusLogProbMetric: 28.8655, val_loss: 29.4900, val_MinusLogProbMetric: 29.4900

Epoch 639: val_loss did not improve from 29.35391
196/196 - 90s - loss: 28.8655 - MinusLogProbMetric: 28.8655 - val_loss: 29.4900 - val_MinusLogProbMetric: 29.4900 - lr: 5.5556e-05 - 90s/epoch - 460ms/step
Epoch 640/1000
2023-09-30 20:24:13.441 
Epoch 640/1000 
	 loss: 28.8506, MinusLogProbMetric: 28.8506, val_loss: 29.3900, val_MinusLogProbMetric: 29.3900

Epoch 640: val_loss did not improve from 29.35391
196/196 - 98s - loss: 28.8506 - MinusLogProbMetric: 28.8506 - val_loss: 29.3900 - val_MinusLogProbMetric: 29.3900 - lr: 5.5556e-05 - 98s/epoch - 500ms/step
Epoch 641/1000
2023-09-30 20:25:54.153 
Epoch 641/1000 
	 loss: 28.8624, MinusLogProbMetric: 28.8624, val_loss: 29.4809, val_MinusLogProbMetric: 29.4809

Epoch 641: val_loss did not improve from 29.35391
196/196 - 101s - loss: 28.8624 - MinusLogProbMetric: 28.8624 - val_loss: 29.4809 - val_MinusLogProbMetric: 29.4809 - lr: 5.5556e-05 - 101s/epoch - 514ms/step
Epoch 642/1000
2023-09-30 20:27:27.693 
Epoch 642/1000 
	 loss: 28.8826, MinusLogProbMetric: 28.8826, val_loss: 29.4943, val_MinusLogProbMetric: 29.4943

Epoch 642: val_loss did not improve from 29.35391
196/196 - 94s - loss: 28.8826 - MinusLogProbMetric: 28.8826 - val_loss: 29.4943 - val_MinusLogProbMetric: 29.4943 - lr: 5.5556e-05 - 94s/epoch - 477ms/step
Epoch 643/1000
2023-09-30 20:28:57.360 
Epoch 643/1000 
	 loss: 28.8484, MinusLogProbMetric: 28.8484, val_loss: 29.3619, val_MinusLogProbMetric: 29.3619

Epoch 643: val_loss did not improve from 29.35391
196/196 - 90s - loss: 28.8484 - MinusLogProbMetric: 28.8484 - val_loss: 29.3619 - val_MinusLogProbMetric: 29.3619 - lr: 5.5556e-05 - 90s/epoch - 457ms/step
Epoch 644/1000
2023-09-30 20:30:28.332 
Epoch 644/1000 
	 loss: 28.8524, MinusLogProbMetric: 28.8524, val_loss: 29.5742, val_MinusLogProbMetric: 29.5742

Epoch 644: val_loss did not improve from 29.35391
196/196 - 91s - loss: 28.8524 - MinusLogProbMetric: 28.8524 - val_loss: 29.5742 - val_MinusLogProbMetric: 29.5742 - lr: 5.5556e-05 - 91s/epoch - 464ms/step
Epoch 645/1000
2023-09-30 20:32:00.925 
Epoch 645/1000 
	 loss: 28.8775, MinusLogProbMetric: 28.8775, val_loss: 29.4762, val_MinusLogProbMetric: 29.4762

Epoch 645: val_loss did not improve from 29.35391
196/196 - 93s - loss: 28.8775 - MinusLogProbMetric: 28.8775 - val_loss: 29.4762 - val_MinusLogProbMetric: 29.4762 - lr: 5.5556e-05 - 93s/epoch - 472ms/step
Epoch 646/1000
2023-09-30 20:33:41.936 
Epoch 646/1000 
	 loss: 28.8440, MinusLogProbMetric: 28.8440, val_loss: 29.5423, val_MinusLogProbMetric: 29.5423

Epoch 646: val_loss did not improve from 29.35391
196/196 - 101s - loss: 28.8440 - MinusLogProbMetric: 28.8440 - val_loss: 29.5423 - val_MinusLogProbMetric: 29.5423 - lr: 5.5556e-05 - 101s/epoch - 515ms/step
Epoch 647/1000
2023-09-30 20:35:13.729 
Epoch 647/1000 
	 loss: 28.7397, MinusLogProbMetric: 28.7397, val_loss: 29.2957, val_MinusLogProbMetric: 29.2957

Epoch 647: val_loss improved from 29.35391 to 29.29569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 95s - loss: 28.7397 - MinusLogProbMetric: 28.7397 - val_loss: 29.2957 - val_MinusLogProbMetric: 29.2957 - lr: 2.7778e-05 - 95s/epoch - 482ms/step
Epoch 648/1000
2023-09-30 20:36:43.952 
Epoch 648/1000 
	 loss: 28.7218, MinusLogProbMetric: 28.7218, val_loss: 29.3418, val_MinusLogProbMetric: 29.3418

Epoch 648: val_loss did not improve from 29.29569
196/196 - 87s - loss: 28.7218 - MinusLogProbMetric: 28.7218 - val_loss: 29.3418 - val_MinusLogProbMetric: 29.3418 - lr: 2.7778e-05 - 87s/epoch - 446ms/step
Epoch 649/1000
2023-09-30 20:38:10.041 
Epoch 649/1000 
	 loss: 28.7312, MinusLogProbMetric: 28.7312, val_loss: 29.3045, val_MinusLogProbMetric: 29.3045

Epoch 649: val_loss did not improve from 29.29569
196/196 - 86s - loss: 28.7312 - MinusLogProbMetric: 28.7312 - val_loss: 29.3045 - val_MinusLogProbMetric: 29.3045 - lr: 2.7778e-05 - 86s/epoch - 439ms/step
Epoch 650/1000
2023-09-30 20:39:42.864 
Epoch 650/1000 
	 loss: 28.7255, MinusLogProbMetric: 28.7255, val_loss: 29.2938, val_MinusLogProbMetric: 29.2938

Epoch 650: val_loss improved from 29.29569 to 29.29383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 95s - loss: 28.7255 - MinusLogProbMetric: 28.7255 - val_loss: 29.2938 - val_MinusLogProbMetric: 29.2938 - lr: 2.7778e-05 - 95s/epoch - 486ms/step
Epoch 651/1000
2023-09-30 20:41:19.511 
Epoch 651/1000 
	 loss: 28.7194, MinusLogProbMetric: 28.7194, val_loss: 29.2834, val_MinusLogProbMetric: 29.2834

Epoch 651: val_loss improved from 29.29383 to 29.28340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 97s - loss: 28.7194 - MinusLogProbMetric: 28.7194 - val_loss: 29.2834 - val_MinusLogProbMetric: 29.2834 - lr: 2.7778e-05 - 97s/epoch - 493ms/step
Epoch 652/1000
2023-09-30 20:42:54.472 
Epoch 652/1000 
	 loss: 28.7310, MinusLogProbMetric: 28.7310, val_loss: 29.4939, val_MinusLogProbMetric: 29.4939

Epoch 652: val_loss did not improve from 29.28340
196/196 - 93s - loss: 28.7310 - MinusLogProbMetric: 28.7310 - val_loss: 29.4939 - val_MinusLogProbMetric: 29.4939 - lr: 2.7778e-05 - 93s/epoch - 472ms/step
Epoch 653/1000
2023-09-30 20:44:27.003 
Epoch 653/1000 
	 loss: 28.7422, MinusLogProbMetric: 28.7422, val_loss: 29.3851, val_MinusLogProbMetric: 29.3851

Epoch 653: val_loss did not improve from 29.28340
196/196 - 93s - loss: 28.7422 - MinusLogProbMetric: 28.7422 - val_loss: 29.3851 - val_MinusLogProbMetric: 29.3851 - lr: 2.7778e-05 - 93s/epoch - 472ms/step
Epoch 654/1000
2023-09-30 20:46:00.613 
Epoch 654/1000 
	 loss: 28.7266, MinusLogProbMetric: 28.7266, val_loss: 29.3515, val_MinusLogProbMetric: 29.3515

Epoch 654: val_loss did not improve from 29.28340
196/196 - 94s - loss: 28.7266 - MinusLogProbMetric: 28.7266 - val_loss: 29.3515 - val_MinusLogProbMetric: 29.3515 - lr: 2.7778e-05 - 94s/epoch - 478ms/step
Epoch 655/1000
2023-09-30 20:47:26.053 
Epoch 655/1000 
	 loss: 28.7253, MinusLogProbMetric: 28.7253, val_loss: 29.3056, val_MinusLogProbMetric: 29.3056

Epoch 655: val_loss did not improve from 29.28340
196/196 - 85s - loss: 28.7253 - MinusLogProbMetric: 28.7253 - val_loss: 29.3056 - val_MinusLogProbMetric: 29.3056 - lr: 2.7778e-05 - 85s/epoch - 436ms/step
Epoch 656/1000
2023-09-30 20:48:51.115 
Epoch 656/1000 
	 loss: 28.7253, MinusLogProbMetric: 28.7253, val_loss: 29.3045, val_MinusLogProbMetric: 29.3045

Epoch 656: val_loss did not improve from 29.28340
196/196 - 85s - loss: 28.7253 - MinusLogProbMetric: 28.7253 - val_loss: 29.3045 - val_MinusLogProbMetric: 29.3045 - lr: 2.7778e-05 - 85s/epoch - 434ms/step
Epoch 657/1000
2023-09-30 20:50:15.724 
Epoch 657/1000 
	 loss: 28.7252, MinusLogProbMetric: 28.7252, val_loss: 29.3775, val_MinusLogProbMetric: 29.3775

Epoch 657: val_loss did not improve from 29.28340
196/196 - 85s - loss: 28.7252 - MinusLogProbMetric: 28.7252 - val_loss: 29.3775 - val_MinusLogProbMetric: 29.3775 - lr: 2.7778e-05 - 85s/epoch - 432ms/step
Epoch 658/1000
2023-09-30 20:51:44.133 
Epoch 658/1000 
	 loss: 28.7343, MinusLogProbMetric: 28.7343, val_loss: 29.3128, val_MinusLogProbMetric: 29.3128

Epoch 658: val_loss did not improve from 29.28340
196/196 - 88s - loss: 28.7343 - MinusLogProbMetric: 28.7343 - val_loss: 29.3128 - val_MinusLogProbMetric: 29.3128 - lr: 2.7778e-05 - 88s/epoch - 451ms/step
Epoch 659/1000
2023-09-30 20:53:07.196 
Epoch 659/1000 
	 loss: 28.7278, MinusLogProbMetric: 28.7278, val_loss: 29.2957, val_MinusLogProbMetric: 29.2957

Epoch 659: val_loss did not improve from 29.28340
196/196 - 83s - loss: 28.7278 - MinusLogProbMetric: 28.7278 - val_loss: 29.2957 - val_MinusLogProbMetric: 29.2957 - lr: 2.7778e-05 - 83s/epoch - 424ms/step
Epoch 660/1000
2023-09-30 20:54:36.664 
Epoch 660/1000 
	 loss: 28.7189, MinusLogProbMetric: 28.7189, val_loss: 29.3379, val_MinusLogProbMetric: 29.3379

Epoch 660: val_loss did not improve from 29.28340
196/196 - 89s - loss: 28.7189 - MinusLogProbMetric: 28.7189 - val_loss: 29.3379 - val_MinusLogProbMetric: 29.3379 - lr: 2.7778e-05 - 89s/epoch - 456ms/step
Epoch 661/1000
2023-09-30 20:56:07.572 
Epoch 661/1000 
	 loss: 28.7163, MinusLogProbMetric: 28.7163, val_loss: 29.3355, val_MinusLogProbMetric: 29.3355

Epoch 661: val_loss did not improve from 29.28340
196/196 - 91s - loss: 28.7163 - MinusLogProbMetric: 28.7163 - val_loss: 29.3355 - val_MinusLogProbMetric: 29.3355 - lr: 2.7778e-05 - 91s/epoch - 464ms/step
Epoch 662/1000
2023-09-30 20:57:40.457 
Epoch 662/1000 
	 loss: 28.7169, MinusLogProbMetric: 28.7169, val_loss: 29.3263, val_MinusLogProbMetric: 29.3263

Epoch 662: val_loss did not improve from 29.28340
196/196 - 93s - loss: 28.7169 - MinusLogProbMetric: 28.7169 - val_loss: 29.3263 - val_MinusLogProbMetric: 29.3263 - lr: 2.7778e-05 - 93s/epoch - 474ms/step
Epoch 663/1000
2023-09-30 20:59:10.240 
Epoch 663/1000 
	 loss: 28.7178, MinusLogProbMetric: 28.7178, val_loss: 29.2968, val_MinusLogProbMetric: 29.2968

Epoch 663: val_loss did not improve from 29.28340
196/196 - 90s - loss: 28.7178 - MinusLogProbMetric: 28.7178 - val_loss: 29.2968 - val_MinusLogProbMetric: 29.2968 - lr: 2.7778e-05 - 90s/epoch - 458ms/step
Epoch 664/1000
2023-09-30 21:00:40.331 
Epoch 664/1000 
	 loss: 28.7177, MinusLogProbMetric: 28.7177, val_loss: 29.2791, val_MinusLogProbMetric: 29.2791

Epoch 664: val_loss improved from 29.28340 to 29.27909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 92s - loss: 28.7177 - MinusLogProbMetric: 28.7177 - val_loss: 29.2791 - val_MinusLogProbMetric: 29.2791 - lr: 2.7778e-05 - 92s/epoch - 470ms/step
Epoch 665/1000
2023-09-30 21:02:14.668 
Epoch 665/1000 
	 loss: 28.7142, MinusLogProbMetric: 28.7142, val_loss: 29.3461, val_MinusLogProbMetric: 29.3461

Epoch 665: val_loss did not improve from 29.27909
196/196 - 92s - loss: 28.7142 - MinusLogProbMetric: 28.7142 - val_loss: 29.3461 - val_MinusLogProbMetric: 29.3461 - lr: 2.7778e-05 - 92s/epoch - 471ms/step
Epoch 666/1000
2023-09-30 21:03:44.332 
Epoch 666/1000 
	 loss: 28.7277, MinusLogProbMetric: 28.7277, val_loss: 29.2784, val_MinusLogProbMetric: 29.2784

Epoch 666: val_loss improved from 29.27909 to 29.27840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 91s - loss: 28.7277 - MinusLogProbMetric: 28.7277 - val_loss: 29.2784 - val_MinusLogProbMetric: 29.2784 - lr: 2.7778e-05 - 91s/epoch - 466ms/step
Epoch 667/1000
2023-09-30 21:05:16.190 
Epoch 667/1000 
	 loss: 28.7169, MinusLogProbMetric: 28.7169, val_loss: 29.3645, val_MinusLogProbMetric: 29.3645

Epoch 667: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7169 - MinusLogProbMetric: 28.7169 - val_loss: 29.3645 - val_MinusLogProbMetric: 29.3645 - lr: 2.7778e-05 - 90s/epoch - 460ms/step
Epoch 668/1000
2023-09-30 21:06:46.432 
Epoch 668/1000 
	 loss: 28.7235, MinusLogProbMetric: 28.7235, val_loss: 29.3509, val_MinusLogProbMetric: 29.3509

Epoch 668: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7235 - MinusLogProbMetric: 28.7235 - val_loss: 29.3509 - val_MinusLogProbMetric: 29.3509 - lr: 2.7778e-05 - 90s/epoch - 460ms/step
Epoch 669/1000
2023-09-30 21:08:15.527 
Epoch 669/1000 
	 loss: 28.7274, MinusLogProbMetric: 28.7274, val_loss: 29.2821, val_MinusLogProbMetric: 29.2821

Epoch 669: val_loss did not improve from 29.27840
196/196 - 89s - loss: 28.7274 - MinusLogProbMetric: 28.7274 - val_loss: 29.2821 - val_MinusLogProbMetric: 29.2821 - lr: 2.7778e-05 - 89s/epoch - 454ms/step
Epoch 670/1000
2023-09-30 21:09:45.469 
Epoch 670/1000 
	 loss: 28.7229, MinusLogProbMetric: 28.7229, val_loss: 29.3011, val_MinusLogProbMetric: 29.3011

Epoch 670: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7229 - MinusLogProbMetric: 28.7229 - val_loss: 29.3011 - val_MinusLogProbMetric: 29.3011 - lr: 2.7778e-05 - 90s/epoch - 459ms/step
Epoch 671/1000
2023-09-30 21:11:12.290 
Epoch 671/1000 
	 loss: 28.7208, MinusLogProbMetric: 28.7208, val_loss: 29.3069, val_MinusLogProbMetric: 29.3069

Epoch 671: val_loss did not improve from 29.27840
196/196 - 87s - loss: 28.7208 - MinusLogProbMetric: 28.7208 - val_loss: 29.3069 - val_MinusLogProbMetric: 29.3069 - lr: 2.7778e-05 - 87s/epoch - 443ms/step
Epoch 672/1000
2023-09-30 21:12:38.053 
Epoch 672/1000 
	 loss: 28.7204, MinusLogProbMetric: 28.7204, val_loss: 29.3122, val_MinusLogProbMetric: 29.3122

Epoch 672: val_loss did not improve from 29.27840
196/196 - 86s - loss: 28.7204 - MinusLogProbMetric: 28.7204 - val_loss: 29.3122 - val_MinusLogProbMetric: 29.3122 - lr: 2.7778e-05 - 86s/epoch - 437ms/step
Epoch 673/1000
2023-09-30 21:14:06.159 
Epoch 673/1000 
	 loss: 28.7156, MinusLogProbMetric: 28.7156, val_loss: 29.3310, val_MinusLogProbMetric: 29.3310

Epoch 673: val_loss did not improve from 29.27840
196/196 - 88s - loss: 28.7156 - MinusLogProbMetric: 28.7156 - val_loss: 29.3310 - val_MinusLogProbMetric: 29.3310 - lr: 2.7778e-05 - 88s/epoch - 450ms/step
Epoch 674/1000
2023-09-30 21:15:34.267 
Epoch 674/1000 
	 loss: 28.7209, MinusLogProbMetric: 28.7209, val_loss: 29.3697, val_MinusLogProbMetric: 29.3697

Epoch 674: val_loss did not improve from 29.27840
196/196 - 88s - loss: 28.7209 - MinusLogProbMetric: 28.7209 - val_loss: 29.3697 - val_MinusLogProbMetric: 29.3697 - lr: 2.7778e-05 - 88s/epoch - 449ms/step
Epoch 675/1000
2023-09-30 21:17:07.729 
Epoch 675/1000 
	 loss: 28.7150, MinusLogProbMetric: 28.7150, val_loss: 29.3556, val_MinusLogProbMetric: 29.3556

Epoch 675: val_loss did not improve from 29.27840
196/196 - 93s - loss: 28.7150 - MinusLogProbMetric: 28.7150 - val_loss: 29.3556 - val_MinusLogProbMetric: 29.3556 - lr: 2.7778e-05 - 93s/epoch - 477ms/step
Epoch 676/1000
2023-09-30 21:18:37.463 
Epoch 676/1000 
	 loss: 28.7173, MinusLogProbMetric: 28.7173, val_loss: 29.3232, val_MinusLogProbMetric: 29.3232

Epoch 676: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7173 - MinusLogProbMetric: 28.7173 - val_loss: 29.3232 - val_MinusLogProbMetric: 29.3232 - lr: 2.7778e-05 - 90s/epoch - 458ms/step
Epoch 677/1000
2023-09-30 21:20:02.928 
Epoch 677/1000 
	 loss: 28.7150, MinusLogProbMetric: 28.7150, val_loss: 29.3414, val_MinusLogProbMetric: 29.3414

Epoch 677: val_loss did not improve from 29.27840
196/196 - 85s - loss: 28.7150 - MinusLogProbMetric: 28.7150 - val_loss: 29.3414 - val_MinusLogProbMetric: 29.3414 - lr: 2.7778e-05 - 85s/epoch - 436ms/step
Epoch 678/1000
2023-09-30 21:21:32.040 
Epoch 678/1000 
	 loss: 28.7246, MinusLogProbMetric: 28.7246, val_loss: 29.3266, val_MinusLogProbMetric: 29.3266

Epoch 678: val_loss did not improve from 29.27840
196/196 - 89s - loss: 28.7246 - MinusLogProbMetric: 28.7246 - val_loss: 29.3266 - val_MinusLogProbMetric: 29.3266 - lr: 2.7778e-05 - 89s/epoch - 454ms/step
Epoch 679/1000
2023-09-30 21:23:04.196 
Epoch 679/1000 
	 loss: 28.7205, MinusLogProbMetric: 28.7205, val_loss: 29.3440, val_MinusLogProbMetric: 29.3440

Epoch 679: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7205 - MinusLogProbMetric: 28.7205 - val_loss: 29.3440 - val_MinusLogProbMetric: 29.3440 - lr: 2.7778e-05 - 92s/epoch - 470ms/step
Epoch 680/1000
2023-09-30 21:24:36.065 
Epoch 680/1000 
	 loss: 28.7259, MinusLogProbMetric: 28.7259, val_loss: 29.3882, val_MinusLogProbMetric: 29.3882

Epoch 680: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7259 - MinusLogProbMetric: 28.7259 - val_loss: 29.3882 - val_MinusLogProbMetric: 29.3882 - lr: 2.7778e-05 - 92s/epoch - 468ms/step
Epoch 681/1000
2023-09-30 21:26:07.397 
Epoch 681/1000 
	 loss: 28.7254, MinusLogProbMetric: 28.7254, val_loss: 29.3427, val_MinusLogProbMetric: 29.3427

Epoch 681: val_loss did not improve from 29.27840
196/196 - 91s - loss: 28.7254 - MinusLogProbMetric: 28.7254 - val_loss: 29.3427 - val_MinusLogProbMetric: 29.3427 - lr: 2.7778e-05 - 91s/epoch - 466ms/step
Epoch 682/1000
2023-09-30 21:27:39.662 
Epoch 682/1000 
	 loss: 28.7283, MinusLogProbMetric: 28.7283, val_loss: 29.3945, val_MinusLogProbMetric: 29.3945

Epoch 682: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7283 - MinusLogProbMetric: 28.7283 - val_loss: 29.3945 - val_MinusLogProbMetric: 29.3945 - lr: 2.7778e-05 - 92s/epoch - 471ms/step
Epoch 683/1000
2023-09-30 21:29:12.375 
Epoch 683/1000 
	 loss: 28.7221, MinusLogProbMetric: 28.7221, val_loss: 29.2809, val_MinusLogProbMetric: 29.2809

Epoch 683: val_loss did not improve from 29.27840
196/196 - 93s - loss: 28.7221 - MinusLogProbMetric: 28.7221 - val_loss: 29.2809 - val_MinusLogProbMetric: 29.2809 - lr: 2.7778e-05 - 93s/epoch - 473ms/step
Epoch 684/1000
2023-09-30 21:30:44.733 
Epoch 684/1000 
	 loss: 28.7212, MinusLogProbMetric: 28.7212, val_loss: 29.3057, val_MinusLogProbMetric: 29.3057

Epoch 684: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7212 - MinusLogProbMetric: 28.7212 - val_loss: 29.3057 - val_MinusLogProbMetric: 29.3057 - lr: 2.7778e-05 - 92s/epoch - 471ms/step
Epoch 685/1000
2023-09-30 21:32:18.628 
Epoch 685/1000 
	 loss: 28.7308, MinusLogProbMetric: 28.7308, val_loss: 29.4276, val_MinusLogProbMetric: 29.4276

Epoch 685: val_loss did not improve from 29.27840
196/196 - 94s - loss: 28.7308 - MinusLogProbMetric: 28.7308 - val_loss: 29.4276 - val_MinusLogProbMetric: 29.4276 - lr: 2.7778e-05 - 94s/epoch - 479ms/step
Epoch 686/1000
2023-09-30 21:33:48.054 
Epoch 686/1000 
	 loss: 28.7221, MinusLogProbMetric: 28.7221, val_loss: 29.3213, val_MinusLogProbMetric: 29.3213

Epoch 686: val_loss did not improve from 29.27840
196/196 - 89s - loss: 28.7221 - MinusLogProbMetric: 28.7221 - val_loss: 29.3213 - val_MinusLogProbMetric: 29.3213 - lr: 2.7778e-05 - 89s/epoch - 456ms/step
Epoch 687/1000
2023-09-30 21:35:14.629 
Epoch 687/1000 
	 loss: 28.7276, MinusLogProbMetric: 28.7276, val_loss: 29.3063, val_MinusLogProbMetric: 29.3063

Epoch 687: val_loss did not improve from 29.27840
196/196 - 87s - loss: 28.7276 - MinusLogProbMetric: 28.7276 - val_loss: 29.3063 - val_MinusLogProbMetric: 29.3063 - lr: 2.7778e-05 - 87s/epoch - 442ms/step
Epoch 688/1000
2023-09-30 21:36:44.690 
Epoch 688/1000 
	 loss: 28.7136, MinusLogProbMetric: 28.7136, val_loss: 29.3102, val_MinusLogProbMetric: 29.3102

Epoch 688: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7136 - MinusLogProbMetric: 28.7136 - val_loss: 29.3102 - val_MinusLogProbMetric: 29.3102 - lr: 2.7778e-05 - 90s/epoch - 459ms/step
Epoch 689/1000
2023-09-30 21:38:13.333 
Epoch 689/1000 
	 loss: 28.7216, MinusLogProbMetric: 28.7216, val_loss: 29.3254, val_MinusLogProbMetric: 29.3254

Epoch 689: val_loss did not improve from 29.27840
196/196 - 89s - loss: 28.7216 - MinusLogProbMetric: 28.7216 - val_loss: 29.3254 - val_MinusLogProbMetric: 29.3254 - lr: 2.7778e-05 - 89s/epoch - 452ms/step
Epoch 690/1000
2023-09-30 21:39:43.474 
Epoch 690/1000 
	 loss: 28.7216, MinusLogProbMetric: 28.7216, val_loss: 29.3228, val_MinusLogProbMetric: 29.3228

Epoch 690: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7216 - MinusLogProbMetric: 28.7216 - val_loss: 29.3228 - val_MinusLogProbMetric: 29.3228 - lr: 2.7778e-05 - 90s/epoch - 460ms/step
Epoch 691/1000
2023-09-30 21:41:14.552 
Epoch 691/1000 
	 loss: 28.7199, MinusLogProbMetric: 28.7199, val_loss: 29.3090, val_MinusLogProbMetric: 29.3090

Epoch 691: val_loss did not improve from 29.27840
196/196 - 91s - loss: 28.7199 - MinusLogProbMetric: 28.7199 - val_loss: 29.3090 - val_MinusLogProbMetric: 29.3090 - lr: 2.7778e-05 - 91s/epoch - 465ms/step
Epoch 692/1000
2023-09-30 21:42:42.294 
Epoch 692/1000 
	 loss: 28.7270, MinusLogProbMetric: 28.7270, val_loss: 29.4631, val_MinusLogProbMetric: 29.4631

Epoch 692: val_loss did not improve from 29.27840
196/196 - 88s - loss: 28.7270 - MinusLogProbMetric: 28.7270 - val_loss: 29.4631 - val_MinusLogProbMetric: 29.4631 - lr: 2.7778e-05 - 88s/epoch - 448ms/step
Epoch 693/1000
2023-09-30 21:44:15.282 
Epoch 693/1000 
	 loss: 28.7270, MinusLogProbMetric: 28.7270, val_loss: 29.3190, val_MinusLogProbMetric: 29.3190

Epoch 693: val_loss did not improve from 29.27840
196/196 - 93s - loss: 28.7270 - MinusLogProbMetric: 28.7270 - val_loss: 29.3190 - val_MinusLogProbMetric: 29.3190 - lr: 2.7778e-05 - 93s/epoch - 474ms/step
Epoch 694/1000
2023-09-30 21:45:42.704 
Epoch 694/1000 
	 loss: 28.7150, MinusLogProbMetric: 28.7150, val_loss: 29.3181, val_MinusLogProbMetric: 29.3181

Epoch 694: val_loss did not improve from 29.27840
196/196 - 87s - loss: 28.7150 - MinusLogProbMetric: 28.7150 - val_loss: 29.3181 - val_MinusLogProbMetric: 29.3181 - lr: 2.7778e-05 - 87s/epoch - 446ms/step
Epoch 695/1000
2023-09-30 21:47:08.073 
Epoch 695/1000 
	 loss: 28.7228, MinusLogProbMetric: 28.7228, val_loss: 29.3474, val_MinusLogProbMetric: 29.3474

Epoch 695: val_loss did not improve from 29.27840
196/196 - 85s - loss: 28.7228 - MinusLogProbMetric: 28.7228 - val_loss: 29.3474 - val_MinusLogProbMetric: 29.3474 - lr: 2.7778e-05 - 85s/epoch - 436ms/step
Epoch 696/1000
2023-09-30 21:48:40.145 
Epoch 696/1000 
	 loss: 28.7196, MinusLogProbMetric: 28.7196, val_loss: 29.3481, val_MinusLogProbMetric: 29.3481

Epoch 696: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7196 - MinusLogProbMetric: 28.7196 - val_loss: 29.3481 - val_MinusLogProbMetric: 29.3481 - lr: 2.7778e-05 - 92s/epoch - 470ms/step
Epoch 697/1000
2023-09-30 21:50:07.971 
Epoch 697/1000 
	 loss: 28.7218, MinusLogProbMetric: 28.7218, val_loss: 29.3810, val_MinusLogProbMetric: 29.3810

Epoch 697: val_loss did not improve from 29.27840
196/196 - 88s - loss: 28.7218 - MinusLogProbMetric: 28.7218 - val_loss: 29.3810 - val_MinusLogProbMetric: 29.3810 - lr: 2.7778e-05 - 88s/epoch - 448ms/step
Epoch 698/1000
2023-09-30 21:51:36.536 
Epoch 698/1000 
	 loss: 28.7179, MinusLogProbMetric: 28.7179, val_loss: 29.2910, val_MinusLogProbMetric: 29.2910

Epoch 698: val_loss did not improve from 29.27840
196/196 - 89s - loss: 28.7179 - MinusLogProbMetric: 28.7179 - val_loss: 29.2910 - val_MinusLogProbMetric: 29.2910 - lr: 2.7778e-05 - 89s/epoch - 452ms/step
Epoch 699/1000
2023-09-30 21:53:08.638 
Epoch 699/1000 
	 loss: 28.7249, MinusLogProbMetric: 28.7249, val_loss: 29.4093, val_MinusLogProbMetric: 29.4093

Epoch 699: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7249 - MinusLogProbMetric: 28.7249 - val_loss: 29.4093 - val_MinusLogProbMetric: 29.4093 - lr: 2.7778e-05 - 92s/epoch - 470ms/step
Epoch 700/1000
2023-09-30 21:54:36.054 
Epoch 700/1000 
	 loss: 28.7190, MinusLogProbMetric: 28.7190, val_loss: 29.3903, val_MinusLogProbMetric: 29.3903

Epoch 700: val_loss did not improve from 29.27840
196/196 - 87s - loss: 28.7190 - MinusLogProbMetric: 28.7190 - val_loss: 29.3903 - val_MinusLogProbMetric: 29.3903 - lr: 2.7778e-05 - 87s/epoch - 446ms/step
Epoch 701/1000
2023-09-30 21:56:07.889 
Epoch 701/1000 
	 loss: 28.7207, MinusLogProbMetric: 28.7207, val_loss: 29.4035, val_MinusLogProbMetric: 29.4035

Epoch 701: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7207 - MinusLogProbMetric: 28.7207 - val_loss: 29.4035 - val_MinusLogProbMetric: 29.4035 - lr: 2.7778e-05 - 92s/epoch - 468ms/step
Epoch 702/1000
2023-09-30 21:57:35.752 
Epoch 702/1000 
	 loss: 28.7221, MinusLogProbMetric: 28.7221, val_loss: 29.3456, val_MinusLogProbMetric: 29.3456

Epoch 702: val_loss did not improve from 29.27840
196/196 - 88s - loss: 28.7221 - MinusLogProbMetric: 28.7221 - val_loss: 29.3456 - val_MinusLogProbMetric: 29.3456 - lr: 2.7778e-05 - 88s/epoch - 448ms/step
Epoch 703/1000
2023-09-30 21:59:05.897 
Epoch 703/1000 
	 loss: 28.7146, MinusLogProbMetric: 28.7146, val_loss: 29.2977, val_MinusLogProbMetric: 29.2977

Epoch 703: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7146 - MinusLogProbMetric: 28.7146 - val_loss: 29.2977 - val_MinusLogProbMetric: 29.2977 - lr: 2.7778e-05 - 90s/epoch - 460ms/step
Epoch 704/1000
2023-09-30 22:00:37.613 
Epoch 704/1000 
	 loss: 28.7127, MinusLogProbMetric: 28.7127, val_loss: 29.2948, val_MinusLogProbMetric: 29.2948

Epoch 704: val_loss did not improve from 29.27840
196/196 - 92s - loss: 28.7127 - MinusLogProbMetric: 28.7127 - val_loss: 29.2948 - val_MinusLogProbMetric: 29.2948 - lr: 2.7778e-05 - 92s/epoch - 468ms/step
Epoch 705/1000
2023-09-30 22:02:07.176 
Epoch 705/1000 
	 loss: 28.7133, MinusLogProbMetric: 28.7133, val_loss: 29.3383, val_MinusLogProbMetric: 29.3383

Epoch 705: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7133 - MinusLogProbMetric: 28.7133 - val_loss: 29.3383 - val_MinusLogProbMetric: 29.3383 - lr: 2.7778e-05 - 90s/epoch - 457ms/step
Epoch 706/1000
2023-09-30 22:03:35.798 
Epoch 706/1000 
	 loss: 28.7200, MinusLogProbMetric: 28.7200, val_loss: 29.3591, val_MinusLogProbMetric: 29.3591

Epoch 706: val_loss did not improve from 29.27840
196/196 - 89s - loss: 28.7200 - MinusLogProbMetric: 28.7200 - val_loss: 29.3591 - val_MinusLogProbMetric: 29.3591 - lr: 2.7778e-05 - 89s/epoch - 452ms/step
Epoch 707/1000
2023-09-30 22:05:05.365 
Epoch 707/1000 
	 loss: 28.7288, MinusLogProbMetric: 28.7288, val_loss: 29.2859, val_MinusLogProbMetric: 29.2859

Epoch 707: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7288 - MinusLogProbMetric: 28.7288 - val_loss: 29.2859 - val_MinusLogProbMetric: 29.2859 - lr: 2.7778e-05 - 90s/epoch - 457ms/step
Epoch 708/1000
2023-09-30 22:06:35.794 
Epoch 708/1000 
	 loss: 28.7105, MinusLogProbMetric: 28.7105, val_loss: 29.3193, val_MinusLogProbMetric: 29.3193

Epoch 708: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7105 - MinusLogProbMetric: 28.7105 - val_loss: 29.3193 - val_MinusLogProbMetric: 29.3193 - lr: 2.7778e-05 - 90s/epoch - 462ms/step
Epoch 709/1000
2023-09-30 22:08:03.924 
Epoch 709/1000 
	 loss: 28.7115, MinusLogProbMetric: 28.7115, val_loss: 29.3328, val_MinusLogProbMetric: 29.3328

Epoch 709: val_loss did not improve from 29.27840
196/196 - 88s - loss: 28.7115 - MinusLogProbMetric: 28.7115 - val_loss: 29.3328 - val_MinusLogProbMetric: 29.3328 - lr: 2.7778e-05 - 88s/epoch - 449ms/step
Epoch 710/1000
2023-09-30 22:09:33.646 
Epoch 710/1000 
	 loss: 28.7257, MinusLogProbMetric: 28.7257, val_loss: 29.3040, val_MinusLogProbMetric: 29.3040

Epoch 710: val_loss did not improve from 29.27840
196/196 - 90s - loss: 28.7257 - MinusLogProbMetric: 28.7257 - val_loss: 29.3040 - val_MinusLogProbMetric: 29.3040 - lr: 2.7778e-05 - 90s/epoch - 458ms/step
Epoch 711/1000
2023-09-30 22:11:07.113 
Epoch 711/1000 
	 loss: 28.7280, MinusLogProbMetric: 28.7280, val_loss: 29.2885, val_MinusLogProbMetric: 29.2885

Epoch 711: val_loss did not improve from 29.27840
196/196 - 93s - loss: 28.7280 - MinusLogProbMetric: 28.7280 - val_loss: 29.2885 - val_MinusLogProbMetric: 29.2885 - lr: 2.7778e-05 - 93s/epoch - 477ms/step
Epoch 712/1000
2023-09-30 22:12:34.338 
Epoch 712/1000 
	 loss: 28.7213, MinusLogProbMetric: 28.7213, val_loss: 29.3305, val_MinusLogProbMetric: 29.3305

Epoch 712: val_loss did not improve from 29.27840
196/196 - 87s - loss: 28.7213 - MinusLogProbMetric: 28.7213 - val_loss: 29.3305 - val_MinusLogProbMetric: 29.3305 - lr: 2.7778e-05 - 87s/epoch - 445ms/step
Epoch 713/1000
2023-09-30 22:14:05.101 
Epoch 713/1000 
	 loss: 28.7071, MinusLogProbMetric: 28.7071, val_loss: 29.3148, val_MinusLogProbMetric: 29.3148

Epoch 713: val_loss did not improve from 29.27840
196/196 - 91s - loss: 28.7071 - MinusLogProbMetric: 28.7071 - val_loss: 29.3148 - val_MinusLogProbMetric: 29.3148 - lr: 2.7778e-05 - 91s/epoch - 463ms/step
Epoch 714/1000
2023-09-30 22:15:34.307 
Epoch 714/1000 
	 loss: 28.7127, MinusLogProbMetric: 28.7127, val_loss: 29.3605, val_MinusLogProbMetric: 29.3605

Epoch 714: val_loss did not improve from 29.27840
196/196 - 89s - loss: 28.7127 - MinusLogProbMetric: 28.7127 - val_loss: 29.3605 - val_MinusLogProbMetric: 29.3605 - lr: 2.7778e-05 - 89s/epoch - 455ms/step
Epoch 715/1000
2023-09-30 22:17:02.499 
Epoch 715/1000 
	 loss: 28.7035, MinusLogProbMetric: 28.7035, val_loss: 29.3209, val_MinusLogProbMetric: 29.3209

Epoch 715: val_loss did not improve from 29.27840
196/196 - 88s - loss: 28.7035 - MinusLogProbMetric: 28.7035 - val_loss: 29.3209 - val_MinusLogProbMetric: 29.3209 - lr: 2.7778e-05 - 88s/epoch - 450ms/step
Epoch 716/1000
2023-09-30 22:18:36.934 
Epoch 716/1000 
	 loss: 28.7116, MinusLogProbMetric: 28.7116, val_loss: 29.3619, val_MinusLogProbMetric: 29.3619

Epoch 716: val_loss did not improve from 29.27840
196/196 - 94s - loss: 28.7116 - MinusLogProbMetric: 28.7116 - val_loss: 29.3619 - val_MinusLogProbMetric: 29.3619 - lr: 2.7778e-05 - 94s/epoch - 482ms/step
Epoch 717/1000
2023-09-30 22:20:06.999 
Epoch 717/1000 
	 loss: 28.6615, MinusLogProbMetric: 28.6615, val_loss: 29.2524, val_MinusLogProbMetric: 29.2524

Epoch 717: val_loss improved from 29.27840 to 29.25243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 93s - loss: 28.6615 - MinusLogProbMetric: 28.6615 - val_loss: 29.2524 - val_MinusLogProbMetric: 29.2524 - lr: 1.3889e-05 - 93s/epoch - 473ms/step
Epoch 718/1000
2023-09-30 22:21:36.480 
Epoch 718/1000 
	 loss: 28.6546, MinusLogProbMetric: 28.6546, val_loss: 29.2415, val_MinusLogProbMetric: 29.2415

Epoch 718: val_loss improved from 29.25243 to 29.24149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 90s - loss: 28.6546 - MinusLogProbMetric: 28.6546 - val_loss: 29.2415 - val_MinusLogProbMetric: 29.2415 - lr: 1.3889e-05 - 90s/epoch - 457ms/step
Epoch 719/1000
2023-09-30 22:23:05.678 
Epoch 719/1000 
	 loss: 28.6560, MinusLogProbMetric: 28.6560, val_loss: 29.2547, val_MinusLogProbMetric: 29.2547

Epoch 719: val_loss did not improve from 29.24149
196/196 - 86s - loss: 28.6560 - MinusLogProbMetric: 28.6560 - val_loss: 29.2547 - val_MinusLogProbMetric: 29.2547 - lr: 1.3889e-05 - 86s/epoch - 441ms/step
Epoch 720/1000
2023-09-30 22:24:34.838 
Epoch 720/1000 
	 loss: 28.6578, MinusLogProbMetric: 28.6578, val_loss: 29.2807, val_MinusLogProbMetric: 29.2807

Epoch 720: val_loss did not improve from 29.24149
196/196 - 89s - loss: 28.6578 - MinusLogProbMetric: 28.6578 - val_loss: 29.2807 - val_MinusLogProbMetric: 29.2807 - lr: 1.3889e-05 - 89s/epoch - 455ms/step
Epoch 721/1000
2023-09-30 22:26:02.772 
Epoch 721/1000 
	 loss: 28.6581, MinusLogProbMetric: 28.6581, val_loss: 29.2654, val_MinusLogProbMetric: 29.2654

Epoch 721: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6581 - MinusLogProbMetric: 28.6581 - val_loss: 29.2654 - val_MinusLogProbMetric: 29.2654 - lr: 1.3889e-05 - 88s/epoch - 449ms/step
Epoch 722/1000
2023-09-30 22:27:32.206 
Epoch 722/1000 
	 loss: 28.6552, MinusLogProbMetric: 28.6552, val_loss: 29.2677, val_MinusLogProbMetric: 29.2677

Epoch 722: val_loss did not improve from 29.24149
196/196 - 89s - loss: 28.6552 - MinusLogProbMetric: 28.6552 - val_loss: 29.2677 - val_MinusLogProbMetric: 29.2677 - lr: 1.3889e-05 - 89s/epoch - 456ms/step
Epoch 723/1000
2023-09-30 22:28:59.729 
Epoch 723/1000 
	 loss: 28.6573, MinusLogProbMetric: 28.6573, val_loss: 29.2791, val_MinusLogProbMetric: 29.2791

Epoch 723: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6573 - MinusLogProbMetric: 28.6573 - val_loss: 29.2791 - val_MinusLogProbMetric: 29.2791 - lr: 1.3889e-05 - 88s/epoch - 447ms/step
Epoch 724/1000
2023-09-30 22:30:28.303 
Epoch 724/1000 
	 loss: 28.6596, MinusLogProbMetric: 28.6596, val_loss: 29.2850, val_MinusLogProbMetric: 29.2850

Epoch 724: val_loss did not improve from 29.24149
196/196 - 89s - loss: 28.6596 - MinusLogProbMetric: 28.6596 - val_loss: 29.2850 - val_MinusLogProbMetric: 29.2850 - lr: 1.3889e-05 - 89s/epoch - 452ms/step
Epoch 725/1000
2023-09-30 22:32:00.767 
Epoch 725/1000 
	 loss: 28.6584, MinusLogProbMetric: 28.6584, val_loss: 29.2580, val_MinusLogProbMetric: 29.2580

Epoch 725: val_loss did not improve from 29.24149
196/196 - 92s - loss: 28.6584 - MinusLogProbMetric: 28.6584 - val_loss: 29.2580 - val_MinusLogProbMetric: 29.2580 - lr: 1.3889e-05 - 92s/epoch - 472ms/step
Epoch 726/1000
2023-09-30 22:33:30.959 
Epoch 726/1000 
	 loss: 28.6597, MinusLogProbMetric: 28.6597, val_loss: 29.2646, val_MinusLogProbMetric: 29.2646

Epoch 726: val_loss did not improve from 29.24149
196/196 - 90s - loss: 28.6597 - MinusLogProbMetric: 28.6597 - val_loss: 29.2646 - val_MinusLogProbMetric: 29.2646 - lr: 1.3889e-05 - 90s/epoch - 460ms/step
Epoch 727/1000
2023-09-30 22:35:03.831 
Epoch 727/1000 
	 loss: 28.6569, MinusLogProbMetric: 28.6569, val_loss: 29.2812, val_MinusLogProbMetric: 29.2812

Epoch 727: val_loss did not improve from 29.24149
196/196 - 93s - loss: 28.6569 - MinusLogProbMetric: 28.6569 - val_loss: 29.2812 - val_MinusLogProbMetric: 29.2812 - lr: 1.3889e-05 - 93s/epoch - 474ms/step
Epoch 728/1000
2023-09-30 22:36:40.604 
Epoch 728/1000 
	 loss: 28.6538, MinusLogProbMetric: 28.6538, val_loss: 29.2664, val_MinusLogProbMetric: 29.2664

Epoch 728: val_loss did not improve from 29.24149
196/196 - 97s - loss: 28.6538 - MinusLogProbMetric: 28.6538 - val_loss: 29.2664 - val_MinusLogProbMetric: 29.2664 - lr: 1.3889e-05 - 97s/epoch - 494ms/step
Epoch 729/1000
2023-09-30 22:38:12.123 
Epoch 729/1000 
	 loss: 28.6538, MinusLogProbMetric: 28.6538, val_loss: 29.2652, val_MinusLogProbMetric: 29.2652

Epoch 729: val_loss did not improve from 29.24149
196/196 - 91s - loss: 28.6538 - MinusLogProbMetric: 28.6538 - val_loss: 29.2652 - val_MinusLogProbMetric: 29.2652 - lr: 1.3889e-05 - 91s/epoch - 467ms/step
Epoch 730/1000
2023-09-30 22:39:38.299 
Epoch 730/1000 
	 loss: 28.6608, MinusLogProbMetric: 28.6608, val_loss: 29.2668, val_MinusLogProbMetric: 29.2668

Epoch 730: val_loss did not improve from 29.24149
196/196 - 86s - loss: 28.6608 - MinusLogProbMetric: 28.6608 - val_loss: 29.2668 - val_MinusLogProbMetric: 29.2668 - lr: 1.3889e-05 - 86s/epoch - 440ms/step
Epoch 731/1000
2023-09-30 22:41:03.684 
Epoch 731/1000 
	 loss: 28.6585, MinusLogProbMetric: 28.6585, val_loss: 29.2616, val_MinusLogProbMetric: 29.2616

Epoch 731: val_loss did not improve from 29.24149
196/196 - 85s - loss: 28.6585 - MinusLogProbMetric: 28.6585 - val_loss: 29.2616 - val_MinusLogProbMetric: 29.2616 - lr: 1.3889e-05 - 85s/epoch - 435ms/step
Epoch 732/1000
2023-09-30 22:42:35.444 
Epoch 732/1000 
	 loss: 28.6560, MinusLogProbMetric: 28.6560, val_loss: 29.2699, val_MinusLogProbMetric: 29.2699

Epoch 732: val_loss did not improve from 29.24149
196/196 - 92s - loss: 28.6560 - MinusLogProbMetric: 28.6560 - val_loss: 29.2699 - val_MinusLogProbMetric: 29.2699 - lr: 1.3889e-05 - 92s/epoch - 468ms/step
Epoch 733/1000
2023-09-30 22:44:01.436 
Epoch 733/1000 
	 loss: 28.6509, MinusLogProbMetric: 28.6509, val_loss: 29.2447, val_MinusLogProbMetric: 29.2447

Epoch 733: val_loss did not improve from 29.24149
196/196 - 86s - loss: 28.6509 - MinusLogProbMetric: 28.6509 - val_loss: 29.2447 - val_MinusLogProbMetric: 29.2447 - lr: 1.3889e-05 - 86s/epoch - 439ms/step
Epoch 734/1000
2023-09-30 22:45:26.914 
Epoch 734/1000 
	 loss: 28.6530, MinusLogProbMetric: 28.6530, val_loss: 29.2955, val_MinusLogProbMetric: 29.2955

Epoch 734: val_loss did not improve from 29.24149
196/196 - 85s - loss: 28.6530 - MinusLogProbMetric: 28.6530 - val_loss: 29.2955 - val_MinusLogProbMetric: 29.2955 - lr: 1.3889e-05 - 85s/epoch - 436ms/step
Epoch 735/1000
2023-09-30 22:46:58.833 
Epoch 735/1000 
	 loss: 28.6535, MinusLogProbMetric: 28.6535, val_loss: 29.2765, val_MinusLogProbMetric: 29.2765

Epoch 735: val_loss did not improve from 29.24149
196/196 - 92s - loss: 28.6535 - MinusLogProbMetric: 28.6535 - val_loss: 29.2765 - val_MinusLogProbMetric: 29.2765 - lr: 1.3889e-05 - 92s/epoch - 469ms/step
Epoch 736/1000
2023-09-30 22:48:36.397 
Epoch 736/1000 
	 loss: 28.6530, MinusLogProbMetric: 28.6530, val_loss: 29.2866, val_MinusLogProbMetric: 29.2866

Epoch 736: val_loss did not improve from 29.24149
196/196 - 98s - loss: 28.6530 - MinusLogProbMetric: 28.6530 - val_loss: 29.2866 - val_MinusLogProbMetric: 29.2866 - lr: 1.3889e-05 - 98s/epoch - 498ms/step
Epoch 737/1000
2023-09-30 22:50:12.313 
Epoch 737/1000 
	 loss: 28.6523, MinusLogProbMetric: 28.6523, val_loss: 29.3057, val_MinusLogProbMetric: 29.3057

Epoch 737: val_loss did not improve from 29.24149
196/196 - 96s - loss: 28.6523 - MinusLogProbMetric: 28.6523 - val_loss: 29.3057 - val_MinusLogProbMetric: 29.3057 - lr: 1.3889e-05 - 96s/epoch - 489ms/step
Epoch 738/1000
2023-09-30 22:51:49.149 
Epoch 738/1000 
	 loss: 28.6579, MinusLogProbMetric: 28.6579, val_loss: 29.2628, val_MinusLogProbMetric: 29.2628

Epoch 738: val_loss did not improve from 29.24149
196/196 - 97s - loss: 28.6579 - MinusLogProbMetric: 28.6579 - val_loss: 29.2628 - val_MinusLogProbMetric: 29.2628 - lr: 1.3889e-05 - 97s/epoch - 494ms/step
Epoch 739/1000
2023-09-30 22:53:24.859 
Epoch 739/1000 
	 loss: 28.6612, MinusLogProbMetric: 28.6612, val_loss: 29.2544, val_MinusLogProbMetric: 29.2544

Epoch 739: val_loss did not improve from 29.24149
196/196 - 96s - loss: 28.6612 - MinusLogProbMetric: 28.6612 - val_loss: 29.2544 - val_MinusLogProbMetric: 29.2544 - lr: 1.3889e-05 - 96s/epoch - 488ms/step
Epoch 740/1000
2023-09-30 22:54:54.603 
Epoch 740/1000 
	 loss: 28.6528, MinusLogProbMetric: 28.6528, val_loss: 29.2685, val_MinusLogProbMetric: 29.2685

Epoch 740: val_loss did not improve from 29.24149
196/196 - 90s - loss: 28.6528 - MinusLogProbMetric: 28.6528 - val_loss: 29.2685 - val_MinusLogProbMetric: 29.2685 - lr: 1.3889e-05 - 90s/epoch - 458ms/step
Epoch 741/1000
2023-09-30 22:56:24.092 
Epoch 741/1000 
	 loss: 28.6538, MinusLogProbMetric: 28.6538, val_loss: 29.2565, val_MinusLogProbMetric: 29.2565

Epoch 741: val_loss did not improve from 29.24149
196/196 - 89s - loss: 28.6538 - MinusLogProbMetric: 28.6538 - val_loss: 29.2565 - val_MinusLogProbMetric: 29.2565 - lr: 1.3889e-05 - 89s/epoch - 456ms/step
Epoch 742/1000
2023-09-30 22:57:54.292 
Epoch 742/1000 
	 loss: 28.6526, MinusLogProbMetric: 28.6526, val_loss: 29.2860, val_MinusLogProbMetric: 29.2860

Epoch 742: val_loss did not improve from 29.24149
196/196 - 90s - loss: 28.6526 - MinusLogProbMetric: 28.6526 - val_loss: 29.2860 - val_MinusLogProbMetric: 29.2860 - lr: 1.3889e-05 - 90s/epoch - 460ms/step
Epoch 743/1000
2023-09-30 22:59:22.439 
Epoch 743/1000 
	 loss: 28.6584, MinusLogProbMetric: 28.6584, val_loss: 29.3201, val_MinusLogProbMetric: 29.3201

Epoch 743: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6584 - MinusLogProbMetric: 28.6584 - val_loss: 29.3201 - val_MinusLogProbMetric: 29.3201 - lr: 1.3889e-05 - 88s/epoch - 449ms/step
Epoch 744/1000
2023-09-30 23:00:53.692 
Epoch 744/1000 
	 loss: 28.6495, MinusLogProbMetric: 28.6495, val_loss: 29.2747, val_MinusLogProbMetric: 29.2747

Epoch 744: val_loss did not improve from 29.24149
196/196 - 91s - loss: 28.6495 - MinusLogProbMetric: 28.6495 - val_loss: 29.2747 - val_MinusLogProbMetric: 29.2747 - lr: 1.3889e-05 - 91s/epoch - 466ms/step
Epoch 745/1000
2023-09-30 23:02:23.751 
Epoch 745/1000 
	 loss: 28.6539, MinusLogProbMetric: 28.6539, val_loss: 29.2668, val_MinusLogProbMetric: 29.2668

Epoch 745: val_loss did not improve from 29.24149
196/196 - 90s - loss: 28.6539 - MinusLogProbMetric: 28.6539 - val_loss: 29.2668 - val_MinusLogProbMetric: 29.2668 - lr: 1.3889e-05 - 90s/epoch - 459ms/step
Epoch 746/1000
2023-09-30 23:03:57.919 
Epoch 746/1000 
	 loss: 28.6498, MinusLogProbMetric: 28.6498, val_loss: 29.2495, val_MinusLogProbMetric: 29.2495

Epoch 746: val_loss did not improve from 29.24149
196/196 - 94s - loss: 28.6498 - MinusLogProbMetric: 28.6498 - val_loss: 29.2495 - val_MinusLogProbMetric: 29.2495 - lr: 1.3889e-05 - 94s/epoch - 480ms/step
Epoch 747/1000
2023-09-30 23:05:25.821 
Epoch 747/1000 
	 loss: 28.6533, MinusLogProbMetric: 28.6533, val_loss: 29.2780, val_MinusLogProbMetric: 29.2780

Epoch 747: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6533 - MinusLogProbMetric: 28.6533 - val_loss: 29.2780 - val_MinusLogProbMetric: 29.2780 - lr: 1.3889e-05 - 88s/epoch - 448ms/step
Epoch 748/1000
2023-09-30 23:07:01.223 
Epoch 748/1000 
	 loss: 28.6553, MinusLogProbMetric: 28.6553, val_loss: 29.3001, val_MinusLogProbMetric: 29.3001

Epoch 748: val_loss did not improve from 29.24149
196/196 - 95s - loss: 28.6553 - MinusLogProbMetric: 28.6553 - val_loss: 29.3001 - val_MinusLogProbMetric: 29.3001 - lr: 1.3889e-05 - 95s/epoch - 487ms/step
Epoch 749/1000
2023-09-30 23:08:29.587 
Epoch 749/1000 
	 loss: 28.6600, MinusLogProbMetric: 28.6600, val_loss: 29.2941, val_MinusLogProbMetric: 29.2941

Epoch 749: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6600 - MinusLogProbMetric: 28.6600 - val_loss: 29.2941 - val_MinusLogProbMetric: 29.2941 - lr: 1.3889e-05 - 88s/epoch - 451ms/step
Epoch 750/1000
2023-09-30 23:10:02.914 
Epoch 750/1000 
	 loss: 28.6555, MinusLogProbMetric: 28.6555, val_loss: 29.2567, val_MinusLogProbMetric: 29.2567

Epoch 750: val_loss did not improve from 29.24149
196/196 - 93s - loss: 28.6555 - MinusLogProbMetric: 28.6555 - val_loss: 29.2567 - val_MinusLogProbMetric: 29.2567 - lr: 1.3889e-05 - 93s/epoch - 476ms/step
Epoch 751/1000
2023-09-30 23:11:32.273 
Epoch 751/1000 
	 loss: 28.6505, MinusLogProbMetric: 28.6505, val_loss: 29.2667, val_MinusLogProbMetric: 29.2667

Epoch 751: val_loss did not improve from 29.24149
196/196 - 89s - loss: 28.6505 - MinusLogProbMetric: 28.6505 - val_loss: 29.2667 - val_MinusLogProbMetric: 29.2667 - lr: 1.3889e-05 - 89s/epoch - 456ms/step
Epoch 752/1000
2023-09-30 23:13:03.648 
Epoch 752/1000 
	 loss: 28.6520, MinusLogProbMetric: 28.6520, val_loss: 29.2531, val_MinusLogProbMetric: 29.2531

Epoch 752: val_loss did not improve from 29.24149
196/196 - 91s - loss: 28.6520 - MinusLogProbMetric: 28.6520 - val_loss: 29.2531 - val_MinusLogProbMetric: 29.2531 - lr: 1.3889e-05 - 91s/epoch - 466ms/step
Epoch 753/1000
2023-09-30 23:14:33.015 
Epoch 753/1000 
	 loss: 28.6515, MinusLogProbMetric: 28.6515, val_loss: 29.2646, val_MinusLogProbMetric: 29.2646

Epoch 753: val_loss did not improve from 29.24149
196/196 - 89s - loss: 28.6515 - MinusLogProbMetric: 28.6515 - val_loss: 29.2646 - val_MinusLogProbMetric: 29.2646 - lr: 1.3889e-05 - 89s/epoch - 456ms/step
Epoch 754/1000
2023-09-30 23:16:00.660 
Epoch 754/1000 
	 loss: 28.6535, MinusLogProbMetric: 28.6535, val_loss: 29.2660, val_MinusLogProbMetric: 29.2660

Epoch 754: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6535 - MinusLogProbMetric: 28.6535 - val_loss: 29.2660 - val_MinusLogProbMetric: 29.2660 - lr: 1.3889e-05 - 88s/epoch - 447ms/step
Epoch 755/1000
2023-09-30 23:17:27.748 
Epoch 755/1000 
	 loss: 28.6490, MinusLogProbMetric: 28.6490, val_loss: 29.2585, val_MinusLogProbMetric: 29.2585

Epoch 755: val_loss did not improve from 29.24149
196/196 - 87s - loss: 28.6490 - MinusLogProbMetric: 28.6490 - val_loss: 29.2585 - val_MinusLogProbMetric: 29.2585 - lr: 1.3889e-05 - 87s/epoch - 444ms/step
Epoch 756/1000
2023-09-30 23:18:55.974 
Epoch 756/1000 
	 loss: 28.6557, MinusLogProbMetric: 28.6557, val_loss: 29.2880, val_MinusLogProbMetric: 29.2880

Epoch 756: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6557 - MinusLogProbMetric: 28.6557 - val_loss: 29.2880 - val_MinusLogProbMetric: 29.2880 - lr: 1.3889e-05 - 88s/epoch - 450ms/step
Epoch 757/1000
2023-09-30 23:20:22.439 
Epoch 757/1000 
	 loss: 28.6545, MinusLogProbMetric: 28.6545, val_loss: 29.2732, val_MinusLogProbMetric: 29.2732

Epoch 757: val_loss did not improve from 29.24149
196/196 - 86s - loss: 28.6545 - MinusLogProbMetric: 28.6545 - val_loss: 29.2732 - val_MinusLogProbMetric: 29.2732 - lr: 1.3889e-05 - 86s/epoch - 441ms/step
Epoch 758/1000
2023-09-30 23:21:55.669 
Epoch 758/1000 
	 loss: 28.6546, MinusLogProbMetric: 28.6546, val_loss: 29.2693, val_MinusLogProbMetric: 29.2693

Epoch 758: val_loss did not improve from 29.24149
196/196 - 93s - loss: 28.6546 - MinusLogProbMetric: 28.6546 - val_loss: 29.2693 - val_MinusLogProbMetric: 29.2693 - lr: 1.3889e-05 - 93s/epoch - 476ms/step
Epoch 759/1000
2023-09-30 23:23:23.028 
Epoch 759/1000 
	 loss: 28.6495, MinusLogProbMetric: 28.6495, val_loss: 29.2650, val_MinusLogProbMetric: 29.2650

Epoch 759: val_loss did not improve from 29.24149
196/196 - 87s - loss: 28.6495 - MinusLogProbMetric: 28.6495 - val_loss: 29.2650 - val_MinusLogProbMetric: 29.2650 - lr: 1.3889e-05 - 87s/epoch - 446ms/step
Epoch 760/1000
2023-09-30 23:24:50.985 
Epoch 760/1000 
	 loss: 28.6536, MinusLogProbMetric: 28.6536, val_loss: 29.2767, val_MinusLogProbMetric: 29.2767

Epoch 760: val_loss did not improve from 29.24149
196/196 - 88s - loss: 28.6536 - MinusLogProbMetric: 28.6536 - val_loss: 29.2767 - val_MinusLogProbMetric: 29.2767 - lr: 1.3889e-05 - 88s/epoch - 449ms/step
Epoch 761/1000
2023-09-30 23:26:24.239 
Epoch 761/1000 
	 loss: 28.6499, MinusLogProbMetric: 28.6499, val_loss: 29.2802, val_MinusLogProbMetric: 29.2802

Epoch 761: val_loss did not improve from 29.24149
196/196 - 93s - loss: 28.6499 - MinusLogProbMetric: 28.6499 - val_loss: 29.2802 - val_MinusLogProbMetric: 29.2802 - lr: 1.3889e-05 - 93s/epoch - 476ms/step
Epoch 762/1000
2023-09-30 23:27:53.713 
Epoch 762/1000 
	 loss: 28.6495, MinusLogProbMetric: 28.6495, val_loss: 29.2804, val_MinusLogProbMetric: 29.2804

Epoch 762: val_loss did not improve from 29.24149
196/196 - 89s - loss: 28.6495 - MinusLogProbMetric: 28.6495 - val_loss: 29.2804 - val_MinusLogProbMetric: 29.2804 - lr: 1.3889e-05 - 89s/epoch - 457ms/step
Epoch 763/1000
2023-09-30 23:29:19.339 
Epoch 763/1000 
	 loss: 28.6573, MinusLogProbMetric: 28.6573, val_loss: 29.3031, val_MinusLogProbMetric: 29.3031

Epoch 763: val_loss did not improve from 29.24149
196/196 - 86s - loss: 28.6573 - MinusLogProbMetric: 28.6573 - val_loss: 29.3031 - val_MinusLogProbMetric: 29.3031 - lr: 1.3889e-05 - 86s/epoch - 437ms/step
Epoch 764/1000
2023-09-30 23:30:49.558 
Epoch 764/1000 
	 loss: 28.6544, MinusLogProbMetric: 28.6544, val_loss: 29.3022, val_MinusLogProbMetric: 29.3022

Epoch 764: val_loss did not improve from 29.24149
196/196 - 90s - loss: 28.6544 - MinusLogProbMetric: 28.6544 - val_loss: 29.3022 - val_MinusLogProbMetric: 29.3022 - lr: 1.3889e-05 - 90s/epoch - 460ms/step
Epoch 765/1000
2023-09-30 23:32:16.452 
Epoch 765/1000 
	 loss: 28.6543, MinusLogProbMetric: 28.6543, val_loss: 29.2874, val_MinusLogProbMetric: 29.2874

Epoch 765: val_loss did not improve from 29.24149
196/196 - 87s - loss: 28.6543 - MinusLogProbMetric: 28.6543 - val_loss: 29.2874 - val_MinusLogProbMetric: 29.2874 - lr: 1.3889e-05 - 87s/epoch - 443ms/step
Epoch 766/1000
2023-09-30 23:33:46.252 
Epoch 766/1000 
	 loss: 28.6572, MinusLogProbMetric: 28.6572, val_loss: 29.2695, val_MinusLogProbMetric: 29.2695

Epoch 766: val_loss did not improve from 29.24149
196/196 - 90s - loss: 28.6572 - MinusLogProbMetric: 28.6572 - val_loss: 29.2695 - val_MinusLogProbMetric: 29.2695 - lr: 1.3889e-05 - 90s/epoch - 458ms/step
Epoch 767/1000
2023-09-30 23:35:10.874 
Epoch 767/1000 
	 loss: 28.6499, MinusLogProbMetric: 28.6499, val_loss: 29.2468, val_MinusLogProbMetric: 29.2468

Epoch 767: val_loss did not improve from 29.24149
196/196 - 85s - loss: 28.6499 - MinusLogProbMetric: 28.6499 - val_loss: 29.2468 - val_MinusLogProbMetric: 29.2468 - lr: 1.3889e-05 - 85s/epoch - 431ms/step
Epoch 768/1000
2023-09-30 23:36:37.314 
Epoch 768/1000 
	 loss: 28.6475, MinusLogProbMetric: 28.6475, val_loss: 29.2876, val_MinusLogProbMetric: 29.2876

Epoch 768: val_loss did not improve from 29.24149
196/196 - 86s - loss: 28.6475 - MinusLogProbMetric: 28.6475 - val_loss: 29.2876 - val_MinusLogProbMetric: 29.2876 - lr: 1.3889e-05 - 86s/epoch - 441ms/step
Epoch 769/1000
2023-09-30 23:38:03.489 
Epoch 769/1000 
	 loss: 28.6271, MinusLogProbMetric: 28.6271, val_loss: 29.2410, val_MinusLogProbMetric: 29.2410

Epoch 769: val_loss improved from 29.24149 to 29.24096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 88s - loss: 28.6271 - MinusLogProbMetric: 28.6271 - val_loss: 29.2410 - val_MinusLogProbMetric: 29.2410 - lr: 6.9444e-06 - 88s/epoch - 451ms/step
Epoch 770/1000
2023-09-30 23:39:31.372 
Epoch 770/1000 
	 loss: 28.6282, MinusLogProbMetric: 28.6282, val_loss: 29.2584, val_MinusLogProbMetric: 29.2584

Epoch 770: val_loss did not improve from 29.24096
196/196 - 86s - loss: 28.6282 - MinusLogProbMetric: 28.6282 - val_loss: 29.2584 - val_MinusLogProbMetric: 29.2584 - lr: 6.9444e-06 - 86s/epoch - 437ms/step
Epoch 771/1000
2023-09-30 23:41:02.092 
Epoch 771/1000 
	 loss: 28.6277, MinusLogProbMetric: 28.6277, val_loss: 29.2468, val_MinusLogProbMetric: 29.2468

Epoch 771: val_loss did not improve from 29.24096
196/196 - 91s - loss: 28.6277 - MinusLogProbMetric: 28.6277 - val_loss: 29.2468 - val_MinusLogProbMetric: 29.2468 - lr: 6.9444e-06 - 91s/epoch - 463ms/step
Epoch 772/1000
2023-09-30 23:42:36.552 
Epoch 772/1000 
	 loss: 28.6252, MinusLogProbMetric: 28.6252, val_loss: 29.2408, val_MinusLogProbMetric: 29.2408

Epoch 772: val_loss improved from 29.24096 to 29.24080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 96s - loss: 28.6252 - MinusLogProbMetric: 28.6252 - val_loss: 29.2408 - val_MinusLogProbMetric: 29.2408 - lr: 6.9444e-06 - 96s/epoch - 489ms/step
Epoch 773/1000
2023-09-30 23:44:08.266 
Epoch 773/1000 
	 loss: 28.6265, MinusLogProbMetric: 28.6265, val_loss: 29.2526, val_MinusLogProbMetric: 29.2526

Epoch 773: val_loss did not improve from 29.24080
196/196 - 90s - loss: 28.6265 - MinusLogProbMetric: 28.6265 - val_loss: 29.2526 - val_MinusLogProbMetric: 29.2526 - lr: 6.9444e-06 - 90s/epoch - 461ms/step
Epoch 774/1000
2023-09-30 23:45:39.052 
Epoch 774/1000 
	 loss: 28.6281, MinusLogProbMetric: 28.6281, val_loss: 29.2516, val_MinusLogProbMetric: 29.2516

Epoch 774: val_loss did not improve from 29.24080
196/196 - 91s - loss: 28.6281 - MinusLogProbMetric: 28.6281 - val_loss: 29.2516 - val_MinusLogProbMetric: 29.2516 - lr: 6.9444e-06 - 91s/epoch - 463ms/step
Epoch 775/1000
2023-09-30 23:47:12.540 
Epoch 775/1000 
	 loss: 28.6248, MinusLogProbMetric: 28.6248, val_loss: 29.2378, val_MinusLogProbMetric: 29.2378

Epoch 775: val_loss improved from 29.24080 to 29.23784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 95s - loss: 28.6248 - MinusLogProbMetric: 28.6248 - val_loss: 29.2378 - val_MinusLogProbMetric: 29.2378 - lr: 6.9444e-06 - 95s/epoch - 487ms/step
Epoch 776/1000
2023-09-30 23:48:39.964 
Epoch 776/1000 
	 loss: 28.6270, MinusLogProbMetric: 28.6270, val_loss: 29.2457, val_MinusLogProbMetric: 29.2457

Epoch 776: val_loss did not improve from 29.23784
196/196 - 86s - loss: 28.6270 - MinusLogProbMetric: 28.6270 - val_loss: 29.2457 - val_MinusLogProbMetric: 29.2457 - lr: 6.9444e-06 - 86s/epoch - 436ms/step
Epoch 777/1000
2023-09-30 23:50:07.717 
Epoch 777/1000 
	 loss: 28.6260, MinusLogProbMetric: 28.6260, val_loss: 29.2397, val_MinusLogProbMetric: 29.2397

Epoch 777: val_loss did not improve from 29.23784
196/196 - 88s - loss: 28.6260 - MinusLogProbMetric: 28.6260 - val_loss: 29.2397 - val_MinusLogProbMetric: 29.2397 - lr: 6.9444e-06 - 88s/epoch - 448ms/step
Epoch 778/1000
2023-09-30 23:51:40.373 
Epoch 778/1000 
	 loss: 28.6240, MinusLogProbMetric: 28.6240, val_loss: 29.2444, val_MinusLogProbMetric: 29.2444

Epoch 778: val_loss did not improve from 29.23784
196/196 - 93s - loss: 28.6240 - MinusLogProbMetric: 28.6240 - val_loss: 29.2444 - val_MinusLogProbMetric: 29.2444 - lr: 6.9444e-06 - 93s/epoch - 473ms/step
Epoch 779/1000
2023-09-30 23:53:15.084 
Epoch 779/1000 
	 loss: 28.6288, MinusLogProbMetric: 28.6288, val_loss: 29.2461, val_MinusLogProbMetric: 29.2461

Epoch 779: val_loss did not improve from 29.23784
196/196 - 95s - loss: 28.6288 - MinusLogProbMetric: 28.6288 - val_loss: 29.2461 - val_MinusLogProbMetric: 29.2461 - lr: 6.9444e-06 - 95s/epoch - 483ms/step
Epoch 780/1000
2023-09-30 23:54:48.132 
Epoch 780/1000 
	 loss: 28.6256, MinusLogProbMetric: 28.6256, val_loss: 29.2422, val_MinusLogProbMetric: 29.2422

Epoch 780: val_loss did not improve from 29.23784
196/196 - 93s - loss: 28.6256 - MinusLogProbMetric: 28.6256 - val_loss: 29.2422 - val_MinusLogProbMetric: 29.2422 - lr: 6.9444e-06 - 93s/epoch - 475ms/step
Epoch 781/1000
2023-09-30 23:56:19.373 
Epoch 781/1000 
	 loss: 28.6248, MinusLogProbMetric: 28.6248, val_loss: 29.2521, val_MinusLogProbMetric: 29.2521

Epoch 781: val_loss did not improve from 29.23784
196/196 - 91s - loss: 28.6248 - MinusLogProbMetric: 28.6248 - val_loss: 29.2521 - val_MinusLogProbMetric: 29.2521 - lr: 6.9444e-06 - 91s/epoch - 465ms/step
Epoch 782/1000
2023-09-30 23:57:54.764 
Epoch 782/1000 
	 loss: 28.6232, MinusLogProbMetric: 28.6232, val_loss: 29.2560, val_MinusLogProbMetric: 29.2560

Epoch 782: val_loss did not improve from 29.23784
196/196 - 95s - loss: 28.6232 - MinusLogProbMetric: 28.6232 - val_loss: 29.2560 - val_MinusLogProbMetric: 29.2560 - lr: 6.9444e-06 - 95s/epoch - 486ms/step
Epoch 783/1000
2023-09-30 23:59:28.650 
Epoch 783/1000 
	 loss: 28.6248, MinusLogProbMetric: 28.6248, val_loss: 29.2333, val_MinusLogProbMetric: 29.2333

Epoch 783: val_loss improved from 29.23784 to 29.23325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 96s - loss: 28.6248 - MinusLogProbMetric: 28.6248 - val_loss: 29.2333 - val_MinusLogProbMetric: 29.2333 - lr: 6.9444e-06 - 96s/epoch - 492ms/step
Epoch 784/1000
2023-10-01 00:01:07.201 
Epoch 784/1000 
	 loss: 28.6266, MinusLogProbMetric: 28.6266, val_loss: 29.2443, val_MinusLogProbMetric: 29.2443

Epoch 784: val_loss did not improve from 29.23325
196/196 - 96s - loss: 28.6266 - MinusLogProbMetric: 28.6266 - val_loss: 29.2443 - val_MinusLogProbMetric: 29.2443 - lr: 6.9444e-06 - 96s/epoch - 490ms/step
Epoch 785/1000
2023-10-01 00:02:40.584 
Epoch 785/1000 
	 loss: 28.6262, MinusLogProbMetric: 28.6262, val_loss: 29.2596, val_MinusLogProbMetric: 29.2596

Epoch 785: val_loss did not improve from 29.23325
196/196 - 93s - loss: 28.6262 - MinusLogProbMetric: 28.6262 - val_loss: 29.2596 - val_MinusLogProbMetric: 29.2596 - lr: 6.9444e-06 - 93s/epoch - 476ms/step
Epoch 786/1000
2023-10-01 00:04:10.169 
Epoch 786/1000 
	 loss: 28.6263, MinusLogProbMetric: 28.6263, val_loss: 29.2548, val_MinusLogProbMetric: 29.2548

Epoch 786: val_loss did not improve from 29.23325
196/196 - 90s - loss: 28.6263 - MinusLogProbMetric: 28.6263 - val_loss: 29.2548 - val_MinusLogProbMetric: 29.2548 - lr: 6.9444e-06 - 90s/epoch - 457ms/step
Epoch 787/1000
2023-10-01 00:05:47.965 
Epoch 787/1000 
	 loss: 28.6247, MinusLogProbMetric: 28.6247, val_loss: 29.2613, val_MinusLogProbMetric: 29.2613

Epoch 787: val_loss did not improve from 29.23325
196/196 - 98s - loss: 28.6247 - MinusLogProbMetric: 28.6247 - val_loss: 29.2613 - val_MinusLogProbMetric: 29.2613 - lr: 6.9444e-06 - 98s/epoch - 499ms/step
Epoch 788/1000
2023-10-01 00:07:21.998 
Epoch 788/1000 
	 loss: 28.6253, MinusLogProbMetric: 28.6253, val_loss: 29.2680, val_MinusLogProbMetric: 29.2680

Epoch 788: val_loss did not improve from 29.23325
196/196 - 94s - loss: 28.6253 - MinusLogProbMetric: 28.6253 - val_loss: 29.2680 - val_MinusLogProbMetric: 29.2680 - lr: 6.9444e-06 - 94s/epoch - 480ms/step
Epoch 789/1000
2023-10-01 00:08:54.314 
Epoch 789/1000 
	 loss: 28.6245, MinusLogProbMetric: 28.6245, val_loss: 29.2265, val_MinusLogProbMetric: 29.2265

Epoch 789: val_loss improved from 29.23325 to 29.22654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 94s - loss: 28.6245 - MinusLogProbMetric: 28.6245 - val_loss: 29.2265 - val_MinusLogProbMetric: 29.2265 - lr: 6.9444e-06 - 94s/epoch - 481ms/step
Epoch 790/1000
2023-10-01 00:10:29.448 
Epoch 790/1000 
	 loss: 28.6267, MinusLogProbMetric: 28.6267, val_loss: 29.2511, val_MinusLogProbMetric: 29.2511

Epoch 790: val_loss did not improve from 29.22654
196/196 - 93s - loss: 28.6267 - MinusLogProbMetric: 28.6267 - val_loss: 29.2511 - val_MinusLogProbMetric: 29.2511 - lr: 6.9444e-06 - 93s/epoch - 476ms/step
Epoch 791/1000
2023-10-01 00:12:00.613 
Epoch 791/1000 
	 loss: 28.6227, MinusLogProbMetric: 28.6227, val_loss: 29.2568, val_MinusLogProbMetric: 29.2568

Epoch 791: val_loss did not improve from 29.22654
196/196 - 91s - loss: 28.6227 - MinusLogProbMetric: 28.6227 - val_loss: 29.2568 - val_MinusLogProbMetric: 29.2568 - lr: 6.9444e-06 - 91s/epoch - 465ms/step
Epoch 792/1000
2023-10-01 00:13:36.724 
Epoch 792/1000 
	 loss: 28.6226, MinusLogProbMetric: 28.6226, val_loss: 29.2417, val_MinusLogProbMetric: 29.2417

Epoch 792: val_loss did not improve from 29.22654
196/196 - 96s - loss: 28.6226 - MinusLogProbMetric: 28.6226 - val_loss: 29.2417 - val_MinusLogProbMetric: 29.2417 - lr: 6.9444e-06 - 96s/epoch - 490ms/step
Epoch 793/1000
2023-10-01 00:15:16.427 
Epoch 793/1000 
	 loss: 28.6230, MinusLogProbMetric: 28.6230, val_loss: 29.2456, val_MinusLogProbMetric: 29.2456

Epoch 793: val_loss did not improve from 29.22654
196/196 - 100s - loss: 28.6230 - MinusLogProbMetric: 28.6230 - val_loss: 29.2456 - val_MinusLogProbMetric: 29.2456 - lr: 6.9444e-06 - 100s/epoch - 508ms/step
Epoch 794/1000
2023-10-01 00:16:44.305 
Epoch 794/1000 
	 loss: 28.6247, MinusLogProbMetric: 28.6247, val_loss: 29.2595, val_MinusLogProbMetric: 29.2595

Epoch 794: val_loss did not improve from 29.22654
196/196 - 88s - loss: 28.6247 - MinusLogProbMetric: 28.6247 - val_loss: 29.2595 - val_MinusLogProbMetric: 29.2595 - lr: 6.9444e-06 - 88s/epoch - 448ms/step
Epoch 795/1000
2023-10-01 00:18:20.097 
Epoch 795/1000 
	 loss: 28.6237, MinusLogProbMetric: 28.6237, val_loss: 29.2405, val_MinusLogProbMetric: 29.2405

Epoch 795: val_loss did not improve from 29.22654
196/196 - 96s - loss: 28.6237 - MinusLogProbMetric: 28.6237 - val_loss: 29.2405 - val_MinusLogProbMetric: 29.2405 - lr: 6.9444e-06 - 96s/epoch - 489ms/step
Epoch 796/1000
2023-10-01 00:20:05.435 
Epoch 796/1000 
	 loss: 28.6240, MinusLogProbMetric: 28.6240, val_loss: 29.2511, val_MinusLogProbMetric: 29.2511

Epoch 796: val_loss did not improve from 29.22654
196/196 - 105s - loss: 28.6240 - MinusLogProbMetric: 28.6240 - val_loss: 29.2511 - val_MinusLogProbMetric: 29.2511 - lr: 6.9444e-06 - 105s/epoch - 538ms/step
Epoch 797/1000
2023-10-01 00:21:46.261 
Epoch 797/1000 
	 loss: 28.6229, MinusLogProbMetric: 28.6229, val_loss: 29.2528, val_MinusLogProbMetric: 29.2528

Epoch 797: val_loss did not improve from 29.22654
196/196 - 101s - loss: 28.6229 - MinusLogProbMetric: 28.6229 - val_loss: 29.2528 - val_MinusLogProbMetric: 29.2528 - lr: 6.9444e-06 - 101s/epoch - 514ms/step
Epoch 798/1000
2023-10-01 00:23:29.401 
Epoch 798/1000 
	 loss: 28.6244, MinusLogProbMetric: 28.6244, val_loss: 29.2389, val_MinusLogProbMetric: 29.2389

Epoch 798: val_loss did not improve from 29.22654
196/196 - 103s - loss: 28.6244 - MinusLogProbMetric: 28.6244 - val_loss: 29.2389 - val_MinusLogProbMetric: 29.2389 - lr: 6.9444e-06 - 103s/epoch - 526ms/step
Epoch 799/1000
2023-10-01 00:25:04.124 
Epoch 799/1000 
	 loss: 28.6249, MinusLogProbMetric: 28.6249, val_loss: 29.2560, val_MinusLogProbMetric: 29.2560

Epoch 799: val_loss did not improve from 29.22654
196/196 - 95s - loss: 28.6249 - MinusLogProbMetric: 28.6249 - val_loss: 29.2560 - val_MinusLogProbMetric: 29.2560 - lr: 6.9444e-06 - 95s/epoch - 483ms/step
Epoch 800/1000
2023-10-01 00:26:49.730 
Epoch 800/1000 
	 loss: 28.6227, MinusLogProbMetric: 28.6227, val_loss: 29.2452, val_MinusLogProbMetric: 29.2452

Epoch 800: val_loss did not improve from 29.22654
196/196 - 106s - loss: 28.6227 - MinusLogProbMetric: 28.6227 - val_loss: 29.2452 - val_MinusLogProbMetric: 29.2452 - lr: 6.9444e-06 - 106s/epoch - 539ms/step
Epoch 801/1000
2023-10-01 00:28:30.276 
Epoch 801/1000 
	 loss: 28.6245, MinusLogProbMetric: 28.6245, val_loss: 29.2389, val_MinusLogProbMetric: 29.2389

Epoch 801: val_loss did not improve from 29.22654
196/196 - 101s - loss: 28.6245 - MinusLogProbMetric: 28.6245 - val_loss: 29.2389 - val_MinusLogProbMetric: 29.2389 - lr: 6.9444e-06 - 101s/epoch - 513ms/step
Epoch 802/1000
2023-10-01 00:29:59.220 
Epoch 802/1000 
	 loss: 28.6213, MinusLogProbMetric: 28.6213, val_loss: 29.2570, val_MinusLogProbMetric: 29.2570

Epoch 802: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6213 - MinusLogProbMetric: 28.6213 - val_loss: 29.2570 - val_MinusLogProbMetric: 29.2570 - lr: 6.9444e-06 - 89s/epoch - 454ms/step
Epoch 803/1000
2023-10-01 00:31:43.300 
Epoch 803/1000 
	 loss: 28.6247, MinusLogProbMetric: 28.6247, val_loss: 29.2797, val_MinusLogProbMetric: 29.2797

Epoch 803: val_loss did not improve from 29.22654
196/196 - 104s - loss: 28.6247 - MinusLogProbMetric: 28.6247 - val_loss: 29.2797 - val_MinusLogProbMetric: 29.2797 - lr: 6.9444e-06 - 104s/epoch - 531ms/step
Epoch 804/1000
2023-10-01 00:33:20.631 
Epoch 804/1000 
	 loss: 28.6238, MinusLogProbMetric: 28.6238, val_loss: 29.2615, val_MinusLogProbMetric: 29.2615

Epoch 804: val_loss did not improve from 29.22654
196/196 - 97s - loss: 28.6238 - MinusLogProbMetric: 28.6238 - val_loss: 29.2615 - val_MinusLogProbMetric: 29.2615 - lr: 6.9444e-06 - 97s/epoch - 496ms/step
Epoch 805/1000
2023-10-01 00:34:52.696 
Epoch 805/1000 
	 loss: 28.6239, MinusLogProbMetric: 28.6239, val_loss: 29.2577, val_MinusLogProbMetric: 29.2577

Epoch 805: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6239 - MinusLogProbMetric: 28.6239 - val_loss: 29.2577 - val_MinusLogProbMetric: 29.2577 - lr: 6.9444e-06 - 92s/epoch - 470ms/step
Epoch 806/1000
2023-10-01 00:36:29.792 
Epoch 806/1000 
	 loss: 28.6257, MinusLogProbMetric: 28.6257, val_loss: 29.2481, val_MinusLogProbMetric: 29.2481

Epoch 806: val_loss did not improve from 29.22654
196/196 - 97s - loss: 28.6257 - MinusLogProbMetric: 28.6257 - val_loss: 29.2481 - val_MinusLogProbMetric: 29.2481 - lr: 6.9444e-06 - 97s/epoch - 495ms/step
Epoch 807/1000
2023-10-01 00:38:03.472 
Epoch 807/1000 
	 loss: 28.6250, MinusLogProbMetric: 28.6250, val_loss: 29.2459, val_MinusLogProbMetric: 29.2459

Epoch 807: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6250 - MinusLogProbMetric: 28.6250 - val_loss: 29.2459 - val_MinusLogProbMetric: 29.2459 - lr: 6.9444e-06 - 94s/epoch - 478ms/step
Epoch 808/1000
2023-10-01 00:39:41.011 
Epoch 808/1000 
	 loss: 28.6245, MinusLogProbMetric: 28.6245, val_loss: 29.2419, val_MinusLogProbMetric: 29.2419

Epoch 808: val_loss did not improve from 29.22654
196/196 - 98s - loss: 28.6245 - MinusLogProbMetric: 28.6245 - val_loss: 29.2419 - val_MinusLogProbMetric: 29.2419 - lr: 6.9444e-06 - 98s/epoch - 498ms/step
Epoch 809/1000
2023-10-01 00:41:16.136 
Epoch 809/1000 
	 loss: 28.6263, MinusLogProbMetric: 28.6263, val_loss: 29.2512, val_MinusLogProbMetric: 29.2512

Epoch 809: val_loss did not improve from 29.22654
196/196 - 95s - loss: 28.6263 - MinusLogProbMetric: 28.6263 - val_loss: 29.2512 - val_MinusLogProbMetric: 29.2512 - lr: 6.9444e-06 - 95s/epoch - 485ms/step
Epoch 810/1000
2023-10-01 00:42:42.340 
Epoch 810/1000 
	 loss: 28.6240, MinusLogProbMetric: 28.6240, val_loss: 29.2426, val_MinusLogProbMetric: 29.2426

Epoch 810: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6240 - MinusLogProbMetric: 28.6240 - val_loss: 29.2426 - val_MinusLogProbMetric: 29.2426 - lr: 6.9444e-06 - 86s/epoch - 439ms/step
Epoch 811/1000
2023-10-01 00:44:11.136 
Epoch 811/1000 
	 loss: 28.6218, MinusLogProbMetric: 28.6218, val_loss: 29.2729, val_MinusLogProbMetric: 29.2729

Epoch 811: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6218 - MinusLogProbMetric: 28.6218 - val_loss: 29.2729 - val_MinusLogProbMetric: 29.2729 - lr: 6.9444e-06 - 89s/epoch - 453ms/step
Epoch 812/1000
2023-10-01 00:45:51.832 
Epoch 812/1000 
	 loss: 28.6238, MinusLogProbMetric: 28.6238, val_loss: 29.2449, val_MinusLogProbMetric: 29.2449

Epoch 812: val_loss did not improve from 29.22654
196/196 - 101s - loss: 28.6238 - MinusLogProbMetric: 28.6238 - val_loss: 29.2449 - val_MinusLogProbMetric: 29.2449 - lr: 6.9444e-06 - 101s/epoch - 514ms/step
Epoch 813/1000
2023-10-01 00:47:24.733 
Epoch 813/1000 
	 loss: 28.6226, MinusLogProbMetric: 28.6226, val_loss: 29.2493, val_MinusLogProbMetric: 29.2493

Epoch 813: val_loss did not improve from 29.22654
196/196 - 93s - loss: 28.6226 - MinusLogProbMetric: 28.6226 - val_loss: 29.2493 - val_MinusLogProbMetric: 29.2493 - lr: 6.9444e-06 - 93s/epoch - 474ms/step
Epoch 814/1000
2023-10-01 00:49:02.914 
Epoch 814/1000 
	 loss: 28.6246, MinusLogProbMetric: 28.6246, val_loss: 29.2603, val_MinusLogProbMetric: 29.2603

Epoch 814: val_loss did not improve from 29.22654
196/196 - 98s - loss: 28.6246 - MinusLogProbMetric: 28.6246 - val_loss: 29.2603 - val_MinusLogProbMetric: 29.2603 - lr: 6.9444e-06 - 98s/epoch - 501ms/step
Epoch 815/1000
2023-10-01 00:50:41.766 
Epoch 815/1000 
	 loss: 28.6240, MinusLogProbMetric: 28.6240, val_loss: 29.2493, val_MinusLogProbMetric: 29.2493

Epoch 815: val_loss did not improve from 29.22654
196/196 - 99s - loss: 28.6240 - MinusLogProbMetric: 28.6240 - val_loss: 29.2493 - val_MinusLogProbMetric: 29.2493 - lr: 6.9444e-06 - 99s/epoch - 504ms/step
Epoch 816/1000
2023-10-01 00:52:09.832 
Epoch 816/1000 
	 loss: 28.6234, MinusLogProbMetric: 28.6234, val_loss: 29.2624, val_MinusLogProbMetric: 29.2624

Epoch 816: val_loss did not improve from 29.22654
196/196 - 88s - loss: 28.6234 - MinusLogProbMetric: 28.6234 - val_loss: 29.2624 - val_MinusLogProbMetric: 29.2624 - lr: 6.9444e-06 - 88s/epoch - 449ms/step
Epoch 817/1000
2023-10-01 00:53:40.664 
Epoch 817/1000 
	 loss: 28.6204, MinusLogProbMetric: 28.6204, val_loss: 29.2654, val_MinusLogProbMetric: 29.2654

Epoch 817: val_loss did not improve from 29.22654
196/196 - 91s - loss: 28.6204 - MinusLogProbMetric: 28.6204 - val_loss: 29.2654 - val_MinusLogProbMetric: 29.2654 - lr: 6.9444e-06 - 91s/epoch - 463ms/step
Epoch 818/1000
2023-10-01 00:55:13.784 
Epoch 818/1000 
	 loss: 28.6232, MinusLogProbMetric: 28.6232, val_loss: 29.2508, val_MinusLogProbMetric: 29.2508

Epoch 818: val_loss did not improve from 29.22654
196/196 - 93s - loss: 28.6232 - MinusLogProbMetric: 28.6232 - val_loss: 29.2508 - val_MinusLogProbMetric: 29.2508 - lr: 6.9444e-06 - 93s/epoch - 475ms/step
Epoch 819/1000
2023-10-01 00:56:44.776 
Epoch 819/1000 
	 loss: 28.6241, MinusLogProbMetric: 28.6241, val_loss: 29.2572, val_MinusLogProbMetric: 29.2572

Epoch 819: val_loss did not improve from 29.22654
196/196 - 91s - loss: 28.6241 - MinusLogProbMetric: 28.6241 - val_loss: 29.2572 - val_MinusLogProbMetric: 29.2572 - lr: 6.9444e-06 - 91s/epoch - 464ms/step
Epoch 820/1000
2023-10-01 00:58:20.611 
Epoch 820/1000 
	 loss: 28.6252, MinusLogProbMetric: 28.6252, val_loss: 29.2555, val_MinusLogProbMetric: 29.2555

Epoch 820: val_loss did not improve from 29.22654
196/196 - 96s - loss: 28.6252 - MinusLogProbMetric: 28.6252 - val_loss: 29.2555 - val_MinusLogProbMetric: 29.2555 - lr: 6.9444e-06 - 96s/epoch - 489ms/step
Epoch 821/1000
2023-10-01 00:59:52.684 
Epoch 821/1000 
	 loss: 28.6237, MinusLogProbMetric: 28.6237, val_loss: 29.2302, val_MinusLogProbMetric: 29.2302

Epoch 821: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6237 - MinusLogProbMetric: 28.6237 - val_loss: 29.2302 - val_MinusLogProbMetric: 29.2302 - lr: 6.9444e-06 - 92s/epoch - 470ms/step
Epoch 822/1000
2023-10-01 01:01:24.753 
Epoch 822/1000 
	 loss: 28.6223, MinusLogProbMetric: 28.6223, val_loss: 29.2612, val_MinusLogProbMetric: 29.2612

Epoch 822: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6223 - MinusLogProbMetric: 28.6223 - val_loss: 29.2612 - val_MinusLogProbMetric: 29.2612 - lr: 6.9444e-06 - 92s/epoch - 470ms/step
Epoch 823/1000
2023-10-01 01:02:56.983 
Epoch 823/1000 
	 loss: 28.6243, MinusLogProbMetric: 28.6243, val_loss: 29.2364, val_MinusLogProbMetric: 29.2364

Epoch 823: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6243 - MinusLogProbMetric: 28.6243 - val_loss: 29.2364 - val_MinusLogProbMetric: 29.2364 - lr: 6.9444e-06 - 92s/epoch - 470ms/step
Epoch 824/1000
2023-10-01 01:04:25.745 
Epoch 824/1000 
	 loss: 28.6239, MinusLogProbMetric: 28.6239, val_loss: 29.2379, val_MinusLogProbMetric: 29.2379

Epoch 824: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6239 - MinusLogProbMetric: 28.6239 - val_loss: 29.2379 - val_MinusLogProbMetric: 29.2379 - lr: 6.9444e-06 - 89s/epoch - 453ms/step
Epoch 825/1000
2023-10-01 01:05:57.976 
Epoch 825/1000 
	 loss: 28.6206, MinusLogProbMetric: 28.6206, val_loss: 29.2593, val_MinusLogProbMetric: 29.2593

Epoch 825: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6206 - MinusLogProbMetric: 28.6206 - val_loss: 29.2593 - val_MinusLogProbMetric: 29.2593 - lr: 6.9444e-06 - 92s/epoch - 470ms/step
Epoch 826/1000
2023-10-01 01:07:35.412 
Epoch 826/1000 
	 loss: 28.6224, MinusLogProbMetric: 28.6224, val_loss: 29.2664, val_MinusLogProbMetric: 29.2664

Epoch 826: val_loss did not improve from 29.22654
196/196 - 97s - loss: 28.6224 - MinusLogProbMetric: 28.6224 - val_loss: 29.2664 - val_MinusLogProbMetric: 29.2664 - lr: 6.9444e-06 - 97s/epoch - 497ms/step
Epoch 827/1000
2023-10-01 01:09:09.652 
Epoch 827/1000 
	 loss: 28.6223, MinusLogProbMetric: 28.6223, val_loss: 29.2361, val_MinusLogProbMetric: 29.2361

Epoch 827: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6223 - MinusLogProbMetric: 28.6223 - val_loss: 29.2361 - val_MinusLogProbMetric: 29.2361 - lr: 6.9444e-06 - 94s/epoch - 481ms/step
Epoch 828/1000
2023-10-01 01:10:41.075 
Epoch 828/1000 
	 loss: 28.6202, MinusLogProbMetric: 28.6202, val_loss: 29.2379, val_MinusLogProbMetric: 29.2379

Epoch 828: val_loss did not improve from 29.22654
196/196 - 91s - loss: 28.6202 - MinusLogProbMetric: 28.6202 - val_loss: 29.2379 - val_MinusLogProbMetric: 29.2379 - lr: 6.9444e-06 - 91s/epoch - 466ms/step
Epoch 829/1000
2023-10-01 01:12:13.034 
Epoch 829/1000 
	 loss: 28.6232, MinusLogProbMetric: 28.6232, val_loss: 29.2807, val_MinusLogProbMetric: 29.2807

Epoch 829: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6232 - MinusLogProbMetric: 28.6232 - val_loss: 29.2807 - val_MinusLogProbMetric: 29.2807 - lr: 6.9444e-06 - 92s/epoch - 469ms/step
Epoch 830/1000
2023-10-01 01:13:48.242 
Epoch 830/1000 
	 loss: 28.6229, MinusLogProbMetric: 28.6229, val_loss: 29.2452, val_MinusLogProbMetric: 29.2452

Epoch 830: val_loss did not improve from 29.22654
196/196 - 95s - loss: 28.6229 - MinusLogProbMetric: 28.6229 - val_loss: 29.2452 - val_MinusLogProbMetric: 29.2452 - lr: 6.9444e-06 - 95s/epoch - 486ms/step
Epoch 831/1000
2023-10-01 01:15:18.285 
Epoch 831/1000 
	 loss: 28.6226, MinusLogProbMetric: 28.6226, val_loss: 29.2501, val_MinusLogProbMetric: 29.2501

Epoch 831: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6226 - MinusLogProbMetric: 28.6226 - val_loss: 29.2501 - val_MinusLogProbMetric: 29.2501 - lr: 6.9444e-06 - 90s/epoch - 459ms/step
Epoch 832/1000
2023-10-01 01:16:45.239 
Epoch 832/1000 
	 loss: 28.6229, MinusLogProbMetric: 28.6229, val_loss: 29.2669, val_MinusLogProbMetric: 29.2669

Epoch 832: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6229 - MinusLogProbMetric: 28.6229 - val_loss: 29.2669 - val_MinusLogProbMetric: 29.2669 - lr: 6.9444e-06 - 87s/epoch - 444ms/step
Epoch 833/1000
2023-10-01 01:18:14.829 
Epoch 833/1000 
	 loss: 28.6236, MinusLogProbMetric: 28.6236, val_loss: 29.2470, val_MinusLogProbMetric: 29.2470

Epoch 833: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6236 - MinusLogProbMetric: 28.6236 - val_loss: 29.2470 - val_MinusLogProbMetric: 29.2470 - lr: 6.9444e-06 - 90s/epoch - 457ms/step
Epoch 834/1000
2023-10-01 01:19:43.672 
Epoch 834/1000 
	 loss: 28.6238, MinusLogProbMetric: 28.6238, val_loss: 29.2504, val_MinusLogProbMetric: 29.2504

Epoch 834: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6238 - MinusLogProbMetric: 28.6238 - val_loss: 29.2504 - val_MinusLogProbMetric: 29.2504 - lr: 6.9444e-06 - 89s/epoch - 453ms/step
Epoch 835/1000
2023-10-01 01:21:13.515 
Epoch 835/1000 
	 loss: 28.6245, MinusLogProbMetric: 28.6245, val_loss: 29.2472, val_MinusLogProbMetric: 29.2472

Epoch 835: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6245 - MinusLogProbMetric: 28.6245 - val_loss: 29.2472 - val_MinusLogProbMetric: 29.2472 - lr: 6.9444e-06 - 90s/epoch - 458ms/step
Epoch 836/1000
2023-10-01 01:22:47.216 
Epoch 836/1000 
	 loss: 28.6233, MinusLogProbMetric: 28.6233, val_loss: 29.2512, val_MinusLogProbMetric: 29.2512

Epoch 836: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6233 - MinusLogProbMetric: 28.6233 - val_loss: 29.2512 - val_MinusLogProbMetric: 29.2512 - lr: 6.9444e-06 - 94s/epoch - 478ms/step
Epoch 837/1000
2023-10-01 01:24:13.327 
Epoch 837/1000 
	 loss: 28.6222, MinusLogProbMetric: 28.6222, val_loss: 29.2468, val_MinusLogProbMetric: 29.2468

Epoch 837: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6222 - MinusLogProbMetric: 28.6222 - val_loss: 29.2468 - val_MinusLogProbMetric: 29.2468 - lr: 6.9444e-06 - 86s/epoch - 439ms/step
Epoch 838/1000
2023-10-01 01:25:44.917 
Epoch 838/1000 
	 loss: 28.6213, MinusLogProbMetric: 28.6213, val_loss: 29.2446, val_MinusLogProbMetric: 29.2446

Epoch 838: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6213 - MinusLogProbMetric: 28.6213 - val_loss: 29.2446 - val_MinusLogProbMetric: 29.2446 - lr: 6.9444e-06 - 92s/epoch - 467ms/step
Epoch 839/1000
2023-10-01 01:27:12.099 
Epoch 839/1000 
	 loss: 28.6212, MinusLogProbMetric: 28.6212, val_loss: 29.2566, val_MinusLogProbMetric: 29.2566

Epoch 839: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6212 - MinusLogProbMetric: 28.6212 - val_loss: 29.2566 - val_MinusLogProbMetric: 29.2566 - lr: 6.9444e-06 - 87s/epoch - 445ms/step
Epoch 840/1000
2023-10-01 01:28:40.856 
Epoch 840/1000 
	 loss: 28.6103, MinusLogProbMetric: 28.6103, val_loss: 29.2377, val_MinusLogProbMetric: 29.2377

Epoch 840: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6103 - MinusLogProbMetric: 28.6103 - val_loss: 29.2377 - val_MinusLogProbMetric: 29.2377 - lr: 3.4722e-06 - 89s/epoch - 453ms/step
Epoch 841/1000
2023-10-01 01:30:06.150 
Epoch 841/1000 
	 loss: 28.6099, MinusLogProbMetric: 28.6099, val_loss: 29.2373, val_MinusLogProbMetric: 29.2373

Epoch 841: val_loss did not improve from 29.22654
196/196 - 85s - loss: 28.6099 - MinusLogProbMetric: 28.6099 - val_loss: 29.2373 - val_MinusLogProbMetric: 29.2373 - lr: 3.4722e-06 - 85s/epoch - 435ms/step
Epoch 842/1000
2023-10-01 01:31:34.389 
Epoch 842/1000 
	 loss: 28.6093, MinusLogProbMetric: 28.6093, val_loss: 29.2506, val_MinusLogProbMetric: 29.2506

Epoch 842: val_loss did not improve from 29.22654
196/196 - 88s - loss: 28.6093 - MinusLogProbMetric: 28.6093 - val_loss: 29.2506 - val_MinusLogProbMetric: 29.2506 - lr: 3.4722e-06 - 88s/epoch - 450ms/step
Epoch 843/1000
2023-10-01 01:33:08.835 
Epoch 843/1000 
	 loss: 28.6101, MinusLogProbMetric: 28.6101, val_loss: 29.2448, val_MinusLogProbMetric: 29.2448

Epoch 843: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6101 - MinusLogProbMetric: 28.6101 - val_loss: 29.2448 - val_MinusLogProbMetric: 29.2448 - lr: 3.4722e-06 - 94s/epoch - 482ms/step
Epoch 844/1000
2023-10-01 01:34:41.849 
Epoch 844/1000 
	 loss: 28.6098, MinusLogProbMetric: 28.6098, val_loss: 29.2331, val_MinusLogProbMetric: 29.2331

Epoch 844: val_loss did not improve from 29.22654
196/196 - 93s - loss: 28.6098 - MinusLogProbMetric: 28.6098 - val_loss: 29.2331 - val_MinusLogProbMetric: 29.2331 - lr: 3.4722e-06 - 93s/epoch - 475ms/step
Epoch 845/1000
2023-10-01 01:36:08.157 
Epoch 845/1000 
	 loss: 28.6095, MinusLogProbMetric: 28.6095, val_loss: 29.2440, val_MinusLogProbMetric: 29.2440

Epoch 845: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6095 - MinusLogProbMetric: 28.6095 - val_loss: 29.2440 - val_MinusLogProbMetric: 29.2440 - lr: 3.4722e-06 - 86s/epoch - 440ms/step
Epoch 846/1000
2023-10-01 01:37:36.893 
Epoch 846/1000 
	 loss: 28.6101, MinusLogProbMetric: 28.6101, val_loss: 29.2393, val_MinusLogProbMetric: 29.2393

Epoch 846: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6101 - MinusLogProbMetric: 28.6101 - val_loss: 29.2393 - val_MinusLogProbMetric: 29.2393 - lr: 3.4722e-06 - 89s/epoch - 453ms/step
Epoch 847/1000
2023-10-01 01:39:10.614 
Epoch 847/1000 
	 loss: 28.6086, MinusLogProbMetric: 28.6086, val_loss: 29.2348, val_MinusLogProbMetric: 29.2348

Epoch 847: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6086 - MinusLogProbMetric: 28.6086 - val_loss: 29.2348 - val_MinusLogProbMetric: 29.2348 - lr: 3.4722e-06 - 94s/epoch - 478ms/step
Epoch 848/1000
2023-10-01 01:40:36.689 
Epoch 848/1000 
	 loss: 28.6104, MinusLogProbMetric: 28.6104, val_loss: 29.2356, val_MinusLogProbMetric: 29.2356

Epoch 848: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6104 - MinusLogProbMetric: 28.6104 - val_loss: 29.2356 - val_MinusLogProbMetric: 29.2356 - lr: 3.4722e-06 - 86s/epoch - 439ms/step
Epoch 849/1000
2023-10-01 01:42:06.912 
Epoch 849/1000 
	 loss: 28.6097, MinusLogProbMetric: 28.6097, val_loss: 29.2344, val_MinusLogProbMetric: 29.2344

Epoch 849: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6097 - MinusLogProbMetric: 28.6097 - val_loss: 29.2344 - val_MinusLogProbMetric: 29.2344 - lr: 3.4722e-06 - 90s/epoch - 460ms/step
Epoch 850/1000
2023-10-01 01:43:33.512 
Epoch 850/1000 
	 loss: 28.6100, MinusLogProbMetric: 28.6100, val_loss: 29.2350, val_MinusLogProbMetric: 29.2350

Epoch 850: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6100 - MinusLogProbMetric: 28.6100 - val_loss: 29.2350 - val_MinusLogProbMetric: 29.2350 - lr: 3.4722e-06 - 87s/epoch - 442ms/step
Epoch 851/1000
2023-10-01 01:45:08.444 
Epoch 851/1000 
	 loss: 28.6103, MinusLogProbMetric: 28.6103, val_loss: 29.2429, val_MinusLogProbMetric: 29.2429

Epoch 851: val_loss did not improve from 29.22654
196/196 - 95s - loss: 28.6103 - MinusLogProbMetric: 28.6103 - val_loss: 29.2429 - val_MinusLogProbMetric: 29.2429 - lr: 3.4722e-06 - 95s/epoch - 484ms/step
Epoch 852/1000
2023-10-01 01:46:38.662 
Epoch 852/1000 
	 loss: 28.6088, MinusLogProbMetric: 28.6088, val_loss: 29.2443, val_MinusLogProbMetric: 29.2443

Epoch 852: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6088 - MinusLogProbMetric: 28.6088 - val_loss: 29.2443 - val_MinusLogProbMetric: 29.2443 - lr: 3.4722e-06 - 90s/epoch - 460ms/step
Epoch 853/1000
2023-10-01 01:48:06.051 
Epoch 853/1000 
	 loss: 28.6096, MinusLogProbMetric: 28.6096, val_loss: 29.2408, val_MinusLogProbMetric: 29.2408

Epoch 853: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6096 - MinusLogProbMetric: 28.6096 - val_loss: 29.2408 - val_MinusLogProbMetric: 29.2408 - lr: 3.4722e-06 - 87s/epoch - 446ms/step
Epoch 854/1000
2023-10-01 01:49:32.312 
Epoch 854/1000 
	 loss: 28.6097, MinusLogProbMetric: 28.6097, val_loss: 29.2433, val_MinusLogProbMetric: 29.2433

Epoch 854: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6097 - MinusLogProbMetric: 28.6097 - val_loss: 29.2433 - val_MinusLogProbMetric: 29.2433 - lr: 3.4722e-06 - 86s/epoch - 440ms/step
Epoch 855/1000
2023-10-01 01:50:59.708 
Epoch 855/1000 
	 loss: 28.6104, MinusLogProbMetric: 28.6104, val_loss: 29.2380, val_MinusLogProbMetric: 29.2380

Epoch 855: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6104 - MinusLogProbMetric: 28.6104 - val_loss: 29.2380 - val_MinusLogProbMetric: 29.2380 - lr: 3.4722e-06 - 87s/epoch - 446ms/step
Epoch 856/1000
2023-10-01 01:52:25.152 
Epoch 856/1000 
	 loss: 28.6098, MinusLogProbMetric: 28.6098, val_loss: 29.2323, val_MinusLogProbMetric: 29.2323

Epoch 856: val_loss did not improve from 29.22654
196/196 - 85s - loss: 28.6098 - MinusLogProbMetric: 28.6098 - val_loss: 29.2323 - val_MinusLogProbMetric: 29.2323 - lr: 3.4722e-06 - 85s/epoch - 436ms/step
Epoch 857/1000
2023-10-01 01:53:51.971 
Epoch 857/1000 
	 loss: 28.6099, MinusLogProbMetric: 28.6099, val_loss: 29.2438, val_MinusLogProbMetric: 29.2438

Epoch 857: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6099 - MinusLogProbMetric: 28.6099 - val_loss: 29.2438 - val_MinusLogProbMetric: 29.2438 - lr: 3.4722e-06 - 87s/epoch - 443ms/step
Epoch 858/1000
2023-10-01 01:55:17.777 
Epoch 858/1000 
	 loss: 28.6091, MinusLogProbMetric: 28.6091, val_loss: 29.2390, val_MinusLogProbMetric: 29.2390

Epoch 858: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6091 - MinusLogProbMetric: 28.6091 - val_loss: 29.2390 - val_MinusLogProbMetric: 29.2390 - lr: 3.4722e-06 - 86s/epoch - 438ms/step
Epoch 859/1000
2023-10-01 01:56:43.620 
Epoch 859/1000 
	 loss: 28.6088, MinusLogProbMetric: 28.6088, val_loss: 29.2272, val_MinusLogProbMetric: 29.2272

Epoch 859: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6088 - MinusLogProbMetric: 28.6088 - val_loss: 29.2272 - val_MinusLogProbMetric: 29.2272 - lr: 3.4722e-06 - 86s/epoch - 438ms/step
Epoch 860/1000
2023-10-01 01:58:09.152 
Epoch 860/1000 
	 loss: 28.6101, MinusLogProbMetric: 28.6101, val_loss: 29.2446, val_MinusLogProbMetric: 29.2446

Epoch 860: val_loss did not improve from 29.22654
196/196 - 86s - loss: 28.6101 - MinusLogProbMetric: 28.6101 - val_loss: 29.2446 - val_MinusLogProbMetric: 29.2446 - lr: 3.4722e-06 - 86s/epoch - 436ms/step
Epoch 861/1000
2023-10-01 01:59:34.023 
Epoch 861/1000 
	 loss: 28.6097, MinusLogProbMetric: 28.6097, val_loss: 29.2379, val_MinusLogProbMetric: 29.2379

Epoch 861: val_loss did not improve from 29.22654
196/196 - 85s - loss: 28.6097 - MinusLogProbMetric: 28.6097 - val_loss: 29.2379 - val_MinusLogProbMetric: 29.2379 - lr: 3.4722e-06 - 85s/epoch - 433ms/step
Epoch 862/1000
2023-10-01 02:00:58.853 
Epoch 862/1000 
	 loss: 28.6079, MinusLogProbMetric: 28.6079, val_loss: 29.2505, val_MinusLogProbMetric: 29.2505

Epoch 862: val_loss did not improve from 29.22654
196/196 - 85s - loss: 28.6079 - MinusLogProbMetric: 28.6079 - val_loss: 29.2505 - val_MinusLogProbMetric: 29.2505 - lr: 3.4722e-06 - 85s/epoch - 433ms/step
Epoch 863/1000
2023-10-01 02:02:28.359 
Epoch 863/1000 
	 loss: 28.6085, MinusLogProbMetric: 28.6085, val_loss: 29.2490, val_MinusLogProbMetric: 29.2490

Epoch 863: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6085 - MinusLogProbMetric: 28.6085 - val_loss: 29.2490 - val_MinusLogProbMetric: 29.2490 - lr: 3.4722e-06 - 90s/epoch - 457ms/step
Epoch 864/1000
2023-10-01 02:04:00.226 
Epoch 864/1000 
	 loss: 28.6105, MinusLogProbMetric: 28.6105, val_loss: 29.2451, val_MinusLogProbMetric: 29.2451

Epoch 864: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6105 - MinusLogProbMetric: 28.6105 - val_loss: 29.2451 - val_MinusLogProbMetric: 29.2451 - lr: 3.4722e-06 - 92s/epoch - 468ms/step
Epoch 865/1000
2023-10-01 02:05:28.662 
Epoch 865/1000 
	 loss: 28.6100, MinusLogProbMetric: 28.6100, val_loss: 29.2381, val_MinusLogProbMetric: 29.2381

Epoch 865: val_loss did not improve from 29.22654
196/196 - 88s - loss: 28.6100 - MinusLogProbMetric: 28.6100 - val_loss: 29.2381 - val_MinusLogProbMetric: 29.2381 - lr: 3.4722e-06 - 88s/epoch - 451ms/step
Epoch 866/1000
2023-10-01 02:06:57.866 
Epoch 866/1000 
	 loss: 28.6099, MinusLogProbMetric: 28.6099, val_loss: 29.2502, val_MinusLogProbMetric: 29.2502

Epoch 866: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6099 - MinusLogProbMetric: 28.6099 - val_loss: 29.2502 - val_MinusLogProbMetric: 29.2502 - lr: 3.4722e-06 - 89s/epoch - 455ms/step
Epoch 867/1000
2023-10-01 02:08:26.141 
Epoch 867/1000 
	 loss: 28.6087, MinusLogProbMetric: 28.6087, val_loss: 29.2332, val_MinusLogProbMetric: 29.2332

Epoch 867: val_loss did not improve from 29.22654
196/196 - 88s - loss: 28.6087 - MinusLogProbMetric: 28.6087 - val_loss: 29.2332 - val_MinusLogProbMetric: 29.2332 - lr: 3.4722e-06 - 88s/epoch - 450ms/step
Epoch 868/1000
2023-10-01 02:09:59.830 
Epoch 868/1000 
	 loss: 28.6112, MinusLogProbMetric: 28.6112, val_loss: 29.2377, val_MinusLogProbMetric: 29.2377

Epoch 868: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6112 - MinusLogProbMetric: 28.6112 - val_loss: 29.2377 - val_MinusLogProbMetric: 29.2377 - lr: 3.4722e-06 - 94s/epoch - 478ms/step
Epoch 869/1000
2023-10-01 02:11:28.475 
Epoch 869/1000 
	 loss: 28.6094, MinusLogProbMetric: 28.6094, val_loss: 29.2366, val_MinusLogProbMetric: 29.2366

Epoch 869: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6094 - MinusLogProbMetric: 28.6094 - val_loss: 29.2366 - val_MinusLogProbMetric: 29.2366 - lr: 3.4722e-06 - 89s/epoch - 452ms/step
Epoch 870/1000
2023-10-01 02:13:00.437 
Epoch 870/1000 
	 loss: 28.6094, MinusLogProbMetric: 28.6094, val_loss: 29.2423, val_MinusLogProbMetric: 29.2423

Epoch 870: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6094 - MinusLogProbMetric: 28.6094 - val_loss: 29.2423 - val_MinusLogProbMetric: 29.2423 - lr: 3.4722e-06 - 92s/epoch - 469ms/step
Epoch 871/1000
2023-10-01 02:14:34.217 
Epoch 871/1000 
	 loss: 28.6088, MinusLogProbMetric: 28.6088, val_loss: 29.2336, val_MinusLogProbMetric: 29.2336

Epoch 871: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6088 - MinusLogProbMetric: 28.6088 - val_loss: 29.2336 - val_MinusLogProbMetric: 29.2336 - lr: 3.4722e-06 - 94s/epoch - 479ms/step
Epoch 872/1000
2023-10-01 02:16:04.518 
Epoch 872/1000 
	 loss: 28.6092, MinusLogProbMetric: 28.6092, val_loss: 29.2404, val_MinusLogProbMetric: 29.2404

Epoch 872: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6092 - MinusLogProbMetric: 28.6092 - val_loss: 29.2404 - val_MinusLogProbMetric: 29.2404 - lr: 3.4722e-06 - 90s/epoch - 461ms/step
Epoch 873/1000
2023-10-01 02:17:39.908 
Epoch 873/1000 
	 loss: 28.6108, MinusLogProbMetric: 28.6108, val_loss: 29.2468, val_MinusLogProbMetric: 29.2468

Epoch 873: val_loss did not improve from 29.22654
196/196 - 95s - loss: 28.6108 - MinusLogProbMetric: 28.6108 - val_loss: 29.2468 - val_MinusLogProbMetric: 29.2468 - lr: 3.4722e-06 - 95s/epoch - 487ms/step
Epoch 874/1000
2023-10-01 02:19:11.783 
Epoch 874/1000 
	 loss: 28.6103, MinusLogProbMetric: 28.6103, val_loss: 29.2418, val_MinusLogProbMetric: 29.2418

Epoch 874: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6103 - MinusLogProbMetric: 28.6103 - val_loss: 29.2418 - val_MinusLogProbMetric: 29.2418 - lr: 3.4722e-06 - 92s/epoch - 469ms/step
Epoch 875/1000
2023-10-01 02:20:37.326 
Epoch 875/1000 
	 loss: 28.6089, MinusLogProbMetric: 28.6089, val_loss: 29.2495, val_MinusLogProbMetric: 29.2495

Epoch 875: val_loss did not improve from 29.22654
196/196 - 85s - loss: 28.6089 - MinusLogProbMetric: 28.6089 - val_loss: 29.2495 - val_MinusLogProbMetric: 29.2495 - lr: 3.4722e-06 - 85s/epoch - 436ms/step
Epoch 876/1000
2023-10-01 02:22:04.483 
Epoch 876/1000 
	 loss: 28.6092, MinusLogProbMetric: 28.6092, val_loss: 29.2422, val_MinusLogProbMetric: 29.2422

Epoch 876: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6092 - MinusLogProbMetric: 28.6092 - val_loss: 29.2422 - val_MinusLogProbMetric: 29.2422 - lr: 3.4722e-06 - 87s/epoch - 445ms/step
Epoch 877/1000
2023-10-01 02:23:39.410 
Epoch 877/1000 
	 loss: 28.6089, MinusLogProbMetric: 28.6089, val_loss: 29.2390, val_MinusLogProbMetric: 29.2390

Epoch 877: val_loss did not improve from 29.22654
196/196 - 95s - loss: 28.6089 - MinusLogProbMetric: 28.6089 - val_loss: 29.2390 - val_MinusLogProbMetric: 29.2390 - lr: 3.4722e-06 - 95s/epoch - 485ms/step
Epoch 878/1000
2023-10-01 02:25:13.533 
Epoch 878/1000 
	 loss: 28.6097, MinusLogProbMetric: 28.6097, val_loss: 29.2362, val_MinusLogProbMetric: 29.2362

Epoch 878: val_loss did not improve from 29.22654
196/196 - 94s - loss: 28.6097 - MinusLogProbMetric: 28.6097 - val_loss: 29.2362 - val_MinusLogProbMetric: 29.2362 - lr: 3.4722e-06 - 94s/epoch - 480ms/step
Epoch 879/1000
2023-10-01 02:26:42.272 
Epoch 879/1000 
	 loss: 28.6093, MinusLogProbMetric: 28.6093, val_loss: 29.2397, val_MinusLogProbMetric: 29.2397

Epoch 879: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6093 - MinusLogProbMetric: 28.6093 - val_loss: 29.2397 - val_MinusLogProbMetric: 29.2397 - lr: 3.4722e-06 - 89s/epoch - 452ms/step
Epoch 880/1000
2023-10-01 02:28:11.636 
Epoch 880/1000 
	 loss: 28.6097, MinusLogProbMetric: 28.6097, val_loss: 29.2482, val_MinusLogProbMetric: 29.2482

Epoch 880: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6097 - MinusLogProbMetric: 28.6097 - val_loss: 29.2482 - val_MinusLogProbMetric: 29.2482 - lr: 3.4722e-06 - 89s/epoch - 456ms/step
Epoch 881/1000
2023-10-01 02:29:41.444 
Epoch 881/1000 
	 loss: 28.6098, MinusLogProbMetric: 28.6098, val_loss: 29.2439, val_MinusLogProbMetric: 29.2439

Epoch 881: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6098 - MinusLogProbMetric: 28.6098 - val_loss: 29.2439 - val_MinusLogProbMetric: 29.2439 - lr: 3.4722e-06 - 90s/epoch - 458ms/step
Epoch 882/1000
2023-10-01 02:31:13.427 
Epoch 882/1000 
	 loss: 28.6082, MinusLogProbMetric: 28.6082, val_loss: 29.2470, val_MinusLogProbMetric: 29.2470

Epoch 882: val_loss did not improve from 29.22654
196/196 - 92s - loss: 28.6082 - MinusLogProbMetric: 28.6082 - val_loss: 29.2470 - val_MinusLogProbMetric: 29.2470 - lr: 3.4722e-06 - 92s/epoch - 469ms/step
Epoch 883/1000
2023-10-01 02:32:42.988 
Epoch 883/1000 
	 loss: 28.6095, MinusLogProbMetric: 28.6095, val_loss: 29.2428, val_MinusLogProbMetric: 29.2428

Epoch 883: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6095 - MinusLogProbMetric: 28.6095 - val_loss: 29.2428 - val_MinusLogProbMetric: 29.2428 - lr: 3.4722e-06 - 90s/epoch - 457ms/step
Epoch 884/1000
2023-10-01 02:34:07.905 
Epoch 884/1000 
	 loss: 28.6084, MinusLogProbMetric: 28.6084, val_loss: 29.2429, val_MinusLogProbMetric: 29.2429

Epoch 884: val_loss did not improve from 29.22654
196/196 - 85s - loss: 28.6084 - MinusLogProbMetric: 28.6084 - val_loss: 29.2429 - val_MinusLogProbMetric: 29.2429 - lr: 3.4722e-06 - 85s/epoch - 433ms/step
Epoch 885/1000
2023-10-01 02:35:36.672 
Epoch 885/1000 
	 loss: 28.6090, MinusLogProbMetric: 28.6090, val_loss: 29.2378, val_MinusLogProbMetric: 29.2378

Epoch 885: val_loss did not improve from 29.22654
196/196 - 89s - loss: 28.6090 - MinusLogProbMetric: 28.6090 - val_loss: 29.2378 - val_MinusLogProbMetric: 29.2378 - lr: 3.4722e-06 - 89s/epoch - 453ms/step
Epoch 886/1000
2023-10-01 02:37:05.142 
Epoch 886/1000 
	 loss: 28.6099, MinusLogProbMetric: 28.6099, val_loss: 29.2510, val_MinusLogProbMetric: 29.2510

Epoch 886: val_loss did not improve from 29.22654
196/196 - 88s - loss: 28.6099 - MinusLogProbMetric: 28.6099 - val_loss: 29.2510 - val_MinusLogProbMetric: 29.2510 - lr: 3.4722e-06 - 88s/epoch - 451ms/step
Epoch 887/1000
2023-10-01 02:38:34.727 
Epoch 887/1000 
	 loss: 28.6092, MinusLogProbMetric: 28.6092, val_loss: 29.2528, val_MinusLogProbMetric: 29.2528

Epoch 887: val_loss did not improve from 29.22654
196/196 - 90s - loss: 28.6092 - MinusLogProbMetric: 28.6092 - val_loss: 29.2528 - val_MinusLogProbMetric: 29.2528 - lr: 3.4722e-06 - 90s/epoch - 457ms/step
Epoch 888/1000
2023-10-01 02:40:01.270 
Epoch 888/1000 
	 loss: 28.6096, MinusLogProbMetric: 28.6096, val_loss: 29.2461, val_MinusLogProbMetric: 29.2461

Epoch 888: val_loss did not improve from 29.22654
196/196 - 87s - loss: 28.6096 - MinusLogProbMetric: 28.6096 - val_loss: 29.2461 - val_MinusLogProbMetric: 29.2461 - lr: 3.4722e-06 - 87s/epoch - 442ms/step
Epoch 889/1000
2023-10-01 02:41:30.118 
Epoch 889/1000 
	 loss: 28.6075, MinusLogProbMetric: 28.6075, val_loss: 29.2452, val_MinusLogProbMetric: 29.2452

Epoch 889: val_loss did not improve from 29.22654
Restoring model weights from the end of the best epoch: 789.
196/196 - 89s - loss: 28.6075 - MinusLogProbMetric: 28.6075 - val_loss: 29.2452 - val_MinusLogProbMetric: 29.2452 - lr: 3.4722e-06 - 89s/epoch - 455ms/step
Epoch 889: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 60.02988206897862 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 34.17974070296623 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 23.128502813982777 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 31.192679592058994 seconds.
Training succeeded with seed 440.
Model trained in 66168.33 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 155.64 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 157.01 s.
===========
Run 347/720 done in 72205.90 s.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

===========
Generating train data for run 349.
===========
Train data generated in 0.72 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_196"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_197 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7fc1d8f37e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb35dd900>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb35dd900>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc0e858abc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbfb352f7c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbfb352fd30>, <keras.callbacks.ModelCheckpoint object at 0x7fbfb352fdf0>, <keras.callbacks.EarlyStopping object at 0x7fbfb352ffa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbfb352fd00>, <keras.callbacks.TerminateOnNaN object at 0x7fbfb352ff40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 02:44:36.594003
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 02:55:22.273 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 644s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 644s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 349.
===========
Train data generated in 0.90 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_207"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_208 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7fc1904af160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc324206560>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc324206560>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc1d82ae530>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc015146bf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc015147160>, <keras.callbacks.ModelCheckpoint object at 0x7fc015147220>, <keras.callbacks.EarlyStopping object at 0x7fc015147490>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc0151474c0>, <keras.callbacks.TerminateOnNaN object at 0x7fc015147100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 02:56:45.264875
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 03:06:37.109 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 591s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 591s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 349.
===========
Train data generated in 0.79 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_218"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_219 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7fc1eca1f430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc1d82de740>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc1d82de740>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc1ecae4ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc0153a9ae0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc0153abaf0>, <keras.callbacks.ModelCheckpoint object at 0x7fc0153aa920>, <keras.callbacks.EarlyStopping object at 0x7fc0153a9ab0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc0153abca0>, <keras.callbacks.TerminateOnNaN object at 0x7fc0153ab550>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 03:07:07.195215
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 03:16:02.173 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 534s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 534s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 349.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_229"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_230 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7fc192373700>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc0153b6890>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc0153b6890>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf303f4790>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc191c37b50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc191c645b0>, <keras.callbacks.ModelCheckpoint object at 0x7fc191c64670>, <keras.callbacks.EarlyStopping object at 0x7fc191c648e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc191c64910>, <keras.callbacks.TerminateOnNaN object at 0x7fc191c64550>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 03:16:30.390064
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 174248 calls to <function Model.make_train_function.<locals>.train_function at 0x7fc140555630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 03:26:36.766 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 606s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 606s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 349.
===========
Train data generated in 1.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_240"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_241 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7fbf988ebbb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb91365f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb91365f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc0681a6fe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbf98802bc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbf98803130>, <keras.callbacks.ModelCheckpoint object at 0x7fbf988031f0>, <keras.callbacks.EarlyStopping object at 0x7fbf98803460>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbf98803490>, <keras.callbacks.TerminateOnNaN object at 0x7fbf988030d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 03:27:04.099729
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 174249 calls to <function Model.make_train_function.<locals>.train_function at 0x7fc1d97ae0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 03:36:37.153 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 573s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 573s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 349.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_251"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_252 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7fbfb913e050>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc01459e7d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc01459e7d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc0006cad10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc06909ccd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc06909dff0>, <keras.callbacks.ModelCheckpoint object at 0x7fc06909db70>, <keras.callbacks.EarlyStopping object at 0x7fc06909db10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc06909d990>, <keras.callbacks.TerminateOnNaN object at 0x7fc06909d000>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 03:37:07.679204
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 03:45:53.780 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 526s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 526s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 349.
===========
Train data generated in 1.72 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_262"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_263 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7fc191aba8f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbf98a12c80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbf98a12c80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf32b41450>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc191153ee0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc19117c490>, <keras.callbacks.ModelCheckpoint object at 0x7fc19117c550>, <keras.callbacks.EarlyStopping object at 0x7fc19117c7c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc19117c7f0>, <keras.callbacks.TerminateOnNaN object at 0x7fc19117c430>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 03:47:11.506410
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 03:56:31.344 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 558s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 558s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 349.
===========
Train data generated in 1.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_273"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_274 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7fc0685223b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc068583a30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc068583a30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf3a7d39a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbf3a71be50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbf01b88400>, <keras.callbacks.ModelCheckpoint object at 0x7fbf01b884c0>, <keras.callbacks.EarlyStopping object at 0x7fbf01b88730>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbf01b88760>, <keras.callbacks.TerminateOnNaN object at 0x7fbf01b883a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 03:57:02.122535
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 04:06:54.962 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 589s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 589s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 349.
===========
Train data generated in 1.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_284"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_285 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7fc000cfe470>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc191cfbfd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc191cfbfd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc0697cdf30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc0545b3bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc0545cc160>, <keras.callbacks.ModelCheckpoint object at 0x7fc0545cc220>, <keras.callbacks.EarlyStopping object at 0x7fc0545cc490>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc0545cc4c0>, <keras.callbacks.TerminateOnNaN object at 0x7fc0545cc100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 04:07:24.209320
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 04:17:03.211 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 577s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 577s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 349.
===========
Train data generated in 1.01 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_295"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_296 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7fc16b9b7e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc190ffd2d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc190ffd2d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf3b2a9ed0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc16b5d3a90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc16b5d3fa0>, <keras.callbacks.ModelCheckpoint object at 0x7fc16b5d3fd0>, <keras.callbacks.EarlyStopping object at 0x7fc16b404340>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc16b4043a0>, <keras.callbacks.TerminateOnNaN object at 0x7fc16b404370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 04:17:31.431421
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 04:27:36.485 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 604s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 604s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 349.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_349/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_349
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_306"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_307 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7fbf696959f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbf30565300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbf30565300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc3c04b2740>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc33c3532b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_349/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc33c353820>, <keras.callbacks.ModelCheckpoint object at 0x7fc33c3538e0>, <keras.callbacks.EarlyStopping object at 0x7fc33c353b50>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc33c353b80>, <keras.callbacks.TerminateOnNaN object at 0x7fc33c3537c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_349/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 349/720 with hyperparameters:
timestamp = 2023-10-01 04:28:03.946700
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 04:38:04.568 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7287.4268, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 599s - loss: nan - MinusLogProbMetric: 7287.4268 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 599s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 349/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

===========
Generating train data for run 351.
===========
Train data generated in 2.96 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_317"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_318 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7fc015627ac0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb35fda50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb35fda50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc09c5c0a60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc000387220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc000387eb0>, <keras.callbacks.ModelCheckpoint object at 0x7fc000386380>, <keras.callbacks.EarlyStopping object at 0x7fc000387c10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc000386b60>, <keras.callbacks.TerminateOnNaN object at 0x7fc000386290>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 04:38:40.966100
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 04:52:08.321 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 807s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 807s/epoch - 4s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 351.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_328"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_329 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7fc16a6fee30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbee2b0d840>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbee2b0d840>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbee22ca170>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc16a52d570>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc16a52dae0>, <keras.callbacks.ModelCheckpoint object at 0x7fc16a52dba0>, <keras.callbacks.EarlyStopping object at 0x7fc16a52de10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc16a52de40>, <keras.callbacks.TerminateOnNaN object at 0x7fc16a52da80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 04:52:46.386750
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 05:06:50.401 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 843s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 843s/epoch - 4s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 351.
===========
Train data generated in 1.07 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_339"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_340 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7fbfa0b96290>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfa0f25600>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfa0f25600>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf39b7b130>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbee8414370>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbee84148e0>, <keras.callbacks.ModelCheckpoint object at 0x7fbee84149a0>, <keras.callbacks.EarlyStopping object at 0x7fbee8414c10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbee8414c40>, <keras.callbacks.TerminateOnNaN object at 0x7fbee8414880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 05:07:25.291573
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 05:20:40.003 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 793s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 793s/epoch - 4s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 351.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_350"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_351 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7fc14850a950>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc00d4c7b50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc00d4c7b50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbee81187c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1485a5090>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1485a5600>, <keras.callbacks.ModelCheckpoint object at 0x7fc1485a56c0>, <keras.callbacks.EarlyStopping object at 0x7fc1485a5930>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1485a5960>, <keras.callbacks.TerminateOnNaN object at 0x7fc1485a55a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 05:21:19.458593
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 05:35:50.007 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 869s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 869s/epoch - 4s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 351.
===========
Train data generated in 2.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_361"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_362 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7fc16a1eb340>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc015502470>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc015502470>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbe40137f70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc1696b3ee0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1696dc490>, <keras.callbacks.ModelCheckpoint object at 0x7fc1696dc550>, <keras.callbacks.EarlyStopping object at 0x7fc1696dc7c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc1696dc7f0>, <keras.callbacks.TerminateOnNaN object at 0x7fc1696dc430>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 05:37:22.122753
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 05:47:02.049 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 578s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 578s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 351.
===========
Train data generated in 1.51 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_372"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_373 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7fbe51ed5a20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc100179e40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc100179e40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbe51e17880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbe59220c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbe592211b0>, <keras.callbacks.ModelCheckpoint object at 0x7fbe59221270>, <keras.callbacks.EarlyStopping object at 0x7fbe592214e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbe59221510>, <keras.callbacks.TerminateOnNaN object at 0x7fbe59221150>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 05:47:29.091802
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 05:55:41.899 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 491s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 491s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 351.
===========
Train data generated in 0.75 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_383"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_384 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7fbf69f92950>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc16a004ee0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc16a004ee0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbe63f40070>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbf011ad780>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbf011ae020>, <keras.callbacks.ModelCheckpoint object at 0x7fbf011ae800>, <keras.callbacks.EarlyStopping object at 0x7fbf011adf60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbf011af880>, <keras.callbacks.TerminateOnNaN object at 0x7fbf011afdf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 05:56:34.374530
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 06:05:08.633 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 514s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 514s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 351.
===========
Train data generated in 0.55 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_394"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_395 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7fc14b1e2f20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc14b2bca00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc14b2bca00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc14b65b9d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc14b011660>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc14b011bd0>, <keras.callbacks.ModelCheckpoint object at 0x7fc14b011c90>, <keras.callbacks.EarlyStopping object at 0x7fc14b011f00>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc14b011f30>, <keras.callbacks.TerminateOnNaN object at 0x7fc14b011b70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 06:05:34.413065
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 06:14:50.537 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 555s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 555s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 351.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_405"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_406 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7fbee10eeb60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbf9981b550>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbf9981b550>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf68aaf910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbf382d0700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbf382d0c70>, <keras.callbacks.ModelCheckpoint object at 0x7fbf382d0d30>, <keras.callbacks.EarlyStopping object at 0x7fbf382d0fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbf382d0fd0>, <keras.callbacks.TerminateOnNaN object at 0x7fbf382d0c10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 06:15:17.750989
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 06:23:13.973 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 475s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 475s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 351.
===========
Train data generated in 0.87 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_416"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_417 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7fbe6317d2a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbeb9fcca90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbeb9fcca90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbe530a7220>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbe519d8be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbe519d9150>, <keras.callbacks.ModelCheckpoint object at 0x7fbe519d9210>, <keras.callbacks.EarlyStopping object at 0x7fbe519d9480>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbe519d94b0>, <keras.callbacks.TerminateOnNaN object at 0x7fbe519d90f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 06:23:38.521528
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 06:32:28.024 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 529s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 529s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 351.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_427"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_428 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7fbf02cb3a00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbee3679bd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbee3679bd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc1d82ac910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbe40635c90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbe40636200>, <keras.callbacks.ModelCheckpoint object at 0x7fbe406362c0>, <keras.callbacks.EarlyStopping object at 0x7fbe40636530>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbe40636560>, <keras.callbacks.TerminateOnNaN object at 0x7fbe406361a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-01 06:32:53.977660
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 06:40:54.078 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 479s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 479s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 351/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 352.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_438"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_439 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7fbe3a7e26b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbe3aa0c550>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbe3aa0c550>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbe4a730fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbe3a764df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbe3a765360>, <keras.callbacks.ModelCheckpoint object at 0x7fbe3a765420>, <keras.callbacks.EarlyStopping object at 0x7fbe3a765690>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbe3a7656c0>, <keras.callbacks.TerminateOnNaN object at 0x7fbe3a765300>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-01 06:41:18.257164
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 06:50:12.588 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6225.3677, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 532s - loss: nan - MinusLogProbMetric: 6225.3677 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 532s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 352.
===========
Train data generated in 0.50 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_449"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_450 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7fc0d7cf3580>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc1087b7910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc1087b7910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc1d97ebb80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc108711c30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc1087121a0>, <keras.callbacks.ModelCheckpoint object at 0x7fc108712260>, <keras.callbacks.EarlyStopping object at 0x7fc1087124d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc108712500>, <keras.callbacks.TerminateOnNaN object at 0x7fc108712140>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-01 06:50:36.403482
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 5: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 06:58:41.521 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6610.3550, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 485s - loss: nan - MinusLogProbMetric: 6610.3550 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 485s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 352.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_460"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_461 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7fbe31c514e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fc168d224a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fc168d224a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf6897b6a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc14aee8cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc14aee9240>, <keras.callbacks.ModelCheckpoint object at 0x7fc14aee9300>, <keras.callbacks.EarlyStopping object at 0x7fc14aee9570>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc14aee95a0>, <keras.callbacks.TerminateOnNaN object at 0x7fc14aee91e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-01 06:59:05.132253
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 24: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 07:08:27.442 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6203.8228, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 561s - loss: nan - MinusLogProbMetric: 6203.8228 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 561s/epoch - 3s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 352.
===========
Train data generated in 0.67 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_471"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_472 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7fbe599272b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb38206d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbfb38206d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc1d9691f60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbee1ebd0c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbee1ebe440>, <keras.callbacks.ModelCheckpoint object at 0x7fbee1ebe8c0>, <keras.callbacks.EarlyStopping object at 0x7fbee1ebf610>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbee1ebe770>, <keras.callbacks.TerminateOnNaN object at 0x7fbee1ebe4a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-01 07:08:52.009042
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-01 07:18:48.436 
Epoch 1/1000 
	 loss: 5387.1016, MinusLogProbMetric: 5387.1016, val_loss: 4115.0894, val_MinusLogProbMetric: 4115.0894

Epoch 1: val_loss improved from inf to 4115.08936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 599s - loss: 5387.1016 - MinusLogProbMetric: 5387.1016 - val_loss: 4115.0894 - val_MinusLogProbMetric: 4115.0894 - lr: 3.7037e-05 - 599s/epoch - 3s/step
Epoch 2/1000
2023-10-01 07:20:57.489 
Epoch 2/1000 
	 loss: 3155.9502, MinusLogProbMetric: 3155.9502, val_loss: 2614.3149, val_MinusLogProbMetric: 2614.3149

Epoch 2: val_loss improved from 4115.08936 to 2614.31494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 128s - loss: 3155.9502 - MinusLogProbMetric: 3155.9502 - val_loss: 2614.3149 - val_MinusLogProbMetric: 2614.3149 - lr: 3.7037e-05 - 128s/epoch - 653ms/step
Epoch 3/1000
2023-10-01 07:23:01.224 
Epoch 3/1000 
	 loss: 2215.9299, MinusLogProbMetric: 2215.9299, val_loss: 1769.4264, val_MinusLogProbMetric: 1769.4264

Epoch 3: val_loss improved from 2614.31494 to 1769.42639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 123s - loss: 2215.9299 - MinusLogProbMetric: 2215.9299 - val_loss: 1769.4264 - val_MinusLogProbMetric: 1769.4264 - lr: 3.7037e-05 - 123s/epoch - 629ms/step
Epoch 4/1000
2023-10-01 07:25:06.653 
Epoch 4/1000 
	 loss: 1532.1226, MinusLogProbMetric: 1532.1226, val_loss: 1698.6646, val_MinusLogProbMetric: 1698.6646

Epoch 4: val_loss improved from 1769.42639 to 1698.66455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 124s - loss: 1532.1226 - MinusLogProbMetric: 1532.1226 - val_loss: 1698.6646 - val_MinusLogProbMetric: 1698.6646 - lr: 3.7037e-05 - 124s/epoch - 633ms/step
Epoch 5/1000
2023-10-01 07:27:09.799 
Epoch 5/1000 
	 loss: 1491.9147, MinusLogProbMetric: 1491.9147, val_loss: 2435.5105, val_MinusLogProbMetric: 2435.5105

Epoch 5: val_loss did not improve from 1698.66455
196/196 - 121s - loss: 1491.9147 - MinusLogProbMetric: 1491.9147 - val_loss: 2435.5105 - val_MinusLogProbMetric: 2435.5105 - lr: 3.7037e-05 - 121s/epoch - 618ms/step
Epoch 6/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 154: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 07:28:47.441 
Epoch 6/1000 
	 loss: nan, MinusLogProbMetric: 1954.0089, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 6: val_loss did not improve from 1698.66455
196/196 - 98s - loss: nan - MinusLogProbMetric: 1954.0089 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 98s/epoch - 498ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 352.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_482"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_483 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7fbe3b4437c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbf3844b370>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbf3844b370>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fc14a964730>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc14a9ba020>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc14a9ba590>, <keras.callbacks.ModelCheckpoint object at 0x7fc14a9ba650>, <keras.callbacks.EarlyStopping object at 0x7fc14a9ba8c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc14a9ba8f0>, <keras.callbacks.TerminateOnNaN object at 0x7fc14a9ba530>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-01 07:29:12.985823
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-01 07:40:10.638 
Epoch 1/1000 
	 loss: 1459.6121, MinusLogProbMetric: 1459.6121, val_loss: 1325.6548, val_MinusLogProbMetric: 1325.6548

Epoch 1: val_loss improved from inf to 1325.65479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 659s - loss: 1459.6121 - MinusLogProbMetric: 1459.6121 - val_loss: 1325.6548 - val_MinusLogProbMetric: 1325.6548 - lr: 1.2346e-05 - 659s/epoch - 3s/step
Epoch 2/1000
2023-10-01 07:42:20.876 
Epoch 2/1000 
	 loss: 1209.8647, MinusLogProbMetric: 1209.8647, val_loss: 1097.7220, val_MinusLogProbMetric: 1097.7220

Epoch 2: val_loss improved from 1325.65479 to 1097.72205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 130s - loss: 1209.8647 - MinusLogProbMetric: 1209.8647 - val_loss: 1097.7220 - val_MinusLogProbMetric: 1097.7220 - lr: 1.2346e-05 - 130s/epoch - 665ms/step
Epoch 3/1000
2023-10-01 07:44:29.853 
Epoch 3/1000 
	 loss: 1133.3956, MinusLogProbMetric: 1133.3956, val_loss: 1171.7261, val_MinusLogProbMetric: 1171.7261

Epoch 3: val_loss did not improve from 1097.72205
196/196 - 125s - loss: 1133.3956 - MinusLogProbMetric: 1133.3956 - val_loss: 1171.7261 - val_MinusLogProbMetric: 1171.7261 - lr: 1.2346e-05 - 125s/epoch - 638ms/step
Epoch 4/1000
2023-10-01 07:46:30.991 
Epoch 4/1000 
	 loss: 1053.7297, MinusLogProbMetric: 1053.7297, val_loss: 940.6510, val_MinusLogProbMetric: 940.6510

Epoch 4: val_loss improved from 1097.72205 to 940.65100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 124s - loss: 1053.7297 - MinusLogProbMetric: 1053.7297 - val_loss: 940.6510 - val_MinusLogProbMetric: 940.6510 - lr: 1.2346e-05 - 124s/epoch - 634ms/step
Epoch 5/1000
2023-10-01 07:48:39.209 
Epoch 5/1000 
	 loss: 869.1777, MinusLogProbMetric: 869.1777, val_loss: 832.0045, val_MinusLogProbMetric: 832.0045

Epoch 5: val_loss improved from 940.65100 to 832.00452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 128s - loss: 869.1777 - MinusLogProbMetric: 869.1777 - val_loss: 832.0045 - val_MinusLogProbMetric: 832.0045 - lr: 1.2346e-05 - 128s/epoch - 653ms/step
Epoch 6/1000
2023-10-01 07:50:48.273 
Epoch 6/1000 
	 loss: 808.9344, MinusLogProbMetric: 808.9344, val_loss: 781.4539, val_MinusLogProbMetric: 781.4539

Epoch 6: val_loss improved from 832.00452 to 781.45392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 128s - loss: 808.9344 - MinusLogProbMetric: 808.9344 - val_loss: 781.4539 - val_MinusLogProbMetric: 781.4539 - lr: 1.2346e-05 - 128s/epoch - 653ms/step
Epoch 7/1000
2023-10-01 07:52:54.774 
Epoch 7/1000 
	 loss: 974.7842, MinusLogProbMetric: 974.7842, val_loss: 928.4293, val_MinusLogProbMetric: 928.4293

Epoch 7: val_loss did not improve from 781.45392
196/196 - 124s - loss: 974.7842 - MinusLogProbMetric: 974.7842 - val_loss: 928.4293 - val_MinusLogProbMetric: 928.4293 - lr: 1.2346e-05 - 124s/epoch - 635ms/step
Epoch 8/1000
2023-10-01 07:54:53.870 
Epoch 8/1000 
	 loss: 1016.6031, MinusLogProbMetric: 1016.6031, val_loss: 1023.9263, val_MinusLogProbMetric: 1023.9263

Epoch 8: val_loss did not improve from 781.45392
196/196 - 119s - loss: 1016.6031 - MinusLogProbMetric: 1016.6031 - val_loss: 1023.9263 - val_MinusLogProbMetric: 1023.9263 - lr: 1.2346e-05 - 119s/epoch - 608ms/step
Epoch 9/1000
2023-10-01 07:56:53.748 
Epoch 9/1000 
	 loss: 951.7120, MinusLogProbMetric: 951.7120, val_loss: 946.5150, val_MinusLogProbMetric: 946.5150

Epoch 9: val_loss did not improve from 781.45392
196/196 - 120s - loss: 951.7120 - MinusLogProbMetric: 951.7120 - val_loss: 946.5150 - val_MinusLogProbMetric: 946.5150 - lr: 1.2346e-05 - 120s/epoch - 612ms/step
Epoch 10/1000
2023-10-01 07:58:56.990 
Epoch 10/1000 
	 loss: 875.3394, MinusLogProbMetric: 875.3394, val_loss: 826.1344, val_MinusLogProbMetric: 826.1344

Epoch 10: val_loss did not improve from 781.45392
196/196 - 123s - loss: 875.3394 - MinusLogProbMetric: 875.3394 - val_loss: 826.1344 - val_MinusLogProbMetric: 826.1344 - lr: 1.2346e-05 - 123s/epoch - 629ms/step
Epoch 11/1000
2023-10-01 08:01:00.511 
Epoch 11/1000 
	 loss: 845.4325, MinusLogProbMetric: 845.4325, val_loss: 823.2039, val_MinusLogProbMetric: 823.2039

Epoch 11: val_loss did not improve from 781.45392
196/196 - 124s - loss: 845.4325 - MinusLogProbMetric: 845.4325 - val_loss: 823.2039 - val_MinusLogProbMetric: 823.2039 - lr: 1.2346e-05 - 124s/epoch - 630ms/step
Epoch 12/1000
2023-10-01 08:03:03.581 
Epoch 12/1000 
	 loss: 880.4996, MinusLogProbMetric: 880.4996, val_loss: 889.9874, val_MinusLogProbMetric: 889.9874

Epoch 12: val_loss did not improve from 781.45392
196/196 - 123s - loss: 880.4996 - MinusLogProbMetric: 880.4996 - val_loss: 889.9874 - val_MinusLogProbMetric: 889.9874 - lr: 1.2346e-05 - 123s/epoch - 628ms/step
Epoch 13/1000
2023-10-01 08:05:07.124 
Epoch 13/1000 
	 loss: 857.1445, MinusLogProbMetric: 857.1445, val_loss: 837.8278, val_MinusLogProbMetric: 837.8278

Epoch 13: val_loss did not improve from 781.45392
196/196 - 124s - loss: 857.1445 - MinusLogProbMetric: 857.1445 - val_loss: 837.8278 - val_MinusLogProbMetric: 837.8278 - lr: 1.2346e-05 - 124s/epoch - 630ms/step
Epoch 14/1000
2023-10-01 08:07:15.104 
Epoch 14/1000 
	 loss: 825.3516, MinusLogProbMetric: 825.3516, val_loss: 821.3126, val_MinusLogProbMetric: 821.3126

Epoch 14: val_loss did not improve from 781.45392
196/196 - 128s - loss: 825.3516 - MinusLogProbMetric: 825.3516 - val_loss: 821.3126 - val_MinusLogProbMetric: 821.3126 - lr: 1.2346e-05 - 128s/epoch - 653ms/step
Epoch 15/1000
2023-10-01 08:09:18.490 
Epoch 15/1000 
	 loss: 792.5603, MinusLogProbMetric: 792.5603, val_loss: 774.3339, val_MinusLogProbMetric: 774.3339

Epoch 15: val_loss improved from 781.45392 to 774.33386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 126s - loss: 792.5603 - MinusLogProbMetric: 792.5603 - val_loss: 774.3339 - val_MinusLogProbMetric: 774.3339 - lr: 1.2346e-05 - 126s/epoch - 642ms/step
Epoch 16/1000
2023-10-01 08:11:25.798 
Epoch 16/1000 
	 loss: 798.5131, MinusLogProbMetric: 798.5131, val_loss: 837.6507, val_MinusLogProbMetric: 837.6507

Epoch 16: val_loss did not improve from 774.33386
196/196 - 125s - loss: 798.5131 - MinusLogProbMetric: 798.5131 - val_loss: 837.6507 - val_MinusLogProbMetric: 837.6507 - lr: 1.2346e-05 - 125s/epoch - 637ms/step
Epoch 17/1000
2023-10-01 08:13:28.934 
Epoch 17/1000 
	 loss: 840.4728, MinusLogProbMetric: 840.4728, val_loss: 857.7519, val_MinusLogProbMetric: 857.7519

Epoch 17: val_loss did not improve from 774.33386
196/196 - 123s - loss: 840.4728 - MinusLogProbMetric: 840.4728 - val_loss: 857.7519 - val_MinusLogProbMetric: 857.7519 - lr: 1.2346e-05 - 123s/epoch - 629ms/step
Epoch 18/1000
2023-10-01 08:15:28.275 
Epoch 18/1000 
	 loss: 846.9800, MinusLogProbMetric: 846.9800, val_loss: 811.6813, val_MinusLogProbMetric: 811.6813

Epoch 18: val_loss did not improve from 774.33386
196/196 - 119s - loss: 846.9800 - MinusLogProbMetric: 846.9800 - val_loss: 811.6813 - val_MinusLogProbMetric: 811.6813 - lr: 1.2346e-05 - 119s/epoch - 609ms/step
Epoch 19/1000
2023-10-01 08:17:31.045 
Epoch 19/1000 
	 loss: 870.2819, MinusLogProbMetric: 870.2819, val_loss: 892.5088, val_MinusLogProbMetric: 892.5088

Epoch 19: val_loss did not improve from 774.33386
196/196 - 123s - loss: 870.2819 - MinusLogProbMetric: 870.2819 - val_loss: 892.5088 - val_MinusLogProbMetric: 892.5088 - lr: 1.2346e-05 - 123s/epoch - 626ms/step
Epoch 20/1000
2023-10-01 08:19:40.073 
Epoch 20/1000 
	 loss: 827.8414, MinusLogProbMetric: 827.8414, val_loss: 801.2090, val_MinusLogProbMetric: 801.2090

Epoch 20: val_loss did not improve from 774.33386
196/196 - 129s - loss: 827.8414 - MinusLogProbMetric: 827.8414 - val_loss: 801.2090 - val_MinusLogProbMetric: 801.2090 - lr: 1.2346e-05 - 129s/epoch - 658ms/step
Epoch 21/1000
2023-10-01 08:21:46.394 
Epoch 21/1000 
	 loss: 768.0958, MinusLogProbMetric: 768.0958, val_loss: 745.0583, val_MinusLogProbMetric: 745.0583

Epoch 21: val_loss improved from 774.33386 to 745.05835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 130s - loss: 768.0958 - MinusLogProbMetric: 768.0958 - val_loss: 745.0583 - val_MinusLogProbMetric: 745.0583 - lr: 1.2346e-05 - 130s/epoch - 664ms/step
Epoch 22/1000
2023-10-01 08:23:56.680 
Epoch 22/1000 
	 loss: 746.6029, MinusLogProbMetric: 746.6029, val_loss: 793.9549, val_MinusLogProbMetric: 793.9549

Epoch 22: val_loss did not improve from 745.05835
196/196 - 126s - loss: 746.6029 - MinusLogProbMetric: 746.6029 - val_loss: 793.9549 - val_MinusLogProbMetric: 793.9549 - lr: 1.2346e-05 - 126s/epoch - 645ms/step
Epoch 23/1000
2023-10-01 08:26:00.759 
Epoch 23/1000 
	 loss: 771.4462, MinusLogProbMetric: 771.4462, val_loss: 740.5470, val_MinusLogProbMetric: 740.5470

Epoch 23: val_loss improved from 745.05835 to 740.54700, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 127s - loss: 771.4462 - MinusLogProbMetric: 771.4462 - val_loss: 740.5470 - val_MinusLogProbMetric: 740.5470 - lr: 1.2346e-05 - 127s/epoch - 649ms/step
Epoch 24/1000
2023-10-01 08:28:08.291 
Epoch 24/1000 
	 loss: 737.3383, MinusLogProbMetric: 737.3383, val_loss: 738.8644, val_MinusLogProbMetric: 738.8644

Epoch 24: val_loss improved from 740.54700 to 738.86438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 128s - loss: 737.3383 - MinusLogProbMetric: 737.3383 - val_loss: 738.8644 - val_MinusLogProbMetric: 738.8644 - lr: 1.2346e-05 - 128s/epoch - 654ms/step
Epoch 25/1000
2023-10-01 08:30:20.997 
Epoch 25/1000 
	 loss: 724.3263, MinusLogProbMetric: 724.3263, val_loss: 709.2518, val_MinusLogProbMetric: 709.2518

Epoch 25: val_loss improved from 738.86438 to 709.25177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 131s - loss: 724.3263 - MinusLogProbMetric: 724.3263 - val_loss: 709.2518 - val_MinusLogProbMetric: 709.2518 - lr: 1.2346e-05 - 131s/epoch - 669ms/step
Epoch 26/1000
2023-10-01 08:32:25.009 
Epoch 26/1000 
	 loss: 705.2529, MinusLogProbMetric: 705.2529, val_loss: 724.9259, val_MinusLogProbMetric: 724.9259

Epoch 26: val_loss did not improve from 709.25177
196/196 - 122s - loss: 705.2529 - MinusLogProbMetric: 705.2529 - val_loss: 724.9259 - val_MinusLogProbMetric: 724.9259 - lr: 1.2346e-05 - 122s/epoch - 622ms/step
Epoch 27/1000
2023-10-01 08:34:26.861 
Epoch 27/1000 
	 loss: 749.2379, MinusLogProbMetric: 749.2379, val_loss: 735.6338, val_MinusLogProbMetric: 735.6338

Epoch 27: val_loss did not improve from 709.25177
196/196 - 122s - loss: 749.2379 - MinusLogProbMetric: 749.2379 - val_loss: 735.6338 - val_MinusLogProbMetric: 735.6338 - lr: 1.2346e-05 - 122s/epoch - 622ms/step
Epoch 28/1000
2023-10-01 08:36:31.094 
Epoch 28/1000 
	 loss: 723.7354, MinusLogProbMetric: 723.7354, val_loss: 715.8870, val_MinusLogProbMetric: 715.8870

Epoch 28: val_loss did not improve from 709.25177
196/196 - 124s - loss: 723.7354 - MinusLogProbMetric: 723.7354 - val_loss: 715.8870 - val_MinusLogProbMetric: 715.8870 - lr: 1.2346e-05 - 124s/epoch - 634ms/step
Epoch 29/1000
2023-10-01 08:38:46.984 
Epoch 29/1000 
	 loss: 746.0934, MinusLogProbMetric: 746.0934, val_loss: 753.3336, val_MinusLogProbMetric: 753.3336

Epoch 29: val_loss did not improve from 709.25177
196/196 - 136s - loss: 746.0934 - MinusLogProbMetric: 746.0934 - val_loss: 753.3336 - val_MinusLogProbMetric: 753.3336 - lr: 1.2346e-05 - 136s/epoch - 694ms/step
Epoch 30/1000
2023-10-01 08:41:16.694 
Epoch 30/1000 
	 loss: 747.6959, MinusLogProbMetric: 747.6959, val_loss: 745.6782, val_MinusLogProbMetric: 745.6782

Epoch 30: val_loss did not improve from 709.25177
196/196 - 150s - loss: 747.6959 - MinusLogProbMetric: 747.6959 - val_loss: 745.6782 - val_MinusLogProbMetric: 745.6782 - lr: 1.2346e-05 - 150s/epoch - 764ms/step
Epoch 31/1000
2023-10-01 08:43:51.227 
Epoch 31/1000 
	 loss: 737.7822, MinusLogProbMetric: 737.7822, val_loss: 721.5742, val_MinusLogProbMetric: 721.5742

Epoch 31: val_loss did not improve from 709.25177
196/196 - 155s - loss: 737.7822 - MinusLogProbMetric: 737.7822 - val_loss: 721.5742 - val_MinusLogProbMetric: 721.5742 - lr: 1.2346e-05 - 155s/epoch - 788ms/step
Epoch 32/1000
2023-10-01 08:46:21.458 
Epoch 32/1000 
	 loss: 762.1896, MinusLogProbMetric: 762.1896, val_loss: 745.4908, val_MinusLogProbMetric: 745.4908

Epoch 32: val_loss did not improve from 709.25177
196/196 - 150s - loss: 762.1896 - MinusLogProbMetric: 762.1896 - val_loss: 745.4908 - val_MinusLogProbMetric: 745.4908 - lr: 1.2346e-05 - 150s/epoch - 767ms/step
Epoch 33/1000
2023-10-01 08:48:44.374 
Epoch 33/1000 
	 loss: 719.9774, MinusLogProbMetric: 719.9774, val_loss: 712.7410, val_MinusLogProbMetric: 712.7410

Epoch 33: val_loss did not improve from 709.25177
196/196 - 143s - loss: 719.9774 - MinusLogProbMetric: 719.9774 - val_loss: 712.7410 - val_MinusLogProbMetric: 712.7410 - lr: 1.2346e-05 - 143s/epoch - 729ms/step
Epoch 34/1000
2023-10-01 08:51:02.318 
Epoch 34/1000 
	 loss: 699.4781, MinusLogProbMetric: 699.4781, val_loss: 696.7419, val_MinusLogProbMetric: 696.7419

Epoch 34: val_loss improved from 709.25177 to 696.74188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 141s - loss: 699.4781 - MinusLogProbMetric: 699.4781 - val_loss: 696.7419 - val_MinusLogProbMetric: 696.7419 - lr: 1.2346e-05 - 141s/epoch - 721ms/step
Epoch 35/1000
2023-10-01 08:53:11.705 
Epoch 35/1000 
	 loss: 682.5319, MinusLogProbMetric: 682.5319, val_loss: 672.3022, val_MinusLogProbMetric: 672.3022

Epoch 35: val_loss improved from 696.74188 to 672.30225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 128s - loss: 682.5319 - MinusLogProbMetric: 682.5319 - val_loss: 672.3022 - val_MinusLogProbMetric: 672.3022 - lr: 1.2346e-05 - 128s/epoch - 654ms/step
Epoch 36/1000
2023-10-01 08:55:18.307 
Epoch 36/1000 
	 loss: 664.3326, MinusLogProbMetric: 664.3326, val_loss: 657.2878, val_MinusLogProbMetric: 657.2878

Epoch 36: val_loss improved from 672.30225 to 657.28784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 128s - loss: 664.3326 - MinusLogProbMetric: 664.3326 - val_loss: 657.2878 - val_MinusLogProbMetric: 657.2878 - lr: 1.2346e-05 - 128s/epoch - 655ms/step
Epoch 37/1000
2023-10-01 08:57:27.521 
Epoch 37/1000 
	 loss: 656.6102, MinusLogProbMetric: 656.6102, val_loss: 655.2763, val_MinusLogProbMetric: 655.2763

Epoch 37: val_loss improved from 657.28784 to 655.27631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 130s - loss: 656.6102 - MinusLogProbMetric: 656.6102 - val_loss: 655.2763 - val_MinusLogProbMetric: 655.2763 - lr: 1.2346e-05 - 130s/epoch - 661ms/step
Epoch 38/1000
2023-10-01 08:59:34.806 
Epoch 38/1000 
	 loss: 649.2307, MinusLogProbMetric: 649.2307, val_loss: 646.4764, val_MinusLogProbMetric: 646.4764

Epoch 38: val_loss improved from 655.27631 to 646.47644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 126s - loss: 649.2307 - MinusLogProbMetric: 649.2307 - val_loss: 646.4764 - val_MinusLogProbMetric: 646.4764 - lr: 1.2346e-05 - 126s/epoch - 641ms/step
Epoch 39/1000
2023-10-01 09:01:42.872 
Epoch 39/1000 
	 loss: 636.4141, MinusLogProbMetric: 636.4141, val_loss: 630.4078, val_MinusLogProbMetric: 630.4078

Epoch 39: val_loss improved from 646.47644 to 630.40778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 128s - loss: 636.4141 - MinusLogProbMetric: 636.4141 - val_loss: 630.4078 - val_MinusLogProbMetric: 630.4078 - lr: 1.2346e-05 - 128s/epoch - 651ms/step
Epoch 40/1000
2023-10-01 09:03:47.206 
Epoch 40/1000 
	 loss: 648.0093, MinusLogProbMetric: 648.0093, val_loss: 642.9085, val_MinusLogProbMetric: 642.9085

Epoch 40: val_loss did not improve from 630.40778
196/196 - 122s - loss: 648.0093 - MinusLogProbMetric: 648.0093 - val_loss: 642.9085 - val_MinusLogProbMetric: 642.9085 - lr: 1.2346e-05 - 122s/epoch - 622ms/step
Epoch 41/1000
2023-10-01 09:05:53.307 
Epoch 41/1000 
	 loss: 635.3182, MinusLogProbMetric: 635.3182, val_loss: 627.2195, val_MinusLogProbMetric: 627.2195

Epoch 41: val_loss improved from 630.40778 to 627.21948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 129s - loss: 635.3182 - MinusLogProbMetric: 635.3182 - val_loss: 627.2195 - val_MinusLogProbMetric: 627.2195 - lr: 1.2346e-05 - 129s/epoch - 658ms/step
Epoch 42/1000
2023-10-01 09:07:59.894 
Epoch 42/1000 
	 loss: 620.9982, MinusLogProbMetric: 620.9982, val_loss: 617.8895, val_MinusLogProbMetric: 617.8895

Epoch 42: val_loss improved from 627.21948 to 617.88953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 125s - loss: 620.9982 - MinusLogProbMetric: 620.9982 - val_loss: 617.8895 - val_MinusLogProbMetric: 617.8895 - lr: 1.2346e-05 - 125s/epoch - 639ms/step
Epoch 43/1000
2023-10-01 09:10:08.139 
Epoch 43/1000 
	 loss: 617.7452, MinusLogProbMetric: 617.7452, val_loss: 614.1313, val_MinusLogProbMetric: 614.1313

Epoch 43: val_loss improved from 617.88953 to 614.13129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 129s - loss: 617.7452 - MinusLogProbMetric: 617.7452 - val_loss: 614.1313 - val_MinusLogProbMetric: 614.1313 - lr: 1.2346e-05 - 129s/epoch - 657ms/step
Epoch 44/1000
2023-10-01 09:12:14.152 
Epoch 44/1000 
	 loss: 609.3913, MinusLogProbMetric: 609.3913, val_loss: 611.5382, val_MinusLogProbMetric: 611.5382

Epoch 44: val_loss improved from 614.13129 to 611.53821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 126s - loss: 609.3913 - MinusLogProbMetric: 609.3913 - val_loss: 611.5382 - val_MinusLogProbMetric: 611.5382 - lr: 1.2346e-05 - 126s/epoch - 644ms/step
Epoch 45/1000
2023-10-01 09:14:21.346 
Epoch 45/1000 
	 loss: 602.1420, MinusLogProbMetric: 602.1420, val_loss: 597.0512, val_MinusLogProbMetric: 597.0512

Epoch 45: val_loss improved from 611.53821 to 597.05121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 127s - loss: 602.1420 - MinusLogProbMetric: 602.1420 - val_loss: 597.0512 - val_MinusLogProbMetric: 597.0512 - lr: 1.2346e-05 - 127s/epoch - 648ms/step
Epoch 46/1000
2023-10-01 09:16:24.628 
Epoch 46/1000 
	 loss: 608.0379, MinusLogProbMetric: 608.0379, val_loss: 599.2477, val_MinusLogProbMetric: 599.2477

Epoch 46: val_loss did not improve from 597.05121
196/196 - 121s - loss: 608.0379 - MinusLogProbMetric: 608.0379 - val_loss: 599.2477 - val_MinusLogProbMetric: 599.2477 - lr: 1.2346e-05 - 121s/epoch - 617ms/step
Epoch 47/1000
2023-10-01 09:18:27.313 
Epoch 47/1000 
	 loss: 614.9301, MinusLogProbMetric: 614.9301, val_loss: 615.8077, val_MinusLogProbMetric: 615.8077

Epoch 47: val_loss did not improve from 597.05121
196/196 - 123s - loss: 614.9301 - MinusLogProbMetric: 614.9301 - val_loss: 615.8077 - val_MinusLogProbMetric: 615.8077 - lr: 1.2346e-05 - 123s/epoch - 626ms/step
Epoch 48/1000
2023-10-01 09:20:34.082 
Epoch 48/1000 
	 loss: 610.8378, MinusLogProbMetric: 610.8378, val_loss: 602.8004, val_MinusLogProbMetric: 602.8004

Epoch 48: val_loss did not improve from 597.05121
196/196 - 127s - loss: 610.8378 - MinusLogProbMetric: 610.8378 - val_loss: 602.8004 - val_MinusLogProbMetric: 602.8004 - lr: 1.2346e-05 - 127s/epoch - 647ms/step
Epoch 49/1000
2023-10-01 09:22:35.298 
Epoch 49/1000 
	 loss: 599.8283, MinusLogProbMetric: 599.8283, val_loss: 597.7890, val_MinusLogProbMetric: 597.7890

Epoch 49: val_loss did not improve from 597.05121
196/196 - 121s - loss: 599.8283 - MinusLogProbMetric: 599.8283 - val_loss: 597.7890 - val_MinusLogProbMetric: 597.7890 - lr: 1.2346e-05 - 121s/epoch - 618ms/step
Epoch 50/1000
2023-10-01 09:24:35.256 
Epoch 50/1000 
	 loss: 590.5029, MinusLogProbMetric: 590.5029, val_loss: 589.6683, val_MinusLogProbMetric: 589.6683

Epoch 50: val_loss improved from 597.05121 to 589.66827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 124s - loss: 590.5029 - MinusLogProbMetric: 590.5029 - val_loss: 589.6683 - val_MinusLogProbMetric: 589.6683 - lr: 1.2346e-05 - 124s/epoch - 632ms/step
Epoch 51/1000
2023-10-01 09:26:41.241 
Epoch 51/1000 
	 loss: 593.0571, MinusLogProbMetric: 593.0571, val_loss: 595.3476, val_MinusLogProbMetric: 595.3476

Epoch 51: val_loss did not improve from 589.66827
196/196 - 122s - loss: 593.0571 - MinusLogProbMetric: 593.0571 - val_loss: 595.3476 - val_MinusLogProbMetric: 595.3476 - lr: 1.2346e-05 - 122s/epoch - 623ms/step
Epoch 52/1000
2023-10-01 09:28:43.678 
Epoch 52/1000 
	 loss: 586.7783, MinusLogProbMetric: 586.7783, val_loss: 578.7448, val_MinusLogProbMetric: 578.7448

Epoch 52: val_loss improved from 589.66827 to 578.74481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 126s - loss: 586.7783 - MinusLogProbMetric: 586.7783 - val_loss: 578.7448 - val_MinusLogProbMetric: 578.7448 - lr: 1.2346e-05 - 126s/epoch - 645ms/step
Epoch 53/1000
2023-10-01 09:30:53.801 
Epoch 53/1000 
	 loss: 595.5616, MinusLogProbMetric: 595.5616, val_loss: 589.1122, val_MinusLogProbMetric: 589.1122

Epoch 53: val_loss did not improve from 578.74481
196/196 - 126s - loss: 595.5616 - MinusLogProbMetric: 595.5616 - val_loss: 589.1122 - val_MinusLogProbMetric: 589.1122 - lr: 1.2346e-05 - 126s/epoch - 644ms/step
Epoch 54/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 171: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-01 09:32:41.170 
Epoch 54/1000 
	 loss: nan, MinusLogProbMetric: 583.9136, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 54: val_loss did not improve from 578.74481
196/196 - 107s - loss: nan - MinusLogProbMetric: 583.9136 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 107s/epoch - 548ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 352.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_493"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_494 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7fc1ee4d1330>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbe593a2f80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbe593a2f80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbf98e497e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fc8b918cbb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fc8b918d120>, <keras.callbacks.ModelCheckpoint object at 0x7fc8b918d1e0>, <keras.callbacks.EarlyStopping object at 0x7fc8b918d450>, <keras.callbacks.ReduceLROnPlateau object at 0x7fc8b918d480>, <keras.callbacks.TerminateOnNaN object at 0x7fc8b918d0c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-01 09:33:10.208151
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-01 09:44:35.534 
Epoch 1/1000 
	 loss: 591.8323, MinusLogProbMetric: 591.8323, val_loss: 597.9696, val_MinusLogProbMetric: 597.9696

Epoch 1: val_loss improved from inf to 597.96960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 689s - loss: 591.8323 - MinusLogProbMetric: 591.8323 - val_loss: 597.9696 - val_MinusLogProbMetric: 597.9696 - lr: 4.1152e-06 - 689s/epoch - 4s/step
Epoch 2/1000
2023-10-01 09:46:45.824 
Epoch 2/1000 
	 loss: 569.1877, MinusLogProbMetric: 569.1877, val_loss: 554.7932, val_MinusLogProbMetric: 554.7932

Epoch 2: val_loss improved from 597.96960 to 554.79315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 130s - loss: 569.1877 - MinusLogProbMetric: 569.1877 - val_loss: 554.7932 - val_MinusLogProbMetric: 554.7932 - lr: 4.1152e-06 - 130s/epoch - 661ms/step
Epoch 3/1000
2023-10-01 09:48:54.872 
Epoch 3/1000 
	 loss: 612.9791, MinusLogProbMetric: 612.9791, val_loss: 636.9128, val_MinusLogProbMetric: 636.9128

Epoch 3: val_loss did not improve from 554.79315
196/196 - 125s - loss: 612.9791 - MinusLogProbMetric: 612.9791 - val_loss: 636.9128 - val_MinusLogProbMetric: 636.9128 - lr: 4.1152e-06 - 125s/epoch - 638ms/step
Epoch 4/1000
2023-10-01 09:51:02.386 
Epoch 4/1000 
	 loss: 603.3251, MinusLogProbMetric: 603.3251, val_loss: 583.0187, val_MinusLogProbMetric: 583.0187

Epoch 4: val_loss did not improve from 554.79315
196/196 - 128s - loss: 603.3251 - MinusLogProbMetric: 603.3251 - val_loss: 583.0187 - val_MinusLogProbMetric: 583.0187 - lr: 4.1152e-06 - 128s/epoch - 651ms/step
Epoch 5/1000
2023-10-01 09:53:07.407 
Epoch 5/1000 
	 loss: 613.1735, MinusLogProbMetric: 613.1735, val_loss: 645.8287, val_MinusLogProbMetric: 645.8287

Epoch 5: val_loss did not improve from 554.79315
196/196 - 125s - loss: 613.1735 - MinusLogProbMetric: 613.1735 - val_loss: 645.8287 - val_MinusLogProbMetric: 645.8287 - lr: 4.1152e-06 - 125s/epoch - 638ms/step
Epoch 6/1000
2023-10-01 09:55:14.853 
Epoch 6/1000 
	 loss: 603.2958, MinusLogProbMetric: 603.2958, val_loss: 579.4142, val_MinusLogProbMetric: 579.4142

Epoch 6: val_loss did not improve from 554.79315
196/196 - 127s - loss: 603.2958 - MinusLogProbMetric: 603.2958 - val_loss: 579.4142 - val_MinusLogProbMetric: 579.4142 - lr: 4.1152e-06 - 127s/epoch - 650ms/step
Epoch 7/1000
2023-10-01 09:57:20.515 
Epoch 7/1000 
	 loss: 573.1122, MinusLogProbMetric: 573.1122, val_loss: 575.4250, val_MinusLogProbMetric: 575.4250

Epoch 7: val_loss did not improve from 554.79315
196/196 - 126s - loss: 573.1122 - MinusLogProbMetric: 573.1122 - val_loss: 575.4250 - val_MinusLogProbMetric: 575.4250 - lr: 4.1152e-06 - 126s/epoch - 641ms/step
Epoch 8/1000
2023-10-01 09:59:22.612 
Epoch 8/1000 
	 loss: 574.6410, MinusLogProbMetric: 574.6410, val_loss: 562.1818, val_MinusLogProbMetric: 562.1818

Epoch 8: val_loss did not improve from 554.79315
196/196 - 122s - loss: 574.6410 - MinusLogProbMetric: 574.6410 - val_loss: 562.1818 - val_MinusLogProbMetric: 562.1818 - lr: 4.1152e-06 - 122s/epoch - 623ms/step
Epoch 9/1000
2023-10-01 10:01:26.060 
Epoch 9/1000 
	 loss: 546.9615, MinusLogProbMetric: 546.9615, val_loss: 534.4640, val_MinusLogProbMetric: 534.4640

Epoch 9: val_loss improved from 554.79315 to 534.46399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 126s - loss: 546.9615 - MinusLogProbMetric: 546.9615 - val_loss: 534.4640 - val_MinusLogProbMetric: 534.4640 - lr: 4.1152e-06 - 126s/epoch - 640ms/step
Epoch 10/1000
2023-10-01 10:03:31.295 
Epoch 10/1000 
	 loss: 558.1478, MinusLogProbMetric: 558.1478, val_loss: 588.3377, val_MinusLogProbMetric: 588.3377

Epoch 10: val_loss did not improve from 534.46399
196/196 - 123s - loss: 558.1478 - MinusLogProbMetric: 558.1478 - val_loss: 588.3377 - val_MinusLogProbMetric: 588.3377 - lr: 4.1152e-06 - 123s/epoch - 628ms/step
Epoch 11/1000
2023-10-01 10:05:34.028 
Epoch 11/1000 
	 loss: 569.7035, MinusLogProbMetric: 569.7035, val_loss: 554.8755, val_MinusLogProbMetric: 554.8755

Epoch 11: val_loss did not improve from 534.46399
196/196 - 123s - loss: 569.7035 - MinusLogProbMetric: 569.7035 - val_loss: 554.8755 - val_MinusLogProbMetric: 554.8755 - lr: 4.1152e-06 - 123s/epoch - 626ms/step
Epoch 12/1000
2023-10-01 10:07:35.632 
Epoch 12/1000 
	 loss: 549.1091, MinusLogProbMetric: 549.1091, val_loss: 540.0121, val_MinusLogProbMetric: 540.0121

Epoch 12: val_loss did not improve from 534.46399
196/196 - 122s - loss: 549.1091 - MinusLogProbMetric: 549.1091 - val_loss: 540.0121 - val_MinusLogProbMetric: 540.0121 - lr: 4.1152e-06 - 122s/epoch - 620ms/step
Epoch 13/1000
2023-10-01 10:09:33.048 
Epoch 13/1000 
	 loss: 557.7279, MinusLogProbMetric: 557.7279, val_loss: 555.3889, val_MinusLogProbMetric: 555.3889

Epoch 13: val_loss did not improve from 534.46399
196/196 - 117s - loss: 557.7279 - MinusLogProbMetric: 557.7279 - val_loss: 555.3889 - val_MinusLogProbMetric: 555.3889 - lr: 4.1152e-06 - 117s/epoch - 599ms/step
Epoch 14/1000
2023-10-01 10:11:35.091 
Epoch 14/1000 
	 loss: 570.7062, MinusLogProbMetric: 570.7062, val_loss: 657.4987, val_MinusLogProbMetric: 657.4987

Epoch 14: val_loss did not improve from 534.46399
196/196 - 122s - loss: 570.7062 - MinusLogProbMetric: 570.7062 - val_loss: 657.4987 - val_MinusLogProbMetric: 657.4987 - lr: 4.1152e-06 - 122s/epoch - 623ms/step
Epoch 15/1000
2023-10-01 10:13:37.245 
Epoch 15/1000 
	 loss: 632.3444, MinusLogProbMetric: 632.3444, val_loss: 651.5053, val_MinusLogProbMetric: 651.5053

Epoch 15: val_loss did not improve from 534.46399
196/196 - 122s - loss: 632.3444 - MinusLogProbMetric: 632.3444 - val_loss: 651.5053 - val_MinusLogProbMetric: 651.5053 - lr: 4.1152e-06 - 122s/epoch - 623ms/step
Epoch 16/1000
2023-10-01 10:15:36.489 
Epoch 16/1000 
	 loss: 631.1853, MinusLogProbMetric: 631.1853, val_loss: 657.7521, val_MinusLogProbMetric: 657.7521

Epoch 16: val_loss did not improve from 534.46399
196/196 - 119s - loss: 631.1853 - MinusLogProbMetric: 631.1853 - val_loss: 657.7521 - val_MinusLogProbMetric: 657.7521 - lr: 4.1152e-06 - 119s/epoch - 608ms/step
Epoch 17/1000
2023-10-01 10:17:40.613 
Epoch 17/1000 
	 loss: 592.6944, MinusLogProbMetric: 592.6944, val_loss: 562.0096, val_MinusLogProbMetric: 562.0096

Epoch 17: val_loss did not improve from 534.46399
196/196 - 124s - loss: 592.6944 - MinusLogProbMetric: 592.6944 - val_loss: 562.0096 - val_MinusLogProbMetric: 562.0096 - lr: 4.1152e-06 - 124s/epoch - 633ms/step
Epoch 18/1000
2023-10-01 10:19:42.154 
Epoch 18/1000 
	 loss: 575.7800, MinusLogProbMetric: 575.7800, val_loss: 577.1835, val_MinusLogProbMetric: 577.1835

Epoch 18: val_loss did not improve from 534.46399
196/196 - 122s - loss: 575.7800 - MinusLogProbMetric: 575.7800 - val_loss: 577.1835 - val_MinusLogProbMetric: 577.1835 - lr: 4.1152e-06 - 122s/epoch - 620ms/step
Epoch 19/1000
2023-10-01 10:21:41.327 
Epoch 19/1000 
	 loss: 572.0454, MinusLogProbMetric: 572.0454, val_loss: 566.0463, val_MinusLogProbMetric: 566.0463

Epoch 19: val_loss did not improve from 534.46399
196/196 - 119s - loss: 572.0454 - MinusLogProbMetric: 572.0454 - val_loss: 566.0463 - val_MinusLogProbMetric: 566.0463 - lr: 4.1152e-06 - 119s/epoch - 608ms/step
Epoch 20/1000
2023-10-01 10:23:47.163 
Epoch 20/1000 
	 loss: 560.0660, MinusLogProbMetric: 560.0660, val_loss: 551.2255, val_MinusLogProbMetric: 551.2255

Epoch 20: val_loss did not improve from 534.46399
196/196 - 126s - loss: 560.0660 - MinusLogProbMetric: 560.0660 - val_loss: 551.2255 - val_MinusLogProbMetric: 551.2255 - lr: 4.1152e-06 - 126s/epoch - 642ms/step
Epoch 21/1000
2023-10-01 10:25:49.202 
Epoch 21/1000 
	 loss: 544.4985, MinusLogProbMetric: 544.4985, val_loss: 544.5132, val_MinusLogProbMetric: 544.5132

Epoch 21: val_loss did not improve from 534.46399
196/196 - 122s - loss: 544.4985 - MinusLogProbMetric: 544.4985 - val_loss: 544.5132 - val_MinusLogProbMetric: 544.5132 - lr: 4.1152e-06 - 122s/epoch - 623ms/step
Epoch 22/1000
2023-10-01 10:27:50.942 
Epoch 22/1000 
	 loss: 539.5731, MinusLogProbMetric: 539.5731, val_loss: 523.1639, val_MinusLogProbMetric: 523.1639

Epoch 22: val_loss improved from 534.46399 to 523.16388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 124s - loss: 539.5731 - MinusLogProbMetric: 539.5731 - val_loss: 523.1639 - val_MinusLogProbMetric: 523.1639 - lr: 4.1152e-06 - 124s/epoch - 632ms/step
Epoch 23/1000
2023-10-01 10:29:49.352 
Epoch 23/1000 
	 loss: 516.5035, MinusLogProbMetric: 516.5035, val_loss: 510.9288, val_MinusLogProbMetric: 510.9288

Epoch 23: val_loss improved from 523.16388 to 510.92883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 121s - loss: 516.5035 - MinusLogProbMetric: 516.5035 - val_loss: 510.9288 - val_MinusLogProbMetric: 510.9288 - lr: 4.1152e-06 - 121s/epoch - 615ms/step
Epoch 24/1000
2023-10-01 10:31:49.823 
Epoch 24/1000 
	 loss: 505.3255, MinusLogProbMetric: 505.3255, val_loss: 500.6978, val_MinusLogProbMetric: 500.6978

Epoch 24: val_loss improved from 510.92883 to 500.69775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 119s - loss: 505.3255 - MinusLogProbMetric: 505.3255 - val_loss: 500.6978 - val_MinusLogProbMetric: 500.6978 - lr: 4.1152e-06 - 119s/epoch - 606ms/step
Epoch 25/1000
2023-10-01 10:33:53.128 
Epoch 25/1000 
	 loss: 536.5300, MinusLogProbMetric: 536.5300, val_loss: 572.3639, val_MinusLogProbMetric: 572.3639

Epoch 25: val_loss did not improve from 500.69775
196/196 - 121s - loss: 536.5300 - MinusLogProbMetric: 536.5300 - val_loss: 572.3639 - val_MinusLogProbMetric: 572.3639 - lr: 4.1152e-06 - 121s/epoch - 616ms/step
Epoch 26/1000
2023-10-01 10:35:51.312 
Epoch 26/1000 
	 loss: 557.4722, MinusLogProbMetric: 557.4722, val_loss: 545.7614, val_MinusLogProbMetric: 545.7614

Epoch 26: val_loss did not improve from 500.69775
196/196 - 118s - loss: 557.4722 - MinusLogProbMetric: 557.4722 - val_loss: 545.7614 - val_MinusLogProbMetric: 545.7614 - lr: 4.1152e-06 - 118s/epoch - 603ms/step
Epoch 27/1000
2023-10-01 10:37:48.406 
Epoch 27/1000 
	 loss: 563.1159, MinusLogProbMetric: 563.1159, val_loss: 548.1483, val_MinusLogProbMetric: 548.1483

Epoch 27: val_loss did not improve from 500.69775
196/196 - 117s - loss: 563.1159 - MinusLogProbMetric: 563.1159 - val_loss: 548.1483 - val_MinusLogProbMetric: 548.1483 - lr: 4.1152e-06 - 117s/epoch - 597ms/step
Epoch 28/1000
2023-10-01 10:39:47.583 
Epoch 28/1000 
	 loss: 542.7972, MinusLogProbMetric: 542.7972, val_loss: 537.1857, val_MinusLogProbMetric: 537.1857

Epoch 28: val_loss did not improve from 500.69775
196/196 - 119s - loss: 542.7972 - MinusLogProbMetric: 542.7972 - val_loss: 537.1857 - val_MinusLogProbMetric: 537.1857 - lr: 4.1152e-06 - 119s/epoch - 608ms/step
Epoch 29/1000
2023-10-01 10:41:48.405 
Epoch 29/1000 
	 loss: 530.8044, MinusLogProbMetric: 530.8044, val_loss: 525.7653, val_MinusLogProbMetric: 525.7653

Epoch 29: val_loss did not improve from 500.69775
196/196 - 121s - loss: 530.8044 - MinusLogProbMetric: 530.8044 - val_loss: 525.7653 - val_MinusLogProbMetric: 525.7653 - lr: 4.1152e-06 - 121s/epoch - 616ms/step
Epoch 30/1000
2023-10-01 10:43:51.068 
Epoch 30/1000 
	 loss: 521.7185, MinusLogProbMetric: 521.7185, val_loss: 518.8076, val_MinusLogProbMetric: 518.8076

Epoch 30: val_loss did not improve from 500.69775
196/196 - 123s - loss: 521.7185 - MinusLogProbMetric: 521.7185 - val_loss: 518.8076 - val_MinusLogProbMetric: 518.8076 - lr: 4.1152e-06 - 123s/epoch - 626ms/step
Epoch 31/1000
2023-10-01 10:45:50.086 
Epoch 31/1000 
	 loss: 530.5777, MinusLogProbMetric: 530.5777, val_loss: 536.5618, val_MinusLogProbMetric: 536.5618

Epoch 31: val_loss did not improve from 500.69775
196/196 - 119s - loss: 530.5777 - MinusLogProbMetric: 530.5777 - val_loss: 536.5618 - val_MinusLogProbMetric: 536.5618 - lr: 4.1152e-06 - 119s/epoch - 607ms/step
Epoch 32/1000
2023-10-01 10:47:50.843 
Epoch 32/1000 
	 loss: 527.2682, MinusLogProbMetric: 527.2682, val_loss: 521.0339, val_MinusLogProbMetric: 521.0339

Epoch 32: val_loss did not improve from 500.69775
196/196 - 121s - loss: 527.2682 - MinusLogProbMetric: 527.2682 - val_loss: 521.0339 - val_MinusLogProbMetric: 521.0339 - lr: 4.1152e-06 - 121s/epoch - 616ms/step
Epoch 33/1000
2023-10-01 10:49:46.842 
Epoch 33/1000 
	 loss: 518.8005, MinusLogProbMetric: 518.8005, val_loss: 515.2625, val_MinusLogProbMetric: 515.2625

Epoch 33: val_loss did not improve from 500.69775
196/196 - 116s - loss: 518.8005 - MinusLogProbMetric: 518.8005 - val_loss: 515.2625 - val_MinusLogProbMetric: 515.2625 - lr: 4.1152e-06 - 116s/epoch - 592ms/step
Epoch 34/1000
2023-10-01 10:51:44.663 
Epoch 34/1000 
	 loss: 511.9785, MinusLogProbMetric: 511.9785, val_loss: 513.4267, val_MinusLogProbMetric: 513.4267

Epoch 34: val_loss did not improve from 500.69775
196/196 - 118s - loss: 511.9785 - MinusLogProbMetric: 511.9785 - val_loss: 513.4267 - val_MinusLogProbMetric: 513.4267 - lr: 4.1152e-06 - 118s/epoch - 601ms/step
Epoch 35/1000
2023-10-01 10:53:45.364 
Epoch 35/1000 
	 loss: 528.3641, MinusLogProbMetric: 528.3641, val_loss: 550.8085, val_MinusLogProbMetric: 550.8085

Epoch 35: val_loss did not improve from 500.69775
196/196 - 121s - loss: 528.3641 - MinusLogProbMetric: 528.3641 - val_loss: 550.8085 - val_MinusLogProbMetric: 550.8085 - lr: 4.1152e-06 - 121s/epoch - 616ms/step
Epoch 36/1000
2023-10-01 10:55:42.234 
Epoch 36/1000 
	 loss: 556.4007, MinusLogProbMetric: 556.4007, val_loss: 559.3345, val_MinusLogProbMetric: 559.3345

Epoch 36: val_loss did not improve from 500.69775
196/196 - 117s - loss: 556.4007 - MinusLogProbMetric: 556.4007 - val_loss: 559.3345 - val_MinusLogProbMetric: 559.3345 - lr: 4.1152e-06 - 117s/epoch - 596ms/step
Epoch 37/1000
2023-10-01 10:57:43.841 
Epoch 37/1000 
	 loss: 575.3123, MinusLogProbMetric: 575.3123, val_loss: 570.8553, val_MinusLogProbMetric: 570.8553

Epoch 37: val_loss did not improve from 500.69775
196/196 - 122s - loss: 575.3123 - MinusLogProbMetric: 575.3123 - val_loss: 570.8553 - val_MinusLogProbMetric: 570.8553 - lr: 4.1152e-06 - 122s/epoch - 620ms/step
Epoch 38/1000
2023-10-01 10:59:42.142 
Epoch 38/1000 
	 loss: 586.8878, MinusLogProbMetric: 586.8878, val_loss: 580.6520, val_MinusLogProbMetric: 580.6520

Epoch 38: val_loss did not improve from 500.69775
196/196 - 118s - loss: 586.8878 - MinusLogProbMetric: 586.8878 - val_loss: 580.6520 - val_MinusLogProbMetric: 580.6520 - lr: 4.1152e-06 - 118s/epoch - 604ms/step
Epoch 39/1000
2023-10-01 11:01:41.405 
Epoch 39/1000 
	 loss: 573.6799, MinusLogProbMetric: 573.6799, val_loss: 570.6649, val_MinusLogProbMetric: 570.6649

Epoch 39: val_loss did not improve from 500.69775
196/196 - 119s - loss: 573.6799 - MinusLogProbMetric: 573.6799 - val_loss: 570.6649 - val_MinusLogProbMetric: 570.6649 - lr: 4.1152e-06 - 119s/epoch - 608ms/step
Epoch 40/1000
2023-10-01 11:03:43.788 
Epoch 40/1000 
	 loss: 592.0180, MinusLogProbMetric: 592.0180, val_loss: 589.2944, val_MinusLogProbMetric: 589.2944

Epoch 40: val_loss did not improve from 500.69775
196/196 - 122s - loss: 592.0180 - MinusLogProbMetric: 592.0180 - val_loss: 589.2944 - val_MinusLogProbMetric: 589.2944 - lr: 4.1152e-06 - 122s/epoch - 624ms/step
Epoch 41/1000
2023-10-01 11:05:41.340 
Epoch 41/1000 
	 loss: 579.8311, MinusLogProbMetric: 579.8311, val_loss: 576.9077, val_MinusLogProbMetric: 576.9077

Epoch 41: val_loss did not improve from 500.69775
196/196 - 118s - loss: 579.8311 - MinusLogProbMetric: 579.8311 - val_loss: 576.9077 - val_MinusLogProbMetric: 576.9077 - lr: 4.1152e-06 - 118s/epoch - 600ms/step
Epoch 42/1000
2023-10-01 11:07:43.334 
Epoch 42/1000 
	 loss: 576.7108, MinusLogProbMetric: 576.7108, val_loss: 570.8766, val_MinusLogProbMetric: 570.8766

Epoch 42: val_loss did not improve from 500.69775
196/196 - 122s - loss: 576.7108 - MinusLogProbMetric: 576.7108 - val_loss: 570.8766 - val_MinusLogProbMetric: 570.8766 - lr: 4.1152e-06 - 122s/epoch - 622ms/step
Epoch 43/1000
2023-10-01 11:09:41.755 
Epoch 43/1000 
	 loss: 571.3097, MinusLogProbMetric: 571.3097, val_loss: 574.0230, val_MinusLogProbMetric: 574.0230

Epoch 43: val_loss did not improve from 500.69775
196/196 - 118s - loss: 571.3097 - MinusLogProbMetric: 571.3097 - val_loss: 574.0230 - val_MinusLogProbMetric: 574.0230 - lr: 4.1152e-06 - 118s/epoch - 604ms/step
Epoch 44/1000
2023-10-01 11:11:43.110 
Epoch 44/1000 
	 loss: 567.6677, MinusLogProbMetric: 567.6677, val_loss: 558.8308, val_MinusLogProbMetric: 558.8308

Epoch 44: val_loss did not improve from 500.69775
196/196 - 121s - loss: 567.6677 - MinusLogProbMetric: 567.6677 - val_loss: 558.8308 - val_MinusLogProbMetric: 558.8308 - lr: 4.1152e-06 - 121s/epoch - 619ms/step
Epoch 45/1000
2023-10-01 11:13:40.903 
Epoch 45/1000 
	 loss: 555.5801, MinusLogProbMetric: 555.5801, val_loss: 552.1364, val_MinusLogProbMetric: 552.1364

Epoch 45: val_loss did not improve from 500.69775
196/196 - 118s - loss: 555.5801 - MinusLogProbMetric: 555.5801 - val_loss: 552.1364 - val_MinusLogProbMetric: 552.1364 - lr: 4.1152e-06 - 118s/epoch - 601ms/step
Epoch 46/1000
2023-10-01 11:15:38.789 
Epoch 46/1000 
	 loss: 563.1445, MinusLogProbMetric: 563.1445, val_loss: 555.9414, val_MinusLogProbMetric: 555.9414

Epoch 46: val_loss did not improve from 500.69775
196/196 - 118s - loss: 563.1445 - MinusLogProbMetric: 563.1445 - val_loss: 555.9414 - val_MinusLogProbMetric: 555.9414 - lr: 4.1152e-06 - 118s/epoch - 602ms/step
Epoch 47/1000
2023-10-01 11:17:42.524 
Epoch 47/1000 
	 loss: 557.8328, MinusLogProbMetric: 557.8328, val_loss: 555.6141, val_MinusLogProbMetric: 555.6141

Epoch 47: val_loss did not improve from 500.69775
196/196 - 124s - loss: 557.8328 - MinusLogProbMetric: 557.8328 - val_loss: 555.6141 - val_MinusLogProbMetric: 555.6141 - lr: 4.1152e-06 - 124s/epoch - 631ms/step
Epoch 48/1000
2023-10-01 11:19:43.699 
Epoch 48/1000 
	 loss: 552.6516, MinusLogProbMetric: 552.6516, val_loss: 549.5633, val_MinusLogProbMetric: 549.5633

Epoch 48: val_loss did not improve from 500.69775
196/196 - 121s - loss: 552.6516 - MinusLogProbMetric: 552.6516 - val_loss: 549.5633 - val_MinusLogProbMetric: 549.5633 - lr: 4.1152e-06 - 121s/epoch - 618ms/step
Epoch 49/1000
2023-10-01 11:21:41.545 
Epoch 49/1000 
	 loss: 545.2330, MinusLogProbMetric: 545.2330, val_loss: 542.1871, val_MinusLogProbMetric: 542.1871

Epoch 49: val_loss did not improve from 500.69775
196/196 - 118s - loss: 545.2330 - MinusLogProbMetric: 545.2330 - val_loss: 542.1871 - val_MinusLogProbMetric: 542.1871 - lr: 4.1152e-06 - 118s/epoch - 601ms/step
Epoch 50/1000
2023-10-01 11:23:42.936 
Epoch 50/1000 
	 loss: 543.7433, MinusLogProbMetric: 543.7433, val_loss: 537.3013, val_MinusLogProbMetric: 537.3013

Epoch 50: val_loss did not improve from 500.69775
196/196 - 121s - loss: 543.7433 - MinusLogProbMetric: 543.7433 - val_loss: 537.3013 - val_MinusLogProbMetric: 537.3013 - lr: 4.1152e-06 - 121s/epoch - 619ms/step
Epoch 51/1000
2023-10-01 11:25:46.090 
Epoch 51/1000 
	 loss: 533.7358, MinusLogProbMetric: 533.7358, val_loss: 530.5934, val_MinusLogProbMetric: 530.5934

Epoch 51: val_loss did not improve from 500.69775
196/196 - 123s - loss: 533.7358 - MinusLogProbMetric: 533.7358 - val_loss: 530.5934 - val_MinusLogProbMetric: 530.5934 - lr: 4.1152e-06 - 123s/epoch - 628ms/step
Epoch 52/1000
2023-10-01 11:27:46.193 
Epoch 52/1000 
	 loss: 525.2673, MinusLogProbMetric: 525.2673, val_loss: 521.5152, val_MinusLogProbMetric: 521.5152

Epoch 52: val_loss did not improve from 500.69775
196/196 - 120s - loss: 525.2673 - MinusLogProbMetric: 525.2673 - val_loss: 521.5152 - val_MinusLogProbMetric: 521.5152 - lr: 4.1152e-06 - 120s/epoch - 613ms/step
Epoch 53/1000
2023-10-01 11:29:46.813 
Epoch 53/1000 
	 loss: 519.3819, MinusLogProbMetric: 519.3819, val_loss: 516.6913, val_MinusLogProbMetric: 516.6913

Epoch 53: val_loss did not improve from 500.69775
196/196 - 121s - loss: 519.3819 - MinusLogProbMetric: 519.3819 - val_loss: 516.6913 - val_MinusLogProbMetric: 516.6913 - lr: 4.1152e-06 - 121s/epoch - 615ms/step
Epoch 54/1000
2023-10-01 11:31:46.208 
Epoch 54/1000 
	 loss: 527.2285, MinusLogProbMetric: 527.2285, val_loss: 532.2348, val_MinusLogProbMetric: 532.2348

Epoch 54: val_loss did not improve from 500.69775
196/196 - 119s - loss: 527.2285 - MinusLogProbMetric: 527.2285 - val_loss: 532.2348 - val_MinusLogProbMetric: 532.2348 - lr: 4.1152e-06 - 119s/epoch - 609ms/step
Epoch 55/1000
2023-10-01 11:33:43.704 
Epoch 55/1000 
	 loss: 525.1529, MinusLogProbMetric: 525.1529, val_loss: 566.1696, val_MinusLogProbMetric: 566.1696

Epoch 55: val_loss did not improve from 500.69775
196/196 - 117s - loss: 525.1529 - MinusLogProbMetric: 525.1529 - val_loss: 566.1696 - val_MinusLogProbMetric: 566.1696 - lr: 4.1152e-06 - 117s/epoch - 599ms/step
Epoch 56/1000
2023-10-01 11:35:40.878 
Epoch 56/1000 
	 loss: 561.6234, MinusLogProbMetric: 561.6234, val_loss: 540.2654, val_MinusLogProbMetric: 540.2654

Epoch 56: val_loss did not improve from 500.69775
196/196 - 117s - loss: 561.6234 - MinusLogProbMetric: 561.6234 - val_loss: 540.2654 - val_MinusLogProbMetric: 540.2654 - lr: 4.1152e-06 - 117s/epoch - 598ms/step
Epoch 57/1000
2023-10-01 11:37:42.416 
Epoch 57/1000 
	 loss: 530.2817, MinusLogProbMetric: 530.2817, val_loss: 523.9619, val_MinusLogProbMetric: 523.9619

Epoch 57: val_loss did not improve from 500.69775
196/196 - 122s - loss: 530.2817 - MinusLogProbMetric: 530.2817 - val_loss: 523.9619 - val_MinusLogProbMetric: 523.9619 - lr: 4.1152e-06 - 122s/epoch - 620ms/step
Epoch 58/1000
2023-10-01 11:39:45.325 
Epoch 58/1000 
	 loss: 525.9434, MinusLogProbMetric: 525.9434, val_loss: 515.4202, val_MinusLogProbMetric: 515.4202

Epoch 58: val_loss did not improve from 500.69775
196/196 - 123s - loss: 525.9434 - MinusLogProbMetric: 525.9434 - val_loss: 515.4202 - val_MinusLogProbMetric: 515.4202 - lr: 4.1152e-06 - 123s/epoch - 627ms/step
Epoch 59/1000
2023-10-01 11:41:44.317 
Epoch 59/1000 
	 loss: 511.3843, MinusLogProbMetric: 511.3843, val_loss: 507.2216, val_MinusLogProbMetric: 507.2216

Epoch 59: val_loss did not improve from 500.69775
196/196 - 119s - loss: 511.3843 - MinusLogProbMetric: 511.3843 - val_loss: 507.2216 - val_MinusLogProbMetric: 507.2216 - lr: 4.1152e-06 - 119s/epoch - 607ms/step
Epoch 60/1000
2023-10-01 11:43:47.373 
Epoch 60/1000 
	 loss: 504.5875, MinusLogProbMetric: 504.5875, val_loss: 501.9709, val_MinusLogProbMetric: 501.9709

Epoch 60: val_loss did not improve from 500.69775
196/196 - 123s - loss: 504.5875 - MinusLogProbMetric: 504.5875 - val_loss: 501.9709 - val_MinusLogProbMetric: 501.9709 - lr: 4.1152e-06 - 123s/epoch - 628ms/step
Epoch 61/1000
2023-10-01 11:45:48.073 
Epoch 61/1000 
	 loss: 501.0969, MinusLogProbMetric: 501.0969, val_loss: 506.3216, val_MinusLogProbMetric: 506.3216

Epoch 61: val_loss did not improve from 500.69775
196/196 - 121s - loss: 501.0969 - MinusLogProbMetric: 501.0969 - val_loss: 506.3216 - val_MinusLogProbMetric: 506.3216 - lr: 4.1152e-06 - 121s/epoch - 616ms/step
Epoch 62/1000
2023-10-01 11:47:48.015 
Epoch 62/1000 
	 loss: 499.9088, MinusLogProbMetric: 499.9088, val_loss: 494.6153, val_MinusLogProbMetric: 494.6153

Epoch 62: val_loss improved from 500.69775 to 494.61526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 122s - loss: 499.9088 - MinusLogProbMetric: 499.9088 - val_loss: 494.6153 - val_MinusLogProbMetric: 494.6153 - lr: 4.1152e-06 - 122s/epoch - 620ms/step
Epoch 63/1000
2023-10-01 11:49:50.904 
Epoch 63/1000 
	 loss: 495.5482, MinusLogProbMetric: 495.5482, val_loss: 498.2192, val_MinusLogProbMetric: 498.2192

Epoch 63: val_loss did not improve from 494.61526
196/196 - 121s - loss: 495.5482 - MinusLogProbMetric: 495.5482 - val_loss: 498.2192 - val_MinusLogProbMetric: 498.2192 - lr: 4.1152e-06 - 121s/epoch - 618ms/step
Epoch 64/1000
2023-10-01 11:51:51.210 
Epoch 64/1000 
	 loss: 493.4324, MinusLogProbMetric: 493.4324, val_loss: 489.9655, val_MinusLogProbMetric: 489.9655

Epoch 64: val_loss improved from 494.61526 to 489.96552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 124s - loss: 493.4324 - MinusLogProbMetric: 493.4324 - val_loss: 489.9655 - val_MinusLogProbMetric: 489.9655 - lr: 4.1152e-06 - 124s/epoch - 633ms/step
Epoch 65/1000
2023-10-01 11:53:57.547 
Epoch 65/1000 
	 loss: 502.7010, MinusLogProbMetric: 502.7010, val_loss: 487.3857, val_MinusLogProbMetric: 487.3857

Epoch 65: val_loss improved from 489.96552 to 487.38571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 125s - loss: 502.7010 - MinusLogProbMetric: 502.7010 - val_loss: 487.3857 - val_MinusLogProbMetric: 487.3857 - lr: 4.1152e-06 - 125s/epoch - 636ms/step
Epoch 66/1000
