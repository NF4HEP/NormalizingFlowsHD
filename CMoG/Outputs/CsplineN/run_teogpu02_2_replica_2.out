2023-10-25 16:09:04.523158: Importing os...
2023-10-25 16:09:04.523225: Importing sys...
2023-10-25 16:09:04.523240: Importing and initializing argparse...
Visible devices: [2]
2023-10-25 16:09:04.540390: Importing timer from timeit...
2023-10-25 16:09:04.541039: Setting env variables for tf import (only device [2] will be available)...
2023-10-25 16:09:04.541088: Importing numpy...
2023-10-25 16:09:04.719311: Importing pandas...
2023-10-25 16:09:04.908581: Importing shutil...
2023-10-25 16:09:04.908610: Importing subprocess...
2023-10-25 16:09:04.908618: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-25 16:09:07.248137: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-25 16:09:07.600302: Importing textwrap...
2023-10-25 16:09:07.600329: Importing timeit...
2023-10-25 16:09:07.600338: Importing traceback...
2023-10-25 16:09:07.600345: Importing typing...
2023-10-25 16:09:07.600355: Setting tf configs...
2023-10-25 16:09:07.731772: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-25 16:09:08.790912: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

===========
Generating train data for run 361.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_361/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_361/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_361/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_361
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  660960    
 r)                                                              
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7ff828315690>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff828659ab0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff828659ab0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff8300f1630>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff8300f37c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff8300f1a80>, <keras.callbacks.ModelCheckpoint object at 0x7ff8300f15a0>, <keras.callbacks.EarlyStopping object at 0x7ff8300f14e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff8300f28f0>, <keras.callbacks.TerminateOnNaN object at 0x7ff8300f1780>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_361/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 361/720 with hyperparameters:
timestamp = 2023-10-25 16:09:13.841954
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 9: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:10:17.656 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5089.6104, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 5089.6104 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 64s/epoch - 325ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 361.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_361/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_361/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_361/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_361
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  660960    
 yer)                                                            
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7ffbd2cb7d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff600761960>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff600761960>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff600783d60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ffbd2b1a230>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ffbd2b1a7a0>, <keras.callbacks.ModelCheckpoint object at 0x7ffbd2b1a860>, <keras.callbacks.EarlyStopping object at 0x7ffbd2b1aad0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ffbd2b1ab00>, <keras.callbacks.TerminateOnNaN object at 0x7ffbd2b1a740>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_361/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 361/720 with hyperparameters:
timestamp = 2023-10-25 16:10:22.345566
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 16:11:48.138 
Epoch 1/1000 
	 loss: 1550.5298, MinusLogProbMetric: 1550.5298, val_loss: 403.4179, val_MinusLogProbMetric: 403.4179

Epoch 1: val_loss improved from inf to 403.41788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 86s - loss: 1550.5298 - MinusLogProbMetric: 1550.5298 - val_loss: 403.4179 - val_MinusLogProbMetric: 403.4179 - lr: 3.3333e-04 - 86s/epoch - 439ms/step
Epoch 2/1000
2023-10-25 16:12:17.028 
Epoch 2/1000 
	 loss: 360.7119, MinusLogProbMetric: 360.7119, val_loss: 297.0875, val_MinusLogProbMetric: 297.0875

Epoch 2: val_loss improved from 403.41788 to 297.08746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 360.7119 - MinusLogProbMetric: 360.7119 - val_loss: 297.0875 - val_MinusLogProbMetric: 297.0875 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 3/1000
2023-10-25 16:12:44.988 
Epoch 3/1000 
	 loss: 275.2233, MinusLogProbMetric: 275.2233, val_loss: 250.4735, val_MinusLogProbMetric: 250.4735

Epoch 3: val_loss improved from 297.08746 to 250.47348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 275.2233 - MinusLogProbMetric: 275.2233 - val_loss: 250.4735 - val_MinusLogProbMetric: 250.4735 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 4/1000
2023-10-25 16:13:12.430 
Epoch 4/1000 
	 loss: 236.8718, MinusLogProbMetric: 236.8718, val_loss: 192.7050, val_MinusLogProbMetric: 192.7050

Epoch 4: val_loss improved from 250.47348 to 192.70499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 236.8718 - MinusLogProbMetric: 236.8718 - val_loss: 192.7050 - val_MinusLogProbMetric: 192.7050 - lr: 3.3333e-04 - 28s/epoch - 140ms/step
Epoch 5/1000
2023-10-25 16:13:40.391 
Epoch 5/1000 
	 loss: 181.6209, MinusLogProbMetric: 181.6209, val_loss: 165.9477, val_MinusLogProbMetric: 165.9477

Epoch 5: val_loss improved from 192.70499 to 165.94772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 181.6209 - MinusLogProbMetric: 181.6209 - val_loss: 165.9477 - val_MinusLogProbMetric: 165.9477 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 6/1000
2023-10-25 16:14:09.466 
Epoch 6/1000 
	 loss: 160.5005, MinusLogProbMetric: 160.5005, val_loss: 153.0277, val_MinusLogProbMetric: 153.0277

Epoch 6: val_loss improved from 165.94772 to 153.02774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 160.5005 - MinusLogProbMetric: 160.5005 - val_loss: 153.0277 - val_MinusLogProbMetric: 153.0277 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 7/1000
2023-10-25 16:14:40.256 
Epoch 7/1000 
	 loss: 146.4580, MinusLogProbMetric: 146.4580, val_loss: 144.3535, val_MinusLogProbMetric: 144.3535

Epoch 7: val_loss improved from 153.02774 to 144.35353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 31s - loss: 146.4580 - MinusLogProbMetric: 146.4580 - val_loss: 144.3535 - val_MinusLogProbMetric: 144.3535 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 8/1000
2023-10-25 16:15:08.000 
Epoch 8/1000 
	 loss: 136.1914, MinusLogProbMetric: 136.1914, val_loss: 130.2362, val_MinusLogProbMetric: 130.2362

Epoch 8: val_loss improved from 144.35353 to 130.23624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 136.1914 - MinusLogProbMetric: 136.1914 - val_loss: 130.2362 - val_MinusLogProbMetric: 130.2362 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 9/1000
2023-10-25 16:15:35.499 
Epoch 9/1000 
	 loss: 125.1933, MinusLogProbMetric: 125.1933, val_loss: 122.4606, val_MinusLogProbMetric: 122.4606

Epoch 9: val_loss improved from 130.23624 to 122.46063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 125.1933 - MinusLogProbMetric: 125.1933 - val_loss: 122.4606 - val_MinusLogProbMetric: 122.4606 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 10/1000
2023-10-25 16:16:03.522 
Epoch 10/1000 
	 loss: 116.2195, MinusLogProbMetric: 116.2195, val_loss: 113.2914, val_MinusLogProbMetric: 113.2914

Epoch 10: val_loss improved from 122.46063 to 113.29138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 116.2195 - MinusLogProbMetric: 116.2195 - val_loss: 113.2914 - val_MinusLogProbMetric: 113.2914 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 11/1000
2023-10-25 16:16:32.322 
Epoch 11/1000 
	 loss: 109.4707, MinusLogProbMetric: 109.4707, val_loss: 105.7311, val_MinusLogProbMetric: 105.7311

Epoch 11: val_loss improved from 113.29138 to 105.73106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 109.4707 - MinusLogProbMetric: 109.4707 - val_loss: 105.7311 - val_MinusLogProbMetric: 105.7311 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 12/1000
2023-10-25 16:17:00.202 
Epoch 12/1000 
	 loss: 103.2104, MinusLogProbMetric: 103.2104, val_loss: 103.1172, val_MinusLogProbMetric: 103.1172

Epoch 12: val_loss improved from 105.73106 to 103.11720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 103.2104 - MinusLogProbMetric: 103.2104 - val_loss: 103.1172 - val_MinusLogProbMetric: 103.1172 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 13/1000
2023-10-25 16:17:27.525 
Epoch 13/1000 
	 loss: 194.2403, MinusLogProbMetric: 194.2403, val_loss: 137.9664, val_MinusLogProbMetric: 137.9664

Epoch 13: val_loss did not improve from 103.11720
196/196 - 27s - loss: 194.2403 - MinusLogProbMetric: 194.2403 - val_loss: 137.9664 - val_MinusLogProbMetric: 137.9664 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 14/1000
2023-10-25 16:17:54.875 
Epoch 14/1000 
	 loss: 127.1076, MinusLogProbMetric: 127.1076, val_loss: 117.0129, val_MinusLogProbMetric: 117.0129

Epoch 14: val_loss did not improve from 103.11720
196/196 - 27s - loss: 127.1076 - MinusLogProbMetric: 127.1076 - val_loss: 117.0129 - val_MinusLogProbMetric: 117.0129 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 15/1000
2023-10-25 16:18:23.917 
Epoch 15/1000 
	 loss: 110.7711, MinusLogProbMetric: 110.7711, val_loss: 105.7461, val_MinusLogProbMetric: 105.7461

Epoch 15: val_loss did not improve from 103.11720
196/196 - 29s - loss: 110.7711 - MinusLogProbMetric: 110.7711 - val_loss: 105.7461 - val_MinusLogProbMetric: 105.7461 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 16/1000
2023-10-25 16:18:54.257 
Epoch 16/1000 
	 loss: 102.0230, MinusLogProbMetric: 102.0230, val_loss: 99.2758, val_MinusLogProbMetric: 99.2758

Epoch 16: val_loss improved from 103.11720 to 99.27576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 31s - loss: 102.0230 - MinusLogProbMetric: 102.0230 - val_loss: 99.2758 - val_MinusLogProbMetric: 99.2758 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 17/1000
2023-10-25 16:19:22.214 
Epoch 17/1000 
	 loss: 95.6382, MinusLogProbMetric: 95.6382, val_loss: 93.7512, val_MinusLogProbMetric: 93.7512

Epoch 17: val_loss improved from 99.27576 to 93.75124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 95.6382 - MinusLogProbMetric: 95.6382 - val_loss: 93.7512 - val_MinusLogProbMetric: 93.7512 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 18/1000
2023-10-25 16:19:49.485 
Epoch 18/1000 
	 loss: 90.5364, MinusLogProbMetric: 90.5364, val_loss: 88.4613, val_MinusLogProbMetric: 88.4613

Epoch 18: val_loss improved from 93.75124 to 88.46133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 27s - loss: 90.5364 - MinusLogProbMetric: 90.5364 - val_loss: 88.4613 - val_MinusLogProbMetric: 88.4613 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 19/1000
2023-10-25 16:20:17.012 
Epoch 19/1000 
	 loss: 95.6395, MinusLogProbMetric: 95.6395, val_loss: 86.8546, val_MinusLogProbMetric: 86.8546

Epoch 19: val_loss improved from 88.46133 to 86.85460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 95.6395 - MinusLogProbMetric: 95.6395 - val_loss: 86.8546 - val_MinusLogProbMetric: 86.8546 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 20/1000
2023-10-25 16:20:45.178 
Epoch 20/1000 
	 loss: 83.7014, MinusLogProbMetric: 83.7014, val_loss: 81.2094, val_MinusLogProbMetric: 81.2094

Epoch 20: val_loss improved from 86.85460 to 81.20944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 83.7014 - MinusLogProbMetric: 83.7014 - val_loss: 81.2094 - val_MinusLogProbMetric: 81.2094 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 21/1000
2023-10-25 16:21:14.764 
Epoch 21/1000 
	 loss: 80.4575, MinusLogProbMetric: 80.4575, val_loss: 78.5398, val_MinusLogProbMetric: 78.5398

Epoch 21: val_loss improved from 81.20944 to 78.53981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 80.4575 - MinusLogProbMetric: 80.4575 - val_loss: 78.5398 - val_MinusLogProbMetric: 78.5398 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 22/1000
2023-10-25 16:21:45.692 
Epoch 22/1000 
	 loss: 77.0452, MinusLogProbMetric: 77.0452, val_loss: 77.1441, val_MinusLogProbMetric: 77.1441

Epoch 22: val_loss improved from 78.53981 to 77.14407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 31s - loss: 77.0452 - MinusLogProbMetric: 77.0452 - val_loss: 77.1441 - val_MinusLogProbMetric: 77.1441 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 23/1000
2023-10-25 16:22:13.591 
Epoch 23/1000 
	 loss: 74.0808, MinusLogProbMetric: 74.0808, val_loss: 72.8820, val_MinusLogProbMetric: 72.8820

Epoch 23: val_loss improved from 77.14407 to 72.88197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 74.0808 - MinusLogProbMetric: 74.0808 - val_loss: 72.8820 - val_MinusLogProbMetric: 72.8820 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 24/1000
2023-10-25 16:22:41.072 
Epoch 24/1000 
	 loss: 71.7249, MinusLogProbMetric: 71.7249, val_loss: 70.3833, val_MinusLogProbMetric: 70.3833

Epoch 24: val_loss improved from 72.88197 to 70.38328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 71.7249 - MinusLogProbMetric: 71.7249 - val_loss: 70.3833 - val_MinusLogProbMetric: 70.3833 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 25/1000
2023-10-25 16:23:09.930 
Epoch 25/1000 
	 loss: 69.4629, MinusLogProbMetric: 69.4629, val_loss: 68.2517, val_MinusLogProbMetric: 68.2517

Epoch 25: val_loss improved from 70.38328 to 68.25174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 69.4629 - MinusLogProbMetric: 69.4629 - val_loss: 68.2517 - val_MinusLogProbMetric: 68.2517 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 26/1000
2023-10-25 16:23:37.697 
Epoch 26/1000 
	 loss: 67.5334, MinusLogProbMetric: 67.5334, val_loss: 66.6799, val_MinusLogProbMetric: 66.6799

Epoch 26: val_loss improved from 68.25174 to 66.67993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 67.5334 - MinusLogProbMetric: 67.5334 - val_loss: 66.6799 - val_MinusLogProbMetric: 66.6799 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 27/1000
2023-10-25 16:24:05.053 
Epoch 27/1000 
	 loss: 65.8593, MinusLogProbMetric: 65.8593, val_loss: 64.9981, val_MinusLogProbMetric: 64.9981

Epoch 27: val_loss improved from 66.67993 to 64.99810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 27s - loss: 65.8593 - MinusLogProbMetric: 65.8593 - val_loss: 64.9981 - val_MinusLogProbMetric: 64.9981 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 28/1000
2023-10-25 16:24:32.302 
Epoch 28/1000 
	 loss: 64.0642, MinusLogProbMetric: 64.0642, val_loss: 63.6530, val_MinusLogProbMetric: 63.6530

Epoch 28: val_loss improved from 64.99810 to 63.65297, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 27s - loss: 64.0642 - MinusLogProbMetric: 64.0642 - val_loss: 63.6530 - val_MinusLogProbMetric: 63.6530 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 29/1000
2023-10-25 16:25:01.027 
Epoch 29/1000 
	 loss: 62.6554, MinusLogProbMetric: 62.6554, val_loss: 62.9442, val_MinusLogProbMetric: 62.9442

Epoch 29: val_loss improved from 63.65297 to 62.94417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 62.6554 - MinusLogProbMetric: 62.6554 - val_loss: 62.9442 - val_MinusLogProbMetric: 62.9442 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 30/1000
2023-10-25 16:25:32.698 
Epoch 30/1000 
	 loss: 61.1615, MinusLogProbMetric: 61.1615, val_loss: 60.3797, val_MinusLogProbMetric: 60.3797

Epoch 30: val_loss improved from 62.94417 to 60.37974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 61.1615 - MinusLogProbMetric: 61.1615 - val_loss: 60.3797 - val_MinusLogProbMetric: 60.3797 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 31/1000
2023-10-25 16:26:05.068 
Epoch 31/1000 
	 loss: 59.7268, MinusLogProbMetric: 59.7268, val_loss: 59.0686, val_MinusLogProbMetric: 59.0686

Epoch 31: val_loss improved from 60.37974 to 59.06859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 59.7268 - MinusLogProbMetric: 59.7268 - val_loss: 59.0686 - val_MinusLogProbMetric: 59.0686 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 32/1000
2023-10-25 16:26:33.260 
Epoch 32/1000 
	 loss: 58.6254, MinusLogProbMetric: 58.6254, val_loss: 58.7734, val_MinusLogProbMetric: 58.7734

Epoch 32: val_loss improved from 59.06859 to 58.77337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 58.6254 - MinusLogProbMetric: 58.6254 - val_loss: 58.7734 - val_MinusLogProbMetric: 58.7734 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 33/1000
2023-10-25 16:27:00.938 
Epoch 33/1000 
	 loss: 57.3652, MinusLogProbMetric: 57.3652, val_loss: 57.4872, val_MinusLogProbMetric: 57.4872

Epoch 33: val_loss improved from 58.77337 to 57.48717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 57.3652 - MinusLogProbMetric: 57.3652 - val_loss: 57.4872 - val_MinusLogProbMetric: 57.4872 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 34/1000
2023-10-25 16:27:29.307 
Epoch 34/1000 
	 loss: 56.3318, MinusLogProbMetric: 56.3318, val_loss: 55.7692, val_MinusLogProbMetric: 55.7692

Epoch 34: val_loss improved from 57.48717 to 55.76921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 56.3318 - MinusLogProbMetric: 56.3318 - val_loss: 55.7692 - val_MinusLogProbMetric: 55.7692 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 35/1000
2023-10-25 16:27:59.621 
Epoch 35/1000 
	 loss: 55.4149, MinusLogProbMetric: 55.4149, val_loss: 54.8924, val_MinusLogProbMetric: 54.8924

Epoch 35: val_loss improved from 55.76921 to 54.89245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 55.4149 - MinusLogProbMetric: 55.4149 - val_loss: 54.8924 - val_MinusLogProbMetric: 54.8924 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 36/1000
2023-10-25 16:28:29.380 
Epoch 36/1000 
	 loss: 54.5430, MinusLogProbMetric: 54.5430, val_loss: 54.1406, val_MinusLogProbMetric: 54.1406

Epoch 36: val_loss improved from 54.89245 to 54.14062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 54.5430 - MinusLogProbMetric: 54.5430 - val_loss: 54.1406 - val_MinusLogProbMetric: 54.1406 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 37/1000
2023-10-25 16:28:57.220 
Epoch 37/1000 
	 loss: 53.5496, MinusLogProbMetric: 53.5496, val_loss: 53.5388, val_MinusLogProbMetric: 53.5388

Epoch 37: val_loss improved from 54.14062 to 53.53882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 53.5496 - MinusLogProbMetric: 53.5496 - val_loss: 53.5388 - val_MinusLogProbMetric: 53.5388 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 38/1000
2023-10-25 16:29:24.934 
Epoch 38/1000 
	 loss: 52.7234, MinusLogProbMetric: 52.7234, val_loss: 52.5236, val_MinusLogProbMetric: 52.5236

Epoch 38: val_loss improved from 53.53882 to 52.52359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 52.7234 - MinusLogProbMetric: 52.7234 - val_loss: 52.5236 - val_MinusLogProbMetric: 52.5236 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 39/1000
2023-10-25 16:29:52.697 
Epoch 39/1000 
	 loss: 52.0324, MinusLogProbMetric: 52.0324, val_loss: 51.9211, val_MinusLogProbMetric: 51.9211

Epoch 39: val_loss improved from 52.52359 to 51.92109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 28s - loss: 52.0324 - MinusLogProbMetric: 52.0324 - val_loss: 51.9211 - val_MinusLogProbMetric: 51.9211 - lr: 3.3333e-04 - 28s/epoch - 142ms/step
Epoch 40/1000
2023-10-25 16:30:21.770 
Epoch 40/1000 
	 loss: 51.3841, MinusLogProbMetric: 51.3841, val_loss: 51.1495, val_MinusLogProbMetric: 51.1495

Epoch 40: val_loss improved from 51.92109 to 51.14946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 51.3841 - MinusLogProbMetric: 51.3841 - val_loss: 51.1495 - val_MinusLogProbMetric: 51.1495 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 41/1000
2023-10-25 16:30:50.316 
Epoch 41/1000 
	 loss: 50.6205, MinusLogProbMetric: 50.6205, val_loss: 50.4942, val_MinusLogProbMetric: 50.4942

Epoch 41: val_loss improved from 51.14946 to 50.49424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 50.6205 - MinusLogProbMetric: 50.6205 - val_loss: 50.4942 - val_MinusLogProbMetric: 50.4942 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 42/1000
2023-10-25 16:31:17.718 
Epoch 42/1000 
	 loss: 50.1724, MinusLogProbMetric: 50.1724, val_loss: 50.6082, val_MinusLogProbMetric: 50.6082

Epoch 42: val_loss did not improve from 50.49424
196/196 - 27s - loss: 50.1724 - MinusLogProbMetric: 50.1724 - val_loss: 50.6082 - val_MinusLogProbMetric: 50.6082 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 43/1000
2023-10-25 16:31:48.960 
Epoch 43/1000 
	 loss: 49.4208, MinusLogProbMetric: 49.4208, val_loss: 49.1688, val_MinusLogProbMetric: 49.1688

Epoch 43: val_loss improved from 50.49424 to 49.16880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 49.4208 - MinusLogProbMetric: 49.4208 - val_loss: 49.1688 - val_MinusLogProbMetric: 49.1688 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 44/1000
2023-10-25 16:32:22.059 
Epoch 44/1000 
	 loss: 48.8553, MinusLogProbMetric: 48.8553, val_loss: 48.8592, val_MinusLogProbMetric: 48.8592

Epoch 44: val_loss improved from 49.16880 to 48.85923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 48.8553 - MinusLogProbMetric: 48.8553 - val_loss: 48.8592 - val_MinusLogProbMetric: 48.8592 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 45/1000
2023-10-25 16:32:56.079 
Epoch 45/1000 
	 loss: 50.8337, MinusLogProbMetric: 50.8337, val_loss: 52.5304, val_MinusLogProbMetric: 52.5304

Epoch 45: val_loss did not improve from 48.85923
196/196 - 33s - loss: 50.8337 - MinusLogProbMetric: 50.8337 - val_loss: 52.5304 - val_MinusLogProbMetric: 52.5304 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 46/1000
2023-10-25 16:33:26.732 
Epoch 46/1000 
	 loss: 48.3674, MinusLogProbMetric: 48.3674, val_loss: 47.9740, val_MinusLogProbMetric: 47.9740

Epoch 46: val_loss improved from 48.85923 to 47.97397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 31s - loss: 48.3674 - MinusLogProbMetric: 48.3674 - val_loss: 47.9740 - val_MinusLogProbMetric: 47.9740 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 47/1000
2023-10-25 16:33:56.526 
Epoch 47/1000 
	 loss: 47.6113, MinusLogProbMetric: 47.6113, val_loss: 47.4487, val_MinusLogProbMetric: 47.4487

Epoch 47: val_loss improved from 47.97397 to 47.44870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 47.6113 - MinusLogProbMetric: 47.6113 - val_loss: 47.4487 - val_MinusLogProbMetric: 47.4487 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 48/1000
2023-10-25 16:34:26.184 
Epoch 48/1000 
	 loss: 47.1567, MinusLogProbMetric: 47.1567, val_loss: 47.2328, val_MinusLogProbMetric: 47.2328

Epoch 48: val_loss improved from 47.44870 to 47.23277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 47.1567 - MinusLogProbMetric: 47.1567 - val_loss: 47.2328 - val_MinusLogProbMetric: 47.2328 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 49/1000
2023-10-25 16:34:56.026 
Epoch 49/1000 
	 loss: 46.7319, MinusLogProbMetric: 46.7319, val_loss: 46.7584, val_MinusLogProbMetric: 46.7584

Epoch 49: val_loss improved from 47.23277 to 46.75843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 46.7319 - MinusLogProbMetric: 46.7319 - val_loss: 46.7584 - val_MinusLogProbMetric: 46.7584 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 50/1000
2023-10-25 16:35:25.908 
Epoch 50/1000 
	 loss: 92.7357, MinusLogProbMetric: 92.7357, val_loss: 80.8800, val_MinusLogProbMetric: 80.8800

Epoch 50: val_loss did not improve from 46.75843
196/196 - 29s - loss: 92.7357 - MinusLogProbMetric: 92.7357 - val_loss: 80.8800 - val_MinusLogProbMetric: 80.8800 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 51/1000
2023-10-25 16:35:55.112 
Epoch 51/1000 
	 loss: 67.3234, MinusLogProbMetric: 67.3234, val_loss: 60.3002, val_MinusLogProbMetric: 60.3002

Epoch 51: val_loss did not improve from 46.75843
196/196 - 29s - loss: 67.3234 - MinusLogProbMetric: 67.3234 - val_loss: 60.3002 - val_MinusLogProbMetric: 60.3002 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 52/1000
2023-10-25 16:36:24.487 
Epoch 52/1000 
	 loss: 57.5633, MinusLogProbMetric: 57.5633, val_loss: 55.6507, val_MinusLogProbMetric: 55.6507

Epoch 52: val_loss did not improve from 46.75843
196/196 - 29s - loss: 57.5633 - MinusLogProbMetric: 57.5633 - val_loss: 55.6507 - val_MinusLogProbMetric: 55.6507 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 53/1000
2023-10-25 16:36:54.564 
Epoch 53/1000 
	 loss: 53.8193, MinusLogProbMetric: 53.8193, val_loss: 52.4249, val_MinusLogProbMetric: 52.4249

Epoch 53: val_loss did not improve from 46.75843
196/196 - 30s - loss: 53.8193 - MinusLogProbMetric: 53.8193 - val_loss: 52.4249 - val_MinusLogProbMetric: 52.4249 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 54/1000
2023-10-25 16:37:24.029 
Epoch 54/1000 
	 loss: 51.5265, MinusLogProbMetric: 51.5265, val_loss: 50.6330, val_MinusLogProbMetric: 50.6330

Epoch 54: val_loss did not improve from 46.75843
196/196 - 29s - loss: 51.5265 - MinusLogProbMetric: 51.5265 - val_loss: 50.6330 - val_MinusLogProbMetric: 50.6330 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 55/1000
2023-10-25 16:37:53.076 
Epoch 55/1000 
	 loss: 49.7636, MinusLogProbMetric: 49.7636, val_loss: 49.5999, val_MinusLogProbMetric: 49.5999

Epoch 55: val_loss did not improve from 46.75843
196/196 - 29s - loss: 49.7636 - MinusLogProbMetric: 49.7636 - val_loss: 49.5999 - val_MinusLogProbMetric: 49.5999 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 56/1000
2023-10-25 16:38:22.486 
Epoch 56/1000 
	 loss: 48.4756, MinusLogProbMetric: 48.4756, val_loss: 47.6112, val_MinusLogProbMetric: 47.6112

Epoch 56: val_loss did not improve from 46.75843
196/196 - 29s - loss: 48.4756 - MinusLogProbMetric: 48.4756 - val_loss: 47.6112 - val_MinusLogProbMetric: 47.6112 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 57/1000
2023-10-25 16:38:51.851 
Epoch 57/1000 
	 loss: 47.2849, MinusLogProbMetric: 47.2849, val_loss: 47.1760, val_MinusLogProbMetric: 47.1760

Epoch 57: val_loss did not improve from 46.75843
196/196 - 29s - loss: 47.2849 - MinusLogProbMetric: 47.2849 - val_loss: 47.1760 - val_MinusLogProbMetric: 47.1760 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 58/1000
2023-10-25 16:39:21.069 
Epoch 58/1000 
	 loss: 46.4112, MinusLogProbMetric: 46.4112, val_loss: 46.8679, val_MinusLogProbMetric: 46.8679

Epoch 58: val_loss did not improve from 46.75843
196/196 - 29s - loss: 46.4112 - MinusLogProbMetric: 46.4112 - val_loss: 46.8679 - val_MinusLogProbMetric: 46.8679 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 59/1000
2023-10-25 16:39:50.518 
Epoch 59/1000 
	 loss: 46.0126, MinusLogProbMetric: 46.0126, val_loss: 46.0556, val_MinusLogProbMetric: 46.0556

Epoch 59: val_loss improved from 46.75843 to 46.05557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 46.0126 - MinusLogProbMetric: 46.0126 - val_loss: 46.0556 - val_MinusLogProbMetric: 46.0556 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 60/1000
2023-10-25 16:40:22.081 
Epoch 60/1000 
	 loss: 45.2902, MinusLogProbMetric: 45.2902, val_loss: 45.1782, val_MinusLogProbMetric: 45.1782

Epoch 60: val_loss improved from 46.05557 to 45.17823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 45.2902 - MinusLogProbMetric: 45.2902 - val_loss: 45.1782 - val_MinusLogProbMetric: 45.1782 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 61/1000
2023-10-25 16:40:55.837 
Epoch 61/1000 
	 loss: 44.7552, MinusLogProbMetric: 44.7552, val_loss: 44.9558, val_MinusLogProbMetric: 44.9558

Epoch 61: val_loss improved from 45.17823 to 44.95576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 44.7552 - MinusLogProbMetric: 44.7552 - val_loss: 44.9558 - val_MinusLogProbMetric: 44.9558 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 62/1000
2023-10-25 16:41:27.201 
Epoch 62/1000 
	 loss: 44.4665, MinusLogProbMetric: 44.4665, val_loss: 44.6802, val_MinusLogProbMetric: 44.6802

Epoch 62: val_loss improved from 44.95576 to 44.68021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 31s - loss: 44.4665 - MinusLogProbMetric: 44.4665 - val_loss: 44.6802 - val_MinusLogProbMetric: 44.6802 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 63/1000
2023-10-25 16:41:56.623 
Epoch 63/1000 
	 loss: 44.0870, MinusLogProbMetric: 44.0870, val_loss: 43.8474, val_MinusLogProbMetric: 43.8474

Epoch 63: val_loss improved from 44.68021 to 43.84740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 29s - loss: 44.0870 - MinusLogProbMetric: 44.0870 - val_loss: 43.8474 - val_MinusLogProbMetric: 43.8474 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 64/1000
2023-10-25 16:42:26.117 
Epoch 64/1000 
	 loss: 43.6573, MinusLogProbMetric: 43.6573, val_loss: 43.3329, val_MinusLogProbMetric: 43.3329

Epoch 64: val_loss improved from 43.84740 to 43.33290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 43.6573 - MinusLogProbMetric: 43.6573 - val_loss: 43.3329 - val_MinusLogProbMetric: 43.3329 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 65/1000
2023-10-25 16:42:55.911 
Epoch 65/1000 
	 loss: 43.6675, MinusLogProbMetric: 43.6675, val_loss: 44.1397, val_MinusLogProbMetric: 44.1397

Epoch 65: val_loss did not improve from 43.33290
196/196 - 29s - loss: 43.6675 - MinusLogProbMetric: 43.6675 - val_loss: 44.1397 - val_MinusLogProbMetric: 44.1397 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 66/1000
2023-10-25 16:43:26.942 
Epoch 66/1000 
	 loss: 42.9222, MinusLogProbMetric: 42.9222, val_loss: 43.4812, val_MinusLogProbMetric: 43.4812

Epoch 66: val_loss did not improve from 43.33290
196/196 - 31s - loss: 42.9222 - MinusLogProbMetric: 42.9222 - val_loss: 43.4812 - val_MinusLogProbMetric: 43.4812 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 67/1000
2023-10-25 16:44:00.583 
Epoch 67/1000 
	 loss: 42.8086, MinusLogProbMetric: 42.8086, val_loss: 42.6399, val_MinusLogProbMetric: 42.6399

Epoch 67: val_loss improved from 43.33290 to 42.63990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 42.8086 - MinusLogProbMetric: 42.8086 - val_loss: 42.6399 - val_MinusLogProbMetric: 42.6399 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 68/1000
2023-10-25 16:44:34.845 
Epoch 68/1000 
	 loss: 42.6159, MinusLogProbMetric: 42.6159, val_loss: 43.6062, val_MinusLogProbMetric: 43.6062

Epoch 68: val_loss did not improve from 42.63990
196/196 - 34s - loss: 42.6159 - MinusLogProbMetric: 42.6159 - val_loss: 43.6062 - val_MinusLogProbMetric: 43.6062 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 69/1000
2023-10-25 16:45:08.928 
Epoch 69/1000 
	 loss: 42.3231, MinusLogProbMetric: 42.3231, val_loss: 42.4009, val_MinusLogProbMetric: 42.4009

Epoch 69: val_loss improved from 42.63990 to 42.40094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 42.3231 - MinusLogProbMetric: 42.3231 - val_loss: 42.4009 - val_MinusLogProbMetric: 42.4009 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 70/1000
2023-10-25 16:45:42.677 
Epoch 70/1000 
	 loss: 42.0738, MinusLogProbMetric: 42.0738, val_loss: 41.9288, val_MinusLogProbMetric: 41.9288

Epoch 70: val_loss improved from 42.40094 to 41.92880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 42.0738 - MinusLogProbMetric: 42.0738 - val_loss: 41.9288 - val_MinusLogProbMetric: 41.9288 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 71/1000
2023-10-25 16:46:17.387 
Epoch 71/1000 
	 loss: 42.2802, MinusLogProbMetric: 42.2802, val_loss: 43.9530, val_MinusLogProbMetric: 43.9530

Epoch 71: val_loss did not improve from 41.92880
196/196 - 34s - loss: 42.2802 - MinusLogProbMetric: 42.2802 - val_loss: 43.9530 - val_MinusLogProbMetric: 43.9530 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 72/1000
2023-10-25 16:46:51.685 
Epoch 72/1000 
	 loss: 41.4743, MinusLogProbMetric: 41.4743, val_loss: 41.7355, val_MinusLogProbMetric: 41.7355

Epoch 72: val_loss improved from 41.92880 to 41.73554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 41.4743 - MinusLogProbMetric: 41.4743 - val_loss: 41.7355 - val_MinusLogProbMetric: 41.7355 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 73/1000
2023-10-25 16:47:26.716 
Epoch 73/1000 
	 loss: 41.1356, MinusLogProbMetric: 41.1356, val_loss: 42.2223, val_MinusLogProbMetric: 42.2223

Epoch 73: val_loss did not improve from 41.73554
196/196 - 34s - loss: 41.1356 - MinusLogProbMetric: 41.1356 - val_loss: 42.2223 - val_MinusLogProbMetric: 42.2223 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 74/1000
2023-10-25 16:48:01.154 
Epoch 74/1000 
	 loss: 41.2180, MinusLogProbMetric: 41.2180, val_loss: 41.0514, val_MinusLogProbMetric: 41.0514

Epoch 74: val_loss improved from 41.73554 to 41.05137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 41.2180 - MinusLogProbMetric: 41.2180 - val_loss: 41.0514 - val_MinusLogProbMetric: 41.0514 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 75/1000
2023-10-25 16:48:36.470 
Epoch 75/1000 
	 loss: 40.8535, MinusLogProbMetric: 40.8535, val_loss: 43.6384, val_MinusLogProbMetric: 43.6384

Epoch 75: val_loss did not improve from 41.05137
196/196 - 35s - loss: 40.8535 - MinusLogProbMetric: 40.8535 - val_loss: 43.6384 - val_MinusLogProbMetric: 43.6384 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 76/1000
2023-10-25 16:49:10.938 
Epoch 76/1000 
	 loss: 40.8913, MinusLogProbMetric: 40.8913, val_loss: 40.9980, val_MinusLogProbMetric: 40.9980

Epoch 76: val_loss improved from 41.05137 to 40.99798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 40.8913 - MinusLogProbMetric: 40.8913 - val_loss: 40.9980 - val_MinusLogProbMetric: 40.9980 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 77/1000
2023-10-25 16:49:45.881 
Epoch 77/1000 
	 loss: 40.5326, MinusLogProbMetric: 40.5326, val_loss: 40.1666, val_MinusLogProbMetric: 40.1666

Epoch 77: val_loss improved from 40.99798 to 40.16660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 40.5326 - MinusLogProbMetric: 40.5326 - val_loss: 40.1666 - val_MinusLogProbMetric: 40.1666 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 78/1000
2023-10-25 16:50:20.411 
Epoch 78/1000 
	 loss: 40.2249, MinusLogProbMetric: 40.2249, val_loss: 40.4657, val_MinusLogProbMetric: 40.4657

Epoch 78: val_loss did not improve from 40.16660
196/196 - 34s - loss: 40.2249 - MinusLogProbMetric: 40.2249 - val_loss: 40.4657 - val_MinusLogProbMetric: 40.4657 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 79/1000
2023-10-25 16:50:52.503 
Epoch 79/1000 
	 loss: 40.3334, MinusLogProbMetric: 40.3334, val_loss: 40.6867, val_MinusLogProbMetric: 40.6867

Epoch 79: val_loss did not improve from 40.16660
196/196 - 32s - loss: 40.3334 - MinusLogProbMetric: 40.3334 - val_loss: 40.6867 - val_MinusLogProbMetric: 40.6867 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 80/1000
2023-10-25 16:51:26.661 
Epoch 80/1000 
	 loss: 40.0139, MinusLogProbMetric: 40.0139, val_loss: 40.2742, val_MinusLogProbMetric: 40.2742

Epoch 80: val_loss did not improve from 40.16660
196/196 - 34s - loss: 40.0139 - MinusLogProbMetric: 40.0139 - val_loss: 40.2742 - val_MinusLogProbMetric: 40.2742 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 81/1000
2023-10-25 16:51:57.692 
Epoch 81/1000 
	 loss: 39.9216, MinusLogProbMetric: 39.9216, val_loss: 40.2577, val_MinusLogProbMetric: 40.2577

Epoch 81: val_loss did not improve from 40.16660
196/196 - 31s - loss: 39.9216 - MinusLogProbMetric: 39.9216 - val_loss: 40.2577 - val_MinusLogProbMetric: 40.2577 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 82/1000
2023-10-25 16:52:29.247 
Epoch 82/1000 
	 loss: 39.6939, MinusLogProbMetric: 39.6939, val_loss: 40.0078, val_MinusLogProbMetric: 40.0078

Epoch 82: val_loss improved from 40.16660 to 40.00777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 39.6939 - MinusLogProbMetric: 39.6939 - val_loss: 40.0078 - val_MinusLogProbMetric: 40.0078 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 83/1000
2023-10-25 16:53:03.801 
Epoch 83/1000 
	 loss: 39.3929, MinusLogProbMetric: 39.3929, val_loss: 39.7086, val_MinusLogProbMetric: 39.7086

Epoch 83: val_loss improved from 40.00777 to 39.70858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 39.3929 - MinusLogProbMetric: 39.3929 - val_loss: 39.7086 - val_MinusLogProbMetric: 39.7086 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 84/1000
2023-10-25 16:53:38.898 
Epoch 84/1000 
	 loss: 39.5990, MinusLogProbMetric: 39.5990, val_loss: 40.0362, val_MinusLogProbMetric: 40.0362

Epoch 84: val_loss did not improve from 39.70858
196/196 - 35s - loss: 39.5990 - MinusLogProbMetric: 39.5990 - val_loss: 40.0362 - val_MinusLogProbMetric: 40.0362 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 85/1000
2023-10-25 16:54:13.213 
Epoch 85/1000 
	 loss: 39.5310, MinusLogProbMetric: 39.5310, val_loss: 39.4074, val_MinusLogProbMetric: 39.4074

Epoch 85: val_loss improved from 39.70858 to 39.40741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 39.5310 - MinusLogProbMetric: 39.5310 - val_loss: 39.4074 - val_MinusLogProbMetric: 39.4074 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 86/1000
2023-10-25 16:54:48.151 
Epoch 86/1000 
	 loss: 39.6427, MinusLogProbMetric: 39.6427, val_loss: 40.4724, val_MinusLogProbMetric: 40.4724

Epoch 86: val_loss did not improve from 39.40741
196/196 - 34s - loss: 39.6427 - MinusLogProbMetric: 39.6427 - val_loss: 40.4724 - val_MinusLogProbMetric: 40.4724 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 87/1000
2023-10-25 16:55:22.819 
Epoch 87/1000 
	 loss: 39.1241, MinusLogProbMetric: 39.1241, val_loss: 38.7708, val_MinusLogProbMetric: 38.7708

Epoch 87: val_loss improved from 39.40741 to 38.77079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 39.1241 - MinusLogProbMetric: 39.1241 - val_loss: 38.7708 - val_MinusLogProbMetric: 38.7708 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 88/1000
2023-10-25 16:55:56.972 
Epoch 88/1000 
	 loss: 39.3541, MinusLogProbMetric: 39.3541, val_loss: 39.7997, val_MinusLogProbMetric: 39.7997

Epoch 88: val_loss did not improve from 38.77079
196/196 - 34s - loss: 39.3541 - MinusLogProbMetric: 39.3541 - val_loss: 39.7997 - val_MinusLogProbMetric: 39.7997 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 89/1000
2023-10-25 16:56:28.601 
Epoch 89/1000 
	 loss: 39.0786, MinusLogProbMetric: 39.0786, val_loss: 38.3751, val_MinusLogProbMetric: 38.3751

Epoch 89: val_loss improved from 38.77079 to 38.37505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 39.0786 - MinusLogProbMetric: 39.0786 - val_loss: 38.3751 - val_MinusLogProbMetric: 38.3751 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 90/1000
2023-10-25 16:57:02.183 
Epoch 90/1000 
	 loss: 38.5293, MinusLogProbMetric: 38.5293, val_loss: 38.2198, val_MinusLogProbMetric: 38.2198

Epoch 90: val_loss improved from 38.37505 to 38.21982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 38.5293 - MinusLogProbMetric: 38.5293 - val_loss: 38.2198 - val_MinusLogProbMetric: 38.2198 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 91/1000
2023-10-25 16:57:35.573 
Epoch 91/1000 
	 loss: 38.8324, MinusLogProbMetric: 38.8324, val_loss: 39.6241, val_MinusLogProbMetric: 39.6241

Epoch 91: val_loss did not improve from 38.21982
196/196 - 33s - loss: 38.8324 - MinusLogProbMetric: 38.8324 - val_loss: 39.6241 - val_MinusLogProbMetric: 39.6241 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 92/1000
2023-10-25 16:58:09.592 
Epoch 92/1000 
	 loss: 38.4109, MinusLogProbMetric: 38.4109, val_loss: 42.3833, val_MinusLogProbMetric: 42.3833

Epoch 92: val_loss did not improve from 38.21982
196/196 - 34s - loss: 38.4109 - MinusLogProbMetric: 38.4109 - val_loss: 42.3833 - val_MinusLogProbMetric: 42.3833 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 93/1000
2023-10-25 16:58:43.980 
Epoch 93/1000 
	 loss: 38.7188, MinusLogProbMetric: 38.7188, val_loss: 38.8238, val_MinusLogProbMetric: 38.8238

Epoch 93: val_loss did not improve from 38.21982
196/196 - 34s - loss: 38.7188 - MinusLogProbMetric: 38.7188 - val_loss: 38.8238 - val_MinusLogProbMetric: 38.8238 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 94/1000
2023-10-25 16:59:18.531 
Epoch 94/1000 
	 loss: 38.0151, MinusLogProbMetric: 38.0151, val_loss: 38.0194, val_MinusLogProbMetric: 38.0194

Epoch 94: val_loss improved from 38.21982 to 38.01942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 38.0151 - MinusLogProbMetric: 38.0151 - val_loss: 38.0194 - val_MinusLogProbMetric: 38.0194 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 95/1000
2023-10-25 16:59:53.539 
Epoch 95/1000 
	 loss: 47.4361, MinusLogProbMetric: 47.4361, val_loss: 123.7414, val_MinusLogProbMetric: 123.7414

Epoch 95: val_loss did not improve from 38.01942
196/196 - 34s - loss: 47.4361 - MinusLogProbMetric: 47.4361 - val_loss: 123.7414 - val_MinusLogProbMetric: 123.7414 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 96/1000
2023-10-25 17:00:28.266 
Epoch 96/1000 
	 loss: 54.1074, MinusLogProbMetric: 54.1074, val_loss: 43.9039, val_MinusLogProbMetric: 43.9039

Epoch 96: val_loss did not improve from 38.01942
196/196 - 35s - loss: 54.1074 - MinusLogProbMetric: 54.1074 - val_loss: 43.9039 - val_MinusLogProbMetric: 43.9039 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 97/1000
2023-10-25 17:01:03.088 
Epoch 97/1000 
	 loss: 42.1348, MinusLogProbMetric: 42.1348, val_loss: 41.2440, val_MinusLogProbMetric: 41.2440

Epoch 97: val_loss did not improve from 38.01942
196/196 - 35s - loss: 42.1348 - MinusLogProbMetric: 42.1348 - val_loss: 41.2440 - val_MinusLogProbMetric: 41.2440 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 98/1000
2023-10-25 17:01:37.746 
Epoch 98/1000 
	 loss: 40.4244, MinusLogProbMetric: 40.4244, val_loss: 42.1631, val_MinusLogProbMetric: 42.1631

Epoch 98: val_loss did not improve from 38.01942
196/196 - 35s - loss: 40.4244 - MinusLogProbMetric: 40.4244 - val_loss: 42.1631 - val_MinusLogProbMetric: 42.1631 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 99/1000
2023-10-25 17:02:12.510 
Epoch 99/1000 
	 loss: 39.8168, MinusLogProbMetric: 39.8168, val_loss: 40.9831, val_MinusLogProbMetric: 40.9831

Epoch 99: val_loss did not improve from 38.01942
196/196 - 35s - loss: 39.8168 - MinusLogProbMetric: 39.8168 - val_loss: 40.9831 - val_MinusLogProbMetric: 40.9831 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 100/1000
2023-10-25 17:02:47.518 
Epoch 100/1000 
	 loss: 39.3386, MinusLogProbMetric: 39.3386, val_loss: 39.1781, val_MinusLogProbMetric: 39.1781

Epoch 100: val_loss did not improve from 38.01942
196/196 - 35s - loss: 39.3386 - MinusLogProbMetric: 39.3386 - val_loss: 39.1781 - val_MinusLogProbMetric: 39.1781 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 101/1000
2023-10-25 17:03:22.272 
Epoch 101/1000 
	 loss: 38.8812, MinusLogProbMetric: 38.8812, val_loss: 39.2483, val_MinusLogProbMetric: 39.2483

Epoch 101: val_loss did not improve from 38.01942
196/196 - 35s - loss: 38.8812 - MinusLogProbMetric: 38.8812 - val_loss: 39.2483 - val_MinusLogProbMetric: 39.2483 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 102/1000
2023-10-25 17:03:55.373 
Epoch 102/1000 
	 loss: 38.5638, MinusLogProbMetric: 38.5638, val_loss: 39.2325, val_MinusLogProbMetric: 39.2325

Epoch 102: val_loss did not improve from 38.01942
196/196 - 33s - loss: 38.5638 - MinusLogProbMetric: 38.5638 - val_loss: 39.2325 - val_MinusLogProbMetric: 39.2325 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 103/1000
2023-10-25 17:04:29.909 
Epoch 103/1000 
	 loss: 38.3172, MinusLogProbMetric: 38.3172, val_loss: 38.9310, val_MinusLogProbMetric: 38.9310

Epoch 103: val_loss did not improve from 38.01942
196/196 - 35s - loss: 38.3172 - MinusLogProbMetric: 38.3172 - val_loss: 38.9310 - val_MinusLogProbMetric: 38.9310 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 104/1000
2023-10-25 17:05:02.810 
Epoch 104/1000 
	 loss: 38.6985, MinusLogProbMetric: 38.6985, val_loss: 38.2908, val_MinusLogProbMetric: 38.2908

Epoch 104: val_loss did not improve from 38.01942
196/196 - 33s - loss: 38.6985 - MinusLogProbMetric: 38.6985 - val_loss: 38.2908 - val_MinusLogProbMetric: 38.2908 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 105/1000
2023-10-25 17:05:37.689 
Epoch 105/1000 
	 loss: 38.1445, MinusLogProbMetric: 38.1445, val_loss: 37.7537, val_MinusLogProbMetric: 37.7537

Epoch 105: val_loss improved from 38.01942 to 37.75372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 38.1445 - MinusLogProbMetric: 38.1445 - val_loss: 37.7537 - val_MinusLogProbMetric: 37.7537 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 106/1000
2023-10-25 17:06:12.961 
Epoch 106/1000 
	 loss: 38.0660, MinusLogProbMetric: 38.0660, val_loss: 37.8477, val_MinusLogProbMetric: 37.8477

Epoch 106: val_loss did not improve from 37.75372
196/196 - 35s - loss: 38.0660 - MinusLogProbMetric: 38.0660 - val_loss: 37.8477 - val_MinusLogProbMetric: 37.8477 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 107/1000
2023-10-25 17:06:47.625 
Epoch 107/1000 
	 loss: 37.6194, MinusLogProbMetric: 37.6194, val_loss: 38.6156, val_MinusLogProbMetric: 38.6156

Epoch 107: val_loss did not improve from 37.75372
196/196 - 35s - loss: 37.6194 - MinusLogProbMetric: 37.6194 - val_loss: 38.6156 - val_MinusLogProbMetric: 38.6156 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 108/1000
2023-10-25 17:07:22.257 
Epoch 108/1000 
	 loss: 37.5221, MinusLogProbMetric: 37.5221, val_loss: 37.7344, val_MinusLogProbMetric: 37.7344

Epoch 108: val_loss improved from 37.75372 to 37.73439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 37.5221 - MinusLogProbMetric: 37.5221 - val_loss: 37.7344 - val_MinusLogProbMetric: 37.7344 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 109/1000
2023-10-25 17:07:58.053 
Epoch 109/1000 
	 loss: 37.3751, MinusLogProbMetric: 37.3751, val_loss: 37.5984, val_MinusLogProbMetric: 37.5984

Epoch 109: val_loss improved from 37.73439 to 37.59838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 37.3751 - MinusLogProbMetric: 37.3751 - val_loss: 37.5984 - val_MinusLogProbMetric: 37.5984 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 110/1000
2023-10-25 17:08:33.486 
Epoch 110/1000 
	 loss: 37.0495, MinusLogProbMetric: 37.0495, val_loss: 37.2584, val_MinusLogProbMetric: 37.2584

Epoch 110: val_loss improved from 37.59838 to 37.25843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 37.0495 - MinusLogProbMetric: 37.0495 - val_loss: 37.2584 - val_MinusLogProbMetric: 37.2584 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 111/1000
2023-10-25 17:09:07.566 
Epoch 111/1000 
	 loss: 36.9673, MinusLogProbMetric: 36.9673, val_loss: 38.4055, val_MinusLogProbMetric: 38.4055

Epoch 111: val_loss did not improve from 37.25843
196/196 - 34s - loss: 36.9673 - MinusLogProbMetric: 36.9673 - val_loss: 38.4055 - val_MinusLogProbMetric: 38.4055 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 112/1000
2023-10-25 17:09:41.469 
Epoch 112/1000 
	 loss: 36.7578, MinusLogProbMetric: 36.7578, val_loss: 36.5045, val_MinusLogProbMetric: 36.5045

Epoch 112: val_loss improved from 37.25843 to 36.50446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 36.7578 - MinusLogProbMetric: 36.7578 - val_loss: 36.5045 - val_MinusLogProbMetric: 36.5045 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 113/1000
2023-10-25 17:10:16.776 
Epoch 113/1000 
	 loss: 36.7581, MinusLogProbMetric: 36.7581, val_loss: 36.4634, val_MinusLogProbMetric: 36.4634

Epoch 113: val_loss improved from 36.50446 to 36.46336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 36.7581 - MinusLogProbMetric: 36.7581 - val_loss: 36.4634 - val_MinusLogProbMetric: 36.4634 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 114/1000
2023-10-25 17:10:47.889 
Epoch 114/1000 
	 loss: 36.6336, MinusLogProbMetric: 36.6336, val_loss: 37.6437, val_MinusLogProbMetric: 37.6437

Epoch 114: val_loss did not improve from 36.46336
196/196 - 31s - loss: 36.6336 - MinusLogProbMetric: 36.6336 - val_loss: 37.6437 - val_MinusLogProbMetric: 37.6437 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 115/1000
2023-10-25 17:11:20.426 
Epoch 115/1000 
	 loss: 36.3751, MinusLogProbMetric: 36.3751, val_loss: 36.4725, val_MinusLogProbMetric: 36.4725

Epoch 115: val_loss did not improve from 36.46336
196/196 - 33s - loss: 36.3751 - MinusLogProbMetric: 36.3751 - val_loss: 36.4725 - val_MinusLogProbMetric: 36.4725 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 116/1000
2023-10-25 17:11:55.026 
Epoch 116/1000 
	 loss: 36.4103, MinusLogProbMetric: 36.4103, val_loss: 38.8568, val_MinusLogProbMetric: 38.8568

Epoch 116: val_loss did not improve from 36.46336
196/196 - 35s - loss: 36.4103 - MinusLogProbMetric: 36.4103 - val_loss: 38.8568 - val_MinusLogProbMetric: 38.8568 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 117/1000
2023-10-25 17:12:29.610 
Epoch 117/1000 
	 loss: 36.6552, MinusLogProbMetric: 36.6552, val_loss: 35.9075, val_MinusLogProbMetric: 35.9075

Epoch 117: val_loss improved from 36.46336 to 35.90755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 36.6552 - MinusLogProbMetric: 36.6552 - val_loss: 35.9075 - val_MinusLogProbMetric: 35.9075 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 118/1000
2023-10-25 17:13:05.345 
Epoch 118/1000 
	 loss: 36.1133, MinusLogProbMetric: 36.1133, val_loss: 36.3639, val_MinusLogProbMetric: 36.3639

Epoch 118: val_loss did not improve from 35.90755
196/196 - 35s - loss: 36.1133 - MinusLogProbMetric: 36.1133 - val_loss: 36.3639 - val_MinusLogProbMetric: 36.3639 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 119/1000
2023-10-25 17:13:39.940 
Epoch 119/1000 
	 loss: 36.1946, MinusLogProbMetric: 36.1946, val_loss: 37.5832, val_MinusLogProbMetric: 37.5832

Epoch 119: val_loss did not improve from 35.90755
196/196 - 35s - loss: 36.1946 - MinusLogProbMetric: 36.1946 - val_loss: 37.5832 - val_MinusLogProbMetric: 37.5832 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 120/1000
2023-10-25 17:14:14.648 
Epoch 120/1000 
	 loss: 36.1167, MinusLogProbMetric: 36.1167, val_loss: 37.1554, val_MinusLogProbMetric: 37.1554

Epoch 120: val_loss did not improve from 35.90755
196/196 - 35s - loss: 36.1167 - MinusLogProbMetric: 36.1167 - val_loss: 37.1554 - val_MinusLogProbMetric: 37.1554 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 121/1000
2023-10-25 17:14:49.172 
Epoch 121/1000 
	 loss: 35.9856, MinusLogProbMetric: 35.9856, val_loss: 37.4169, val_MinusLogProbMetric: 37.4169

Epoch 121: val_loss did not improve from 35.90755
196/196 - 35s - loss: 35.9856 - MinusLogProbMetric: 35.9856 - val_loss: 37.4169 - val_MinusLogProbMetric: 37.4169 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 122/1000
2023-10-25 17:15:23.893 
Epoch 122/1000 
	 loss: 36.1462, MinusLogProbMetric: 36.1462, val_loss: 36.6237, val_MinusLogProbMetric: 36.6237

Epoch 122: val_loss did not improve from 35.90755
196/196 - 35s - loss: 36.1462 - MinusLogProbMetric: 36.1462 - val_loss: 36.6237 - val_MinusLogProbMetric: 36.6237 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 123/1000
2023-10-25 17:15:58.619 
Epoch 123/1000 
	 loss: 35.8960, MinusLogProbMetric: 35.8960, val_loss: 37.0998, val_MinusLogProbMetric: 37.0998

Epoch 123: val_loss did not improve from 35.90755
196/196 - 35s - loss: 35.8960 - MinusLogProbMetric: 35.8960 - val_loss: 37.0998 - val_MinusLogProbMetric: 37.0998 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 124/1000
2023-10-25 17:16:33.263 
Epoch 124/1000 
	 loss: 36.1946, MinusLogProbMetric: 36.1946, val_loss: 35.9710, val_MinusLogProbMetric: 35.9710

Epoch 124: val_loss did not improve from 35.90755
196/196 - 35s - loss: 36.1946 - MinusLogProbMetric: 36.1946 - val_loss: 35.9710 - val_MinusLogProbMetric: 35.9710 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 125/1000
2023-10-25 17:17:08.205 
Epoch 125/1000 
	 loss: 35.6948, MinusLogProbMetric: 35.6948, val_loss: 35.7760, val_MinusLogProbMetric: 35.7760

Epoch 125: val_loss improved from 35.90755 to 35.77599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 35.6948 - MinusLogProbMetric: 35.6948 - val_loss: 35.7760 - val_MinusLogProbMetric: 35.7760 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 126/1000
2023-10-25 17:17:43.395 
Epoch 126/1000 
	 loss: 35.4813, MinusLogProbMetric: 35.4813, val_loss: 36.5721, val_MinusLogProbMetric: 36.5721

Epoch 126: val_loss did not improve from 35.77599
196/196 - 35s - loss: 35.4813 - MinusLogProbMetric: 35.4813 - val_loss: 36.5721 - val_MinusLogProbMetric: 36.5721 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 127/1000
2023-10-25 17:18:18.095 
Epoch 127/1000 
	 loss: 35.7843, MinusLogProbMetric: 35.7843, val_loss: 36.5754, val_MinusLogProbMetric: 36.5754

Epoch 127: val_loss did not improve from 35.77599
196/196 - 35s - loss: 35.7843 - MinusLogProbMetric: 35.7843 - val_loss: 36.5754 - val_MinusLogProbMetric: 36.5754 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 128/1000
2023-10-25 17:18:52.836 
Epoch 128/1000 
	 loss: 35.5182, MinusLogProbMetric: 35.5182, val_loss: 36.3324, val_MinusLogProbMetric: 36.3324

Epoch 128: val_loss did not improve from 35.77599
196/196 - 35s - loss: 35.5182 - MinusLogProbMetric: 35.5182 - val_loss: 36.3324 - val_MinusLogProbMetric: 36.3324 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 129/1000
2023-10-25 17:19:27.667 
Epoch 129/1000 
	 loss: 35.5844, MinusLogProbMetric: 35.5844, val_loss: 36.8871, val_MinusLogProbMetric: 36.8871

Epoch 129: val_loss did not improve from 35.77599
196/196 - 35s - loss: 35.5844 - MinusLogProbMetric: 35.5844 - val_loss: 36.8871 - val_MinusLogProbMetric: 36.8871 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 130/1000
2023-10-25 17:20:02.423 
Epoch 130/1000 
	 loss: 35.5392, MinusLogProbMetric: 35.5392, val_loss: 36.6399, val_MinusLogProbMetric: 36.6399

Epoch 130: val_loss did not improve from 35.77599
196/196 - 35s - loss: 35.5392 - MinusLogProbMetric: 35.5392 - val_loss: 36.6399 - val_MinusLogProbMetric: 36.6399 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 131/1000
2023-10-25 17:20:37.102 
Epoch 131/1000 
	 loss: 35.2652, MinusLogProbMetric: 35.2652, val_loss: 35.4460, val_MinusLogProbMetric: 35.4460

Epoch 131: val_loss improved from 35.77599 to 35.44598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 35.2652 - MinusLogProbMetric: 35.2652 - val_loss: 35.4460 - val_MinusLogProbMetric: 35.4460 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 132/1000
2023-10-25 17:21:12.436 
Epoch 132/1000 
	 loss: 35.2220, MinusLogProbMetric: 35.2220, val_loss: 35.4559, val_MinusLogProbMetric: 35.4559

Epoch 132: val_loss did not improve from 35.44598
196/196 - 35s - loss: 35.2220 - MinusLogProbMetric: 35.2220 - val_loss: 35.4559 - val_MinusLogProbMetric: 35.4559 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 133/1000
2023-10-25 17:21:47.147 
Epoch 133/1000 
	 loss: 35.6209, MinusLogProbMetric: 35.6209, val_loss: 34.9898, val_MinusLogProbMetric: 34.9898

Epoch 133: val_loss improved from 35.44598 to 34.98978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 35.6209 - MinusLogProbMetric: 35.6209 - val_loss: 34.9898 - val_MinusLogProbMetric: 34.9898 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 134/1000
2023-10-25 17:22:22.533 
Epoch 134/1000 
	 loss: 35.2538, MinusLogProbMetric: 35.2538, val_loss: 34.8976, val_MinusLogProbMetric: 34.8976

Epoch 134: val_loss improved from 34.98978 to 34.89757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 35.2538 - MinusLogProbMetric: 35.2538 - val_loss: 34.8976 - val_MinusLogProbMetric: 34.8976 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 135/1000
2023-10-25 17:22:58.053 
Epoch 135/1000 
	 loss: 35.3241, MinusLogProbMetric: 35.3241, val_loss: 36.7048, val_MinusLogProbMetric: 36.7048

Epoch 135: val_loss did not improve from 34.89757
196/196 - 35s - loss: 35.3241 - MinusLogProbMetric: 35.3241 - val_loss: 36.7048 - val_MinusLogProbMetric: 36.7048 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 136/1000
2023-10-25 17:23:32.707 
Epoch 136/1000 
	 loss: 35.0832, MinusLogProbMetric: 35.0832, val_loss: 34.9185, val_MinusLogProbMetric: 34.9185

Epoch 136: val_loss did not improve from 34.89757
196/196 - 35s - loss: 35.0832 - MinusLogProbMetric: 35.0832 - val_loss: 34.9185 - val_MinusLogProbMetric: 34.9185 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 137/1000
2023-10-25 17:24:07.260 
Epoch 137/1000 
	 loss: 34.8334, MinusLogProbMetric: 34.8334, val_loss: 35.1330, val_MinusLogProbMetric: 35.1330

Epoch 137: val_loss did not improve from 34.89757
196/196 - 35s - loss: 34.8334 - MinusLogProbMetric: 34.8334 - val_loss: 35.1330 - val_MinusLogProbMetric: 35.1330 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 138/1000
2023-10-25 17:24:41.983 
Epoch 138/1000 
	 loss: 35.3786, MinusLogProbMetric: 35.3786, val_loss: 35.6744, val_MinusLogProbMetric: 35.6744

Epoch 138: val_loss did not improve from 34.89757
196/196 - 35s - loss: 35.3786 - MinusLogProbMetric: 35.3786 - val_loss: 35.6744 - val_MinusLogProbMetric: 35.6744 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 139/1000
2023-10-25 17:25:16.614 
Epoch 139/1000 
	 loss: 35.8052, MinusLogProbMetric: 35.8052, val_loss: 34.8969, val_MinusLogProbMetric: 34.8969

Epoch 139: val_loss improved from 34.89757 to 34.89690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 35.8052 - MinusLogProbMetric: 35.8052 - val_loss: 34.8969 - val_MinusLogProbMetric: 34.8969 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 140/1000
2023-10-25 17:25:51.686 
Epoch 140/1000 
	 loss: 34.5834, MinusLogProbMetric: 34.5834, val_loss: 35.0137, val_MinusLogProbMetric: 35.0137

Epoch 140: val_loss did not improve from 34.89690
196/196 - 35s - loss: 34.5834 - MinusLogProbMetric: 34.5834 - val_loss: 35.0137 - val_MinusLogProbMetric: 35.0137 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 141/1000
2023-10-25 17:26:26.477 
Epoch 141/1000 
	 loss: 35.0188, MinusLogProbMetric: 35.0188, val_loss: 35.9159, val_MinusLogProbMetric: 35.9159

Epoch 141: val_loss did not improve from 34.89690
196/196 - 35s - loss: 35.0188 - MinusLogProbMetric: 35.0188 - val_loss: 35.9159 - val_MinusLogProbMetric: 35.9159 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 142/1000
2023-10-25 17:27:01.307 
Epoch 142/1000 
	 loss: 34.8141, MinusLogProbMetric: 34.8141, val_loss: 34.6462, val_MinusLogProbMetric: 34.6462

Epoch 142: val_loss improved from 34.89690 to 34.64621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 34.8141 - MinusLogProbMetric: 34.8141 - val_loss: 34.6462 - val_MinusLogProbMetric: 34.6462 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 143/1000
2023-10-25 17:27:36.611 
Epoch 143/1000 
	 loss: 34.7725, MinusLogProbMetric: 34.7725, val_loss: 34.6470, val_MinusLogProbMetric: 34.6470

Epoch 143: val_loss did not improve from 34.64621
196/196 - 35s - loss: 34.7725 - MinusLogProbMetric: 34.7725 - val_loss: 34.6470 - val_MinusLogProbMetric: 34.6470 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 144/1000
2023-10-25 17:28:11.433 
Epoch 144/1000 
	 loss: 34.6982, MinusLogProbMetric: 34.6982, val_loss: 35.1256, val_MinusLogProbMetric: 35.1256

Epoch 144: val_loss did not improve from 34.64621
196/196 - 35s - loss: 34.6982 - MinusLogProbMetric: 34.6982 - val_loss: 35.1256 - val_MinusLogProbMetric: 35.1256 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 145/1000
2023-10-25 17:28:46.018 
Epoch 145/1000 
	 loss: 35.0013, MinusLogProbMetric: 35.0013, val_loss: 35.9968, val_MinusLogProbMetric: 35.9968

Epoch 145: val_loss did not improve from 34.64621
196/196 - 35s - loss: 35.0013 - MinusLogProbMetric: 35.0013 - val_loss: 35.9968 - val_MinusLogProbMetric: 35.9968 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 146/1000
2023-10-25 17:29:20.854 
Epoch 146/1000 
	 loss: 35.0470, MinusLogProbMetric: 35.0470, val_loss: 35.0349, val_MinusLogProbMetric: 35.0349

Epoch 146: val_loss did not improve from 34.64621
196/196 - 35s - loss: 35.0470 - MinusLogProbMetric: 35.0470 - val_loss: 35.0349 - val_MinusLogProbMetric: 35.0349 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 147/1000
2023-10-25 17:29:55.410 
Epoch 147/1000 
	 loss: 34.6383, MinusLogProbMetric: 34.6383, val_loss: 34.5812, val_MinusLogProbMetric: 34.5812

Epoch 147: val_loss improved from 34.64621 to 34.58121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 34.6383 - MinusLogProbMetric: 34.6383 - val_loss: 34.5812 - val_MinusLogProbMetric: 34.5812 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 148/1000
2023-10-25 17:30:30.787 
Epoch 148/1000 
	 loss: 34.6237, MinusLogProbMetric: 34.6237, val_loss: 34.7753, val_MinusLogProbMetric: 34.7753

Epoch 148: val_loss did not improve from 34.58121
196/196 - 35s - loss: 34.6237 - MinusLogProbMetric: 34.6237 - val_loss: 34.7753 - val_MinusLogProbMetric: 34.7753 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 149/1000
2023-10-25 17:31:05.753 
Epoch 149/1000 
	 loss: 34.4470, MinusLogProbMetric: 34.4470, val_loss: 34.4614, val_MinusLogProbMetric: 34.4614

Epoch 149: val_loss improved from 34.58121 to 34.46136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 34.4470 - MinusLogProbMetric: 34.4470 - val_loss: 34.4614 - val_MinusLogProbMetric: 34.4614 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 150/1000
2023-10-25 17:31:40.957 
Epoch 150/1000 
	 loss: 34.4093, MinusLogProbMetric: 34.4093, val_loss: 34.6602, val_MinusLogProbMetric: 34.6602

Epoch 150: val_loss did not improve from 34.46136
196/196 - 35s - loss: 34.4093 - MinusLogProbMetric: 34.4093 - val_loss: 34.6602 - val_MinusLogProbMetric: 34.6602 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 151/1000
2023-10-25 17:32:15.697 
Epoch 151/1000 
	 loss: 34.8309, MinusLogProbMetric: 34.8309, val_loss: 34.3400, val_MinusLogProbMetric: 34.3400

Epoch 151: val_loss improved from 34.46136 to 34.33995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 34.8309 - MinusLogProbMetric: 34.8309 - val_loss: 34.3400 - val_MinusLogProbMetric: 34.3400 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 152/1000
2023-10-25 17:32:51.046 
Epoch 152/1000 
	 loss: 34.1738, MinusLogProbMetric: 34.1738, val_loss: 34.3871, val_MinusLogProbMetric: 34.3871

Epoch 152: val_loss did not improve from 34.33995
196/196 - 35s - loss: 34.1738 - MinusLogProbMetric: 34.1738 - val_loss: 34.3871 - val_MinusLogProbMetric: 34.3871 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 153/1000
2023-10-25 17:33:25.757 
Epoch 153/1000 
	 loss: 34.4158, MinusLogProbMetric: 34.4158, val_loss: 35.2447, val_MinusLogProbMetric: 35.2447

Epoch 153: val_loss did not improve from 34.33995
196/196 - 35s - loss: 34.4158 - MinusLogProbMetric: 34.4158 - val_loss: 35.2447 - val_MinusLogProbMetric: 35.2447 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 154/1000
2023-10-25 17:34:00.244 
Epoch 154/1000 
	 loss: 34.8397, MinusLogProbMetric: 34.8397, val_loss: 40.2775, val_MinusLogProbMetric: 40.2775

Epoch 154: val_loss did not improve from 34.33995
196/196 - 34s - loss: 34.8397 - MinusLogProbMetric: 34.8397 - val_loss: 40.2775 - val_MinusLogProbMetric: 40.2775 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 155/1000
2023-10-25 17:34:35.014 
Epoch 155/1000 
	 loss: 34.6229, MinusLogProbMetric: 34.6229, val_loss: 34.9589, val_MinusLogProbMetric: 34.9589

Epoch 155: val_loss did not improve from 34.33995
196/196 - 35s - loss: 34.6229 - MinusLogProbMetric: 34.6229 - val_loss: 34.9589 - val_MinusLogProbMetric: 34.9589 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 156/1000
2023-10-25 17:35:09.647 
Epoch 156/1000 
	 loss: 33.9929, MinusLogProbMetric: 33.9929, val_loss: 33.9269, val_MinusLogProbMetric: 33.9269

Epoch 156: val_loss improved from 34.33995 to 33.92692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 33.9929 - MinusLogProbMetric: 33.9929 - val_loss: 33.9269 - val_MinusLogProbMetric: 33.9269 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 157/1000
2023-10-25 17:35:44.519 
Epoch 157/1000 
	 loss: 33.9735, MinusLogProbMetric: 33.9735, val_loss: 35.0778, val_MinusLogProbMetric: 35.0778

Epoch 157: val_loss did not improve from 33.92692
196/196 - 34s - loss: 33.9735 - MinusLogProbMetric: 33.9735 - val_loss: 35.0778 - val_MinusLogProbMetric: 35.0778 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 158/1000
2023-10-25 17:36:19.402 
Epoch 158/1000 
	 loss: 34.4909, MinusLogProbMetric: 34.4909, val_loss: 34.3441, val_MinusLogProbMetric: 34.3441

Epoch 158: val_loss did not improve from 33.92692
196/196 - 35s - loss: 34.4909 - MinusLogProbMetric: 34.4909 - val_loss: 34.3441 - val_MinusLogProbMetric: 34.3441 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 159/1000
2023-10-25 17:36:54.291 
Epoch 159/1000 
	 loss: 34.6652, MinusLogProbMetric: 34.6652, val_loss: 35.8799, val_MinusLogProbMetric: 35.8799

Epoch 159: val_loss did not improve from 33.92692
196/196 - 35s - loss: 34.6652 - MinusLogProbMetric: 34.6652 - val_loss: 35.8799 - val_MinusLogProbMetric: 35.8799 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 160/1000
2023-10-25 17:37:28.805 
Epoch 160/1000 
	 loss: 34.0069, MinusLogProbMetric: 34.0069, val_loss: 33.7604, val_MinusLogProbMetric: 33.7604

Epoch 160: val_loss improved from 33.92692 to 33.76040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 34.0069 - MinusLogProbMetric: 34.0069 - val_loss: 33.7604 - val_MinusLogProbMetric: 33.7604 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 161/1000
2023-10-25 17:38:03.990 
Epoch 161/1000 
	 loss: 34.0961, MinusLogProbMetric: 34.0961, val_loss: 36.7380, val_MinusLogProbMetric: 36.7380

Epoch 161: val_loss did not improve from 33.76040
196/196 - 35s - loss: 34.0961 - MinusLogProbMetric: 34.0961 - val_loss: 36.7380 - val_MinusLogProbMetric: 36.7380 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 162/1000
2023-10-25 17:38:38.879 
Epoch 162/1000 
	 loss: 34.6115, MinusLogProbMetric: 34.6115, val_loss: 35.0579, val_MinusLogProbMetric: 35.0579

Epoch 162: val_loss did not improve from 33.76040
196/196 - 35s - loss: 34.6115 - MinusLogProbMetric: 34.6115 - val_loss: 35.0579 - val_MinusLogProbMetric: 35.0579 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 163/1000
2023-10-25 17:39:13.632 
Epoch 163/1000 
	 loss: 34.0451, MinusLogProbMetric: 34.0451, val_loss: 34.1665, val_MinusLogProbMetric: 34.1665

Epoch 163: val_loss did not improve from 33.76040
196/196 - 35s - loss: 34.0451 - MinusLogProbMetric: 34.0451 - val_loss: 34.1665 - val_MinusLogProbMetric: 34.1665 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 164/1000
2023-10-25 17:39:48.545 
Epoch 164/1000 
	 loss: 33.9541, MinusLogProbMetric: 33.9541, val_loss: 34.6212, val_MinusLogProbMetric: 34.6212

Epoch 164: val_loss did not improve from 33.76040
196/196 - 35s - loss: 33.9541 - MinusLogProbMetric: 33.9541 - val_loss: 34.6212 - val_MinusLogProbMetric: 34.6212 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 165/1000
2023-10-25 17:40:23.537 
Epoch 165/1000 
	 loss: 34.0338, MinusLogProbMetric: 34.0338, val_loss: 33.6273, val_MinusLogProbMetric: 33.6273

Epoch 165: val_loss improved from 33.76040 to 33.62725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 34.0338 - MinusLogProbMetric: 34.0338 - val_loss: 33.6273 - val_MinusLogProbMetric: 33.6273 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 166/1000
2023-10-25 17:40:58.800 
Epoch 166/1000 
	 loss: 33.9655, MinusLogProbMetric: 33.9655, val_loss: 33.7819, val_MinusLogProbMetric: 33.7819

Epoch 166: val_loss did not improve from 33.62725
196/196 - 35s - loss: 33.9655 - MinusLogProbMetric: 33.9655 - val_loss: 33.7819 - val_MinusLogProbMetric: 33.7819 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 167/1000
2023-10-25 17:41:33.351 
Epoch 167/1000 
	 loss: 34.0077, MinusLogProbMetric: 34.0077, val_loss: 35.0659, val_MinusLogProbMetric: 35.0659

Epoch 167: val_loss did not improve from 33.62725
196/196 - 35s - loss: 34.0077 - MinusLogProbMetric: 34.0077 - val_loss: 35.0659 - val_MinusLogProbMetric: 35.0659 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 168/1000
2023-10-25 17:42:08.231 
Epoch 168/1000 
	 loss: 34.2966, MinusLogProbMetric: 34.2966, val_loss: 33.6188, val_MinusLogProbMetric: 33.6188

Epoch 168: val_loss improved from 33.62725 to 33.61879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 34.2966 - MinusLogProbMetric: 34.2966 - val_loss: 33.6188 - val_MinusLogProbMetric: 33.6188 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 169/1000
2023-10-25 17:42:43.551 
Epoch 169/1000 
	 loss: 33.8736, MinusLogProbMetric: 33.8736, val_loss: 34.7539, val_MinusLogProbMetric: 34.7539

Epoch 169: val_loss did not improve from 33.61879
196/196 - 35s - loss: 33.8736 - MinusLogProbMetric: 33.8736 - val_loss: 34.7539 - val_MinusLogProbMetric: 34.7539 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 170/1000
2023-10-25 17:43:18.340 
Epoch 170/1000 
	 loss: 33.7822, MinusLogProbMetric: 33.7822, val_loss: 33.4140, val_MinusLogProbMetric: 33.4140

Epoch 170: val_loss improved from 33.61879 to 33.41395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 33.7822 - MinusLogProbMetric: 33.7822 - val_loss: 33.4140 - val_MinusLogProbMetric: 33.4140 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 171/1000
2023-10-25 17:43:53.756 
Epoch 171/1000 
	 loss: 34.6318, MinusLogProbMetric: 34.6318, val_loss: 35.8092, val_MinusLogProbMetric: 35.8092

Epoch 171: val_loss did not improve from 33.41395
196/196 - 35s - loss: 34.6318 - MinusLogProbMetric: 34.6318 - val_loss: 35.8092 - val_MinusLogProbMetric: 35.8092 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 172/1000
2023-10-25 17:44:28.356 
Epoch 172/1000 
	 loss: 33.6423, MinusLogProbMetric: 33.6423, val_loss: 33.4336, val_MinusLogProbMetric: 33.4336

Epoch 172: val_loss did not improve from 33.41395
196/196 - 35s - loss: 33.6423 - MinusLogProbMetric: 33.6423 - val_loss: 33.4336 - val_MinusLogProbMetric: 33.4336 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 173/1000
2023-10-25 17:45:02.683 
Epoch 173/1000 
	 loss: 33.5430, MinusLogProbMetric: 33.5430, val_loss: 33.7306, val_MinusLogProbMetric: 33.7306

Epoch 173: val_loss did not improve from 33.41395
196/196 - 34s - loss: 33.5430 - MinusLogProbMetric: 33.5430 - val_loss: 33.7306 - val_MinusLogProbMetric: 33.7306 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 174/1000
2023-10-25 17:45:37.054 
Epoch 174/1000 
	 loss: 33.9596, MinusLogProbMetric: 33.9596, val_loss: 33.4183, val_MinusLogProbMetric: 33.4183

Epoch 174: val_loss did not improve from 33.41395
196/196 - 34s - loss: 33.9596 - MinusLogProbMetric: 33.9596 - val_loss: 33.4183 - val_MinusLogProbMetric: 33.4183 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 175/1000
2023-10-25 17:46:12.060 
Epoch 175/1000 
	 loss: 33.7368, MinusLogProbMetric: 33.7368, val_loss: 34.3535, val_MinusLogProbMetric: 34.3535

Epoch 175: val_loss did not improve from 33.41395
196/196 - 35s - loss: 33.7368 - MinusLogProbMetric: 33.7368 - val_loss: 34.3535 - val_MinusLogProbMetric: 34.3535 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 176/1000
2023-10-25 17:46:46.690 
Epoch 176/1000 
	 loss: 33.7629, MinusLogProbMetric: 33.7629, val_loss: 34.9028, val_MinusLogProbMetric: 34.9028

Epoch 176: val_loss did not improve from 33.41395
196/196 - 35s - loss: 33.7629 - MinusLogProbMetric: 33.7629 - val_loss: 34.9028 - val_MinusLogProbMetric: 34.9028 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 177/1000
2023-10-25 17:47:21.325 
Epoch 177/1000 
	 loss: 33.8478, MinusLogProbMetric: 33.8478, val_loss: 33.2148, val_MinusLogProbMetric: 33.2148

Epoch 177: val_loss improved from 33.41395 to 33.21477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 33.8478 - MinusLogProbMetric: 33.8478 - val_loss: 33.2148 - val_MinusLogProbMetric: 33.2148 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 178/1000
2023-10-25 17:47:56.540 
Epoch 178/1000 
	 loss: 33.5726, MinusLogProbMetric: 33.5726, val_loss: 34.2356, val_MinusLogProbMetric: 34.2356

Epoch 178: val_loss did not improve from 33.21477
196/196 - 35s - loss: 33.5726 - MinusLogProbMetric: 33.5726 - val_loss: 34.2356 - val_MinusLogProbMetric: 34.2356 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 179/1000
2023-10-25 17:48:31.312 
Epoch 179/1000 
	 loss: 33.9555, MinusLogProbMetric: 33.9555, val_loss: 34.1048, val_MinusLogProbMetric: 34.1048

Epoch 179: val_loss did not improve from 33.21477
196/196 - 35s - loss: 33.9555 - MinusLogProbMetric: 33.9555 - val_loss: 34.1048 - val_MinusLogProbMetric: 34.1048 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 180/1000
2023-10-25 17:49:05.917 
Epoch 180/1000 
	 loss: 50.7472, MinusLogProbMetric: 50.7472, val_loss: 310.7793, val_MinusLogProbMetric: 310.7793

Epoch 180: val_loss did not improve from 33.21477
196/196 - 35s - loss: 50.7472 - MinusLogProbMetric: 50.7472 - val_loss: 310.7793 - val_MinusLogProbMetric: 310.7793 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 181/1000
2023-10-25 17:49:40.560 
Epoch 181/1000 
	 loss: 74.6507, MinusLogProbMetric: 74.6507, val_loss: 49.8773, val_MinusLogProbMetric: 49.8773

Epoch 181: val_loss did not improve from 33.21477
196/196 - 35s - loss: 74.6507 - MinusLogProbMetric: 74.6507 - val_loss: 49.8773 - val_MinusLogProbMetric: 49.8773 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 182/1000
2023-10-25 17:50:15.299 
Epoch 182/1000 
	 loss: 46.7219, MinusLogProbMetric: 46.7219, val_loss: 44.6065, val_MinusLogProbMetric: 44.6065

Epoch 182: val_loss did not improve from 33.21477
196/196 - 35s - loss: 46.7219 - MinusLogProbMetric: 46.7219 - val_loss: 44.6065 - val_MinusLogProbMetric: 44.6065 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 183/1000
2023-10-25 17:50:50.094 
Epoch 183/1000 
	 loss: 42.9143, MinusLogProbMetric: 42.9143, val_loss: 41.6742, val_MinusLogProbMetric: 41.6742

Epoch 183: val_loss did not improve from 33.21477
196/196 - 35s - loss: 42.9143 - MinusLogProbMetric: 42.9143 - val_loss: 41.6742 - val_MinusLogProbMetric: 41.6742 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 184/1000
2023-10-25 17:51:24.936 
Epoch 184/1000 
	 loss: 40.6554, MinusLogProbMetric: 40.6554, val_loss: 39.9717, val_MinusLogProbMetric: 39.9717

Epoch 184: val_loss did not improve from 33.21477
196/196 - 35s - loss: 40.6554 - MinusLogProbMetric: 40.6554 - val_loss: 39.9717 - val_MinusLogProbMetric: 39.9717 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 185/1000
2023-10-25 17:51:59.699 
Epoch 185/1000 
	 loss: 39.3410, MinusLogProbMetric: 39.3410, val_loss: 39.0741, val_MinusLogProbMetric: 39.0741

Epoch 185: val_loss did not improve from 33.21477
196/196 - 35s - loss: 39.3410 - MinusLogProbMetric: 39.3410 - val_loss: 39.0741 - val_MinusLogProbMetric: 39.0741 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 186/1000
2023-10-25 17:52:34.236 
Epoch 186/1000 
	 loss: 38.5870, MinusLogProbMetric: 38.5870, val_loss: 38.2746, val_MinusLogProbMetric: 38.2746

Epoch 186: val_loss did not improve from 33.21477
196/196 - 35s - loss: 38.5870 - MinusLogProbMetric: 38.5870 - val_loss: 38.2746 - val_MinusLogProbMetric: 38.2746 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 187/1000
2023-10-25 17:53:09.173 
Epoch 187/1000 
	 loss: 37.8309, MinusLogProbMetric: 37.8309, val_loss: 37.5164, val_MinusLogProbMetric: 37.5164

Epoch 187: val_loss did not improve from 33.21477
196/196 - 35s - loss: 37.8309 - MinusLogProbMetric: 37.8309 - val_loss: 37.5164 - val_MinusLogProbMetric: 37.5164 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 188/1000
2023-10-25 17:53:43.877 
Epoch 188/1000 
	 loss: 36.8355, MinusLogProbMetric: 36.8355, val_loss: 36.1715, val_MinusLogProbMetric: 36.1715

Epoch 188: val_loss did not improve from 33.21477
196/196 - 35s - loss: 36.8355 - MinusLogProbMetric: 36.8355 - val_loss: 36.1715 - val_MinusLogProbMetric: 36.1715 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 189/1000
2023-10-25 17:54:18.991 
Epoch 189/1000 
	 loss: 36.4455, MinusLogProbMetric: 36.4455, val_loss: 35.8897, val_MinusLogProbMetric: 35.8897

Epoch 189: val_loss did not improve from 33.21477
196/196 - 35s - loss: 36.4455 - MinusLogProbMetric: 36.4455 - val_loss: 35.8897 - val_MinusLogProbMetric: 35.8897 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 190/1000
2023-10-25 17:54:53.808 
Epoch 190/1000 
	 loss: 35.7102, MinusLogProbMetric: 35.7102, val_loss: 37.0446, val_MinusLogProbMetric: 37.0446

Epoch 190: val_loss did not improve from 33.21477
196/196 - 35s - loss: 35.7102 - MinusLogProbMetric: 35.7102 - val_loss: 37.0446 - val_MinusLogProbMetric: 37.0446 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 191/1000
2023-10-25 17:55:26.658 
Epoch 191/1000 
	 loss: 35.4414, MinusLogProbMetric: 35.4414, val_loss: 35.7223, val_MinusLogProbMetric: 35.7223

Epoch 191: val_loss did not improve from 33.21477
196/196 - 33s - loss: 35.4414 - MinusLogProbMetric: 35.4414 - val_loss: 35.7223 - val_MinusLogProbMetric: 35.7223 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 192/1000
2023-10-25 17:55:56.305 
Epoch 192/1000 
	 loss: 35.1430, MinusLogProbMetric: 35.1430, val_loss: 37.5622, val_MinusLogProbMetric: 37.5622

Epoch 192: val_loss did not improve from 33.21477
196/196 - 30s - loss: 35.1430 - MinusLogProbMetric: 35.1430 - val_loss: 37.5622 - val_MinusLogProbMetric: 37.5622 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 193/1000
2023-10-25 17:56:27.275 
Epoch 193/1000 
	 loss: 34.9648, MinusLogProbMetric: 34.9648, val_loss: 34.7745, val_MinusLogProbMetric: 34.7745

Epoch 193: val_loss did not improve from 33.21477
196/196 - 31s - loss: 34.9648 - MinusLogProbMetric: 34.9648 - val_loss: 34.7745 - val_MinusLogProbMetric: 34.7745 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 194/1000
2023-10-25 17:57:00.704 
Epoch 194/1000 
	 loss: 34.9341, MinusLogProbMetric: 34.9341, val_loss: 34.9529, val_MinusLogProbMetric: 34.9529

Epoch 194: val_loss did not improve from 33.21477
196/196 - 33s - loss: 34.9341 - MinusLogProbMetric: 34.9341 - val_loss: 34.9529 - val_MinusLogProbMetric: 34.9529 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 195/1000
2023-10-25 17:57:35.207 
Epoch 195/1000 
	 loss: 35.0217, MinusLogProbMetric: 35.0217, val_loss: 34.7373, val_MinusLogProbMetric: 34.7373

Epoch 195: val_loss did not improve from 33.21477
196/196 - 34s - loss: 35.0217 - MinusLogProbMetric: 35.0217 - val_loss: 34.7373 - val_MinusLogProbMetric: 34.7373 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 196/1000
2023-10-25 17:58:08.979 
Epoch 196/1000 
	 loss: 34.4582, MinusLogProbMetric: 34.4582, val_loss: 35.2591, val_MinusLogProbMetric: 35.2591

Epoch 196: val_loss did not improve from 33.21477
196/196 - 34s - loss: 34.4582 - MinusLogProbMetric: 34.4582 - val_loss: 35.2591 - val_MinusLogProbMetric: 35.2591 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 197/1000
2023-10-25 17:58:40.497 
Epoch 197/1000 
	 loss: 34.7063, MinusLogProbMetric: 34.7063, val_loss: 35.0631, val_MinusLogProbMetric: 35.0631

Epoch 197: val_loss did not improve from 33.21477
196/196 - 32s - loss: 34.7063 - MinusLogProbMetric: 34.7063 - val_loss: 35.0631 - val_MinusLogProbMetric: 35.0631 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 198/1000
2023-10-25 17:59:14.795 
Epoch 198/1000 
	 loss: 34.2639, MinusLogProbMetric: 34.2639, val_loss: 34.4096, val_MinusLogProbMetric: 34.4096

Epoch 198: val_loss did not improve from 33.21477
196/196 - 34s - loss: 34.2639 - MinusLogProbMetric: 34.2639 - val_loss: 34.4096 - val_MinusLogProbMetric: 34.4096 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 199/1000
2023-10-25 17:59:47.905 
Epoch 199/1000 
	 loss: 34.0596, MinusLogProbMetric: 34.0596, val_loss: 34.0439, val_MinusLogProbMetric: 34.0439

Epoch 199: val_loss did not improve from 33.21477
196/196 - 33s - loss: 34.0596 - MinusLogProbMetric: 34.0596 - val_loss: 34.0439 - val_MinusLogProbMetric: 34.0439 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 200/1000
2023-10-25 18:00:21.939 
Epoch 200/1000 
	 loss: 34.1776, MinusLogProbMetric: 34.1776, val_loss: 34.4084, val_MinusLogProbMetric: 34.4084

Epoch 200: val_loss did not improve from 33.21477
196/196 - 34s - loss: 34.1776 - MinusLogProbMetric: 34.1776 - val_loss: 34.4084 - val_MinusLogProbMetric: 34.4084 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 201/1000
2023-10-25 18:00:55.957 
Epoch 201/1000 
	 loss: 34.1893, MinusLogProbMetric: 34.1893, val_loss: 34.2238, val_MinusLogProbMetric: 34.2238

Epoch 201: val_loss did not improve from 33.21477
196/196 - 34s - loss: 34.1893 - MinusLogProbMetric: 34.1893 - val_loss: 34.2238 - val_MinusLogProbMetric: 34.2238 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 202/1000
2023-10-25 18:01:27.921 
Epoch 202/1000 
	 loss: 34.2688, MinusLogProbMetric: 34.2688, val_loss: 33.8067, val_MinusLogProbMetric: 33.8067

Epoch 202: val_loss did not improve from 33.21477
196/196 - 32s - loss: 34.2688 - MinusLogProbMetric: 34.2688 - val_loss: 33.8067 - val_MinusLogProbMetric: 33.8067 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 203/1000
2023-10-25 18:01:58.331 
Epoch 203/1000 
	 loss: 33.9410, MinusLogProbMetric: 33.9410, val_loss: 34.0513, val_MinusLogProbMetric: 34.0513

Epoch 203: val_loss did not improve from 33.21477
196/196 - 30s - loss: 33.9410 - MinusLogProbMetric: 33.9410 - val_loss: 34.0513 - val_MinusLogProbMetric: 34.0513 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 204/1000
2023-10-25 18:02:28.582 
Epoch 204/1000 
	 loss: 33.9931, MinusLogProbMetric: 33.9931, val_loss: 34.2067, val_MinusLogProbMetric: 34.2067

Epoch 204: val_loss did not improve from 33.21477
196/196 - 30s - loss: 33.9931 - MinusLogProbMetric: 33.9931 - val_loss: 34.2067 - val_MinusLogProbMetric: 34.2067 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 205/1000
2023-10-25 18:03:03.198 
Epoch 205/1000 
	 loss: 33.9764, MinusLogProbMetric: 33.9764, val_loss: 34.4260, val_MinusLogProbMetric: 34.4260

Epoch 205: val_loss did not improve from 33.21477
196/196 - 35s - loss: 33.9764 - MinusLogProbMetric: 33.9764 - val_loss: 34.4260 - val_MinusLogProbMetric: 34.4260 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 206/1000
2023-10-25 18:03:36.988 
Epoch 206/1000 
	 loss: 33.9541, MinusLogProbMetric: 33.9541, val_loss: 33.6587, val_MinusLogProbMetric: 33.6587

Epoch 206: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.9541 - MinusLogProbMetric: 33.9541 - val_loss: 33.6587 - val_MinusLogProbMetric: 33.6587 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 207/1000
2023-10-25 18:04:08.979 
Epoch 207/1000 
	 loss: 33.9742, MinusLogProbMetric: 33.9742, val_loss: 35.2007, val_MinusLogProbMetric: 35.2007

Epoch 207: val_loss did not improve from 33.21477
196/196 - 32s - loss: 33.9742 - MinusLogProbMetric: 33.9742 - val_loss: 35.2007 - val_MinusLogProbMetric: 35.2007 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 208/1000
2023-10-25 18:04:39.826 
Epoch 208/1000 
	 loss: 34.0495, MinusLogProbMetric: 34.0495, val_loss: 33.8671, val_MinusLogProbMetric: 33.8671

Epoch 208: val_loss did not improve from 33.21477
196/196 - 31s - loss: 34.0495 - MinusLogProbMetric: 34.0495 - val_loss: 33.8671 - val_MinusLogProbMetric: 33.8671 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 209/1000
2023-10-25 18:05:11.917 
Epoch 209/1000 
	 loss: 33.8203, MinusLogProbMetric: 33.8203, val_loss: 35.2428, val_MinusLogProbMetric: 35.2428

Epoch 209: val_loss did not improve from 33.21477
196/196 - 32s - loss: 33.8203 - MinusLogProbMetric: 33.8203 - val_loss: 35.2428 - val_MinusLogProbMetric: 35.2428 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 210/1000
2023-10-25 18:05:46.636 
Epoch 210/1000 
	 loss: 33.7479, MinusLogProbMetric: 33.7479, val_loss: 33.7253, val_MinusLogProbMetric: 33.7253

Epoch 210: val_loss did not improve from 33.21477
196/196 - 35s - loss: 33.7479 - MinusLogProbMetric: 33.7479 - val_loss: 33.7253 - val_MinusLogProbMetric: 33.7253 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 211/1000
2023-10-25 18:06:20.972 
Epoch 211/1000 
	 loss: 33.7826, MinusLogProbMetric: 33.7826, val_loss: 33.4585, val_MinusLogProbMetric: 33.4585

Epoch 211: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.7826 - MinusLogProbMetric: 33.7826 - val_loss: 33.4585 - val_MinusLogProbMetric: 33.4585 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 212/1000
2023-10-25 18:06:54.392 
Epoch 212/1000 
	 loss: 33.7735, MinusLogProbMetric: 33.7735, val_loss: 33.6445, val_MinusLogProbMetric: 33.6445

Epoch 212: val_loss did not improve from 33.21477
196/196 - 33s - loss: 33.7735 - MinusLogProbMetric: 33.7735 - val_loss: 33.6445 - val_MinusLogProbMetric: 33.6445 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 213/1000
2023-10-25 18:07:28.796 
Epoch 213/1000 
	 loss: 33.5891, MinusLogProbMetric: 33.5891, val_loss: 34.0141, val_MinusLogProbMetric: 34.0141

Epoch 213: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.5891 - MinusLogProbMetric: 33.5891 - val_loss: 34.0141 - val_MinusLogProbMetric: 34.0141 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 214/1000
2023-10-25 18:08:01.897 
Epoch 214/1000 
	 loss: 33.6931, MinusLogProbMetric: 33.6931, val_loss: 33.7334, val_MinusLogProbMetric: 33.7334

Epoch 214: val_loss did not improve from 33.21477
196/196 - 33s - loss: 33.6931 - MinusLogProbMetric: 33.6931 - val_loss: 33.7334 - val_MinusLogProbMetric: 33.7334 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 215/1000
2023-10-25 18:08:34.908 
Epoch 215/1000 
	 loss: 33.4733, MinusLogProbMetric: 33.4733, val_loss: 33.2390, val_MinusLogProbMetric: 33.2390

Epoch 215: val_loss did not improve from 33.21477
196/196 - 33s - loss: 33.4733 - MinusLogProbMetric: 33.4733 - val_loss: 33.2390 - val_MinusLogProbMetric: 33.2390 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 216/1000
2023-10-25 18:09:09.011 
Epoch 216/1000 
	 loss: 33.3182, MinusLogProbMetric: 33.3182, val_loss: 34.3141, val_MinusLogProbMetric: 34.3141

Epoch 216: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.3182 - MinusLogProbMetric: 33.3182 - val_loss: 34.3141 - val_MinusLogProbMetric: 34.3141 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 217/1000
2023-10-25 18:09:43.355 
Epoch 217/1000 
	 loss: 33.5933, MinusLogProbMetric: 33.5933, val_loss: 33.2620, val_MinusLogProbMetric: 33.2620

Epoch 217: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.5933 - MinusLogProbMetric: 33.5933 - val_loss: 33.2620 - val_MinusLogProbMetric: 33.2620 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 218/1000
2023-10-25 18:10:17.308 
Epoch 218/1000 
	 loss: 33.4227, MinusLogProbMetric: 33.4227, val_loss: 34.2160, val_MinusLogProbMetric: 34.2160

Epoch 218: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.4227 - MinusLogProbMetric: 33.4227 - val_loss: 34.2160 - val_MinusLogProbMetric: 34.2160 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 219/1000
2023-10-25 18:10:48.945 
Epoch 219/1000 
	 loss: 33.5456, MinusLogProbMetric: 33.5456, val_loss: 34.2645, val_MinusLogProbMetric: 34.2645

Epoch 219: val_loss did not improve from 33.21477
196/196 - 32s - loss: 33.5456 - MinusLogProbMetric: 33.5456 - val_loss: 34.2645 - val_MinusLogProbMetric: 34.2645 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 220/1000
2023-10-25 18:11:18.714 
Epoch 220/1000 
	 loss: 33.6665, MinusLogProbMetric: 33.6665, val_loss: 34.3301, val_MinusLogProbMetric: 34.3301

Epoch 220: val_loss did not improve from 33.21477
196/196 - 30s - loss: 33.6665 - MinusLogProbMetric: 33.6665 - val_loss: 34.3301 - val_MinusLogProbMetric: 34.3301 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 221/1000
2023-10-25 18:11:53.041 
Epoch 221/1000 
	 loss: 33.1936, MinusLogProbMetric: 33.1936, val_loss: 34.0156, val_MinusLogProbMetric: 34.0156

Epoch 221: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.1936 - MinusLogProbMetric: 33.1936 - val_loss: 34.0156 - val_MinusLogProbMetric: 34.0156 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 222/1000
2023-10-25 18:12:27.083 
Epoch 222/1000 
	 loss: 33.4572, MinusLogProbMetric: 33.4572, val_loss: 34.4088, val_MinusLogProbMetric: 34.4088

Epoch 222: val_loss did not improve from 33.21477
196/196 - 34s - loss: 33.4572 - MinusLogProbMetric: 33.4572 - val_loss: 34.4088 - val_MinusLogProbMetric: 34.4088 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 223/1000
2023-10-25 18:12:57.671 
Epoch 223/1000 
	 loss: 33.1837, MinusLogProbMetric: 33.1837, val_loss: 33.5430, val_MinusLogProbMetric: 33.5430

Epoch 223: val_loss did not improve from 33.21477
196/196 - 31s - loss: 33.1837 - MinusLogProbMetric: 33.1837 - val_loss: 33.5430 - val_MinusLogProbMetric: 33.5430 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 224/1000
2023-10-25 18:13:29.232 
Epoch 224/1000 
	 loss: 33.0991, MinusLogProbMetric: 33.0991, val_loss: 33.0658, val_MinusLogProbMetric: 33.0658

Epoch 224: val_loss improved from 33.21477 to 33.06580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 33.0991 - MinusLogProbMetric: 33.0991 - val_loss: 33.0658 - val_MinusLogProbMetric: 33.0658 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 225/1000
2023-10-25 18:14:01.431 
Epoch 225/1000 
	 loss: 33.4192, MinusLogProbMetric: 33.4192, val_loss: 34.0711, val_MinusLogProbMetric: 34.0711

Epoch 225: val_loss did not improve from 33.06580
196/196 - 32s - loss: 33.4192 - MinusLogProbMetric: 33.4192 - val_loss: 34.0711 - val_MinusLogProbMetric: 34.0711 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 226/1000
2023-10-25 18:14:35.250 
Epoch 226/1000 
	 loss: 33.2573, MinusLogProbMetric: 33.2573, val_loss: 33.1615, val_MinusLogProbMetric: 33.1615

Epoch 226: val_loss did not improve from 33.06580
196/196 - 34s - loss: 33.2573 - MinusLogProbMetric: 33.2573 - val_loss: 33.1615 - val_MinusLogProbMetric: 33.1615 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 227/1000
2023-10-25 18:15:09.221 
Epoch 227/1000 
	 loss: 33.0900, MinusLogProbMetric: 33.0900, val_loss: 33.3701, val_MinusLogProbMetric: 33.3701

Epoch 227: val_loss did not improve from 33.06580
196/196 - 34s - loss: 33.0900 - MinusLogProbMetric: 33.0900 - val_loss: 33.3701 - val_MinusLogProbMetric: 33.3701 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 228/1000
2023-10-25 18:15:42.049 
Epoch 228/1000 
	 loss: 32.9387, MinusLogProbMetric: 32.9387, val_loss: 33.0922, val_MinusLogProbMetric: 33.0922

Epoch 228: val_loss did not improve from 33.06580
196/196 - 33s - loss: 32.9387 - MinusLogProbMetric: 32.9387 - val_loss: 33.0922 - val_MinusLogProbMetric: 33.0922 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 229/1000
2023-10-25 18:16:13.651 
Epoch 229/1000 
	 loss: 33.3002, MinusLogProbMetric: 33.3002, val_loss: 33.2682, val_MinusLogProbMetric: 33.2682

Epoch 229: val_loss did not improve from 33.06580
196/196 - 32s - loss: 33.3002 - MinusLogProbMetric: 33.3002 - val_loss: 33.2682 - val_MinusLogProbMetric: 33.2682 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 230/1000
2023-10-25 18:16:46.819 
Epoch 230/1000 
	 loss: 32.8880, MinusLogProbMetric: 32.8880, val_loss: 33.0467, val_MinusLogProbMetric: 33.0467

Epoch 230: val_loss improved from 33.06580 to 33.04667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 32.8880 - MinusLogProbMetric: 32.8880 - val_loss: 33.0467 - val_MinusLogProbMetric: 33.0467 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 231/1000
2023-10-25 18:17:19.998 
Epoch 231/1000 
	 loss: 33.0588, MinusLogProbMetric: 33.0588, val_loss: 32.9698, val_MinusLogProbMetric: 32.9698

Epoch 231: val_loss improved from 33.04667 to 32.96985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 33.0588 - MinusLogProbMetric: 33.0588 - val_loss: 32.9698 - val_MinusLogProbMetric: 32.9698 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 232/1000
2023-10-25 18:17:55.153 
Epoch 232/1000 
	 loss: 32.8759, MinusLogProbMetric: 32.8759, val_loss: 33.4528, val_MinusLogProbMetric: 33.4528

Epoch 232: val_loss did not improve from 32.96985
196/196 - 35s - loss: 32.8759 - MinusLogProbMetric: 32.8759 - val_loss: 33.4528 - val_MinusLogProbMetric: 33.4528 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 233/1000
2023-10-25 18:18:28.430 
Epoch 233/1000 
	 loss: 33.1257, MinusLogProbMetric: 33.1257, val_loss: 32.9588, val_MinusLogProbMetric: 32.9588

Epoch 233: val_loss improved from 32.96985 to 32.95882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 33.1257 - MinusLogProbMetric: 33.1257 - val_loss: 32.9588 - val_MinusLogProbMetric: 32.9588 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 234/1000
2023-10-25 18:19:01.497 
Epoch 234/1000 
	 loss: 32.8723, MinusLogProbMetric: 32.8723, val_loss: 32.6222, val_MinusLogProbMetric: 32.6222

Epoch 234: val_loss improved from 32.95882 to 32.62218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 32.8723 - MinusLogProbMetric: 32.8723 - val_loss: 32.6222 - val_MinusLogProbMetric: 32.6222 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 235/1000
2023-10-25 18:19:36.204 
Epoch 235/1000 
	 loss: 32.9027, MinusLogProbMetric: 32.9027, val_loss: 33.0592, val_MinusLogProbMetric: 33.0592

Epoch 235: val_loss did not improve from 32.62218
196/196 - 34s - loss: 32.9027 - MinusLogProbMetric: 32.9027 - val_loss: 33.0592 - val_MinusLogProbMetric: 33.0592 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 236/1000
2023-10-25 18:20:08.412 
Epoch 236/1000 
	 loss: 32.8296, MinusLogProbMetric: 32.8296, val_loss: 32.7715, val_MinusLogProbMetric: 32.7715

Epoch 236: val_loss did not improve from 32.62218
196/196 - 32s - loss: 32.8296 - MinusLogProbMetric: 32.8296 - val_loss: 32.7715 - val_MinusLogProbMetric: 32.7715 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 237/1000
2023-10-25 18:20:43.220 
Epoch 237/1000 
	 loss: 33.0403, MinusLogProbMetric: 33.0403, val_loss: 34.2323, val_MinusLogProbMetric: 34.2323

Epoch 237: val_loss did not improve from 32.62218
196/196 - 35s - loss: 33.0403 - MinusLogProbMetric: 33.0403 - val_loss: 34.2323 - val_MinusLogProbMetric: 34.2323 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 238/1000
2023-10-25 18:21:16.429 
Epoch 238/1000 
	 loss: 33.0028, MinusLogProbMetric: 33.0028, val_loss: 33.7052, val_MinusLogProbMetric: 33.7052

Epoch 238: val_loss did not improve from 32.62218
196/196 - 33s - loss: 33.0028 - MinusLogProbMetric: 33.0028 - val_loss: 33.7052 - val_MinusLogProbMetric: 33.7052 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 239/1000
2023-10-25 18:21:50.673 
Epoch 239/1000 
	 loss: 32.9497, MinusLogProbMetric: 32.9497, val_loss: 33.1666, val_MinusLogProbMetric: 33.1666

Epoch 239: val_loss did not improve from 32.62218
196/196 - 34s - loss: 32.9497 - MinusLogProbMetric: 32.9497 - val_loss: 33.1666 - val_MinusLogProbMetric: 33.1666 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 240/1000
2023-10-25 18:22:23.572 
Epoch 240/1000 
	 loss: 32.7080, MinusLogProbMetric: 32.7080, val_loss: 32.6194, val_MinusLogProbMetric: 32.6194

Epoch 240: val_loss improved from 32.62218 to 32.61937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 32.7080 - MinusLogProbMetric: 32.7080 - val_loss: 32.6194 - val_MinusLogProbMetric: 32.6194 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 241/1000
2023-10-25 18:22:54.108 
Epoch 241/1000 
	 loss: 32.8623, MinusLogProbMetric: 32.8623, val_loss: 33.4094, val_MinusLogProbMetric: 33.4094

Epoch 241: val_loss did not improve from 32.61937
196/196 - 30s - loss: 32.8623 - MinusLogProbMetric: 32.8623 - val_loss: 33.4094 - val_MinusLogProbMetric: 33.4094 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 242/1000
2023-10-25 18:23:27.603 
Epoch 242/1000 
	 loss: 32.9269, MinusLogProbMetric: 32.9269, val_loss: 32.9994, val_MinusLogProbMetric: 32.9994

Epoch 242: val_loss did not improve from 32.61937
196/196 - 33s - loss: 32.9269 - MinusLogProbMetric: 32.9269 - val_loss: 32.9994 - val_MinusLogProbMetric: 32.9994 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 243/1000
2023-10-25 18:24:02.575 
Epoch 243/1000 
	 loss: 32.8807, MinusLogProbMetric: 32.8807, val_loss: 32.8985, val_MinusLogProbMetric: 32.8985

Epoch 243: val_loss did not improve from 32.61937
196/196 - 35s - loss: 32.8807 - MinusLogProbMetric: 32.8807 - val_loss: 32.8985 - val_MinusLogProbMetric: 32.8985 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 244/1000
2023-10-25 18:24:36.671 
Epoch 244/1000 
	 loss: 32.8745, MinusLogProbMetric: 32.8745, val_loss: 34.9987, val_MinusLogProbMetric: 34.9987

Epoch 244: val_loss did not improve from 32.61937
196/196 - 34s - loss: 32.8745 - MinusLogProbMetric: 32.8745 - val_loss: 34.9987 - val_MinusLogProbMetric: 34.9987 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 245/1000
2023-10-25 18:25:08.149 
Epoch 245/1000 
	 loss: 32.8522, MinusLogProbMetric: 32.8522, val_loss: 32.5865, val_MinusLogProbMetric: 32.5865

Epoch 245: val_loss improved from 32.61937 to 32.58652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 32s - loss: 32.8522 - MinusLogProbMetric: 32.8522 - val_loss: 32.5865 - val_MinusLogProbMetric: 32.5865 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 246/1000
2023-10-25 18:25:40.443 
Epoch 246/1000 
	 loss: 32.7048, MinusLogProbMetric: 32.7048, val_loss: 32.8164, val_MinusLogProbMetric: 32.8164

Epoch 246: val_loss did not improve from 32.58652
196/196 - 32s - loss: 32.7048 - MinusLogProbMetric: 32.7048 - val_loss: 32.8164 - val_MinusLogProbMetric: 32.8164 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 247/1000
2023-10-25 18:26:13.597 
Epoch 247/1000 
	 loss: 32.7852, MinusLogProbMetric: 32.7852, val_loss: 32.8567, val_MinusLogProbMetric: 32.8567

Epoch 247: val_loss did not improve from 32.58652
196/196 - 33s - loss: 32.7852 - MinusLogProbMetric: 32.7852 - val_loss: 32.8567 - val_MinusLogProbMetric: 32.8567 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 248/1000
2023-10-25 18:26:48.587 
Epoch 248/1000 
	 loss: 32.6017, MinusLogProbMetric: 32.6017, val_loss: 32.4518, val_MinusLogProbMetric: 32.4518

Epoch 248: val_loss improved from 32.58652 to 32.45181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 32.6017 - MinusLogProbMetric: 32.6017 - val_loss: 32.4518 - val_MinusLogProbMetric: 32.4518 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 249/1000
2023-10-25 18:27:21.485 
Epoch 249/1000 
	 loss: 32.4781, MinusLogProbMetric: 32.4781, val_loss: 32.8467, val_MinusLogProbMetric: 32.8467

Epoch 249: val_loss did not improve from 32.45181
196/196 - 32s - loss: 32.4781 - MinusLogProbMetric: 32.4781 - val_loss: 32.8467 - val_MinusLogProbMetric: 32.8467 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 250/1000
2023-10-25 18:27:52.406 
Epoch 250/1000 
	 loss: 32.6853, MinusLogProbMetric: 32.6853, val_loss: 33.0584, val_MinusLogProbMetric: 33.0584

Epoch 250: val_loss did not improve from 32.45181
196/196 - 31s - loss: 32.6853 - MinusLogProbMetric: 32.6853 - val_loss: 33.0584 - val_MinusLogProbMetric: 33.0584 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 251/1000
2023-10-25 18:28:24.300 
Epoch 251/1000 
	 loss: 32.4455, MinusLogProbMetric: 32.4455, val_loss: 34.7572, val_MinusLogProbMetric: 34.7572

Epoch 251: val_loss did not improve from 32.45181
196/196 - 32s - loss: 32.4455 - MinusLogProbMetric: 32.4455 - val_loss: 34.7572 - val_MinusLogProbMetric: 34.7572 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 252/1000
2023-10-25 18:28:58.013 
Epoch 252/1000 
	 loss: 32.5856, MinusLogProbMetric: 32.5856, val_loss: 32.5624, val_MinusLogProbMetric: 32.5624

Epoch 252: val_loss did not improve from 32.45181
196/196 - 34s - loss: 32.5856 - MinusLogProbMetric: 32.5856 - val_loss: 32.5624 - val_MinusLogProbMetric: 32.5624 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 253/1000
2023-10-25 18:29:32.748 
Epoch 253/1000 
	 loss: 32.6514, MinusLogProbMetric: 32.6514, val_loss: 33.6932, val_MinusLogProbMetric: 33.6932

Epoch 253: val_loss did not improve from 32.45181
196/196 - 35s - loss: 32.6514 - MinusLogProbMetric: 32.6514 - val_loss: 33.6932 - val_MinusLogProbMetric: 33.6932 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 254/1000
2023-10-25 18:30:05.167 
Epoch 254/1000 
	 loss: 32.7944, MinusLogProbMetric: 32.7944, val_loss: 33.4743, val_MinusLogProbMetric: 33.4743

Epoch 254: val_loss did not improve from 32.45181
196/196 - 32s - loss: 32.7944 - MinusLogProbMetric: 32.7944 - val_loss: 33.4743 - val_MinusLogProbMetric: 33.4743 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 255/1000
2023-10-25 18:30:36.434 
Epoch 255/1000 
	 loss: 32.6074, MinusLogProbMetric: 32.6074, val_loss: 33.3449, val_MinusLogProbMetric: 33.3449

Epoch 255: val_loss did not improve from 32.45181
196/196 - 31s - loss: 32.6074 - MinusLogProbMetric: 32.6074 - val_loss: 33.3449 - val_MinusLogProbMetric: 33.3449 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 256/1000
2023-10-25 18:31:08.604 
Epoch 256/1000 
	 loss: 32.7486, MinusLogProbMetric: 32.7486, val_loss: 32.6016, val_MinusLogProbMetric: 32.6016

Epoch 256: val_loss did not improve from 32.45181
196/196 - 32s - loss: 32.7486 - MinusLogProbMetric: 32.7486 - val_loss: 32.6016 - val_MinusLogProbMetric: 32.6016 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 257/1000
2023-10-25 18:31:39.302 
Epoch 257/1000 
	 loss: 32.2251, MinusLogProbMetric: 32.2251, val_loss: 32.8206, val_MinusLogProbMetric: 32.8206

Epoch 257: val_loss did not improve from 32.45181
196/196 - 31s - loss: 32.2251 - MinusLogProbMetric: 32.2251 - val_loss: 32.8206 - val_MinusLogProbMetric: 32.8206 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 258/1000
2023-10-25 18:32:13.879 
Epoch 258/1000 
	 loss: 32.6304, MinusLogProbMetric: 32.6304, val_loss: 32.5067, val_MinusLogProbMetric: 32.5067

Epoch 258: val_loss did not improve from 32.45181
196/196 - 35s - loss: 32.6304 - MinusLogProbMetric: 32.6304 - val_loss: 32.5067 - val_MinusLogProbMetric: 32.5067 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 259/1000
2023-10-25 18:32:48.723 
Epoch 259/1000 
	 loss: 32.3107, MinusLogProbMetric: 32.3107, val_loss: 32.5185, val_MinusLogProbMetric: 32.5185

Epoch 259: val_loss did not improve from 32.45181
196/196 - 35s - loss: 32.3107 - MinusLogProbMetric: 32.3107 - val_loss: 32.5185 - val_MinusLogProbMetric: 32.5185 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 260/1000
2023-10-25 18:33:22.319 
Epoch 260/1000 
	 loss: 32.5114, MinusLogProbMetric: 32.5114, val_loss: 32.3404, val_MinusLogProbMetric: 32.3404

Epoch 260: val_loss improved from 32.45181 to 32.34044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 32.5114 - MinusLogProbMetric: 32.5114 - val_loss: 32.3404 - val_MinusLogProbMetric: 32.3404 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 261/1000
2023-10-25 18:33:56.685 
Epoch 261/1000 
	 loss: 32.4192, MinusLogProbMetric: 32.4192, val_loss: 33.1538, val_MinusLogProbMetric: 33.1538

Epoch 261: val_loss did not improve from 32.34044
196/196 - 34s - loss: 32.4192 - MinusLogProbMetric: 32.4192 - val_loss: 33.1538 - val_MinusLogProbMetric: 33.1538 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 262/1000
2023-10-25 18:34:29.336 
Epoch 262/1000 
	 loss: 32.4349, MinusLogProbMetric: 32.4349, val_loss: 32.8089, val_MinusLogProbMetric: 32.8089

Epoch 262: val_loss did not improve from 32.34044
196/196 - 33s - loss: 32.4349 - MinusLogProbMetric: 32.4349 - val_loss: 32.8089 - val_MinusLogProbMetric: 32.8089 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 263/1000
2023-10-25 18:35:03.466 
Epoch 263/1000 
	 loss: 32.2026, MinusLogProbMetric: 32.2026, val_loss: 32.6928, val_MinusLogProbMetric: 32.6928

Epoch 263: val_loss did not improve from 32.34044
196/196 - 34s - loss: 32.2026 - MinusLogProbMetric: 32.2026 - val_loss: 32.6928 - val_MinusLogProbMetric: 32.6928 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 264/1000
2023-10-25 18:35:38.325 
Epoch 264/1000 
	 loss: 32.2321, MinusLogProbMetric: 32.2321, val_loss: 32.9114, val_MinusLogProbMetric: 32.9114

Epoch 264: val_loss did not improve from 32.34044
196/196 - 35s - loss: 32.2321 - MinusLogProbMetric: 32.2321 - val_loss: 32.9114 - val_MinusLogProbMetric: 32.9114 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 265/1000
2023-10-25 18:36:12.962 
Epoch 265/1000 
	 loss: 32.1459, MinusLogProbMetric: 32.1459, val_loss: 33.5852, val_MinusLogProbMetric: 33.5852

Epoch 265: val_loss did not improve from 32.34044
196/196 - 35s - loss: 32.1459 - MinusLogProbMetric: 32.1459 - val_loss: 33.5852 - val_MinusLogProbMetric: 33.5852 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 266/1000
2023-10-25 18:36:45.603 
Epoch 266/1000 
	 loss: 33.2083, MinusLogProbMetric: 33.2083, val_loss: 32.3319, val_MinusLogProbMetric: 32.3319

Epoch 266: val_loss improved from 32.34044 to 32.33188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 33.2083 - MinusLogProbMetric: 33.2083 - val_loss: 32.3319 - val_MinusLogProbMetric: 32.3319 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 267/1000
2023-10-25 18:37:16.155 
Epoch 267/1000 
	 loss: 32.2857, MinusLogProbMetric: 32.2857, val_loss: 32.6560, val_MinusLogProbMetric: 32.6560

Epoch 267: val_loss did not improve from 32.33188
196/196 - 30s - loss: 32.2857 - MinusLogProbMetric: 32.2857 - val_loss: 32.6560 - val_MinusLogProbMetric: 32.6560 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 268/1000
2023-10-25 18:37:48.642 
Epoch 268/1000 
	 loss: 32.1591, MinusLogProbMetric: 32.1591, val_loss: 32.1482, val_MinusLogProbMetric: 32.1482

Epoch 268: val_loss improved from 32.33188 to 32.14816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 32.1591 - MinusLogProbMetric: 32.1591 - val_loss: 32.1482 - val_MinusLogProbMetric: 32.1482 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 269/1000
2023-10-25 18:38:23.724 
Epoch 269/1000 
	 loss: 32.1820, MinusLogProbMetric: 32.1820, val_loss: 31.8352, val_MinusLogProbMetric: 31.8352

Epoch 269: val_loss improved from 32.14816 to 31.83523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 32.1820 - MinusLogProbMetric: 32.1820 - val_loss: 31.8352 - val_MinusLogProbMetric: 31.8352 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 270/1000
2023-10-25 18:38:58.377 
Epoch 270/1000 
	 loss: 32.0890, MinusLogProbMetric: 32.0890, val_loss: 33.2996, val_MinusLogProbMetric: 33.2996

Epoch 270: val_loss did not improve from 31.83523
196/196 - 34s - loss: 32.0890 - MinusLogProbMetric: 32.0890 - val_loss: 33.2996 - val_MinusLogProbMetric: 33.2996 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 271/1000
2023-10-25 18:39:31.085 
Epoch 271/1000 
	 loss: 32.1879, MinusLogProbMetric: 32.1879, val_loss: 31.8117, val_MinusLogProbMetric: 31.8117

Epoch 271: val_loss improved from 31.83523 to 31.81174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 32.1879 - MinusLogProbMetric: 32.1879 - val_loss: 31.8117 - val_MinusLogProbMetric: 31.8117 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 272/1000
2023-10-25 18:40:04.813 
Epoch 272/1000 
	 loss: 32.2294, MinusLogProbMetric: 32.2294, val_loss: 34.3512, val_MinusLogProbMetric: 34.3512

Epoch 272: val_loss did not improve from 31.81174
196/196 - 33s - loss: 32.2294 - MinusLogProbMetric: 32.2294 - val_loss: 34.3512 - val_MinusLogProbMetric: 34.3512 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 273/1000
2023-10-25 18:40:39.123 
Epoch 273/1000 
	 loss: 32.1360, MinusLogProbMetric: 32.1360, val_loss: 32.7812, val_MinusLogProbMetric: 32.7812

Epoch 273: val_loss did not improve from 31.81174
196/196 - 34s - loss: 32.1360 - MinusLogProbMetric: 32.1360 - val_loss: 32.7812 - val_MinusLogProbMetric: 32.7812 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 274/1000
2023-10-25 18:41:14.026 
Epoch 274/1000 
	 loss: 32.3885, MinusLogProbMetric: 32.3885, val_loss: 31.6968, val_MinusLogProbMetric: 31.6968

Epoch 274: val_loss improved from 31.81174 to 31.69680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 32.3885 - MinusLogProbMetric: 32.3885 - val_loss: 31.6968 - val_MinusLogProbMetric: 31.6968 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 275/1000
2023-10-25 18:41:49.455 
Epoch 275/1000 
	 loss: 32.0776, MinusLogProbMetric: 32.0776, val_loss: 32.7116, val_MinusLogProbMetric: 32.7116

Epoch 275: val_loss did not improve from 31.69680
196/196 - 35s - loss: 32.0776 - MinusLogProbMetric: 32.0776 - val_loss: 32.7116 - val_MinusLogProbMetric: 32.7116 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 276/1000
2023-10-25 18:42:21.223 
Epoch 276/1000 
	 loss: 32.1278, MinusLogProbMetric: 32.1278, val_loss: 35.2453, val_MinusLogProbMetric: 35.2453

Epoch 276: val_loss did not improve from 31.69680
196/196 - 32s - loss: 32.1278 - MinusLogProbMetric: 32.1278 - val_loss: 35.2453 - val_MinusLogProbMetric: 35.2453 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 277/1000
2023-10-25 18:42:53.939 
Epoch 277/1000 
	 loss: 32.1627, MinusLogProbMetric: 32.1627, val_loss: 32.4868, val_MinusLogProbMetric: 32.4868

Epoch 277: val_loss did not improve from 31.69680
196/196 - 33s - loss: 32.1627 - MinusLogProbMetric: 32.1627 - val_loss: 32.4868 - val_MinusLogProbMetric: 32.4868 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 278/1000
2023-10-25 18:43:27.239 
Epoch 278/1000 
	 loss: 32.1474, MinusLogProbMetric: 32.1474, val_loss: 32.0694, val_MinusLogProbMetric: 32.0694

Epoch 278: val_loss did not improve from 31.69680
196/196 - 33s - loss: 32.1474 - MinusLogProbMetric: 32.1474 - val_loss: 32.0694 - val_MinusLogProbMetric: 32.0694 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 279/1000
2023-10-25 18:43:58.453 
Epoch 279/1000 
	 loss: 32.2216, MinusLogProbMetric: 32.2216, val_loss: 41.0826, val_MinusLogProbMetric: 41.0826

Epoch 279: val_loss did not improve from 31.69680
196/196 - 31s - loss: 32.2216 - MinusLogProbMetric: 32.2216 - val_loss: 41.0826 - val_MinusLogProbMetric: 41.0826 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 280/1000
2023-10-25 18:44:33.190 
Epoch 280/1000 
	 loss: 32.6998, MinusLogProbMetric: 32.6998, val_loss: 31.5944, val_MinusLogProbMetric: 31.5944

Epoch 280: val_loss improved from 31.69680 to 31.59436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 32.6998 - MinusLogProbMetric: 32.6998 - val_loss: 31.5944 - val_MinusLogProbMetric: 31.5944 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 281/1000
2023-10-25 18:45:08.303 
Epoch 281/1000 
	 loss: 32.1053, MinusLogProbMetric: 32.1053, val_loss: 32.4065, val_MinusLogProbMetric: 32.4065

Epoch 281: val_loss did not improve from 31.59436
196/196 - 35s - loss: 32.1053 - MinusLogProbMetric: 32.1053 - val_loss: 32.4065 - val_MinusLogProbMetric: 32.4065 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 282/1000
2023-10-25 18:45:42.439 
Epoch 282/1000 
	 loss: 31.8287, MinusLogProbMetric: 31.8287, val_loss: 31.9545, val_MinusLogProbMetric: 31.9545

Epoch 282: val_loss did not improve from 31.59436
196/196 - 34s - loss: 31.8287 - MinusLogProbMetric: 31.8287 - val_loss: 31.9545 - val_MinusLogProbMetric: 31.9545 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 283/1000
2023-10-25 18:46:13.572 
Epoch 283/1000 
	 loss: 31.7843, MinusLogProbMetric: 31.7843, val_loss: 31.8269, val_MinusLogProbMetric: 31.8269

Epoch 283: val_loss did not improve from 31.59436
196/196 - 31s - loss: 31.7843 - MinusLogProbMetric: 31.7843 - val_loss: 31.8269 - val_MinusLogProbMetric: 31.8269 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 284/1000
2023-10-25 18:46:45.108 
Epoch 284/1000 
	 loss: 31.8817, MinusLogProbMetric: 31.8817, val_loss: 32.8024, val_MinusLogProbMetric: 32.8024

Epoch 284: val_loss did not improve from 31.59436
196/196 - 32s - loss: 31.8817 - MinusLogProbMetric: 31.8817 - val_loss: 32.8024 - val_MinusLogProbMetric: 32.8024 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 285/1000
2023-10-25 18:47:14.629 
Epoch 285/1000 
	 loss: 32.0297, MinusLogProbMetric: 32.0297, val_loss: 32.7024, val_MinusLogProbMetric: 32.7024

Epoch 285: val_loss did not improve from 31.59436
196/196 - 30s - loss: 32.0297 - MinusLogProbMetric: 32.0297 - val_loss: 32.7024 - val_MinusLogProbMetric: 32.7024 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 286/1000
2023-10-25 18:47:44.856 
Epoch 286/1000 
	 loss: 31.9708, MinusLogProbMetric: 31.9708, val_loss: 32.1420, val_MinusLogProbMetric: 32.1420

Epoch 286: val_loss did not improve from 31.59436
196/196 - 30s - loss: 31.9708 - MinusLogProbMetric: 31.9708 - val_loss: 32.1420 - val_MinusLogProbMetric: 32.1420 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 287/1000
2023-10-25 18:48:16.395 
Epoch 287/1000 
	 loss: 31.9370, MinusLogProbMetric: 31.9370, val_loss: 31.9576, val_MinusLogProbMetric: 31.9576

Epoch 287: val_loss did not improve from 31.59436
196/196 - 32s - loss: 31.9370 - MinusLogProbMetric: 31.9370 - val_loss: 31.9576 - val_MinusLogProbMetric: 31.9576 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 288/1000
2023-10-25 18:48:50.973 
Epoch 288/1000 
	 loss: 32.2339, MinusLogProbMetric: 32.2339, val_loss: 32.5499, val_MinusLogProbMetric: 32.5499

Epoch 288: val_loss did not improve from 31.59436
196/196 - 35s - loss: 32.2339 - MinusLogProbMetric: 32.2339 - val_loss: 32.5499 - val_MinusLogProbMetric: 32.5499 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 289/1000
2023-10-25 18:49:24.576 
Epoch 289/1000 
	 loss: 31.6793, MinusLogProbMetric: 31.6793, val_loss: 32.2474, val_MinusLogProbMetric: 32.2474

Epoch 289: val_loss did not improve from 31.59436
196/196 - 34s - loss: 31.6793 - MinusLogProbMetric: 31.6793 - val_loss: 32.2474 - val_MinusLogProbMetric: 32.2474 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 290/1000
2023-10-25 18:49:53.930 
Epoch 290/1000 
	 loss: 32.4062, MinusLogProbMetric: 32.4062, val_loss: 32.6617, val_MinusLogProbMetric: 32.6617

Epoch 290: val_loss did not improve from 31.59436
196/196 - 29s - loss: 32.4062 - MinusLogProbMetric: 32.4062 - val_loss: 32.6617 - val_MinusLogProbMetric: 32.6617 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 291/1000
2023-10-25 18:50:23.508 
Epoch 291/1000 
	 loss: 31.9243, MinusLogProbMetric: 31.9243, val_loss: 31.6890, val_MinusLogProbMetric: 31.6890

Epoch 291: val_loss did not improve from 31.59436
196/196 - 30s - loss: 31.9243 - MinusLogProbMetric: 31.9243 - val_loss: 31.6890 - val_MinusLogProbMetric: 31.6890 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 292/1000
2023-10-25 18:50:54.411 
Epoch 292/1000 
	 loss: 31.9499, MinusLogProbMetric: 31.9499, val_loss: 31.8792, val_MinusLogProbMetric: 31.8792

Epoch 292: val_loss did not improve from 31.59436
196/196 - 31s - loss: 31.9499 - MinusLogProbMetric: 31.9499 - val_loss: 31.8792 - val_MinusLogProbMetric: 31.8792 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 293/1000
2023-10-25 18:51:24.267 
Epoch 293/1000 
	 loss: 31.8831, MinusLogProbMetric: 31.8831, val_loss: 31.9298, val_MinusLogProbMetric: 31.9298

Epoch 293: val_loss did not improve from 31.59436
196/196 - 30s - loss: 31.8831 - MinusLogProbMetric: 31.8831 - val_loss: 31.9298 - val_MinusLogProbMetric: 31.9298 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 294/1000
2023-10-25 18:51:56.706 
Epoch 294/1000 
	 loss: 31.7565, MinusLogProbMetric: 31.7565, val_loss: 32.1680, val_MinusLogProbMetric: 32.1680

Epoch 294: val_loss did not improve from 31.59436
196/196 - 32s - loss: 31.7565 - MinusLogProbMetric: 31.7565 - val_loss: 32.1680 - val_MinusLogProbMetric: 32.1680 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 295/1000
2023-10-25 18:52:30.167 
Epoch 295/1000 
	 loss: 31.9305, MinusLogProbMetric: 31.9305, val_loss: 31.8397, val_MinusLogProbMetric: 31.8397

Epoch 295: val_loss did not improve from 31.59436
196/196 - 33s - loss: 31.9305 - MinusLogProbMetric: 31.9305 - val_loss: 31.8397 - val_MinusLogProbMetric: 31.8397 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 296/1000
2023-10-25 18:53:04.155 
Epoch 296/1000 
	 loss: 32.6091, MinusLogProbMetric: 32.6091, val_loss: 32.6069, val_MinusLogProbMetric: 32.6069

Epoch 296: val_loss did not improve from 31.59436
196/196 - 34s - loss: 32.6091 - MinusLogProbMetric: 32.6091 - val_loss: 32.6069 - val_MinusLogProbMetric: 32.6069 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 297/1000
2023-10-25 18:53:35.032 
Epoch 297/1000 
	 loss: 31.6264, MinusLogProbMetric: 31.6264, val_loss: 32.5193, val_MinusLogProbMetric: 32.5193

Epoch 297: val_loss did not improve from 31.59436
196/196 - 31s - loss: 31.6264 - MinusLogProbMetric: 31.6264 - val_loss: 32.5193 - val_MinusLogProbMetric: 32.5193 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 298/1000
2023-10-25 18:54:02.476 
Epoch 298/1000 
	 loss: 31.8796, MinusLogProbMetric: 31.8796, val_loss: 31.9766, val_MinusLogProbMetric: 31.9766

Epoch 298: val_loss did not improve from 31.59436
196/196 - 27s - loss: 31.8796 - MinusLogProbMetric: 31.8796 - val_loss: 31.9766 - val_MinusLogProbMetric: 31.9766 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 299/1000
2023-10-25 18:54:30.613 
Epoch 299/1000 
	 loss: 31.6754, MinusLogProbMetric: 31.6754, val_loss: 31.6993, val_MinusLogProbMetric: 31.6993

Epoch 299: val_loss did not improve from 31.59436
196/196 - 28s - loss: 31.6754 - MinusLogProbMetric: 31.6754 - val_loss: 31.6993 - val_MinusLogProbMetric: 31.6993 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 300/1000
2023-10-25 18:55:00.629 
Epoch 300/1000 
	 loss: 31.7421, MinusLogProbMetric: 31.7421, val_loss: 31.6793, val_MinusLogProbMetric: 31.6793

Epoch 300: val_loss did not improve from 31.59436
196/196 - 30s - loss: 31.7421 - MinusLogProbMetric: 31.7421 - val_loss: 31.6793 - val_MinusLogProbMetric: 31.6793 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 301/1000
2023-10-25 18:55:30.944 
Epoch 301/1000 
	 loss: 31.7841, MinusLogProbMetric: 31.7841, val_loss: 31.5945, val_MinusLogProbMetric: 31.5945

Epoch 301: val_loss did not improve from 31.59436
196/196 - 30s - loss: 31.7841 - MinusLogProbMetric: 31.7841 - val_loss: 31.5945 - val_MinusLogProbMetric: 31.5945 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 302/1000
2023-10-25 18:56:02.325 
Epoch 302/1000 
	 loss: 31.7238, MinusLogProbMetric: 31.7238, val_loss: 33.7226, val_MinusLogProbMetric: 33.7226

Epoch 302: val_loss did not improve from 31.59436
196/196 - 31s - loss: 31.7238 - MinusLogProbMetric: 31.7238 - val_loss: 33.7226 - val_MinusLogProbMetric: 33.7226 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 303/1000
2023-10-25 18:56:34.161 
Epoch 303/1000 
	 loss: 31.7404, MinusLogProbMetric: 31.7404, val_loss: 32.1377, val_MinusLogProbMetric: 32.1377

Epoch 303: val_loss did not improve from 31.59436
196/196 - 32s - loss: 31.7404 - MinusLogProbMetric: 31.7404 - val_loss: 32.1377 - val_MinusLogProbMetric: 32.1377 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 304/1000
2023-10-25 18:57:06.145 
Epoch 304/1000 
	 loss: 31.9594, MinusLogProbMetric: 31.9594, val_loss: 32.2005, val_MinusLogProbMetric: 32.2005

Epoch 304: val_loss did not improve from 31.59436
196/196 - 32s - loss: 31.9594 - MinusLogProbMetric: 31.9594 - val_loss: 32.2005 - val_MinusLogProbMetric: 32.2005 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 305/1000
2023-10-25 18:57:38.183 
Epoch 305/1000 
	 loss: 31.5099, MinusLogProbMetric: 31.5099, val_loss: 31.7963, val_MinusLogProbMetric: 31.7963

Epoch 305: val_loss did not improve from 31.59436
196/196 - 32s - loss: 31.5099 - MinusLogProbMetric: 31.5099 - val_loss: 31.7963 - val_MinusLogProbMetric: 31.7963 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 306/1000
2023-10-25 18:58:08.344 
Epoch 306/1000 
	 loss: 31.5618, MinusLogProbMetric: 31.5618, val_loss: 32.2184, val_MinusLogProbMetric: 32.2184

Epoch 306: val_loss did not improve from 31.59436
196/196 - 30s - loss: 31.5618 - MinusLogProbMetric: 31.5618 - val_loss: 32.2184 - val_MinusLogProbMetric: 32.2184 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 307/1000
2023-10-25 18:58:38.308 
Epoch 307/1000 
	 loss: 31.8813, MinusLogProbMetric: 31.8813, val_loss: 33.1188, val_MinusLogProbMetric: 33.1188

Epoch 307: val_loss did not improve from 31.59436
196/196 - 30s - loss: 31.8813 - MinusLogProbMetric: 31.8813 - val_loss: 33.1188 - val_MinusLogProbMetric: 33.1188 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 308/1000
2023-10-25 18:59:08.586 
Epoch 308/1000 
	 loss: 31.7613, MinusLogProbMetric: 31.7613, val_loss: 31.4472, val_MinusLogProbMetric: 31.4472

Epoch 308: val_loss improved from 31.59436 to 31.44725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 31s - loss: 31.7613 - MinusLogProbMetric: 31.7613 - val_loss: 31.4472 - val_MinusLogProbMetric: 31.4472 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 309/1000
2023-10-25 18:59:42.020 
Epoch 309/1000 
	 loss: 32.3144, MinusLogProbMetric: 32.3144, val_loss: 32.2945, val_MinusLogProbMetric: 32.2945

Epoch 309: val_loss did not improve from 31.44725
196/196 - 33s - loss: 32.3144 - MinusLogProbMetric: 32.3144 - val_loss: 32.2945 - val_MinusLogProbMetric: 32.2945 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 310/1000
2023-10-25 19:00:14.619 
Epoch 310/1000 
	 loss: 31.6466, MinusLogProbMetric: 31.6466, val_loss: 32.2974, val_MinusLogProbMetric: 32.2974

Epoch 310: val_loss did not improve from 31.44725
196/196 - 33s - loss: 31.6466 - MinusLogProbMetric: 31.6466 - val_loss: 32.2974 - val_MinusLogProbMetric: 32.2974 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 311/1000
2023-10-25 19:00:46.935 
Epoch 311/1000 
	 loss: 31.5796, MinusLogProbMetric: 31.5796, val_loss: 31.2780, val_MinusLogProbMetric: 31.2780

Epoch 311: val_loss improved from 31.44725 to 31.27796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 31.5796 - MinusLogProbMetric: 31.5796 - val_loss: 31.2780 - val_MinusLogProbMetric: 31.2780 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 312/1000
2023-10-25 19:01:16.558 
Epoch 312/1000 
	 loss: 31.4788, MinusLogProbMetric: 31.4788, val_loss: 33.3427, val_MinusLogProbMetric: 33.3427

Epoch 312: val_loss did not improve from 31.27796
196/196 - 29s - loss: 31.4788 - MinusLogProbMetric: 31.4788 - val_loss: 33.3427 - val_MinusLogProbMetric: 33.3427 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 313/1000
2023-10-25 19:01:47.146 
Epoch 313/1000 
	 loss: 31.6567, MinusLogProbMetric: 31.6567, val_loss: 32.2873, val_MinusLogProbMetric: 32.2873

Epoch 313: val_loss did not improve from 31.27796
196/196 - 31s - loss: 31.6567 - MinusLogProbMetric: 31.6567 - val_loss: 32.2873 - val_MinusLogProbMetric: 32.2873 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 314/1000
2023-10-25 19:02:17.790 
Epoch 314/1000 
	 loss: 31.7573, MinusLogProbMetric: 31.7573, val_loss: 32.1706, val_MinusLogProbMetric: 32.1706

Epoch 314: val_loss did not improve from 31.27796
196/196 - 31s - loss: 31.7573 - MinusLogProbMetric: 31.7573 - val_loss: 32.1706 - val_MinusLogProbMetric: 32.1706 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 315/1000
2023-10-25 19:02:49.967 
Epoch 315/1000 
	 loss: 31.6412, MinusLogProbMetric: 31.6412, val_loss: 32.6504, val_MinusLogProbMetric: 32.6504

Epoch 315: val_loss did not improve from 31.27796
196/196 - 32s - loss: 31.6412 - MinusLogProbMetric: 31.6412 - val_loss: 32.6504 - val_MinusLogProbMetric: 32.6504 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 316/1000
2023-10-25 19:03:23.547 
Epoch 316/1000 
	 loss: 31.4348, MinusLogProbMetric: 31.4348, val_loss: 31.0776, val_MinusLogProbMetric: 31.0776

Epoch 316: val_loss improved from 31.27796 to 31.07760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 31.4348 - MinusLogProbMetric: 31.4348 - val_loss: 31.0776 - val_MinusLogProbMetric: 31.0776 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 317/1000
2023-10-25 19:03:55.596 
Epoch 317/1000 
	 loss: 31.5243, MinusLogProbMetric: 31.5243, val_loss: 31.1498, val_MinusLogProbMetric: 31.1498

Epoch 317: val_loss did not improve from 31.07760
196/196 - 32s - loss: 31.5243 - MinusLogProbMetric: 31.5243 - val_loss: 31.1498 - val_MinusLogProbMetric: 31.1498 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 318/1000
2023-10-25 19:04:25.433 
Epoch 318/1000 
	 loss: 31.8555, MinusLogProbMetric: 31.8555, val_loss: 31.4109, val_MinusLogProbMetric: 31.4109

Epoch 318: val_loss did not improve from 31.07760
196/196 - 30s - loss: 31.8555 - MinusLogProbMetric: 31.8555 - val_loss: 31.4109 - val_MinusLogProbMetric: 31.4109 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 319/1000
2023-10-25 19:04:53.946 
Epoch 319/1000 
	 loss: 31.4087, MinusLogProbMetric: 31.4087, val_loss: 31.4667, val_MinusLogProbMetric: 31.4667

Epoch 319: val_loss did not improve from 31.07760
196/196 - 29s - loss: 31.4087 - MinusLogProbMetric: 31.4087 - val_loss: 31.4667 - val_MinusLogProbMetric: 31.4667 - lr: 3.3333e-04 - 29s/epoch - 145ms/step
Epoch 320/1000
2023-10-25 19:05:22.006 
Epoch 320/1000 
	 loss: 31.5604, MinusLogProbMetric: 31.5604, val_loss: 32.3462, val_MinusLogProbMetric: 32.3462

Epoch 320: val_loss did not improve from 31.07760
196/196 - 28s - loss: 31.5604 - MinusLogProbMetric: 31.5604 - val_loss: 32.3462 - val_MinusLogProbMetric: 32.3462 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 321/1000
2023-10-25 19:05:53.393 
Epoch 321/1000 
	 loss: 31.6247, MinusLogProbMetric: 31.6247, val_loss: 32.2537, val_MinusLogProbMetric: 32.2537

Epoch 321: val_loss did not improve from 31.07760
196/196 - 31s - loss: 31.6247 - MinusLogProbMetric: 31.6247 - val_loss: 32.2537 - val_MinusLogProbMetric: 32.2537 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 322/1000
2023-10-25 19:06:26.963 
Epoch 322/1000 
	 loss: 31.6382, MinusLogProbMetric: 31.6382, val_loss: 31.6690, val_MinusLogProbMetric: 31.6690

Epoch 322: val_loss did not improve from 31.07760
196/196 - 34s - loss: 31.6382 - MinusLogProbMetric: 31.6382 - val_loss: 31.6690 - val_MinusLogProbMetric: 31.6690 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 323/1000
2023-10-25 19:06:59.552 
Epoch 323/1000 
	 loss: 31.5215, MinusLogProbMetric: 31.5215, val_loss: 32.1592, val_MinusLogProbMetric: 32.1592

Epoch 323: val_loss did not improve from 31.07760
196/196 - 33s - loss: 31.5215 - MinusLogProbMetric: 31.5215 - val_loss: 32.1592 - val_MinusLogProbMetric: 32.1592 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 324/1000
2023-10-25 19:07:28.792 
Epoch 324/1000 
	 loss: 31.3663, MinusLogProbMetric: 31.3663, val_loss: 32.1463, val_MinusLogProbMetric: 32.1463

Epoch 324: val_loss did not improve from 31.07760
196/196 - 29s - loss: 31.3663 - MinusLogProbMetric: 31.3663 - val_loss: 32.1463 - val_MinusLogProbMetric: 32.1463 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 325/1000
2023-10-25 19:08:02.557 
Epoch 325/1000 
	 loss: 32.2389, MinusLogProbMetric: 32.2389, val_loss: 31.2669, val_MinusLogProbMetric: 31.2669

Epoch 325: val_loss did not improve from 31.07760
196/196 - 34s - loss: 32.2389 - MinusLogProbMetric: 32.2389 - val_loss: 31.2669 - val_MinusLogProbMetric: 31.2669 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 326/1000
2023-10-25 19:08:35.688 
Epoch 326/1000 
	 loss: 31.9947, MinusLogProbMetric: 31.9947, val_loss: 31.2406, val_MinusLogProbMetric: 31.2406

Epoch 326: val_loss did not improve from 31.07760
196/196 - 33s - loss: 31.9947 - MinusLogProbMetric: 31.9947 - val_loss: 31.2406 - val_MinusLogProbMetric: 31.2406 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 327/1000
2023-10-25 19:09:07.647 
Epoch 327/1000 
	 loss: 31.4580, MinusLogProbMetric: 31.4580, val_loss: 31.1811, val_MinusLogProbMetric: 31.1811

Epoch 327: val_loss did not improve from 31.07760
196/196 - 32s - loss: 31.4580 - MinusLogProbMetric: 31.4580 - val_loss: 31.1811 - val_MinusLogProbMetric: 31.1811 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 328/1000
2023-10-25 19:09:37.048 
Epoch 328/1000 
	 loss: 31.3530, MinusLogProbMetric: 31.3530, val_loss: 31.5566, val_MinusLogProbMetric: 31.5566

Epoch 328: val_loss did not improve from 31.07760
196/196 - 29s - loss: 31.3530 - MinusLogProbMetric: 31.3530 - val_loss: 31.5566 - val_MinusLogProbMetric: 31.5566 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 329/1000
2023-10-25 19:10:09.442 
Epoch 329/1000 
	 loss: 31.5928, MinusLogProbMetric: 31.5928, val_loss: 31.0549, val_MinusLogProbMetric: 31.0549

Epoch 329: val_loss improved from 31.07760 to 31.05493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 33s - loss: 31.5928 - MinusLogProbMetric: 31.5928 - val_loss: 31.0549 - val_MinusLogProbMetric: 31.0549 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 330/1000
2023-10-25 19:10:41.526 
Epoch 330/1000 
	 loss: 31.1763, MinusLogProbMetric: 31.1763, val_loss: 32.1303, val_MinusLogProbMetric: 32.1303

Epoch 330: val_loss did not improve from 31.05493
196/196 - 32s - loss: 31.1763 - MinusLogProbMetric: 31.1763 - val_loss: 32.1303 - val_MinusLogProbMetric: 32.1303 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 331/1000
2023-10-25 19:11:10.376 
Epoch 331/1000 
	 loss: 31.6261, MinusLogProbMetric: 31.6261, val_loss: 31.2317, val_MinusLogProbMetric: 31.2317

Epoch 331: val_loss did not improve from 31.05493
196/196 - 29s - loss: 31.6261 - MinusLogProbMetric: 31.6261 - val_loss: 31.2317 - val_MinusLogProbMetric: 31.2317 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 332/1000
2023-10-25 19:11:42.586 
Epoch 332/1000 
	 loss: 31.6230, MinusLogProbMetric: 31.6230, val_loss: 31.3506, val_MinusLogProbMetric: 31.3506

Epoch 332: val_loss did not improve from 31.05493
196/196 - 32s - loss: 31.6230 - MinusLogProbMetric: 31.6230 - val_loss: 31.3506 - val_MinusLogProbMetric: 31.3506 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 333/1000
2023-10-25 19:12:11.162 
Epoch 333/1000 
	 loss: 31.4274, MinusLogProbMetric: 31.4274, val_loss: 33.5310, val_MinusLogProbMetric: 33.5310

Epoch 333: val_loss did not improve from 31.05493
196/196 - 29s - loss: 31.4274 - MinusLogProbMetric: 31.4274 - val_loss: 33.5310 - val_MinusLogProbMetric: 33.5310 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 334/1000
2023-10-25 19:12:38.678 
Epoch 334/1000 
	 loss: 31.7773, MinusLogProbMetric: 31.7773, val_loss: 31.6725, val_MinusLogProbMetric: 31.6725

Epoch 334: val_loss did not improve from 31.05493
196/196 - 28s - loss: 31.7773 - MinusLogProbMetric: 31.7773 - val_loss: 31.6725 - val_MinusLogProbMetric: 31.6725 - lr: 3.3333e-04 - 28s/epoch - 140ms/step
Epoch 335/1000
2023-10-25 19:13:10.093 
Epoch 335/1000 
	 loss: 31.4670, MinusLogProbMetric: 31.4670, val_loss: 31.1087, val_MinusLogProbMetric: 31.1087

Epoch 335: val_loss did not improve from 31.05493
196/196 - 31s - loss: 31.4670 - MinusLogProbMetric: 31.4670 - val_loss: 31.1087 - val_MinusLogProbMetric: 31.1087 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 336/1000
2023-10-25 19:13:43.244 
Epoch 336/1000 
	 loss: 31.2502, MinusLogProbMetric: 31.2502, val_loss: 31.1704, val_MinusLogProbMetric: 31.1704

Epoch 336: val_loss did not improve from 31.05493
196/196 - 33s - loss: 31.2502 - MinusLogProbMetric: 31.2502 - val_loss: 31.1704 - val_MinusLogProbMetric: 31.1704 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 337/1000
2023-10-25 19:14:14.571 
Epoch 337/1000 
	 loss: 31.4411, MinusLogProbMetric: 31.4411, val_loss: 32.0264, val_MinusLogProbMetric: 32.0264

Epoch 337: val_loss did not improve from 31.05493
196/196 - 31s - loss: 31.4411 - MinusLogProbMetric: 31.4411 - val_loss: 32.0264 - val_MinusLogProbMetric: 32.0264 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 338/1000
2023-10-25 19:14:47.698 
Epoch 338/1000 
	 loss: 32.7093, MinusLogProbMetric: 32.7093, val_loss: 33.6404, val_MinusLogProbMetric: 33.6404

Epoch 338: val_loss did not improve from 31.05493
196/196 - 33s - loss: 32.7093 - MinusLogProbMetric: 32.7093 - val_loss: 33.6404 - val_MinusLogProbMetric: 33.6404 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 339/1000
2023-10-25 19:15:22.171 
Epoch 339/1000 
	 loss: 32.0179, MinusLogProbMetric: 32.0179, val_loss: 31.1903, val_MinusLogProbMetric: 31.1903

Epoch 339: val_loss did not improve from 31.05493
196/196 - 34s - loss: 32.0179 - MinusLogProbMetric: 32.0179 - val_loss: 31.1903 - val_MinusLogProbMetric: 31.1903 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 340/1000
2023-10-25 19:15:52.656 
Epoch 340/1000 
	 loss: 31.3270, MinusLogProbMetric: 31.3270, val_loss: 31.5760, val_MinusLogProbMetric: 31.5760

Epoch 340: val_loss did not improve from 31.05493
196/196 - 30s - loss: 31.3270 - MinusLogProbMetric: 31.3270 - val_loss: 31.5760 - val_MinusLogProbMetric: 31.5760 - lr: 3.3333e-04 - 30s/epoch - 156ms/step
Epoch 341/1000
2023-10-25 19:16:25.282 
Epoch 341/1000 
	 loss: 31.3221, MinusLogProbMetric: 31.3221, val_loss: 31.5414, val_MinusLogProbMetric: 31.5414

Epoch 341: val_loss did not improve from 31.05493
196/196 - 33s - loss: 31.3221 - MinusLogProbMetric: 31.3221 - val_loss: 31.5414 - val_MinusLogProbMetric: 31.5414 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 342/1000
2023-10-25 19:17:00.038 
Epoch 342/1000 
	 loss: 31.2265, MinusLogProbMetric: 31.2265, val_loss: 31.2551, val_MinusLogProbMetric: 31.2551

Epoch 342: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.2265 - MinusLogProbMetric: 31.2265 - val_loss: 31.2551 - val_MinusLogProbMetric: 31.2551 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 343/1000
2023-10-25 19:17:34.741 
Epoch 343/1000 
	 loss: 31.1856, MinusLogProbMetric: 31.1856, val_loss: 31.0771, val_MinusLogProbMetric: 31.0771

Epoch 343: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.1856 - MinusLogProbMetric: 31.1856 - val_loss: 31.0771 - val_MinusLogProbMetric: 31.0771 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 344/1000
2023-10-25 19:18:09.500 
Epoch 344/1000 
	 loss: 31.7532, MinusLogProbMetric: 31.7532, val_loss: 31.7736, val_MinusLogProbMetric: 31.7736

Epoch 344: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.7532 - MinusLogProbMetric: 31.7532 - val_loss: 31.7736 - val_MinusLogProbMetric: 31.7736 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 345/1000
2023-10-25 19:18:44.120 
Epoch 345/1000 
	 loss: 31.1761, MinusLogProbMetric: 31.1761, val_loss: 31.4923, val_MinusLogProbMetric: 31.4923

Epoch 345: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.1761 - MinusLogProbMetric: 31.1761 - val_loss: 31.4923 - val_MinusLogProbMetric: 31.4923 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 346/1000
2023-10-25 19:19:18.234 
Epoch 346/1000 
	 loss: 31.1427, MinusLogProbMetric: 31.1427, val_loss: 32.7951, val_MinusLogProbMetric: 32.7951

Epoch 346: val_loss did not improve from 31.05493
196/196 - 34s - loss: 31.1427 - MinusLogProbMetric: 31.1427 - val_loss: 32.7951 - val_MinusLogProbMetric: 32.7951 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 347/1000
2023-10-25 19:19:52.832 
Epoch 347/1000 
	 loss: 31.2039, MinusLogProbMetric: 31.2039, val_loss: 31.2119, val_MinusLogProbMetric: 31.2119

Epoch 347: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.2039 - MinusLogProbMetric: 31.2039 - val_loss: 31.2119 - val_MinusLogProbMetric: 31.2119 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 348/1000
2023-10-25 19:20:27.310 
Epoch 348/1000 
	 loss: 31.4246, MinusLogProbMetric: 31.4246, val_loss: 32.2064, val_MinusLogProbMetric: 32.2064

Epoch 348: val_loss did not improve from 31.05493
196/196 - 34s - loss: 31.4246 - MinusLogProbMetric: 31.4246 - val_loss: 32.2064 - val_MinusLogProbMetric: 32.2064 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 349/1000
2023-10-25 19:21:01.660 
Epoch 349/1000 
	 loss: 31.5288, MinusLogProbMetric: 31.5288, val_loss: 32.1313, val_MinusLogProbMetric: 32.1313

Epoch 349: val_loss did not improve from 31.05493
196/196 - 34s - loss: 31.5288 - MinusLogProbMetric: 31.5288 - val_loss: 32.1313 - val_MinusLogProbMetric: 32.1313 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 350/1000
2023-10-25 19:21:36.339 
Epoch 350/1000 
	 loss: 31.0355, MinusLogProbMetric: 31.0355, val_loss: 31.4309, val_MinusLogProbMetric: 31.4309

Epoch 350: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.0355 - MinusLogProbMetric: 31.0355 - val_loss: 31.4309 - val_MinusLogProbMetric: 31.4309 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 351/1000
2023-10-25 19:22:10.905 
Epoch 351/1000 
	 loss: 31.4432, MinusLogProbMetric: 31.4432, val_loss: 31.1999, val_MinusLogProbMetric: 31.1999

Epoch 351: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.4432 - MinusLogProbMetric: 31.4432 - val_loss: 31.1999 - val_MinusLogProbMetric: 31.1999 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 352/1000
2023-10-25 19:22:45.402 
Epoch 352/1000 
	 loss: 30.8633, MinusLogProbMetric: 30.8633, val_loss: 31.2448, val_MinusLogProbMetric: 31.2448

Epoch 352: val_loss did not improve from 31.05493
196/196 - 34s - loss: 30.8633 - MinusLogProbMetric: 30.8633 - val_loss: 31.2448 - val_MinusLogProbMetric: 31.2448 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 353/1000
2023-10-25 19:23:19.997 
Epoch 353/1000 
	 loss: 31.0907, MinusLogProbMetric: 31.0907, val_loss: 31.2159, val_MinusLogProbMetric: 31.2159

Epoch 353: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.0907 - MinusLogProbMetric: 31.0907 - val_loss: 31.2159 - val_MinusLogProbMetric: 31.2159 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 354/1000
2023-10-25 19:23:54.727 
Epoch 354/1000 
	 loss: 31.2068, MinusLogProbMetric: 31.2068, val_loss: 31.1987, val_MinusLogProbMetric: 31.1987

Epoch 354: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.2068 - MinusLogProbMetric: 31.2068 - val_loss: 31.1987 - val_MinusLogProbMetric: 31.1987 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 355/1000
2023-10-25 19:24:29.387 
Epoch 355/1000 
	 loss: 31.5704, MinusLogProbMetric: 31.5704, val_loss: 33.5378, val_MinusLogProbMetric: 33.5378

Epoch 355: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.5704 - MinusLogProbMetric: 31.5704 - val_loss: 33.5378 - val_MinusLogProbMetric: 33.5378 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 356/1000
2023-10-25 19:25:04.060 
Epoch 356/1000 
	 loss: 31.0810, MinusLogProbMetric: 31.0810, val_loss: 31.0842, val_MinusLogProbMetric: 31.0842

Epoch 356: val_loss did not improve from 31.05493
196/196 - 35s - loss: 31.0810 - MinusLogProbMetric: 31.0810 - val_loss: 31.0842 - val_MinusLogProbMetric: 31.0842 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 357/1000
2023-10-25 19:25:38.588 
Epoch 357/1000 
	 loss: 31.2430, MinusLogProbMetric: 31.2430, val_loss: 31.0485, val_MinusLogProbMetric: 31.0485

Epoch 357: val_loss improved from 31.05493 to 31.04848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 31.2430 - MinusLogProbMetric: 31.2430 - val_loss: 31.0485 - val_MinusLogProbMetric: 31.0485 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 358/1000
2023-10-25 19:26:13.836 
Epoch 358/1000 
	 loss: 31.3320, MinusLogProbMetric: 31.3320, val_loss: 31.2854, val_MinusLogProbMetric: 31.2854

Epoch 358: val_loss did not improve from 31.04848
196/196 - 35s - loss: 31.3320 - MinusLogProbMetric: 31.3320 - val_loss: 31.2854 - val_MinusLogProbMetric: 31.2854 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 359/1000
2023-10-25 19:26:48.239 
Epoch 359/1000 
	 loss: 31.5070, MinusLogProbMetric: 31.5070, val_loss: 31.3039, val_MinusLogProbMetric: 31.3039

Epoch 359: val_loss did not improve from 31.04848
196/196 - 34s - loss: 31.5070 - MinusLogProbMetric: 31.5070 - val_loss: 31.3039 - val_MinusLogProbMetric: 31.3039 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 360/1000
2023-10-25 19:27:23.021 
Epoch 360/1000 
	 loss: 31.1941, MinusLogProbMetric: 31.1941, val_loss: 31.0405, val_MinusLogProbMetric: 31.0405

Epoch 360: val_loss improved from 31.04848 to 31.04054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 31.1941 - MinusLogProbMetric: 31.1941 - val_loss: 31.0405 - val_MinusLogProbMetric: 31.0405 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 361/1000
2023-10-25 19:27:58.350 
Epoch 361/1000 
	 loss: 31.0601, MinusLogProbMetric: 31.0601, val_loss: 31.1308, val_MinusLogProbMetric: 31.1308

Epoch 361: val_loss did not improve from 31.04054
196/196 - 35s - loss: 31.0601 - MinusLogProbMetric: 31.0601 - val_loss: 31.1308 - val_MinusLogProbMetric: 31.1308 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 362/1000
2023-10-25 19:28:32.614 
Epoch 362/1000 
	 loss: 30.9809, MinusLogProbMetric: 30.9809, val_loss: 31.3429, val_MinusLogProbMetric: 31.3429

Epoch 362: val_loss did not improve from 31.04054
196/196 - 34s - loss: 30.9809 - MinusLogProbMetric: 30.9809 - val_loss: 31.3429 - val_MinusLogProbMetric: 31.3429 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 363/1000
2023-10-25 19:29:07.242 
Epoch 363/1000 
	 loss: 30.9306, MinusLogProbMetric: 30.9306, val_loss: 31.6428, val_MinusLogProbMetric: 31.6428

Epoch 363: val_loss did not improve from 31.04054
196/196 - 35s - loss: 30.9306 - MinusLogProbMetric: 30.9306 - val_loss: 31.6428 - val_MinusLogProbMetric: 31.6428 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 364/1000
2023-10-25 19:29:41.915 
Epoch 364/1000 
	 loss: 31.2481, MinusLogProbMetric: 31.2481, val_loss: 31.8534, val_MinusLogProbMetric: 31.8534

Epoch 364: val_loss did not improve from 31.04054
196/196 - 35s - loss: 31.2481 - MinusLogProbMetric: 31.2481 - val_loss: 31.8534 - val_MinusLogProbMetric: 31.8534 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 365/1000
2023-10-25 19:30:16.686 
Epoch 365/1000 
	 loss: 31.2349, MinusLogProbMetric: 31.2349, val_loss: 31.8276, val_MinusLogProbMetric: 31.8276

Epoch 365: val_loss did not improve from 31.04054
196/196 - 35s - loss: 31.2349 - MinusLogProbMetric: 31.2349 - val_loss: 31.8276 - val_MinusLogProbMetric: 31.8276 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 366/1000
2023-10-25 19:30:51.389 
Epoch 366/1000 
	 loss: 31.4254, MinusLogProbMetric: 31.4254, val_loss: 31.3136, val_MinusLogProbMetric: 31.3136

Epoch 366: val_loss did not improve from 31.04054
196/196 - 35s - loss: 31.4254 - MinusLogProbMetric: 31.4254 - val_loss: 31.3136 - val_MinusLogProbMetric: 31.3136 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 367/1000
2023-10-25 19:31:25.808 
Epoch 367/1000 
	 loss: 31.0153, MinusLogProbMetric: 31.0153, val_loss: 30.8627, val_MinusLogProbMetric: 30.8627

Epoch 367: val_loss improved from 31.04054 to 30.86269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 31.0153 - MinusLogProbMetric: 31.0153 - val_loss: 30.8627 - val_MinusLogProbMetric: 30.8627 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 368/1000
2023-10-25 19:32:01.205 
Epoch 368/1000 
	 loss: 31.5262, MinusLogProbMetric: 31.5262, val_loss: 31.1933, val_MinusLogProbMetric: 31.1933

Epoch 368: val_loss did not improve from 30.86269
196/196 - 35s - loss: 31.5262 - MinusLogProbMetric: 31.5262 - val_loss: 31.1933 - val_MinusLogProbMetric: 31.1933 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 369/1000
2023-10-25 19:32:35.929 
Epoch 369/1000 
	 loss: 31.0719, MinusLogProbMetric: 31.0719, val_loss: 31.6302, val_MinusLogProbMetric: 31.6302

Epoch 369: val_loss did not improve from 30.86269
196/196 - 35s - loss: 31.0719 - MinusLogProbMetric: 31.0719 - val_loss: 31.6302 - val_MinusLogProbMetric: 31.6302 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 370/1000
2023-10-25 19:33:10.550 
Epoch 370/1000 
	 loss: 31.4188, MinusLogProbMetric: 31.4188, val_loss: 31.3971, val_MinusLogProbMetric: 31.3971

Epoch 370: val_loss did not improve from 30.86269
196/196 - 35s - loss: 31.4188 - MinusLogProbMetric: 31.4188 - val_loss: 31.3971 - val_MinusLogProbMetric: 31.3971 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 371/1000
2023-10-25 19:33:45.149 
Epoch 371/1000 
	 loss: 31.0248, MinusLogProbMetric: 31.0248, val_loss: 31.0897, val_MinusLogProbMetric: 31.0897

Epoch 371: val_loss did not improve from 30.86269
196/196 - 35s - loss: 31.0248 - MinusLogProbMetric: 31.0248 - val_loss: 31.0897 - val_MinusLogProbMetric: 31.0897 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 372/1000
2023-10-25 19:34:20.014 
Epoch 372/1000 
	 loss: 30.9601, MinusLogProbMetric: 30.9601, val_loss: 31.7480, val_MinusLogProbMetric: 31.7480

Epoch 372: val_loss did not improve from 30.86269
196/196 - 35s - loss: 30.9601 - MinusLogProbMetric: 30.9601 - val_loss: 31.7480 - val_MinusLogProbMetric: 31.7480 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 373/1000
2023-10-25 19:34:54.887 
Epoch 373/1000 
	 loss: 30.9768, MinusLogProbMetric: 30.9768, val_loss: 30.9280, val_MinusLogProbMetric: 30.9280

Epoch 373: val_loss did not improve from 30.86269
196/196 - 35s - loss: 30.9768 - MinusLogProbMetric: 30.9768 - val_loss: 30.9280 - val_MinusLogProbMetric: 30.9280 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 374/1000
2023-10-25 19:35:29.553 
Epoch 374/1000 
	 loss: 31.2851, MinusLogProbMetric: 31.2851, val_loss: 30.8117, val_MinusLogProbMetric: 30.8117

Epoch 374: val_loss improved from 30.86269 to 30.81169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 31.2851 - MinusLogProbMetric: 31.2851 - val_loss: 30.8117 - val_MinusLogProbMetric: 30.8117 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 375/1000
2023-10-25 19:36:04.901 
Epoch 375/1000 
	 loss: 31.1018, MinusLogProbMetric: 31.1018, val_loss: 31.2722, val_MinusLogProbMetric: 31.2722

Epoch 375: val_loss did not improve from 30.81169
196/196 - 35s - loss: 31.1018 - MinusLogProbMetric: 31.1018 - val_loss: 31.2722 - val_MinusLogProbMetric: 31.2722 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 376/1000
2023-10-25 19:36:39.286 
Epoch 376/1000 
	 loss: 31.0458, MinusLogProbMetric: 31.0458, val_loss: 31.5858, val_MinusLogProbMetric: 31.5858

Epoch 376: val_loss did not improve from 30.81169
196/196 - 34s - loss: 31.0458 - MinusLogProbMetric: 31.0458 - val_loss: 31.5858 - val_MinusLogProbMetric: 31.5858 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 377/1000
2023-10-25 19:37:14.037 
Epoch 377/1000 
	 loss: 30.8333, MinusLogProbMetric: 30.8333, val_loss: 31.5530, val_MinusLogProbMetric: 31.5530

Epoch 377: val_loss did not improve from 30.81169
196/196 - 35s - loss: 30.8333 - MinusLogProbMetric: 30.8333 - val_loss: 31.5530 - val_MinusLogProbMetric: 31.5530 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 378/1000
2023-10-25 19:37:48.869 
Epoch 378/1000 
	 loss: 32.9560, MinusLogProbMetric: 32.9560, val_loss: 32.2769, val_MinusLogProbMetric: 32.2769

Epoch 378: val_loss did not improve from 30.81169
196/196 - 35s - loss: 32.9560 - MinusLogProbMetric: 32.9560 - val_loss: 32.2769 - val_MinusLogProbMetric: 32.2769 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 379/1000
2023-10-25 19:38:23.839 
Epoch 379/1000 
	 loss: 31.9314, MinusLogProbMetric: 31.9314, val_loss: 31.7516, val_MinusLogProbMetric: 31.7516

Epoch 379: val_loss did not improve from 30.81169
196/196 - 35s - loss: 31.9314 - MinusLogProbMetric: 31.9314 - val_loss: 31.7516 - val_MinusLogProbMetric: 31.7516 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 380/1000
2023-10-25 19:38:58.633 
Epoch 380/1000 
	 loss: 31.8365, MinusLogProbMetric: 31.8365, val_loss: 31.9507, val_MinusLogProbMetric: 31.9507

Epoch 380: val_loss did not improve from 30.81169
196/196 - 35s - loss: 31.8365 - MinusLogProbMetric: 31.8365 - val_loss: 31.9507 - val_MinusLogProbMetric: 31.9507 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 381/1000
2023-10-25 19:39:33.497 
Epoch 381/1000 
	 loss: 31.6605, MinusLogProbMetric: 31.6605, val_loss: 31.6218, val_MinusLogProbMetric: 31.6218

Epoch 381: val_loss did not improve from 30.81169
196/196 - 35s - loss: 31.6605 - MinusLogProbMetric: 31.6605 - val_loss: 31.6218 - val_MinusLogProbMetric: 31.6218 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 382/1000
2023-10-25 19:40:08.110 
Epoch 382/1000 
	 loss: 31.9373, MinusLogProbMetric: 31.9373, val_loss: 31.6890, val_MinusLogProbMetric: 31.6890

Epoch 382: val_loss did not improve from 30.81169
196/196 - 35s - loss: 31.9373 - MinusLogProbMetric: 31.9373 - val_loss: 31.6890 - val_MinusLogProbMetric: 31.6890 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 383/1000
2023-10-25 19:40:42.400 
Epoch 383/1000 
	 loss: 31.4633, MinusLogProbMetric: 31.4633, val_loss: 31.1826, val_MinusLogProbMetric: 31.1826

Epoch 383: val_loss did not improve from 30.81169
196/196 - 34s - loss: 31.4633 - MinusLogProbMetric: 31.4633 - val_loss: 31.1826 - val_MinusLogProbMetric: 31.1826 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 384/1000
2023-10-25 19:41:12.331 
Epoch 384/1000 
	 loss: 31.1860, MinusLogProbMetric: 31.1860, val_loss: 30.7866, val_MinusLogProbMetric: 30.7866

Epoch 384: val_loss improved from 30.81169 to 30.78655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 30s - loss: 31.1860 - MinusLogProbMetric: 31.1860 - val_loss: 30.7866 - val_MinusLogProbMetric: 30.7866 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 385/1000
2023-10-25 19:41:46.915 
Epoch 385/1000 
	 loss: 31.0174, MinusLogProbMetric: 31.0174, val_loss: 30.7049, val_MinusLogProbMetric: 30.7049

Epoch 385: val_loss improved from 30.78655 to 30.70487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 31.0174 - MinusLogProbMetric: 31.0174 - val_loss: 30.7049 - val_MinusLogProbMetric: 30.7049 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 386/1000
2023-10-25 19:42:20.533 
Epoch 386/1000 
	 loss: 31.1512, MinusLogProbMetric: 31.1512, val_loss: 30.8609, val_MinusLogProbMetric: 30.8609

Epoch 386: val_loss did not improve from 30.70487
196/196 - 33s - loss: 31.1512 - MinusLogProbMetric: 31.1512 - val_loss: 30.8609 - val_MinusLogProbMetric: 30.8609 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 387/1000
2023-10-25 19:42:52.791 
Epoch 387/1000 
	 loss: 31.3593, MinusLogProbMetric: 31.3593, val_loss: 31.3659, val_MinusLogProbMetric: 31.3659

Epoch 387: val_loss did not improve from 30.70487
196/196 - 32s - loss: 31.3593 - MinusLogProbMetric: 31.3593 - val_loss: 31.3659 - val_MinusLogProbMetric: 31.3659 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 388/1000
2023-10-25 19:43:27.775 
Epoch 388/1000 
	 loss: 30.7692, MinusLogProbMetric: 30.7692, val_loss: 30.8320, val_MinusLogProbMetric: 30.8320

Epoch 388: val_loss did not improve from 30.70487
196/196 - 35s - loss: 30.7692 - MinusLogProbMetric: 30.7692 - val_loss: 30.8320 - val_MinusLogProbMetric: 30.8320 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 389/1000
2023-10-25 19:44:02.628 
Epoch 389/1000 
	 loss: 30.8046, MinusLogProbMetric: 30.8046, val_loss: 31.4289, val_MinusLogProbMetric: 31.4289

Epoch 389: val_loss did not improve from 30.70487
196/196 - 35s - loss: 30.8046 - MinusLogProbMetric: 30.8046 - val_loss: 31.4289 - val_MinusLogProbMetric: 31.4289 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 390/1000
2023-10-25 19:44:37.301 
Epoch 390/1000 
	 loss: 30.8953, MinusLogProbMetric: 30.8953, val_loss: 30.6631, val_MinusLogProbMetric: 30.6631

Epoch 390: val_loss improved from 30.70487 to 30.66314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.8953 - MinusLogProbMetric: 30.8953 - val_loss: 30.6631 - val_MinusLogProbMetric: 30.6631 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 391/1000
2023-10-25 19:45:11.020 
Epoch 391/1000 
	 loss: 31.1194, MinusLogProbMetric: 31.1194, val_loss: 30.9698, val_MinusLogProbMetric: 30.9698

Epoch 391: val_loss did not improve from 30.66314
196/196 - 33s - loss: 31.1194 - MinusLogProbMetric: 31.1194 - val_loss: 30.9698 - val_MinusLogProbMetric: 30.9698 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 392/1000
2023-10-25 19:45:45.195 
Epoch 392/1000 
	 loss: 31.0541, MinusLogProbMetric: 31.0541, val_loss: 31.3752, val_MinusLogProbMetric: 31.3752

Epoch 392: val_loss did not improve from 30.66314
196/196 - 34s - loss: 31.0541 - MinusLogProbMetric: 31.0541 - val_loss: 31.3752 - val_MinusLogProbMetric: 31.3752 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 393/1000
2023-10-25 19:46:19.926 
Epoch 393/1000 
	 loss: 31.2440, MinusLogProbMetric: 31.2440, val_loss: 31.7739, val_MinusLogProbMetric: 31.7739

Epoch 393: val_loss did not improve from 30.66314
196/196 - 35s - loss: 31.2440 - MinusLogProbMetric: 31.2440 - val_loss: 31.7739 - val_MinusLogProbMetric: 31.7739 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 394/1000
2023-10-25 19:46:54.544 
Epoch 394/1000 
	 loss: 30.7647, MinusLogProbMetric: 30.7647, val_loss: 30.8277, val_MinusLogProbMetric: 30.8277

Epoch 394: val_loss did not improve from 30.66314
196/196 - 35s - loss: 30.7647 - MinusLogProbMetric: 30.7647 - val_loss: 30.8277 - val_MinusLogProbMetric: 30.8277 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 395/1000
2023-10-25 19:47:29.256 
Epoch 395/1000 
	 loss: 30.9400, MinusLogProbMetric: 30.9400, val_loss: 30.9927, val_MinusLogProbMetric: 30.9927

Epoch 395: val_loss did not improve from 30.66314
196/196 - 35s - loss: 30.9400 - MinusLogProbMetric: 30.9400 - val_loss: 30.9927 - val_MinusLogProbMetric: 30.9927 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 396/1000
2023-10-25 19:48:04.410 
Epoch 396/1000 
	 loss: 31.0685, MinusLogProbMetric: 31.0685, val_loss: 31.0653, val_MinusLogProbMetric: 31.0653

Epoch 396: val_loss did not improve from 30.66314
196/196 - 35s - loss: 31.0685 - MinusLogProbMetric: 31.0685 - val_loss: 31.0653 - val_MinusLogProbMetric: 31.0653 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 397/1000
2023-10-25 19:48:39.011 
Epoch 397/1000 
	 loss: 31.1089, MinusLogProbMetric: 31.1089, val_loss: 33.4250, val_MinusLogProbMetric: 33.4250

Epoch 397: val_loss did not improve from 30.66314
196/196 - 35s - loss: 31.1089 - MinusLogProbMetric: 31.1089 - val_loss: 33.4250 - val_MinusLogProbMetric: 33.4250 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 398/1000
2023-10-25 19:49:13.774 
Epoch 398/1000 
	 loss: 31.1393, MinusLogProbMetric: 31.1393, val_loss: 31.3991, val_MinusLogProbMetric: 31.3991

Epoch 398: val_loss did not improve from 30.66314
196/196 - 35s - loss: 31.1393 - MinusLogProbMetric: 31.1393 - val_loss: 31.3991 - val_MinusLogProbMetric: 31.3991 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 399/1000
2023-10-25 19:49:48.607 
Epoch 399/1000 
	 loss: 30.8911, MinusLogProbMetric: 30.8911, val_loss: 32.0018, val_MinusLogProbMetric: 32.0018

Epoch 399: val_loss did not improve from 30.66314
196/196 - 35s - loss: 30.8911 - MinusLogProbMetric: 30.8911 - val_loss: 32.0018 - val_MinusLogProbMetric: 32.0018 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 400/1000
2023-10-25 19:50:23.511 
Epoch 400/1000 
	 loss: 31.0973, MinusLogProbMetric: 31.0973, val_loss: 30.8397, val_MinusLogProbMetric: 30.8397

Epoch 400: val_loss did not improve from 30.66314
196/196 - 35s - loss: 31.0973 - MinusLogProbMetric: 31.0973 - val_loss: 30.8397 - val_MinusLogProbMetric: 30.8397 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 401/1000
2023-10-25 19:50:58.419 
Epoch 401/1000 
	 loss: 30.7906, MinusLogProbMetric: 30.7906, val_loss: 32.7746, val_MinusLogProbMetric: 32.7746

Epoch 401: val_loss did not improve from 30.66314
196/196 - 35s - loss: 30.7906 - MinusLogProbMetric: 30.7906 - val_loss: 32.7746 - val_MinusLogProbMetric: 32.7746 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 402/1000
2023-10-25 19:51:33.063 
Epoch 402/1000 
	 loss: 30.8100, MinusLogProbMetric: 30.8100, val_loss: 30.5564, val_MinusLogProbMetric: 30.5564

Epoch 402: val_loss improved from 30.66314 to 30.55643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.8100 - MinusLogProbMetric: 30.8100 - val_loss: 30.5564 - val_MinusLogProbMetric: 30.5564 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 403/1000
2023-10-25 19:52:08.273 
Epoch 403/1000 
	 loss: 30.7968, MinusLogProbMetric: 30.7968, val_loss: 30.7752, val_MinusLogProbMetric: 30.7752

Epoch 403: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.7968 - MinusLogProbMetric: 30.7968 - val_loss: 30.7752 - val_MinusLogProbMetric: 30.7752 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 404/1000
2023-10-25 19:52:43.354 
Epoch 404/1000 
	 loss: 30.8320, MinusLogProbMetric: 30.8320, val_loss: 30.6689, val_MinusLogProbMetric: 30.6689

Epoch 404: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.8320 - MinusLogProbMetric: 30.8320 - val_loss: 30.6689 - val_MinusLogProbMetric: 30.6689 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 405/1000
2023-10-25 19:53:18.378 
Epoch 405/1000 
	 loss: 30.9411, MinusLogProbMetric: 30.9411, val_loss: 31.5601, val_MinusLogProbMetric: 31.5601

Epoch 405: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.9411 - MinusLogProbMetric: 30.9411 - val_loss: 31.5601 - val_MinusLogProbMetric: 31.5601 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 406/1000
2023-10-25 19:53:52.992 
Epoch 406/1000 
	 loss: 30.8088, MinusLogProbMetric: 30.8088, val_loss: 30.9416, val_MinusLogProbMetric: 30.9416

Epoch 406: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.8088 - MinusLogProbMetric: 30.8088 - val_loss: 30.9416 - val_MinusLogProbMetric: 30.9416 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 407/1000
2023-10-25 19:54:27.803 
Epoch 407/1000 
	 loss: 30.9374, MinusLogProbMetric: 30.9374, val_loss: 31.1410, val_MinusLogProbMetric: 31.1410

Epoch 407: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.9374 - MinusLogProbMetric: 30.9374 - val_loss: 31.1410 - val_MinusLogProbMetric: 31.1410 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 408/1000
2023-10-25 19:55:02.709 
Epoch 408/1000 
	 loss: 30.8141, MinusLogProbMetric: 30.8141, val_loss: 30.7533, val_MinusLogProbMetric: 30.7533

Epoch 408: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.8141 - MinusLogProbMetric: 30.8141 - val_loss: 30.7533 - val_MinusLogProbMetric: 30.7533 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 409/1000
2023-10-25 19:55:37.415 
Epoch 409/1000 
	 loss: 30.8170, MinusLogProbMetric: 30.8170, val_loss: 32.4479, val_MinusLogProbMetric: 32.4479

Epoch 409: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.8170 - MinusLogProbMetric: 30.8170 - val_loss: 32.4479 - val_MinusLogProbMetric: 32.4479 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 410/1000
2023-10-25 19:56:12.199 
Epoch 410/1000 
	 loss: 30.9889, MinusLogProbMetric: 30.9889, val_loss: 33.3861, val_MinusLogProbMetric: 33.3861

Epoch 410: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.9889 - MinusLogProbMetric: 30.9889 - val_loss: 33.3861 - val_MinusLogProbMetric: 33.3861 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 411/1000
2023-10-25 19:56:47.001 
Epoch 411/1000 
	 loss: 32.0378, MinusLogProbMetric: 32.0378, val_loss: 31.0254, val_MinusLogProbMetric: 31.0254

Epoch 411: val_loss did not improve from 30.55643
196/196 - 35s - loss: 32.0378 - MinusLogProbMetric: 32.0378 - val_loss: 31.0254 - val_MinusLogProbMetric: 31.0254 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 412/1000
2023-10-25 19:57:21.841 
Epoch 412/1000 
	 loss: 30.8577, MinusLogProbMetric: 30.8577, val_loss: 30.8275, val_MinusLogProbMetric: 30.8275

Epoch 412: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.8577 - MinusLogProbMetric: 30.8577 - val_loss: 30.8275 - val_MinusLogProbMetric: 30.8275 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 413/1000
2023-10-25 19:57:56.668 
Epoch 413/1000 
	 loss: 30.6873, MinusLogProbMetric: 30.6873, val_loss: 31.1185, val_MinusLogProbMetric: 31.1185

Epoch 413: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.6873 - MinusLogProbMetric: 30.6873 - val_loss: 31.1185 - val_MinusLogProbMetric: 31.1185 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 414/1000
2023-10-25 19:58:31.620 
Epoch 414/1000 
	 loss: 30.8715, MinusLogProbMetric: 30.8715, val_loss: 31.0041, val_MinusLogProbMetric: 31.0041

Epoch 414: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.8715 - MinusLogProbMetric: 30.8715 - val_loss: 31.0041 - val_MinusLogProbMetric: 31.0041 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 415/1000
2023-10-25 19:59:06.664 
Epoch 415/1000 
	 loss: 30.9720, MinusLogProbMetric: 30.9720, val_loss: 30.9915, val_MinusLogProbMetric: 30.9915

Epoch 415: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.9720 - MinusLogProbMetric: 30.9720 - val_loss: 30.9915 - val_MinusLogProbMetric: 30.9915 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 416/1000
2023-10-25 19:59:41.527 
Epoch 416/1000 
	 loss: 30.8502, MinusLogProbMetric: 30.8502, val_loss: 31.1358, val_MinusLogProbMetric: 31.1358

Epoch 416: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.8502 - MinusLogProbMetric: 30.8502 - val_loss: 31.1358 - val_MinusLogProbMetric: 31.1358 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 417/1000
2023-10-25 20:00:16.141 
Epoch 417/1000 
	 loss: 30.6143, MinusLogProbMetric: 30.6143, val_loss: 30.6219, val_MinusLogProbMetric: 30.6219

Epoch 417: val_loss did not improve from 30.55643
196/196 - 35s - loss: 30.6143 - MinusLogProbMetric: 30.6143 - val_loss: 30.6219 - val_MinusLogProbMetric: 30.6219 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 418/1000
2023-10-25 20:00:50.880 
Epoch 418/1000 
	 loss: 30.9666, MinusLogProbMetric: 30.9666, val_loss: 30.4613, val_MinusLogProbMetric: 30.4613

Epoch 418: val_loss improved from 30.55643 to 30.46127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.9666 - MinusLogProbMetric: 30.9666 - val_loss: 30.4613 - val_MinusLogProbMetric: 30.4613 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 419/1000
2023-10-25 20:01:26.069 
Epoch 419/1000 
	 loss: 30.7924, MinusLogProbMetric: 30.7924, val_loss: 30.6264, val_MinusLogProbMetric: 30.6264

Epoch 419: val_loss did not improve from 30.46127
196/196 - 35s - loss: 30.7924 - MinusLogProbMetric: 30.7924 - val_loss: 30.6264 - val_MinusLogProbMetric: 30.6264 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 420/1000
2023-10-25 20:02:01.107 
Epoch 420/1000 
	 loss: 30.7310, MinusLogProbMetric: 30.7310, val_loss: 30.3930, val_MinusLogProbMetric: 30.3930

Epoch 420: val_loss improved from 30.46127 to 30.39305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 30.7310 - MinusLogProbMetric: 30.7310 - val_loss: 30.3930 - val_MinusLogProbMetric: 30.3930 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 421/1000
2023-10-25 20:02:36.262 
Epoch 421/1000 
	 loss: 30.8711, MinusLogProbMetric: 30.8711, val_loss: 30.9122, val_MinusLogProbMetric: 30.9122

Epoch 421: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.8711 - MinusLogProbMetric: 30.8711 - val_loss: 30.9122 - val_MinusLogProbMetric: 30.9122 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 422/1000
2023-10-25 20:03:11.173 
Epoch 422/1000 
	 loss: 30.5708, MinusLogProbMetric: 30.5708, val_loss: 31.3697, val_MinusLogProbMetric: 31.3697

Epoch 422: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.5708 - MinusLogProbMetric: 30.5708 - val_loss: 31.3697 - val_MinusLogProbMetric: 31.3697 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 423/1000
2023-10-25 20:03:45.545 
Epoch 423/1000 
	 loss: 30.9092, MinusLogProbMetric: 30.9092, val_loss: 31.5695, val_MinusLogProbMetric: 31.5695

Epoch 423: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.9092 - MinusLogProbMetric: 30.9092 - val_loss: 31.5695 - val_MinusLogProbMetric: 31.5695 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 424/1000
2023-10-25 20:04:20.377 
Epoch 424/1000 
	 loss: 30.7108, MinusLogProbMetric: 30.7108, val_loss: 31.2843, val_MinusLogProbMetric: 31.2843

Epoch 424: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.7108 - MinusLogProbMetric: 30.7108 - val_loss: 31.2843 - val_MinusLogProbMetric: 31.2843 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 425/1000
2023-10-25 20:04:55.282 
Epoch 425/1000 
	 loss: 30.6838, MinusLogProbMetric: 30.6838, val_loss: 30.9104, val_MinusLogProbMetric: 30.9104

Epoch 425: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.6838 - MinusLogProbMetric: 30.6838 - val_loss: 30.9104 - val_MinusLogProbMetric: 30.9104 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 426/1000
2023-10-25 20:05:30.110 
Epoch 426/1000 
	 loss: 30.7668, MinusLogProbMetric: 30.7668, val_loss: 30.6431, val_MinusLogProbMetric: 30.6431

Epoch 426: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.7668 - MinusLogProbMetric: 30.7668 - val_loss: 30.6431 - val_MinusLogProbMetric: 30.6431 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 427/1000
2023-10-25 20:06:04.599 
Epoch 427/1000 
	 loss: 30.9368, MinusLogProbMetric: 30.9368, val_loss: 31.2216, val_MinusLogProbMetric: 31.2216

Epoch 427: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.9368 - MinusLogProbMetric: 30.9368 - val_loss: 31.2216 - val_MinusLogProbMetric: 31.2216 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 428/1000
2023-10-25 20:06:39.531 
Epoch 428/1000 
	 loss: 30.4877, MinusLogProbMetric: 30.4877, val_loss: 30.8772, val_MinusLogProbMetric: 30.8772

Epoch 428: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.4877 - MinusLogProbMetric: 30.4877 - val_loss: 30.8772 - val_MinusLogProbMetric: 30.8772 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 429/1000
2023-10-25 20:07:14.040 
Epoch 429/1000 
	 loss: 30.7801, MinusLogProbMetric: 30.7801, val_loss: 30.5843, val_MinusLogProbMetric: 30.5843

Epoch 429: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.7801 - MinusLogProbMetric: 30.7801 - val_loss: 30.5843 - val_MinusLogProbMetric: 30.5843 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 430/1000
2023-10-25 20:07:48.602 
Epoch 430/1000 
	 loss: 30.4768, MinusLogProbMetric: 30.4768, val_loss: 31.9467, val_MinusLogProbMetric: 31.9467

Epoch 430: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.4768 - MinusLogProbMetric: 30.4768 - val_loss: 31.9467 - val_MinusLogProbMetric: 31.9467 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 431/1000
2023-10-25 20:08:23.429 
Epoch 431/1000 
	 loss: 30.6312, MinusLogProbMetric: 30.6312, val_loss: 30.6445, val_MinusLogProbMetric: 30.6445

Epoch 431: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.6312 - MinusLogProbMetric: 30.6312 - val_loss: 30.6445 - val_MinusLogProbMetric: 30.6445 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 432/1000
2023-10-25 20:08:57.919 
Epoch 432/1000 
	 loss: 30.7583, MinusLogProbMetric: 30.7583, val_loss: 30.4058, val_MinusLogProbMetric: 30.4058

Epoch 432: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.7583 - MinusLogProbMetric: 30.7583 - val_loss: 30.4058 - val_MinusLogProbMetric: 30.4058 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 433/1000
2023-10-25 20:09:32.414 
Epoch 433/1000 
	 loss: 30.8776, MinusLogProbMetric: 30.8776, val_loss: 30.8692, val_MinusLogProbMetric: 30.8692

Epoch 433: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.8776 - MinusLogProbMetric: 30.8776 - val_loss: 30.8692 - val_MinusLogProbMetric: 30.8692 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 434/1000
2023-10-25 20:10:07.092 
Epoch 434/1000 
	 loss: 31.0489, MinusLogProbMetric: 31.0489, val_loss: 30.6637, val_MinusLogProbMetric: 30.6637

Epoch 434: val_loss did not improve from 30.39305
196/196 - 35s - loss: 31.0489 - MinusLogProbMetric: 31.0489 - val_loss: 30.6637 - val_MinusLogProbMetric: 30.6637 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 435/1000
2023-10-25 20:10:41.680 
Epoch 435/1000 
	 loss: 30.6483, MinusLogProbMetric: 30.6483, val_loss: 32.1023, val_MinusLogProbMetric: 32.1023

Epoch 435: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.6483 - MinusLogProbMetric: 30.6483 - val_loss: 32.1023 - val_MinusLogProbMetric: 32.1023 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 436/1000
2023-10-25 20:11:16.452 
Epoch 436/1000 
	 loss: 30.9006, MinusLogProbMetric: 30.9006, val_loss: 30.9410, val_MinusLogProbMetric: 30.9410

Epoch 436: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.9006 - MinusLogProbMetric: 30.9006 - val_loss: 30.9410 - val_MinusLogProbMetric: 30.9410 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 437/1000
2023-10-25 20:11:50.993 
Epoch 437/1000 
	 loss: 30.7437, MinusLogProbMetric: 30.7437, val_loss: 30.8880, val_MinusLogProbMetric: 30.8880

Epoch 437: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.7437 - MinusLogProbMetric: 30.7437 - val_loss: 30.8880 - val_MinusLogProbMetric: 30.8880 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 438/1000
2023-10-25 20:12:25.578 
Epoch 438/1000 
	 loss: 30.6607, MinusLogProbMetric: 30.6607, val_loss: 31.3998, val_MinusLogProbMetric: 31.3998

Epoch 438: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.6607 - MinusLogProbMetric: 30.6607 - val_loss: 31.3998 - val_MinusLogProbMetric: 31.3998 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 439/1000
2023-10-25 20:12:59.964 
Epoch 439/1000 
	 loss: 30.9864, MinusLogProbMetric: 30.9864, val_loss: 30.5818, val_MinusLogProbMetric: 30.5818

Epoch 439: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.9864 - MinusLogProbMetric: 30.9864 - val_loss: 30.5818 - val_MinusLogProbMetric: 30.5818 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 440/1000
2023-10-25 20:13:32.524 
Epoch 440/1000 
	 loss: 30.5220, MinusLogProbMetric: 30.5220, val_loss: 31.1881, val_MinusLogProbMetric: 31.1881

Epoch 440: val_loss did not improve from 30.39305
196/196 - 33s - loss: 30.5220 - MinusLogProbMetric: 30.5220 - val_loss: 31.1881 - val_MinusLogProbMetric: 31.1881 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 441/1000
2023-10-25 20:14:04.111 
Epoch 441/1000 
	 loss: 30.6085, MinusLogProbMetric: 30.6085, val_loss: 33.4727, val_MinusLogProbMetric: 33.4727

Epoch 441: val_loss did not improve from 30.39305
196/196 - 32s - loss: 30.6085 - MinusLogProbMetric: 30.6085 - val_loss: 33.4727 - val_MinusLogProbMetric: 33.4727 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 442/1000
2023-10-25 20:14:37.159 
Epoch 442/1000 
	 loss: 31.4739, MinusLogProbMetric: 31.4739, val_loss: 30.8320, val_MinusLogProbMetric: 30.8320

Epoch 442: val_loss did not improve from 30.39305
196/196 - 33s - loss: 31.4739 - MinusLogProbMetric: 31.4739 - val_loss: 30.8320 - val_MinusLogProbMetric: 30.8320 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 443/1000
2023-10-25 20:15:10.638 
Epoch 443/1000 
	 loss: 30.7165, MinusLogProbMetric: 30.7165, val_loss: 30.8104, val_MinusLogProbMetric: 30.8104

Epoch 443: val_loss did not improve from 30.39305
196/196 - 33s - loss: 30.7165 - MinusLogProbMetric: 30.7165 - val_loss: 30.8104 - val_MinusLogProbMetric: 30.8104 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 444/1000
2023-10-25 20:15:45.071 
Epoch 444/1000 
	 loss: 30.8227, MinusLogProbMetric: 30.8227, val_loss: 31.0205, val_MinusLogProbMetric: 31.0205

Epoch 444: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.8227 - MinusLogProbMetric: 30.8227 - val_loss: 31.0205 - val_MinusLogProbMetric: 31.0205 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 445/1000
2023-10-25 20:16:19.519 
Epoch 445/1000 
	 loss: 30.8170, MinusLogProbMetric: 30.8170, val_loss: 32.3402, val_MinusLogProbMetric: 32.3402

Epoch 445: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.8170 - MinusLogProbMetric: 30.8170 - val_loss: 32.3402 - val_MinusLogProbMetric: 32.3402 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 446/1000
2023-10-25 20:16:54.001 
Epoch 446/1000 
	 loss: 30.5722, MinusLogProbMetric: 30.5722, val_loss: 30.5486, val_MinusLogProbMetric: 30.5486

Epoch 446: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.5722 - MinusLogProbMetric: 30.5722 - val_loss: 30.5486 - val_MinusLogProbMetric: 30.5486 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 447/1000
2023-10-25 20:17:28.467 
Epoch 447/1000 
	 loss: 30.7900, MinusLogProbMetric: 30.7900, val_loss: 31.5302, val_MinusLogProbMetric: 31.5302

Epoch 447: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.7900 - MinusLogProbMetric: 30.7900 - val_loss: 31.5302 - val_MinusLogProbMetric: 31.5302 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 448/1000
2023-10-25 20:18:01.117 
Epoch 448/1000 
	 loss: 30.5623, MinusLogProbMetric: 30.5623, val_loss: 30.7328, val_MinusLogProbMetric: 30.7328

Epoch 448: val_loss did not improve from 30.39305
196/196 - 33s - loss: 30.5623 - MinusLogProbMetric: 30.5623 - val_loss: 30.7328 - val_MinusLogProbMetric: 30.7328 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 449/1000
2023-10-25 20:18:35.807 
Epoch 449/1000 
	 loss: 30.5319, MinusLogProbMetric: 30.5319, val_loss: 31.5742, val_MinusLogProbMetric: 31.5742

Epoch 449: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.5319 - MinusLogProbMetric: 30.5319 - val_loss: 31.5742 - val_MinusLogProbMetric: 31.5742 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 450/1000
2023-10-25 20:19:10.234 
Epoch 450/1000 
	 loss: 31.0309, MinusLogProbMetric: 31.0309, val_loss: 30.4413, val_MinusLogProbMetric: 30.4413

Epoch 450: val_loss did not improve from 30.39305
196/196 - 34s - loss: 31.0309 - MinusLogProbMetric: 31.0309 - val_loss: 30.4413 - val_MinusLogProbMetric: 30.4413 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 451/1000
2023-10-25 20:19:44.724 
Epoch 451/1000 
	 loss: 30.5280, MinusLogProbMetric: 30.5280, val_loss: 33.4565, val_MinusLogProbMetric: 33.4565

Epoch 451: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.5280 - MinusLogProbMetric: 30.5280 - val_loss: 33.4565 - val_MinusLogProbMetric: 33.4565 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 452/1000
2023-10-25 20:20:19.417 
Epoch 452/1000 
	 loss: 30.8106, MinusLogProbMetric: 30.8106, val_loss: 30.9833, val_MinusLogProbMetric: 30.9833

Epoch 452: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.8106 - MinusLogProbMetric: 30.8106 - val_loss: 30.9833 - val_MinusLogProbMetric: 30.9833 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 453/1000
2023-10-25 20:20:53.379 
Epoch 453/1000 
	 loss: 30.6373, MinusLogProbMetric: 30.6373, val_loss: 32.3925, val_MinusLogProbMetric: 32.3925

Epoch 453: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.6373 - MinusLogProbMetric: 30.6373 - val_loss: 32.3925 - val_MinusLogProbMetric: 32.3925 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 454/1000
2023-10-25 20:21:27.782 
Epoch 454/1000 
	 loss: 30.4848, MinusLogProbMetric: 30.4848, val_loss: 31.4281, val_MinusLogProbMetric: 31.4281

Epoch 454: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.4848 - MinusLogProbMetric: 30.4848 - val_loss: 31.4281 - val_MinusLogProbMetric: 31.4281 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 455/1000
2023-10-25 20:22:02.189 
Epoch 455/1000 
	 loss: 30.6077, MinusLogProbMetric: 30.6077, val_loss: 31.9796, val_MinusLogProbMetric: 31.9796

Epoch 455: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.6077 - MinusLogProbMetric: 30.6077 - val_loss: 31.9796 - val_MinusLogProbMetric: 31.9796 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 456/1000
2023-10-25 20:22:36.524 
Epoch 456/1000 
	 loss: 30.8726, MinusLogProbMetric: 30.8726, val_loss: 31.5829, val_MinusLogProbMetric: 31.5829

Epoch 456: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.8726 - MinusLogProbMetric: 30.8726 - val_loss: 31.5829 - val_MinusLogProbMetric: 31.5829 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 457/1000
2023-10-25 20:23:10.618 
Epoch 457/1000 
	 loss: 30.6498, MinusLogProbMetric: 30.6498, val_loss: 30.6128, val_MinusLogProbMetric: 30.6128

Epoch 457: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.6498 - MinusLogProbMetric: 30.6498 - val_loss: 30.6128 - val_MinusLogProbMetric: 30.6128 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 458/1000
2023-10-25 20:23:45.030 
Epoch 458/1000 
	 loss: 30.8971, MinusLogProbMetric: 30.8971, val_loss: 31.2621, val_MinusLogProbMetric: 31.2621

Epoch 458: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.8971 - MinusLogProbMetric: 30.8971 - val_loss: 31.2621 - val_MinusLogProbMetric: 31.2621 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 459/1000
2023-10-25 20:24:19.411 
Epoch 459/1000 
	 loss: 30.9042, MinusLogProbMetric: 30.9042, val_loss: 30.7232, val_MinusLogProbMetric: 30.7232

Epoch 459: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.9042 - MinusLogProbMetric: 30.9042 - val_loss: 30.7232 - val_MinusLogProbMetric: 30.7232 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 460/1000
2023-10-25 20:24:53.870 
Epoch 460/1000 
	 loss: 30.5257, MinusLogProbMetric: 30.5257, val_loss: 30.5084, val_MinusLogProbMetric: 30.5084

Epoch 460: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.5257 - MinusLogProbMetric: 30.5257 - val_loss: 30.5084 - val_MinusLogProbMetric: 30.5084 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 461/1000
2023-10-25 20:25:28.514 
Epoch 461/1000 
	 loss: 30.6918, MinusLogProbMetric: 30.6918, val_loss: 31.3860, val_MinusLogProbMetric: 31.3860

Epoch 461: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.6918 - MinusLogProbMetric: 30.6918 - val_loss: 31.3860 - val_MinusLogProbMetric: 31.3860 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 462/1000
2023-10-25 20:26:03.086 
Epoch 462/1000 
	 loss: 30.6335, MinusLogProbMetric: 30.6335, val_loss: 30.5628, val_MinusLogProbMetric: 30.5628

Epoch 462: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.6335 - MinusLogProbMetric: 30.6335 - val_loss: 30.5628 - val_MinusLogProbMetric: 30.5628 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 463/1000
2023-10-25 20:26:37.604 
Epoch 463/1000 
	 loss: 30.5420, MinusLogProbMetric: 30.5420, val_loss: 30.4394, val_MinusLogProbMetric: 30.4394

Epoch 463: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.5420 - MinusLogProbMetric: 30.5420 - val_loss: 30.4394 - val_MinusLogProbMetric: 30.4394 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 464/1000
2023-10-25 20:27:12.085 
Epoch 464/1000 
	 loss: 30.6993, MinusLogProbMetric: 30.6993, val_loss: 30.8067, val_MinusLogProbMetric: 30.8067

Epoch 464: val_loss did not improve from 30.39305
196/196 - 34s - loss: 30.6993 - MinusLogProbMetric: 30.6993 - val_loss: 30.8067 - val_MinusLogProbMetric: 30.8067 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 465/1000
2023-10-25 20:27:46.682 
Epoch 465/1000 
	 loss: 30.4699, MinusLogProbMetric: 30.4699, val_loss: 30.6085, val_MinusLogProbMetric: 30.6085

Epoch 465: val_loss did not improve from 30.39305
196/196 - 35s - loss: 30.4699 - MinusLogProbMetric: 30.4699 - val_loss: 30.6085 - val_MinusLogProbMetric: 30.6085 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 466/1000
2023-10-25 20:28:21.358 
Epoch 466/1000 
	 loss: 30.3721, MinusLogProbMetric: 30.3721, val_loss: 30.3475, val_MinusLogProbMetric: 30.3475

Epoch 466: val_loss improved from 30.39305 to 30.34752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.3721 - MinusLogProbMetric: 30.3721 - val_loss: 30.3475 - val_MinusLogProbMetric: 30.3475 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 467/1000
2023-10-25 20:28:56.831 
Epoch 467/1000 
	 loss: 31.0249, MinusLogProbMetric: 31.0249, val_loss: 31.0074, val_MinusLogProbMetric: 31.0074

Epoch 467: val_loss did not improve from 30.34752
196/196 - 35s - loss: 31.0249 - MinusLogProbMetric: 31.0249 - val_loss: 31.0074 - val_MinusLogProbMetric: 31.0074 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 468/1000
2023-10-25 20:29:31.569 
Epoch 468/1000 
	 loss: 30.7945, MinusLogProbMetric: 30.7945, val_loss: 31.4252, val_MinusLogProbMetric: 31.4252

Epoch 468: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.7945 - MinusLogProbMetric: 30.7945 - val_loss: 31.4252 - val_MinusLogProbMetric: 31.4252 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 469/1000
2023-10-25 20:30:06.089 
Epoch 469/1000 
	 loss: 30.7616, MinusLogProbMetric: 30.7616, val_loss: 30.3766, val_MinusLogProbMetric: 30.3766

Epoch 469: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.7616 - MinusLogProbMetric: 30.7616 - val_loss: 30.3766 - val_MinusLogProbMetric: 30.3766 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 470/1000
2023-10-25 20:30:40.929 
Epoch 470/1000 
	 loss: 30.5656, MinusLogProbMetric: 30.5656, val_loss: 30.4461, val_MinusLogProbMetric: 30.4461

Epoch 470: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.5656 - MinusLogProbMetric: 30.5656 - val_loss: 30.4461 - val_MinusLogProbMetric: 30.4461 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 471/1000
2023-10-25 20:31:15.374 
Epoch 471/1000 
	 loss: 30.6019, MinusLogProbMetric: 30.6019, val_loss: 30.8705, val_MinusLogProbMetric: 30.8705

Epoch 471: val_loss did not improve from 30.34752
196/196 - 34s - loss: 30.6019 - MinusLogProbMetric: 30.6019 - val_loss: 30.8705 - val_MinusLogProbMetric: 30.8705 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 472/1000
2023-10-25 20:31:50.340 
Epoch 472/1000 
	 loss: 30.5179, MinusLogProbMetric: 30.5179, val_loss: 31.0808, val_MinusLogProbMetric: 31.0808

Epoch 472: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.5179 - MinusLogProbMetric: 30.5179 - val_loss: 31.0808 - val_MinusLogProbMetric: 31.0808 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 473/1000
2023-10-25 20:32:24.950 
Epoch 473/1000 
	 loss: 32.4370, MinusLogProbMetric: 32.4370, val_loss: 31.2955, val_MinusLogProbMetric: 31.2955

Epoch 473: val_loss did not improve from 30.34752
196/196 - 35s - loss: 32.4370 - MinusLogProbMetric: 32.4370 - val_loss: 31.2955 - val_MinusLogProbMetric: 31.2955 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 474/1000
2023-10-25 20:32:59.980 
Epoch 474/1000 
	 loss: 31.2636, MinusLogProbMetric: 31.2636, val_loss: 31.3995, val_MinusLogProbMetric: 31.3995

Epoch 474: val_loss did not improve from 30.34752
196/196 - 35s - loss: 31.2636 - MinusLogProbMetric: 31.2636 - val_loss: 31.3995 - val_MinusLogProbMetric: 31.3995 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 475/1000
2023-10-25 20:33:34.210 
Epoch 475/1000 
	 loss: 31.1902, MinusLogProbMetric: 31.1902, val_loss: 30.7895, val_MinusLogProbMetric: 30.7895

Epoch 475: val_loss did not improve from 30.34752
196/196 - 34s - loss: 31.1902 - MinusLogProbMetric: 31.1902 - val_loss: 30.7895 - val_MinusLogProbMetric: 30.7895 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 476/1000
2023-10-25 20:34:08.793 
Epoch 476/1000 
	 loss: 30.8024, MinusLogProbMetric: 30.8024, val_loss: 30.9529, val_MinusLogProbMetric: 30.9529

Epoch 476: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.8024 - MinusLogProbMetric: 30.8024 - val_loss: 30.9529 - val_MinusLogProbMetric: 30.9529 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 477/1000
2023-10-25 20:34:43.476 
Epoch 477/1000 
	 loss: 30.5399, MinusLogProbMetric: 30.5399, val_loss: 30.4875, val_MinusLogProbMetric: 30.4875

Epoch 477: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.5399 - MinusLogProbMetric: 30.5399 - val_loss: 30.4875 - val_MinusLogProbMetric: 30.4875 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 478/1000
2023-10-25 20:35:18.061 
Epoch 478/1000 
	 loss: 30.4966, MinusLogProbMetric: 30.4966, val_loss: 30.4111, val_MinusLogProbMetric: 30.4111

Epoch 478: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.4966 - MinusLogProbMetric: 30.4966 - val_loss: 30.4111 - val_MinusLogProbMetric: 30.4111 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 479/1000
2023-10-25 20:35:52.805 
Epoch 479/1000 
	 loss: 30.6889, MinusLogProbMetric: 30.6889, val_loss: 31.0034, val_MinusLogProbMetric: 31.0034

Epoch 479: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.6889 - MinusLogProbMetric: 30.6889 - val_loss: 31.0034 - val_MinusLogProbMetric: 31.0034 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 480/1000
2023-10-25 20:36:27.664 
Epoch 480/1000 
	 loss: 30.5340, MinusLogProbMetric: 30.5340, val_loss: 30.8943, val_MinusLogProbMetric: 30.8943

Epoch 480: val_loss did not improve from 30.34752
196/196 - 35s - loss: 30.5340 - MinusLogProbMetric: 30.5340 - val_loss: 30.8943 - val_MinusLogProbMetric: 30.8943 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 481/1000
2023-10-25 20:37:02.129 
Epoch 481/1000 
	 loss: 30.7745, MinusLogProbMetric: 30.7745, val_loss: 30.2933, val_MinusLogProbMetric: 30.2933

Epoch 481: val_loss improved from 30.34752 to 30.29326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.7745 - MinusLogProbMetric: 30.7745 - val_loss: 30.2933 - val_MinusLogProbMetric: 30.2933 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 482/1000
2023-10-25 20:37:37.277 
Epoch 482/1000 
	 loss: 30.7255, MinusLogProbMetric: 30.7255, val_loss: 30.7788, val_MinusLogProbMetric: 30.7788

Epoch 482: val_loss did not improve from 30.29326
196/196 - 35s - loss: 30.7255 - MinusLogProbMetric: 30.7255 - val_loss: 30.7788 - val_MinusLogProbMetric: 30.7788 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 483/1000
2023-10-25 20:38:11.983 
Epoch 483/1000 
	 loss: 30.3789, MinusLogProbMetric: 30.3789, val_loss: 31.1348, val_MinusLogProbMetric: 31.1348

Epoch 483: val_loss did not improve from 30.29326
196/196 - 35s - loss: 30.3789 - MinusLogProbMetric: 30.3789 - val_loss: 31.1348 - val_MinusLogProbMetric: 31.1348 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 484/1000
2023-10-25 20:38:46.895 
Epoch 484/1000 
	 loss: 30.4187, MinusLogProbMetric: 30.4187, val_loss: 30.6810, val_MinusLogProbMetric: 30.6810

Epoch 484: val_loss did not improve from 30.29326
196/196 - 35s - loss: 30.4187 - MinusLogProbMetric: 30.4187 - val_loss: 30.6810 - val_MinusLogProbMetric: 30.6810 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 485/1000
2023-10-25 20:39:21.653 
Epoch 485/1000 
	 loss: 30.5312, MinusLogProbMetric: 30.5312, val_loss: 30.8750, val_MinusLogProbMetric: 30.8750

Epoch 485: val_loss did not improve from 30.29326
196/196 - 35s - loss: 30.5312 - MinusLogProbMetric: 30.5312 - val_loss: 30.8750 - val_MinusLogProbMetric: 30.8750 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 486/1000
2023-10-25 20:39:56.425 
Epoch 486/1000 
	 loss: 31.0551, MinusLogProbMetric: 31.0551, val_loss: 30.4449, val_MinusLogProbMetric: 30.4449

Epoch 486: val_loss did not improve from 30.29326
196/196 - 35s - loss: 31.0551 - MinusLogProbMetric: 31.0551 - val_loss: 30.4449 - val_MinusLogProbMetric: 30.4449 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 487/1000
2023-10-25 20:40:31.275 
Epoch 487/1000 
	 loss: 30.3483, MinusLogProbMetric: 30.3483, val_loss: 30.6717, val_MinusLogProbMetric: 30.6717

Epoch 487: val_loss did not improve from 30.29326
196/196 - 35s - loss: 30.3483 - MinusLogProbMetric: 30.3483 - val_loss: 30.6717 - val_MinusLogProbMetric: 30.6717 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 488/1000
2023-10-25 20:41:06.218 
Epoch 488/1000 
	 loss: 30.3598, MinusLogProbMetric: 30.3598, val_loss: 30.5272, val_MinusLogProbMetric: 30.5272

Epoch 488: val_loss did not improve from 30.29326
196/196 - 35s - loss: 30.3598 - MinusLogProbMetric: 30.3598 - val_loss: 30.5272 - val_MinusLogProbMetric: 30.5272 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 489/1000
2023-10-25 20:41:40.175 
Epoch 489/1000 
	 loss: 31.1165, MinusLogProbMetric: 31.1165, val_loss: 30.7591, val_MinusLogProbMetric: 30.7591

Epoch 489: val_loss did not improve from 30.29326
196/196 - 34s - loss: 31.1165 - MinusLogProbMetric: 31.1165 - val_loss: 30.7591 - val_MinusLogProbMetric: 30.7591 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 490/1000
2023-10-25 20:42:11.240 
Epoch 490/1000 
	 loss: 30.7688, MinusLogProbMetric: 30.7688, val_loss: 30.4511, val_MinusLogProbMetric: 30.4511

Epoch 490: val_loss did not improve from 30.29326
196/196 - 31s - loss: 30.7688 - MinusLogProbMetric: 30.7688 - val_loss: 30.4511 - val_MinusLogProbMetric: 30.4511 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 491/1000
2023-10-25 20:42:45.529 
Epoch 491/1000 
	 loss: 30.2713, MinusLogProbMetric: 30.2713, val_loss: 30.9864, val_MinusLogProbMetric: 30.9864

Epoch 491: val_loss did not improve from 30.29326
196/196 - 34s - loss: 30.2713 - MinusLogProbMetric: 30.2713 - val_loss: 30.9864 - val_MinusLogProbMetric: 30.9864 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 492/1000
2023-10-25 20:43:17.708 
Epoch 492/1000 
	 loss: 30.3540, MinusLogProbMetric: 30.3540, val_loss: 31.3644, val_MinusLogProbMetric: 31.3644

Epoch 492: val_loss did not improve from 30.29326
196/196 - 32s - loss: 30.3540 - MinusLogProbMetric: 30.3540 - val_loss: 31.3644 - val_MinusLogProbMetric: 31.3644 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 493/1000
2023-10-25 20:43:52.043 
Epoch 493/1000 
	 loss: 30.5301, MinusLogProbMetric: 30.5301, val_loss: 30.3869, val_MinusLogProbMetric: 30.3869

Epoch 493: val_loss did not improve from 30.29326
196/196 - 34s - loss: 30.5301 - MinusLogProbMetric: 30.5301 - val_loss: 30.3869 - val_MinusLogProbMetric: 30.3869 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 494/1000
2023-10-25 20:44:24.240 
Epoch 494/1000 
	 loss: 30.9579, MinusLogProbMetric: 30.9579, val_loss: 30.5707, val_MinusLogProbMetric: 30.5707

Epoch 494: val_loss did not improve from 30.29326
196/196 - 32s - loss: 30.9579 - MinusLogProbMetric: 30.9579 - val_loss: 30.5707 - val_MinusLogProbMetric: 30.5707 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 495/1000
2023-10-25 20:44:57.998 
Epoch 495/1000 
	 loss: 30.4993, MinusLogProbMetric: 30.4993, val_loss: 30.1776, val_MinusLogProbMetric: 30.1776

Epoch 495: val_loss improved from 30.29326 to 30.17756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 30.4993 - MinusLogProbMetric: 30.4993 - val_loss: 30.1776 - val_MinusLogProbMetric: 30.1776 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 496/1000
2023-10-25 20:45:32.811 
Epoch 496/1000 
	 loss: 30.3816, MinusLogProbMetric: 30.3816, val_loss: 30.4391, val_MinusLogProbMetric: 30.4391

Epoch 496: val_loss did not improve from 30.17756
196/196 - 34s - loss: 30.3816 - MinusLogProbMetric: 30.3816 - val_loss: 30.4391 - val_MinusLogProbMetric: 30.4391 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 497/1000
2023-10-25 20:46:07.398 
Epoch 497/1000 
	 loss: 30.6117, MinusLogProbMetric: 30.6117, val_loss: 30.7917, val_MinusLogProbMetric: 30.7917

Epoch 497: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.6117 - MinusLogProbMetric: 30.6117 - val_loss: 30.7917 - val_MinusLogProbMetric: 30.7917 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 498/1000
2023-10-25 20:46:39.663 
Epoch 498/1000 
	 loss: 30.8920, MinusLogProbMetric: 30.8920, val_loss: 30.9283, val_MinusLogProbMetric: 30.9283

Epoch 498: val_loss did not improve from 30.17756
196/196 - 32s - loss: 30.8920 - MinusLogProbMetric: 30.8920 - val_loss: 30.9283 - val_MinusLogProbMetric: 30.9283 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 499/1000
2023-10-25 20:47:13.968 
Epoch 499/1000 
	 loss: 30.3538, MinusLogProbMetric: 30.3538, val_loss: 35.0508, val_MinusLogProbMetric: 35.0508

Epoch 499: val_loss did not improve from 30.17756
196/196 - 34s - loss: 30.3538 - MinusLogProbMetric: 30.3538 - val_loss: 35.0508 - val_MinusLogProbMetric: 35.0508 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 500/1000
2023-10-25 20:47:48.576 
Epoch 500/1000 
	 loss: 30.9157, MinusLogProbMetric: 30.9157, val_loss: 30.9200, val_MinusLogProbMetric: 30.9200

Epoch 500: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.9157 - MinusLogProbMetric: 30.9157 - val_loss: 30.9200 - val_MinusLogProbMetric: 30.9200 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 501/1000
2023-10-25 20:48:23.573 
Epoch 501/1000 
	 loss: 30.3857, MinusLogProbMetric: 30.3857, val_loss: 30.5288, val_MinusLogProbMetric: 30.5288

Epoch 501: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.3857 - MinusLogProbMetric: 30.3857 - val_loss: 30.5288 - val_MinusLogProbMetric: 30.5288 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 502/1000
2023-10-25 20:48:58.463 
Epoch 502/1000 
	 loss: 30.4554, MinusLogProbMetric: 30.4554, val_loss: 30.8589, val_MinusLogProbMetric: 30.8589

Epoch 502: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.4554 - MinusLogProbMetric: 30.4554 - val_loss: 30.8589 - val_MinusLogProbMetric: 30.8589 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 503/1000
2023-10-25 20:49:33.263 
Epoch 503/1000 
	 loss: 30.5400, MinusLogProbMetric: 30.5400, val_loss: 30.4271, val_MinusLogProbMetric: 30.4271

Epoch 503: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.5400 - MinusLogProbMetric: 30.5400 - val_loss: 30.4271 - val_MinusLogProbMetric: 30.4271 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 504/1000
2023-10-25 20:50:07.990 
Epoch 504/1000 
	 loss: 30.1984, MinusLogProbMetric: 30.1984, val_loss: 30.6901, val_MinusLogProbMetric: 30.6901

Epoch 504: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.1984 - MinusLogProbMetric: 30.1984 - val_loss: 30.6901 - val_MinusLogProbMetric: 30.6901 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 505/1000
2023-10-25 20:50:42.742 
Epoch 505/1000 
	 loss: 31.4834, MinusLogProbMetric: 31.4834, val_loss: 30.4618, val_MinusLogProbMetric: 30.4618

Epoch 505: val_loss did not improve from 30.17756
196/196 - 35s - loss: 31.4834 - MinusLogProbMetric: 31.4834 - val_loss: 30.4618 - val_MinusLogProbMetric: 30.4618 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 506/1000
2023-10-25 20:51:17.078 
Epoch 506/1000 
	 loss: 30.6272, MinusLogProbMetric: 30.6272, val_loss: 30.5297, val_MinusLogProbMetric: 30.5297

Epoch 506: val_loss did not improve from 30.17756
196/196 - 34s - loss: 30.6272 - MinusLogProbMetric: 30.6272 - val_loss: 30.5297 - val_MinusLogProbMetric: 30.5297 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 507/1000
2023-10-25 20:51:51.996 
Epoch 507/1000 
	 loss: 30.4011, MinusLogProbMetric: 30.4011, val_loss: 30.4033, val_MinusLogProbMetric: 30.4033

Epoch 507: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.4011 - MinusLogProbMetric: 30.4011 - val_loss: 30.4033 - val_MinusLogProbMetric: 30.4033 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 508/1000
2023-10-25 20:52:26.856 
Epoch 508/1000 
	 loss: 30.4072, MinusLogProbMetric: 30.4072, val_loss: 30.2733, val_MinusLogProbMetric: 30.2733

Epoch 508: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.4072 - MinusLogProbMetric: 30.4072 - val_loss: 30.2733 - val_MinusLogProbMetric: 30.2733 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 509/1000
2023-10-25 20:53:01.555 
Epoch 509/1000 
	 loss: 30.3492, MinusLogProbMetric: 30.3492, val_loss: 30.2367, val_MinusLogProbMetric: 30.2367

Epoch 509: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.3492 - MinusLogProbMetric: 30.3492 - val_loss: 30.2367 - val_MinusLogProbMetric: 30.2367 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 510/1000
2023-10-25 20:53:36.492 
Epoch 510/1000 
	 loss: 30.4949, MinusLogProbMetric: 30.4949, val_loss: 31.4238, val_MinusLogProbMetric: 31.4238

Epoch 510: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.4949 - MinusLogProbMetric: 30.4949 - val_loss: 31.4238 - val_MinusLogProbMetric: 31.4238 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 511/1000
2023-10-25 20:54:11.242 
Epoch 511/1000 
	 loss: 30.5610, MinusLogProbMetric: 30.5610, val_loss: 30.3175, val_MinusLogProbMetric: 30.3175

Epoch 511: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.5610 - MinusLogProbMetric: 30.5610 - val_loss: 30.3175 - val_MinusLogProbMetric: 30.3175 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 512/1000
2023-10-25 20:54:46.295 
Epoch 512/1000 
	 loss: 30.3504, MinusLogProbMetric: 30.3504, val_loss: 32.8219, val_MinusLogProbMetric: 32.8219

Epoch 512: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.3504 - MinusLogProbMetric: 30.3504 - val_loss: 32.8219 - val_MinusLogProbMetric: 32.8219 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 513/1000
2023-10-25 20:55:21.180 
Epoch 513/1000 
	 loss: 30.5101, MinusLogProbMetric: 30.5101, val_loss: 30.2542, val_MinusLogProbMetric: 30.2542

Epoch 513: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.5101 - MinusLogProbMetric: 30.5101 - val_loss: 30.2542 - val_MinusLogProbMetric: 30.2542 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 514/1000
2023-10-25 20:55:55.862 
Epoch 514/1000 
	 loss: 30.5339, MinusLogProbMetric: 30.5339, val_loss: 30.2075, val_MinusLogProbMetric: 30.2075

Epoch 514: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.5339 - MinusLogProbMetric: 30.5339 - val_loss: 30.2075 - val_MinusLogProbMetric: 30.2075 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 515/1000
2023-10-25 20:56:30.162 
Epoch 515/1000 
	 loss: 30.3327, MinusLogProbMetric: 30.3327, val_loss: 30.5140, val_MinusLogProbMetric: 30.5140

Epoch 515: val_loss did not improve from 30.17756
196/196 - 34s - loss: 30.3327 - MinusLogProbMetric: 30.3327 - val_loss: 30.5140 - val_MinusLogProbMetric: 30.5140 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 516/1000
2023-10-25 20:57:05.111 
Epoch 516/1000 
	 loss: 30.3130, MinusLogProbMetric: 30.3130, val_loss: 30.3751, val_MinusLogProbMetric: 30.3751

Epoch 516: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.3130 - MinusLogProbMetric: 30.3130 - val_loss: 30.3751 - val_MinusLogProbMetric: 30.3751 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 517/1000
2023-10-25 20:57:40.153 
Epoch 517/1000 
	 loss: 30.5437, MinusLogProbMetric: 30.5437, val_loss: 30.3247, val_MinusLogProbMetric: 30.3247

Epoch 517: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.5437 - MinusLogProbMetric: 30.5437 - val_loss: 30.3247 - val_MinusLogProbMetric: 30.3247 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 518/1000
2023-10-25 20:58:15.150 
Epoch 518/1000 
	 loss: 30.2019, MinusLogProbMetric: 30.2019, val_loss: 30.8051, val_MinusLogProbMetric: 30.8051

Epoch 518: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.2019 - MinusLogProbMetric: 30.2019 - val_loss: 30.8051 - val_MinusLogProbMetric: 30.8051 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 519/1000
2023-10-25 20:58:49.828 
Epoch 519/1000 
	 loss: 30.6880, MinusLogProbMetric: 30.6880, val_loss: 30.4579, val_MinusLogProbMetric: 30.4579

Epoch 519: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.6880 - MinusLogProbMetric: 30.6880 - val_loss: 30.4579 - val_MinusLogProbMetric: 30.4579 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 520/1000
2023-10-25 20:59:24.483 
Epoch 520/1000 
	 loss: 30.4123, MinusLogProbMetric: 30.4123, val_loss: 31.1405, val_MinusLogProbMetric: 31.1405

Epoch 520: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.4123 - MinusLogProbMetric: 30.4123 - val_loss: 31.1405 - val_MinusLogProbMetric: 31.1405 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 521/1000
2023-10-25 20:59:59.151 
Epoch 521/1000 
	 loss: 30.4049, MinusLogProbMetric: 30.4049, val_loss: 31.0354, val_MinusLogProbMetric: 31.0354

Epoch 521: val_loss did not improve from 30.17756
196/196 - 35s - loss: 30.4049 - MinusLogProbMetric: 30.4049 - val_loss: 31.0354 - val_MinusLogProbMetric: 31.0354 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 522/1000
2023-10-25 21:00:34.036 
Epoch 522/1000 
	 loss: 30.1827, MinusLogProbMetric: 30.1827, val_loss: 30.0078, val_MinusLogProbMetric: 30.0078

Epoch 522: val_loss improved from 30.17756 to 30.00780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.1827 - MinusLogProbMetric: 30.1827 - val_loss: 30.0078 - val_MinusLogProbMetric: 30.0078 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 523/1000
2023-10-25 21:01:09.362 
Epoch 523/1000 
	 loss: 30.4284, MinusLogProbMetric: 30.4284, val_loss: 30.7912, val_MinusLogProbMetric: 30.7912

Epoch 523: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.4284 - MinusLogProbMetric: 30.4284 - val_loss: 30.7912 - val_MinusLogProbMetric: 30.7912 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 524/1000
2023-10-25 21:01:44.542 
Epoch 524/1000 
	 loss: 30.1844, MinusLogProbMetric: 30.1844, val_loss: 30.5822, val_MinusLogProbMetric: 30.5822

Epoch 524: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.1844 - MinusLogProbMetric: 30.1844 - val_loss: 30.5822 - val_MinusLogProbMetric: 30.5822 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 525/1000
2023-10-25 21:02:19.318 
Epoch 525/1000 
	 loss: 30.7926, MinusLogProbMetric: 30.7926, val_loss: 30.1458, val_MinusLogProbMetric: 30.1458

Epoch 525: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.7926 - MinusLogProbMetric: 30.7926 - val_loss: 30.1458 - val_MinusLogProbMetric: 30.1458 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 526/1000
2023-10-25 21:02:52.319 
Epoch 526/1000 
	 loss: 30.4440, MinusLogProbMetric: 30.4440, val_loss: 30.3580, val_MinusLogProbMetric: 30.3580

Epoch 526: val_loss did not improve from 30.00780
196/196 - 33s - loss: 30.4440 - MinusLogProbMetric: 30.4440 - val_loss: 30.3580 - val_MinusLogProbMetric: 30.3580 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 527/1000
2023-10-25 21:03:25.746 
Epoch 527/1000 
	 loss: 30.4940, MinusLogProbMetric: 30.4940, val_loss: 30.2540, val_MinusLogProbMetric: 30.2540

Epoch 527: val_loss did not improve from 30.00780
196/196 - 33s - loss: 30.4940 - MinusLogProbMetric: 30.4940 - val_loss: 30.2540 - val_MinusLogProbMetric: 30.2540 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 528/1000
2023-10-25 21:04:00.200 
Epoch 528/1000 
	 loss: 30.2821, MinusLogProbMetric: 30.2821, val_loss: 30.1132, val_MinusLogProbMetric: 30.1132

Epoch 528: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.2821 - MinusLogProbMetric: 30.2821 - val_loss: 30.1132 - val_MinusLogProbMetric: 30.1132 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 529/1000
2023-10-25 21:04:34.420 
Epoch 529/1000 
	 loss: 30.1376, MinusLogProbMetric: 30.1376, val_loss: 30.1698, val_MinusLogProbMetric: 30.1698

Epoch 529: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.1376 - MinusLogProbMetric: 30.1376 - val_loss: 30.1698 - val_MinusLogProbMetric: 30.1698 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 530/1000
2023-10-25 21:05:08.021 
Epoch 530/1000 
	 loss: 30.2652, MinusLogProbMetric: 30.2652, val_loss: 31.8384, val_MinusLogProbMetric: 31.8384

Epoch 530: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.2652 - MinusLogProbMetric: 30.2652 - val_loss: 31.8384 - val_MinusLogProbMetric: 31.8384 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 531/1000
2023-10-25 21:05:42.106 
Epoch 531/1000 
	 loss: 30.7126, MinusLogProbMetric: 30.7126, val_loss: 33.1880, val_MinusLogProbMetric: 33.1880

Epoch 531: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.7126 - MinusLogProbMetric: 30.7126 - val_loss: 33.1880 - val_MinusLogProbMetric: 33.1880 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 532/1000
2023-10-25 21:06:16.282 
Epoch 532/1000 
	 loss: 30.3476, MinusLogProbMetric: 30.3476, val_loss: 30.0344, val_MinusLogProbMetric: 30.0344

Epoch 532: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.3476 - MinusLogProbMetric: 30.3476 - val_loss: 30.0344 - val_MinusLogProbMetric: 30.0344 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 533/1000
2023-10-25 21:06:51.009 
Epoch 533/1000 
	 loss: 30.4000, MinusLogProbMetric: 30.4000, val_loss: 30.5416, val_MinusLogProbMetric: 30.5416

Epoch 533: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.4000 - MinusLogProbMetric: 30.4000 - val_loss: 30.5416 - val_MinusLogProbMetric: 30.5416 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 534/1000
2023-10-25 21:07:26.002 
Epoch 534/1000 
	 loss: 31.4478, MinusLogProbMetric: 31.4478, val_loss: 31.2226, val_MinusLogProbMetric: 31.2226

Epoch 534: val_loss did not improve from 30.00780
196/196 - 35s - loss: 31.4478 - MinusLogProbMetric: 31.4478 - val_loss: 31.2226 - val_MinusLogProbMetric: 31.2226 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 535/1000
2023-10-25 21:07:59.593 
Epoch 535/1000 
	 loss: 30.3090, MinusLogProbMetric: 30.3090, val_loss: 30.0758, val_MinusLogProbMetric: 30.0758

Epoch 535: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.3090 - MinusLogProbMetric: 30.3090 - val_loss: 30.0758 - val_MinusLogProbMetric: 30.0758 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 536/1000
2023-10-25 21:08:34.506 
Epoch 536/1000 
	 loss: 30.4005, MinusLogProbMetric: 30.4005, val_loss: 30.3146, val_MinusLogProbMetric: 30.3146

Epoch 536: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.4005 - MinusLogProbMetric: 30.4005 - val_loss: 30.3146 - val_MinusLogProbMetric: 30.3146 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 537/1000
2023-10-25 21:09:09.561 
Epoch 537/1000 
	 loss: 30.3046, MinusLogProbMetric: 30.3046, val_loss: 30.4501, val_MinusLogProbMetric: 30.4501

Epoch 537: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.3046 - MinusLogProbMetric: 30.3046 - val_loss: 30.4501 - val_MinusLogProbMetric: 30.4501 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 538/1000
2023-10-25 21:09:43.769 
Epoch 538/1000 
	 loss: 30.3034, MinusLogProbMetric: 30.3034, val_loss: 30.2117, val_MinusLogProbMetric: 30.2117

Epoch 538: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.3034 - MinusLogProbMetric: 30.3034 - val_loss: 30.2117 - val_MinusLogProbMetric: 30.2117 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 539/1000
2023-10-25 21:10:18.179 
Epoch 539/1000 
	 loss: 30.3529, MinusLogProbMetric: 30.3529, val_loss: 30.3932, val_MinusLogProbMetric: 30.3932

Epoch 539: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.3529 - MinusLogProbMetric: 30.3529 - val_loss: 30.3932 - val_MinusLogProbMetric: 30.3932 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 540/1000
2023-10-25 21:10:52.506 
Epoch 540/1000 
	 loss: 30.1874, MinusLogProbMetric: 30.1874, val_loss: 33.2950, val_MinusLogProbMetric: 33.2950

Epoch 540: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.1874 - MinusLogProbMetric: 30.1874 - val_loss: 33.2950 - val_MinusLogProbMetric: 33.2950 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 541/1000
2023-10-25 21:11:27.029 
Epoch 541/1000 
	 loss: 30.6677, MinusLogProbMetric: 30.6677, val_loss: 30.1553, val_MinusLogProbMetric: 30.1553

Epoch 541: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.6677 - MinusLogProbMetric: 30.6677 - val_loss: 30.1553 - val_MinusLogProbMetric: 30.1553 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 542/1000
2023-10-25 21:12:01.715 
Epoch 542/1000 
	 loss: 30.4480, MinusLogProbMetric: 30.4480, val_loss: 30.5195, val_MinusLogProbMetric: 30.5195

Epoch 542: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.4480 - MinusLogProbMetric: 30.4480 - val_loss: 30.5195 - val_MinusLogProbMetric: 30.5195 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 543/1000
2023-10-25 21:12:36.224 
Epoch 543/1000 
	 loss: 30.2430, MinusLogProbMetric: 30.2430, val_loss: 30.5714, val_MinusLogProbMetric: 30.5714

Epoch 543: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.2430 - MinusLogProbMetric: 30.2430 - val_loss: 30.5714 - val_MinusLogProbMetric: 30.5714 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 544/1000
2023-10-25 21:13:10.985 
Epoch 544/1000 
	 loss: 30.1846, MinusLogProbMetric: 30.1846, val_loss: 30.2373, val_MinusLogProbMetric: 30.2373

Epoch 544: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.1846 - MinusLogProbMetric: 30.1846 - val_loss: 30.2373 - val_MinusLogProbMetric: 30.2373 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 545/1000
2023-10-25 21:13:45.665 
Epoch 545/1000 
	 loss: 30.1813, MinusLogProbMetric: 30.1813, val_loss: 30.2956, val_MinusLogProbMetric: 30.2956

Epoch 545: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.1813 - MinusLogProbMetric: 30.1813 - val_loss: 30.2956 - val_MinusLogProbMetric: 30.2956 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 546/1000
2023-10-25 21:14:20.366 
Epoch 546/1000 
	 loss: 30.4459, MinusLogProbMetric: 30.4459, val_loss: 30.7743, val_MinusLogProbMetric: 30.7743

Epoch 546: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.4459 - MinusLogProbMetric: 30.4459 - val_loss: 30.7743 - val_MinusLogProbMetric: 30.7743 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 547/1000
2023-10-25 21:14:54.926 
Epoch 547/1000 
	 loss: 30.1336, MinusLogProbMetric: 30.1336, val_loss: 31.0919, val_MinusLogProbMetric: 31.0919

Epoch 547: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.1336 - MinusLogProbMetric: 30.1336 - val_loss: 31.0919 - val_MinusLogProbMetric: 31.0919 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 548/1000
2023-10-25 21:15:29.611 
Epoch 548/1000 
	 loss: 30.2557, MinusLogProbMetric: 30.2557, val_loss: 30.6770, val_MinusLogProbMetric: 30.6770

Epoch 548: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.2557 - MinusLogProbMetric: 30.2557 - val_loss: 30.6770 - val_MinusLogProbMetric: 30.6770 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 549/1000
2023-10-25 21:16:04.048 
Epoch 549/1000 
	 loss: 30.1857, MinusLogProbMetric: 30.1857, val_loss: 30.1036, val_MinusLogProbMetric: 30.1036

Epoch 549: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.1857 - MinusLogProbMetric: 30.1857 - val_loss: 30.1036 - val_MinusLogProbMetric: 30.1036 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 550/1000
2023-10-25 21:16:38.456 
Epoch 550/1000 
	 loss: 30.3539, MinusLogProbMetric: 30.3539, val_loss: 30.2932, val_MinusLogProbMetric: 30.2932

Epoch 550: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.3539 - MinusLogProbMetric: 30.3539 - val_loss: 30.2932 - val_MinusLogProbMetric: 30.2932 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 551/1000
2023-10-25 21:17:12.883 
Epoch 551/1000 
	 loss: 30.3971, MinusLogProbMetric: 30.3971, val_loss: 30.5035, val_MinusLogProbMetric: 30.5035

Epoch 551: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.3971 - MinusLogProbMetric: 30.3971 - val_loss: 30.5035 - val_MinusLogProbMetric: 30.5035 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 552/1000
2023-10-25 21:17:47.354 
Epoch 552/1000 
	 loss: 30.2156, MinusLogProbMetric: 30.2156, val_loss: 30.1478, val_MinusLogProbMetric: 30.1478

Epoch 552: val_loss did not improve from 30.00780
196/196 - 34s - loss: 30.2156 - MinusLogProbMetric: 30.2156 - val_loss: 30.1478 - val_MinusLogProbMetric: 30.1478 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 553/1000
2023-10-25 21:18:22.005 
Epoch 553/1000 
	 loss: 30.6163, MinusLogProbMetric: 30.6163, val_loss: 30.6602, val_MinusLogProbMetric: 30.6602

Epoch 553: val_loss did not improve from 30.00780
196/196 - 35s - loss: 30.6163 - MinusLogProbMetric: 30.6163 - val_loss: 30.6602 - val_MinusLogProbMetric: 30.6602 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 554/1000
2023-10-25 21:18:56.566 
Epoch 554/1000 
	 loss: 30.2580, MinusLogProbMetric: 30.2580, val_loss: 29.9731, val_MinusLogProbMetric: 29.9731

Epoch 554: val_loss improved from 30.00780 to 29.97313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.2580 - MinusLogProbMetric: 30.2580 - val_loss: 29.9731 - val_MinusLogProbMetric: 29.9731 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 555/1000
2023-10-25 21:19:31.610 
Epoch 555/1000 
	 loss: 30.2578, MinusLogProbMetric: 30.2578, val_loss: 30.7388, val_MinusLogProbMetric: 30.7388

Epoch 555: val_loss did not improve from 29.97313
196/196 - 34s - loss: 30.2578 - MinusLogProbMetric: 30.2578 - val_loss: 30.7388 - val_MinusLogProbMetric: 30.7388 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 556/1000
2023-10-25 21:20:06.197 
Epoch 556/1000 
	 loss: 30.1480, MinusLogProbMetric: 30.1480, val_loss: 31.1502, val_MinusLogProbMetric: 31.1502

Epoch 556: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.1480 - MinusLogProbMetric: 30.1480 - val_loss: 31.1502 - val_MinusLogProbMetric: 31.1502 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 557/1000
2023-10-25 21:20:40.973 
Epoch 557/1000 
	 loss: 30.2106, MinusLogProbMetric: 30.2106, val_loss: 30.9091, val_MinusLogProbMetric: 30.9091

Epoch 557: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.2106 - MinusLogProbMetric: 30.2106 - val_loss: 30.9091 - val_MinusLogProbMetric: 30.9091 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 558/1000
2023-10-25 21:21:15.587 
Epoch 558/1000 
	 loss: 30.7639, MinusLogProbMetric: 30.7639, val_loss: 30.5930, val_MinusLogProbMetric: 30.5930

Epoch 558: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.7639 - MinusLogProbMetric: 30.7639 - val_loss: 30.5930 - val_MinusLogProbMetric: 30.5930 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 559/1000
2023-10-25 21:21:49.965 
Epoch 559/1000 
	 loss: 30.2627, MinusLogProbMetric: 30.2627, val_loss: 30.6182, val_MinusLogProbMetric: 30.6182

Epoch 559: val_loss did not improve from 29.97313
196/196 - 34s - loss: 30.2627 - MinusLogProbMetric: 30.2627 - val_loss: 30.6182 - val_MinusLogProbMetric: 30.6182 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 560/1000
2023-10-25 21:22:24.699 
Epoch 560/1000 
	 loss: 30.3198, MinusLogProbMetric: 30.3198, val_loss: 30.7003, val_MinusLogProbMetric: 30.7003

Epoch 560: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.3198 - MinusLogProbMetric: 30.3198 - val_loss: 30.7003 - val_MinusLogProbMetric: 30.7003 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 561/1000
2023-10-25 21:22:59.207 
Epoch 561/1000 
	 loss: 30.1814, MinusLogProbMetric: 30.1814, val_loss: 30.6990, val_MinusLogProbMetric: 30.6990

Epoch 561: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.1814 - MinusLogProbMetric: 30.1814 - val_loss: 30.6990 - val_MinusLogProbMetric: 30.6990 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 562/1000
2023-10-25 21:23:33.443 
Epoch 562/1000 
	 loss: 30.4462, MinusLogProbMetric: 30.4462, val_loss: 31.1575, val_MinusLogProbMetric: 31.1575

Epoch 562: val_loss did not improve from 29.97313
196/196 - 34s - loss: 30.4462 - MinusLogProbMetric: 30.4462 - val_loss: 31.1575 - val_MinusLogProbMetric: 31.1575 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 563/1000
2023-10-25 21:24:08.122 
Epoch 563/1000 
	 loss: 30.2410, MinusLogProbMetric: 30.2410, val_loss: 30.6593, val_MinusLogProbMetric: 30.6593

Epoch 563: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.2410 - MinusLogProbMetric: 30.2410 - val_loss: 30.6593 - val_MinusLogProbMetric: 30.6593 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 564/1000
2023-10-25 21:24:42.548 
Epoch 564/1000 
	 loss: 31.1077, MinusLogProbMetric: 31.1077, val_loss: 31.6157, val_MinusLogProbMetric: 31.6157

Epoch 564: val_loss did not improve from 29.97313
196/196 - 34s - loss: 31.1077 - MinusLogProbMetric: 31.1077 - val_loss: 31.6157 - val_MinusLogProbMetric: 31.6157 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 565/1000
2023-10-25 21:25:16.923 
Epoch 565/1000 
	 loss: 30.5646, MinusLogProbMetric: 30.5646, val_loss: 30.0283, val_MinusLogProbMetric: 30.0283

Epoch 565: val_loss did not improve from 29.97313
196/196 - 34s - loss: 30.5646 - MinusLogProbMetric: 30.5646 - val_loss: 30.0283 - val_MinusLogProbMetric: 30.0283 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 566/1000
2023-10-25 21:25:51.408 
Epoch 566/1000 
	 loss: 30.3600, MinusLogProbMetric: 30.3600, val_loss: 30.0362, val_MinusLogProbMetric: 30.0362

Epoch 566: val_loss did not improve from 29.97313
196/196 - 34s - loss: 30.3600 - MinusLogProbMetric: 30.3600 - val_loss: 30.0362 - val_MinusLogProbMetric: 30.0362 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 567/1000
2023-10-25 21:26:25.758 
Epoch 567/1000 
	 loss: 30.4893, MinusLogProbMetric: 30.4893, val_loss: 30.5206, val_MinusLogProbMetric: 30.5206

Epoch 567: val_loss did not improve from 29.97313
196/196 - 34s - loss: 30.4893 - MinusLogProbMetric: 30.4893 - val_loss: 30.5206 - val_MinusLogProbMetric: 30.5206 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 568/1000
2023-10-25 21:27:00.302 
Epoch 568/1000 
	 loss: 30.1500, MinusLogProbMetric: 30.1500, val_loss: 30.9494, val_MinusLogProbMetric: 30.9494

Epoch 568: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.1500 - MinusLogProbMetric: 30.1500 - val_loss: 30.9494 - val_MinusLogProbMetric: 30.9494 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 569/1000
2023-10-25 21:27:35.101 
Epoch 569/1000 
	 loss: 30.2909, MinusLogProbMetric: 30.2909, val_loss: 30.2893, val_MinusLogProbMetric: 30.2893

Epoch 569: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.2909 - MinusLogProbMetric: 30.2909 - val_loss: 30.2893 - val_MinusLogProbMetric: 30.2893 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 570/1000
2023-10-25 21:28:10.053 
Epoch 570/1000 
	 loss: 30.3061, MinusLogProbMetric: 30.3061, val_loss: 32.3640, val_MinusLogProbMetric: 32.3640

Epoch 570: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.3061 - MinusLogProbMetric: 30.3061 - val_loss: 32.3640 - val_MinusLogProbMetric: 32.3640 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 571/1000
2023-10-25 21:28:44.907 
Epoch 571/1000 
	 loss: 30.2486, MinusLogProbMetric: 30.2486, val_loss: 30.0954, val_MinusLogProbMetric: 30.0954

Epoch 571: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.2486 - MinusLogProbMetric: 30.2486 - val_loss: 30.0954 - val_MinusLogProbMetric: 30.0954 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 572/1000
2023-10-25 21:29:19.568 
Epoch 572/1000 
	 loss: 30.1488, MinusLogProbMetric: 30.1488, val_loss: 30.4037, val_MinusLogProbMetric: 30.4037

Epoch 572: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.1488 - MinusLogProbMetric: 30.1488 - val_loss: 30.4037 - val_MinusLogProbMetric: 30.4037 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 573/1000
2023-10-25 21:29:54.515 
Epoch 573/1000 
	 loss: 30.0067, MinusLogProbMetric: 30.0067, val_loss: 30.1324, val_MinusLogProbMetric: 30.1324

Epoch 573: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.0067 - MinusLogProbMetric: 30.0067 - val_loss: 30.1324 - val_MinusLogProbMetric: 30.1324 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 574/1000
2023-10-25 21:30:29.407 
Epoch 574/1000 
	 loss: 30.5026, MinusLogProbMetric: 30.5026, val_loss: 31.1500, val_MinusLogProbMetric: 31.1500

Epoch 574: val_loss did not improve from 29.97313
196/196 - 35s - loss: 30.5026 - MinusLogProbMetric: 30.5026 - val_loss: 31.1500 - val_MinusLogProbMetric: 31.1500 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 575/1000
2023-10-25 21:31:04.060 
Epoch 575/1000 
	 loss: 30.3347, MinusLogProbMetric: 30.3347, val_loss: 29.9255, val_MinusLogProbMetric: 29.9255

Epoch 575: val_loss improved from 29.97313 to 29.92554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.3347 - MinusLogProbMetric: 30.3347 - val_loss: 29.9255 - val_MinusLogProbMetric: 29.9255 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 576/1000
2023-10-25 21:31:39.375 
Epoch 576/1000 
	 loss: 30.6904, MinusLogProbMetric: 30.6904, val_loss: 30.6339, val_MinusLogProbMetric: 30.6339

Epoch 576: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.6904 - MinusLogProbMetric: 30.6904 - val_loss: 30.6339 - val_MinusLogProbMetric: 30.6339 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 577/1000
2023-10-25 21:32:13.882 
Epoch 577/1000 
	 loss: 30.0516, MinusLogProbMetric: 30.0516, val_loss: 30.5251, val_MinusLogProbMetric: 30.5251

Epoch 577: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0516 - MinusLogProbMetric: 30.0516 - val_loss: 30.5251 - val_MinusLogProbMetric: 30.5251 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 578/1000
2023-10-25 21:32:48.882 
Epoch 578/1000 
	 loss: 30.0286, MinusLogProbMetric: 30.0286, val_loss: 30.0964, val_MinusLogProbMetric: 30.0964

Epoch 578: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0286 - MinusLogProbMetric: 30.0286 - val_loss: 30.0964 - val_MinusLogProbMetric: 30.0964 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 579/1000
2023-10-25 21:33:23.876 
Epoch 579/1000 
	 loss: 30.2203, MinusLogProbMetric: 30.2203, val_loss: 30.4392, val_MinusLogProbMetric: 30.4392

Epoch 579: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.2203 - MinusLogProbMetric: 30.2203 - val_loss: 30.4392 - val_MinusLogProbMetric: 30.4392 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 580/1000
2023-10-25 21:33:58.475 
Epoch 580/1000 
	 loss: 30.2153, MinusLogProbMetric: 30.2153, val_loss: 30.2211, val_MinusLogProbMetric: 30.2211

Epoch 580: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.2153 - MinusLogProbMetric: 30.2153 - val_loss: 30.2211 - val_MinusLogProbMetric: 30.2211 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 581/1000
2023-10-25 21:34:33.319 
Epoch 581/1000 
	 loss: 30.5592, MinusLogProbMetric: 30.5592, val_loss: 30.6385, val_MinusLogProbMetric: 30.6385

Epoch 581: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.5592 - MinusLogProbMetric: 30.5592 - val_loss: 30.6385 - val_MinusLogProbMetric: 30.6385 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 582/1000
2023-10-25 21:35:08.060 
Epoch 582/1000 
	 loss: 30.2400, MinusLogProbMetric: 30.2400, val_loss: 29.9527, val_MinusLogProbMetric: 29.9527

Epoch 582: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.2400 - MinusLogProbMetric: 30.2400 - val_loss: 29.9527 - val_MinusLogProbMetric: 29.9527 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 583/1000
2023-10-25 21:35:42.472 
Epoch 583/1000 
	 loss: 30.3244, MinusLogProbMetric: 30.3244, val_loss: 30.7560, val_MinusLogProbMetric: 30.7560

Epoch 583: val_loss did not improve from 29.92554
196/196 - 34s - loss: 30.3244 - MinusLogProbMetric: 30.3244 - val_loss: 30.7560 - val_MinusLogProbMetric: 30.7560 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 584/1000
2023-10-25 21:36:17.223 
Epoch 584/1000 
	 loss: 30.3139, MinusLogProbMetric: 30.3139, val_loss: 30.4591, val_MinusLogProbMetric: 30.4591

Epoch 584: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.3139 - MinusLogProbMetric: 30.3139 - val_loss: 30.4591 - val_MinusLogProbMetric: 30.4591 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 585/1000
2023-10-25 21:36:51.879 
Epoch 585/1000 
	 loss: 30.5902, MinusLogProbMetric: 30.5902, val_loss: 30.7577, val_MinusLogProbMetric: 30.7577

Epoch 585: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.5902 - MinusLogProbMetric: 30.5902 - val_loss: 30.7577 - val_MinusLogProbMetric: 30.7577 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 586/1000
2023-10-25 21:37:26.896 
Epoch 586/1000 
	 loss: 30.3509, MinusLogProbMetric: 30.3509, val_loss: 31.4062, val_MinusLogProbMetric: 31.4062

Epoch 586: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.3509 - MinusLogProbMetric: 30.3509 - val_loss: 31.4062 - val_MinusLogProbMetric: 31.4062 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 587/1000
2023-10-25 21:38:01.311 
Epoch 587/1000 
	 loss: 30.2921, MinusLogProbMetric: 30.2921, val_loss: 30.5780, val_MinusLogProbMetric: 30.5780

Epoch 587: val_loss did not improve from 29.92554
196/196 - 34s - loss: 30.2921 - MinusLogProbMetric: 30.2921 - val_loss: 30.5780 - val_MinusLogProbMetric: 30.5780 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 588/1000
2023-10-25 21:38:35.836 
Epoch 588/1000 
	 loss: 30.0843, MinusLogProbMetric: 30.0843, val_loss: 30.2236, val_MinusLogProbMetric: 30.2236

Epoch 588: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0843 - MinusLogProbMetric: 30.0843 - val_loss: 30.2236 - val_MinusLogProbMetric: 30.2236 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 589/1000
2023-10-25 21:39:10.616 
Epoch 589/1000 
	 loss: 30.6000, MinusLogProbMetric: 30.6000, val_loss: 30.2562, val_MinusLogProbMetric: 30.2562

Epoch 589: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.6000 - MinusLogProbMetric: 30.6000 - val_loss: 30.2562 - val_MinusLogProbMetric: 30.2562 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 590/1000
2023-10-25 21:39:45.276 
Epoch 590/1000 
	 loss: 30.0782, MinusLogProbMetric: 30.0782, val_loss: 31.2310, val_MinusLogProbMetric: 31.2310

Epoch 590: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0782 - MinusLogProbMetric: 30.0782 - val_loss: 31.2310 - val_MinusLogProbMetric: 31.2310 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 591/1000
2023-10-25 21:40:19.997 
Epoch 591/1000 
	 loss: 30.8663, MinusLogProbMetric: 30.8663, val_loss: 30.8655, val_MinusLogProbMetric: 30.8655

Epoch 591: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.8663 - MinusLogProbMetric: 30.8663 - val_loss: 30.8655 - val_MinusLogProbMetric: 30.8655 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 592/1000
2023-10-25 21:40:54.991 
Epoch 592/1000 
	 loss: 30.1580, MinusLogProbMetric: 30.1580, val_loss: 31.1084, val_MinusLogProbMetric: 31.1084

Epoch 592: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.1580 - MinusLogProbMetric: 30.1580 - val_loss: 31.1084 - val_MinusLogProbMetric: 31.1084 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 593/1000
2023-10-25 21:41:29.696 
Epoch 593/1000 
	 loss: 30.3289, MinusLogProbMetric: 30.3289, val_loss: 30.1566, val_MinusLogProbMetric: 30.1566

Epoch 593: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.3289 - MinusLogProbMetric: 30.3289 - val_loss: 30.1566 - val_MinusLogProbMetric: 30.1566 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 594/1000
2023-10-25 21:42:04.757 
Epoch 594/1000 
	 loss: 30.3421, MinusLogProbMetric: 30.3421, val_loss: 30.4739, val_MinusLogProbMetric: 30.4739

Epoch 594: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.3421 - MinusLogProbMetric: 30.3421 - val_loss: 30.4739 - val_MinusLogProbMetric: 30.4739 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 595/1000
2023-10-25 21:42:39.419 
Epoch 595/1000 
	 loss: 30.0615, MinusLogProbMetric: 30.0615, val_loss: 30.9207, val_MinusLogProbMetric: 30.9207

Epoch 595: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0615 - MinusLogProbMetric: 30.0615 - val_loss: 30.9207 - val_MinusLogProbMetric: 30.9207 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 596/1000
2023-10-25 21:43:14.363 
Epoch 596/1000 
	 loss: 29.9927, MinusLogProbMetric: 29.9927, val_loss: 30.0948, val_MinusLogProbMetric: 30.0948

Epoch 596: val_loss did not improve from 29.92554
196/196 - 35s - loss: 29.9927 - MinusLogProbMetric: 29.9927 - val_loss: 30.0948 - val_MinusLogProbMetric: 30.0948 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 597/1000
2023-10-25 21:43:49.248 
Epoch 597/1000 
	 loss: 30.0200, MinusLogProbMetric: 30.0200, val_loss: 30.3362, val_MinusLogProbMetric: 30.3362

Epoch 597: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0200 - MinusLogProbMetric: 30.0200 - val_loss: 30.3362 - val_MinusLogProbMetric: 30.3362 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 598/1000
2023-10-25 21:44:24.208 
Epoch 598/1000 
	 loss: 30.2521, MinusLogProbMetric: 30.2521, val_loss: 30.3125, val_MinusLogProbMetric: 30.3125

Epoch 598: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.2521 - MinusLogProbMetric: 30.2521 - val_loss: 30.3125 - val_MinusLogProbMetric: 30.3125 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 599/1000
2023-10-25 21:44:59.360 
Epoch 599/1000 
	 loss: 29.9267, MinusLogProbMetric: 29.9267, val_loss: 30.2410, val_MinusLogProbMetric: 30.2410

Epoch 599: val_loss did not improve from 29.92554
196/196 - 35s - loss: 29.9267 - MinusLogProbMetric: 29.9267 - val_loss: 30.2410 - val_MinusLogProbMetric: 30.2410 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 600/1000
2023-10-25 21:45:34.077 
Epoch 600/1000 
	 loss: 30.3708, MinusLogProbMetric: 30.3708, val_loss: 30.1535, val_MinusLogProbMetric: 30.1535

Epoch 600: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.3708 - MinusLogProbMetric: 30.3708 - val_loss: 30.1535 - val_MinusLogProbMetric: 30.1535 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 601/1000
2023-10-25 21:46:08.870 
Epoch 601/1000 
	 loss: 31.0379, MinusLogProbMetric: 31.0379, val_loss: 30.5528, val_MinusLogProbMetric: 30.5528

Epoch 601: val_loss did not improve from 29.92554
196/196 - 35s - loss: 31.0379 - MinusLogProbMetric: 31.0379 - val_loss: 30.5528 - val_MinusLogProbMetric: 30.5528 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 602/1000
2023-10-25 21:46:43.667 
Epoch 602/1000 
	 loss: 30.1752, MinusLogProbMetric: 30.1752, val_loss: 30.1017, val_MinusLogProbMetric: 30.1017

Epoch 602: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.1752 - MinusLogProbMetric: 30.1752 - val_loss: 30.1017 - val_MinusLogProbMetric: 30.1017 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 603/1000
2023-10-25 21:47:18.442 
Epoch 603/1000 
	 loss: 30.0696, MinusLogProbMetric: 30.0696, val_loss: 30.7934, val_MinusLogProbMetric: 30.7934

Epoch 603: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0696 - MinusLogProbMetric: 30.0696 - val_loss: 30.7934 - val_MinusLogProbMetric: 30.7934 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 604/1000
2023-10-25 21:47:53.377 
Epoch 604/1000 
	 loss: 30.5239, MinusLogProbMetric: 30.5239, val_loss: 30.2367, val_MinusLogProbMetric: 30.2367

Epoch 604: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.5239 - MinusLogProbMetric: 30.5239 - val_loss: 30.2367 - val_MinusLogProbMetric: 30.2367 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 605/1000
2023-10-25 21:48:28.042 
Epoch 605/1000 
	 loss: 30.0825, MinusLogProbMetric: 30.0825, val_loss: 31.4688, val_MinusLogProbMetric: 31.4688

Epoch 605: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0825 - MinusLogProbMetric: 30.0825 - val_loss: 31.4688 - val_MinusLogProbMetric: 31.4688 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 606/1000
2023-10-25 21:49:02.736 
Epoch 606/1000 
	 loss: 30.3080, MinusLogProbMetric: 30.3080, val_loss: 30.3589, val_MinusLogProbMetric: 30.3589

Epoch 606: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.3080 - MinusLogProbMetric: 30.3080 - val_loss: 30.3589 - val_MinusLogProbMetric: 30.3589 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 607/1000
2023-10-25 21:49:37.697 
Epoch 607/1000 
	 loss: 30.4190, MinusLogProbMetric: 30.4190, val_loss: 30.8407, val_MinusLogProbMetric: 30.8407

Epoch 607: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.4190 - MinusLogProbMetric: 30.4190 - val_loss: 30.8407 - val_MinusLogProbMetric: 30.8407 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 608/1000
2023-10-25 21:50:12.603 
Epoch 608/1000 
	 loss: 30.1506, MinusLogProbMetric: 30.1506, val_loss: 30.1251, val_MinusLogProbMetric: 30.1251

Epoch 608: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.1506 - MinusLogProbMetric: 30.1506 - val_loss: 30.1251 - val_MinusLogProbMetric: 30.1251 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 609/1000
2023-10-25 21:50:47.382 
Epoch 609/1000 
	 loss: 30.0346, MinusLogProbMetric: 30.0346, val_loss: 30.0792, val_MinusLogProbMetric: 30.0792

Epoch 609: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0346 - MinusLogProbMetric: 30.0346 - val_loss: 30.0792 - val_MinusLogProbMetric: 30.0792 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 610/1000
2023-10-25 21:51:22.393 
Epoch 610/1000 
	 loss: 30.2859, MinusLogProbMetric: 30.2859, val_loss: 30.3947, val_MinusLogProbMetric: 30.3947

Epoch 610: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.2859 - MinusLogProbMetric: 30.2859 - val_loss: 30.3947 - val_MinusLogProbMetric: 30.3947 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 611/1000
2023-10-25 21:51:57.071 
Epoch 611/1000 
	 loss: 30.0055, MinusLogProbMetric: 30.0055, val_loss: 30.2687, val_MinusLogProbMetric: 30.2687

Epoch 611: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.0055 - MinusLogProbMetric: 30.0055 - val_loss: 30.2687 - val_MinusLogProbMetric: 30.2687 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 612/1000
2023-10-25 21:52:31.889 
Epoch 612/1000 
	 loss: 30.2518, MinusLogProbMetric: 30.2518, val_loss: 29.9900, val_MinusLogProbMetric: 29.9900

Epoch 612: val_loss did not improve from 29.92554
196/196 - 35s - loss: 30.2518 - MinusLogProbMetric: 30.2518 - val_loss: 29.9900 - val_MinusLogProbMetric: 29.9900 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 613/1000
2023-10-25 21:53:06.609 
Epoch 613/1000 
	 loss: 30.0108, MinusLogProbMetric: 30.0108, val_loss: 29.8341, val_MinusLogProbMetric: 29.8341

Epoch 613: val_loss improved from 29.92554 to 29.83414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.0108 - MinusLogProbMetric: 30.0108 - val_loss: 29.8341 - val_MinusLogProbMetric: 29.8341 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 614/1000
2023-10-25 21:53:41.949 
Epoch 614/1000 
	 loss: 29.9417, MinusLogProbMetric: 29.9417, val_loss: 29.8228, val_MinusLogProbMetric: 29.8228

Epoch 614: val_loss improved from 29.83414 to 29.82277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 29.9417 - MinusLogProbMetric: 29.9417 - val_loss: 29.8228 - val_MinusLogProbMetric: 29.8228 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 615/1000
2023-10-25 21:54:17.749 
Epoch 615/1000 
	 loss: 29.9513, MinusLogProbMetric: 29.9513, val_loss: 30.5845, val_MinusLogProbMetric: 30.5845

Epoch 615: val_loss did not improve from 29.82277
196/196 - 35s - loss: 29.9513 - MinusLogProbMetric: 29.9513 - val_loss: 30.5845 - val_MinusLogProbMetric: 30.5845 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 616/1000
2023-10-25 21:54:52.594 
Epoch 616/1000 
	 loss: 30.3526, MinusLogProbMetric: 30.3526, val_loss: 30.3015, val_MinusLogProbMetric: 30.3015

Epoch 616: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.3526 - MinusLogProbMetric: 30.3526 - val_loss: 30.3015 - val_MinusLogProbMetric: 30.3015 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 617/1000
2023-10-25 21:55:27.327 
Epoch 617/1000 
	 loss: 29.9719, MinusLogProbMetric: 29.9719, val_loss: 31.1056, val_MinusLogProbMetric: 31.1056

Epoch 617: val_loss did not improve from 29.82277
196/196 - 35s - loss: 29.9719 - MinusLogProbMetric: 29.9719 - val_loss: 31.1056 - val_MinusLogProbMetric: 31.1056 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 618/1000
2023-10-25 21:56:02.198 
Epoch 618/1000 
	 loss: 30.0658, MinusLogProbMetric: 30.0658, val_loss: 30.3512, val_MinusLogProbMetric: 30.3512

Epoch 618: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.0658 - MinusLogProbMetric: 30.0658 - val_loss: 30.3512 - val_MinusLogProbMetric: 30.3512 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 619/1000
2023-10-25 21:56:36.993 
Epoch 619/1000 
	 loss: 30.1300, MinusLogProbMetric: 30.1300, val_loss: 30.3576, val_MinusLogProbMetric: 30.3576

Epoch 619: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.1300 - MinusLogProbMetric: 30.1300 - val_loss: 30.3576 - val_MinusLogProbMetric: 30.3576 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 620/1000
2023-10-25 21:57:12.061 
Epoch 620/1000 
	 loss: 30.6882, MinusLogProbMetric: 30.6882, val_loss: 30.4899, val_MinusLogProbMetric: 30.4899

Epoch 620: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.6882 - MinusLogProbMetric: 30.6882 - val_loss: 30.4899 - val_MinusLogProbMetric: 30.4899 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 621/1000
2023-10-25 21:57:47.040 
Epoch 621/1000 
	 loss: 30.0680, MinusLogProbMetric: 30.0680, val_loss: 30.0926, val_MinusLogProbMetric: 30.0926

Epoch 621: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.0680 - MinusLogProbMetric: 30.0680 - val_loss: 30.0926 - val_MinusLogProbMetric: 30.0926 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 622/1000
2023-10-25 21:58:21.923 
Epoch 622/1000 
	 loss: 31.3007, MinusLogProbMetric: 31.3007, val_loss: 30.1074, val_MinusLogProbMetric: 30.1074

Epoch 622: val_loss did not improve from 29.82277
196/196 - 35s - loss: 31.3007 - MinusLogProbMetric: 31.3007 - val_loss: 30.1074 - val_MinusLogProbMetric: 30.1074 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 623/1000
2023-10-25 21:58:56.703 
Epoch 623/1000 
	 loss: 30.0334, MinusLogProbMetric: 30.0334, val_loss: 30.1535, val_MinusLogProbMetric: 30.1535

Epoch 623: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.0334 - MinusLogProbMetric: 30.0334 - val_loss: 30.1535 - val_MinusLogProbMetric: 30.1535 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 624/1000
2023-10-25 21:59:31.434 
Epoch 624/1000 
	 loss: 30.1071, MinusLogProbMetric: 30.1071, val_loss: 29.8639, val_MinusLogProbMetric: 29.8639

Epoch 624: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.1071 - MinusLogProbMetric: 30.1071 - val_loss: 29.8639 - val_MinusLogProbMetric: 29.8639 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 625/1000
2023-10-25 22:00:06.204 
Epoch 625/1000 
	 loss: 30.2057, MinusLogProbMetric: 30.2057, val_loss: 30.2471, val_MinusLogProbMetric: 30.2471

Epoch 625: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.2057 - MinusLogProbMetric: 30.2057 - val_loss: 30.2471 - val_MinusLogProbMetric: 30.2471 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 626/1000
2023-10-25 22:00:40.457 
Epoch 626/1000 
	 loss: 29.9339, MinusLogProbMetric: 29.9339, val_loss: 32.6389, val_MinusLogProbMetric: 32.6389

Epoch 626: val_loss did not improve from 29.82277
196/196 - 34s - loss: 29.9339 - MinusLogProbMetric: 29.9339 - val_loss: 32.6389 - val_MinusLogProbMetric: 32.6389 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 627/1000
2023-10-25 22:01:15.435 
Epoch 627/1000 
	 loss: 30.2889, MinusLogProbMetric: 30.2889, val_loss: 31.1234, val_MinusLogProbMetric: 31.1234

Epoch 627: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.2889 - MinusLogProbMetric: 30.2889 - val_loss: 31.1234 - val_MinusLogProbMetric: 31.1234 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 628/1000
2023-10-25 22:01:50.456 
Epoch 628/1000 
	 loss: 30.0317, MinusLogProbMetric: 30.0317, val_loss: 30.4479, val_MinusLogProbMetric: 30.4479

Epoch 628: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.0317 - MinusLogProbMetric: 30.0317 - val_loss: 30.4479 - val_MinusLogProbMetric: 30.4479 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 629/1000
2023-10-25 22:02:25.506 
Epoch 629/1000 
	 loss: 30.2016, MinusLogProbMetric: 30.2016, val_loss: 30.2230, val_MinusLogProbMetric: 30.2230

Epoch 629: val_loss did not improve from 29.82277
196/196 - 35s - loss: 30.2016 - MinusLogProbMetric: 30.2016 - val_loss: 30.2230 - val_MinusLogProbMetric: 30.2230 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 630/1000
2023-10-25 22:03:00.623 
Epoch 630/1000 
	 loss: 29.9840, MinusLogProbMetric: 29.9840, val_loss: 31.5455, val_MinusLogProbMetric: 31.5455

Epoch 630: val_loss did not improve from 29.82277
196/196 - 35s - loss: 29.9840 - MinusLogProbMetric: 29.9840 - val_loss: 31.5455 - val_MinusLogProbMetric: 31.5455 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 631/1000
2023-10-25 22:03:35.751 
Epoch 631/1000 
	 loss: 29.8947, MinusLogProbMetric: 29.8947, val_loss: 29.9422, val_MinusLogProbMetric: 29.9422

Epoch 631: val_loss did not improve from 29.82277
196/196 - 35s - loss: 29.8947 - MinusLogProbMetric: 29.8947 - val_loss: 29.9422 - val_MinusLogProbMetric: 29.9422 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 632/1000
2023-10-25 22:04:10.933 
Epoch 632/1000 
	 loss: 30.5602, MinusLogProbMetric: 30.5602, val_loss: 29.7851, val_MinusLogProbMetric: 29.7851

Epoch 632: val_loss improved from 29.82277 to 29.78508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 30.5602 - MinusLogProbMetric: 30.5602 - val_loss: 29.7851 - val_MinusLogProbMetric: 29.7851 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 633/1000
2023-10-25 22:04:46.222 
Epoch 633/1000 
	 loss: 30.2412, MinusLogProbMetric: 30.2412, val_loss: 30.1258, val_MinusLogProbMetric: 30.1258

Epoch 633: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.2412 - MinusLogProbMetric: 30.2412 - val_loss: 30.1258 - val_MinusLogProbMetric: 30.1258 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 634/1000
2023-10-25 22:05:21.294 
Epoch 634/1000 
	 loss: 31.3750, MinusLogProbMetric: 31.3750, val_loss: 35.3645, val_MinusLogProbMetric: 35.3645

Epoch 634: val_loss did not improve from 29.78508
196/196 - 35s - loss: 31.3750 - MinusLogProbMetric: 31.3750 - val_loss: 35.3645 - val_MinusLogProbMetric: 35.3645 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 635/1000
2023-10-25 22:05:56.625 
Epoch 635/1000 
	 loss: 31.0924, MinusLogProbMetric: 31.0924, val_loss: 30.5894, val_MinusLogProbMetric: 30.5894

Epoch 635: val_loss did not improve from 29.78508
196/196 - 35s - loss: 31.0924 - MinusLogProbMetric: 31.0924 - val_loss: 30.5894 - val_MinusLogProbMetric: 30.5894 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 636/1000
2023-10-25 22:06:31.690 
Epoch 636/1000 
	 loss: 30.3134, MinusLogProbMetric: 30.3134, val_loss: 30.4783, val_MinusLogProbMetric: 30.4783

Epoch 636: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.3134 - MinusLogProbMetric: 30.3134 - val_loss: 30.4783 - val_MinusLogProbMetric: 30.4783 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 637/1000
2023-10-25 22:07:06.527 
Epoch 637/1000 
	 loss: 30.3698, MinusLogProbMetric: 30.3698, val_loss: 34.8874, val_MinusLogProbMetric: 34.8874

Epoch 637: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.3698 - MinusLogProbMetric: 30.3698 - val_loss: 34.8874 - val_MinusLogProbMetric: 34.8874 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 638/1000
2023-10-25 22:07:41.425 
Epoch 638/1000 
	 loss: 30.1998, MinusLogProbMetric: 30.1998, val_loss: 30.3973, val_MinusLogProbMetric: 30.3973

Epoch 638: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1998 - MinusLogProbMetric: 30.1998 - val_loss: 30.3973 - val_MinusLogProbMetric: 30.3973 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 639/1000
2023-10-25 22:08:16.271 
Epoch 639/1000 
	 loss: 30.0576, MinusLogProbMetric: 30.0576, val_loss: 31.0161, val_MinusLogProbMetric: 31.0161

Epoch 639: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.0576 - MinusLogProbMetric: 30.0576 - val_loss: 31.0161 - val_MinusLogProbMetric: 31.0161 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 640/1000
2023-10-25 22:08:51.291 
Epoch 640/1000 
	 loss: 29.9872, MinusLogProbMetric: 29.9872, val_loss: 30.1318, val_MinusLogProbMetric: 30.1318

Epoch 640: val_loss did not improve from 29.78508
196/196 - 35s - loss: 29.9872 - MinusLogProbMetric: 29.9872 - val_loss: 30.1318 - val_MinusLogProbMetric: 30.1318 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 641/1000
2023-10-25 22:09:26.272 
Epoch 641/1000 
	 loss: 30.0169, MinusLogProbMetric: 30.0169, val_loss: 30.4996, val_MinusLogProbMetric: 30.4996

Epoch 641: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.0169 - MinusLogProbMetric: 30.0169 - val_loss: 30.4996 - val_MinusLogProbMetric: 30.4996 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 642/1000
2023-10-25 22:10:01.076 
Epoch 642/1000 
	 loss: 30.1168, MinusLogProbMetric: 30.1168, val_loss: 30.6974, val_MinusLogProbMetric: 30.6974

Epoch 642: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1168 - MinusLogProbMetric: 30.1168 - val_loss: 30.6974 - val_MinusLogProbMetric: 30.6974 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 643/1000
2023-10-25 22:10:36.173 
Epoch 643/1000 
	 loss: 30.3586, MinusLogProbMetric: 30.3586, val_loss: 36.0340, val_MinusLogProbMetric: 36.0340

Epoch 643: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.3586 - MinusLogProbMetric: 30.3586 - val_loss: 36.0340 - val_MinusLogProbMetric: 36.0340 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 644/1000
2023-10-25 22:11:10.993 
Epoch 644/1000 
	 loss: 30.7766, MinusLogProbMetric: 30.7766, val_loss: 30.5259, val_MinusLogProbMetric: 30.5259

Epoch 644: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.7766 - MinusLogProbMetric: 30.7766 - val_loss: 30.5259 - val_MinusLogProbMetric: 30.5259 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 645/1000
2023-10-25 22:11:46.429 
Epoch 645/1000 
	 loss: 30.0902, MinusLogProbMetric: 30.0902, val_loss: 30.4095, val_MinusLogProbMetric: 30.4095

Epoch 645: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.0902 - MinusLogProbMetric: 30.0902 - val_loss: 30.4095 - val_MinusLogProbMetric: 30.4095 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 646/1000
2023-10-25 22:12:21.562 
Epoch 646/1000 
	 loss: 30.1919, MinusLogProbMetric: 30.1919, val_loss: 30.0783, val_MinusLogProbMetric: 30.0783

Epoch 646: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1919 - MinusLogProbMetric: 30.1919 - val_loss: 30.0783 - val_MinusLogProbMetric: 30.0783 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 647/1000
2023-10-25 22:12:56.577 
Epoch 647/1000 
	 loss: 30.0395, MinusLogProbMetric: 30.0395, val_loss: 29.7956, val_MinusLogProbMetric: 29.7956

Epoch 647: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.0395 - MinusLogProbMetric: 30.0395 - val_loss: 29.7956 - val_MinusLogProbMetric: 29.7956 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 648/1000
2023-10-25 22:13:31.527 
Epoch 648/1000 
	 loss: 29.9073, MinusLogProbMetric: 29.9073, val_loss: 30.2865, val_MinusLogProbMetric: 30.2865

Epoch 648: val_loss did not improve from 29.78508
196/196 - 35s - loss: 29.9073 - MinusLogProbMetric: 29.9073 - val_loss: 30.2865 - val_MinusLogProbMetric: 30.2865 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 649/1000
2023-10-25 22:14:06.632 
Epoch 649/1000 
	 loss: 30.1895, MinusLogProbMetric: 30.1895, val_loss: 30.0780, val_MinusLogProbMetric: 30.0780

Epoch 649: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1895 - MinusLogProbMetric: 30.1895 - val_loss: 30.0780 - val_MinusLogProbMetric: 30.0780 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 650/1000
2023-10-25 22:14:41.723 
Epoch 650/1000 
	 loss: 30.0064, MinusLogProbMetric: 30.0064, val_loss: 31.3661, val_MinusLogProbMetric: 31.3661

Epoch 650: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.0064 - MinusLogProbMetric: 30.0064 - val_loss: 31.3661 - val_MinusLogProbMetric: 31.3661 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 651/1000
2023-10-25 22:15:17.005 
Epoch 651/1000 
	 loss: 30.2402, MinusLogProbMetric: 30.2402, val_loss: 30.1678, val_MinusLogProbMetric: 30.1678

Epoch 651: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.2402 - MinusLogProbMetric: 30.2402 - val_loss: 30.1678 - val_MinusLogProbMetric: 30.1678 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 652/1000
2023-10-25 22:15:52.048 
Epoch 652/1000 
	 loss: 30.1545, MinusLogProbMetric: 30.1545, val_loss: 30.5521, val_MinusLogProbMetric: 30.5521

Epoch 652: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1545 - MinusLogProbMetric: 30.1545 - val_loss: 30.5521 - val_MinusLogProbMetric: 30.5521 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 653/1000
2023-10-25 22:16:26.984 
Epoch 653/1000 
	 loss: 30.1801, MinusLogProbMetric: 30.1801, val_loss: 29.9709, val_MinusLogProbMetric: 29.9709

Epoch 653: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1801 - MinusLogProbMetric: 30.1801 - val_loss: 29.9709 - val_MinusLogProbMetric: 29.9709 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 654/1000
2023-10-25 22:17:02.322 
Epoch 654/1000 
	 loss: 30.1115, MinusLogProbMetric: 30.1115, val_loss: 30.0733, val_MinusLogProbMetric: 30.0733

Epoch 654: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1115 - MinusLogProbMetric: 30.1115 - val_loss: 30.0733 - val_MinusLogProbMetric: 30.0733 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 655/1000
2023-10-25 22:17:37.309 
Epoch 655/1000 
	 loss: 29.9283, MinusLogProbMetric: 29.9283, val_loss: 30.1730, val_MinusLogProbMetric: 30.1730

Epoch 655: val_loss did not improve from 29.78508
196/196 - 35s - loss: 29.9283 - MinusLogProbMetric: 29.9283 - val_loss: 30.1730 - val_MinusLogProbMetric: 30.1730 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 656/1000
2023-10-25 22:18:12.349 
Epoch 656/1000 
	 loss: 29.9978, MinusLogProbMetric: 29.9978, val_loss: 32.0519, val_MinusLogProbMetric: 32.0519

Epoch 656: val_loss did not improve from 29.78508
196/196 - 35s - loss: 29.9978 - MinusLogProbMetric: 29.9978 - val_loss: 32.0519 - val_MinusLogProbMetric: 32.0519 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 657/1000
2023-10-25 22:18:47.094 
Epoch 657/1000 
	 loss: 30.2745, MinusLogProbMetric: 30.2745, val_loss: 30.2760, val_MinusLogProbMetric: 30.2760

Epoch 657: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.2745 - MinusLogProbMetric: 30.2745 - val_loss: 30.2760 - val_MinusLogProbMetric: 30.2760 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 658/1000
2023-10-25 22:19:22.334 
Epoch 658/1000 
	 loss: 29.9738, MinusLogProbMetric: 29.9738, val_loss: 30.9205, val_MinusLogProbMetric: 30.9205

Epoch 658: val_loss did not improve from 29.78508
196/196 - 35s - loss: 29.9738 - MinusLogProbMetric: 29.9738 - val_loss: 30.9205 - val_MinusLogProbMetric: 30.9205 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 659/1000
2023-10-25 22:19:57.295 
Epoch 659/1000 
	 loss: 30.4352, MinusLogProbMetric: 30.4352, val_loss: 30.2549, val_MinusLogProbMetric: 30.2549

Epoch 659: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.4352 - MinusLogProbMetric: 30.4352 - val_loss: 30.2549 - val_MinusLogProbMetric: 30.2549 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 660/1000
2023-10-25 22:20:32.387 
Epoch 660/1000 
	 loss: 30.0783, MinusLogProbMetric: 30.0783, val_loss: 30.5129, val_MinusLogProbMetric: 30.5129

Epoch 660: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.0783 - MinusLogProbMetric: 30.0783 - val_loss: 30.5129 - val_MinusLogProbMetric: 30.5129 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 661/1000
2023-10-25 22:21:07.255 
Epoch 661/1000 
	 loss: 30.1898, MinusLogProbMetric: 30.1898, val_loss: 29.8015, val_MinusLogProbMetric: 29.8015

Epoch 661: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1898 - MinusLogProbMetric: 30.1898 - val_loss: 29.8015 - val_MinusLogProbMetric: 29.8015 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 662/1000
2023-10-25 22:21:42.096 
Epoch 662/1000 
	 loss: 30.1642, MinusLogProbMetric: 30.1642, val_loss: 30.2511, val_MinusLogProbMetric: 30.2511

Epoch 662: val_loss did not improve from 29.78508
196/196 - 35s - loss: 30.1642 - MinusLogProbMetric: 30.1642 - val_loss: 30.2511 - val_MinusLogProbMetric: 30.2511 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 663/1000
2023-10-25 22:22:17.171 
Epoch 663/1000 
	 loss: 30.0563, MinusLogProbMetric: 30.0563, val_loss: 29.6871, val_MinusLogProbMetric: 29.6871

Epoch 663: val_loss improved from 29.78508 to 29.68712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 30.0563 - MinusLogProbMetric: 30.0563 - val_loss: 29.6871 - val_MinusLogProbMetric: 29.6871 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 664/1000
2023-10-25 22:22:52.562 
Epoch 664/1000 
	 loss: 29.9362, MinusLogProbMetric: 29.9362, val_loss: 30.5584, val_MinusLogProbMetric: 30.5584

Epoch 664: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9362 - MinusLogProbMetric: 29.9362 - val_loss: 30.5584 - val_MinusLogProbMetric: 30.5584 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 665/1000
2023-10-25 22:23:27.559 
Epoch 665/1000 
	 loss: 30.2638, MinusLogProbMetric: 30.2638, val_loss: 30.0331, val_MinusLogProbMetric: 30.0331

Epoch 665: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.2638 - MinusLogProbMetric: 30.2638 - val_loss: 30.0331 - val_MinusLogProbMetric: 30.0331 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 666/1000
2023-10-25 22:24:02.051 
Epoch 666/1000 
	 loss: 29.7460, MinusLogProbMetric: 29.7460, val_loss: 29.9602, val_MinusLogProbMetric: 29.9602

Epoch 666: val_loss did not improve from 29.68712
196/196 - 34s - loss: 29.7460 - MinusLogProbMetric: 29.7460 - val_loss: 29.9602 - val_MinusLogProbMetric: 29.9602 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 667/1000
2023-10-25 22:24:36.736 
Epoch 667/1000 
	 loss: 29.8117, MinusLogProbMetric: 29.8117, val_loss: 30.2361, val_MinusLogProbMetric: 30.2361

Epoch 667: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.8117 - MinusLogProbMetric: 29.8117 - val_loss: 30.2361 - val_MinusLogProbMetric: 30.2361 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 668/1000
2023-10-25 22:25:11.615 
Epoch 668/1000 
	 loss: 30.9634, MinusLogProbMetric: 30.9634, val_loss: 32.7190, val_MinusLogProbMetric: 32.7190

Epoch 668: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.9634 - MinusLogProbMetric: 30.9634 - val_loss: 32.7190 - val_MinusLogProbMetric: 32.7190 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 669/1000
2023-10-25 22:25:46.381 
Epoch 669/1000 
	 loss: 30.4374, MinusLogProbMetric: 30.4374, val_loss: 30.4203, val_MinusLogProbMetric: 30.4203

Epoch 669: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.4374 - MinusLogProbMetric: 30.4374 - val_loss: 30.4203 - val_MinusLogProbMetric: 30.4203 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 670/1000
2023-10-25 22:26:20.984 
Epoch 670/1000 
	 loss: 30.0759, MinusLogProbMetric: 30.0759, val_loss: 30.0449, val_MinusLogProbMetric: 30.0449

Epoch 670: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.0759 - MinusLogProbMetric: 30.0759 - val_loss: 30.0449 - val_MinusLogProbMetric: 30.0449 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 671/1000
2023-10-25 22:26:56.104 
Epoch 671/1000 
	 loss: 29.8159, MinusLogProbMetric: 29.8159, val_loss: 30.6741, val_MinusLogProbMetric: 30.6741

Epoch 671: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.8159 - MinusLogProbMetric: 29.8159 - val_loss: 30.6741 - val_MinusLogProbMetric: 30.6741 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 672/1000
2023-10-25 22:27:31.135 
Epoch 672/1000 
	 loss: 30.2280, MinusLogProbMetric: 30.2280, val_loss: 29.8936, val_MinusLogProbMetric: 29.8936

Epoch 672: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.2280 - MinusLogProbMetric: 30.2280 - val_loss: 29.8936 - val_MinusLogProbMetric: 29.8936 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 673/1000
2023-10-25 22:28:05.670 
Epoch 673/1000 
	 loss: 30.7686, MinusLogProbMetric: 30.7686, val_loss: 30.1512, val_MinusLogProbMetric: 30.1512

Epoch 673: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.7686 - MinusLogProbMetric: 30.7686 - val_loss: 30.1512 - val_MinusLogProbMetric: 30.1512 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 674/1000
2023-10-25 22:28:40.260 
Epoch 674/1000 
	 loss: 30.5098, MinusLogProbMetric: 30.5098, val_loss: 30.7159, val_MinusLogProbMetric: 30.7159

Epoch 674: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.5098 - MinusLogProbMetric: 30.5098 - val_loss: 30.7159 - val_MinusLogProbMetric: 30.7159 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 675/1000
2023-10-25 22:29:13.364 
Epoch 675/1000 
	 loss: 29.8788, MinusLogProbMetric: 29.8788, val_loss: 29.9179, val_MinusLogProbMetric: 29.9179

Epoch 675: val_loss did not improve from 29.68712
196/196 - 33s - loss: 29.8788 - MinusLogProbMetric: 29.8788 - val_loss: 29.9179 - val_MinusLogProbMetric: 29.9179 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 676/1000
2023-10-25 22:29:41.796 
Epoch 676/1000 
	 loss: 30.3509, MinusLogProbMetric: 30.3509, val_loss: 29.8442, val_MinusLogProbMetric: 29.8442

Epoch 676: val_loss did not improve from 29.68712
196/196 - 28s - loss: 30.3509 - MinusLogProbMetric: 30.3509 - val_loss: 29.8442 - val_MinusLogProbMetric: 29.8442 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 677/1000
2023-10-25 22:30:13.925 
Epoch 677/1000 
	 loss: 29.8531, MinusLogProbMetric: 29.8531, val_loss: 29.8813, val_MinusLogProbMetric: 29.8813

Epoch 677: val_loss did not improve from 29.68712
196/196 - 32s - loss: 29.8531 - MinusLogProbMetric: 29.8531 - val_loss: 29.8813 - val_MinusLogProbMetric: 29.8813 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 678/1000
2023-10-25 22:30:48.750 
Epoch 678/1000 
	 loss: 29.9649, MinusLogProbMetric: 29.9649, val_loss: 32.8927, val_MinusLogProbMetric: 32.8927

Epoch 678: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9649 - MinusLogProbMetric: 29.9649 - val_loss: 32.8927 - val_MinusLogProbMetric: 32.8927 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 679/1000
2023-10-25 22:31:23.543 
Epoch 679/1000 
	 loss: 29.9327, MinusLogProbMetric: 29.9327, val_loss: 30.2120, val_MinusLogProbMetric: 30.2120

Epoch 679: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9327 - MinusLogProbMetric: 29.9327 - val_loss: 30.2120 - val_MinusLogProbMetric: 30.2120 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 680/1000
2023-10-25 22:31:58.167 
Epoch 680/1000 
	 loss: 29.8783, MinusLogProbMetric: 29.8783, val_loss: 30.9224, val_MinusLogProbMetric: 30.9224

Epoch 680: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.8783 - MinusLogProbMetric: 29.8783 - val_loss: 30.9224 - val_MinusLogProbMetric: 30.9224 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 681/1000
2023-10-25 22:32:32.760 
Epoch 681/1000 
	 loss: 30.0572, MinusLogProbMetric: 30.0572, val_loss: 30.2429, val_MinusLogProbMetric: 30.2429

Epoch 681: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.0572 - MinusLogProbMetric: 30.0572 - val_loss: 30.2429 - val_MinusLogProbMetric: 30.2429 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 682/1000
2023-10-25 22:33:07.231 
Epoch 682/1000 
	 loss: 29.9976, MinusLogProbMetric: 29.9976, val_loss: 30.8960, val_MinusLogProbMetric: 30.8960

Epoch 682: val_loss did not improve from 29.68712
196/196 - 34s - loss: 29.9976 - MinusLogProbMetric: 29.9976 - val_loss: 30.8960 - val_MinusLogProbMetric: 30.8960 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 683/1000
2023-10-25 22:33:42.019 
Epoch 683/1000 
	 loss: 29.8814, MinusLogProbMetric: 29.8814, val_loss: 30.2690, val_MinusLogProbMetric: 30.2690

Epoch 683: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.8814 - MinusLogProbMetric: 29.8814 - val_loss: 30.2690 - val_MinusLogProbMetric: 30.2690 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 684/1000
2023-10-25 22:34:16.685 
Epoch 684/1000 
	 loss: 29.9908, MinusLogProbMetric: 29.9908, val_loss: 30.5513, val_MinusLogProbMetric: 30.5513

Epoch 684: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9908 - MinusLogProbMetric: 29.9908 - val_loss: 30.5513 - val_MinusLogProbMetric: 30.5513 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 685/1000
2023-10-25 22:34:51.238 
Epoch 685/1000 
	 loss: 29.9792, MinusLogProbMetric: 29.9792, val_loss: 29.8288, val_MinusLogProbMetric: 29.8288

Epoch 685: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9792 - MinusLogProbMetric: 29.9792 - val_loss: 29.8288 - val_MinusLogProbMetric: 29.8288 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 686/1000
2023-10-25 22:35:25.602 
Epoch 686/1000 
	 loss: 29.8046, MinusLogProbMetric: 29.8046, val_loss: 30.4012, val_MinusLogProbMetric: 30.4012

Epoch 686: val_loss did not improve from 29.68712
196/196 - 34s - loss: 29.8046 - MinusLogProbMetric: 29.8046 - val_loss: 30.4012 - val_MinusLogProbMetric: 30.4012 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 687/1000
2023-10-25 22:36:00.021 
Epoch 687/1000 
	 loss: 30.2181, MinusLogProbMetric: 30.2181, val_loss: 30.1612, val_MinusLogProbMetric: 30.1612

Epoch 687: val_loss did not improve from 29.68712
196/196 - 34s - loss: 30.2181 - MinusLogProbMetric: 30.2181 - val_loss: 30.1612 - val_MinusLogProbMetric: 30.1612 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 688/1000
2023-10-25 22:36:34.559 
Epoch 688/1000 
	 loss: 29.8402, MinusLogProbMetric: 29.8402, val_loss: 29.7467, val_MinusLogProbMetric: 29.7467

Epoch 688: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.8402 - MinusLogProbMetric: 29.8402 - val_loss: 29.7467 - val_MinusLogProbMetric: 29.7467 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 689/1000
2023-10-25 22:37:09.066 
Epoch 689/1000 
	 loss: 30.0347, MinusLogProbMetric: 30.0347, val_loss: 30.2369, val_MinusLogProbMetric: 30.2369

Epoch 689: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.0347 - MinusLogProbMetric: 30.0347 - val_loss: 30.2369 - val_MinusLogProbMetric: 30.2369 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 690/1000
2023-10-25 22:37:43.644 
Epoch 690/1000 
	 loss: 30.1263, MinusLogProbMetric: 30.1263, val_loss: 30.0192, val_MinusLogProbMetric: 30.0192

Epoch 690: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.1263 - MinusLogProbMetric: 30.1263 - val_loss: 30.0192 - val_MinusLogProbMetric: 30.0192 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 691/1000
2023-10-25 22:38:18.264 
Epoch 691/1000 
	 loss: 29.9093, MinusLogProbMetric: 29.9093, val_loss: 30.1573, val_MinusLogProbMetric: 30.1573

Epoch 691: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9093 - MinusLogProbMetric: 29.9093 - val_loss: 30.1573 - val_MinusLogProbMetric: 30.1573 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 692/1000
2023-10-25 22:38:52.980 
Epoch 692/1000 
	 loss: 29.8582, MinusLogProbMetric: 29.8582, val_loss: 30.0782, val_MinusLogProbMetric: 30.0782

Epoch 692: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.8582 - MinusLogProbMetric: 29.8582 - val_loss: 30.0782 - val_MinusLogProbMetric: 30.0782 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 693/1000
2023-10-25 22:39:27.709 
Epoch 693/1000 
	 loss: 30.1442, MinusLogProbMetric: 30.1442, val_loss: 31.5133, val_MinusLogProbMetric: 31.5133

Epoch 693: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.1442 - MinusLogProbMetric: 30.1442 - val_loss: 31.5133 - val_MinusLogProbMetric: 31.5133 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 694/1000
2023-10-25 22:40:02.460 
Epoch 694/1000 
	 loss: 30.1146, MinusLogProbMetric: 30.1146, val_loss: 30.0229, val_MinusLogProbMetric: 30.0229

Epoch 694: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.1146 - MinusLogProbMetric: 30.1146 - val_loss: 30.0229 - val_MinusLogProbMetric: 30.0229 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 695/1000
2023-10-25 22:40:36.951 
Epoch 695/1000 
	 loss: 29.7199, MinusLogProbMetric: 29.7199, val_loss: 30.4680, val_MinusLogProbMetric: 30.4680

Epoch 695: val_loss did not improve from 29.68712
196/196 - 34s - loss: 29.7199 - MinusLogProbMetric: 29.7199 - val_loss: 30.4680 - val_MinusLogProbMetric: 30.4680 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 696/1000
2023-10-25 22:41:11.596 
Epoch 696/1000 
	 loss: 30.0377, MinusLogProbMetric: 30.0377, val_loss: 30.2136, val_MinusLogProbMetric: 30.2136

Epoch 696: val_loss did not improve from 29.68712
196/196 - 35s - loss: 30.0377 - MinusLogProbMetric: 30.0377 - val_loss: 30.2136 - val_MinusLogProbMetric: 30.2136 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 697/1000
2023-10-25 22:41:46.396 
Epoch 697/1000 
	 loss: 29.9671, MinusLogProbMetric: 29.9671, val_loss: 30.6010, val_MinusLogProbMetric: 30.6010

Epoch 697: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9671 - MinusLogProbMetric: 29.9671 - val_loss: 30.6010 - val_MinusLogProbMetric: 30.6010 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 698/1000
2023-10-25 22:42:20.815 
Epoch 698/1000 
	 loss: 29.8793, MinusLogProbMetric: 29.8793, val_loss: 30.2845, val_MinusLogProbMetric: 30.2845

Epoch 698: val_loss did not improve from 29.68712
196/196 - 34s - loss: 29.8793 - MinusLogProbMetric: 29.8793 - val_loss: 30.2845 - val_MinusLogProbMetric: 30.2845 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 699/1000
2023-10-25 22:42:55.489 
Epoch 699/1000 
	 loss: 29.9781, MinusLogProbMetric: 29.9781, val_loss: 31.6419, val_MinusLogProbMetric: 31.6419

Epoch 699: val_loss did not improve from 29.68712
196/196 - 35s - loss: 29.9781 - MinusLogProbMetric: 29.9781 - val_loss: 31.6419 - val_MinusLogProbMetric: 31.6419 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 700/1000
2023-10-25 22:43:29.919 
Epoch 700/1000 
	 loss: 30.1626, MinusLogProbMetric: 30.1626, val_loss: 29.6095, val_MinusLogProbMetric: 29.6095

Epoch 700: val_loss improved from 29.68712 to 29.60947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 30.1626 - MinusLogProbMetric: 30.1626 - val_loss: 29.6095 - val_MinusLogProbMetric: 29.6095 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 701/1000
2023-10-25 22:44:05.164 
Epoch 701/1000 
	 loss: 30.2408, MinusLogProbMetric: 30.2408, val_loss: 30.2017, val_MinusLogProbMetric: 30.2017

Epoch 701: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.2408 - MinusLogProbMetric: 30.2408 - val_loss: 30.2017 - val_MinusLogProbMetric: 30.2017 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 702/1000
2023-10-25 22:44:39.892 
Epoch 702/1000 
	 loss: 29.8513, MinusLogProbMetric: 29.8513, val_loss: 30.4787, val_MinusLogProbMetric: 30.4787

Epoch 702: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.8513 - MinusLogProbMetric: 29.8513 - val_loss: 30.4787 - val_MinusLogProbMetric: 30.4787 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 703/1000
2023-10-25 22:45:14.710 
Epoch 703/1000 
	 loss: 30.1386, MinusLogProbMetric: 30.1386, val_loss: 30.0127, val_MinusLogProbMetric: 30.0127

Epoch 703: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.1386 - MinusLogProbMetric: 30.1386 - val_loss: 30.0127 - val_MinusLogProbMetric: 30.0127 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 704/1000
2023-10-25 22:45:49.501 
Epoch 704/1000 
	 loss: 29.7272, MinusLogProbMetric: 29.7272, val_loss: 30.0525, val_MinusLogProbMetric: 30.0525

Epoch 704: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7272 - MinusLogProbMetric: 29.7272 - val_loss: 30.0525 - val_MinusLogProbMetric: 30.0525 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 705/1000
2023-10-25 22:46:24.398 
Epoch 705/1000 
	 loss: 30.0091, MinusLogProbMetric: 30.0091, val_loss: 30.5539, val_MinusLogProbMetric: 30.5539

Epoch 705: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.0091 - MinusLogProbMetric: 30.0091 - val_loss: 30.5539 - val_MinusLogProbMetric: 30.5539 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 706/1000
2023-10-25 22:46:59.180 
Epoch 706/1000 
	 loss: 29.7593, MinusLogProbMetric: 29.7593, val_loss: 30.1580, val_MinusLogProbMetric: 30.1580

Epoch 706: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7593 - MinusLogProbMetric: 29.7593 - val_loss: 30.1580 - val_MinusLogProbMetric: 30.1580 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 707/1000
2023-10-25 22:47:33.788 
Epoch 707/1000 
	 loss: 30.0251, MinusLogProbMetric: 30.0251, val_loss: 30.0432, val_MinusLogProbMetric: 30.0432

Epoch 707: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.0251 - MinusLogProbMetric: 30.0251 - val_loss: 30.0432 - val_MinusLogProbMetric: 30.0432 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 708/1000
2023-10-25 22:48:08.424 
Epoch 708/1000 
	 loss: 29.9365, MinusLogProbMetric: 29.9365, val_loss: 30.2426, val_MinusLogProbMetric: 30.2426

Epoch 708: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9365 - MinusLogProbMetric: 29.9365 - val_loss: 30.2426 - val_MinusLogProbMetric: 30.2426 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 709/1000
2023-10-25 22:48:43.512 
Epoch 709/1000 
	 loss: 29.9651, MinusLogProbMetric: 29.9651, val_loss: 30.3072, val_MinusLogProbMetric: 30.3072

Epoch 709: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9651 - MinusLogProbMetric: 29.9651 - val_loss: 30.3072 - val_MinusLogProbMetric: 30.3072 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 710/1000
2023-10-25 22:49:18.011 
Epoch 710/1000 
	 loss: 30.0175, MinusLogProbMetric: 30.0175, val_loss: 30.1589, val_MinusLogProbMetric: 30.1589

Epoch 710: val_loss did not improve from 29.60947
196/196 - 34s - loss: 30.0175 - MinusLogProbMetric: 30.0175 - val_loss: 30.1589 - val_MinusLogProbMetric: 30.1589 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 711/1000
2023-10-25 22:49:52.572 
Epoch 711/1000 
	 loss: 29.9995, MinusLogProbMetric: 29.9995, val_loss: 30.4222, val_MinusLogProbMetric: 30.4222

Epoch 711: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9995 - MinusLogProbMetric: 29.9995 - val_loss: 30.4222 - val_MinusLogProbMetric: 30.4222 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 712/1000
2023-10-25 22:50:27.268 
Epoch 712/1000 
	 loss: 30.0148, MinusLogProbMetric: 30.0148, val_loss: 30.1732, val_MinusLogProbMetric: 30.1732

Epoch 712: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.0148 - MinusLogProbMetric: 30.0148 - val_loss: 30.1732 - val_MinusLogProbMetric: 30.1732 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 713/1000
2023-10-25 22:51:02.343 
Epoch 713/1000 
	 loss: 30.1048, MinusLogProbMetric: 30.1048, val_loss: 30.5560, val_MinusLogProbMetric: 30.5560

Epoch 713: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.1048 - MinusLogProbMetric: 30.1048 - val_loss: 30.5560 - val_MinusLogProbMetric: 30.5560 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 714/1000
2023-10-25 22:51:37.021 
Epoch 714/1000 
	 loss: 29.9347, MinusLogProbMetric: 29.9347, val_loss: 32.0770, val_MinusLogProbMetric: 32.0770

Epoch 714: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9347 - MinusLogProbMetric: 29.9347 - val_loss: 32.0770 - val_MinusLogProbMetric: 32.0770 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 715/1000
2023-10-25 22:52:11.901 
Epoch 715/1000 
	 loss: 29.9360, MinusLogProbMetric: 29.9360, val_loss: 30.2848, val_MinusLogProbMetric: 30.2848

Epoch 715: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9360 - MinusLogProbMetric: 29.9360 - val_loss: 30.2848 - val_MinusLogProbMetric: 30.2848 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 716/1000
2023-10-25 22:52:46.774 
Epoch 716/1000 
	 loss: 29.6776, MinusLogProbMetric: 29.6776, val_loss: 30.3636, val_MinusLogProbMetric: 30.3636

Epoch 716: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.6776 - MinusLogProbMetric: 29.6776 - val_loss: 30.3636 - val_MinusLogProbMetric: 30.3636 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 717/1000
2023-10-25 22:53:21.608 
Epoch 717/1000 
	 loss: 30.1060, MinusLogProbMetric: 30.1060, val_loss: 31.1060, val_MinusLogProbMetric: 31.1060

Epoch 717: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.1060 - MinusLogProbMetric: 30.1060 - val_loss: 31.1060 - val_MinusLogProbMetric: 31.1060 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 718/1000
2023-10-25 22:53:56.455 
Epoch 718/1000 
	 loss: 29.9129, MinusLogProbMetric: 29.9129, val_loss: 30.1045, val_MinusLogProbMetric: 30.1045

Epoch 718: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9129 - MinusLogProbMetric: 29.9129 - val_loss: 30.1045 - val_MinusLogProbMetric: 30.1045 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 719/1000
2023-10-25 22:54:31.078 
Epoch 719/1000 
	 loss: 30.0604, MinusLogProbMetric: 30.0604, val_loss: 30.0285, val_MinusLogProbMetric: 30.0285

Epoch 719: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.0604 - MinusLogProbMetric: 30.0604 - val_loss: 30.0285 - val_MinusLogProbMetric: 30.0285 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 720/1000
2023-10-25 22:55:05.701 
Epoch 720/1000 
	 loss: 29.9217, MinusLogProbMetric: 29.9217, val_loss: 29.8475, val_MinusLogProbMetric: 29.8475

Epoch 720: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9217 - MinusLogProbMetric: 29.9217 - val_loss: 29.8475 - val_MinusLogProbMetric: 29.8475 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 721/1000
2023-10-25 22:55:40.495 
Epoch 721/1000 
	 loss: 30.3116, MinusLogProbMetric: 30.3116, val_loss: 29.9245, val_MinusLogProbMetric: 29.9245

Epoch 721: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.3116 - MinusLogProbMetric: 30.3116 - val_loss: 29.9245 - val_MinusLogProbMetric: 29.9245 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 722/1000
2023-10-25 22:56:15.077 
Epoch 722/1000 
	 loss: 30.1069, MinusLogProbMetric: 30.1069, val_loss: 30.9788, val_MinusLogProbMetric: 30.9788

Epoch 722: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.1069 - MinusLogProbMetric: 30.1069 - val_loss: 30.9788 - val_MinusLogProbMetric: 30.9788 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 723/1000
2023-10-25 22:56:49.621 
Epoch 723/1000 
	 loss: 29.9934, MinusLogProbMetric: 29.9934, val_loss: 30.4817, val_MinusLogProbMetric: 30.4817

Epoch 723: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9934 - MinusLogProbMetric: 29.9934 - val_loss: 30.4817 - val_MinusLogProbMetric: 30.4817 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 724/1000
2023-10-25 22:57:24.949 
Epoch 724/1000 
	 loss: 29.8738, MinusLogProbMetric: 29.8738, val_loss: 29.8503, val_MinusLogProbMetric: 29.8503

Epoch 724: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.8738 - MinusLogProbMetric: 29.8738 - val_loss: 29.8503 - val_MinusLogProbMetric: 29.8503 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 725/1000
2023-10-25 22:57:59.715 
Epoch 725/1000 
	 loss: 29.8403, MinusLogProbMetric: 29.8403, val_loss: 29.9561, val_MinusLogProbMetric: 29.9561

Epoch 725: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.8403 - MinusLogProbMetric: 29.8403 - val_loss: 29.9561 - val_MinusLogProbMetric: 29.9561 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 726/1000
2023-10-25 22:58:34.469 
Epoch 726/1000 
	 loss: 29.9196, MinusLogProbMetric: 29.9196, val_loss: 30.1086, val_MinusLogProbMetric: 30.1086

Epoch 726: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9196 - MinusLogProbMetric: 29.9196 - val_loss: 30.1086 - val_MinusLogProbMetric: 30.1086 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 727/1000
2023-10-25 22:59:09.544 
Epoch 727/1000 
	 loss: 29.9498, MinusLogProbMetric: 29.9498, val_loss: 29.8767, val_MinusLogProbMetric: 29.8767

Epoch 727: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9498 - MinusLogProbMetric: 29.9498 - val_loss: 29.8767 - val_MinusLogProbMetric: 29.8767 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 728/1000
2023-10-25 22:59:44.697 
Epoch 728/1000 
	 loss: 29.9924, MinusLogProbMetric: 29.9924, val_loss: 30.0291, val_MinusLogProbMetric: 30.0291

Epoch 728: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9924 - MinusLogProbMetric: 29.9924 - val_loss: 30.0291 - val_MinusLogProbMetric: 30.0291 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 729/1000
2023-10-25 23:00:19.546 
Epoch 729/1000 
	 loss: 29.9891, MinusLogProbMetric: 29.9891, val_loss: 30.1463, val_MinusLogProbMetric: 30.1463

Epoch 729: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9891 - MinusLogProbMetric: 29.9891 - val_loss: 30.1463 - val_MinusLogProbMetric: 30.1463 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 730/1000
2023-10-25 23:00:54.439 
Epoch 730/1000 
	 loss: 29.7569, MinusLogProbMetric: 29.7569, val_loss: 30.1031, val_MinusLogProbMetric: 30.1031

Epoch 730: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7569 - MinusLogProbMetric: 29.7569 - val_loss: 30.1031 - val_MinusLogProbMetric: 30.1031 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 731/1000
2023-10-25 23:01:29.470 
Epoch 731/1000 
	 loss: 29.7721, MinusLogProbMetric: 29.7721, val_loss: 30.2856, val_MinusLogProbMetric: 30.2856

Epoch 731: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7721 - MinusLogProbMetric: 29.7721 - val_loss: 30.2856 - val_MinusLogProbMetric: 30.2856 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 732/1000
2023-10-25 23:02:03.965 
Epoch 732/1000 
	 loss: 29.9372, MinusLogProbMetric: 29.9372, val_loss: 30.0409, val_MinusLogProbMetric: 30.0409

Epoch 732: val_loss did not improve from 29.60947
196/196 - 34s - loss: 29.9372 - MinusLogProbMetric: 29.9372 - val_loss: 30.0409 - val_MinusLogProbMetric: 30.0409 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 733/1000
2023-10-25 23:02:38.980 
Epoch 733/1000 
	 loss: 29.9551, MinusLogProbMetric: 29.9551, val_loss: 30.1885, val_MinusLogProbMetric: 30.1885

Epoch 733: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9551 - MinusLogProbMetric: 29.9551 - val_loss: 30.1885 - val_MinusLogProbMetric: 30.1885 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 734/1000
2023-10-25 23:03:13.758 
Epoch 734/1000 
	 loss: 30.0014, MinusLogProbMetric: 30.0014, val_loss: 29.6547, val_MinusLogProbMetric: 29.6547

Epoch 734: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.0014 - MinusLogProbMetric: 30.0014 - val_loss: 29.6547 - val_MinusLogProbMetric: 29.6547 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 735/1000
2023-10-25 23:03:48.620 
Epoch 735/1000 
	 loss: 29.7085, MinusLogProbMetric: 29.7085, val_loss: 30.0055, val_MinusLogProbMetric: 30.0055

Epoch 735: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7085 - MinusLogProbMetric: 29.7085 - val_loss: 30.0055 - val_MinusLogProbMetric: 30.0055 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 736/1000
2023-10-25 23:04:23.621 
Epoch 736/1000 
	 loss: 29.8670, MinusLogProbMetric: 29.8670, val_loss: 30.0030, val_MinusLogProbMetric: 30.0030

Epoch 736: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.8670 - MinusLogProbMetric: 29.8670 - val_loss: 30.0030 - val_MinusLogProbMetric: 30.0030 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 737/1000
2023-10-25 23:04:58.177 
Epoch 737/1000 
	 loss: 29.7454, MinusLogProbMetric: 29.7454, val_loss: 30.0402, val_MinusLogProbMetric: 30.0402

Epoch 737: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7454 - MinusLogProbMetric: 29.7454 - val_loss: 30.0402 - val_MinusLogProbMetric: 30.0402 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 738/1000
2023-10-25 23:05:32.931 
Epoch 738/1000 
	 loss: 29.9492, MinusLogProbMetric: 29.9492, val_loss: 29.9876, val_MinusLogProbMetric: 29.9876

Epoch 738: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9492 - MinusLogProbMetric: 29.9492 - val_loss: 29.9876 - val_MinusLogProbMetric: 29.9876 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 739/1000
2023-10-25 23:06:07.720 
Epoch 739/1000 
	 loss: 29.9751, MinusLogProbMetric: 29.9751, val_loss: 30.1015, val_MinusLogProbMetric: 30.1015

Epoch 739: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9751 - MinusLogProbMetric: 29.9751 - val_loss: 30.1015 - val_MinusLogProbMetric: 30.1015 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 740/1000
2023-10-25 23:06:42.503 
Epoch 740/1000 
	 loss: 29.7436, MinusLogProbMetric: 29.7436, val_loss: 30.2369, val_MinusLogProbMetric: 30.2369

Epoch 740: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7436 - MinusLogProbMetric: 29.7436 - val_loss: 30.2369 - val_MinusLogProbMetric: 30.2369 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 741/1000
2023-10-25 23:07:17.455 
Epoch 741/1000 
	 loss: 30.1085, MinusLogProbMetric: 30.1085, val_loss: 29.6677, val_MinusLogProbMetric: 29.6677

Epoch 741: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.1085 - MinusLogProbMetric: 30.1085 - val_loss: 29.6677 - val_MinusLogProbMetric: 29.6677 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 742/1000
2023-10-25 23:07:52.374 
Epoch 742/1000 
	 loss: 29.7978, MinusLogProbMetric: 29.7978, val_loss: 31.9151, val_MinusLogProbMetric: 31.9151

Epoch 742: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7978 - MinusLogProbMetric: 29.7978 - val_loss: 31.9151 - val_MinusLogProbMetric: 31.9151 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 743/1000
2023-10-25 23:08:27.194 
Epoch 743/1000 
	 loss: 29.8928, MinusLogProbMetric: 29.8928, val_loss: 29.8384, val_MinusLogProbMetric: 29.8384

Epoch 743: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.8928 - MinusLogProbMetric: 29.8928 - val_loss: 29.8384 - val_MinusLogProbMetric: 29.8384 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 744/1000
2023-10-25 23:09:02.287 
Epoch 744/1000 
	 loss: 30.0111, MinusLogProbMetric: 30.0111, val_loss: 30.3166, val_MinusLogProbMetric: 30.3166

Epoch 744: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.0111 - MinusLogProbMetric: 30.0111 - val_loss: 30.3166 - val_MinusLogProbMetric: 30.3166 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 745/1000
2023-10-25 23:09:36.951 
Epoch 745/1000 
	 loss: 29.6039, MinusLogProbMetric: 29.6039, val_loss: 29.8684, val_MinusLogProbMetric: 29.8684

Epoch 745: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.6039 - MinusLogProbMetric: 29.6039 - val_loss: 29.8684 - val_MinusLogProbMetric: 29.8684 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 746/1000
2023-10-25 23:10:11.821 
Epoch 746/1000 
	 loss: 29.7133, MinusLogProbMetric: 29.7133, val_loss: 29.6404, val_MinusLogProbMetric: 29.6404

Epoch 746: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7133 - MinusLogProbMetric: 29.7133 - val_loss: 29.6404 - val_MinusLogProbMetric: 29.6404 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 747/1000
2023-10-25 23:10:46.867 
Epoch 747/1000 
	 loss: 29.7714, MinusLogProbMetric: 29.7714, val_loss: 29.7334, val_MinusLogProbMetric: 29.7334

Epoch 747: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.7714 - MinusLogProbMetric: 29.7714 - val_loss: 29.7334 - val_MinusLogProbMetric: 29.7334 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 748/1000
2023-10-25 23:11:21.636 
Epoch 748/1000 
	 loss: 29.8729, MinusLogProbMetric: 29.8729, val_loss: 30.0109, val_MinusLogProbMetric: 30.0109

Epoch 748: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.8729 - MinusLogProbMetric: 29.8729 - val_loss: 30.0109 - val_MinusLogProbMetric: 30.0109 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 749/1000
2023-10-25 23:11:56.675 
Epoch 749/1000 
	 loss: 29.9042, MinusLogProbMetric: 29.9042, val_loss: 31.4120, val_MinusLogProbMetric: 31.4120

Epoch 749: val_loss did not improve from 29.60947
196/196 - 35s - loss: 29.9042 - MinusLogProbMetric: 29.9042 - val_loss: 31.4120 - val_MinusLogProbMetric: 31.4120 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 750/1000
2023-10-25 23:12:31.325 
Epoch 750/1000 
	 loss: 30.0265, MinusLogProbMetric: 30.0265, val_loss: 29.9477, val_MinusLogProbMetric: 29.9477

Epoch 750: val_loss did not improve from 29.60947
196/196 - 35s - loss: 30.0265 - MinusLogProbMetric: 30.0265 - val_loss: 29.9477 - val_MinusLogProbMetric: 29.9477 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 751/1000
2023-10-25 23:13:06.011 
Epoch 751/1000 
	 loss: 29.1055, MinusLogProbMetric: 29.1055, val_loss: 29.2751, val_MinusLogProbMetric: 29.2751

Epoch 751: val_loss improved from 29.60947 to 29.27510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 29.1055 - MinusLogProbMetric: 29.1055 - val_loss: 29.2751 - val_MinusLogProbMetric: 29.2751 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 752/1000
2023-10-25 23:13:41.347 
Epoch 752/1000 
	 loss: 29.0840, MinusLogProbMetric: 29.0840, val_loss: 29.5353, val_MinusLogProbMetric: 29.5353

Epoch 752: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.0840 - MinusLogProbMetric: 29.0840 - val_loss: 29.5353 - val_MinusLogProbMetric: 29.5353 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 753/1000
2023-10-25 23:14:16.242 
Epoch 753/1000 
	 loss: 29.0782, MinusLogProbMetric: 29.0782, val_loss: 29.6327, val_MinusLogProbMetric: 29.6327

Epoch 753: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.0782 - MinusLogProbMetric: 29.0782 - val_loss: 29.6327 - val_MinusLogProbMetric: 29.6327 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 754/1000
2023-10-25 23:14:51.233 
Epoch 754/1000 
	 loss: 29.1311, MinusLogProbMetric: 29.1311, val_loss: 29.7549, val_MinusLogProbMetric: 29.7549

Epoch 754: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.1311 - MinusLogProbMetric: 29.1311 - val_loss: 29.7549 - val_MinusLogProbMetric: 29.7549 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 755/1000
2023-10-25 23:15:26.091 
Epoch 755/1000 
	 loss: 29.1125, MinusLogProbMetric: 29.1125, val_loss: 29.3002, val_MinusLogProbMetric: 29.3002

Epoch 755: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.1125 - MinusLogProbMetric: 29.1125 - val_loss: 29.3002 - val_MinusLogProbMetric: 29.3002 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 756/1000
2023-10-25 23:16:00.893 
Epoch 756/1000 
	 loss: 29.0434, MinusLogProbMetric: 29.0434, val_loss: 30.0237, val_MinusLogProbMetric: 30.0237

Epoch 756: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.0434 - MinusLogProbMetric: 29.0434 - val_loss: 30.0237 - val_MinusLogProbMetric: 30.0237 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 757/1000
2023-10-25 23:16:36.095 
Epoch 757/1000 
	 loss: 29.1121, MinusLogProbMetric: 29.1121, val_loss: 29.3497, val_MinusLogProbMetric: 29.3497

Epoch 757: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.1121 - MinusLogProbMetric: 29.1121 - val_loss: 29.3497 - val_MinusLogProbMetric: 29.3497 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 758/1000
2023-10-25 23:17:11.179 
Epoch 758/1000 
	 loss: 29.1004, MinusLogProbMetric: 29.1004, val_loss: 29.5887, val_MinusLogProbMetric: 29.5887

Epoch 758: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.1004 - MinusLogProbMetric: 29.1004 - val_loss: 29.5887 - val_MinusLogProbMetric: 29.5887 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 759/1000
2023-10-25 23:17:46.002 
Epoch 759/1000 
	 loss: 29.0890, MinusLogProbMetric: 29.0890, val_loss: 29.2816, val_MinusLogProbMetric: 29.2816

Epoch 759: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.0890 - MinusLogProbMetric: 29.0890 - val_loss: 29.2816 - val_MinusLogProbMetric: 29.2816 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 760/1000
2023-10-25 23:18:20.599 
Epoch 760/1000 
	 loss: 29.0737, MinusLogProbMetric: 29.0737, val_loss: 29.3830, val_MinusLogProbMetric: 29.3830

Epoch 760: val_loss did not improve from 29.27510
196/196 - 35s - loss: 29.0737 - MinusLogProbMetric: 29.0737 - val_loss: 29.3830 - val_MinusLogProbMetric: 29.3830 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 761/1000
2023-10-25 23:18:55.644 
Epoch 761/1000 
	 loss: 29.0573, MinusLogProbMetric: 29.0573, val_loss: 29.2637, val_MinusLogProbMetric: 29.2637

Epoch 761: val_loss improved from 29.27510 to 29.26370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 29.0573 - MinusLogProbMetric: 29.0573 - val_loss: 29.2637 - val_MinusLogProbMetric: 29.2637 - lr: 1.6667e-04 - 36s/epoch - 182ms/step
Epoch 762/1000
2023-10-25 23:19:30.930 
Epoch 762/1000 
	 loss: 29.0223, MinusLogProbMetric: 29.0223, val_loss: 29.4118, val_MinusLogProbMetric: 29.4118

Epoch 762: val_loss did not improve from 29.26370
196/196 - 35s - loss: 29.0223 - MinusLogProbMetric: 29.0223 - val_loss: 29.4118 - val_MinusLogProbMetric: 29.4118 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 763/1000
2023-10-25 23:20:05.681 
Epoch 763/1000 
	 loss: 29.0707, MinusLogProbMetric: 29.0707, val_loss: 29.4522, val_MinusLogProbMetric: 29.4522

Epoch 763: val_loss did not improve from 29.26370
196/196 - 35s - loss: 29.0707 - MinusLogProbMetric: 29.0707 - val_loss: 29.4522 - val_MinusLogProbMetric: 29.4522 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 764/1000
2023-10-25 23:20:40.646 
Epoch 764/1000 
	 loss: 29.0861, MinusLogProbMetric: 29.0861, val_loss: 29.5188, val_MinusLogProbMetric: 29.5188

Epoch 764: val_loss did not improve from 29.26370
196/196 - 35s - loss: 29.0861 - MinusLogProbMetric: 29.0861 - val_loss: 29.5188 - val_MinusLogProbMetric: 29.5188 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 765/1000
2023-10-25 23:21:15.584 
Epoch 765/1000 
	 loss: 29.0857, MinusLogProbMetric: 29.0857, val_loss: 30.1856, val_MinusLogProbMetric: 30.1856

Epoch 765: val_loss did not improve from 29.26370
196/196 - 35s - loss: 29.0857 - MinusLogProbMetric: 29.0857 - val_loss: 30.1856 - val_MinusLogProbMetric: 30.1856 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 766/1000
2023-10-25 23:21:50.434 
Epoch 766/1000 
	 loss: 29.1098, MinusLogProbMetric: 29.1098, val_loss: 29.3378, val_MinusLogProbMetric: 29.3378

Epoch 766: val_loss did not improve from 29.26370
196/196 - 35s - loss: 29.1098 - MinusLogProbMetric: 29.1098 - val_loss: 29.3378 - val_MinusLogProbMetric: 29.3378 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 767/1000
2023-10-25 23:22:25.646 
Epoch 767/1000 
	 loss: 29.0509, MinusLogProbMetric: 29.0509, val_loss: 29.2726, val_MinusLogProbMetric: 29.2726

Epoch 767: val_loss did not improve from 29.26370
196/196 - 35s - loss: 29.0509 - MinusLogProbMetric: 29.0509 - val_loss: 29.2726 - val_MinusLogProbMetric: 29.2726 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 768/1000
2023-10-25 23:23:00.487 
Epoch 768/1000 
	 loss: 29.1349, MinusLogProbMetric: 29.1349, val_loss: 29.2250, val_MinusLogProbMetric: 29.2250

Epoch 768: val_loss improved from 29.26370 to 29.22505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 29.1349 - MinusLogProbMetric: 29.1349 - val_loss: 29.2250 - val_MinusLogProbMetric: 29.2250 - lr: 1.6667e-04 - 35s/epoch - 180ms/step
Epoch 769/1000
2023-10-25 23:23:35.876 
Epoch 769/1000 
	 loss: 29.0486, MinusLogProbMetric: 29.0486, val_loss: 29.5269, val_MinusLogProbMetric: 29.5269

Epoch 769: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.0486 - MinusLogProbMetric: 29.0486 - val_loss: 29.5269 - val_MinusLogProbMetric: 29.5269 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 770/1000
2023-10-25 23:24:10.762 
Epoch 770/1000 
	 loss: 29.0858, MinusLogProbMetric: 29.0858, val_loss: 29.2423, val_MinusLogProbMetric: 29.2423

Epoch 770: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.0858 - MinusLogProbMetric: 29.0858 - val_loss: 29.2423 - val_MinusLogProbMetric: 29.2423 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 771/1000
2023-10-25 23:24:45.623 
Epoch 771/1000 
	 loss: 29.2238, MinusLogProbMetric: 29.2238, val_loss: 29.3251, val_MinusLogProbMetric: 29.3251

Epoch 771: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.2238 - MinusLogProbMetric: 29.2238 - val_loss: 29.3251 - val_MinusLogProbMetric: 29.3251 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 772/1000
2023-10-25 23:25:20.213 
Epoch 772/1000 
	 loss: 29.1361, MinusLogProbMetric: 29.1361, val_loss: 29.2356, val_MinusLogProbMetric: 29.2356

Epoch 772: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.1361 - MinusLogProbMetric: 29.1361 - val_loss: 29.2356 - val_MinusLogProbMetric: 29.2356 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 773/1000
2023-10-25 23:25:55.130 
Epoch 773/1000 
	 loss: 29.1583, MinusLogProbMetric: 29.1583, val_loss: 29.6675, val_MinusLogProbMetric: 29.6675

Epoch 773: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.1583 - MinusLogProbMetric: 29.1583 - val_loss: 29.6675 - val_MinusLogProbMetric: 29.6675 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 774/1000
2023-10-25 23:26:30.010 
Epoch 774/1000 
	 loss: 29.1126, MinusLogProbMetric: 29.1126, val_loss: 29.5245, val_MinusLogProbMetric: 29.5245

Epoch 774: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.1126 - MinusLogProbMetric: 29.1126 - val_loss: 29.5245 - val_MinusLogProbMetric: 29.5245 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 775/1000
2023-10-25 23:27:05.024 
Epoch 775/1000 
	 loss: 29.1017, MinusLogProbMetric: 29.1017, val_loss: 29.3503, val_MinusLogProbMetric: 29.3503

Epoch 775: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.1017 - MinusLogProbMetric: 29.1017 - val_loss: 29.3503 - val_MinusLogProbMetric: 29.3503 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 776/1000
2023-10-25 23:27:39.832 
Epoch 776/1000 
	 loss: 29.1080, MinusLogProbMetric: 29.1080, val_loss: 29.2303, val_MinusLogProbMetric: 29.2303

Epoch 776: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.1080 - MinusLogProbMetric: 29.1080 - val_loss: 29.2303 - val_MinusLogProbMetric: 29.2303 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 777/1000
2023-10-25 23:28:14.633 
Epoch 777/1000 
	 loss: 29.0464, MinusLogProbMetric: 29.0464, val_loss: 29.3508, val_MinusLogProbMetric: 29.3508

Epoch 777: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.0464 - MinusLogProbMetric: 29.0464 - val_loss: 29.3508 - val_MinusLogProbMetric: 29.3508 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 778/1000
2023-10-25 23:28:49.249 
Epoch 778/1000 
	 loss: 29.1447, MinusLogProbMetric: 29.1447, val_loss: 29.2311, val_MinusLogProbMetric: 29.2311

Epoch 778: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.1447 - MinusLogProbMetric: 29.1447 - val_loss: 29.2311 - val_MinusLogProbMetric: 29.2311 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 779/1000
2023-10-25 23:29:24.156 
Epoch 779/1000 
	 loss: 29.0151, MinusLogProbMetric: 29.0151, val_loss: 29.7452, val_MinusLogProbMetric: 29.7452

Epoch 779: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.0151 - MinusLogProbMetric: 29.0151 - val_loss: 29.7452 - val_MinusLogProbMetric: 29.7452 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 780/1000
2023-10-25 23:29:58.896 
Epoch 780/1000 
	 loss: 29.0762, MinusLogProbMetric: 29.0762, val_loss: 29.2539, val_MinusLogProbMetric: 29.2539

Epoch 780: val_loss did not improve from 29.22505
196/196 - 35s - loss: 29.0762 - MinusLogProbMetric: 29.0762 - val_loss: 29.2539 - val_MinusLogProbMetric: 29.2539 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 781/1000
2023-10-25 23:30:33.554 
Epoch 781/1000 
	 loss: 29.0762, MinusLogProbMetric: 29.0762, val_loss: 29.1650, val_MinusLogProbMetric: 29.1650

Epoch 781: val_loss improved from 29.22505 to 29.16501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 29.0762 - MinusLogProbMetric: 29.0762 - val_loss: 29.1650 - val_MinusLogProbMetric: 29.1650 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 782/1000
2023-10-25 23:31:08.656 
Epoch 782/1000 
	 loss: 29.1451, MinusLogProbMetric: 29.1451, val_loss: 29.9161, val_MinusLogProbMetric: 29.9161

Epoch 782: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.1451 - MinusLogProbMetric: 29.1451 - val_loss: 29.9161 - val_MinusLogProbMetric: 29.9161 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 783/1000
2023-10-25 23:31:43.253 
Epoch 783/1000 
	 loss: 29.0817, MinusLogProbMetric: 29.0817, val_loss: 29.2518, val_MinusLogProbMetric: 29.2518

Epoch 783: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0817 - MinusLogProbMetric: 29.0817 - val_loss: 29.2518 - val_MinusLogProbMetric: 29.2518 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 784/1000
2023-10-25 23:32:18.047 
Epoch 784/1000 
	 loss: 29.0646, MinusLogProbMetric: 29.0646, val_loss: 29.2400, val_MinusLogProbMetric: 29.2400

Epoch 784: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0646 - MinusLogProbMetric: 29.0646 - val_loss: 29.2400 - val_MinusLogProbMetric: 29.2400 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 785/1000
2023-10-25 23:32:52.914 
Epoch 785/1000 
	 loss: 29.0705, MinusLogProbMetric: 29.0705, val_loss: 29.3487, val_MinusLogProbMetric: 29.3487

Epoch 785: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0705 - MinusLogProbMetric: 29.0705 - val_loss: 29.3487 - val_MinusLogProbMetric: 29.3487 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 786/1000
2023-10-25 23:33:27.874 
Epoch 786/1000 
	 loss: 29.0195, MinusLogProbMetric: 29.0195, val_loss: 29.5022, val_MinusLogProbMetric: 29.5022

Epoch 786: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0195 - MinusLogProbMetric: 29.0195 - val_loss: 29.5022 - val_MinusLogProbMetric: 29.5022 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 787/1000
2023-10-25 23:34:02.733 
Epoch 787/1000 
	 loss: 29.0781, MinusLogProbMetric: 29.0781, val_loss: 29.6321, val_MinusLogProbMetric: 29.6321

Epoch 787: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0781 - MinusLogProbMetric: 29.0781 - val_loss: 29.6321 - val_MinusLogProbMetric: 29.6321 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 788/1000
2023-10-25 23:34:37.394 
Epoch 788/1000 
	 loss: 29.1085, MinusLogProbMetric: 29.1085, val_loss: 29.4412, val_MinusLogProbMetric: 29.4412

Epoch 788: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.1085 - MinusLogProbMetric: 29.1085 - val_loss: 29.4412 - val_MinusLogProbMetric: 29.4412 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 789/1000
2023-10-25 23:35:12.068 
Epoch 789/1000 
	 loss: 29.0317, MinusLogProbMetric: 29.0317, val_loss: 29.3287, val_MinusLogProbMetric: 29.3287

Epoch 789: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0317 - MinusLogProbMetric: 29.0317 - val_loss: 29.3287 - val_MinusLogProbMetric: 29.3287 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 790/1000
2023-10-25 23:35:46.595 
Epoch 790/1000 
	 loss: 29.1120, MinusLogProbMetric: 29.1120, val_loss: 29.8651, val_MinusLogProbMetric: 29.8651

Epoch 790: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.1120 - MinusLogProbMetric: 29.1120 - val_loss: 29.8651 - val_MinusLogProbMetric: 29.8651 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 791/1000
2023-10-25 23:36:21.400 
Epoch 791/1000 
	 loss: 29.1186, MinusLogProbMetric: 29.1186, val_loss: 29.3911, val_MinusLogProbMetric: 29.3911

Epoch 791: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.1186 - MinusLogProbMetric: 29.1186 - val_loss: 29.3911 - val_MinusLogProbMetric: 29.3911 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 792/1000
2023-10-25 23:36:56.185 
Epoch 792/1000 
	 loss: 29.1145, MinusLogProbMetric: 29.1145, val_loss: 29.2217, val_MinusLogProbMetric: 29.2217

Epoch 792: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.1145 - MinusLogProbMetric: 29.1145 - val_loss: 29.2217 - val_MinusLogProbMetric: 29.2217 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 793/1000
2023-10-25 23:37:30.832 
Epoch 793/1000 
	 loss: 29.1519, MinusLogProbMetric: 29.1519, val_loss: 29.3583, val_MinusLogProbMetric: 29.3583

Epoch 793: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.1519 - MinusLogProbMetric: 29.1519 - val_loss: 29.3583 - val_MinusLogProbMetric: 29.3583 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 794/1000
2023-10-25 23:38:05.551 
Epoch 794/1000 
	 loss: 29.0353, MinusLogProbMetric: 29.0353, val_loss: 29.2304, val_MinusLogProbMetric: 29.2304

Epoch 794: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0353 - MinusLogProbMetric: 29.0353 - val_loss: 29.2304 - val_MinusLogProbMetric: 29.2304 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 795/1000
2023-10-25 23:38:40.266 
Epoch 795/1000 
	 loss: 29.0245, MinusLogProbMetric: 29.0245, val_loss: 29.1929, val_MinusLogProbMetric: 29.1929

Epoch 795: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0245 - MinusLogProbMetric: 29.0245 - val_loss: 29.1929 - val_MinusLogProbMetric: 29.1929 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 796/1000
2023-10-25 23:39:14.833 
Epoch 796/1000 
	 loss: 29.0538, MinusLogProbMetric: 29.0538, val_loss: 29.4272, val_MinusLogProbMetric: 29.4272

Epoch 796: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0538 - MinusLogProbMetric: 29.0538 - val_loss: 29.4272 - val_MinusLogProbMetric: 29.4272 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 797/1000
2023-10-25 23:39:49.722 
Epoch 797/1000 
	 loss: 29.0727, MinusLogProbMetric: 29.0727, val_loss: 29.1877, val_MinusLogProbMetric: 29.1877

Epoch 797: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0727 - MinusLogProbMetric: 29.0727 - val_loss: 29.1877 - val_MinusLogProbMetric: 29.1877 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 798/1000
2023-10-25 23:40:24.566 
Epoch 798/1000 
	 loss: 29.1192, MinusLogProbMetric: 29.1192, val_loss: 29.3381, val_MinusLogProbMetric: 29.3381

Epoch 798: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.1192 - MinusLogProbMetric: 29.1192 - val_loss: 29.3381 - val_MinusLogProbMetric: 29.3381 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 799/1000
2023-10-25 23:40:59.140 
Epoch 799/1000 
	 loss: 29.0178, MinusLogProbMetric: 29.0178, val_loss: 29.1955, val_MinusLogProbMetric: 29.1955

Epoch 799: val_loss did not improve from 29.16501
196/196 - 35s - loss: 29.0178 - MinusLogProbMetric: 29.0178 - val_loss: 29.1955 - val_MinusLogProbMetric: 29.1955 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 800/1000
2023-10-25 23:41:33.986 
Epoch 800/1000 
	 loss: 29.0703, MinusLogProbMetric: 29.0703, val_loss: 29.1501, val_MinusLogProbMetric: 29.1501

Epoch 800: val_loss improved from 29.16501 to 29.15008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 29.0703 - MinusLogProbMetric: 29.0703 - val_loss: 29.1501 - val_MinusLogProbMetric: 29.1501 - lr: 1.6667e-04 - 35s/epoch - 181ms/step
Epoch 801/1000
2023-10-25 23:42:09.455 
Epoch 801/1000 
	 loss: 29.0102, MinusLogProbMetric: 29.0102, val_loss: 29.1985, val_MinusLogProbMetric: 29.1985

Epoch 801: val_loss did not improve from 29.15008
196/196 - 35s - loss: 29.0102 - MinusLogProbMetric: 29.0102 - val_loss: 29.1985 - val_MinusLogProbMetric: 29.1985 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 802/1000
2023-10-25 23:42:44.160 
Epoch 802/1000 
	 loss: 29.1419, MinusLogProbMetric: 29.1419, val_loss: 29.6707, val_MinusLogProbMetric: 29.6707

Epoch 802: val_loss did not improve from 29.15008
196/196 - 35s - loss: 29.1419 - MinusLogProbMetric: 29.1419 - val_loss: 29.6707 - val_MinusLogProbMetric: 29.6707 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 803/1000
2023-10-25 23:43:19.121 
Epoch 803/1000 
	 loss: 28.9810, MinusLogProbMetric: 28.9810, val_loss: 29.1604, val_MinusLogProbMetric: 29.1604

Epoch 803: val_loss did not improve from 29.15008
196/196 - 35s - loss: 28.9810 - MinusLogProbMetric: 28.9810 - val_loss: 29.1604 - val_MinusLogProbMetric: 29.1604 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 804/1000
2023-10-25 23:43:54.087 
Epoch 804/1000 
	 loss: 29.0874, MinusLogProbMetric: 29.0874, val_loss: 29.2215, val_MinusLogProbMetric: 29.2215

Epoch 804: val_loss did not improve from 29.15008
196/196 - 35s - loss: 29.0874 - MinusLogProbMetric: 29.0874 - val_loss: 29.2215 - val_MinusLogProbMetric: 29.2215 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 805/1000
2023-10-25 23:44:28.832 
Epoch 805/1000 
	 loss: 29.0804, MinusLogProbMetric: 29.0804, val_loss: 29.5427, val_MinusLogProbMetric: 29.5427

Epoch 805: val_loss did not improve from 29.15008
196/196 - 35s - loss: 29.0804 - MinusLogProbMetric: 29.0804 - val_loss: 29.5427 - val_MinusLogProbMetric: 29.5427 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 806/1000
2023-10-25 23:45:03.767 
Epoch 806/1000 
	 loss: 28.9973, MinusLogProbMetric: 28.9973, val_loss: 29.9126, val_MinusLogProbMetric: 29.9126

Epoch 806: val_loss did not improve from 29.15008
196/196 - 35s - loss: 28.9973 - MinusLogProbMetric: 28.9973 - val_loss: 29.9126 - val_MinusLogProbMetric: 29.9126 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 807/1000
2023-10-25 23:45:38.612 
Epoch 807/1000 
	 loss: 29.0463, MinusLogProbMetric: 29.0463, val_loss: 29.4391, val_MinusLogProbMetric: 29.4391

Epoch 807: val_loss did not improve from 29.15008
196/196 - 35s - loss: 29.0463 - MinusLogProbMetric: 29.0463 - val_loss: 29.4391 - val_MinusLogProbMetric: 29.4391 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 808/1000
2023-10-25 23:46:13.182 
Epoch 808/1000 
	 loss: 29.0279, MinusLogProbMetric: 29.0279, val_loss: 29.0866, val_MinusLogProbMetric: 29.0866

Epoch 808: val_loss improved from 29.15008 to 29.08662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 29.0279 - MinusLogProbMetric: 29.0279 - val_loss: 29.0866 - val_MinusLogProbMetric: 29.0866 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 809/1000
2023-10-25 23:46:48.223 
Epoch 809/1000 
	 loss: 29.0405, MinusLogProbMetric: 29.0405, val_loss: 29.3070, val_MinusLogProbMetric: 29.3070

Epoch 809: val_loss did not improve from 29.08662
196/196 - 34s - loss: 29.0405 - MinusLogProbMetric: 29.0405 - val_loss: 29.3070 - val_MinusLogProbMetric: 29.3070 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 810/1000
2023-10-25 23:47:22.939 
Epoch 810/1000 
	 loss: 29.0678, MinusLogProbMetric: 29.0678, val_loss: 29.1535, val_MinusLogProbMetric: 29.1535

Epoch 810: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0678 - MinusLogProbMetric: 29.0678 - val_loss: 29.1535 - val_MinusLogProbMetric: 29.1535 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 811/1000
2023-10-25 23:47:56.967 
Epoch 811/1000 
	 loss: 28.9750, MinusLogProbMetric: 28.9750, val_loss: 29.4207, val_MinusLogProbMetric: 29.4207

Epoch 811: val_loss did not improve from 29.08662
196/196 - 34s - loss: 28.9750 - MinusLogProbMetric: 28.9750 - val_loss: 29.4207 - val_MinusLogProbMetric: 29.4207 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 812/1000
2023-10-25 23:48:31.815 
Epoch 812/1000 
	 loss: 29.0737, MinusLogProbMetric: 29.0737, val_loss: 29.2548, val_MinusLogProbMetric: 29.2548

Epoch 812: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0737 - MinusLogProbMetric: 29.0737 - val_loss: 29.2548 - val_MinusLogProbMetric: 29.2548 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 813/1000
2023-10-25 23:49:06.617 
Epoch 813/1000 
	 loss: 29.0485, MinusLogProbMetric: 29.0485, val_loss: 29.1723, val_MinusLogProbMetric: 29.1723

Epoch 813: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0485 - MinusLogProbMetric: 29.0485 - val_loss: 29.1723 - val_MinusLogProbMetric: 29.1723 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 814/1000
2023-10-25 23:49:41.318 
Epoch 814/1000 
	 loss: 29.0081, MinusLogProbMetric: 29.0081, val_loss: 29.3254, val_MinusLogProbMetric: 29.3254

Epoch 814: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0081 - MinusLogProbMetric: 29.0081 - val_loss: 29.3254 - val_MinusLogProbMetric: 29.3254 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 815/1000
2023-10-25 23:50:16.216 
Epoch 815/1000 
	 loss: 29.1113, MinusLogProbMetric: 29.1113, val_loss: 29.4573, val_MinusLogProbMetric: 29.4573

Epoch 815: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.1113 - MinusLogProbMetric: 29.1113 - val_loss: 29.4573 - val_MinusLogProbMetric: 29.4573 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 816/1000
2023-10-25 23:50:51.006 
Epoch 816/1000 
	 loss: 29.0860, MinusLogProbMetric: 29.0860, val_loss: 29.2318, val_MinusLogProbMetric: 29.2318

Epoch 816: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0860 - MinusLogProbMetric: 29.0860 - val_loss: 29.2318 - val_MinusLogProbMetric: 29.2318 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 817/1000
2023-10-25 23:51:25.561 
Epoch 817/1000 
	 loss: 28.9509, MinusLogProbMetric: 28.9509, val_loss: 29.3595, val_MinusLogProbMetric: 29.3595

Epoch 817: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9509 - MinusLogProbMetric: 28.9509 - val_loss: 29.3595 - val_MinusLogProbMetric: 29.3595 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 818/1000
2023-10-25 23:52:00.090 
Epoch 818/1000 
	 loss: 28.9938, MinusLogProbMetric: 28.9938, val_loss: 29.2690, val_MinusLogProbMetric: 29.2690

Epoch 818: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9938 - MinusLogProbMetric: 28.9938 - val_loss: 29.2690 - val_MinusLogProbMetric: 29.2690 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 819/1000
2023-10-25 23:52:35.004 
Epoch 819/1000 
	 loss: 29.0776, MinusLogProbMetric: 29.0776, val_loss: 29.5767, val_MinusLogProbMetric: 29.5767

Epoch 819: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0776 - MinusLogProbMetric: 29.0776 - val_loss: 29.5767 - val_MinusLogProbMetric: 29.5767 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 820/1000
2023-10-25 23:53:09.630 
Epoch 820/1000 
	 loss: 29.0582, MinusLogProbMetric: 29.0582, val_loss: 29.2770, val_MinusLogProbMetric: 29.2770

Epoch 820: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0582 - MinusLogProbMetric: 29.0582 - val_loss: 29.2770 - val_MinusLogProbMetric: 29.2770 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 821/1000
2023-10-25 23:53:44.739 
Epoch 821/1000 
	 loss: 29.0005, MinusLogProbMetric: 29.0005, val_loss: 29.3136, val_MinusLogProbMetric: 29.3136

Epoch 821: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0005 - MinusLogProbMetric: 29.0005 - val_loss: 29.3136 - val_MinusLogProbMetric: 29.3136 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 822/1000
2023-10-25 23:54:19.462 
Epoch 822/1000 
	 loss: 29.1121, MinusLogProbMetric: 29.1121, val_loss: 29.4150, val_MinusLogProbMetric: 29.4150

Epoch 822: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.1121 - MinusLogProbMetric: 29.1121 - val_loss: 29.4150 - val_MinusLogProbMetric: 29.4150 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 823/1000
2023-10-25 23:54:54.038 
Epoch 823/1000 
	 loss: 29.0516, MinusLogProbMetric: 29.0516, val_loss: 29.3443, val_MinusLogProbMetric: 29.3443

Epoch 823: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0516 - MinusLogProbMetric: 29.0516 - val_loss: 29.3443 - val_MinusLogProbMetric: 29.3443 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 824/1000
2023-10-25 23:55:28.604 
Epoch 824/1000 
	 loss: 28.9759, MinusLogProbMetric: 28.9759, val_loss: 29.1499, val_MinusLogProbMetric: 29.1499

Epoch 824: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9759 - MinusLogProbMetric: 28.9759 - val_loss: 29.1499 - val_MinusLogProbMetric: 29.1499 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 825/1000
2023-10-25 23:56:03.382 
Epoch 825/1000 
	 loss: 29.0558, MinusLogProbMetric: 29.0558, val_loss: 29.3547, val_MinusLogProbMetric: 29.3547

Epoch 825: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0558 - MinusLogProbMetric: 29.0558 - val_loss: 29.3547 - val_MinusLogProbMetric: 29.3547 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 826/1000
2023-10-25 23:56:37.761 
Epoch 826/1000 
	 loss: 29.0175, MinusLogProbMetric: 29.0175, val_loss: 29.4397, val_MinusLogProbMetric: 29.4397

Epoch 826: val_loss did not improve from 29.08662
196/196 - 34s - loss: 29.0175 - MinusLogProbMetric: 29.0175 - val_loss: 29.4397 - val_MinusLogProbMetric: 29.4397 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 827/1000
2023-10-25 23:57:12.349 
Epoch 827/1000 
	 loss: 29.0434, MinusLogProbMetric: 29.0434, val_loss: 29.4145, val_MinusLogProbMetric: 29.4145

Epoch 827: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0434 - MinusLogProbMetric: 29.0434 - val_loss: 29.4145 - val_MinusLogProbMetric: 29.4145 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 828/1000
2023-10-25 23:57:46.812 
Epoch 828/1000 
	 loss: 29.0536, MinusLogProbMetric: 29.0536, val_loss: 29.7084, val_MinusLogProbMetric: 29.7084

Epoch 828: val_loss did not improve from 29.08662
196/196 - 34s - loss: 29.0536 - MinusLogProbMetric: 29.0536 - val_loss: 29.7084 - val_MinusLogProbMetric: 29.7084 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 829/1000
2023-10-25 23:58:21.373 
Epoch 829/1000 
	 loss: 28.9522, MinusLogProbMetric: 28.9522, val_loss: 29.3749, val_MinusLogProbMetric: 29.3749

Epoch 829: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9522 - MinusLogProbMetric: 28.9522 - val_loss: 29.3749 - val_MinusLogProbMetric: 29.3749 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 830/1000
2023-10-25 23:58:56.165 
Epoch 830/1000 
	 loss: 29.0467, MinusLogProbMetric: 29.0467, val_loss: 29.3907, val_MinusLogProbMetric: 29.3907

Epoch 830: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0467 - MinusLogProbMetric: 29.0467 - val_loss: 29.3907 - val_MinusLogProbMetric: 29.3907 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 831/1000
2023-10-25 23:59:30.923 
Epoch 831/1000 
	 loss: 29.0600, MinusLogProbMetric: 29.0600, val_loss: 29.3738, val_MinusLogProbMetric: 29.3738

Epoch 831: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0600 - MinusLogProbMetric: 29.0600 - val_loss: 29.3738 - val_MinusLogProbMetric: 29.3738 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 832/1000
2023-10-26 00:00:05.811 
Epoch 832/1000 
	 loss: 28.9665, MinusLogProbMetric: 28.9665, val_loss: 29.3867, val_MinusLogProbMetric: 29.3867

Epoch 832: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9665 - MinusLogProbMetric: 28.9665 - val_loss: 29.3867 - val_MinusLogProbMetric: 29.3867 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 833/1000
2023-10-26 00:00:40.354 
Epoch 833/1000 
	 loss: 29.3426, MinusLogProbMetric: 29.3426, val_loss: 29.3105, val_MinusLogProbMetric: 29.3105

Epoch 833: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.3426 - MinusLogProbMetric: 29.3426 - val_loss: 29.3105 - val_MinusLogProbMetric: 29.3105 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 834/1000
2023-10-26 00:01:10.541 
Epoch 834/1000 
	 loss: 28.9449, MinusLogProbMetric: 28.9449, val_loss: 29.1447, val_MinusLogProbMetric: 29.1447

Epoch 834: val_loss did not improve from 29.08662
196/196 - 30s - loss: 28.9449 - MinusLogProbMetric: 28.9449 - val_loss: 29.1447 - val_MinusLogProbMetric: 29.1447 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 835/1000
2023-10-26 00:01:40.891 
Epoch 835/1000 
	 loss: 29.0218, MinusLogProbMetric: 29.0218, val_loss: 29.3175, val_MinusLogProbMetric: 29.3175

Epoch 835: val_loss did not improve from 29.08662
196/196 - 30s - loss: 29.0218 - MinusLogProbMetric: 29.0218 - val_loss: 29.3175 - val_MinusLogProbMetric: 29.3175 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 836/1000
2023-10-26 00:02:15.364 
Epoch 836/1000 
	 loss: 28.9501, MinusLogProbMetric: 28.9501, val_loss: 29.3236, val_MinusLogProbMetric: 29.3236

Epoch 836: val_loss did not improve from 29.08662
196/196 - 34s - loss: 28.9501 - MinusLogProbMetric: 28.9501 - val_loss: 29.3236 - val_MinusLogProbMetric: 29.3236 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 837/1000
2023-10-26 00:02:50.068 
Epoch 837/1000 
	 loss: 29.0164, MinusLogProbMetric: 29.0164, val_loss: 29.5957, val_MinusLogProbMetric: 29.5957

Epoch 837: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0164 - MinusLogProbMetric: 29.0164 - val_loss: 29.5957 - val_MinusLogProbMetric: 29.5957 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 838/1000
2023-10-26 00:03:24.915 
Epoch 838/1000 
	 loss: 29.1239, MinusLogProbMetric: 29.1239, val_loss: 29.2513, val_MinusLogProbMetric: 29.2513

Epoch 838: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.1239 - MinusLogProbMetric: 29.1239 - val_loss: 29.2513 - val_MinusLogProbMetric: 29.2513 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 839/1000
2023-10-26 00:03:59.621 
Epoch 839/1000 
	 loss: 28.9867, MinusLogProbMetric: 28.9867, val_loss: 29.4316, val_MinusLogProbMetric: 29.4316

Epoch 839: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9867 - MinusLogProbMetric: 28.9867 - val_loss: 29.4316 - val_MinusLogProbMetric: 29.4316 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 840/1000
2023-10-26 00:04:33.532 
Epoch 840/1000 
	 loss: 28.9570, MinusLogProbMetric: 28.9570, val_loss: 29.2915, val_MinusLogProbMetric: 29.2915

Epoch 840: val_loss did not improve from 29.08662
196/196 - 34s - loss: 28.9570 - MinusLogProbMetric: 28.9570 - val_loss: 29.2915 - val_MinusLogProbMetric: 29.2915 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 841/1000
2023-10-26 00:05:01.402 
Epoch 841/1000 
	 loss: 29.0087, MinusLogProbMetric: 29.0087, val_loss: 29.4320, val_MinusLogProbMetric: 29.4320

Epoch 841: val_loss did not improve from 29.08662
196/196 - 28s - loss: 29.0087 - MinusLogProbMetric: 29.0087 - val_loss: 29.4320 - val_MinusLogProbMetric: 29.4320 - lr: 1.6667e-04 - 28s/epoch - 142ms/step
Epoch 842/1000
2023-10-26 00:05:35.394 
Epoch 842/1000 
	 loss: 29.0021, MinusLogProbMetric: 29.0021, val_loss: 29.3667, val_MinusLogProbMetric: 29.3667

Epoch 842: val_loss did not improve from 29.08662
196/196 - 34s - loss: 29.0021 - MinusLogProbMetric: 29.0021 - val_loss: 29.3667 - val_MinusLogProbMetric: 29.3667 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 843/1000
2023-10-26 00:06:10.302 
Epoch 843/1000 
	 loss: 29.0687, MinusLogProbMetric: 29.0687, val_loss: 29.6250, val_MinusLogProbMetric: 29.6250

Epoch 843: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0687 - MinusLogProbMetric: 29.0687 - val_loss: 29.6250 - val_MinusLogProbMetric: 29.6250 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 844/1000
2023-10-26 00:06:45.025 
Epoch 844/1000 
	 loss: 29.0225, MinusLogProbMetric: 29.0225, val_loss: 29.4062, val_MinusLogProbMetric: 29.4062

Epoch 844: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0225 - MinusLogProbMetric: 29.0225 - val_loss: 29.4062 - val_MinusLogProbMetric: 29.4062 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 845/1000
2023-10-26 00:07:20.043 
Epoch 845/1000 
	 loss: 28.9802, MinusLogProbMetric: 28.9802, val_loss: 29.2313, val_MinusLogProbMetric: 29.2313

Epoch 845: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9802 - MinusLogProbMetric: 28.9802 - val_loss: 29.2313 - val_MinusLogProbMetric: 29.2313 - lr: 1.6667e-04 - 35s/epoch - 179ms/step
Epoch 846/1000
2023-10-26 00:07:54.995 
Epoch 846/1000 
	 loss: 28.9758, MinusLogProbMetric: 28.9758, val_loss: 29.1739, val_MinusLogProbMetric: 29.1739

Epoch 846: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9758 - MinusLogProbMetric: 28.9758 - val_loss: 29.1739 - val_MinusLogProbMetric: 29.1739 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 847/1000
2023-10-26 00:08:29.688 
Epoch 847/1000 
	 loss: 28.9379, MinusLogProbMetric: 28.9379, val_loss: 29.3240, val_MinusLogProbMetric: 29.3240

Epoch 847: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9379 - MinusLogProbMetric: 28.9379 - val_loss: 29.3240 - val_MinusLogProbMetric: 29.3240 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 848/1000
2023-10-26 00:09:04.446 
Epoch 848/1000 
	 loss: 28.9625, MinusLogProbMetric: 28.9625, val_loss: 29.1816, val_MinusLogProbMetric: 29.1816

Epoch 848: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9625 - MinusLogProbMetric: 28.9625 - val_loss: 29.1816 - val_MinusLogProbMetric: 29.1816 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 849/1000
2023-10-26 00:09:39.337 
Epoch 849/1000 
	 loss: 29.0124, MinusLogProbMetric: 29.0124, val_loss: 29.3638, val_MinusLogProbMetric: 29.3638

Epoch 849: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0124 - MinusLogProbMetric: 29.0124 - val_loss: 29.3638 - val_MinusLogProbMetric: 29.3638 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 850/1000
2023-10-26 00:10:14.321 
Epoch 850/1000 
	 loss: 28.9457, MinusLogProbMetric: 28.9457, val_loss: 29.2929, val_MinusLogProbMetric: 29.2929

Epoch 850: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9457 - MinusLogProbMetric: 28.9457 - val_loss: 29.2929 - val_MinusLogProbMetric: 29.2929 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 851/1000
2023-10-26 00:10:49.178 
Epoch 851/1000 
	 loss: 29.1293, MinusLogProbMetric: 29.1293, val_loss: 29.4423, val_MinusLogProbMetric: 29.4423

Epoch 851: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.1293 - MinusLogProbMetric: 29.1293 - val_loss: 29.4423 - val_MinusLogProbMetric: 29.4423 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 852/1000
2023-10-26 00:11:23.917 
Epoch 852/1000 
	 loss: 28.9914, MinusLogProbMetric: 28.9914, val_loss: 29.1923, val_MinusLogProbMetric: 29.1923

Epoch 852: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9914 - MinusLogProbMetric: 28.9914 - val_loss: 29.1923 - val_MinusLogProbMetric: 29.1923 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 853/1000
2023-10-26 00:11:58.360 
Epoch 853/1000 
	 loss: 29.0555, MinusLogProbMetric: 29.0555, val_loss: 29.2335, val_MinusLogProbMetric: 29.2335

Epoch 853: val_loss did not improve from 29.08662
196/196 - 34s - loss: 29.0555 - MinusLogProbMetric: 29.0555 - val_loss: 29.2335 - val_MinusLogProbMetric: 29.2335 - lr: 1.6667e-04 - 34s/epoch - 176ms/step
Epoch 854/1000
2023-10-26 00:12:33.055 
Epoch 854/1000 
	 loss: 28.9878, MinusLogProbMetric: 28.9878, val_loss: 29.2669, val_MinusLogProbMetric: 29.2669

Epoch 854: val_loss did not improve from 29.08662
196/196 - 35s - loss: 28.9878 - MinusLogProbMetric: 28.9878 - val_loss: 29.2669 - val_MinusLogProbMetric: 29.2669 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 855/1000
2023-10-26 00:13:07.903 
Epoch 855/1000 
	 loss: 29.0874, MinusLogProbMetric: 29.0874, val_loss: 29.3211, val_MinusLogProbMetric: 29.3211

Epoch 855: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0874 - MinusLogProbMetric: 29.0874 - val_loss: 29.3211 - val_MinusLogProbMetric: 29.3211 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 856/1000
2023-10-26 00:13:42.810 
Epoch 856/1000 
	 loss: 29.0327, MinusLogProbMetric: 29.0327, val_loss: 29.1193, val_MinusLogProbMetric: 29.1193

Epoch 856: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0327 - MinusLogProbMetric: 29.0327 - val_loss: 29.1193 - val_MinusLogProbMetric: 29.1193 - lr: 1.6667e-04 - 35s/epoch - 178ms/step
Epoch 857/1000
2023-10-26 00:14:17.508 
Epoch 857/1000 
	 loss: 29.0281, MinusLogProbMetric: 29.0281, val_loss: 29.3630, val_MinusLogProbMetric: 29.3630

Epoch 857: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0281 - MinusLogProbMetric: 29.0281 - val_loss: 29.3630 - val_MinusLogProbMetric: 29.3630 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 858/1000
2023-10-26 00:14:52.064 
Epoch 858/1000 
	 loss: 29.0280, MinusLogProbMetric: 29.0280, val_loss: 29.5751, val_MinusLogProbMetric: 29.5751

Epoch 858: val_loss did not improve from 29.08662
196/196 - 35s - loss: 29.0280 - MinusLogProbMetric: 29.0280 - val_loss: 29.5751 - val_MinusLogProbMetric: 29.5751 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 859/1000
2023-10-26 00:15:26.933 
Epoch 859/1000 
	 loss: 28.7413, MinusLogProbMetric: 28.7413, val_loss: 29.0220, val_MinusLogProbMetric: 29.0220

Epoch 859: val_loss improved from 29.08662 to 29.02201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 28.7413 - MinusLogProbMetric: 28.7413 - val_loss: 29.0220 - val_MinusLogProbMetric: 29.0220 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 860/1000
2023-10-26 00:16:02.101 
Epoch 860/1000 
	 loss: 28.7091, MinusLogProbMetric: 28.7091, val_loss: 29.0504, val_MinusLogProbMetric: 29.0504

Epoch 860: val_loss did not improve from 29.02201
196/196 - 35s - loss: 28.7091 - MinusLogProbMetric: 28.7091 - val_loss: 29.0504 - val_MinusLogProbMetric: 29.0504 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 861/1000
2023-10-26 00:16:37.112 
Epoch 861/1000 
	 loss: 28.7155, MinusLogProbMetric: 28.7155, val_loss: 29.1185, val_MinusLogProbMetric: 29.1185

Epoch 861: val_loss did not improve from 29.02201
196/196 - 35s - loss: 28.7155 - MinusLogProbMetric: 28.7155 - val_loss: 29.1185 - val_MinusLogProbMetric: 29.1185 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 862/1000
2023-10-26 00:17:12.175 
Epoch 862/1000 
	 loss: 28.7320, MinusLogProbMetric: 28.7320, val_loss: 29.0711, val_MinusLogProbMetric: 29.0711

Epoch 862: val_loss did not improve from 29.02201
196/196 - 35s - loss: 28.7320 - MinusLogProbMetric: 28.7320 - val_loss: 29.0711 - val_MinusLogProbMetric: 29.0711 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 863/1000
2023-10-26 00:17:46.981 
Epoch 863/1000 
	 loss: 28.7918, MinusLogProbMetric: 28.7918, val_loss: 29.0241, val_MinusLogProbMetric: 29.0241

Epoch 863: val_loss did not improve from 29.02201
196/196 - 35s - loss: 28.7918 - MinusLogProbMetric: 28.7918 - val_loss: 29.0241 - val_MinusLogProbMetric: 29.0241 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 864/1000
2023-10-26 00:18:21.833 
Epoch 864/1000 
	 loss: 28.7103, MinusLogProbMetric: 28.7103, val_loss: 29.0049, val_MinusLogProbMetric: 29.0049

Epoch 864: val_loss improved from 29.02201 to 29.00487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 28.7103 - MinusLogProbMetric: 28.7103 - val_loss: 29.0049 - val_MinusLogProbMetric: 29.0049 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 865/1000
2023-10-26 00:18:57.408 
Epoch 865/1000 
	 loss: 28.7278, MinusLogProbMetric: 28.7278, val_loss: 29.1947, val_MinusLogProbMetric: 29.1947

Epoch 865: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7278 - MinusLogProbMetric: 28.7278 - val_loss: 29.1947 - val_MinusLogProbMetric: 29.1947 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 866/1000
2023-10-26 00:19:32.217 
Epoch 866/1000 
	 loss: 28.7691, MinusLogProbMetric: 28.7691, val_loss: 29.0983, val_MinusLogProbMetric: 29.0983

Epoch 866: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7691 - MinusLogProbMetric: 28.7691 - val_loss: 29.0983 - val_MinusLogProbMetric: 29.0983 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 867/1000
2023-10-26 00:20:06.908 
Epoch 867/1000 
	 loss: 28.7265, MinusLogProbMetric: 28.7265, val_loss: 29.0660, val_MinusLogProbMetric: 29.0660

Epoch 867: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7265 - MinusLogProbMetric: 28.7265 - val_loss: 29.0660 - val_MinusLogProbMetric: 29.0660 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 868/1000
2023-10-26 00:20:41.666 
Epoch 868/1000 
	 loss: 28.7481, MinusLogProbMetric: 28.7481, val_loss: 29.0754, val_MinusLogProbMetric: 29.0754

Epoch 868: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7481 - MinusLogProbMetric: 28.7481 - val_loss: 29.0754 - val_MinusLogProbMetric: 29.0754 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 869/1000
2023-10-26 00:21:16.559 
Epoch 869/1000 
	 loss: 28.7428, MinusLogProbMetric: 28.7428, val_loss: 29.0726, val_MinusLogProbMetric: 29.0726

Epoch 869: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7428 - MinusLogProbMetric: 28.7428 - val_loss: 29.0726 - val_MinusLogProbMetric: 29.0726 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 870/1000
2023-10-26 00:21:51.419 
Epoch 870/1000 
	 loss: 28.7041, MinusLogProbMetric: 28.7041, val_loss: 29.1870, val_MinusLogProbMetric: 29.1870

Epoch 870: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7041 - MinusLogProbMetric: 28.7041 - val_loss: 29.1870 - val_MinusLogProbMetric: 29.1870 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 871/1000
2023-10-26 00:22:26.192 
Epoch 871/1000 
	 loss: 28.7192, MinusLogProbMetric: 28.7192, val_loss: 29.0942, val_MinusLogProbMetric: 29.0942

Epoch 871: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7192 - MinusLogProbMetric: 28.7192 - val_loss: 29.0942 - val_MinusLogProbMetric: 29.0942 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 872/1000
2023-10-26 00:23:01.112 
Epoch 872/1000 
	 loss: 28.7705, MinusLogProbMetric: 28.7705, val_loss: 29.1288, val_MinusLogProbMetric: 29.1288

Epoch 872: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7705 - MinusLogProbMetric: 28.7705 - val_loss: 29.1288 - val_MinusLogProbMetric: 29.1288 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 873/1000
2023-10-26 00:23:36.241 
Epoch 873/1000 
	 loss: 28.7376, MinusLogProbMetric: 28.7376, val_loss: 29.1098, val_MinusLogProbMetric: 29.1098

Epoch 873: val_loss did not improve from 29.00487
196/196 - 35s - loss: 28.7376 - MinusLogProbMetric: 28.7376 - val_loss: 29.1098 - val_MinusLogProbMetric: 29.1098 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 874/1000
2023-10-26 00:24:11.150 
Epoch 874/1000 
	 loss: 28.6973, MinusLogProbMetric: 28.6973, val_loss: 28.9882, val_MinusLogProbMetric: 28.9882

Epoch 874: val_loss improved from 29.00487 to 28.98818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 28.6973 - MinusLogProbMetric: 28.6973 - val_loss: 28.9882 - val_MinusLogProbMetric: 28.9882 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 875/1000
2023-10-26 00:24:46.620 
Epoch 875/1000 
	 loss: 28.7241, MinusLogProbMetric: 28.7241, val_loss: 29.0206, val_MinusLogProbMetric: 29.0206

Epoch 875: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7241 - MinusLogProbMetric: 28.7241 - val_loss: 29.0206 - val_MinusLogProbMetric: 29.0206 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 876/1000
2023-10-26 00:25:21.211 
Epoch 876/1000 
	 loss: 28.7524, MinusLogProbMetric: 28.7524, val_loss: 29.0416, val_MinusLogProbMetric: 29.0416

Epoch 876: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7524 - MinusLogProbMetric: 28.7524 - val_loss: 29.0416 - val_MinusLogProbMetric: 29.0416 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 877/1000
2023-10-26 00:25:55.987 
Epoch 877/1000 
	 loss: 28.7235, MinusLogProbMetric: 28.7235, val_loss: 29.1275, val_MinusLogProbMetric: 29.1275

Epoch 877: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7235 - MinusLogProbMetric: 28.7235 - val_loss: 29.1275 - val_MinusLogProbMetric: 29.1275 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 878/1000
2023-10-26 00:26:30.733 
Epoch 878/1000 
	 loss: 28.7219, MinusLogProbMetric: 28.7219, val_loss: 29.0707, val_MinusLogProbMetric: 29.0707

Epoch 878: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7219 - MinusLogProbMetric: 28.7219 - val_loss: 29.0707 - val_MinusLogProbMetric: 29.0707 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 879/1000
2023-10-26 00:27:05.494 
Epoch 879/1000 
	 loss: 28.7181, MinusLogProbMetric: 28.7181, val_loss: 29.0410, val_MinusLogProbMetric: 29.0410

Epoch 879: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7181 - MinusLogProbMetric: 28.7181 - val_loss: 29.0410 - val_MinusLogProbMetric: 29.0410 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 880/1000
2023-10-26 00:27:40.186 
Epoch 880/1000 
	 loss: 28.7100, MinusLogProbMetric: 28.7100, val_loss: 28.9921, val_MinusLogProbMetric: 28.9921

Epoch 880: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7100 - MinusLogProbMetric: 28.7100 - val_loss: 28.9921 - val_MinusLogProbMetric: 28.9921 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 881/1000
2023-10-26 00:28:14.842 
Epoch 881/1000 
	 loss: 28.7119, MinusLogProbMetric: 28.7119, val_loss: 29.0767, val_MinusLogProbMetric: 29.0767

Epoch 881: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7119 - MinusLogProbMetric: 28.7119 - val_loss: 29.0767 - val_MinusLogProbMetric: 29.0767 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 882/1000
2023-10-26 00:28:49.644 
Epoch 882/1000 
	 loss: 28.7255, MinusLogProbMetric: 28.7255, val_loss: 29.0935, val_MinusLogProbMetric: 29.0935

Epoch 882: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7255 - MinusLogProbMetric: 28.7255 - val_loss: 29.0935 - val_MinusLogProbMetric: 29.0935 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 883/1000
2023-10-26 00:29:24.446 
Epoch 883/1000 
	 loss: 28.7591, MinusLogProbMetric: 28.7591, val_loss: 28.9937, val_MinusLogProbMetric: 28.9937

Epoch 883: val_loss did not improve from 28.98818
196/196 - 35s - loss: 28.7591 - MinusLogProbMetric: 28.7591 - val_loss: 28.9937 - val_MinusLogProbMetric: 28.9937 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 884/1000
2023-10-26 00:29:59.070 
Epoch 884/1000 
	 loss: 28.7155, MinusLogProbMetric: 28.7155, val_loss: 28.9658, val_MinusLogProbMetric: 28.9658

Epoch 884: val_loss improved from 28.98818 to 28.96580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 28.7155 - MinusLogProbMetric: 28.7155 - val_loss: 28.9658 - val_MinusLogProbMetric: 28.9658 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 885/1000
2023-10-26 00:30:34.406 
Epoch 885/1000 
	 loss: 28.7322, MinusLogProbMetric: 28.7322, val_loss: 29.0190, val_MinusLogProbMetric: 29.0190

Epoch 885: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7322 - MinusLogProbMetric: 28.7322 - val_loss: 29.0190 - val_MinusLogProbMetric: 29.0190 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 886/1000
2023-10-26 00:31:09.299 
Epoch 886/1000 
	 loss: 28.6946, MinusLogProbMetric: 28.6946, val_loss: 29.1373, val_MinusLogProbMetric: 29.1373

Epoch 886: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.6946 - MinusLogProbMetric: 28.6946 - val_loss: 29.1373 - val_MinusLogProbMetric: 29.1373 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 887/1000
2023-10-26 00:31:44.052 
Epoch 887/1000 
	 loss: 28.6892, MinusLogProbMetric: 28.6892, val_loss: 29.0636, val_MinusLogProbMetric: 29.0636

Epoch 887: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.6892 - MinusLogProbMetric: 28.6892 - val_loss: 29.0636 - val_MinusLogProbMetric: 29.0636 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 888/1000
2023-10-26 00:32:18.637 
Epoch 888/1000 
	 loss: 28.7430, MinusLogProbMetric: 28.7430, val_loss: 29.1873, val_MinusLogProbMetric: 29.1873

Epoch 888: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7430 - MinusLogProbMetric: 28.7430 - val_loss: 29.1873 - val_MinusLogProbMetric: 29.1873 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 889/1000
2023-10-26 00:32:53.433 
Epoch 889/1000 
	 loss: 28.6962, MinusLogProbMetric: 28.6962, val_loss: 29.4298, val_MinusLogProbMetric: 29.4298

Epoch 889: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.6962 - MinusLogProbMetric: 28.6962 - val_loss: 29.4298 - val_MinusLogProbMetric: 29.4298 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 890/1000
2023-10-26 00:33:28.288 
Epoch 890/1000 
	 loss: 28.7383, MinusLogProbMetric: 28.7383, val_loss: 29.0874, val_MinusLogProbMetric: 29.0874

Epoch 890: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7383 - MinusLogProbMetric: 28.7383 - val_loss: 29.0874 - val_MinusLogProbMetric: 29.0874 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 891/1000
2023-10-26 00:34:03.225 
Epoch 891/1000 
	 loss: 28.7121, MinusLogProbMetric: 28.7121, val_loss: 29.0976, val_MinusLogProbMetric: 29.0976

Epoch 891: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7121 - MinusLogProbMetric: 28.7121 - val_loss: 29.0976 - val_MinusLogProbMetric: 29.0976 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 892/1000
2023-10-26 00:34:37.916 
Epoch 892/1000 
	 loss: 28.7120, MinusLogProbMetric: 28.7120, val_loss: 29.1166, val_MinusLogProbMetric: 29.1166

Epoch 892: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7120 - MinusLogProbMetric: 28.7120 - val_loss: 29.1166 - val_MinusLogProbMetric: 29.1166 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 893/1000
2023-10-26 00:35:12.917 
Epoch 893/1000 
	 loss: 28.7520, MinusLogProbMetric: 28.7520, val_loss: 29.0839, val_MinusLogProbMetric: 29.0839

Epoch 893: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7520 - MinusLogProbMetric: 28.7520 - val_loss: 29.0839 - val_MinusLogProbMetric: 29.0839 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 894/1000
2023-10-26 00:35:47.933 
Epoch 894/1000 
	 loss: 28.7652, MinusLogProbMetric: 28.7652, val_loss: 29.0125, val_MinusLogProbMetric: 29.0125

Epoch 894: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7652 - MinusLogProbMetric: 28.7652 - val_loss: 29.0125 - val_MinusLogProbMetric: 29.0125 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 895/1000
2023-10-26 00:36:22.737 
Epoch 895/1000 
	 loss: 28.7052, MinusLogProbMetric: 28.7052, val_loss: 29.0786, val_MinusLogProbMetric: 29.0786

Epoch 895: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7052 - MinusLogProbMetric: 28.7052 - val_loss: 29.0786 - val_MinusLogProbMetric: 29.0786 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 896/1000
2023-10-26 00:36:57.618 
Epoch 896/1000 
	 loss: 28.7832, MinusLogProbMetric: 28.7832, val_loss: 29.0731, val_MinusLogProbMetric: 29.0731

Epoch 896: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7832 - MinusLogProbMetric: 28.7832 - val_loss: 29.0731 - val_MinusLogProbMetric: 29.0731 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 897/1000
2023-10-26 00:37:32.644 
Epoch 897/1000 
	 loss: 28.7348, MinusLogProbMetric: 28.7348, val_loss: 29.0325, val_MinusLogProbMetric: 29.0325

Epoch 897: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7348 - MinusLogProbMetric: 28.7348 - val_loss: 29.0325 - val_MinusLogProbMetric: 29.0325 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 898/1000
2023-10-26 00:38:07.422 
Epoch 898/1000 
	 loss: 28.7742, MinusLogProbMetric: 28.7742, val_loss: 28.9860, val_MinusLogProbMetric: 28.9860

Epoch 898: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7742 - MinusLogProbMetric: 28.7742 - val_loss: 28.9860 - val_MinusLogProbMetric: 28.9860 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 899/1000
2023-10-26 00:38:42.129 
Epoch 899/1000 
	 loss: 28.6820, MinusLogProbMetric: 28.6820, val_loss: 28.9838, val_MinusLogProbMetric: 28.9838

Epoch 899: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.6820 - MinusLogProbMetric: 28.6820 - val_loss: 28.9838 - val_MinusLogProbMetric: 28.9838 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 900/1000
2023-10-26 00:39:16.970 
Epoch 900/1000 
	 loss: 28.7794, MinusLogProbMetric: 28.7794, val_loss: 29.0179, val_MinusLogProbMetric: 29.0179

Epoch 900: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7794 - MinusLogProbMetric: 28.7794 - val_loss: 29.0179 - val_MinusLogProbMetric: 29.0179 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 901/1000
2023-10-26 00:39:51.881 
Epoch 901/1000 
	 loss: 28.7397, MinusLogProbMetric: 28.7397, val_loss: 29.3155, val_MinusLogProbMetric: 29.3155

Epoch 901: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7397 - MinusLogProbMetric: 28.7397 - val_loss: 29.3155 - val_MinusLogProbMetric: 29.3155 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 902/1000
2023-10-26 00:40:26.716 
Epoch 902/1000 
	 loss: 28.7673, MinusLogProbMetric: 28.7673, val_loss: 28.9787, val_MinusLogProbMetric: 28.9787

Epoch 902: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7673 - MinusLogProbMetric: 28.7673 - val_loss: 28.9787 - val_MinusLogProbMetric: 28.9787 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 903/1000
2023-10-26 00:41:01.444 
Epoch 903/1000 
	 loss: 28.7916, MinusLogProbMetric: 28.7916, val_loss: 29.2321, val_MinusLogProbMetric: 29.2321

Epoch 903: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7916 - MinusLogProbMetric: 28.7916 - val_loss: 29.2321 - val_MinusLogProbMetric: 29.2321 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 904/1000
2023-10-26 00:41:36.261 
Epoch 904/1000 
	 loss: 28.7912, MinusLogProbMetric: 28.7912, val_loss: 29.0294, val_MinusLogProbMetric: 29.0294

Epoch 904: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7912 - MinusLogProbMetric: 28.7912 - val_loss: 29.0294 - val_MinusLogProbMetric: 29.0294 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 905/1000
2023-10-26 00:42:11.060 
Epoch 905/1000 
	 loss: 28.7374, MinusLogProbMetric: 28.7374, val_loss: 29.0008, val_MinusLogProbMetric: 29.0008

Epoch 905: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7374 - MinusLogProbMetric: 28.7374 - val_loss: 29.0008 - val_MinusLogProbMetric: 29.0008 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 906/1000
2023-10-26 00:42:45.734 
Epoch 906/1000 
	 loss: 28.7662, MinusLogProbMetric: 28.7662, val_loss: 29.1878, val_MinusLogProbMetric: 29.1878

Epoch 906: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7662 - MinusLogProbMetric: 28.7662 - val_loss: 29.1878 - val_MinusLogProbMetric: 29.1878 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 907/1000
2023-10-26 00:43:21.010 
Epoch 907/1000 
	 loss: 28.7763, MinusLogProbMetric: 28.7763, val_loss: 29.0184, val_MinusLogProbMetric: 29.0184

Epoch 907: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7763 - MinusLogProbMetric: 28.7763 - val_loss: 29.0184 - val_MinusLogProbMetric: 29.0184 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 908/1000
2023-10-26 00:43:56.001 
Epoch 908/1000 
	 loss: 28.7225, MinusLogProbMetric: 28.7225, val_loss: 29.0437, val_MinusLogProbMetric: 29.0437

Epoch 908: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7225 - MinusLogProbMetric: 28.7225 - val_loss: 29.0437 - val_MinusLogProbMetric: 29.0437 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 909/1000
2023-10-26 00:44:30.911 
Epoch 909/1000 
	 loss: 28.7225, MinusLogProbMetric: 28.7225, val_loss: 28.9912, val_MinusLogProbMetric: 28.9912

Epoch 909: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7225 - MinusLogProbMetric: 28.7225 - val_loss: 28.9912 - val_MinusLogProbMetric: 28.9912 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 910/1000
2023-10-26 00:45:05.734 
Epoch 910/1000 
	 loss: 28.7599, MinusLogProbMetric: 28.7599, val_loss: 29.0346, val_MinusLogProbMetric: 29.0346

Epoch 910: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7599 - MinusLogProbMetric: 28.7599 - val_loss: 29.0346 - val_MinusLogProbMetric: 29.0346 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 911/1000
2023-10-26 00:45:40.069 
Epoch 911/1000 
	 loss: 28.7598, MinusLogProbMetric: 28.7598, val_loss: 29.0199, val_MinusLogProbMetric: 29.0199

Epoch 911: val_loss did not improve from 28.96580
196/196 - 34s - loss: 28.7598 - MinusLogProbMetric: 28.7598 - val_loss: 29.0199 - val_MinusLogProbMetric: 29.0199 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 912/1000
2023-10-26 00:46:14.911 
Epoch 912/1000 
	 loss: 28.7114, MinusLogProbMetric: 28.7114, val_loss: 29.0385, val_MinusLogProbMetric: 29.0385

Epoch 912: val_loss did not improve from 28.96580
196/196 - 35s - loss: 28.7114 - MinusLogProbMetric: 28.7114 - val_loss: 29.0385 - val_MinusLogProbMetric: 29.0385 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 913/1000
2023-10-26 00:46:49.762 
Epoch 913/1000 
	 loss: 28.7488, MinusLogProbMetric: 28.7488, val_loss: 28.9590, val_MinusLogProbMetric: 28.9590

Epoch 913: val_loss improved from 28.96580 to 28.95900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 28.7488 - MinusLogProbMetric: 28.7488 - val_loss: 28.9590 - val_MinusLogProbMetric: 28.9590 - lr: 8.3333e-05 - 35s/epoch - 181ms/step
Epoch 914/1000
2023-10-26 00:47:25.201 
Epoch 914/1000 
	 loss: 28.7360, MinusLogProbMetric: 28.7360, val_loss: 29.0348, val_MinusLogProbMetric: 29.0348

Epoch 914: val_loss did not improve from 28.95900
196/196 - 35s - loss: 28.7360 - MinusLogProbMetric: 28.7360 - val_loss: 29.0348 - val_MinusLogProbMetric: 29.0348 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 915/1000
2023-10-26 00:48:00.162 
Epoch 915/1000 
	 loss: 28.6805, MinusLogProbMetric: 28.6805, val_loss: 29.1207, val_MinusLogProbMetric: 29.1207

Epoch 915: val_loss did not improve from 28.95900
196/196 - 35s - loss: 28.6805 - MinusLogProbMetric: 28.6805 - val_loss: 29.1207 - val_MinusLogProbMetric: 29.1207 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 916/1000
2023-10-26 00:48:35.189 
Epoch 916/1000 
	 loss: 28.6675, MinusLogProbMetric: 28.6675, val_loss: 29.0665, val_MinusLogProbMetric: 29.0665

Epoch 916: val_loss did not improve from 28.95900
196/196 - 35s - loss: 28.6675 - MinusLogProbMetric: 28.6675 - val_loss: 29.0665 - val_MinusLogProbMetric: 29.0665 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 917/1000
2023-10-26 00:49:10.333 
Epoch 917/1000 
	 loss: 28.7081, MinusLogProbMetric: 28.7081, val_loss: 29.1294, val_MinusLogProbMetric: 29.1294

Epoch 917: val_loss did not improve from 28.95900
196/196 - 35s - loss: 28.7081 - MinusLogProbMetric: 28.7081 - val_loss: 29.1294 - val_MinusLogProbMetric: 29.1294 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 918/1000
2023-10-26 00:49:45.468 
Epoch 918/1000 
	 loss: 28.6667, MinusLogProbMetric: 28.6667, val_loss: 29.0137, val_MinusLogProbMetric: 29.0137

Epoch 918: val_loss did not improve from 28.95900
196/196 - 35s - loss: 28.6667 - MinusLogProbMetric: 28.6667 - val_loss: 29.0137 - val_MinusLogProbMetric: 29.0137 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 919/1000
2023-10-26 00:50:20.362 
Epoch 919/1000 
	 loss: 28.6542, MinusLogProbMetric: 28.6542, val_loss: 28.9805, val_MinusLogProbMetric: 28.9805

Epoch 919: val_loss did not improve from 28.95900
196/196 - 35s - loss: 28.6542 - MinusLogProbMetric: 28.6542 - val_loss: 28.9805 - val_MinusLogProbMetric: 28.9805 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 920/1000
2023-10-26 00:50:55.336 
Epoch 920/1000 
	 loss: 28.6865, MinusLogProbMetric: 28.6865, val_loss: 28.9582, val_MinusLogProbMetric: 28.9582

Epoch 920: val_loss improved from 28.95900 to 28.95818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 28.6865 - MinusLogProbMetric: 28.6865 - val_loss: 28.9582 - val_MinusLogProbMetric: 28.9582 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 921/1000
2023-10-26 00:51:30.744 
Epoch 921/1000 
	 loss: 28.7537, MinusLogProbMetric: 28.7537, val_loss: 29.0337, val_MinusLogProbMetric: 29.0337

Epoch 921: val_loss did not improve from 28.95818
196/196 - 35s - loss: 28.7537 - MinusLogProbMetric: 28.7537 - val_loss: 29.0337 - val_MinusLogProbMetric: 29.0337 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 922/1000
2023-10-26 00:52:03.877 
Epoch 922/1000 
	 loss: 28.6699, MinusLogProbMetric: 28.6699, val_loss: 28.9426, val_MinusLogProbMetric: 28.9426

Epoch 922: val_loss improved from 28.95818 to 28.94258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 34s - loss: 28.6699 - MinusLogProbMetric: 28.6699 - val_loss: 28.9426 - val_MinusLogProbMetric: 28.9426 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 923/1000
2023-10-26 00:52:34.370 
Epoch 923/1000 
	 loss: 28.6759, MinusLogProbMetric: 28.6759, val_loss: 28.9624, val_MinusLogProbMetric: 28.9624

Epoch 923: val_loss did not improve from 28.94258
196/196 - 30s - loss: 28.6759 - MinusLogProbMetric: 28.6759 - val_loss: 28.9624 - val_MinusLogProbMetric: 28.9624 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 924/1000
2023-10-26 00:53:09.225 
Epoch 924/1000 
	 loss: 28.6635, MinusLogProbMetric: 28.6635, val_loss: 29.0211, val_MinusLogProbMetric: 29.0211

Epoch 924: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6635 - MinusLogProbMetric: 28.6635 - val_loss: 29.0211 - val_MinusLogProbMetric: 29.0211 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 925/1000
2023-10-26 00:53:44.007 
Epoch 925/1000 
	 loss: 28.6631, MinusLogProbMetric: 28.6631, val_loss: 28.9675, val_MinusLogProbMetric: 28.9675

Epoch 925: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6631 - MinusLogProbMetric: 28.6631 - val_loss: 28.9675 - val_MinusLogProbMetric: 28.9675 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 926/1000
2023-10-26 00:54:18.768 
Epoch 926/1000 
	 loss: 28.6876, MinusLogProbMetric: 28.6876, val_loss: 29.0208, val_MinusLogProbMetric: 29.0208

Epoch 926: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6876 - MinusLogProbMetric: 28.6876 - val_loss: 29.0208 - val_MinusLogProbMetric: 29.0208 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 927/1000
2023-10-26 00:54:53.298 
Epoch 927/1000 
	 loss: 28.7026, MinusLogProbMetric: 28.7026, val_loss: 29.1320, val_MinusLogProbMetric: 29.1320

Epoch 927: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7026 - MinusLogProbMetric: 28.7026 - val_loss: 29.1320 - val_MinusLogProbMetric: 29.1320 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 928/1000
2023-10-26 00:55:28.224 
Epoch 928/1000 
	 loss: 28.7186, MinusLogProbMetric: 28.7186, val_loss: 29.0605, val_MinusLogProbMetric: 29.0605

Epoch 928: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7186 - MinusLogProbMetric: 28.7186 - val_loss: 29.0605 - val_MinusLogProbMetric: 29.0605 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 929/1000
2023-10-26 00:56:03.090 
Epoch 929/1000 
	 loss: 28.6775, MinusLogProbMetric: 28.6775, val_loss: 29.0403, val_MinusLogProbMetric: 29.0403

Epoch 929: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6775 - MinusLogProbMetric: 28.6775 - val_loss: 29.0403 - val_MinusLogProbMetric: 29.0403 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 930/1000
2023-10-26 00:56:38.031 
Epoch 930/1000 
	 loss: 28.6980, MinusLogProbMetric: 28.6980, val_loss: 29.0635, val_MinusLogProbMetric: 29.0635

Epoch 930: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6980 - MinusLogProbMetric: 28.6980 - val_loss: 29.0635 - val_MinusLogProbMetric: 29.0635 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 931/1000
2023-10-26 00:57:12.987 
Epoch 931/1000 
	 loss: 28.6946, MinusLogProbMetric: 28.6946, val_loss: 28.9914, val_MinusLogProbMetric: 28.9914

Epoch 931: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6946 - MinusLogProbMetric: 28.6946 - val_loss: 28.9914 - val_MinusLogProbMetric: 28.9914 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 932/1000
2023-10-26 00:57:47.796 
Epoch 932/1000 
	 loss: 28.6931, MinusLogProbMetric: 28.6931, val_loss: 29.2847, val_MinusLogProbMetric: 29.2847

Epoch 932: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6931 - MinusLogProbMetric: 28.6931 - val_loss: 29.2847 - val_MinusLogProbMetric: 29.2847 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 933/1000
2023-10-26 00:58:22.388 
Epoch 933/1000 
	 loss: 28.7275, MinusLogProbMetric: 28.7275, val_loss: 29.2053, val_MinusLogProbMetric: 29.2053

Epoch 933: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7275 - MinusLogProbMetric: 28.7275 - val_loss: 29.2053 - val_MinusLogProbMetric: 29.2053 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 934/1000
2023-10-26 00:58:57.189 
Epoch 934/1000 
	 loss: 28.7053, MinusLogProbMetric: 28.7053, val_loss: 28.9706, val_MinusLogProbMetric: 28.9706

Epoch 934: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7053 - MinusLogProbMetric: 28.7053 - val_loss: 28.9706 - val_MinusLogProbMetric: 28.9706 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 935/1000
2023-10-26 00:59:31.994 
Epoch 935/1000 
	 loss: 28.6598, MinusLogProbMetric: 28.6598, val_loss: 29.0936, val_MinusLogProbMetric: 29.0936

Epoch 935: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6598 - MinusLogProbMetric: 28.6598 - val_loss: 29.0936 - val_MinusLogProbMetric: 29.0936 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 936/1000
2023-10-26 01:00:06.940 
Epoch 936/1000 
	 loss: 28.6795, MinusLogProbMetric: 28.6795, val_loss: 28.9851, val_MinusLogProbMetric: 28.9851

Epoch 936: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6795 - MinusLogProbMetric: 28.6795 - val_loss: 28.9851 - val_MinusLogProbMetric: 28.9851 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 937/1000
2023-10-26 01:00:41.983 
Epoch 937/1000 
	 loss: 28.6472, MinusLogProbMetric: 28.6472, val_loss: 29.0087, val_MinusLogProbMetric: 29.0087

Epoch 937: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6472 - MinusLogProbMetric: 28.6472 - val_loss: 29.0087 - val_MinusLogProbMetric: 29.0087 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 938/1000
2023-10-26 01:01:16.335 
Epoch 938/1000 
	 loss: 28.6726, MinusLogProbMetric: 28.6726, val_loss: 29.0861, val_MinusLogProbMetric: 29.0861

Epoch 938: val_loss did not improve from 28.94258
196/196 - 34s - loss: 28.6726 - MinusLogProbMetric: 28.6726 - val_loss: 29.0861 - val_MinusLogProbMetric: 29.0861 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 939/1000
2023-10-26 01:01:51.152 
Epoch 939/1000 
	 loss: 28.6749, MinusLogProbMetric: 28.6749, val_loss: 28.9845, val_MinusLogProbMetric: 28.9845

Epoch 939: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6749 - MinusLogProbMetric: 28.6749 - val_loss: 28.9845 - val_MinusLogProbMetric: 28.9845 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 940/1000
2023-10-26 01:02:26.046 
Epoch 940/1000 
	 loss: 28.6433, MinusLogProbMetric: 28.6433, val_loss: 28.9891, val_MinusLogProbMetric: 28.9891

Epoch 940: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6433 - MinusLogProbMetric: 28.6433 - val_loss: 28.9891 - val_MinusLogProbMetric: 28.9891 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 941/1000
2023-10-26 01:03:00.888 
Epoch 941/1000 
	 loss: 28.6783, MinusLogProbMetric: 28.6783, val_loss: 29.0132, val_MinusLogProbMetric: 29.0132

Epoch 941: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6783 - MinusLogProbMetric: 28.6783 - val_loss: 29.0132 - val_MinusLogProbMetric: 29.0132 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 942/1000
2023-10-26 01:03:35.791 
Epoch 942/1000 
	 loss: 28.6847, MinusLogProbMetric: 28.6847, val_loss: 29.1853, val_MinusLogProbMetric: 29.1853

Epoch 942: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6847 - MinusLogProbMetric: 28.6847 - val_loss: 29.1853 - val_MinusLogProbMetric: 29.1853 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 943/1000
2023-10-26 01:04:10.475 
Epoch 943/1000 
	 loss: 28.6638, MinusLogProbMetric: 28.6638, val_loss: 29.0179, val_MinusLogProbMetric: 29.0179

Epoch 943: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6638 - MinusLogProbMetric: 28.6638 - val_loss: 29.0179 - val_MinusLogProbMetric: 29.0179 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 944/1000
2023-10-26 01:04:45.225 
Epoch 944/1000 
	 loss: 28.6471, MinusLogProbMetric: 28.6471, val_loss: 29.0020, val_MinusLogProbMetric: 29.0020

Epoch 944: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6471 - MinusLogProbMetric: 28.6471 - val_loss: 29.0020 - val_MinusLogProbMetric: 29.0020 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 945/1000
2023-10-26 01:05:20.310 
Epoch 945/1000 
	 loss: 28.7054, MinusLogProbMetric: 28.7054, val_loss: 28.9445, val_MinusLogProbMetric: 28.9445

Epoch 945: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7054 - MinusLogProbMetric: 28.7054 - val_loss: 28.9445 - val_MinusLogProbMetric: 28.9445 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 946/1000
2023-10-26 01:05:55.245 
Epoch 946/1000 
	 loss: 28.6728, MinusLogProbMetric: 28.6728, val_loss: 29.0699, val_MinusLogProbMetric: 29.0699

Epoch 946: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6728 - MinusLogProbMetric: 28.6728 - val_loss: 29.0699 - val_MinusLogProbMetric: 29.0699 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 947/1000
2023-10-26 01:06:29.977 
Epoch 947/1000 
	 loss: 28.8125, MinusLogProbMetric: 28.8125, val_loss: 29.2756, val_MinusLogProbMetric: 29.2756

Epoch 947: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.8125 - MinusLogProbMetric: 28.8125 - val_loss: 29.2756 - val_MinusLogProbMetric: 29.2756 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 948/1000
2023-10-26 01:07:05.092 
Epoch 948/1000 
	 loss: 28.6792, MinusLogProbMetric: 28.6792, val_loss: 29.0104, val_MinusLogProbMetric: 29.0104

Epoch 948: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6792 - MinusLogProbMetric: 28.6792 - val_loss: 29.0104 - val_MinusLogProbMetric: 29.0104 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 949/1000
2023-10-26 01:07:40.070 
Epoch 949/1000 
	 loss: 28.7038, MinusLogProbMetric: 28.7038, val_loss: 29.1186, val_MinusLogProbMetric: 29.1186

Epoch 949: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7038 - MinusLogProbMetric: 28.7038 - val_loss: 29.1186 - val_MinusLogProbMetric: 29.1186 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 950/1000
2023-10-26 01:08:15.310 
Epoch 950/1000 
	 loss: 28.7008, MinusLogProbMetric: 28.7008, val_loss: 29.1395, val_MinusLogProbMetric: 29.1395

Epoch 950: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7008 - MinusLogProbMetric: 28.7008 - val_loss: 29.1395 - val_MinusLogProbMetric: 29.1395 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 951/1000
2023-10-26 01:08:50.012 
Epoch 951/1000 
	 loss: 28.6819, MinusLogProbMetric: 28.6819, val_loss: 28.9512, val_MinusLogProbMetric: 28.9512

Epoch 951: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6819 - MinusLogProbMetric: 28.6819 - val_loss: 28.9512 - val_MinusLogProbMetric: 28.9512 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 952/1000
2023-10-26 01:09:24.742 
Epoch 952/1000 
	 loss: 28.6907, MinusLogProbMetric: 28.6907, val_loss: 28.9472, val_MinusLogProbMetric: 28.9472

Epoch 952: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6907 - MinusLogProbMetric: 28.6907 - val_loss: 28.9472 - val_MinusLogProbMetric: 28.9472 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 953/1000
2023-10-26 01:09:59.841 
Epoch 953/1000 
	 loss: 28.6224, MinusLogProbMetric: 28.6224, val_loss: 29.0641, val_MinusLogProbMetric: 29.0641

Epoch 953: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6224 - MinusLogProbMetric: 28.6224 - val_loss: 29.0641 - val_MinusLogProbMetric: 29.0641 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 954/1000
2023-10-26 01:10:34.703 
Epoch 954/1000 
	 loss: 28.6944, MinusLogProbMetric: 28.6944, val_loss: 28.9745, val_MinusLogProbMetric: 28.9745

Epoch 954: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6944 - MinusLogProbMetric: 28.6944 - val_loss: 28.9745 - val_MinusLogProbMetric: 28.9745 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 955/1000
2023-10-26 01:11:09.948 
Epoch 955/1000 
	 loss: 28.6948, MinusLogProbMetric: 28.6948, val_loss: 28.9654, val_MinusLogProbMetric: 28.9654

Epoch 955: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6948 - MinusLogProbMetric: 28.6948 - val_loss: 28.9654 - val_MinusLogProbMetric: 28.9654 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 956/1000
2023-10-26 01:11:44.641 
Epoch 956/1000 
	 loss: 28.7145, MinusLogProbMetric: 28.7145, val_loss: 29.2894, val_MinusLogProbMetric: 29.2894

Epoch 956: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7145 - MinusLogProbMetric: 28.7145 - val_loss: 29.2894 - val_MinusLogProbMetric: 29.2894 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 957/1000
2023-10-26 01:12:19.568 
Epoch 957/1000 
	 loss: 28.7449, MinusLogProbMetric: 28.7449, val_loss: 29.0802, val_MinusLogProbMetric: 29.0802

Epoch 957: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7449 - MinusLogProbMetric: 28.7449 - val_loss: 29.0802 - val_MinusLogProbMetric: 29.0802 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 958/1000
2023-10-26 01:12:54.560 
Epoch 958/1000 
	 loss: 28.6693, MinusLogProbMetric: 28.6693, val_loss: 28.9921, val_MinusLogProbMetric: 28.9921

Epoch 958: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.6693 - MinusLogProbMetric: 28.6693 - val_loss: 28.9921 - val_MinusLogProbMetric: 28.9921 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 959/1000
2023-10-26 01:13:29.324 
Epoch 959/1000 
	 loss: 28.7098, MinusLogProbMetric: 28.7098, val_loss: 29.1320, val_MinusLogProbMetric: 29.1320

Epoch 959: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7098 - MinusLogProbMetric: 28.7098 - val_loss: 29.1320 - val_MinusLogProbMetric: 29.1320 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 960/1000
2023-10-26 01:14:04.284 
Epoch 960/1000 
	 loss: 28.7584, MinusLogProbMetric: 28.7584, val_loss: 28.9810, val_MinusLogProbMetric: 28.9810

Epoch 960: val_loss did not improve from 28.94258
196/196 - 35s - loss: 28.7584 - MinusLogProbMetric: 28.7584 - val_loss: 28.9810 - val_MinusLogProbMetric: 28.9810 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 961/1000
2023-10-26 01:14:39.249 
Epoch 961/1000 
	 loss: 28.6620, MinusLogProbMetric: 28.6620, val_loss: 28.9366, val_MinusLogProbMetric: 28.9366

Epoch 961: val_loss improved from 28.94258 to 28.93657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 28.6620 - MinusLogProbMetric: 28.6620 - val_loss: 28.9366 - val_MinusLogProbMetric: 28.9366 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 962/1000
2023-10-26 01:15:15.084 
Epoch 962/1000 
	 loss: 28.6907, MinusLogProbMetric: 28.6907, val_loss: 29.1798, val_MinusLogProbMetric: 29.1798

Epoch 962: val_loss did not improve from 28.93657
196/196 - 35s - loss: 28.6907 - MinusLogProbMetric: 28.6907 - val_loss: 29.1798 - val_MinusLogProbMetric: 29.1798 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 963/1000
2023-10-26 01:15:49.938 
Epoch 963/1000 
	 loss: 28.6894, MinusLogProbMetric: 28.6894, val_loss: 28.9471, val_MinusLogProbMetric: 28.9471

Epoch 963: val_loss did not improve from 28.93657
196/196 - 35s - loss: 28.6894 - MinusLogProbMetric: 28.6894 - val_loss: 28.9471 - val_MinusLogProbMetric: 28.9471 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 964/1000
2023-10-26 01:16:24.839 
Epoch 964/1000 
	 loss: 28.6695, MinusLogProbMetric: 28.6695, val_loss: 28.9259, val_MinusLogProbMetric: 28.9259

Epoch 964: val_loss improved from 28.93657 to 28.92590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 28.6695 - MinusLogProbMetric: 28.6695 - val_loss: 28.9259 - val_MinusLogProbMetric: 28.9259 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 965/1000
2023-10-26 01:17:00.567 
Epoch 965/1000 
	 loss: 28.6923, MinusLogProbMetric: 28.6923, val_loss: 29.0055, val_MinusLogProbMetric: 29.0055

Epoch 965: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.6923 - MinusLogProbMetric: 28.6923 - val_loss: 29.0055 - val_MinusLogProbMetric: 29.0055 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 966/1000
2023-10-26 01:17:35.443 
Epoch 966/1000 
	 loss: 28.6666, MinusLogProbMetric: 28.6666, val_loss: 28.9855, val_MinusLogProbMetric: 28.9855

Epoch 966: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.6666 - MinusLogProbMetric: 28.6666 - val_loss: 28.9855 - val_MinusLogProbMetric: 28.9855 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 967/1000
2023-10-26 01:18:10.441 
Epoch 967/1000 
	 loss: 28.6541, MinusLogProbMetric: 28.6541, val_loss: 29.0064, val_MinusLogProbMetric: 29.0064

Epoch 967: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.6541 - MinusLogProbMetric: 28.6541 - val_loss: 29.0064 - val_MinusLogProbMetric: 29.0064 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 968/1000
2023-10-26 01:18:45.271 
Epoch 968/1000 
	 loss: 28.6423, MinusLogProbMetric: 28.6423, val_loss: 28.9564, val_MinusLogProbMetric: 28.9564

Epoch 968: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.6423 - MinusLogProbMetric: 28.6423 - val_loss: 28.9564 - val_MinusLogProbMetric: 28.9564 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 969/1000
2023-10-26 01:19:20.355 
Epoch 969/1000 
	 loss: 28.6710, MinusLogProbMetric: 28.6710, val_loss: 29.2419, val_MinusLogProbMetric: 29.2419

Epoch 969: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.6710 - MinusLogProbMetric: 28.6710 - val_loss: 29.2419 - val_MinusLogProbMetric: 29.2419 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 970/1000
2023-10-26 01:19:55.435 
Epoch 970/1000 
	 loss: 28.7010, MinusLogProbMetric: 28.7010, val_loss: 29.0825, val_MinusLogProbMetric: 29.0825

Epoch 970: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.7010 - MinusLogProbMetric: 28.7010 - val_loss: 29.0825 - val_MinusLogProbMetric: 29.0825 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 971/1000
2023-10-26 01:20:30.368 
Epoch 971/1000 
	 loss: 28.7241, MinusLogProbMetric: 28.7241, val_loss: 29.0580, val_MinusLogProbMetric: 29.0580

Epoch 971: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.7241 - MinusLogProbMetric: 28.7241 - val_loss: 29.0580 - val_MinusLogProbMetric: 29.0580 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 972/1000
2023-10-26 01:21:05.220 
Epoch 972/1000 
	 loss: 28.6642, MinusLogProbMetric: 28.6642, val_loss: 29.1814, val_MinusLogProbMetric: 29.1814

Epoch 972: val_loss did not improve from 28.92590
196/196 - 35s - loss: 28.6642 - MinusLogProbMetric: 28.6642 - val_loss: 29.1814 - val_MinusLogProbMetric: 29.1814 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 973/1000
2023-10-26 01:21:39.932 
Epoch 973/1000 
	 loss: 28.6579, MinusLogProbMetric: 28.6579, val_loss: 28.9256, val_MinusLogProbMetric: 28.9256

Epoch 973: val_loss improved from 28.92590 to 28.92564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 35s - loss: 28.6579 - MinusLogProbMetric: 28.6579 - val_loss: 28.9256 - val_MinusLogProbMetric: 28.9256 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 974/1000
2023-10-26 01:22:15.441 
Epoch 974/1000 
	 loss: 28.6791, MinusLogProbMetric: 28.6791, val_loss: 28.8565, val_MinusLogProbMetric: 28.8565

Epoch 974: val_loss improved from 28.92564 to 28.85651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_361/weights/best_weights.h5
196/196 - 36s - loss: 28.6791 - MinusLogProbMetric: 28.6791 - val_loss: 28.8565 - val_MinusLogProbMetric: 28.8565 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 975/1000
2023-10-26 01:22:51.005 
Epoch 975/1000 
	 loss: 28.6812, MinusLogProbMetric: 28.6812, val_loss: 28.9452, val_MinusLogProbMetric: 28.9452

Epoch 975: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6812 - MinusLogProbMetric: 28.6812 - val_loss: 28.9452 - val_MinusLogProbMetric: 28.9452 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 976/1000
2023-10-26 01:23:26.016 
Epoch 976/1000 
	 loss: 28.6758, MinusLogProbMetric: 28.6758, val_loss: 29.1690, val_MinusLogProbMetric: 29.1690

Epoch 976: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6758 - MinusLogProbMetric: 28.6758 - val_loss: 29.1690 - val_MinusLogProbMetric: 29.1690 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 977/1000
2023-10-26 01:24:00.926 
Epoch 977/1000 
	 loss: 28.7060, MinusLogProbMetric: 28.7060, val_loss: 29.0791, val_MinusLogProbMetric: 29.0791

Epoch 977: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.7060 - MinusLogProbMetric: 28.7060 - val_loss: 29.0791 - val_MinusLogProbMetric: 29.0791 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 978/1000
2023-10-26 01:24:35.857 
Epoch 978/1000 
	 loss: 28.6292, MinusLogProbMetric: 28.6292, val_loss: 29.0490, val_MinusLogProbMetric: 29.0490

Epoch 978: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6292 - MinusLogProbMetric: 28.6292 - val_loss: 29.0490 - val_MinusLogProbMetric: 29.0490 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 979/1000
2023-10-26 01:25:10.422 
Epoch 979/1000 
	 loss: 28.6413, MinusLogProbMetric: 28.6413, val_loss: 28.9608, val_MinusLogProbMetric: 28.9608

Epoch 979: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6413 - MinusLogProbMetric: 28.6413 - val_loss: 28.9608 - val_MinusLogProbMetric: 28.9608 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 980/1000
2023-10-26 01:25:45.569 
Epoch 980/1000 
	 loss: 28.6409, MinusLogProbMetric: 28.6409, val_loss: 29.0475, val_MinusLogProbMetric: 29.0475

Epoch 980: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6409 - MinusLogProbMetric: 28.6409 - val_loss: 29.0475 - val_MinusLogProbMetric: 29.0475 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 981/1000
2023-10-26 01:26:20.641 
Epoch 981/1000 
	 loss: 28.6855, MinusLogProbMetric: 28.6855, val_loss: 29.0315, val_MinusLogProbMetric: 29.0315

Epoch 981: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6855 - MinusLogProbMetric: 28.6855 - val_loss: 29.0315 - val_MinusLogProbMetric: 29.0315 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 982/1000
2023-10-26 01:26:55.424 
Epoch 982/1000 
	 loss: 28.7160, MinusLogProbMetric: 28.7160, val_loss: 29.3451, val_MinusLogProbMetric: 29.3451

Epoch 982: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.7160 - MinusLogProbMetric: 28.7160 - val_loss: 29.3451 - val_MinusLogProbMetric: 29.3451 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 983/1000
2023-10-26 01:27:30.273 
Epoch 983/1000 
	 loss: 28.6681, MinusLogProbMetric: 28.6681, val_loss: 29.0232, val_MinusLogProbMetric: 29.0232

Epoch 983: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6681 - MinusLogProbMetric: 28.6681 - val_loss: 29.0232 - val_MinusLogProbMetric: 29.0232 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 984/1000
2023-10-26 01:28:05.170 
Epoch 984/1000 
	 loss: 28.6456, MinusLogProbMetric: 28.6456, val_loss: 29.0149, val_MinusLogProbMetric: 29.0149

Epoch 984: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6456 - MinusLogProbMetric: 28.6456 - val_loss: 29.0149 - val_MinusLogProbMetric: 29.0149 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 985/1000
2023-10-26 01:28:39.777 
Epoch 985/1000 
	 loss: 28.6669, MinusLogProbMetric: 28.6669, val_loss: 29.0769, val_MinusLogProbMetric: 29.0769

Epoch 985: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6669 - MinusLogProbMetric: 28.6669 - val_loss: 29.0769 - val_MinusLogProbMetric: 29.0769 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 986/1000
2023-10-26 01:29:14.765 
Epoch 986/1000 
	 loss: 28.6743, MinusLogProbMetric: 28.6743, val_loss: 28.9959, val_MinusLogProbMetric: 28.9959

Epoch 986: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6743 - MinusLogProbMetric: 28.6743 - val_loss: 28.9959 - val_MinusLogProbMetric: 28.9959 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 987/1000
2023-10-26 01:29:49.533 
Epoch 987/1000 
	 loss: 28.6709, MinusLogProbMetric: 28.6709, val_loss: 29.0080, val_MinusLogProbMetric: 29.0080

Epoch 987: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6709 - MinusLogProbMetric: 28.6709 - val_loss: 29.0080 - val_MinusLogProbMetric: 29.0080 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 988/1000
2023-10-26 01:30:24.446 
Epoch 988/1000 
	 loss: 28.6554, MinusLogProbMetric: 28.6554, val_loss: 29.5490, val_MinusLogProbMetric: 29.5490

Epoch 988: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6554 - MinusLogProbMetric: 28.6554 - val_loss: 29.5490 - val_MinusLogProbMetric: 29.5490 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 989/1000
2023-10-26 01:30:59.447 
Epoch 989/1000 
	 loss: 28.6748, MinusLogProbMetric: 28.6748, val_loss: 28.9113, val_MinusLogProbMetric: 28.9113

Epoch 989: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6748 - MinusLogProbMetric: 28.6748 - val_loss: 28.9113 - val_MinusLogProbMetric: 28.9113 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 990/1000
2023-10-26 01:31:34.395 
Epoch 990/1000 
	 loss: 28.6309, MinusLogProbMetric: 28.6309, val_loss: 29.0592, val_MinusLogProbMetric: 29.0592

Epoch 990: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6309 - MinusLogProbMetric: 28.6309 - val_loss: 29.0592 - val_MinusLogProbMetric: 29.0592 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 991/1000
2023-10-26 01:32:09.258 
Epoch 991/1000 
	 loss: 28.6333, MinusLogProbMetric: 28.6333, val_loss: 29.0549, val_MinusLogProbMetric: 29.0549

Epoch 991: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6333 - MinusLogProbMetric: 28.6333 - val_loss: 29.0549 - val_MinusLogProbMetric: 29.0549 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 992/1000
2023-10-26 01:32:44.060 
Epoch 992/1000 
	 loss: 28.6996, MinusLogProbMetric: 28.6996, val_loss: 29.0895, val_MinusLogProbMetric: 29.0895

Epoch 992: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6996 - MinusLogProbMetric: 28.6996 - val_loss: 29.0895 - val_MinusLogProbMetric: 29.0895 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 993/1000
2023-10-26 01:33:19.132 
Epoch 993/1000 
	 loss: 28.6577, MinusLogProbMetric: 28.6577, val_loss: 28.9053, val_MinusLogProbMetric: 28.9053

Epoch 993: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6577 - MinusLogProbMetric: 28.6577 - val_loss: 28.9053 - val_MinusLogProbMetric: 28.9053 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 994/1000
2023-10-26 01:33:54.036 
Epoch 994/1000 
	 loss: 28.6286, MinusLogProbMetric: 28.6286, val_loss: 29.4824, val_MinusLogProbMetric: 29.4824

Epoch 994: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6286 - MinusLogProbMetric: 28.6286 - val_loss: 29.4824 - val_MinusLogProbMetric: 29.4824 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 995/1000
2023-10-26 01:34:28.839 
Epoch 995/1000 
	 loss: 28.6830, MinusLogProbMetric: 28.6830, val_loss: 28.9238, val_MinusLogProbMetric: 28.9238

Epoch 995: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6830 - MinusLogProbMetric: 28.6830 - val_loss: 28.9238 - val_MinusLogProbMetric: 28.9238 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 996/1000
2023-10-26 01:35:03.527 
Epoch 996/1000 
	 loss: 28.6628, MinusLogProbMetric: 28.6628, val_loss: 28.8934, val_MinusLogProbMetric: 28.8934

Epoch 996: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6628 - MinusLogProbMetric: 28.6628 - val_loss: 28.8934 - val_MinusLogProbMetric: 28.8934 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 997/1000
2023-10-26 01:35:38.544 
Epoch 997/1000 
	 loss: 28.7319, MinusLogProbMetric: 28.7319, val_loss: 29.1512, val_MinusLogProbMetric: 29.1512

Epoch 997: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.7319 - MinusLogProbMetric: 28.7319 - val_loss: 29.1512 - val_MinusLogProbMetric: 29.1512 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 998/1000
2023-10-26 01:36:13.380 
Epoch 998/1000 
	 loss: 28.7280, MinusLogProbMetric: 28.7280, val_loss: 28.9824, val_MinusLogProbMetric: 28.9824

Epoch 998: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.7280 - MinusLogProbMetric: 28.7280 - val_loss: 28.9824 - val_MinusLogProbMetric: 28.9824 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 999/1000
2023-10-26 01:36:48.529 
Epoch 999/1000 
	 loss: 28.6885, MinusLogProbMetric: 28.6885, val_loss: 28.9153, val_MinusLogProbMetric: 28.9153

Epoch 999: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6885 - MinusLogProbMetric: 28.6885 - val_loss: 28.9153 - val_MinusLogProbMetric: 28.9153 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 1000/1000
2023-10-26 01:37:23.748 
Epoch 1000/1000 
	 loss: 28.6426, MinusLogProbMetric: 28.6426, val_loss: 28.9526, val_MinusLogProbMetric: 28.9526

Epoch 1000: val_loss did not improve from 28.85651
196/196 - 35s - loss: 28.6426 - MinusLogProbMetric: 28.6426 - val_loss: 28.9526 - val_MinusLogProbMetric: 28.9526 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 541.
Model trained in 34021.49 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.90 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.39 s.
===========
Run 361/720 done in 34095.09 s.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

===========
Generating train data for run 372.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_372/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_372/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_372/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_372
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  2139360   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7ff6306f3370>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffbc22c1f60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffbc22c1f60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ffbc22c3970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ffbc22f02b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ffbc22f0820>, <keras.callbacks.ModelCheckpoint object at 0x7ffbc22f08e0>, <keras.callbacks.EarlyStopping object at 0x7ffbc22f0b50>, <keras.callbacks.ReduceLROnPlateau object at 0x7ffbc22f0b80>, <keras.callbacks.TerminateOnNaN object at 0x7ffbc22f07c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_372/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 372/720 with hyperparameters:
timestamp = 2023-10-26 01:37:30.844790
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 27: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 01:38:50.567 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3014.8987, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 3014.8987 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 80s/epoch - 407ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 372.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_372/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_372/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_372/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_372
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  2139360   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7ffbb10e6dd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffba8dca7d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffba8dca7d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ffbc22e9720>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ffba8c41ea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ffba8c42410>, <keras.callbacks.ModelCheckpoint object at 0x7ffba8c424d0>, <keras.callbacks.EarlyStopping object at 0x7ffba8c42740>, <keras.callbacks.ReduceLROnPlateau object at 0x7ffba8c42770>, <keras.callbacks.TerminateOnNaN object at 0x7ffba8c423b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_372/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 372/720 with hyperparameters:
timestamp = 2023-10-26 01:38:57.471173
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 01:40:57.037 
Epoch 1/1000 
	 loss: 730.8568, MinusLogProbMetric: 730.8568, val_loss: 246.6870, val_MinusLogProbMetric: 246.6870

Epoch 1: val_loss improved from inf to 246.68703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 120s - loss: 730.8568 - MinusLogProbMetric: 730.8568 - val_loss: 246.6870 - val_MinusLogProbMetric: 246.6870 - lr: 3.3333e-04 - 120s/epoch - 614ms/step
Epoch 2/1000
2023-10-26 01:41:41.216 
Epoch 2/1000 
	 loss: 234.6672, MinusLogProbMetric: 234.6672, val_loss: 173.0201, val_MinusLogProbMetric: 173.0201

Epoch 2: val_loss improved from 246.68703 to 173.02008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 234.6672 - MinusLogProbMetric: 234.6672 - val_loss: 173.0201 - val_MinusLogProbMetric: 173.0201 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 3/1000
2023-10-26 01:42:24.863 
Epoch 3/1000 
	 loss: 140.1892, MinusLogProbMetric: 140.1892, val_loss: 124.3883, val_MinusLogProbMetric: 124.3883

Epoch 3: val_loss improved from 173.02008 to 124.38826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 140.1892 - MinusLogProbMetric: 140.1892 - val_loss: 124.3883 - val_MinusLogProbMetric: 124.3883 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 4/1000
2023-10-26 01:43:08.201 
Epoch 4/1000 
	 loss: 149.0181, MinusLogProbMetric: 149.0181, val_loss: 150.3386, val_MinusLogProbMetric: 150.3386

Epoch 4: val_loss did not improve from 124.38826
196/196 - 43s - loss: 149.0181 - MinusLogProbMetric: 149.0181 - val_loss: 150.3386 - val_MinusLogProbMetric: 150.3386 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 5/1000
2023-10-26 01:43:50.802 
Epoch 5/1000 
	 loss: 125.2633, MinusLogProbMetric: 125.2633, val_loss: 103.6245, val_MinusLogProbMetric: 103.6245

Epoch 5: val_loss improved from 124.38826 to 103.62446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 125.2633 - MinusLogProbMetric: 125.2633 - val_loss: 103.6245 - val_MinusLogProbMetric: 103.6245 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 6/1000
2023-10-26 01:44:33.870 
Epoch 6/1000 
	 loss: 93.9855, MinusLogProbMetric: 93.9855, val_loss: 86.9683, val_MinusLogProbMetric: 86.9683

Epoch 6: val_loss improved from 103.62446 to 86.96832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 93.9855 - MinusLogProbMetric: 93.9855 - val_loss: 86.9683 - val_MinusLogProbMetric: 86.9683 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 7/1000
2023-10-26 01:45:17.526 
Epoch 7/1000 
	 loss: 82.2359, MinusLogProbMetric: 82.2359, val_loss: 77.8015, val_MinusLogProbMetric: 77.8015

Epoch 7: val_loss improved from 86.96832 to 77.80151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 82.2359 - MinusLogProbMetric: 82.2359 - val_loss: 77.8015 - val_MinusLogProbMetric: 77.8015 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 8/1000
2023-10-26 01:46:00.367 
Epoch 8/1000 
	 loss: 73.4275, MinusLogProbMetric: 73.4275, val_loss: 69.3224, val_MinusLogProbMetric: 69.3224

Epoch 8: val_loss improved from 77.80151 to 69.32236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 73.4275 - MinusLogProbMetric: 73.4275 - val_loss: 69.3224 - val_MinusLogProbMetric: 69.3224 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 9/1000
2023-10-26 01:46:43.349 
Epoch 9/1000 
	 loss: 65.9949, MinusLogProbMetric: 65.9949, val_loss: 64.7166, val_MinusLogProbMetric: 64.7166

Epoch 9: val_loss improved from 69.32236 to 64.71662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 65.9949 - MinusLogProbMetric: 65.9949 - val_loss: 64.7166 - val_MinusLogProbMetric: 64.7166 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 10/1000
2023-10-26 01:47:26.490 
Epoch 10/1000 
	 loss: 62.1941, MinusLogProbMetric: 62.1941, val_loss: 147.9578, val_MinusLogProbMetric: 147.9578

Epoch 10: val_loss did not improve from 64.71662
196/196 - 42s - loss: 62.1941 - MinusLogProbMetric: 62.1941 - val_loss: 147.9578 - val_MinusLogProbMetric: 147.9578 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 11/1000
2023-10-26 01:48:09.136 
Epoch 11/1000 
	 loss: 66.8937, MinusLogProbMetric: 66.8937, val_loss: 56.8143, val_MinusLogProbMetric: 56.8143

Epoch 11: val_loss improved from 64.71662 to 56.81429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 66.8937 - MinusLogProbMetric: 66.8937 - val_loss: 56.8143 - val_MinusLogProbMetric: 56.8143 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 12/1000
2023-10-26 01:48:51.891 
Epoch 12/1000 
	 loss: 55.0538, MinusLogProbMetric: 55.0538, val_loss: 53.5709, val_MinusLogProbMetric: 53.5709

Epoch 12: val_loss improved from 56.81429 to 53.57091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 55.0538 - MinusLogProbMetric: 55.0538 - val_loss: 53.5709 - val_MinusLogProbMetric: 53.5709 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 13/1000
2023-10-26 01:49:35.353 
Epoch 13/1000 
	 loss: 52.1211, MinusLogProbMetric: 52.1211, val_loss: 51.1197, val_MinusLogProbMetric: 51.1197

Epoch 13: val_loss improved from 53.57091 to 51.11966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 52.1211 - MinusLogProbMetric: 52.1211 - val_loss: 51.1197 - val_MinusLogProbMetric: 51.1197 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 14/1000
2023-10-26 01:50:18.684 
Epoch 14/1000 
	 loss: 49.7976, MinusLogProbMetric: 49.7976, val_loss: 48.7851, val_MinusLogProbMetric: 48.7851

Epoch 14: val_loss improved from 51.11966 to 48.78507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 49.7976 - MinusLogProbMetric: 49.7976 - val_loss: 48.7851 - val_MinusLogProbMetric: 48.7851 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 15/1000
2023-10-26 01:51:02.038 
Epoch 15/1000 
	 loss: 47.9380, MinusLogProbMetric: 47.9380, val_loss: 47.5756, val_MinusLogProbMetric: 47.5756

Epoch 15: val_loss improved from 48.78507 to 47.57564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 47.9380 - MinusLogProbMetric: 47.9380 - val_loss: 47.5756 - val_MinusLogProbMetric: 47.5756 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 16/1000
2023-10-26 01:51:45.909 
Epoch 16/1000 
	 loss: 46.4442, MinusLogProbMetric: 46.4442, val_loss: 47.4536, val_MinusLogProbMetric: 47.4536

Epoch 16: val_loss improved from 47.57564 to 47.45362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 46.4442 - MinusLogProbMetric: 46.4442 - val_loss: 47.4536 - val_MinusLogProbMetric: 47.4536 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 17/1000
2023-10-26 01:52:29.291 
Epoch 17/1000 
	 loss: 45.1789, MinusLogProbMetric: 45.1789, val_loss: 44.8546, val_MinusLogProbMetric: 44.8546

Epoch 17: val_loss improved from 47.45362 to 44.85458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 45.1789 - MinusLogProbMetric: 45.1789 - val_loss: 44.8546 - val_MinusLogProbMetric: 44.8546 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 18/1000
2023-10-26 01:53:12.818 
Epoch 18/1000 
	 loss: 43.9901, MinusLogProbMetric: 43.9901, val_loss: 43.3894, val_MinusLogProbMetric: 43.3894

Epoch 18: val_loss improved from 44.85458 to 43.38939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 43.9901 - MinusLogProbMetric: 43.9901 - val_loss: 43.3894 - val_MinusLogProbMetric: 43.3894 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 19/1000
2023-10-26 01:53:56.146 
Epoch 19/1000 
	 loss: 42.9612, MinusLogProbMetric: 42.9612, val_loss: 42.4407, val_MinusLogProbMetric: 42.4407

Epoch 19: val_loss improved from 43.38939 to 42.44072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 42.9612 - MinusLogProbMetric: 42.9612 - val_loss: 42.4407 - val_MinusLogProbMetric: 42.4407 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 20/1000
2023-10-26 01:54:39.879 
Epoch 20/1000 
	 loss: 42.0777, MinusLogProbMetric: 42.0777, val_loss: 42.5800, val_MinusLogProbMetric: 42.5800

Epoch 20: val_loss did not improve from 42.44072
196/196 - 43s - loss: 42.0777 - MinusLogProbMetric: 42.0777 - val_loss: 42.5800 - val_MinusLogProbMetric: 42.5800 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 21/1000
2023-10-26 01:55:22.474 
Epoch 21/1000 
	 loss: 41.2492, MinusLogProbMetric: 41.2492, val_loss: 41.3712, val_MinusLogProbMetric: 41.3712

Epoch 21: val_loss improved from 42.44072 to 41.37120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 41.2492 - MinusLogProbMetric: 41.2492 - val_loss: 41.3712 - val_MinusLogProbMetric: 41.3712 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 22/1000
2023-10-26 01:56:05.659 
Epoch 22/1000 
	 loss: 40.5787, MinusLogProbMetric: 40.5787, val_loss: 41.4732, val_MinusLogProbMetric: 41.4732

Epoch 22: val_loss did not improve from 41.37120
196/196 - 42s - loss: 40.5787 - MinusLogProbMetric: 40.5787 - val_loss: 41.4732 - val_MinusLogProbMetric: 41.4732 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 23/1000
2023-10-26 01:56:48.383 
Epoch 23/1000 
	 loss: 39.9906, MinusLogProbMetric: 39.9906, val_loss: 39.9943, val_MinusLogProbMetric: 39.9943

Epoch 23: val_loss improved from 41.37120 to 39.99427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 39.9906 - MinusLogProbMetric: 39.9906 - val_loss: 39.9943 - val_MinusLogProbMetric: 39.9943 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 24/1000
2023-10-26 01:57:31.980 
Epoch 24/1000 
	 loss: 39.5868, MinusLogProbMetric: 39.5868, val_loss: 40.1461, val_MinusLogProbMetric: 40.1461

Epoch 24: val_loss did not improve from 39.99427
196/196 - 43s - loss: 39.5868 - MinusLogProbMetric: 39.5868 - val_loss: 40.1461 - val_MinusLogProbMetric: 40.1461 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 25/1000
2023-10-26 01:58:14.710 
Epoch 25/1000 
	 loss: 38.9545, MinusLogProbMetric: 38.9545, val_loss: 38.5160, val_MinusLogProbMetric: 38.5160

Epoch 25: val_loss improved from 39.99427 to 38.51604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 38.9545 - MinusLogProbMetric: 38.9545 - val_loss: 38.5160 - val_MinusLogProbMetric: 38.5160 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 26/1000
2023-10-26 01:58:58.194 
Epoch 26/1000 
	 loss: 38.5947, MinusLogProbMetric: 38.5947, val_loss: 38.4833, val_MinusLogProbMetric: 38.4833

Epoch 26: val_loss improved from 38.51604 to 38.48330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 38.5947 - MinusLogProbMetric: 38.5947 - val_loss: 38.4833 - val_MinusLogProbMetric: 38.4833 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 27/1000
2023-10-26 01:59:41.752 
Epoch 27/1000 
	 loss: 38.0995, MinusLogProbMetric: 38.0995, val_loss: 38.9183, val_MinusLogProbMetric: 38.9183

Epoch 27: val_loss did not improve from 38.48330
196/196 - 43s - loss: 38.0995 - MinusLogProbMetric: 38.0995 - val_loss: 38.9183 - val_MinusLogProbMetric: 38.9183 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 28/1000
2023-10-26 02:00:24.450 
Epoch 28/1000 
	 loss: 37.9423, MinusLogProbMetric: 37.9423, val_loss: 37.8737, val_MinusLogProbMetric: 37.8737

Epoch 28: val_loss improved from 38.48330 to 37.87374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 37.9423 - MinusLogProbMetric: 37.9423 - val_loss: 37.8737 - val_MinusLogProbMetric: 37.8737 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 29/1000
2023-10-26 02:01:07.683 
Epoch 29/1000 
	 loss: 37.5676, MinusLogProbMetric: 37.5676, val_loss: 37.8913, val_MinusLogProbMetric: 37.8913

Epoch 29: val_loss did not improve from 37.87374
196/196 - 43s - loss: 37.5676 - MinusLogProbMetric: 37.5676 - val_loss: 37.8913 - val_MinusLogProbMetric: 37.8913 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 30/1000
2023-10-26 02:01:50.545 
Epoch 30/1000 
	 loss: 37.1910, MinusLogProbMetric: 37.1910, val_loss: 37.3034, val_MinusLogProbMetric: 37.3034

Epoch 30: val_loss improved from 37.87374 to 37.30341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 37.1910 - MinusLogProbMetric: 37.1910 - val_loss: 37.3034 - val_MinusLogProbMetric: 37.3034 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 31/1000
2023-10-26 02:02:34.123 
Epoch 31/1000 
	 loss: 36.8438, MinusLogProbMetric: 36.8438, val_loss: 37.0255, val_MinusLogProbMetric: 37.0255

Epoch 31: val_loss improved from 37.30341 to 37.02553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 36.8438 - MinusLogProbMetric: 36.8438 - val_loss: 37.0255 - val_MinusLogProbMetric: 37.0255 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 32/1000
2023-10-26 02:03:17.741 
Epoch 32/1000 
	 loss: 36.7799, MinusLogProbMetric: 36.7799, val_loss: 36.5340, val_MinusLogProbMetric: 36.5340

Epoch 32: val_loss improved from 37.02553 to 36.53402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 36.7799 - MinusLogProbMetric: 36.7799 - val_loss: 36.5340 - val_MinusLogProbMetric: 36.5340 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 33/1000
2023-10-26 02:04:00.968 
Epoch 33/1000 
	 loss: 36.5164, MinusLogProbMetric: 36.5164, val_loss: 36.9411, val_MinusLogProbMetric: 36.9411

Epoch 33: val_loss did not improve from 36.53402
196/196 - 43s - loss: 36.5164 - MinusLogProbMetric: 36.5164 - val_loss: 36.9411 - val_MinusLogProbMetric: 36.9411 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 34/1000
2023-10-26 02:04:43.546 
Epoch 34/1000 
	 loss: 36.1560, MinusLogProbMetric: 36.1560, val_loss: 36.7373, val_MinusLogProbMetric: 36.7373

Epoch 34: val_loss did not improve from 36.53402
196/196 - 43s - loss: 36.1560 - MinusLogProbMetric: 36.1560 - val_loss: 36.7373 - val_MinusLogProbMetric: 36.7373 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 35/1000
2023-10-26 02:05:26.077 
Epoch 35/1000 
	 loss: 36.0999, MinusLogProbMetric: 36.0999, val_loss: 36.2495, val_MinusLogProbMetric: 36.2495

Epoch 35: val_loss improved from 36.53402 to 36.24945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 36.0999 - MinusLogProbMetric: 36.0999 - val_loss: 36.2495 - val_MinusLogProbMetric: 36.2495 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 36/1000
2023-10-26 02:06:09.710 
Epoch 36/1000 
	 loss: 35.8102, MinusLogProbMetric: 35.8102, val_loss: 35.4446, val_MinusLogProbMetric: 35.4446

Epoch 36: val_loss improved from 36.24945 to 35.44458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 35.8102 - MinusLogProbMetric: 35.8102 - val_loss: 35.4446 - val_MinusLogProbMetric: 35.4446 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 37/1000
2023-10-26 02:06:53.351 
Epoch 37/1000 
	 loss: 35.5697, MinusLogProbMetric: 35.5697, val_loss: 36.0620, val_MinusLogProbMetric: 36.0620

Epoch 37: val_loss did not improve from 35.44458
196/196 - 43s - loss: 35.5697 - MinusLogProbMetric: 35.5697 - val_loss: 36.0620 - val_MinusLogProbMetric: 36.0620 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 38/1000
2023-10-26 02:07:36.062 
Epoch 38/1000 
	 loss: 35.4917, MinusLogProbMetric: 35.4917, val_loss: 35.1912, val_MinusLogProbMetric: 35.1912

Epoch 38: val_loss improved from 35.44458 to 35.19117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 35.4917 - MinusLogProbMetric: 35.4917 - val_loss: 35.1912 - val_MinusLogProbMetric: 35.1912 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 39/1000
2023-10-26 02:08:19.667 
Epoch 39/1000 
	 loss: 35.2286, MinusLogProbMetric: 35.2286, val_loss: 36.5965, val_MinusLogProbMetric: 36.5965

Epoch 39: val_loss did not improve from 35.19117
196/196 - 43s - loss: 35.2286 - MinusLogProbMetric: 35.2286 - val_loss: 36.5965 - val_MinusLogProbMetric: 36.5965 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 40/1000
2023-10-26 02:09:02.648 
Epoch 40/1000 
	 loss: 35.2687, MinusLogProbMetric: 35.2687, val_loss: 34.9287, val_MinusLogProbMetric: 34.9287

Epoch 40: val_loss improved from 35.19117 to 34.92868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 35.2687 - MinusLogProbMetric: 35.2687 - val_loss: 34.9287 - val_MinusLogProbMetric: 34.9287 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 41/1000
2023-10-26 02:09:46.389 
Epoch 41/1000 
	 loss: 35.1465, MinusLogProbMetric: 35.1465, val_loss: 35.1324, val_MinusLogProbMetric: 35.1324

Epoch 41: val_loss did not improve from 34.92868
196/196 - 43s - loss: 35.1465 - MinusLogProbMetric: 35.1465 - val_loss: 35.1324 - val_MinusLogProbMetric: 35.1324 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 42/1000
2023-10-26 02:10:28.931 
Epoch 42/1000 
	 loss: 34.7989, MinusLogProbMetric: 34.7989, val_loss: 35.0225, val_MinusLogProbMetric: 35.0225

Epoch 42: val_loss did not improve from 34.92868
196/196 - 43s - loss: 34.7989 - MinusLogProbMetric: 34.7989 - val_loss: 35.0225 - val_MinusLogProbMetric: 35.0225 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 43/1000
2023-10-26 02:11:11.583 
Epoch 43/1000 
	 loss: 34.6792, MinusLogProbMetric: 34.6792, val_loss: 34.8479, val_MinusLogProbMetric: 34.8479

Epoch 43: val_loss improved from 34.92868 to 34.84790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 34.6792 - MinusLogProbMetric: 34.6792 - val_loss: 34.8479 - val_MinusLogProbMetric: 34.8479 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 44/1000
2023-10-26 02:11:54.729 
Epoch 44/1000 
	 loss: 34.6568, MinusLogProbMetric: 34.6568, val_loss: 36.6028, val_MinusLogProbMetric: 36.6028

Epoch 44: val_loss did not improve from 34.84790
196/196 - 42s - loss: 34.6568 - MinusLogProbMetric: 34.6568 - val_loss: 36.6028 - val_MinusLogProbMetric: 36.6028 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 45/1000
2023-10-26 02:12:37.475 
Epoch 45/1000 
	 loss: 34.4519, MinusLogProbMetric: 34.4519, val_loss: 35.2821, val_MinusLogProbMetric: 35.2821

Epoch 45: val_loss did not improve from 34.84790
196/196 - 43s - loss: 34.4519 - MinusLogProbMetric: 34.4519 - val_loss: 35.2821 - val_MinusLogProbMetric: 35.2821 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 46/1000
2023-10-26 02:13:20.146 
Epoch 46/1000 
	 loss: 34.4189, MinusLogProbMetric: 34.4189, val_loss: 34.7558, val_MinusLogProbMetric: 34.7558

Epoch 46: val_loss improved from 34.84790 to 34.75579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 34.4189 - MinusLogProbMetric: 34.4189 - val_loss: 34.7558 - val_MinusLogProbMetric: 34.7558 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 47/1000
2023-10-26 02:14:03.596 
Epoch 47/1000 
	 loss: 34.2568, MinusLogProbMetric: 34.2568, val_loss: 34.6975, val_MinusLogProbMetric: 34.6975

Epoch 47: val_loss improved from 34.75579 to 34.69750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 34.2568 - MinusLogProbMetric: 34.2568 - val_loss: 34.6975 - val_MinusLogProbMetric: 34.6975 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 48/1000
2023-10-26 02:14:47.215 
Epoch 48/1000 
	 loss: 34.1951, MinusLogProbMetric: 34.1951, val_loss: 34.4932, val_MinusLogProbMetric: 34.4932

Epoch 48: val_loss improved from 34.69750 to 34.49316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 34.1951 - MinusLogProbMetric: 34.1951 - val_loss: 34.4932 - val_MinusLogProbMetric: 34.4932 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 49/1000
2023-10-26 02:15:30.501 
Epoch 49/1000 
	 loss: 34.0932, MinusLogProbMetric: 34.0932, val_loss: 35.1242, val_MinusLogProbMetric: 35.1242

Epoch 49: val_loss did not improve from 34.49316
196/196 - 43s - loss: 34.0932 - MinusLogProbMetric: 34.0932 - val_loss: 35.1242 - val_MinusLogProbMetric: 35.1242 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 50/1000
2023-10-26 02:16:12.997 
Epoch 50/1000 
	 loss: 34.0307, MinusLogProbMetric: 34.0307, val_loss: 35.2188, val_MinusLogProbMetric: 35.2188

Epoch 50: val_loss did not improve from 34.49316
196/196 - 42s - loss: 34.0307 - MinusLogProbMetric: 34.0307 - val_loss: 35.2188 - val_MinusLogProbMetric: 35.2188 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 51/1000
2023-10-26 02:16:55.314 
Epoch 51/1000 
	 loss: 33.8158, MinusLogProbMetric: 33.8158, val_loss: 34.3135, val_MinusLogProbMetric: 34.3135

Epoch 51: val_loss improved from 34.49316 to 34.31353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 33.8158 - MinusLogProbMetric: 33.8158 - val_loss: 34.3135 - val_MinusLogProbMetric: 34.3135 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 52/1000
2023-10-26 02:17:38.455 
Epoch 52/1000 
	 loss: 33.8074, MinusLogProbMetric: 33.8074, val_loss: 34.2426, val_MinusLogProbMetric: 34.2426

Epoch 52: val_loss improved from 34.31353 to 34.24256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 33.8074 - MinusLogProbMetric: 33.8074 - val_loss: 34.2426 - val_MinusLogProbMetric: 34.2426 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 53/1000
2023-10-26 02:18:21.804 
Epoch 53/1000 
	 loss: 33.6627, MinusLogProbMetric: 33.6627, val_loss: 33.9909, val_MinusLogProbMetric: 33.9909

Epoch 53: val_loss improved from 34.24256 to 33.99088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 33.6627 - MinusLogProbMetric: 33.6627 - val_loss: 33.9909 - val_MinusLogProbMetric: 33.9909 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 54/1000
2023-10-26 02:19:05.240 
Epoch 54/1000 
	 loss: 33.7253, MinusLogProbMetric: 33.7253, val_loss: 34.3826, val_MinusLogProbMetric: 34.3826

Epoch 54: val_loss did not improve from 33.99088
196/196 - 43s - loss: 33.7253 - MinusLogProbMetric: 33.7253 - val_loss: 34.3826 - val_MinusLogProbMetric: 34.3826 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 55/1000
2023-10-26 02:19:48.361 
Epoch 55/1000 
	 loss: 33.5221, MinusLogProbMetric: 33.5221, val_loss: 33.9805, val_MinusLogProbMetric: 33.9805

Epoch 55: val_loss improved from 33.99088 to 33.98054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 33.5221 - MinusLogProbMetric: 33.5221 - val_loss: 33.9805 - val_MinusLogProbMetric: 33.9805 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 56/1000
2023-10-26 02:20:32.177 
Epoch 56/1000 
	 loss: 33.3530, MinusLogProbMetric: 33.3530, val_loss: 33.5040, val_MinusLogProbMetric: 33.5040

Epoch 56: val_loss improved from 33.98054 to 33.50396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 33.3530 - MinusLogProbMetric: 33.3530 - val_loss: 33.5040 - val_MinusLogProbMetric: 33.5040 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 57/1000
2023-10-26 02:21:15.423 
Epoch 57/1000 
	 loss: 33.4031, MinusLogProbMetric: 33.4031, val_loss: 34.7190, val_MinusLogProbMetric: 34.7190

Epoch 57: val_loss did not improve from 33.50396
196/196 - 43s - loss: 33.4031 - MinusLogProbMetric: 33.4031 - val_loss: 34.7190 - val_MinusLogProbMetric: 34.7190 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 58/1000
2023-10-26 02:21:58.447 
Epoch 58/1000 
	 loss: 33.1795, MinusLogProbMetric: 33.1795, val_loss: 33.2700, val_MinusLogProbMetric: 33.2700

Epoch 58: val_loss improved from 33.50396 to 33.27002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 33.1795 - MinusLogProbMetric: 33.1795 - val_loss: 33.2700 - val_MinusLogProbMetric: 33.2700 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 59/1000
2023-10-26 02:22:41.773 
Epoch 59/1000 
	 loss: 33.0570, MinusLogProbMetric: 33.0570, val_loss: 34.2590, val_MinusLogProbMetric: 34.2590

Epoch 59: val_loss did not improve from 33.27002
196/196 - 43s - loss: 33.0570 - MinusLogProbMetric: 33.0570 - val_loss: 34.2590 - val_MinusLogProbMetric: 34.2590 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 60/1000
2023-10-26 02:23:24.765 
Epoch 60/1000 
	 loss: 33.2067, MinusLogProbMetric: 33.2067, val_loss: 35.4634, val_MinusLogProbMetric: 35.4634

Epoch 60: val_loss did not improve from 33.27002
196/196 - 43s - loss: 33.2067 - MinusLogProbMetric: 33.2067 - val_loss: 35.4634 - val_MinusLogProbMetric: 35.4634 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 61/1000
2023-10-26 02:24:07.886 
Epoch 61/1000 
	 loss: 33.1289, MinusLogProbMetric: 33.1289, val_loss: 33.5137, val_MinusLogProbMetric: 33.5137

Epoch 61: val_loss did not improve from 33.27002
196/196 - 43s - loss: 33.1289 - MinusLogProbMetric: 33.1289 - val_loss: 33.5137 - val_MinusLogProbMetric: 33.5137 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 62/1000
2023-10-26 02:24:50.795 
Epoch 62/1000 
	 loss: 33.0170, MinusLogProbMetric: 33.0170, val_loss: 32.6632, val_MinusLogProbMetric: 32.6632

Epoch 62: val_loss improved from 33.27002 to 32.66317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 33.0170 - MinusLogProbMetric: 33.0170 - val_loss: 32.6632 - val_MinusLogProbMetric: 32.6632 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 63/1000
2023-10-26 02:25:34.281 
Epoch 63/1000 
	 loss: 33.0416, MinusLogProbMetric: 33.0416, val_loss: 34.3245, val_MinusLogProbMetric: 34.3245

Epoch 63: val_loss did not improve from 32.66317
196/196 - 43s - loss: 33.0416 - MinusLogProbMetric: 33.0416 - val_loss: 34.3245 - val_MinusLogProbMetric: 34.3245 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 64/1000
2023-10-26 02:26:17.174 
Epoch 64/1000 
	 loss: 32.9400, MinusLogProbMetric: 32.9400, val_loss: 33.3335, val_MinusLogProbMetric: 33.3335

Epoch 64: val_loss did not improve from 32.66317
196/196 - 43s - loss: 32.9400 - MinusLogProbMetric: 32.9400 - val_loss: 33.3335 - val_MinusLogProbMetric: 33.3335 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 65/1000
2023-10-26 02:26:59.900 
Epoch 65/1000 
	 loss: 32.8762, MinusLogProbMetric: 32.8762, val_loss: 32.9761, val_MinusLogProbMetric: 32.9761

Epoch 65: val_loss did not improve from 32.66317
196/196 - 43s - loss: 32.8762 - MinusLogProbMetric: 32.8762 - val_loss: 32.9761 - val_MinusLogProbMetric: 32.9761 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 66/1000
2023-10-26 02:27:42.985 
Epoch 66/1000 
	 loss: 32.7069, MinusLogProbMetric: 32.7069, val_loss: 32.9924, val_MinusLogProbMetric: 32.9924

Epoch 66: val_loss did not improve from 32.66317
196/196 - 43s - loss: 32.7069 - MinusLogProbMetric: 32.7069 - val_loss: 32.9924 - val_MinusLogProbMetric: 32.9924 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 67/1000
2023-10-26 02:28:25.580 
Epoch 67/1000 
	 loss: 32.6758, MinusLogProbMetric: 32.6758, val_loss: 33.3825, val_MinusLogProbMetric: 33.3825

Epoch 67: val_loss did not improve from 32.66317
196/196 - 43s - loss: 32.6758 - MinusLogProbMetric: 32.6758 - val_loss: 33.3825 - val_MinusLogProbMetric: 33.3825 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 68/1000
2023-10-26 02:29:08.095 
Epoch 68/1000 
	 loss: 32.5268, MinusLogProbMetric: 32.5268, val_loss: 32.7342, val_MinusLogProbMetric: 32.7342

Epoch 68: val_loss did not improve from 32.66317
196/196 - 43s - loss: 32.5268 - MinusLogProbMetric: 32.5268 - val_loss: 32.7342 - val_MinusLogProbMetric: 32.7342 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 69/1000
2023-10-26 02:29:50.933 
Epoch 69/1000 
	 loss: 32.5022, MinusLogProbMetric: 32.5022, val_loss: 33.0526, val_MinusLogProbMetric: 33.0526

Epoch 69: val_loss did not improve from 32.66317
196/196 - 43s - loss: 32.5022 - MinusLogProbMetric: 32.5022 - val_loss: 33.0526 - val_MinusLogProbMetric: 33.0526 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 70/1000
2023-10-26 02:30:33.961 
Epoch 70/1000 
	 loss: 32.6073, MinusLogProbMetric: 32.6073, val_loss: 32.3334, val_MinusLogProbMetric: 32.3334

Epoch 70: val_loss improved from 32.66317 to 32.33341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 32.6073 - MinusLogProbMetric: 32.6073 - val_loss: 32.3334 - val_MinusLogProbMetric: 32.3334 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 71/1000
2023-10-26 02:31:17.754 
Epoch 71/1000 
	 loss: 32.3157, MinusLogProbMetric: 32.3157, val_loss: 32.8008, val_MinusLogProbMetric: 32.8008

Epoch 71: val_loss did not improve from 32.33341
196/196 - 43s - loss: 32.3157 - MinusLogProbMetric: 32.3157 - val_loss: 32.8008 - val_MinusLogProbMetric: 32.8008 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 72/1000
2023-10-26 02:32:00.526 
Epoch 72/1000 
	 loss: 32.3512, MinusLogProbMetric: 32.3512, val_loss: 32.6500, val_MinusLogProbMetric: 32.6500

Epoch 72: val_loss did not improve from 32.33341
196/196 - 43s - loss: 32.3512 - MinusLogProbMetric: 32.3512 - val_loss: 32.6500 - val_MinusLogProbMetric: 32.6500 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 73/1000
2023-10-26 02:32:43.231 
Epoch 73/1000 
	 loss: 32.2459, MinusLogProbMetric: 32.2459, val_loss: 32.8905, val_MinusLogProbMetric: 32.8905

Epoch 73: val_loss did not improve from 32.33341
196/196 - 43s - loss: 32.2459 - MinusLogProbMetric: 32.2459 - val_loss: 32.8905 - val_MinusLogProbMetric: 32.8905 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 74/1000
2023-10-26 02:33:26.126 
Epoch 74/1000 
	 loss: 32.1897, MinusLogProbMetric: 32.1897, val_loss: 32.2740, val_MinusLogProbMetric: 32.2740

Epoch 74: val_loss improved from 32.33341 to 32.27399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 32.1897 - MinusLogProbMetric: 32.1897 - val_loss: 32.2740 - val_MinusLogProbMetric: 32.2740 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 75/1000
2023-10-26 02:34:09.754 
Epoch 75/1000 
	 loss: 32.3946, MinusLogProbMetric: 32.3946, val_loss: 32.3290, val_MinusLogProbMetric: 32.3290

Epoch 75: val_loss did not improve from 32.27399
196/196 - 43s - loss: 32.3946 - MinusLogProbMetric: 32.3946 - val_loss: 32.3290 - val_MinusLogProbMetric: 32.3290 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 76/1000
2023-10-26 02:34:53.125 
Epoch 76/1000 
	 loss: 32.0069, MinusLogProbMetric: 32.0069, val_loss: 31.9225, val_MinusLogProbMetric: 31.9225

Epoch 76: val_loss improved from 32.27399 to 31.92252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 32.0069 - MinusLogProbMetric: 32.0069 - val_loss: 31.9225 - val_MinusLogProbMetric: 31.9225 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 77/1000
2023-10-26 02:35:36.688 
Epoch 77/1000 
	 loss: 32.0172, MinusLogProbMetric: 32.0172, val_loss: 31.9060, val_MinusLogProbMetric: 31.9060

Epoch 77: val_loss improved from 31.92252 to 31.90600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 32.0172 - MinusLogProbMetric: 32.0172 - val_loss: 31.9060 - val_MinusLogProbMetric: 31.9060 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 78/1000
2023-10-26 02:36:20.151 
Epoch 78/1000 
	 loss: 32.0687, MinusLogProbMetric: 32.0687, val_loss: 32.3863, val_MinusLogProbMetric: 32.3863

Epoch 78: val_loss did not improve from 31.90600
196/196 - 43s - loss: 32.0687 - MinusLogProbMetric: 32.0687 - val_loss: 32.3863 - val_MinusLogProbMetric: 32.3863 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 79/1000
2023-10-26 02:37:03.005 
Epoch 79/1000 
	 loss: 31.9389, MinusLogProbMetric: 31.9389, val_loss: 32.4611, val_MinusLogProbMetric: 32.4611

Epoch 79: val_loss did not improve from 31.90600
196/196 - 43s - loss: 31.9389 - MinusLogProbMetric: 31.9389 - val_loss: 32.4611 - val_MinusLogProbMetric: 32.4611 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 80/1000
2023-10-26 02:37:45.693 
Epoch 80/1000 
	 loss: 31.9106, MinusLogProbMetric: 31.9106, val_loss: 31.6253, val_MinusLogProbMetric: 31.6253

Epoch 80: val_loss improved from 31.90600 to 31.62534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 31.9106 - MinusLogProbMetric: 31.9106 - val_loss: 31.6253 - val_MinusLogProbMetric: 31.6253 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 81/1000
2023-10-26 02:38:28.928 
Epoch 81/1000 
	 loss: 31.7766, MinusLogProbMetric: 31.7766, val_loss: 32.4343, val_MinusLogProbMetric: 32.4343

Epoch 81: val_loss did not improve from 31.62534
196/196 - 43s - loss: 31.7766 - MinusLogProbMetric: 31.7766 - val_loss: 32.4343 - val_MinusLogProbMetric: 32.4343 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 82/1000
2023-10-26 02:39:11.764 
Epoch 82/1000 
	 loss: 31.8230, MinusLogProbMetric: 31.8230, val_loss: 33.9547, val_MinusLogProbMetric: 33.9547

Epoch 82: val_loss did not improve from 31.62534
196/196 - 43s - loss: 31.8230 - MinusLogProbMetric: 31.8230 - val_loss: 33.9547 - val_MinusLogProbMetric: 33.9547 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 83/1000
2023-10-26 02:39:54.665 
Epoch 83/1000 
	 loss: 31.9440, MinusLogProbMetric: 31.9440, val_loss: 33.2905, val_MinusLogProbMetric: 33.2905

Epoch 83: val_loss did not improve from 31.62534
196/196 - 43s - loss: 31.9440 - MinusLogProbMetric: 31.9440 - val_loss: 33.2905 - val_MinusLogProbMetric: 33.2905 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 84/1000
2023-10-26 02:40:37.544 
Epoch 84/1000 
	 loss: 31.7490, MinusLogProbMetric: 31.7490, val_loss: 32.1098, val_MinusLogProbMetric: 32.1098

Epoch 84: val_loss did not improve from 31.62534
196/196 - 43s - loss: 31.7490 - MinusLogProbMetric: 31.7490 - val_loss: 32.1098 - val_MinusLogProbMetric: 32.1098 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 85/1000
2023-10-26 02:41:20.250 
Epoch 85/1000 
	 loss: 31.6758, MinusLogProbMetric: 31.6758, val_loss: 31.6360, val_MinusLogProbMetric: 31.6360

Epoch 85: val_loss did not improve from 31.62534
196/196 - 43s - loss: 31.6758 - MinusLogProbMetric: 31.6758 - val_loss: 31.6360 - val_MinusLogProbMetric: 31.6360 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 86/1000
2023-10-26 02:42:03.474 
Epoch 86/1000 
	 loss: 31.6191, MinusLogProbMetric: 31.6191, val_loss: 31.6208, val_MinusLogProbMetric: 31.6208

Epoch 86: val_loss improved from 31.62534 to 31.62081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 31.6191 - MinusLogProbMetric: 31.6191 - val_loss: 31.6208 - val_MinusLogProbMetric: 31.6208 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 87/1000
2023-10-26 02:42:47.062 
Epoch 87/1000 
	 loss: 31.6112, MinusLogProbMetric: 31.6112, val_loss: 31.5418, val_MinusLogProbMetric: 31.5418

Epoch 87: val_loss improved from 31.62081 to 31.54183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 31.6112 - MinusLogProbMetric: 31.6112 - val_loss: 31.5418 - val_MinusLogProbMetric: 31.5418 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 88/1000
2023-10-26 02:43:30.237 
Epoch 88/1000 
	 loss: 31.5162, MinusLogProbMetric: 31.5162, val_loss: 31.9468, val_MinusLogProbMetric: 31.9468

Epoch 88: val_loss did not improve from 31.54183
196/196 - 42s - loss: 31.5162 - MinusLogProbMetric: 31.5162 - val_loss: 31.9468 - val_MinusLogProbMetric: 31.9468 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 89/1000
2023-10-26 02:44:12.321 
Epoch 89/1000 
	 loss: 31.5469, MinusLogProbMetric: 31.5469, val_loss: 32.1553, val_MinusLogProbMetric: 32.1553

Epoch 89: val_loss did not improve from 31.54183
196/196 - 42s - loss: 31.5469 - MinusLogProbMetric: 31.5469 - val_loss: 32.1553 - val_MinusLogProbMetric: 32.1553 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 90/1000
2023-10-26 02:44:54.949 
Epoch 90/1000 
	 loss: 31.4456, MinusLogProbMetric: 31.4456, val_loss: 32.5972, val_MinusLogProbMetric: 32.5972

Epoch 90: val_loss did not improve from 31.54183
196/196 - 43s - loss: 31.4456 - MinusLogProbMetric: 31.4456 - val_loss: 32.5972 - val_MinusLogProbMetric: 32.5972 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 91/1000
2023-10-26 02:45:37.825 
Epoch 91/1000 
	 loss: 31.5996, MinusLogProbMetric: 31.5996, val_loss: 32.8639, val_MinusLogProbMetric: 32.8639

Epoch 91: val_loss did not improve from 31.54183
196/196 - 43s - loss: 31.5996 - MinusLogProbMetric: 31.5996 - val_loss: 32.8639 - val_MinusLogProbMetric: 32.8639 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 92/1000
2023-10-26 02:46:20.519 
Epoch 92/1000 
	 loss: 31.4571, MinusLogProbMetric: 31.4571, val_loss: 32.0119, val_MinusLogProbMetric: 32.0119

Epoch 92: val_loss did not improve from 31.54183
196/196 - 43s - loss: 31.4571 - MinusLogProbMetric: 31.4571 - val_loss: 32.0119 - val_MinusLogProbMetric: 32.0119 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 93/1000
2023-10-26 02:47:03.297 
Epoch 93/1000 
	 loss: 31.2636, MinusLogProbMetric: 31.2636, val_loss: 31.4469, val_MinusLogProbMetric: 31.4469

Epoch 93: val_loss improved from 31.54183 to 31.44695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 31.2636 - MinusLogProbMetric: 31.2636 - val_loss: 31.4469 - val_MinusLogProbMetric: 31.4469 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 94/1000
2023-10-26 02:47:46.492 
Epoch 94/1000 
	 loss: 31.4671, MinusLogProbMetric: 31.4671, val_loss: 31.7600, val_MinusLogProbMetric: 31.7600

Epoch 94: val_loss did not improve from 31.44695
196/196 - 42s - loss: 31.4671 - MinusLogProbMetric: 31.4671 - val_loss: 31.7600 - val_MinusLogProbMetric: 31.7600 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 95/1000
2023-10-26 02:48:29.250 
Epoch 95/1000 
	 loss: 31.2593, MinusLogProbMetric: 31.2593, val_loss: 31.2973, val_MinusLogProbMetric: 31.2973

Epoch 95: val_loss improved from 31.44695 to 31.29732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 31.2593 - MinusLogProbMetric: 31.2593 - val_loss: 31.2973 - val_MinusLogProbMetric: 31.2973 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 96/1000
2023-10-26 02:49:12.226 
Epoch 96/1000 
	 loss: 31.3871, MinusLogProbMetric: 31.3871, val_loss: 31.5151, val_MinusLogProbMetric: 31.5151

Epoch 96: val_loss did not improve from 31.29732
196/196 - 42s - loss: 31.3871 - MinusLogProbMetric: 31.3871 - val_loss: 31.5151 - val_MinusLogProbMetric: 31.5151 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 97/1000
2023-10-26 02:49:54.874 
Epoch 97/1000 
	 loss: 31.3015, MinusLogProbMetric: 31.3015, val_loss: 31.8377, val_MinusLogProbMetric: 31.8377

Epoch 97: val_loss did not improve from 31.29732
196/196 - 43s - loss: 31.3015 - MinusLogProbMetric: 31.3015 - val_loss: 31.8377 - val_MinusLogProbMetric: 31.8377 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 98/1000
2023-10-26 02:50:38.007 
Epoch 98/1000 
	 loss: 31.2195, MinusLogProbMetric: 31.2195, val_loss: 31.4546, val_MinusLogProbMetric: 31.4546

Epoch 98: val_loss did not improve from 31.29732
196/196 - 43s - loss: 31.2195 - MinusLogProbMetric: 31.2195 - val_loss: 31.4546 - val_MinusLogProbMetric: 31.4546 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 99/1000
2023-10-26 02:51:20.880 
Epoch 99/1000 
	 loss: 31.1554, MinusLogProbMetric: 31.1554, val_loss: 31.3513, val_MinusLogProbMetric: 31.3513

Epoch 99: val_loss did not improve from 31.29732
196/196 - 43s - loss: 31.1554 - MinusLogProbMetric: 31.1554 - val_loss: 31.3513 - val_MinusLogProbMetric: 31.3513 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 100/1000
2023-10-26 02:52:03.730 
Epoch 100/1000 
	 loss: 31.0566, MinusLogProbMetric: 31.0566, val_loss: 31.8210, val_MinusLogProbMetric: 31.8210

Epoch 100: val_loss did not improve from 31.29732
196/196 - 43s - loss: 31.0566 - MinusLogProbMetric: 31.0566 - val_loss: 31.8210 - val_MinusLogProbMetric: 31.8210 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 101/1000
2023-10-26 02:52:46.324 
Epoch 101/1000 
	 loss: 31.0998, MinusLogProbMetric: 31.0998, val_loss: 31.2985, val_MinusLogProbMetric: 31.2985

Epoch 101: val_loss did not improve from 31.29732
196/196 - 43s - loss: 31.0998 - MinusLogProbMetric: 31.0998 - val_loss: 31.2985 - val_MinusLogProbMetric: 31.2985 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 102/1000
2023-10-26 02:53:28.647 
Epoch 102/1000 
	 loss: 31.2357, MinusLogProbMetric: 31.2357, val_loss: 31.6958, val_MinusLogProbMetric: 31.6958

Epoch 102: val_loss did not improve from 31.29732
196/196 - 42s - loss: 31.2357 - MinusLogProbMetric: 31.2357 - val_loss: 31.6958 - val_MinusLogProbMetric: 31.6958 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 103/1000
2023-10-26 02:54:10.820 
Epoch 103/1000 
	 loss: 31.1372, MinusLogProbMetric: 31.1372, val_loss: 31.3259, val_MinusLogProbMetric: 31.3259

Epoch 103: val_loss did not improve from 31.29732
196/196 - 42s - loss: 31.1372 - MinusLogProbMetric: 31.1372 - val_loss: 31.3259 - val_MinusLogProbMetric: 31.3259 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 104/1000
2023-10-26 02:54:53.468 
Epoch 104/1000 
	 loss: 31.0571, MinusLogProbMetric: 31.0571, val_loss: 31.5135, val_MinusLogProbMetric: 31.5135

Epoch 104: val_loss did not improve from 31.29732
196/196 - 43s - loss: 31.0571 - MinusLogProbMetric: 31.0571 - val_loss: 31.5135 - val_MinusLogProbMetric: 31.5135 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 105/1000
2023-10-26 02:55:35.743 
Epoch 105/1000 
	 loss: 31.0761, MinusLogProbMetric: 31.0761, val_loss: 30.8630, val_MinusLogProbMetric: 30.8630

Epoch 105: val_loss improved from 31.29732 to 30.86303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 31.0761 - MinusLogProbMetric: 31.0761 - val_loss: 30.8630 - val_MinusLogProbMetric: 30.8630 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 106/1000
2023-10-26 02:56:19.066 
Epoch 106/1000 
	 loss: 30.9373, MinusLogProbMetric: 30.9373, val_loss: 32.0210, val_MinusLogProbMetric: 32.0210

Epoch 106: val_loss did not improve from 30.86303
196/196 - 43s - loss: 30.9373 - MinusLogProbMetric: 30.9373 - val_loss: 32.0210 - val_MinusLogProbMetric: 32.0210 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 107/1000
2023-10-26 02:57:01.243 
Epoch 107/1000 
	 loss: 30.9592, MinusLogProbMetric: 30.9592, val_loss: 32.6104, val_MinusLogProbMetric: 32.6104

Epoch 107: val_loss did not improve from 30.86303
196/196 - 42s - loss: 30.9592 - MinusLogProbMetric: 30.9592 - val_loss: 32.6104 - val_MinusLogProbMetric: 32.6104 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 108/1000
2023-10-26 02:57:43.723 
Epoch 108/1000 
	 loss: 30.9421, MinusLogProbMetric: 30.9421, val_loss: 31.5357, val_MinusLogProbMetric: 31.5357

Epoch 108: val_loss did not improve from 30.86303
196/196 - 42s - loss: 30.9421 - MinusLogProbMetric: 30.9421 - val_loss: 31.5357 - val_MinusLogProbMetric: 31.5357 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 109/1000
2023-10-26 02:58:26.266 
Epoch 109/1000 
	 loss: 30.8180, MinusLogProbMetric: 30.8180, val_loss: 31.4269, val_MinusLogProbMetric: 31.4269

Epoch 109: val_loss did not improve from 30.86303
196/196 - 43s - loss: 30.8180 - MinusLogProbMetric: 30.8180 - val_loss: 31.4269 - val_MinusLogProbMetric: 31.4269 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 110/1000
2023-10-26 02:59:08.912 
Epoch 110/1000 
	 loss: 30.8122, MinusLogProbMetric: 30.8122, val_loss: 31.4713, val_MinusLogProbMetric: 31.4713

Epoch 110: val_loss did not improve from 30.86303
196/196 - 43s - loss: 30.8122 - MinusLogProbMetric: 30.8122 - val_loss: 31.4713 - val_MinusLogProbMetric: 31.4713 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 111/1000
2023-10-26 02:59:51.572 
Epoch 111/1000 
	 loss: 30.8775, MinusLogProbMetric: 30.8775, val_loss: 30.7868, val_MinusLogProbMetric: 30.7868

Epoch 111: val_loss improved from 30.86303 to 30.78682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 30.8775 - MinusLogProbMetric: 30.8775 - val_loss: 30.7868 - val_MinusLogProbMetric: 30.7868 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 112/1000
2023-10-26 03:00:34.901 
Epoch 112/1000 
	 loss: 30.7439, MinusLogProbMetric: 30.7439, val_loss: 31.0120, val_MinusLogProbMetric: 31.0120

Epoch 112: val_loss did not improve from 30.78682
196/196 - 43s - loss: 30.7439 - MinusLogProbMetric: 30.7439 - val_loss: 31.0120 - val_MinusLogProbMetric: 31.0120 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 113/1000
2023-10-26 03:01:17.596 
Epoch 113/1000 
	 loss: 30.8746, MinusLogProbMetric: 30.8746, val_loss: 31.2096, val_MinusLogProbMetric: 31.2096

Epoch 113: val_loss did not improve from 30.78682
196/196 - 43s - loss: 30.8746 - MinusLogProbMetric: 30.8746 - val_loss: 31.2096 - val_MinusLogProbMetric: 31.2096 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 114/1000
2023-10-26 03:02:00.267 
Epoch 114/1000 
	 loss: 30.7233, MinusLogProbMetric: 30.7233, val_loss: 31.1556, val_MinusLogProbMetric: 31.1556

Epoch 114: val_loss did not improve from 30.78682
196/196 - 43s - loss: 30.7233 - MinusLogProbMetric: 30.7233 - val_loss: 31.1556 - val_MinusLogProbMetric: 31.1556 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 115/1000
2023-10-26 03:02:42.606 
Epoch 115/1000 
	 loss: 30.9066, MinusLogProbMetric: 30.9066, val_loss: 31.4440, val_MinusLogProbMetric: 31.4440

Epoch 115: val_loss did not improve from 30.78682
196/196 - 42s - loss: 30.9066 - MinusLogProbMetric: 30.9066 - val_loss: 31.4440 - val_MinusLogProbMetric: 31.4440 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 116/1000
2023-10-26 03:03:25.519 
Epoch 116/1000 
	 loss: 30.7159, MinusLogProbMetric: 30.7159, val_loss: 30.9726, val_MinusLogProbMetric: 30.9726

Epoch 116: val_loss did not improve from 30.78682
196/196 - 43s - loss: 30.7159 - MinusLogProbMetric: 30.7159 - val_loss: 30.9726 - val_MinusLogProbMetric: 30.9726 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 117/1000
2023-10-26 03:04:07.804 
Epoch 117/1000 
	 loss: 30.8529, MinusLogProbMetric: 30.8529, val_loss: 31.2977, val_MinusLogProbMetric: 31.2977

Epoch 117: val_loss did not improve from 30.78682
196/196 - 42s - loss: 30.8529 - MinusLogProbMetric: 30.8529 - val_loss: 31.2977 - val_MinusLogProbMetric: 31.2977 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 118/1000
2023-10-26 03:04:49.821 
Epoch 118/1000 
	 loss: 30.7524, MinusLogProbMetric: 30.7524, val_loss: 30.7044, val_MinusLogProbMetric: 30.7044

Epoch 118: val_loss improved from 30.78682 to 30.70438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 30.7524 - MinusLogProbMetric: 30.7524 - val_loss: 30.7044 - val_MinusLogProbMetric: 30.7044 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 119/1000
2023-10-26 03:05:33.342 
Epoch 119/1000 
	 loss: 30.5933, MinusLogProbMetric: 30.5933, val_loss: 30.7698, val_MinusLogProbMetric: 30.7698

Epoch 119: val_loss did not improve from 30.70438
196/196 - 43s - loss: 30.5933 - MinusLogProbMetric: 30.5933 - val_loss: 30.7698 - val_MinusLogProbMetric: 30.7698 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 120/1000
2023-10-26 03:06:15.457 
Epoch 120/1000 
	 loss: 30.5977, MinusLogProbMetric: 30.5977, val_loss: 31.2783, val_MinusLogProbMetric: 31.2783

Epoch 120: val_loss did not improve from 30.70438
196/196 - 42s - loss: 30.5977 - MinusLogProbMetric: 30.5977 - val_loss: 31.2783 - val_MinusLogProbMetric: 31.2783 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 121/1000
2023-10-26 03:06:57.975 
Epoch 121/1000 
	 loss: 30.6229, MinusLogProbMetric: 30.6229, val_loss: 30.7194, val_MinusLogProbMetric: 30.7194

Epoch 121: val_loss did not improve from 30.70438
196/196 - 43s - loss: 30.6229 - MinusLogProbMetric: 30.6229 - val_loss: 30.7194 - val_MinusLogProbMetric: 30.7194 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 122/1000
2023-10-26 03:07:40.341 
Epoch 122/1000 
	 loss: 30.5266, MinusLogProbMetric: 30.5266, val_loss: 30.7716, val_MinusLogProbMetric: 30.7716

Epoch 122: val_loss did not improve from 30.70438
196/196 - 42s - loss: 30.5266 - MinusLogProbMetric: 30.5266 - val_loss: 30.7716 - val_MinusLogProbMetric: 30.7716 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 123/1000
2023-10-26 03:08:22.969 
Epoch 123/1000 
	 loss: 30.4030, MinusLogProbMetric: 30.4030, val_loss: 31.3958, val_MinusLogProbMetric: 31.3958

Epoch 123: val_loss did not improve from 30.70438
196/196 - 43s - loss: 30.4030 - MinusLogProbMetric: 30.4030 - val_loss: 31.3958 - val_MinusLogProbMetric: 31.3958 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 124/1000
2023-10-26 03:09:05.070 
Epoch 124/1000 
	 loss: 30.6078, MinusLogProbMetric: 30.6078, val_loss: 30.7272, val_MinusLogProbMetric: 30.7272

Epoch 124: val_loss did not improve from 30.70438
196/196 - 42s - loss: 30.6078 - MinusLogProbMetric: 30.6078 - val_loss: 30.7272 - val_MinusLogProbMetric: 30.7272 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 125/1000
2023-10-26 03:09:46.630 
Epoch 125/1000 
	 loss: 30.4266, MinusLogProbMetric: 30.4266, val_loss: 33.4386, val_MinusLogProbMetric: 33.4386

Epoch 125: val_loss did not improve from 30.70438
196/196 - 42s - loss: 30.4266 - MinusLogProbMetric: 30.4266 - val_loss: 33.4386 - val_MinusLogProbMetric: 33.4386 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 126/1000
2023-10-26 03:10:26.879 
Epoch 126/1000 
	 loss: 30.6451, MinusLogProbMetric: 30.6451, val_loss: 30.3614, val_MinusLogProbMetric: 30.3614

Epoch 126: val_loss improved from 30.70438 to 30.36137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 41s - loss: 30.6451 - MinusLogProbMetric: 30.6451 - val_loss: 30.3614 - val_MinusLogProbMetric: 30.3614 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 127/1000
2023-10-26 03:11:07.008 
Epoch 127/1000 
	 loss: 30.3619, MinusLogProbMetric: 30.3619, val_loss: 30.6250, val_MinusLogProbMetric: 30.6250

Epoch 127: val_loss did not improve from 30.36137
196/196 - 40s - loss: 30.3619 - MinusLogProbMetric: 30.3619 - val_loss: 30.6250 - val_MinusLogProbMetric: 30.6250 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 128/1000
2023-10-26 03:11:48.596 
Epoch 128/1000 
	 loss: 30.4545, MinusLogProbMetric: 30.4545, val_loss: 31.9621, val_MinusLogProbMetric: 31.9621

Epoch 128: val_loss did not improve from 30.36137
196/196 - 42s - loss: 30.4545 - MinusLogProbMetric: 30.4545 - val_loss: 31.9621 - val_MinusLogProbMetric: 31.9621 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 129/1000
2023-10-26 03:12:30.547 
Epoch 129/1000 
	 loss: 30.4727, MinusLogProbMetric: 30.4727, val_loss: 31.1806, val_MinusLogProbMetric: 31.1806

Epoch 129: val_loss did not improve from 30.36137
196/196 - 42s - loss: 30.4727 - MinusLogProbMetric: 30.4727 - val_loss: 31.1806 - val_MinusLogProbMetric: 31.1806 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 130/1000
2023-10-26 03:13:12.638 
Epoch 130/1000 
	 loss: 30.3690, MinusLogProbMetric: 30.3690, val_loss: 31.8180, val_MinusLogProbMetric: 31.8180

Epoch 130: val_loss did not improve from 30.36137
196/196 - 42s - loss: 30.3690 - MinusLogProbMetric: 30.3690 - val_loss: 31.8180 - val_MinusLogProbMetric: 31.8180 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 131/1000
2023-10-26 03:13:55.318 
Epoch 131/1000 
	 loss: 30.4853, MinusLogProbMetric: 30.4853, val_loss: 31.3779, val_MinusLogProbMetric: 31.3779

Epoch 131: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.4853 - MinusLogProbMetric: 30.4853 - val_loss: 31.3779 - val_MinusLogProbMetric: 31.3779 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 132/1000
2023-10-26 03:14:37.848 
Epoch 132/1000 
	 loss: 30.3096, MinusLogProbMetric: 30.3096, val_loss: 30.9777, val_MinusLogProbMetric: 30.9777

Epoch 132: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.3096 - MinusLogProbMetric: 30.3096 - val_loss: 30.9777 - val_MinusLogProbMetric: 30.9777 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 133/1000
2023-10-26 03:15:20.766 
Epoch 133/1000 
	 loss: 30.3134, MinusLogProbMetric: 30.3134, val_loss: 30.7498, val_MinusLogProbMetric: 30.7498

Epoch 133: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.3134 - MinusLogProbMetric: 30.3134 - val_loss: 30.7498 - val_MinusLogProbMetric: 30.7498 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 134/1000
2023-10-26 03:16:03.402 
Epoch 134/1000 
	 loss: 30.2967, MinusLogProbMetric: 30.2967, val_loss: 31.0692, val_MinusLogProbMetric: 31.0692

Epoch 134: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.2967 - MinusLogProbMetric: 30.2967 - val_loss: 31.0692 - val_MinusLogProbMetric: 31.0692 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 135/1000
2023-10-26 03:16:45.942 
Epoch 135/1000 
	 loss: 30.3375, MinusLogProbMetric: 30.3375, val_loss: 30.7701, val_MinusLogProbMetric: 30.7701

Epoch 135: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.3375 - MinusLogProbMetric: 30.3375 - val_loss: 30.7701 - val_MinusLogProbMetric: 30.7701 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 136/1000
2023-10-26 03:17:28.488 
Epoch 136/1000 
	 loss: 30.2177, MinusLogProbMetric: 30.2177, val_loss: 30.7375, val_MinusLogProbMetric: 30.7375

Epoch 136: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.2177 - MinusLogProbMetric: 30.2177 - val_loss: 30.7375 - val_MinusLogProbMetric: 30.7375 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 137/1000
2023-10-26 03:18:10.916 
Epoch 137/1000 
	 loss: 30.2672, MinusLogProbMetric: 30.2672, val_loss: 30.5515, val_MinusLogProbMetric: 30.5515

Epoch 137: val_loss did not improve from 30.36137
196/196 - 42s - loss: 30.2672 - MinusLogProbMetric: 30.2672 - val_loss: 30.5515 - val_MinusLogProbMetric: 30.5515 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 138/1000
2023-10-26 03:18:53.715 
Epoch 138/1000 
	 loss: 30.2921, MinusLogProbMetric: 30.2921, val_loss: 30.9309, val_MinusLogProbMetric: 30.9309

Epoch 138: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.2921 - MinusLogProbMetric: 30.2921 - val_loss: 30.9309 - val_MinusLogProbMetric: 30.9309 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 139/1000
2023-10-26 03:19:36.409 
Epoch 139/1000 
	 loss: 30.2198, MinusLogProbMetric: 30.2198, val_loss: 30.4145, val_MinusLogProbMetric: 30.4145

Epoch 139: val_loss did not improve from 30.36137
196/196 - 43s - loss: 30.2198 - MinusLogProbMetric: 30.2198 - val_loss: 30.4145 - val_MinusLogProbMetric: 30.4145 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 140/1000
2023-10-26 03:20:18.837 
Epoch 140/1000 
	 loss: 30.1391, MinusLogProbMetric: 30.1391, val_loss: 30.6093, val_MinusLogProbMetric: 30.6093

Epoch 140: val_loss did not improve from 30.36137
196/196 - 42s - loss: 30.1391 - MinusLogProbMetric: 30.1391 - val_loss: 30.6093 - val_MinusLogProbMetric: 30.6093 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 141/1000
2023-10-26 03:21:01.337 
Epoch 141/1000 
	 loss: 30.1615, MinusLogProbMetric: 30.1615, val_loss: 30.4403, val_MinusLogProbMetric: 30.4403

Epoch 141: val_loss did not improve from 30.36137
196/196 - 42s - loss: 30.1615 - MinusLogProbMetric: 30.1615 - val_loss: 30.4403 - val_MinusLogProbMetric: 30.4403 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 142/1000
2023-10-26 03:21:43.826 
Epoch 142/1000 
	 loss: 30.1915, MinusLogProbMetric: 30.1915, val_loss: 30.1643, val_MinusLogProbMetric: 30.1643

Epoch 142: val_loss improved from 30.36137 to 30.16435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 30.1915 - MinusLogProbMetric: 30.1915 - val_loss: 30.1643 - val_MinusLogProbMetric: 30.1643 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 143/1000
2023-10-26 03:22:27.112 
Epoch 143/1000 
	 loss: 30.3490, MinusLogProbMetric: 30.3490, val_loss: 30.8782, val_MinusLogProbMetric: 30.8782

Epoch 143: val_loss did not improve from 30.16435
196/196 - 43s - loss: 30.3490 - MinusLogProbMetric: 30.3490 - val_loss: 30.8782 - val_MinusLogProbMetric: 30.8782 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 144/1000
2023-10-26 03:23:09.718 
Epoch 144/1000 
	 loss: 30.1159, MinusLogProbMetric: 30.1159, val_loss: 30.4212, val_MinusLogProbMetric: 30.4212

Epoch 144: val_loss did not improve from 30.16435
196/196 - 43s - loss: 30.1159 - MinusLogProbMetric: 30.1159 - val_loss: 30.4212 - val_MinusLogProbMetric: 30.4212 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 145/1000
2023-10-26 03:23:52.509 
Epoch 145/1000 
	 loss: 30.1474, MinusLogProbMetric: 30.1474, val_loss: 30.8041, val_MinusLogProbMetric: 30.8041

Epoch 145: val_loss did not improve from 30.16435
196/196 - 43s - loss: 30.1474 - MinusLogProbMetric: 30.1474 - val_loss: 30.8041 - val_MinusLogProbMetric: 30.8041 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 146/1000
2023-10-26 03:24:34.984 
Epoch 146/1000 
	 loss: 30.1541, MinusLogProbMetric: 30.1541, val_loss: 31.0580, val_MinusLogProbMetric: 31.0580

Epoch 146: val_loss did not improve from 30.16435
196/196 - 42s - loss: 30.1541 - MinusLogProbMetric: 30.1541 - val_loss: 31.0580 - val_MinusLogProbMetric: 31.0580 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 147/1000
2023-10-26 03:25:17.408 
Epoch 147/1000 
	 loss: 30.0689, MinusLogProbMetric: 30.0689, val_loss: 31.1429, val_MinusLogProbMetric: 31.1429

Epoch 147: val_loss did not improve from 30.16435
196/196 - 42s - loss: 30.0689 - MinusLogProbMetric: 30.0689 - val_loss: 31.1429 - val_MinusLogProbMetric: 31.1429 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 148/1000
2023-10-26 03:26:00.118 
Epoch 148/1000 
	 loss: 30.1521, MinusLogProbMetric: 30.1521, val_loss: 31.4259, val_MinusLogProbMetric: 31.4259

Epoch 148: val_loss did not improve from 30.16435
196/196 - 43s - loss: 30.1521 - MinusLogProbMetric: 30.1521 - val_loss: 31.4259 - val_MinusLogProbMetric: 31.4259 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 149/1000
2023-10-26 03:26:42.653 
Epoch 149/1000 
	 loss: 30.1405, MinusLogProbMetric: 30.1405, val_loss: 30.0374, val_MinusLogProbMetric: 30.0374

Epoch 149: val_loss improved from 30.16435 to 30.03740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 30.1405 - MinusLogProbMetric: 30.1405 - val_loss: 30.0374 - val_MinusLogProbMetric: 30.0374 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 150/1000
2023-10-26 03:27:25.451 
Epoch 150/1000 
	 loss: 30.0853, MinusLogProbMetric: 30.0853, val_loss: 31.3081, val_MinusLogProbMetric: 31.3081

Epoch 150: val_loss did not improve from 30.03740
196/196 - 42s - loss: 30.0853 - MinusLogProbMetric: 30.0853 - val_loss: 31.3081 - val_MinusLogProbMetric: 31.3081 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 151/1000
2023-10-26 03:28:08.018 
Epoch 151/1000 
	 loss: 30.0762, MinusLogProbMetric: 30.0762, val_loss: 30.2948, val_MinusLogProbMetric: 30.2948

Epoch 151: val_loss did not improve from 30.03740
196/196 - 43s - loss: 30.0762 - MinusLogProbMetric: 30.0762 - val_loss: 30.2948 - val_MinusLogProbMetric: 30.2948 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 152/1000
2023-10-26 03:28:50.378 
Epoch 152/1000 
	 loss: 30.0546, MinusLogProbMetric: 30.0546, val_loss: 30.1244, val_MinusLogProbMetric: 30.1244

Epoch 152: val_loss did not improve from 30.03740
196/196 - 42s - loss: 30.0546 - MinusLogProbMetric: 30.0546 - val_loss: 30.1244 - val_MinusLogProbMetric: 30.1244 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 153/1000
2023-10-26 03:29:32.685 
Epoch 153/1000 
	 loss: 29.9327, MinusLogProbMetric: 29.9327, val_loss: 30.3337, val_MinusLogProbMetric: 30.3337

Epoch 153: val_loss did not improve from 30.03740
196/196 - 42s - loss: 29.9327 - MinusLogProbMetric: 29.9327 - val_loss: 30.3337 - val_MinusLogProbMetric: 30.3337 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 154/1000
2023-10-26 03:30:15.037 
Epoch 154/1000 
	 loss: 29.9901, MinusLogProbMetric: 29.9901, val_loss: 30.2118, val_MinusLogProbMetric: 30.2118

Epoch 154: val_loss did not improve from 30.03740
196/196 - 42s - loss: 29.9901 - MinusLogProbMetric: 29.9901 - val_loss: 30.2118 - val_MinusLogProbMetric: 30.2118 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 155/1000
2023-10-26 03:30:57.488 
Epoch 155/1000 
	 loss: 29.9829, MinusLogProbMetric: 29.9829, val_loss: 30.5840, val_MinusLogProbMetric: 30.5840

Epoch 155: val_loss did not improve from 30.03740
196/196 - 42s - loss: 29.9829 - MinusLogProbMetric: 29.9829 - val_loss: 30.5840 - val_MinusLogProbMetric: 30.5840 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 156/1000
2023-10-26 03:31:39.954 
Epoch 156/1000 
	 loss: 30.0344, MinusLogProbMetric: 30.0344, val_loss: 30.1546, val_MinusLogProbMetric: 30.1546

Epoch 156: val_loss did not improve from 30.03740
196/196 - 42s - loss: 30.0344 - MinusLogProbMetric: 30.0344 - val_loss: 30.1546 - val_MinusLogProbMetric: 30.1546 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 157/1000
2023-10-26 03:32:22.472 
Epoch 157/1000 
	 loss: 29.9161, MinusLogProbMetric: 29.9161, val_loss: 30.0530, val_MinusLogProbMetric: 30.0530

Epoch 157: val_loss did not improve from 30.03740
196/196 - 43s - loss: 29.9161 - MinusLogProbMetric: 29.9161 - val_loss: 30.0530 - val_MinusLogProbMetric: 30.0530 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 158/1000
2023-10-26 03:33:04.937 
Epoch 158/1000 
	 loss: 30.0817, MinusLogProbMetric: 30.0817, val_loss: 30.0404, val_MinusLogProbMetric: 30.0404

Epoch 158: val_loss did not improve from 30.03740
196/196 - 42s - loss: 30.0817 - MinusLogProbMetric: 30.0817 - val_loss: 30.0404 - val_MinusLogProbMetric: 30.0404 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 159/1000
2023-10-26 03:33:47.484 
Epoch 159/1000 
	 loss: 29.9714, MinusLogProbMetric: 29.9714, val_loss: 31.0267, val_MinusLogProbMetric: 31.0267

Epoch 159: val_loss did not improve from 30.03740
196/196 - 43s - loss: 29.9714 - MinusLogProbMetric: 29.9714 - val_loss: 31.0267 - val_MinusLogProbMetric: 31.0267 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 160/1000
2023-10-26 03:34:29.633 
Epoch 160/1000 
	 loss: 29.8910, MinusLogProbMetric: 29.8910, val_loss: 30.2113, val_MinusLogProbMetric: 30.2113

Epoch 160: val_loss did not improve from 30.03740
196/196 - 42s - loss: 29.8910 - MinusLogProbMetric: 29.8910 - val_loss: 30.2113 - val_MinusLogProbMetric: 30.2113 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 161/1000
2023-10-26 03:35:12.328 
Epoch 161/1000 
	 loss: 29.9179, MinusLogProbMetric: 29.9179, val_loss: 30.0179, val_MinusLogProbMetric: 30.0179

Epoch 161: val_loss improved from 30.03740 to 30.01794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 29.9179 - MinusLogProbMetric: 29.9179 - val_loss: 30.0179 - val_MinusLogProbMetric: 30.0179 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 162/1000
2023-10-26 03:35:55.334 
Epoch 162/1000 
	 loss: 29.8290, MinusLogProbMetric: 29.8290, val_loss: 30.6387, val_MinusLogProbMetric: 30.6387

Epoch 162: val_loss did not improve from 30.01794
196/196 - 42s - loss: 29.8290 - MinusLogProbMetric: 29.8290 - val_loss: 30.6387 - val_MinusLogProbMetric: 30.6387 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 163/1000
2023-10-26 03:36:37.558 
Epoch 163/1000 
	 loss: 29.9742, MinusLogProbMetric: 29.9742, val_loss: 29.9485, val_MinusLogProbMetric: 29.9485

Epoch 163: val_loss improved from 30.01794 to 29.94850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 29.9742 - MinusLogProbMetric: 29.9742 - val_loss: 29.9485 - val_MinusLogProbMetric: 29.9485 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 164/1000
2023-10-26 03:37:20.565 
Epoch 164/1000 
	 loss: 29.9087, MinusLogProbMetric: 29.9087, val_loss: 29.9283, val_MinusLogProbMetric: 29.9283

Epoch 164: val_loss improved from 29.94850 to 29.92834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 29.9087 - MinusLogProbMetric: 29.9087 - val_loss: 29.9283 - val_MinusLogProbMetric: 29.9283 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 165/1000
2023-10-26 03:38:03.867 
Epoch 165/1000 
	 loss: 29.8713, MinusLogProbMetric: 29.8713, val_loss: 30.1248, val_MinusLogProbMetric: 30.1248

Epoch 165: val_loss did not improve from 29.92834
196/196 - 43s - loss: 29.8713 - MinusLogProbMetric: 29.8713 - val_loss: 30.1248 - val_MinusLogProbMetric: 30.1248 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 166/1000
2023-10-26 03:38:46.239 
Epoch 166/1000 
	 loss: 29.8193, MinusLogProbMetric: 29.8193, val_loss: 30.3007, val_MinusLogProbMetric: 30.3007

Epoch 166: val_loss did not improve from 29.92834
196/196 - 42s - loss: 29.8193 - MinusLogProbMetric: 29.8193 - val_loss: 30.3007 - val_MinusLogProbMetric: 30.3007 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 167/1000
2023-10-26 03:39:28.521 
Epoch 167/1000 
	 loss: 29.7534, MinusLogProbMetric: 29.7534, val_loss: 30.9786, val_MinusLogProbMetric: 30.9786

Epoch 167: val_loss did not improve from 29.92834
196/196 - 42s - loss: 29.7534 - MinusLogProbMetric: 29.7534 - val_loss: 30.9786 - val_MinusLogProbMetric: 30.9786 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 168/1000
2023-10-26 03:40:10.885 
Epoch 168/1000 
	 loss: 29.8099, MinusLogProbMetric: 29.8099, val_loss: 30.5610, val_MinusLogProbMetric: 30.5610

Epoch 168: val_loss did not improve from 29.92834
196/196 - 42s - loss: 29.8099 - MinusLogProbMetric: 29.8099 - val_loss: 30.5610 - val_MinusLogProbMetric: 30.5610 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 169/1000
2023-10-26 03:40:52.589 
Epoch 169/1000 
	 loss: 29.8510, MinusLogProbMetric: 29.8510, val_loss: 29.8332, val_MinusLogProbMetric: 29.8332

Epoch 169: val_loss improved from 29.92834 to 29.83319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 29.8510 - MinusLogProbMetric: 29.8510 - val_loss: 29.8332 - val_MinusLogProbMetric: 29.8332 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 170/1000
2023-10-26 03:41:35.922 
Epoch 170/1000 
	 loss: 29.7729, MinusLogProbMetric: 29.7729, val_loss: 30.6383, val_MinusLogProbMetric: 30.6383

Epoch 170: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.7729 - MinusLogProbMetric: 29.7729 - val_loss: 30.6383 - val_MinusLogProbMetric: 30.6383 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 171/1000
2023-10-26 03:42:18.031 
Epoch 171/1000 
	 loss: 29.7183, MinusLogProbMetric: 29.7183, val_loss: 29.9566, val_MinusLogProbMetric: 29.9566

Epoch 171: val_loss did not improve from 29.83319
196/196 - 42s - loss: 29.7183 - MinusLogProbMetric: 29.7183 - val_loss: 29.9566 - val_MinusLogProbMetric: 29.9566 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 172/1000
2023-10-26 03:43:00.848 
Epoch 172/1000 
	 loss: 29.7118, MinusLogProbMetric: 29.7118, val_loss: 30.4442, val_MinusLogProbMetric: 30.4442

Epoch 172: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.7118 - MinusLogProbMetric: 29.7118 - val_loss: 30.4442 - val_MinusLogProbMetric: 30.4442 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 173/1000
2023-10-26 03:43:43.529 
Epoch 173/1000 
	 loss: 29.7892, MinusLogProbMetric: 29.7892, val_loss: 29.8792, val_MinusLogProbMetric: 29.8792

Epoch 173: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.7892 - MinusLogProbMetric: 29.7892 - val_loss: 29.8792 - val_MinusLogProbMetric: 29.8792 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 174/1000
2023-10-26 03:44:25.844 
Epoch 174/1000 
	 loss: 29.7355, MinusLogProbMetric: 29.7355, val_loss: 30.2056, val_MinusLogProbMetric: 30.2056

Epoch 174: val_loss did not improve from 29.83319
196/196 - 42s - loss: 29.7355 - MinusLogProbMetric: 29.7355 - val_loss: 30.2056 - val_MinusLogProbMetric: 30.2056 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 175/1000
2023-10-26 03:45:07.940 
Epoch 175/1000 
	 loss: 29.8326, MinusLogProbMetric: 29.8326, val_loss: 30.3040, val_MinusLogProbMetric: 30.3040

Epoch 175: val_loss did not improve from 29.83319
196/196 - 42s - loss: 29.8326 - MinusLogProbMetric: 29.8326 - val_loss: 30.3040 - val_MinusLogProbMetric: 30.3040 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 176/1000
2023-10-26 03:45:50.231 
Epoch 176/1000 
	 loss: 29.7247, MinusLogProbMetric: 29.7247, val_loss: 30.5477, val_MinusLogProbMetric: 30.5477

Epoch 176: val_loss did not improve from 29.83319
196/196 - 42s - loss: 29.7247 - MinusLogProbMetric: 29.7247 - val_loss: 30.5477 - val_MinusLogProbMetric: 30.5477 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 177/1000
2023-10-26 03:46:32.664 
Epoch 177/1000 
	 loss: 29.7253, MinusLogProbMetric: 29.7253, val_loss: 30.5465, val_MinusLogProbMetric: 30.5465

Epoch 177: val_loss did not improve from 29.83319
196/196 - 42s - loss: 29.7253 - MinusLogProbMetric: 29.7253 - val_loss: 30.5465 - val_MinusLogProbMetric: 30.5465 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 178/1000
2023-10-26 03:47:15.212 
Epoch 178/1000 
	 loss: 29.6806, MinusLogProbMetric: 29.6806, val_loss: 30.0375, val_MinusLogProbMetric: 30.0375

Epoch 178: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.6806 - MinusLogProbMetric: 29.6806 - val_loss: 30.0375 - val_MinusLogProbMetric: 30.0375 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 179/1000
2023-10-26 03:47:58.148 
Epoch 179/1000 
	 loss: 29.6951, MinusLogProbMetric: 29.6951, val_loss: 31.2370, val_MinusLogProbMetric: 31.2370

Epoch 179: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.6951 - MinusLogProbMetric: 29.6951 - val_loss: 31.2370 - val_MinusLogProbMetric: 31.2370 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 180/1000
2023-10-26 03:48:40.539 
Epoch 180/1000 
	 loss: 29.6860, MinusLogProbMetric: 29.6860, val_loss: 30.1985, val_MinusLogProbMetric: 30.1985

Epoch 180: val_loss did not improve from 29.83319
196/196 - 42s - loss: 29.6860 - MinusLogProbMetric: 29.6860 - val_loss: 30.1985 - val_MinusLogProbMetric: 30.1985 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 181/1000
2023-10-26 03:49:23.195 
Epoch 181/1000 
	 loss: 29.6267, MinusLogProbMetric: 29.6267, val_loss: 30.2898, val_MinusLogProbMetric: 30.2898

Epoch 181: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.6267 - MinusLogProbMetric: 29.6267 - val_loss: 30.2898 - val_MinusLogProbMetric: 30.2898 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 182/1000
2023-10-26 03:50:05.741 
Epoch 182/1000 
	 loss: 29.7035, MinusLogProbMetric: 29.7035, val_loss: 30.2644, val_MinusLogProbMetric: 30.2644

Epoch 182: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.7035 - MinusLogProbMetric: 29.7035 - val_loss: 30.2644 - val_MinusLogProbMetric: 30.2644 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 183/1000
2023-10-26 03:50:48.364 
Epoch 183/1000 
	 loss: 29.6737, MinusLogProbMetric: 29.6737, val_loss: 30.6126, val_MinusLogProbMetric: 30.6126

Epoch 183: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.6737 - MinusLogProbMetric: 29.6737 - val_loss: 30.6126 - val_MinusLogProbMetric: 30.6126 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 184/1000
2023-10-26 03:51:30.584 
Epoch 184/1000 
	 loss: 29.5945, MinusLogProbMetric: 29.5945, val_loss: 30.0902, val_MinusLogProbMetric: 30.0902

Epoch 184: val_loss did not improve from 29.83319
196/196 - 42s - loss: 29.5945 - MinusLogProbMetric: 29.5945 - val_loss: 30.0902 - val_MinusLogProbMetric: 30.0902 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 185/1000
2023-10-26 03:52:13.159 
Epoch 185/1000 
	 loss: 29.6408, MinusLogProbMetric: 29.6408, val_loss: 29.8985, val_MinusLogProbMetric: 29.8985

Epoch 185: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.6408 - MinusLogProbMetric: 29.6408 - val_loss: 29.8985 - val_MinusLogProbMetric: 29.8985 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 186/1000
2023-10-26 03:52:55.691 
Epoch 186/1000 
	 loss: 29.5980, MinusLogProbMetric: 29.5980, val_loss: 30.0163, val_MinusLogProbMetric: 30.0163

Epoch 186: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.5980 - MinusLogProbMetric: 29.5980 - val_loss: 30.0163 - val_MinusLogProbMetric: 30.0163 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 187/1000
2023-10-26 03:53:38.422 
Epoch 187/1000 
	 loss: 29.6001, MinusLogProbMetric: 29.6001, val_loss: 29.9465, val_MinusLogProbMetric: 29.9465

Epoch 187: val_loss did not improve from 29.83319
196/196 - 43s - loss: 29.6001 - MinusLogProbMetric: 29.6001 - val_loss: 29.9465 - val_MinusLogProbMetric: 29.9465 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 188/1000
2023-10-26 03:54:21.226 
Epoch 188/1000 
	 loss: 29.6443, MinusLogProbMetric: 29.6443, val_loss: 29.8233, val_MinusLogProbMetric: 29.8233

Epoch 188: val_loss improved from 29.83319 to 29.82334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 29.6443 - MinusLogProbMetric: 29.6443 - val_loss: 29.8233 - val_MinusLogProbMetric: 29.8233 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 189/1000
2023-10-26 03:55:04.133 
Epoch 189/1000 
	 loss: 29.6925, MinusLogProbMetric: 29.6925, val_loss: 30.1843, val_MinusLogProbMetric: 30.1843

Epoch 189: val_loss did not improve from 29.82334
196/196 - 42s - loss: 29.6925 - MinusLogProbMetric: 29.6925 - val_loss: 30.1843 - val_MinusLogProbMetric: 30.1843 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 190/1000
2023-10-26 03:55:46.422 
Epoch 190/1000 
	 loss: 29.6166, MinusLogProbMetric: 29.6166, val_loss: 29.8352, val_MinusLogProbMetric: 29.8352

Epoch 190: val_loss did not improve from 29.82334
196/196 - 42s - loss: 29.6166 - MinusLogProbMetric: 29.6166 - val_loss: 29.8352 - val_MinusLogProbMetric: 29.8352 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 191/1000
2023-10-26 03:56:28.955 
Epoch 191/1000 
	 loss: 29.5954, MinusLogProbMetric: 29.5954, val_loss: 29.8159, val_MinusLogProbMetric: 29.8159

Epoch 191: val_loss improved from 29.82334 to 29.81593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 29.5954 - MinusLogProbMetric: 29.5954 - val_loss: 29.8159 - val_MinusLogProbMetric: 29.8159 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 192/1000
2023-10-26 03:57:11.931 
Epoch 192/1000 
	 loss: 29.5812, MinusLogProbMetric: 29.5812, val_loss: 30.1229, val_MinusLogProbMetric: 30.1229

Epoch 192: val_loss did not improve from 29.81593
196/196 - 42s - loss: 29.5812 - MinusLogProbMetric: 29.5812 - val_loss: 30.1229 - val_MinusLogProbMetric: 30.1229 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 193/1000
2023-10-26 03:57:54.507 
Epoch 193/1000 
	 loss: 29.5579, MinusLogProbMetric: 29.5579, val_loss: 30.0454, val_MinusLogProbMetric: 30.0454

Epoch 193: val_loss did not improve from 29.81593
196/196 - 43s - loss: 29.5579 - MinusLogProbMetric: 29.5579 - val_loss: 30.0454 - val_MinusLogProbMetric: 30.0454 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 194/1000
2023-10-26 03:58:36.776 
Epoch 194/1000 
	 loss: 29.5990, MinusLogProbMetric: 29.5990, val_loss: 29.5928, val_MinusLogProbMetric: 29.5928

Epoch 194: val_loss improved from 29.81593 to 29.59278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 29.5990 - MinusLogProbMetric: 29.5990 - val_loss: 29.5928 - val_MinusLogProbMetric: 29.5928 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 195/1000
2023-10-26 03:59:20.181 
Epoch 195/1000 
	 loss: 29.5533, MinusLogProbMetric: 29.5533, val_loss: 29.9603, val_MinusLogProbMetric: 29.9603

Epoch 195: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.5533 - MinusLogProbMetric: 29.5533 - val_loss: 29.9603 - val_MinusLogProbMetric: 29.9603 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 196/1000
2023-10-26 04:00:02.701 
Epoch 196/1000 
	 loss: 29.5504, MinusLogProbMetric: 29.5504, val_loss: 29.7773, val_MinusLogProbMetric: 29.7773

Epoch 196: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.5504 - MinusLogProbMetric: 29.5504 - val_loss: 29.7773 - val_MinusLogProbMetric: 29.7773 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 197/1000
2023-10-26 04:00:45.028 
Epoch 197/1000 
	 loss: 29.4469, MinusLogProbMetric: 29.4469, val_loss: 29.7065, val_MinusLogProbMetric: 29.7065

Epoch 197: val_loss did not improve from 29.59278
196/196 - 42s - loss: 29.4469 - MinusLogProbMetric: 29.4469 - val_loss: 29.7065 - val_MinusLogProbMetric: 29.7065 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 198/1000
2023-10-26 04:01:27.941 
Epoch 198/1000 
	 loss: 29.5200, MinusLogProbMetric: 29.5200, val_loss: 29.6597, val_MinusLogProbMetric: 29.6597

Epoch 198: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.5200 - MinusLogProbMetric: 29.5200 - val_loss: 29.6597 - val_MinusLogProbMetric: 29.6597 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 199/1000
2023-10-26 04:02:10.535 
Epoch 199/1000 
	 loss: 29.4611, MinusLogProbMetric: 29.4611, val_loss: 30.6752, val_MinusLogProbMetric: 30.6752

Epoch 199: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.4611 - MinusLogProbMetric: 29.4611 - val_loss: 30.6752 - val_MinusLogProbMetric: 30.6752 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 200/1000
2023-10-26 04:02:53.188 
Epoch 200/1000 
	 loss: 29.4478, MinusLogProbMetric: 29.4478, val_loss: 30.3218, val_MinusLogProbMetric: 30.3218

Epoch 200: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.4478 - MinusLogProbMetric: 29.4478 - val_loss: 30.3218 - val_MinusLogProbMetric: 30.3218 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 201/1000
2023-10-26 04:03:35.872 
Epoch 201/1000 
	 loss: 29.4299, MinusLogProbMetric: 29.4299, val_loss: 31.1376, val_MinusLogProbMetric: 31.1376

Epoch 201: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.4299 - MinusLogProbMetric: 29.4299 - val_loss: 31.1376 - val_MinusLogProbMetric: 31.1376 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 202/1000
2023-10-26 04:04:18.708 
Epoch 202/1000 
	 loss: 29.5811, MinusLogProbMetric: 29.5811, val_loss: 29.7046, val_MinusLogProbMetric: 29.7046

Epoch 202: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.5811 - MinusLogProbMetric: 29.5811 - val_loss: 29.7046 - val_MinusLogProbMetric: 29.7046 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 203/1000
2023-10-26 04:05:01.073 
Epoch 203/1000 
	 loss: 29.4433, MinusLogProbMetric: 29.4433, val_loss: 30.0054, val_MinusLogProbMetric: 30.0054

Epoch 203: val_loss did not improve from 29.59278
196/196 - 42s - loss: 29.4433 - MinusLogProbMetric: 29.4433 - val_loss: 30.0054 - val_MinusLogProbMetric: 30.0054 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 204/1000
2023-10-26 04:05:44.020 
Epoch 204/1000 
	 loss: 29.3859, MinusLogProbMetric: 29.3859, val_loss: 29.8998, val_MinusLogProbMetric: 29.8998

Epoch 204: val_loss did not improve from 29.59278
196/196 - 43s - loss: 29.3859 - MinusLogProbMetric: 29.3859 - val_loss: 29.8998 - val_MinusLogProbMetric: 29.8998 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 205/1000
2023-10-26 04:06:26.124 
Epoch 205/1000 
	 loss: 29.4172, MinusLogProbMetric: 29.4172, val_loss: 29.9056, val_MinusLogProbMetric: 29.9056

Epoch 205: val_loss did not improve from 29.59278
196/196 - 42s - loss: 29.4172 - MinusLogProbMetric: 29.4172 - val_loss: 29.9056 - val_MinusLogProbMetric: 29.9056 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 206/1000
2023-10-26 04:07:08.627 
Epoch 206/1000 
	 loss: 29.4932, MinusLogProbMetric: 29.4932, val_loss: 29.7882, val_MinusLogProbMetric: 29.7882

Epoch 206: val_loss did not improve from 29.59278
196/196 - 42s - loss: 29.4932 - MinusLogProbMetric: 29.4932 - val_loss: 29.7882 - val_MinusLogProbMetric: 29.7882 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 207/1000
2023-10-26 04:07:50.922 
Epoch 207/1000 
	 loss: 29.4040, MinusLogProbMetric: 29.4040, val_loss: 30.0443, val_MinusLogProbMetric: 30.0443

Epoch 207: val_loss did not improve from 29.59278
196/196 - 42s - loss: 29.4040 - MinusLogProbMetric: 29.4040 - val_loss: 30.0443 - val_MinusLogProbMetric: 30.0443 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 208/1000
2023-10-26 04:08:33.421 
Epoch 208/1000 
	 loss: 29.4005, MinusLogProbMetric: 29.4005, val_loss: 29.4762, val_MinusLogProbMetric: 29.4762

Epoch 208: val_loss improved from 29.59278 to 29.47624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 29.4005 - MinusLogProbMetric: 29.4005 - val_loss: 29.4762 - val_MinusLogProbMetric: 29.4762 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 209/1000
2023-10-26 04:09:17.037 
Epoch 209/1000 
	 loss: 29.3844, MinusLogProbMetric: 29.3844, val_loss: 29.6662, val_MinusLogProbMetric: 29.6662

Epoch 209: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.3844 - MinusLogProbMetric: 29.3844 - val_loss: 29.6662 - val_MinusLogProbMetric: 29.6662 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 210/1000
2023-10-26 04:09:59.678 
Epoch 210/1000 
	 loss: 29.3693, MinusLogProbMetric: 29.3693, val_loss: 29.8058, val_MinusLogProbMetric: 29.8058

Epoch 210: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.3693 - MinusLogProbMetric: 29.3693 - val_loss: 29.8058 - val_MinusLogProbMetric: 29.8058 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 211/1000
2023-10-26 04:10:42.248 
Epoch 211/1000 
	 loss: 29.4111, MinusLogProbMetric: 29.4111, val_loss: 31.1742, val_MinusLogProbMetric: 31.1742

Epoch 211: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.4111 - MinusLogProbMetric: 29.4111 - val_loss: 31.1742 - val_MinusLogProbMetric: 31.1742 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 212/1000
2023-10-26 04:11:24.930 
Epoch 212/1000 
	 loss: 29.3789, MinusLogProbMetric: 29.3789, val_loss: 30.0591, val_MinusLogProbMetric: 30.0591

Epoch 212: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.3789 - MinusLogProbMetric: 29.3789 - val_loss: 30.0591 - val_MinusLogProbMetric: 30.0591 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 213/1000
2023-10-26 04:12:06.208 
Epoch 213/1000 
	 loss: 29.3312, MinusLogProbMetric: 29.3312, val_loss: 30.1553, val_MinusLogProbMetric: 30.1553

Epoch 213: val_loss did not improve from 29.47624
196/196 - 41s - loss: 29.3312 - MinusLogProbMetric: 29.3312 - val_loss: 30.1553 - val_MinusLogProbMetric: 30.1553 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 214/1000
2023-10-26 04:12:47.382 
Epoch 214/1000 
	 loss: 29.3744, MinusLogProbMetric: 29.3744, val_loss: 29.6639, val_MinusLogProbMetric: 29.6639

Epoch 214: val_loss did not improve from 29.47624
196/196 - 41s - loss: 29.3744 - MinusLogProbMetric: 29.3744 - val_loss: 29.6639 - val_MinusLogProbMetric: 29.6639 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 215/1000
2023-10-26 04:13:28.535 
Epoch 215/1000 
	 loss: 29.2766, MinusLogProbMetric: 29.2766, val_loss: 30.1043, val_MinusLogProbMetric: 30.1043

Epoch 215: val_loss did not improve from 29.47624
196/196 - 41s - loss: 29.2766 - MinusLogProbMetric: 29.2766 - val_loss: 30.1043 - val_MinusLogProbMetric: 30.1043 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 216/1000
2023-10-26 04:14:08.423 
Epoch 216/1000 
	 loss: 29.3632, MinusLogProbMetric: 29.3632, val_loss: 29.8077, val_MinusLogProbMetric: 29.8077

Epoch 216: val_loss did not improve from 29.47624
196/196 - 40s - loss: 29.3632 - MinusLogProbMetric: 29.3632 - val_loss: 29.8077 - val_MinusLogProbMetric: 29.8077 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 217/1000
2023-10-26 04:14:50.364 
Epoch 217/1000 
	 loss: 29.2893, MinusLogProbMetric: 29.2893, val_loss: 29.5383, val_MinusLogProbMetric: 29.5383

Epoch 217: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.2893 - MinusLogProbMetric: 29.2893 - val_loss: 29.5383 - val_MinusLogProbMetric: 29.5383 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 218/1000
2023-10-26 04:15:32.438 
Epoch 218/1000 
	 loss: 29.3793, MinusLogProbMetric: 29.3793, val_loss: 29.8294, val_MinusLogProbMetric: 29.8294

Epoch 218: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.3793 - MinusLogProbMetric: 29.3793 - val_loss: 29.8294 - val_MinusLogProbMetric: 29.8294 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 219/1000
2023-10-26 04:16:14.881 
Epoch 219/1000 
	 loss: 29.3789, MinusLogProbMetric: 29.3789, val_loss: 29.9847, val_MinusLogProbMetric: 29.9847

Epoch 219: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.3789 - MinusLogProbMetric: 29.3789 - val_loss: 29.9847 - val_MinusLogProbMetric: 29.9847 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 220/1000
2023-10-26 04:16:56.884 
Epoch 220/1000 
	 loss: 29.3176, MinusLogProbMetric: 29.3176, val_loss: 29.9310, val_MinusLogProbMetric: 29.9310

Epoch 220: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.3176 - MinusLogProbMetric: 29.3176 - val_loss: 29.9310 - val_MinusLogProbMetric: 29.9310 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 221/1000
2023-10-26 04:17:39.941 
Epoch 221/1000 
	 loss: 29.2439, MinusLogProbMetric: 29.2439, val_loss: 30.1238, val_MinusLogProbMetric: 30.1238

Epoch 221: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.2439 - MinusLogProbMetric: 29.2439 - val_loss: 30.1238 - val_MinusLogProbMetric: 30.1238 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 222/1000
2023-10-26 04:18:22.652 
Epoch 222/1000 
	 loss: 29.2713, MinusLogProbMetric: 29.2713, val_loss: 30.3253, val_MinusLogProbMetric: 30.3253

Epoch 222: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.2713 - MinusLogProbMetric: 29.2713 - val_loss: 30.3253 - val_MinusLogProbMetric: 30.3253 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 223/1000
2023-10-26 04:19:05.134 
Epoch 223/1000 
	 loss: 29.2209, MinusLogProbMetric: 29.2209, val_loss: 29.9737, val_MinusLogProbMetric: 29.9737

Epoch 223: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.2209 - MinusLogProbMetric: 29.2209 - val_loss: 29.9737 - val_MinusLogProbMetric: 29.9737 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 224/1000
2023-10-26 04:19:47.878 
Epoch 224/1000 
	 loss: 29.2574, MinusLogProbMetric: 29.2574, val_loss: 29.7532, val_MinusLogProbMetric: 29.7532

Epoch 224: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.2574 - MinusLogProbMetric: 29.2574 - val_loss: 29.7532 - val_MinusLogProbMetric: 29.7532 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 225/1000
2023-10-26 04:20:30.388 
Epoch 225/1000 
	 loss: 29.2946, MinusLogProbMetric: 29.2946, val_loss: 29.9722, val_MinusLogProbMetric: 29.9722

Epoch 225: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.2946 - MinusLogProbMetric: 29.2946 - val_loss: 29.9722 - val_MinusLogProbMetric: 29.9722 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 226/1000
2023-10-26 04:21:12.763 
Epoch 226/1000 
	 loss: 29.2474, MinusLogProbMetric: 29.2474, val_loss: 30.1619, val_MinusLogProbMetric: 30.1619

Epoch 226: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.2474 - MinusLogProbMetric: 29.2474 - val_loss: 30.1619 - val_MinusLogProbMetric: 30.1619 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 227/1000
2023-10-26 04:21:55.463 
Epoch 227/1000 
	 loss: 29.1845, MinusLogProbMetric: 29.1845, val_loss: 29.8494, val_MinusLogProbMetric: 29.8494

Epoch 227: val_loss did not improve from 29.47624
196/196 - 43s - loss: 29.1845 - MinusLogProbMetric: 29.1845 - val_loss: 29.8494 - val_MinusLogProbMetric: 29.8494 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 228/1000
2023-10-26 04:22:37.848 
Epoch 228/1000 
	 loss: 29.2808, MinusLogProbMetric: 29.2808, val_loss: 29.6591, val_MinusLogProbMetric: 29.6591

Epoch 228: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.2808 - MinusLogProbMetric: 29.2808 - val_loss: 29.6591 - val_MinusLogProbMetric: 29.6591 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 229/1000
2023-10-26 04:23:15.340 
Epoch 229/1000 
	 loss: 29.2641, MinusLogProbMetric: 29.2641, val_loss: 29.5533, val_MinusLogProbMetric: 29.5533

Epoch 229: val_loss did not improve from 29.47624
196/196 - 37s - loss: 29.2641 - MinusLogProbMetric: 29.2641 - val_loss: 29.5533 - val_MinusLogProbMetric: 29.5533 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 230/1000
2023-10-26 04:23:51.432 
Epoch 230/1000 
	 loss: 29.2339, MinusLogProbMetric: 29.2339, val_loss: 29.6462, val_MinusLogProbMetric: 29.6462

Epoch 230: val_loss did not improve from 29.47624
196/196 - 36s - loss: 29.2339 - MinusLogProbMetric: 29.2339 - val_loss: 29.6462 - val_MinusLogProbMetric: 29.6462 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 231/1000
2023-10-26 04:24:29.885 
Epoch 231/1000 
	 loss: 29.1512, MinusLogProbMetric: 29.1512, val_loss: 30.3942, val_MinusLogProbMetric: 30.3942

Epoch 231: val_loss did not improve from 29.47624
196/196 - 38s - loss: 29.1512 - MinusLogProbMetric: 29.1512 - val_loss: 30.3942 - val_MinusLogProbMetric: 30.3942 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 232/1000
2023-10-26 04:25:11.417 
Epoch 232/1000 
	 loss: 29.2169, MinusLogProbMetric: 29.2169, val_loss: 29.6373, val_MinusLogProbMetric: 29.6373

Epoch 232: val_loss did not improve from 29.47624
196/196 - 42s - loss: 29.2169 - MinusLogProbMetric: 29.2169 - val_loss: 29.6373 - val_MinusLogProbMetric: 29.6373 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 233/1000
2023-10-26 04:25:47.957 
Epoch 233/1000 
	 loss: 29.1912, MinusLogProbMetric: 29.1912, val_loss: 29.8711, val_MinusLogProbMetric: 29.8711

Epoch 233: val_loss did not improve from 29.47624
196/196 - 37s - loss: 29.1912 - MinusLogProbMetric: 29.1912 - val_loss: 29.8711 - val_MinusLogProbMetric: 29.8711 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 234/1000
2023-10-26 04:26:23.308 
Epoch 234/1000 
	 loss: 29.2334, MinusLogProbMetric: 29.2334, val_loss: 30.3653, val_MinusLogProbMetric: 30.3653

Epoch 234: val_loss did not improve from 29.47624
196/196 - 35s - loss: 29.2334 - MinusLogProbMetric: 29.2334 - val_loss: 30.3653 - val_MinusLogProbMetric: 30.3653 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 235/1000
2023-10-26 04:27:00.910 
Epoch 235/1000 
	 loss: 29.2131, MinusLogProbMetric: 29.2131, val_loss: 30.4533, val_MinusLogProbMetric: 30.4533

Epoch 235: val_loss did not improve from 29.47624
196/196 - 38s - loss: 29.2131 - MinusLogProbMetric: 29.2131 - val_loss: 30.4533 - val_MinusLogProbMetric: 30.4533 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 236/1000
2023-10-26 04:27:42.288 
Epoch 236/1000 
	 loss: 29.2225, MinusLogProbMetric: 29.2225, val_loss: 29.7129, val_MinusLogProbMetric: 29.7129

Epoch 236: val_loss did not improve from 29.47624
196/196 - 41s - loss: 29.2225 - MinusLogProbMetric: 29.2225 - val_loss: 29.7129 - val_MinusLogProbMetric: 29.7129 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 237/1000
2023-10-26 04:28:17.512 
Epoch 237/1000 
	 loss: 29.1705, MinusLogProbMetric: 29.1705, val_loss: 29.3285, val_MinusLogProbMetric: 29.3285

Epoch 237: val_loss improved from 29.47624 to 29.32850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 36s - loss: 29.1705 - MinusLogProbMetric: 29.1705 - val_loss: 29.3285 - val_MinusLogProbMetric: 29.3285 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 238/1000
2023-10-26 04:28:53.432 
Epoch 238/1000 
	 loss: 29.3055, MinusLogProbMetric: 29.3055, val_loss: 29.5393, val_MinusLogProbMetric: 29.5393

Epoch 238: val_loss did not improve from 29.32850
196/196 - 35s - loss: 29.3055 - MinusLogProbMetric: 29.3055 - val_loss: 29.5393 - val_MinusLogProbMetric: 29.5393 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 239/1000
2023-10-26 04:29:31.442 
Epoch 239/1000 
	 loss: 29.3028, MinusLogProbMetric: 29.3028, val_loss: 29.8984, val_MinusLogProbMetric: 29.8984

Epoch 239: val_loss did not improve from 29.32850
196/196 - 38s - loss: 29.3028 - MinusLogProbMetric: 29.3028 - val_loss: 29.8984 - val_MinusLogProbMetric: 29.8984 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 240/1000
2023-10-26 04:30:07.590 
Epoch 240/1000 
	 loss: 29.1379, MinusLogProbMetric: 29.1379, val_loss: 29.4484, val_MinusLogProbMetric: 29.4484

Epoch 240: val_loss did not improve from 29.32850
196/196 - 36s - loss: 29.1379 - MinusLogProbMetric: 29.1379 - val_loss: 29.4484 - val_MinusLogProbMetric: 29.4484 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 241/1000
2023-10-26 04:30:43.709 
Epoch 241/1000 
	 loss: 29.1574, MinusLogProbMetric: 29.1574, val_loss: 29.6415, val_MinusLogProbMetric: 29.6415

Epoch 241: val_loss did not improve from 29.32850
196/196 - 36s - loss: 29.1574 - MinusLogProbMetric: 29.1574 - val_loss: 29.6415 - val_MinusLogProbMetric: 29.6415 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 242/1000
2023-10-26 04:31:19.581 
Epoch 242/1000 
	 loss: 29.2122, MinusLogProbMetric: 29.2122, val_loss: 29.9097, val_MinusLogProbMetric: 29.9097

Epoch 242: val_loss did not improve from 29.32850
196/196 - 36s - loss: 29.2122 - MinusLogProbMetric: 29.2122 - val_loss: 29.9097 - val_MinusLogProbMetric: 29.9097 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 243/1000
2023-10-26 04:31:57.550 
Epoch 243/1000 
	 loss: 29.1093, MinusLogProbMetric: 29.1093, val_loss: 30.4920, val_MinusLogProbMetric: 30.4920

Epoch 243: val_loss did not improve from 29.32850
196/196 - 38s - loss: 29.1093 - MinusLogProbMetric: 29.1093 - val_loss: 30.4920 - val_MinusLogProbMetric: 30.4920 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 244/1000
2023-10-26 04:32:36.577 
Epoch 244/1000 
	 loss: 29.1811, MinusLogProbMetric: 29.1811, val_loss: 29.7248, val_MinusLogProbMetric: 29.7248

Epoch 244: val_loss did not improve from 29.32850
196/196 - 39s - loss: 29.1811 - MinusLogProbMetric: 29.1811 - val_loss: 29.7248 - val_MinusLogProbMetric: 29.7248 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 245/1000
2023-10-26 04:33:11.770 
Epoch 245/1000 
	 loss: 29.1375, MinusLogProbMetric: 29.1375, val_loss: 29.5077, val_MinusLogProbMetric: 29.5077

Epoch 245: val_loss did not improve from 29.32850
196/196 - 35s - loss: 29.1375 - MinusLogProbMetric: 29.1375 - val_loss: 29.5077 - val_MinusLogProbMetric: 29.5077 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 246/1000
2023-10-26 04:33:48.373 
Epoch 246/1000 
	 loss: 29.0699, MinusLogProbMetric: 29.0699, val_loss: 30.3340, val_MinusLogProbMetric: 30.3340

Epoch 246: val_loss did not improve from 29.32850
196/196 - 37s - loss: 29.0699 - MinusLogProbMetric: 29.0699 - val_loss: 30.3340 - val_MinusLogProbMetric: 30.3340 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 247/1000
2023-10-26 04:34:29.109 
Epoch 247/1000 
	 loss: 29.0627, MinusLogProbMetric: 29.0627, val_loss: 29.6395, val_MinusLogProbMetric: 29.6395

Epoch 247: val_loss did not improve from 29.32850
196/196 - 41s - loss: 29.0627 - MinusLogProbMetric: 29.0627 - val_loss: 29.6395 - val_MinusLogProbMetric: 29.6395 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 248/1000
2023-10-26 04:35:07.030 
Epoch 248/1000 
	 loss: 29.0959, MinusLogProbMetric: 29.0959, val_loss: 29.8735, val_MinusLogProbMetric: 29.8735

Epoch 248: val_loss did not improve from 29.32850
196/196 - 38s - loss: 29.0959 - MinusLogProbMetric: 29.0959 - val_loss: 29.8735 - val_MinusLogProbMetric: 29.8735 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 249/1000
2023-10-26 04:35:42.460 
Epoch 249/1000 
	 loss: 29.1055, MinusLogProbMetric: 29.1055, val_loss: 29.3255, val_MinusLogProbMetric: 29.3255

Epoch 249: val_loss improved from 29.32850 to 29.32551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 36s - loss: 29.1055 - MinusLogProbMetric: 29.1055 - val_loss: 29.3255 - val_MinusLogProbMetric: 29.3255 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 250/1000
2023-10-26 04:36:22.078 
Epoch 250/1000 
	 loss: 29.0199, MinusLogProbMetric: 29.0199, val_loss: 30.1349, val_MinusLogProbMetric: 30.1349

Epoch 250: val_loss did not improve from 29.32551
196/196 - 39s - loss: 29.0199 - MinusLogProbMetric: 29.0199 - val_loss: 30.1349 - val_MinusLogProbMetric: 30.1349 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 251/1000
2023-10-26 04:37:02.136 
Epoch 251/1000 
	 loss: 29.0449, MinusLogProbMetric: 29.0449, val_loss: 29.6246, val_MinusLogProbMetric: 29.6246

Epoch 251: val_loss did not improve from 29.32551
196/196 - 40s - loss: 29.0449 - MinusLogProbMetric: 29.0449 - val_loss: 29.6246 - val_MinusLogProbMetric: 29.6246 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 252/1000
2023-10-26 04:37:39.112 
Epoch 252/1000 
	 loss: 29.0570, MinusLogProbMetric: 29.0570, val_loss: 29.9966, val_MinusLogProbMetric: 29.9966

Epoch 252: val_loss did not improve from 29.32551
196/196 - 37s - loss: 29.0570 - MinusLogProbMetric: 29.0570 - val_loss: 29.9966 - val_MinusLogProbMetric: 29.9966 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 253/1000
2023-10-26 04:38:14.641 
Epoch 253/1000 
	 loss: 29.0783, MinusLogProbMetric: 29.0783, val_loss: 29.3331, val_MinusLogProbMetric: 29.3331

Epoch 253: val_loss did not improve from 29.32551
196/196 - 36s - loss: 29.0783 - MinusLogProbMetric: 29.0783 - val_loss: 29.3331 - val_MinusLogProbMetric: 29.3331 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 254/1000
2023-10-26 04:38:54.238 
Epoch 254/1000 
	 loss: 29.1008, MinusLogProbMetric: 29.1008, val_loss: 29.7620, val_MinusLogProbMetric: 29.7620

Epoch 254: val_loss did not improve from 29.32551
196/196 - 40s - loss: 29.1008 - MinusLogProbMetric: 29.1008 - val_loss: 29.7620 - val_MinusLogProbMetric: 29.7620 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 255/1000
2023-10-26 04:39:34.562 
Epoch 255/1000 
	 loss: 29.0672, MinusLogProbMetric: 29.0672, val_loss: 29.3388, val_MinusLogProbMetric: 29.3388

Epoch 255: val_loss did not improve from 29.32551
196/196 - 40s - loss: 29.0672 - MinusLogProbMetric: 29.0672 - val_loss: 29.3388 - val_MinusLogProbMetric: 29.3388 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 256/1000
2023-10-26 04:40:10.036 
Epoch 256/1000 
	 loss: 28.9951, MinusLogProbMetric: 28.9951, val_loss: 29.8252, val_MinusLogProbMetric: 29.8252

Epoch 256: val_loss did not improve from 29.32551
196/196 - 35s - loss: 28.9951 - MinusLogProbMetric: 28.9951 - val_loss: 29.8252 - val_MinusLogProbMetric: 29.8252 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 257/1000
2023-10-26 04:40:45.703 
Epoch 257/1000 
	 loss: 29.0225, MinusLogProbMetric: 29.0225, val_loss: 30.1784, val_MinusLogProbMetric: 30.1784

Epoch 257: val_loss did not improve from 29.32551
196/196 - 36s - loss: 29.0225 - MinusLogProbMetric: 29.0225 - val_loss: 30.1784 - val_MinusLogProbMetric: 30.1784 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 258/1000
2023-10-26 04:41:26.204 
Epoch 258/1000 
	 loss: 29.1353, MinusLogProbMetric: 29.1353, val_loss: 29.3080, val_MinusLogProbMetric: 29.3080

Epoch 258: val_loss improved from 29.32551 to 29.30803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 41s - loss: 29.1353 - MinusLogProbMetric: 29.1353 - val_loss: 29.3080 - val_MinusLogProbMetric: 29.3080 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 259/1000
2023-10-26 04:42:07.334 
Epoch 259/1000 
	 loss: 29.0694, MinusLogProbMetric: 29.0694, val_loss: 29.5581, val_MinusLogProbMetric: 29.5581

Epoch 259: val_loss did not improve from 29.30803
196/196 - 40s - loss: 29.0694 - MinusLogProbMetric: 29.0694 - val_loss: 29.5581 - val_MinusLogProbMetric: 29.5581 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 260/1000
2023-10-26 04:42:42.799 
Epoch 260/1000 
	 loss: 28.9697, MinusLogProbMetric: 28.9697, val_loss: 29.6395, val_MinusLogProbMetric: 29.6395

Epoch 260: val_loss did not improve from 29.30803
196/196 - 35s - loss: 28.9697 - MinusLogProbMetric: 28.9697 - val_loss: 29.6395 - val_MinusLogProbMetric: 29.6395 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 261/1000
2023-10-26 04:43:18.604 
Epoch 261/1000 
	 loss: 29.0289, MinusLogProbMetric: 29.0289, val_loss: 29.6080, val_MinusLogProbMetric: 29.6080

Epoch 261: val_loss did not improve from 29.30803
196/196 - 36s - loss: 29.0289 - MinusLogProbMetric: 29.0289 - val_loss: 29.6080 - val_MinusLogProbMetric: 29.6080 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 262/1000
2023-10-26 04:43:59.205 
Epoch 262/1000 
	 loss: 29.0254, MinusLogProbMetric: 29.0254, val_loss: 29.6449, val_MinusLogProbMetric: 29.6449

Epoch 262: val_loss did not improve from 29.30803
196/196 - 41s - loss: 29.0254 - MinusLogProbMetric: 29.0254 - val_loss: 29.6449 - val_MinusLogProbMetric: 29.6449 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 263/1000
2023-10-26 04:44:39.848 
Epoch 263/1000 
	 loss: 29.0208, MinusLogProbMetric: 29.0208, val_loss: 29.1943, val_MinusLogProbMetric: 29.1943

Epoch 263: val_loss improved from 29.30803 to 29.19427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 41s - loss: 29.0208 - MinusLogProbMetric: 29.0208 - val_loss: 29.1943 - val_MinusLogProbMetric: 29.1943 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 264/1000
2023-10-26 04:45:16.051 
Epoch 264/1000 
	 loss: 28.9555, MinusLogProbMetric: 28.9555, val_loss: 29.4895, val_MinusLogProbMetric: 29.4895

Epoch 264: val_loss did not improve from 29.19427
196/196 - 36s - loss: 28.9555 - MinusLogProbMetric: 28.9555 - val_loss: 29.4895 - val_MinusLogProbMetric: 29.4895 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 265/1000
2023-10-26 04:45:52.482 
Epoch 265/1000 
	 loss: 28.9106, MinusLogProbMetric: 28.9106, val_loss: 29.6056, val_MinusLogProbMetric: 29.6056

Epoch 265: val_loss did not improve from 29.19427
196/196 - 36s - loss: 28.9106 - MinusLogProbMetric: 28.9106 - val_loss: 29.6056 - val_MinusLogProbMetric: 29.6056 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 266/1000
2023-10-26 04:46:30.176 
Epoch 266/1000 
	 loss: 29.1055, MinusLogProbMetric: 29.1055, val_loss: 29.6001, val_MinusLogProbMetric: 29.6001

Epoch 266: val_loss did not improve from 29.19427
196/196 - 38s - loss: 29.1055 - MinusLogProbMetric: 29.1055 - val_loss: 29.6001 - val_MinusLogProbMetric: 29.6001 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 267/1000
2023-10-26 04:47:07.292 
Epoch 267/1000 
	 loss: 28.9713, MinusLogProbMetric: 28.9713, val_loss: 29.9440, val_MinusLogProbMetric: 29.9440

Epoch 267: val_loss did not improve from 29.19427
196/196 - 37s - loss: 28.9713 - MinusLogProbMetric: 28.9713 - val_loss: 29.9440 - val_MinusLogProbMetric: 29.9440 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 268/1000
2023-10-26 04:47:42.501 
Epoch 268/1000 
	 loss: 28.9510, MinusLogProbMetric: 28.9510, val_loss: 29.5023, val_MinusLogProbMetric: 29.5023

Epoch 268: val_loss did not improve from 29.19427
196/196 - 35s - loss: 28.9510 - MinusLogProbMetric: 28.9510 - val_loss: 29.5023 - val_MinusLogProbMetric: 29.5023 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 269/1000
2023-10-26 04:48:18.410 
Epoch 269/1000 
	 loss: 29.0719, MinusLogProbMetric: 29.0719, val_loss: 29.3982, val_MinusLogProbMetric: 29.3982

Epoch 269: val_loss did not improve from 29.19427
196/196 - 36s - loss: 29.0719 - MinusLogProbMetric: 29.0719 - val_loss: 29.3982 - val_MinusLogProbMetric: 29.3982 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 270/1000
2023-10-26 04:48:59.652 
Epoch 270/1000 
	 loss: 29.0013, MinusLogProbMetric: 29.0013, val_loss: 29.1627, val_MinusLogProbMetric: 29.1627

Epoch 270: val_loss improved from 29.19427 to 29.16271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 29.0013 - MinusLogProbMetric: 29.0013 - val_loss: 29.1627 - val_MinusLogProbMetric: 29.1627 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 271/1000
2023-10-26 04:49:36.400 
Epoch 271/1000 
	 loss: 28.9473, MinusLogProbMetric: 28.9473, val_loss: 29.5203, val_MinusLogProbMetric: 29.5203

Epoch 271: val_loss did not improve from 29.16271
196/196 - 36s - loss: 28.9473 - MinusLogProbMetric: 28.9473 - val_loss: 29.5203 - val_MinusLogProbMetric: 29.5203 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 272/1000
2023-10-26 04:50:12.711 
Epoch 272/1000 
	 loss: 28.9807, MinusLogProbMetric: 28.9807, val_loss: 29.5976, val_MinusLogProbMetric: 29.5976

Epoch 272: val_loss did not improve from 29.16271
196/196 - 36s - loss: 28.9807 - MinusLogProbMetric: 28.9807 - val_loss: 29.5976 - val_MinusLogProbMetric: 29.5976 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 273/1000
2023-10-26 04:50:50.701 
Epoch 273/1000 
	 loss: 29.0176, MinusLogProbMetric: 29.0176, val_loss: 29.2957, val_MinusLogProbMetric: 29.2957

Epoch 273: val_loss did not improve from 29.16271
196/196 - 38s - loss: 29.0176 - MinusLogProbMetric: 29.0176 - val_loss: 29.2957 - val_MinusLogProbMetric: 29.2957 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 274/1000
2023-10-26 04:51:31.827 
Epoch 274/1000 
	 loss: 29.0866, MinusLogProbMetric: 29.0866, val_loss: 29.7112, val_MinusLogProbMetric: 29.7112

Epoch 274: val_loss did not improve from 29.16271
196/196 - 41s - loss: 29.0866 - MinusLogProbMetric: 29.0866 - val_loss: 29.7112 - val_MinusLogProbMetric: 29.7112 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 275/1000
2023-10-26 04:52:09.712 
Epoch 275/1000 
	 loss: 28.9699, MinusLogProbMetric: 28.9699, val_loss: 29.5602, val_MinusLogProbMetric: 29.5602

Epoch 275: val_loss did not improve from 29.16271
196/196 - 38s - loss: 28.9699 - MinusLogProbMetric: 28.9699 - val_loss: 29.5602 - val_MinusLogProbMetric: 29.5602 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 276/1000
2023-10-26 04:52:44.697 
Epoch 276/1000 
	 loss: 28.9681, MinusLogProbMetric: 28.9681, val_loss: 29.3174, val_MinusLogProbMetric: 29.3174

Epoch 276: val_loss did not improve from 29.16271
196/196 - 35s - loss: 28.9681 - MinusLogProbMetric: 28.9681 - val_loss: 29.3174 - val_MinusLogProbMetric: 29.3174 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 277/1000
2023-10-26 04:53:19.744 
Epoch 277/1000 
	 loss: 28.9395, MinusLogProbMetric: 28.9395, val_loss: 30.4747, val_MinusLogProbMetric: 30.4747

Epoch 277: val_loss did not improve from 29.16271
196/196 - 35s - loss: 28.9395 - MinusLogProbMetric: 28.9395 - val_loss: 30.4747 - val_MinusLogProbMetric: 30.4747 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 278/1000
2023-10-26 04:53:56.187 
Epoch 278/1000 
	 loss: 28.9960, MinusLogProbMetric: 28.9960, val_loss: 29.7090, val_MinusLogProbMetric: 29.7090

Epoch 278: val_loss did not improve from 29.16271
196/196 - 36s - loss: 28.9960 - MinusLogProbMetric: 28.9960 - val_loss: 29.7090 - val_MinusLogProbMetric: 29.7090 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 279/1000
2023-10-26 04:54:35.212 
Epoch 279/1000 
	 loss: 28.8825, MinusLogProbMetric: 28.8825, val_loss: 29.5517, val_MinusLogProbMetric: 29.5517

Epoch 279: val_loss did not improve from 29.16271
196/196 - 39s - loss: 28.8825 - MinusLogProbMetric: 28.8825 - val_loss: 29.5517 - val_MinusLogProbMetric: 29.5517 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 280/1000
2023-10-26 04:55:10.933 
Epoch 280/1000 
	 loss: 28.9325, MinusLogProbMetric: 28.9325, val_loss: 29.8925, val_MinusLogProbMetric: 29.8925

Epoch 280: val_loss did not improve from 29.16271
196/196 - 36s - loss: 28.9325 - MinusLogProbMetric: 28.9325 - val_loss: 29.8925 - val_MinusLogProbMetric: 29.8925 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 281/1000
2023-10-26 04:55:46.128 
Epoch 281/1000 
	 loss: 29.0326, MinusLogProbMetric: 29.0326, val_loss: 29.4883, val_MinusLogProbMetric: 29.4883

Epoch 281: val_loss did not improve from 29.16271
196/196 - 35s - loss: 29.0326 - MinusLogProbMetric: 29.0326 - val_loss: 29.4883 - val_MinusLogProbMetric: 29.4883 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 282/1000
2023-10-26 04:56:21.872 
Epoch 282/1000 
	 loss: 28.9430, MinusLogProbMetric: 28.9430, val_loss: 29.3706, val_MinusLogProbMetric: 29.3706

Epoch 282: val_loss did not improve from 29.16271
196/196 - 36s - loss: 28.9430 - MinusLogProbMetric: 28.9430 - val_loss: 29.3706 - val_MinusLogProbMetric: 29.3706 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 283/1000
2023-10-26 04:57:01.385 
Epoch 283/1000 
	 loss: 28.9414, MinusLogProbMetric: 28.9414, val_loss: 29.3070, val_MinusLogProbMetric: 29.3070

Epoch 283: val_loss did not improve from 29.16271
196/196 - 40s - loss: 28.9414 - MinusLogProbMetric: 28.9414 - val_loss: 29.3070 - val_MinusLogProbMetric: 29.3070 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 284/1000
2023-10-26 04:57:41.805 
Epoch 284/1000 
	 loss: 28.8606, MinusLogProbMetric: 28.8606, val_loss: 29.1562, val_MinusLogProbMetric: 29.1562

Epoch 284: val_loss improved from 29.16271 to 29.15617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 41s - loss: 28.8606 - MinusLogProbMetric: 28.8606 - val_loss: 29.1562 - val_MinusLogProbMetric: 29.1562 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 285/1000
2023-10-26 04:58:23.316 
Epoch 285/1000 
	 loss: 28.8751, MinusLogProbMetric: 28.8751, val_loss: 29.9714, val_MinusLogProbMetric: 29.9714

Epoch 285: val_loss did not improve from 29.15617
196/196 - 41s - loss: 28.8751 - MinusLogProbMetric: 28.8751 - val_loss: 29.9714 - val_MinusLogProbMetric: 29.9714 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 286/1000
2023-10-26 04:59:01.750 
Epoch 286/1000 
	 loss: 28.9041, MinusLogProbMetric: 28.9041, val_loss: 29.5053, val_MinusLogProbMetric: 29.5053

Epoch 286: val_loss did not improve from 29.15617
196/196 - 38s - loss: 28.9041 - MinusLogProbMetric: 28.9041 - val_loss: 29.5053 - val_MinusLogProbMetric: 29.5053 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 287/1000
2023-10-26 04:59:40.362 
Epoch 287/1000 
	 loss: 28.9258, MinusLogProbMetric: 28.9258, val_loss: 29.3064, val_MinusLogProbMetric: 29.3064

Epoch 287: val_loss did not improve from 29.15617
196/196 - 39s - loss: 28.9258 - MinusLogProbMetric: 28.9258 - val_loss: 29.3064 - val_MinusLogProbMetric: 29.3064 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 288/1000
2023-10-26 05:00:17.020 
Epoch 288/1000 
	 loss: 28.9052, MinusLogProbMetric: 28.9052, val_loss: 29.5579, val_MinusLogProbMetric: 29.5579

Epoch 288: val_loss did not improve from 29.15617
196/196 - 37s - loss: 28.9052 - MinusLogProbMetric: 28.9052 - val_loss: 29.5579 - val_MinusLogProbMetric: 29.5579 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 289/1000
2023-10-26 05:00:55.414 
Epoch 289/1000 
	 loss: 28.9243, MinusLogProbMetric: 28.9243, val_loss: 29.5101, val_MinusLogProbMetric: 29.5101

Epoch 289: val_loss did not improve from 29.15617
196/196 - 38s - loss: 28.9243 - MinusLogProbMetric: 28.9243 - val_loss: 29.5101 - val_MinusLogProbMetric: 29.5101 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 290/1000
2023-10-26 05:01:36.112 
Epoch 290/1000 
	 loss: 28.9217, MinusLogProbMetric: 28.9217, val_loss: 29.2814, val_MinusLogProbMetric: 29.2814

Epoch 290: val_loss did not improve from 29.15617
196/196 - 41s - loss: 28.9217 - MinusLogProbMetric: 28.9217 - val_loss: 29.2814 - val_MinusLogProbMetric: 29.2814 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 291/1000
2023-10-26 05:02:13.415 
Epoch 291/1000 
	 loss: 28.8347, MinusLogProbMetric: 28.8347, val_loss: 29.6953, val_MinusLogProbMetric: 29.6953

Epoch 291: val_loss did not improve from 29.15617
196/196 - 37s - loss: 28.8347 - MinusLogProbMetric: 28.8347 - val_loss: 29.6953 - val_MinusLogProbMetric: 29.6953 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 292/1000
2023-10-26 05:02:50.273 
Epoch 292/1000 
	 loss: 28.8667, MinusLogProbMetric: 28.8667, val_loss: 29.6743, val_MinusLogProbMetric: 29.6743

Epoch 292: val_loss did not improve from 29.15617
196/196 - 37s - loss: 28.8667 - MinusLogProbMetric: 28.8667 - val_loss: 29.6743 - val_MinusLogProbMetric: 29.6743 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 293/1000
2023-10-26 05:03:26.897 
Epoch 293/1000 
	 loss: 28.9833, MinusLogProbMetric: 28.9833, val_loss: 29.4724, val_MinusLogProbMetric: 29.4724

Epoch 293: val_loss did not improve from 29.15617
196/196 - 37s - loss: 28.9833 - MinusLogProbMetric: 28.9833 - val_loss: 29.4724 - val_MinusLogProbMetric: 29.4724 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 294/1000
2023-10-26 05:04:06.660 
Epoch 294/1000 
	 loss: 28.9073, MinusLogProbMetric: 28.9073, val_loss: 29.3884, val_MinusLogProbMetric: 29.3884

Epoch 294: val_loss did not improve from 29.15617
196/196 - 40s - loss: 28.9073 - MinusLogProbMetric: 28.9073 - val_loss: 29.3884 - val_MinusLogProbMetric: 29.3884 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 295/1000
2023-10-26 05:04:46.863 
Epoch 295/1000 
	 loss: 28.8307, MinusLogProbMetric: 28.8307, val_loss: 29.5495, val_MinusLogProbMetric: 29.5495

Epoch 295: val_loss did not improve from 29.15617
196/196 - 40s - loss: 28.8307 - MinusLogProbMetric: 28.8307 - val_loss: 29.5495 - val_MinusLogProbMetric: 29.5495 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 296/1000
2023-10-26 05:05:27.035 
Epoch 296/1000 
	 loss: 28.8770, MinusLogProbMetric: 28.8770, val_loss: 29.0723, val_MinusLogProbMetric: 29.0723

Epoch 296: val_loss improved from 29.15617 to 29.07234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 41s - loss: 28.8770 - MinusLogProbMetric: 28.8770 - val_loss: 29.0723 - val_MinusLogProbMetric: 29.0723 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 297/1000
2023-10-26 05:06:05.140 
Epoch 297/1000 
	 loss: 28.9193, MinusLogProbMetric: 28.9193, val_loss: 29.6697, val_MinusLogProbMetric: 29.6697

Epoch 297: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.9193 - MinusLogProbMetric: 28.9193 - val_loss: 29.6697 - val_MinusLogProbMetric: 29.6697 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 298/1000
2023-10-26 05:06:41.847 
Epoch 298/1000 
	 loss: 28.8283, MinusLogProbMetric: 28.8283, val_loss: 29.2765, val_MinusLogProbMetric: 29.2765

Epoch 298: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.8283 - MinusLogProbMetric: 28.8283 - val_loss: 29.2765 - val_MinusLogProbMetric: 29.2765 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 299/1000
2023-10-26 05:07:23.246 
Epoch 299/1000 
	 loss: 28.7789, MinusLogProbMetric: 28.7789, val_loss: 29.5941, val_MinusLogProbMetric: 29.5941

Epoch 299: val_loss did not improve from 29.07234
196/196 - 41s - loss: 28.7789 - MinusLogProbMetric: 28.7789 - val_loss: 29.5941 - val_MinusLogProbMetric: 29.5941 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 300/1000
2023-10-26 05:08:04.440 
Epoch 300/1000 
	 loss: 28.7700, MinusLogProbMetric: 28.7700, val_loss: 29.3437, val_MinusLogProbMetric: 29.3437

Epoch 300: val_loss did not improve from 29.07234
196/196 - 41s - loss: 28.7700 - MinusLogProbMetric: 28.7700 - val_loss: 29.3437 - val_MinusLogProbMetric: 29.3437 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 301/1000
2023-10-26 05:08:42.889 
Epoch 301/1000 
	 loss: 28.7837, MinusLogProbMetric: 28.7837, val_loss: 29.9536, val_MinusLogProbMetric: 29.9536

Epoch 301: val_loss did not improve from 29.07234
196/196 - 38s - loss: 28.7837 - MinusLogProbMetric: 28.7837 - val_loss: 29.9536 - val_MinusLogProbMetric: 29.9536 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 302/1000
2023-10-26 05:09:20.175 
Epoch 302/1000 
	 loss: 28.8209, MinusLogProbMetric: 28.8209, val_loss: 29.4148, val_MinusLogProbMetric: 29.4148

Epoch 302: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.8209 - MinusLogProbMetric: 28.8209 - val_loss: 29.4148 - val_MinusLogProbMetric: 29.4148 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 303/1000
2023-10-26 05:09:56.681 
Epoch 303/1000 
	 loss: 28.7515, MinusLogProbMetric: 28.7515, val_loss: 29.1830, val_MinusLogProbMetric: 29.1830

Epoch 303: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.7515 - MinusLogProbMetric: 28.7515 - val_loss: 29.1830 - val_MinusLogProbMetric: 29.1830 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 304/1000
2023-10-26 05:10:36.108 
Epoch 304/1000 
	 loss: 28.8391, MinusLogProbMetric: 28.8391, val_loss: 29.6444, val_MinusLogProbMetric: 29.6444

Epoch 304: val_loss did not improve from 29.07234
196/196 - 39s - loss: 28.8391 - MinusLogProbMetric: 28.8391 - val_loss: 29.6444 - val_MinusLogProbMetric: 29.6444 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 305/1000
2023-10-26 05:11:17.905 
Epoch 305/1000 
	 loss: 28.7939, MinusLogProbMetric: 28.7939, val_loss: 29.1717, val_MinusLogProbMetric: 29.1717

Epoch 305: val_loss did not improve from 29.07234
196/196 - 42s - loss: 28.7939 - MinusLogProbMetric: 28.7939 - val_loss: 29.1717 - val_MinusLogProbMetric: 29.1717 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 306/1000
2023-10-26 05:11:55.076 
Epoch 306/1000 
	 loss: 28.7802, MinusLogProbMetric: 28.7802, val_loss: 29.6089, val_MinusLogProbMetric: 29.6089

Epoch 306: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.7802 - MinusLogProbMetric: 28.7802 - val_loss: 29.6089 - val_MinusLogProbMetric: 29.6089 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 307/1000
2023-10-26 05:12:31.950 
Epoch 307/1000 
	 loss: 28.8323, MinusLogProbMetric: 28.8323, val_loss: 29.4255, val_MinusLogProbMetric: 29.4255

Epoch 307: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.8323 - MinusLogProbMetric: 28.8323 - val_loss: 29.4255 - val_MinusLogProbMetric: 29.4255 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 308/1000
2023-10-26 05:13:08.870 
Epoch 308/1000 
	 loss: 28.7770, MinusLogProbMetric: 28.7770, val_loss: 29.1878, val_MinusLogProbMetric: 29.1878

Epoch 308: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.7770 - MinusLogProbMetric: 28.7770 - val_loss: 29.1878 - val_MinusLogProbMetric: 29.1878 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 309/1000
2023-10-26 05:13:48.766 
Epoch 309/1000 
	 loss: 28.8566, MinusLogProbMetric: 28.8566, val_loss: 29.3436, val_MinusLogProbMetric: 29.3436

Epoch 309: val_loss did not improve from 29.07234
196/196 - 40s - loss: 28.8566 - MinusLogProbMetric: 28.8566 - val_loss: 29.3436 - val_MinusLogProbMetric: 29.3436 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 310/1000
2023-10-26 05:14:31.271 
Epoch 310/1000 
	 loss: 28.8104, MinusLogProbMetric: 28.8104, val_loss: 29.7100, val_MinusLogProbMetric: 29.7100

Epoch 310: val_loss did not improve from 29.07234
196/196 - 43s - loss: 28.8104 - MinusLogProbMetric: 28.8104 - val_loss: 29.7100 - val_MinusLogProbMetric: 29.7100 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 311/1000
2023-10-26 05:15:10.738 
Epoch 311/1000 
	 loss: 28.7293, MinusLogProbMetric: 28.7293, val_loss: 29.2204, val_MinusLogProbMetric: 29.2204

Epoch 311: val_loss did not improve from 29.07234
196/196 - 39s - loss: 28.7293 - MinusLogProbMetric: 28.7293 - val_loss: 29.2204 - val_MinusLogProbMetric: 29.2204 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 312/1000
2023-10-26 05:15:47.506 
Epoch 312/1000 
	 loss: 28.7976, MinusLogProbMetric: 28.7976, val_loss: 29.2804, val_MinusLogProbMetric: 29.2804

Epoch 312: val_loss did not improve from 29.07234
196/196 - 37s - loss: 28.7976 - MinusLogProbMetric: 28.7976 - val_loss: 29.2804 - val_MinusLogProbMetric: 29.2804 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 313/1000
2023-10-26 05:16:24.465 
Epoch 313/1000 
	 loss: 28.7592, MinusLogProbMetric: 28.7592, val_loss: 29.0119, val_MinusLogProbMetric: 29.0119

Epoch 313: val_loss improved from 29.07234 to 29.01191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 38s - loss: 28.7592 - MinusLogProbMetric: 28.7592 - val_loss: 29.0119 - val_MinusLogProbMetric: 29.0119 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 314/1000
2023-10-26 05:17:03.768 
Epoch 314/1000 
	 loss: 28.7984, MinusLogProbMetric: 28.7984, val_loss: 29.5705, val_MinusLogProbMetric: 29.5705

Epoch 314: val_loss did not improve from 29.01191
196/196 - 39s - loss: 28.7984 - MinusLogProbMetric: 28.7984 - val_loss: 29.5705 - val_MinusLogProbMetric: 29.5705 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 315/1000
2023-10-26 05:17:46.289 
Epoch 315/1000 
	 loss: 28.7224, MinusLogProbMetric: 28.7224, val_loss: 29.4634, val_MinusLogProbMetric: 29.4634

Epoch 315: val_loss did not improve from 29.01191
196/196 - 43s - loss: 28.7224 - MinusLogProbMetric: 28.7224 - val_loss: 29.4634 - val_MinusLogProbMetric: 29.4634 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 316/1000
2023-10-26 05:18:24.330 
Epoch 316/1000 
	 loss: 28.8097, MinusLogProbMetric: 28.8097, val_loss: 29.5074, val_MinusLogProbMetric: 29.5074

Epoch 316: val_loss did not improve from 29.01191
196/196 - 38s - loss: 28.8097 - MinusLogProbMetric: 28.8097 - val_loss: 29.5074 - val_MinusLogProbMetric: 29.5074 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 317/1000
2023-10-26 05:19:00.560 
Epoch 317/1000 
	 loss: 28.7997, MinusLogProbMetric: 28.7997, val_loss: 29.6511, val_MinusLogProbMetric: 29.6511

Epoch 317: val_loss did not improve from 29.01191
196/196 - 36s - loss: 28.7997 - MinusLogProbMetric: 28.7997 - val_loss: 29.6511 - val_MinusLogProbMetric: 29.6511 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 318/1000
2023-10-26 05:19:37.085 
Epoch 318/1000 
	 loss: 28.7897, MinusLogProbMetric: 28.7897, val_loss: 29.2508, val_MinusLogProbMetric: 29.2508

Epoch 318: val_loss did not improve from 29.01191
196/196 - 37s - loss: 28.7897 - MinusLogProbMetric: 28.7897 - val_loss: 29.2508 - val_MinusLogProbMetric: 29.2508 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 319/1000
2023-10-26 05:20:15.068 
Epoch 319/1000 
	 loss: 28.7518, MinusLogProbMetric: 28.7518, val_loss: 29.1098, val_MinusLogProbMetric: 29.1098

Epoch 319: val_loss did not improve from 29.01191
196/196 - 38s - loss: 28.7518 - MinusLogProbMetric: 28.7518 - val_loss: 29.1098 - val_MinusLogProbMetric: 29.1098 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 320/1000
2023-10-26 05:20:57.331 
Epoch 320/1000 
	 loss: 28.7307, MinusLogProbMetric: 28.7307, val_loss: 29.4316, val_MinusLogProbMetric: 29.4316

Epoch 320: val_loss did not improve from 29.01191
196/196 - 42s - loss: 28.7307 - MinusLogProbMetric: 28.7307 - val_loss: 29.4316 - val_MinusLogProbMetric: 29.4316 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 321/1000
2023-10-26 05:21:38.813 
Epoch 321/1000 
	 loss: 28.7153, MinusLogProbMetric: 28.7153, val_loss: 29.1382, val_MinusLogProbMetric: 29.1382

Epoch 321: val_loss did not improve from 29.01191
196/196 - 41s - loss: 28.7153 - MinusLogProbMetric: 28.7153 - val_loss: 29.1382 - val_MinusLogProbMetric: 29.1382 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 322/1000
2023-10-26 05:22:15.527 
Epoch 322/1000 
	 loss: 28.6455, MinusLogProbMetric: 28.6455, val_loss: 29.1635, val_MinusLogProbMetric: 29.1635

Epoch 322: val_loss did not improve from 29.01191
196/196 - 37s - loss: 28.6455 - MinusLogProbMetric: 28.6455 - val_loss: 29.1635 - val_MinusLogProbMetric: 29.1635 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 323/1000
2023-10-26 05:22:52.691 
Epoch 323/1000 
	 loss: 28.7897, MinusLogProbMetric: 28.7897, val_loss: 29.8768, val_MinusLogProbMetric: 29.8768

Epoch 323: val_loss did not improve from 29.01191
196/196 - 37s - loss: 28.7897 - MinusLogProbMetric: 28.7897 - val_loss: 29.8768 - val_MinusLogProbMetric: 29.8768 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 324/1000
2023-10-26 05:23:29.122 
Epoch 324/1000 
	 loss: 28.7674, MinusLogProbMetric: 28.7674, val_loss: 29.4186, val_MinusLogProbMetric: 29.4186

Epoch 324: val_loss did not improve from 29.01191
196/196 - 36s - loss: 28.7674 - MinusLogProbMetric: 28.7674 - val_loss: 29.4186 - val_MinusLogProbMetric: 29.4186 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 325/1000
2023-10-26 05:24:11.006 
Epoch 325/1000 
	 loss: 28.7125, MinusLogProbMetric: 28.7125, val_loss: 29.2485, val_MinusLogProbMetric: 29.2485

Epoch 325: val_loss did not improve from 29.01191
196/196 - 42s - loss: 28.7125 - MinusLogProbMetric: 28.7125 - val_loss: 29.2485 - val_MinusLogProbMetric: 29.2485 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 326/1000
2023-10-26 05:24:51.528 
Epoch 326/1000 
	 loss: 28.7415, MinusLogProbMetric: 28.7415, val_loss: 29.5066, val_MinusLogProbMetric: 29.5066

Epoch 326: val_loss did not improve from 29.01191
196/196 - 41s - loss: 28.7415 - MinusLogProbMetric: 28.7415 - val_loss: 29.5066 - val_MinusLogProbMetric: 29.5066 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 327/1000
2023-10-26 05:25:29.312 
Epoch 327/1000 
	 loss: 28.7143, MinusLogProbMetric: 28.7143, val_loss: 29.1164, val_MinusLogProbMetric: 29.1164

Epoch 327: val_loss did not improve from 29.01191
196/196 - 38s - loss: 28.7143 - MinusLogProbMetric: 28.7143 - val_loss: 29.1164 - val_MinusLogProbMetric: 29.1164 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 328/1000
2023-10-26 05:26:05.713 
Epoch 328/1000 
	 loss: 28.7248, MinusLogProbMetric: 28.7248, val_loss: 28.9817, val_MinusLogProbMetric: 28.9817

Epoch 328: val_loss improved from 29.01191 to 28.98171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 37s - loss: 28.7248 - MinusLogProbMetric: 28.7248 - val_loss: 28.9817 - val_MinusLogProbMetric: 28.9817 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 329/1000
2023-10-26 05:26:42.155 
Epoch 329/1000 
	 loss: 28.6357, MinusLogProbMetric: 28.6357, val_loss: 29.2865, val_MinusLogProbMetric: 29.2865

Epoch 329: val_loss did not improve from 28.98171
196/196 - 36s - loss: 28.6357 - MinusLogProbMetric: 28.6357 - val_loss: 29.2865 - val_MinusLogProbMetric: 29.2865 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 330/1000
2023-10-26 05:27:16.477 
Epoch 330/1000 
	 loss: 28.7738, MinusLogProbMetric: 28.7738, val_loss: 29.5319, val_MinusLogProbMetric: 29.5319

Epoch 330: val_loss did not improve from 28.98171
196/196 - 34s - loss: 28.7738 - MinusLogProbMetric: 28.7738 - val_loss: 29.5319 - val_MinusLogProbMetric: 29.5319 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 331/1000
2023-10-26 05:27:50.720 
Epoch 331/1000 
	 loss: 28.7309, MinusLogProbMetric: 28.7309, val_loss: 29.0191, val_MinusLogProbMetric: 29.0191

Epoch 331: val_loss did not improve from 28.98171
196/196 - 34s - loss: 28.7309 - MinusLogProbMetric: 28.7309 - val_loss: 29.0191 - val_MinusLogProbMetric: 29.0191 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 332/1000
2023-10-26 05:28:26.261 
Epoch 332/1000 
	 loss: 28.7314, MinusLogProbMetric: 28.7314, val_loss: 29.2764, val_MinusLogProbMetric: 29.2764

Epoch 332: val_loss did not improve from 28.98171
196/196 - 36s - loss: 28.7314 - MinusLogProbMetric: 28.7314 - val_loss: 29.2764 - val_MinusLogProbMetric: 29.2764 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 333/1000
2023-10-26 05:29:05.868 
Epoch 333/1000 
	 loss: 28.6600, MinusLogProbMetric: 28.6600, val_loss: 29.0134, val_MinusLogProbMetric: 29.0134

Epoch 333: val_loss did not improve from 28.98171
196/196 - 40s - loss: 28.6600 - MinusLogProbMetric: 28.6600 - val_loss: 29.0134 - val_MinusLogProbMetric: 29.0134 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 334/1000
2023-10-26 05:29:40.659 
Epoch 334/1000 
	 loss: 28.7499, MinusLogProbMetric: 28.7499, val_loss: 29.1332, val_MinusLogProbMetric: 29.1332

Epoch 334: val_loss did not improve from 28.98171
196/196 - 35s - loss: 28.7499 - MinusLogProbMetric: 28.7499 - val_loss: 29.1332 - val_MinusLogProbMetric: 29.1332 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 335/1000
2023-10-26 05:30:14.750 
Epoch 335/1000 
	 loss: 28.6546, MinusLogProbMetric: 28.6546, val_loss: 29.3220, val_MinusLogProbMetric: 29.3220

Epoch 335: val_loss did not improve from 28.98171
196/196 - 34s - loss: 28.6546 - MinusLogProbMetric: 28.6546 - val_loss: 29.3220 - val_MinusLogProbMetric: 29.3220 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 336/1000
2023-10-26 05:30:48.701 
Epoch 336/1000 
	 loss: 28.7268, MinusLogProbMetric: 28.7268, val_loss: 29.3253, val_MinusLogProbMetric: 29.3253

Epoch 336: val_loss did not improve from 28.98171
196/196 - 34s - loss: 28.7268 - MinusLogProbMetric: 28.7268 - val_loss: 29.3253 - val_MinusLogProbMetric: 29.3253 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 337/1000
2023-10-26 05:31:25.196 
Epoch 337/1000 
	 loss: 28.6726, MinusLogProbMetric: 28.6726, val_loss: 29.3627, val_MinusLogProbMetric: 29.3627

Epoch 337: val_loss did not improve from 28.98171
196/196 - 36s - loss: 28.6726 - MinusLogProbMetric: 28.6726 - val_loss: 29.3627 - val_MinusLogProbMetric: 29.3627 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 338/1000
2023-10-26 05:32:02.205 
Epoch 338/1000 
	 loss: 28.6539, MinusLogProbMetric: 28.6539, val_loss: 28.9650, val_MinusLogProbMetric: 28.9650

Epoch 338: val_loss improved from 28.98171 to 28.96502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 38s - loss: 28.6539 - MinusLogProbMetric: 28.6539 - val_loss: 28.9650 - val_MinusLogProbMetric: 28.9650 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 339/1000
2023-10-26 05:32:44.444 
Epoch 339/1000 
	 loss: 28.6684, MinusLogProbMetric: 28.6684, val_loss: 29.3964, val_MinusLogProbMetric: 29.3964

Epoch 339: val_loss did not improve from 28.96502
196/196 - 42s - loss: 28.6684 - MinusLogProbMetric: 28.6684 - val_loss: 29.3964 - val_MinusLogProbMetric: 29.3964 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 340/1000
2023-10-26 05:33:18.387 
Epoch 340/1000 
	 loss: 28.6961, MinusLogProbMetric: 28.6961, val_loss: 29.4047, val_MinusLogProbMetric: 29.4047

Epoch 340: val_loss did not improve from 28.96502
196/196 - 34s - loss: 28.6961 - MinusLogProbMetric: 28.6961 - val_loss: 29.4047 - val_MinusLogProbMetric: 29.4047 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 341/1000
2023-10-26 05:33:52.827 
Epoch 341/1000 
	 loss: 28.6966, MinusLogProbMetric: 28.6966, val_loss: 29.1084, val_MinusLogProbMetric: 29.1084

Epoch 341: val_loss did not improve from 28.96502
196/196 - 34s - loss: 28.6966 - MinusLogProbMetric: 28.6966 - val_loss: 29.1084 - val_MinusLogProbMetric: 29.1084 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 342/1000
2023-10-26 05:34:26.742 
Epoch 342/1000 
	 loss: 28.7360, MinusLogProbMetric: 28.7360, val_loss: 29.7313, val_MinusLogProbMetric: 29.7313

Epoch 342: val_loss did not improve from 28.96502
196/196 - 34s - loss: 28.7360 - MinusLogProbMetric: 28.7360 - val_loss: 29.7313 - val_MinusLogProbMetric: 29.7313 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 343/1000
2023-10-26 05:35:01.410 
Epoch 343/1000 
	 loss: 28.6311, MinusLogProbMetric: 28.6311, val_loss: 30.2821, val_MinusLogProbMetric: 30.2821

Epoch 343: val_loss did not improve from 28.96502
196/196 - 35s - loss: 28.6311 - MinusLogProbMetric: 28.6311 - val_loss: 30.2821 - val_MinusLogProbMetric: 30.2821 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 344/1000
2023-10-26 05:35:37.251 
Epoch 344/1000 
	 loss: 28.7066, MinusLogProbMetric: 28.7066, val_loss: 28.9551, val_MinusLogProbMetric: 28.9551

Epoch 344: val_loss improved from 28.96502 to 28.95510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 37s - loss: 28.7066 - MinusLogProbMetric: 28.7066 - val_loss: 28.9551 - val_MinusLogProbMetric: 28.9551 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 345/1000
2023-10-26 05:36:19.191 
Epoch 345/1000 
	 loss: 28.6693, MinusLogProbMetric: 28.6693, val_loss: 29.0075, val_MinusLogProbMetric: 29.0075

Epoch 345: val_loss did not improve from 28.95510
196/196 - 41s - loss: 28.6693 - MinusLogProbMetric: 28.6693 - val_loss: 29.0075 - val_MinusLogProbMetric: 29.0075 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 346/1000
2023-10-26 05:36:57.111 
Epoch 346/1000 
	 loss: 28.6231, MinusLogProbMetric: 28.6231, val_loss: 29.2552, val_MinusLogProbMetric: 29.2552

Epoch 346: val_loss did not improve from 28.95510
196/196 - 38s - loss: 28.6231 - MinusLogProbMetric: 28.6231 - val_loss: 29.2552 - val_MinusLogProbMetric: 29.2552 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 347/1000
2023-10-26 05:37:35.795 
Epoch 347/1000 
	 loss: 28.5900, MinusLogProbMetric: 28.5900, val_loss: 29.2605, val_MinusLogProbMetric: 29.2605

Epoch 347: val_loss did not improve from 28.95510
196/196 - 39s - loss: 28.5900 - MinusLogProbMetric: 28.5900 - val_loss: 29.2605 - val_MinusLogProbMetric: 29.2605 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 348/1000
2023-10-26 05:38:10.494 
Epoch 348/1000 
	 loss: 28.6971, MinusLogProbMetric: 28.6971, val_loss: 29.1280, val_MinusLogProbMetric: 29.1280

Epoch 348: val_loss did not improve from 28.95510
196/196 - 35s - loss: 28.6971 - MinusLogProbMetric: 28.6971 - val_loss: 29.1280 - val_MinusLogProbMetric: 29.1280 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 349/1000
2023-10-26 05:38:44.333 
Epoch 349/1000 
	 loss: 28.6577, MinusLogProbMetric: 28.6577, val_loss: 29.8605, val_MinusLogProbMetric: 29.8605

Epoch 349: val_loss did not improve from 28.95510
196/196 - 34s - loss: 28.6577 - MinusLogProbMetric: 28.6577 - val_loss: 29.8605 - val_MinusLogProbMetric: 29.8605 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 350/1000
2023-10-26 05:39:18.056 
Epoch 350/1000 
	 loss: 28.7375, MinusLogProbMetric: 28.7375, val_loss: 29.2101, val_MinusLogProbMetric: 29.2101

Epoch 350: val_loss did not improve from 28.95510
196/196 - 34s - loss: 28.7375 - MinusLogProbMetric: 28.7375 - val_loss: 29.2101 - val_MinusLogProbMetric: 29.2101 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 351/1000
2023-10-26 05:39:53.124 
Epoch 351/1000 
	 loss: 28.6633, MinusLogProbMetric: 28.6633, val_loss: 29.1160, val_MinusLogProbMetric: 29.1160

Epoch 351: val_loss did not improve from 28.95510
196/196 - 35s - loss: 28.6633 - MinusLogProbMetric: 28.6633 - val_loss: 29.1160 - val_MinusLogProbMetric: 29.1160 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 352/1000
2023-10-26 05:40:30.016 
Epoch 352/1000 
	 loss: 28.6415, MinusLogProbMetric: 28.6415, val_loss: 29.0983, val_MinusLogProbMetric: 29.0983

Epoch 352: val_loss did not improve from 28.95510
196/196 - 37s - loss: 28.6415 - MinusLogProbMetric: 28.6415 - val_loss: 29.0983 - val_MinusLogProbMetric: 29.0983 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 353/1000
2023-10-26 05:41:03.814 
Epoch 353/1000 
	 loss: 28.5491, MinusLogProbMetric: 28.5491, val_loss: 28.9144, val_MinusLogProbMetric: 28.9144

Epoch 353: val_loss improved from 28.95510 to 28.91437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 34s - loss: 28.5491 - MinusLogProbMetric: 28.5491 - val_loss: 28.9144 - val_MinusLogProbMetric: 28.9144 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 354/1000
2023-10-26 05:41:37.805 
Epoch 354/1000 
	 loss: 28.6101, MinusLogProbMetric: 28.6101, val_loss: 29.0089, val_MinusLogProbMetric: 29.0089

Epoch 354: val_loss did not improve from 28.91437
196/196 - 33s - loss: 28.6101 - MinusLogProbMetric: 28.6101 - val_loss: 29.0089 - val_MinusLogProbMetric: 29.0089 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 355/1000
2023-10-26 05:42:11.589 
Epoch 355/1000 
	 loss: 28.6509, MinusLogProbMetric: 28.6509, val_loss: 29.0294, val_MinusLogProbMetric: 29.0294

Epoch 355: val_loss did not improve from 28.91437
196/196 - 34s - loss: 28.6509 - MinusLogProbMetric: 28.6509 - val_loss: 29.0294 - val_MinusLogProbMetric: 29.0294 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 356/1000
2023-10-26 05:42:45.452 
Epoch 356/1000 
	 loss: 28.5425, MinusLogProbMetric: 28.5425, val_loss: 29.6148, val_MinusLogProbMetric: 29.6148

Epoch 356: val_loss did not improve from 28.91437
196/196 - 34s - loss: 28.5425 - MinusLogProbMetric: 28.5425 - val_loss: 29.6148 - val_MinusLogProbMetric: 29.6148 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 357/1000
2023-10-26 05:43:26.440 
Epoch 357/1000 
	 loss: 28.5922, MinusLogProbMetric: 28.5922, val_loss: 29.4970, val_MinusLogProbMetric: 29.4970

Epoch 357: val_loss did not improve from 28.91437
196/196 - 41s - loss: 28.5922 - MinusLogProbMetric: 28.5922 - val_loss: 29.4970 - val_MinusLogProbMetric: 29.4970 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 358/1000
2023-10-26 05:44:08.950 
Epoch 358/1000 
	 loss: 28.6054, MinusLogProbMetric: 28.6054, val_loss: 30.6699, val_MinusLogProbMetric: 30.6699

Epoch 358: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6054 - MinusLogProbMetric: 28.6054 - val_loss: 30.6699 - val_MinusLogProbMetric: 30.6699 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 359/1000
2023-10-26 05:44:49.642 
Epoch 359/1000 
	 loss: 28.6438, MinusLogProbMetric: 28.6438, val_loss: 29.0801, val_MinusLogProbMetric: 29.0801

Epoch 359: val_loss did not improve from 28.91437
196/196 - 41s - loss: 28.6438 - MinusLogProbMetric: 28.6438 - val_loss: 29.0801 - val_MinusLogProbMetric: 29.0801 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 360/1000
2023-10-26 05:45:32.258 
Epoch 360/1000 
	 loss: 28.6325, MinusLogProbMetric: 28.6325, val_loss: 29.1468, val_MinusLogProbMetric: 29.1468

Epoch 360: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6325 - MinusLogProbMetric: 28.6325 - val_loss: 29.1468 - val_MinusLogProbMetric: 29.1468 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 361/1000
2023-10-26 05:46:14.907 
Epoch 361/1000 
	 loss: 28.6250, MinusLogProbMetric: 28.6250, val_loss: 29.5083, val_MinusLogProbMetric: 29.5083

Epoch 361: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6250 - MinusLogProbMetric: 28.6250 - val_loss: 29.5083 - val_MinusLogProbMetric: 29.5083 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 362/1000
2023-10-26 05:46:57.909 
Epoch 362/1000 
	 loss: 28.5741, MinusLogProbMetric: 28.5741, val_loss: 29.0987, val_MinusLogProbMetric: 29.0987

Epoch 362: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5741 - MinusLogProbMetric: 28.5741 - val_loss: 29.0987 - val_MinusLogProbMetric: 29.0987 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 363/1000
2023-10-26 05:47:40.578 
Epoch 363/1000 
	 loss: 28.6661, MinusLogProbMetric: 28.6661, val_loss: 29.3525, val_MinusLogProbMetric: 29.3525

Epoch 363: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6661 - MinusLogProbMetric: 28.6661 - val_loss: 29.3525 - val_MinusLogProbMetric: 29.3525 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 364/1000
2023-10-26 05:48:23.202 
Epoch 364/1000 
	 loss: 28.5438, MinusLogProbMetric: 28.5438, val_loss: 29.6213, val_MinusLogProbMetric: 29.6213

Epoch 364: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5438 - MinusLogProbMetric: 28.5438 - val_loss: 29.6213 - val_MinusLogProbMetric: 29.6213 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 365/1000
2023-10-26 05:49:05.972 
Epoch 365/1000 
	 loss: 28.6905, MinusLogProbMetric: 28.6905, val_loss: 29.6157, val_MinusLogProbMetric: 29.6157

Epoch 365: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6905 - MinusLogProbMetric: 28.6905 - val_loss: 29.6157 - val_MinusLogProbMetric: 29.6157 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 366/1000
2023-10-26 05:49:48.805 
Epoch 366/1000 
	 loss: 28.5983, MinusLogProbMetric: 28.5983, val_loss: 29.3448, val_MinusLogProbMetric: 29.3448

Epoch 366: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5983 - MinusLogProbMetric: 28.5983 - val_loss: 29.3448 - val_MinusLogProbMetric: 29.3448 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 367/1000
2023-10-26 05:50:31.604 
Epoch 367/1000 
	 loss: 28.5328, MinusLogProbMetric: 28.5328, val_loss: 29.2529, val_MinusLogProbMetric: 29.2529

Epoch 367: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5328 - MinusLogProbMetric: 28.5328 - val_loss: 29.2529 - val_MinusLogProbMetric: 29.2529 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 368/1000
2023-10-26 05:51:14.644 
Epoch 368/1000 
	 loss: 28.5719, MinusLogProbMetric: 28.5719, val_loss: 29.1348, val_MinusLogProbMetric: 29.1348

Epoch 368: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5719 - MinusLogProbMetric: 28.5719 - val_loss: 29.1348 - val_MinusLogProbMetric: 29.1348 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 369/1000
2023-10-26 05:51:57.333 
Epoch 369/1000 
	 loss: 28.6022, MinusLogProbMetric: 28.6022, val_loss: 29.2536, val_MinusLogProbMetric: 29.2536

Epoch 369: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6022 - MinusLogProbMetric: 28.6022 - val_loss: 29.2536 - val_MinusLogProbMetric: 29.2536 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 370/1000
2023-10-26 05:52:39.986 
Epoch 370/1000 
	 loss: 28.6202, MinusLogProbMetric: 28.6202, val_loss: 29.4519, val_MinusLogProbMetric: 29.4519

Epoch 370: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6202 - MinusLogProbMetric: 28.6202 - val_loss: 29.4519 - val_MinusLogProbMetric: 29.4519 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 371/1000
2023-10-26 05:53:22.859 
Epoch 371/1000 
	 loss: 28.6054, MinusLogProbMetric: 28.6054, val_loss: 29.0153, val_MinusLogProbMetric: 29.0153

Epoch 371: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6054 - MinusLogProbMetric: 28.6054 - val_loss: 29.0153 - val_MinusLogProbMetric: 29.0153 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 372/1000
2023-10-26 05:54:05.775 
Epoch 372/1000 
	 loss: 28.5628, MinusLogProbMetric: 28.5628, val_loss: 29.0526, val_MinusLogProbMetric: 29.0526

Epoch 372: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5628 - MinusLogProbMetric: 28.5628 - val_loss: 29.0526 - val_MinusLogProbMetric: 29.0526 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 373/1000
2023-10-26 05:54:48.677 
Epoch 373/1000 
	 loss: 28.5345, MinusLogProbMetric: 28.5345, val_loss: 29.2510, val_MinusLogProbMetric: 29.2510

Epoch 373: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5345 - MinusLogProbMetric: 28.5345 - val_loss: 29.2510 - val_MinusLogProbMetric: 29.2510 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 374/1000
2023-10-26 05:55:31.465 
Epoch 374/1000 
	 loss: 28.5266, MinusLogProbMetric: 28.5266, val_loss: 29.2029, val_MinusLogProbMetric: 29.2029

Epoch 374: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5266 - MinusLogProbMetric: 28.5266 - val_loss: 29.2029 - val_MinusLogProbMetric: 29.2029 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 375/1000
2023-10-26 05:56:13.952 
Epoch 375/1000 
	 loss: 28.5648, MinusLogProbMetric: 28.5648, val_loss: 29.3345, val_MinusLogProbMetric: 29.3345

Epoch 375: val_loss did not improve from 28.91437
196/196 - 42s - loss: 28.5648 - MinusLogProbMetric: 28.5648 - val_loss: 29.3345 - val_MinusLogProbMetric: 29.3345 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 376/1000
2023-10-26 05:56:57.029 
Epoch 376/1000 
	 loss: 28.5880, MinusLogProbMetric: 28.5880, val_loss: 28.9692, val_MinusLogProbMetric: 28.9692

Epoch 376: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5880 - MinusLogProbMetric: 28.5880 - val_loss: 28.9692 - val_MinusLogProbMetric: 28.9692 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 377/1000
2023-10-26 05:57:39.725 
Epoch 377/1000 
	 loss: 28.6119, MinusLogProbMetric: 28.6119, val_loss: 29.1619, val_MinusLogProbMetric: 29.1619

Epoch 377: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.6119 - MinusLogProbMetric: 28.6119 - val_loss: 29.1619 - val_MinusLogProbMetric: 29.1619 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 378/1000
2023-10-26 05:58:22.511 
Epoch 378/1000 
	 loss: 28.5421, MinusLogProbMetric: 28.5421, val_loss: 29.1106, val_MinusLogProbMetric: 29.1106

Epoch 378: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5421 - MinusLogProbMetric: 28.5421 - val_loss: 29.1106 - val_MinusLogProbMetric: 29.1106 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 379/1000
2023-10-26 05:59:05.019 
Epoch 379/1000 
	 loss: 28.5368, MinusLogProbMetric: 28.5368, val_loss: 29.4005, val_MinusLogProbMetric: 29.4005

Epoch 379: val_loss did not improve from 28.91437
196/196 - 43s - loss: 28.5368 - MinusLogProbMetric: 28.5368 - val_loss: 29.4005 - val_MinusLogProbMetric: 29.4005 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 380/1000
2023-10-26 05:59:47.643 
Epoch 380/1000 
	 loss: 28.5167, MinusLogProbMetric: 28.5167, val_loss: 28.8846, val_MinusLogProbMetric: 28.8846

Epoch 380: val_loss improved from 28.91437 to 28.88460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 44s - loss: 28.5167 - MinusLogProbMetric: 28.5167 - val_loss: 28.8846 - val_MinusLogProbMetric: 28.8846 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 381/1000
2023-10-26 06:00:31.157 
Epoch 381/1000 
	 loss: 28.5767, MinusLogProbMetric: 28.5767, val_loss: 28.9079, val_MinusLogProbMetric: 28.9079

Epoch 381: val_loss did not improve from 28.88460
196/196 - 43s - loss: 28.5767 - MinusLogProbMetric: 28.5767 - val_loss: 28.9079 - val_MinusLogProbMetric: 28.9079 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 382/1000
2023-10-26 06:01:14.026 
Epoch 382/1000 
	 loss: 28.5630, MinusLogProbMetric: 28.5630, val_loss: 29.3912, val_MinusLogProbMetric: 29.3912

Epoch 382: val_loss did not improve from 28.88460
196/196 - 43s - loss: 28.5630 - MinusLogProbMetric: 28.5630 - val_loss: 29.3912 - val_MinusLogProbMetric: 29.3912 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 383/1000
2023-10-26 06:01:56.529 
Epoch 383/1000 
	 loss: 28.5787, MinusLogProbMetric: 28.5787, val_loss: 28.9027, val_MinusLogProbMetric: 28.9027

Epoch 383: val_loss did not improve from 28.88460
196/196 - 42s - loss: 28.5787 - MinusLogProbMetric: 28.5787 - val_loss: 28.9027 - val_MinusLogProbMetric: 28.9027 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 384/1000
2023-10-26 06:02:39.719 
Epoch 384/1000 
	 loss: 28.5683, MinusLogProbMetric: 28.5683, val_loss: 29.1084, val_MinusLogProbMetric: 29.1084

Epoch 384: val_loss did not improve from 28.88460
196/196 - 43s - loss: 28.5683 - MinusLogProbMetric: 28.5683 - val_loss: 29.1084 - val_MinusLogProbMetric: 29.1084 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 385/1000
2023-10-26 06:03:22.408 
Epoch 385/1000 
	 loss: 28.5827, MinusLogProbMetric: 28.5827, val_loss: 28.8284, val_MinusLogProbMetric: 28.8284

Epoch 385: val_loss improved from 28.88460 to 28.82837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 28.5827 - MinusLogProbMetric: 28.5827 - val_loss: 28.8284 - val_MinusLogProbMetric: 28.8284 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 386/1000
2023-10-26 06:04:05.795 
Epoch 386/1000 
	 loss: 28.5869, MinusLogProbMetric: 28.5869, val_loss: 29.0153, val_MinusLogProbMetric: 29.0153

Epoch 386: val_loss did not improve from 28.82837
196/196 - 43s - loss: 28.5869 - MinusLogProbMetric: 28.5869 - val_loss: 29.0153 - val_MinusLogProbMetric: 29.0153 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 387/1000
2023-10-26 06:04:48.328 
Epoch 387/1000 
	 loss: 28.5472, MinusLogProbMetric: 28.5472, val_loss: 29.2536, val_MinusLogProbMetric: 29.2536

Epoch 387: val_loss did not improve from 28.82837
196/196 - 43s - loss: 28.5472 - MinusLogProbMetric: 28.5472 - val_loss: 29.2536 - val_MinusLogProbMetric: 29.2536 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 388/1000
2023-10-26 06:05:30.938 
Epoch 388/1000 
	 loss: 28.5468, MinusLogProbMetric: 28.5468, val_loss: 29.2153, val_MinusLogProbMetric: 29.2153

Epoch 388: val_loss did not improve from 28.82837
196/196 - 43s - loss: 28.5468 - MinusLogProbMetric: 28.5468 - val_loss: 29.2153 - val_MinusLogProbMetric: 29.2153 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 389/1000
2023-10-26 06:06:13.754 
Epoch 389/1000 
	 loss: 28.5689, MinusLogProbMetric: 28.5689, val_loss: 28.9870, val_MinusLogProbMetric: 28.9870

Epoch 389: val_loss did not improve from 28.82837
196/196 - 43s - loss: 28.5689 - MinusLogProbMetric: 28.5689 - val_loss: 28.9870 - val_MinusLogProbMetric: 28.9870 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 390/1000
2023-10-26 06:06:55.947 
Epoch 390/1000 
	 loss: 28.4718, MinusLogProbMetric: 28.4718, val_loss: 28.8080, val_MinusLogProbMetric: 28.8080

Epoch 390: val_loss improved from 28.82837 to 28.80803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 28.4718 - MinusLogProbMetric: 28.4718 - val_loss: 28.8080 - val_MinusLogProbMetric: 28.8080 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 391/1000
2023-10-26 06:07:39.958 
Epoch 391/1000 
	 loss: 28.5298, MinusLogProbMetric: 28.5298, val_loss: 28.9085, val_MinusLogProbMetric: 28.9085

Epoch 391: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5298 - MinusLogProbMetric: 28.5298 - val_loss: 28.9085 - val_MinusLogProbMetric: 28.9085 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 392/1000
2023-10-26 06:08:22.275 
Epoch 392/1000 
	 loss: 28.5140, MinusLogProbMetric: 28.5140, val_loss: 29.5765, val_MinusLogProbMetric: 29.5765

Epoch 392: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.5140 - MinusLogProbMetric: 28.5140 - val_loss: 29.5765 - val_MinusLogProbMetric: 29.5765 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 393/1000
2023-10-26 06:09:04.840 
Epoch 393/1000 
	 loss: 28.5244, MinusLogProbMetric: 28.5244, val_loss: 29.2523, val_MinusLogProbMetric: 29.2523

Epoch 393: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5244 - MinusLogProbMetric: 28.5244 - val_loss: 29.2523 - val_MinusLogProbMetric: 29.2523 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 394/1000
2023-10-26 06:09:47.558 
Epoch 394/1000 
	 loss: 28.5231, MinusLogProbMetric: 28.5231, val_loss: 29.1683, val_MinusLogProbMetric: 29.1683

Epoch 394: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5231 - MinusLogProbMetric: 28.5231 - val_loss: 29.1683 - val_MinusLogProbMetric: 29.1683 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 395/1000
2023-10-26 06:10:30.335 
Epoch 395/1000 
	 loss: 28.6053, MinusLogProbMetric: 28.6053, val_loss: 29.7041, val_MinusLogProbMetric: 29.7041

Epoch 395: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.6053 - MinusLogProbMetric: 28.6053 - val_loss: 29.7041 - val_MinusLogProbMetric: 29.7041 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 396/1000
2023-10-26 06:11:13.209 
Epoch 396/1000 
	 loss: 28.5256, MinusLogProbMetric: 28.5256, val_loss: 28.9629, val_MinusLogProbMetric: 28.9629

Epoch 396: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5256 - MinusLogProbMetric: 28.5256 - val_loss: 28.9629 - val_MinusLogProbMetric: 28.9629 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 397/1000
2023-10-26 06:11:56.096 
Epoch 397/1000 
	 loss: 28.4546, MinusLogProbMetric: 28.4546, val_loss: 29.2130, val_MinusLogProbMetric: 29.2130

Epoch 397: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4546 - MinusLogProbMetric: 28.4546 - val_loss: 29.2130 - val_MinusLogProbMetric: 29.2130 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 398/1000
2023-10-26 06:12:38.757 
Epoch 398/1000 
	 loss: 28.5436, MinusLogProbMetric: 28.5436, val_loss: 29.0590, val_MinusLogProbMetric: 29.0590

Epoch 398: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5436 - MinusLogProbMetric: 28.5436 - val_loss: 29.0590 - val_MinusLogProbMetric: 29.0590 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 399/1000
2023-10-26 06:13:21.622 
Epoch 399/1000 
	 loss: 28.4309, MinusLogProbMetric: 28.4309, val_loss: 29.4536, val_MinusLogProbMetric: 29.4536

Epoch 399: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4309 - MinusLogProbMetric: 28.4309 - val_loss: 29.4536 - val_MinusLogProbMetric: 29.4536 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 400/1000
2023-10-26 06:14:04.351 
Epoch 400/1000 
	 loss: 28.5509, MinusLogProbMetric: 28.5509, val_loss: 29.0630, val_MinusLogProbMetric: 29.0630

Epoch 400: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5509 - MinusLogProbMetric: 28.5509 - val_loss: 29.0630 - val_MinusLogProbMetric: 29.0630 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 401/1000
2023-10-26 06:14:47.398 
Epoch 401/1000 
	 loss: 28.4948, MinusLogProbMetric: 28.4948, val_loss: 29.0561, val_MinusLogProbMetric: 29.0561

Epoch 401: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4948 - MinusLogProbMetric: 28.4948 - val_loss: 29.0561 - val_MinusLogProbMetric: 29.0561 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 402/1000
2023-10-26 06:15:30.019 
Epoch 402/1000 
	 loss: 28.4486, MinusLogProbMetric: 28.4486, val_loss: 29.0881, val_MinusLogProbMetric: 29.0881

Epoch 402: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4486 - MinusLogProbMetric: 28.4486 - val_loss: 29.0881 - val_MinusLogProbMetric: 29.0881 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 403/1000
2023-10-26 06:16:12.362 
Epoch 403/1000 
	 loss: 28.4720, MinusLogProbMetric: 28.4720, val_loss: 29.1264, val_MinusLogProbMetric: 29.1264

Epoch 403: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.4720 - MinusLogProbMetric: 28.4720 - val_loss: 29.1264 - val_MinusLogProbMetric: 29.1264 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 404/1000
2023-10-26 06:16:55.484 
Epoch 404/1000 
	 loss: 28.5383, MinusLogProbMetric: 28.5383, val_loss: 29.3571, val_MinusLogProbMetric: 29.3571

Epoch 404: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5383 - MinusLogProbMetric: 28.5383 - val_loss: 29.3571 - val_MinusLogProbMetric: 29.3571 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 405/1000
2023-10-26 06:17:38.010 
Epoch 405/1000 
	 loss: 28.4859, MinusLogProbMetric: 28.4859, val_loss: 29.5232, val_MinusLogProbMetric: 29.5232

Epoch 405: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4859 - MinusLogProbMetric: 28.4859 - val_loss: 29.5232 - val_MinusLogProbMetric: 29.5232 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 406/1000
2023-10-26 06:18:20.390 
Epoch 406/1000 
	 loss: 28.5085, MinusLogProbMetric: 28.5085, val_loss: 29.3466, val_MinusLogProbMetric: 29.3466

Epoch 406: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.5085 - MinusLogProbMetric: 28.5085 - val_loss: 29.3466 - val_MinusLogProbMetric: 29.3466 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 407/1000
2023-10-26 06:19:02.579 
Epoch 407/1000 
	 loss: 28.4856, MinusLogProbMetric: 28.4856, val_loss: 29.0178, val_MinusLogProbMetric: 29.0178

Epoch 407: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.4856 - MinusLogProbMetric: 28.4856 - val_loss: 29.0178 - val_MinusLogProbMetric: 29.0178 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 408/1000
2023-10-26 06:19:44.677 
Epoch 408/1000 
	 loss: 28.5300, MinusLogProbMetric: 28.5300, val_loss: 28.8938, val_MinusLogProbMetric: 28.8938

Epoch 408: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.5300 - MinusLogProbMetric: 28.5300 - val_loss: 28.8938 - val_MinusLogProbMetric: 28.8938 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 409/1000
2023-10-26 06:20:27.177 
Epoch 409/1000 
	 loss: 28.5392, MinusLogProbMetric: 28.5392, val_loss: 29.1384, val_MinusLogProbMetric: 29.1384

Epoch 409: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.5392 - MinusLogProbMetric: 28.5392 - val_loss: 29.1384 - val_MinusLogProbMetric: 29.1384 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 410/1000
2023-10-26 06:21:09.976 
Epoch 410/1000 
	 loss: 28.4672, MinusLogProbMetric: 28.4672, val_loss: 28.9953, val_MinusLogProbMetric: 28.9953

Epoch 410: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4672 - MinusLogProbMetric: 28.4672 - val_loss: 28.9953 - val_MinusLogProbMetric: 28.9953 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 411/1000
2023-10-26 06:21:52.484 
Epoch 411/1000 
	 loss: 28.4672, MinusLogProbMetric: 28.4672, val_loss: 29.1286, val_MinusLogProbMetric: 29.1286

Epoch 411: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4672 - MinusLogProbMetric: 28.4672 - val_loss: 29.1286 - val_MinusLogProbMetric: 29.1286 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 412/1000
2023-10-26 06:22:34.875 
Epoch 412/1000 
	 loss: 28.4932, MinusLogProbMetric: 28.4932, val_loss: 29.4679, val_MinusLogProbMetric: 29.4679

Epoch 412: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.4932 - MinusLogProbMetric: 28.4932 - val_loss: 29.4679 - val_MinusLogProbMetric: 29.4679 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 413/1000
2023-10-26 06:23:17.552 
Epoch 413/1000 
	 loss: 28.4218, MinusLogProbMetric: 28.4218, val_loss: 30.2797, val_MinusLogProbMetric: 30.2797

Epoch 413: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4218 - MinusLogProbMetric: 28.4218 - val_loss: 30.2797 - val_MinusLogProbMetric: 30.2797 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 414/1000
2023-10-26 06:24:00.555 
Epoch 414/1000 
	 loss: 28.4392, MinusLogProbMetric: 28.4392, val_loss: 29.5233, val_MinusLogProbMetric: 29.5233

Epoch 414: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4392 - MinusLogProbMetric: 28.4392 - val_loss: 29.5233 - val_MinusLogProbMetric: 29.5233 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 415/1000
2023-10-26 06:24:43.445 
Epoch 415/1000 
	 loss: 28.4337, MinusLogProbMetric: 28.4337, val_loss: 29.7068, val_MinusLogProbMetric: 29.7068

Epoch 415: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4337 - MinusLogProbMetric: 28.4337 - val_loss: 29.7068 - val_MinusLogProbMetric: 29.7068 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 416/1000
2023-10-26 06:25:25.745 
Epoch 416/1000 
	 loss: 28.4855, MinusLogProbMetric: 28.4855, val_loss: 29.0672, val_MinusLogProbMetric: 29.0672

Epoch 416: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.4855 - MinusLogProbMetric: 28.4855 - val_loss: 29.0672 - val_MinusLogProbMetric: 29.0672 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 417/1000
2023-10-26 06:26:08.429 
Epoch 417/1000 
	 loss: 28.3861, MinusLogProbMetric: 28.3861, val_loss: 28.8400, val_MinusLogProbMetric: 28.8400

Epoch 417: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.3861 - MinusLogProbMetric: 28.3861 - val_loss: 28.8400 - val_MinusLogProbMetric: 28.8400 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 418/1000
2023-10-26 06:26:51.225 
Epoch 418/1000 
	 loss: 28.5122, MinusLogProbMetric: 28.5122, val_loss: 29.7590, val_MinusLogProbMetric: 29.7590

Epoch 418: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5122 - MinusLogProbMetric: 28.5122 - val_loss: 29.7590 - val_MinusLogProbMetric: 29.7590 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 419/1000
2023-10-26 06:27:34.113 
Epoch 419/1000 
	 loss: 28.5759, MinusLogProbMetric: 28.5759, val_loss: 29.5542, val_MinusLogProbMetric: 29.5542

Epoch 419: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5759 - MinusLogProbMetric: 28.5759 - val_loss: 29.5542 - val_MinusLogProbMetric: 29.5542 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 420/1000
2023-10-26 06:28:16.744 
Epoch 420/1000 
	 loss: 28.4459, MinusLogProbMetric: 28.4459, val_loss: 28.8217, val_MinusLogProbMetric: 28.8217

Epoch 420: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4459 - MinusLogProbMetric: 28.4459 - val_loss: 28.8217 - val_MinusLogProbMetric: 28.8217 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 421/1000
2023-10-26 06:28:59.582 
Epoch 421/1000 
	 loss: 28.5729, MinusLogProbMetric: 28.5729, val_loss: 28.9628, val_MinusLogProbMetric: 28.9628

Epoch 421: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.5729 - MinusLogProbMetric: 28.5729 - val_loss: 28.9628 - val_MinusLogProbMetric: 28.9628 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 422/1000
2023-10-26 06:29:42.204 
Epoch 422/1000 
	 loss: 28.4068, MinusLogProbMetric: 28.4068, val_loss: 29.2282, val_MinusLogProbMetric: 29.2282

Epoch 422: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4068 - MinusLogProbMetric: 28.4068 - val_loss: 29.2282 - val_MinusLogProbMetric: 29.2282 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 423/1000
2023-10-26 06:30:24.862 
Epoch 423/1000 
	 loss: 28.4839, MinusLogProbMetric: 28.4839, val_loss: 28.8876, val_MinusLogProbMetric: 28.8876

Epoch 423: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4839 - MinusLogProbMetric: 28.4839 - val_loss: 28.8876 - val_MinusLogProbMetric: 28.8876 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 424/1000
2023-10-26 06:31:07.731 
Epoch 424/1000 
	 loss: 28.4050, MinusLogProbMetric: 28.4050, val_loss: 29.5512, val_MinusLogProbMetric: 29.5512

Epoch 424: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4050 - MinusLogProbMetric: 28.4050 - val_loss: 29.5512 - val_MinusLogProbMetric: 29.5512 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 425/1000
2023-10-26 06:31:50.194 
Epoch 425/1000 
	 loss: 28.3823, MinusLogProbMetric: 28.3823, val_loss: 29.3557, val_MinusLogProbMetric: 29.3557

Epoch 425: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.3823 - MinusLogProbMetric: 28.3823 - val_loss: 29.3557 - val_MinusLogProbMetric: 29.3557 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 426/1000
2023-10-26 06:32:32.797 
Epoch 426/1000 
	 loss: 28.4530, MinusLogProbMetric: 28.4530, val_loss: 29.1520, val_MinusLogProbMetric: 29.1520

Epoch 426: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4530 - MinusLogProbMetric: 28.4530 - val_loss: 29.1520 - val_MinusLogProbMetric: 29.1520 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 427/1000
2023-10-26 06:33:15.500 
Epoch 427/1000 
	 loss: 28.4171, MinusLogProbMetric: 28.4171, val_loss: 29.2814, val_MinusLogProbMetric: 29.2814

Epoch 427: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4171 - MinusLogProbMetric: 28.4171 - val_loss: 29.2814 - val_MinusLogProbMetric: 29.2814 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 428/1000
2023-10-26 06:33:58.321 
Epoch 428/1000 
	 loss: 28.4310, MinusLogProbMetric: 28.4310, val_loss: 29.1528, val_MinusLogProbMetric: 29.1528

Epoch 428: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4310 - MinusLogProbMetric: 28.4310 - val_loss: 29.1528 - val_MinusLogProbMetric: 29.1528 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 429/1000
2023-10-26 06:34:41.510 
Epoch 429/1000 
	 loss: 28.4632, MinusLogProbMetric: 28.4632, val_loss: 28.8927, val_MinusLogProbMetric: 28.8927

Epoch 429: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4632 - MinusLogProbMetric: 28.4632 - val_loss: 28.8927 - val_MinusLogProbMetric: 28.8927 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 430/1000
2023-10-26 06:35:23.852 
Epoch 430/1000 
	 loss: 28.4855, MinusLogProbMetric: 28.4855, val_loss: 29.1620, val_MinusLogProbMetric: 29.1620

Epoch 430: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.4855 - MinusLogProbMetric: 28.4855 - val_loss: 29.1620 - val_MinusLogProbMetric: 29.1620 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 431/1000
2023-10-26 06:36:06.694 
Epoch 431/1000 
	 loss: 28.3821, MinusLogProbMetric: 28.3821, val_loss: 29.0651, val_MinusLogProbMetric: 29.0651

Epoch 431: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.3821 - MinusLogProbMetric: 28.3821 - val_loss: 29.0651 - val_MinusLogProbMetric: 29.0651 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 432/1000
2023-10-26 06:36:49.509 
Epoch 432/1000 
	 loss: 28.4640, MinusLogProbMetric: 28.4640, val_loss: 28.8330, val_MinusLogProbMetric: 28.8330

Epoch 432: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4640 - MinusLogProbMetric: 28.4640 - val_loss: 28.8330 - val_MinusLogProbMetric: 28.8330 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 433/1000
2023-10-26 06:37:32.387 
Epoch 433/1000 
	 loss: 28.4413, MinusLogProbMetric: 28.4413, val_loss: 29.4414, val_MinusLogProbMetric: 29.4414

Epoch 433: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4413 - MinusLogProbMetric: 28.4413 - val_loss: 29.4414 - val_MinusLogProbMetric: 29.4414 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 434/1000
2023-10-26 06:38:15.111 
Epoch 434/1000 
	 loss: 28.3861, MinusLogProbMetric: 28.3861, val_loss: 29.2760, val_MinusLogProbMetric: 29.2760

Epoch 434: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.3861 - MinusLogProbMetric: 28.3861 - val_loss: 29.2760 - val_MinusLogProbMetric: 29.2760 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 435/1000
2023-10-26 06:38:57.633 
Epoch 435/1000 
	 loss: 28.3440, MinusLogProbMetric: 28.3440, val_loss: 28.8708, val_MinusLogProbMetric: 28.8708

Epoch 435: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.3440 - MinusLogProbMetric: 28.3440 - val_loss: 28.8708 - val_MinusLogProbMetric: 28.8708 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 436/1000
2023-10-26 06:39:40.012 
Epoch 436/1000 
	 loss: 28.3678, MinusLogProbMetric: 28.3678, val_loss: 29.2138, val_MinusLogProbMetric: 29.2138

Epoch 436: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.3678 - MinusLogProbMetric: 28.3678 - val_loss: 29.2138 - val_MinusLogProbMetric: 29.2138 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 437/1000
2023-10-26 06:40:22.290 
Epoch 437/1000 
	 loss: 28.3578, MinusLogProbMetric: 28.3578, val_loss: 29.0447, val_MinusLogProbMetric: 29.0447

Epoch 437: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.3578 - MinusLogProbMetric: 28.3578 - val_loss: 29.0447 - val_MinusLogProbMetric: 29.0447 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 438/1000
2023-10-26 06:41:05.001 
Epoch 438/1000 
	 loss: 28.3869, MinusLogProbMetric: 28.3869, val_loss: 28.9380, val_MinusLogProbMetric: 28.9380

Epoch 438: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.3869 - MinusLogProbMetric: 28.3869 - val_loss: 28.9380 - val_MinusLogProbMetric: 28.9380 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 439/1000
2023-10-26 06:41:48.026 
Epoch 439/1000 
	 loss: 28.4628, MinusLogProbMetric: 28.4628, val_loss: 29.0249, val_MinusLogProbMetric: 29.0249

Epoch 439: val_loss did not improve from 28.80803
196/196 - 43s - loss: 28.4628 - MinusLogProbMetric: 28.4628 - val_loss: 29.0249 - val_MinusLogProbMetric: 29.0249 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 440/1000
2023-10-26 06:42:30.453 
Epoch 440/1000 
	 loss: 28.3697, MinusLogProbMetric: 28.3697, val_loss: 29.6509, val_MinusLogProbMetric: 29.6509

Epoch 440: val_loss did not improve from 28.80803
196/196 - 42s - loss: 28.3697 - MinusLogProbMetric: 28.3697 - val_loss: 29.6509 - val_MinusLogProbMetric: 29.6509 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 441/1000
2023-10-26 06:43:12.817 
Epoch 441/1000 
	 loss: 27.9490, MinusLogProbMetric: 27.9490, val_loss: 28.5796, val_MinusLogProbMetric: 28.5796

Epoch 441: val_loss improved from 28.80803 to 28.57957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 27.9490 - MinusLogProbMetric: 27.9490 - val_loss: 28.5796 - val_MinusLogProbMetric: 28.5796 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 442/1000
2023-10-26 06:43:55.959 
Epoch 442/1000 
	 loss: 27.9328, MinusLogProbMetric: 27.9328, val_loss: 28.4934, val_MinusLogProbMetric: 28.4934

Epoch 442: val_loss improved from 28.57957 to 28.49342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 27.9328 - MinusLogProbMetric: 27.9328 - val_loss: 28.4934 - val_MinusLogProbMetric: 28.4934 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 443/1000
2023-10-26 06:44:39.319 
Epoch 443/1000 
	 loss: 27.9487, MinusLogProbMetric: 27.9487, val_loss: 28.5421, val_MinusLogProbMetric: 28.5421

Epoch 443: val_loss did not improve from 28.49342
196/196 - 42s - loss: 27.9487 - MinusLogProbMetric: 27.9487 - val_loss: 28.5421 - val_MinusLogProbMetric: 28.5421 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 444/1000
2023-10-26 06:45:21.952 
Epoch 444/1000 
	 loss: 27.9396, MinusLogProbMetric: 27.9396, val_loss: 28.6125, val_MinusLogProbMetric: 28.6125

Epoch 444: val_loss did not improve from 28.49342
196/196 - 43s - loss: 27.9396 - MinusLogProbMetric: 27.9396 - val_loss: 28.6125 - val_MinusLogProbMetric: 28.6125 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 445/1000
2023-10-26 06:46:04.774 
Epoch 445/1000 
	 loss: 27.9000, MinusLogProbMetric: 27.9000, val_loss: 28.6219, val_MinusLogProbMetric: 28.6219

Epoch 445: val_loss did not improve from 28.49342
196/196 - 43s - loss: 27.9000 - MinusLogProbMetric: 27.9000 - val_loss: 28.6219 - val_MinusLogProbMetric: 28.6219 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 446/1000
2023-10-26 06:46:47.443 
Epoch 446/1000 
	 loss: 27.9184, MinusLogProbMetric: 27.9184, val_loss: 28.5858, val_MinusLogProbMetric: 28.5858

Epoch 446: val_loss did not improve from 28.49342
196/196 - 43s - loss: 27.9184 - MinusLogProbMetric: 27.9184 - val_loss: 28.5858 - val_MinusLogProbMetric: 28.5858 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 447/1000
2023-10-26 06:47:30.009 
Epoch 447/1000 
	 loss: 27.9092, MinusLogProbMetric: 27.9092, val_loss: 28.5421, val_MinusLogProbMetric: 28.5421

Epoch 447: val_loss did not improve from 28.49342
196/196 - 43s - loss: 27.9092 - MinusLogProbMetric: 27.9092 - val_loss: 28.5421 - val_MinusLogProbMetric: 28.5421 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 448/1000
2023-10-26 06:48:12.471 
Epoch 448/1000 
	 loss: 27.9123, MinusLogProbMetric: 27.9123, val_loss: 28.6445, val_MinusLogProbMetric: 28.6445

Epoch 448: val_loss did not improve from 28.49342
196/196 - 42s - loss: 27.9123 - MinusLogProbMetric: 27.9123 - val_loss: 28.6445 - val_MinusLogProbMetric: 28.6445 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 449/1000
2023-10-26 06:48:54.958 
Epoch 449/1000 
	 loss: 27.9587, MinusLogProbMetric: 27.9587, val_loss: 28.5039, val_MinusLogProbMetric: 28.5039

Epoch 449: val_loss did not improve from 28.49342
196/196 - 42s - loss: 27.9587 - MinusLogProbMetric: 27.9587 - val_loss: 28.5039 - val_MinusLogProbMetric: 28.5039 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 450/1000
2023-10-26 06:49:37.469 
Epoch 450/1000 
	 loss: 27.9298, MinusLogProbMetric: 27.9298, val_loss: 28.5118, val_MinusLogProbMetric: 28.5118

Epoch 450: val_loss did not improve from 28.49342
196/196 - 43s - loss: 27.9298 - MinusLogProbMetric: 27.9298 - val_loss: 28.5118 - val_MinusLogProbMetric: 28.5118 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 451/1000
2023-10-26 06:50:20.065 
Epoch 451/1000 
	 loss: 27.9145, MinusLogProbMetric: 27.9145, val_loss: 28.4966, val_MinusLogProbMetric: 28.4966

Epoch 451: val_loss did not improve from 28.49342
196/196 - 43s - loss: 27.9145 - MinusLogProbMetric: 27.9145 - val_loss: 28.4966 - val_MinusLogProbMetric: 28.4966 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 452/1000
2023-10-26 06:51:02.973 
Epoch 452/1000 
	 loss: 27.9092, MinusLogProbMetric: 27.9092, val_loss: 28.5621, val_MinusLogProbMetric: 28.5621

Epoch 452: val_loss did not improve from 28.49342
196/196 - 43s - loss: 27.9092 - MinusLogProbMetric: 27.9092 - val_loss: 28.5621 - val_MinusLogProbMetric: 28.5621 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 453/1000
2023-10-26 06:51:43.994 
Epoch 453/1000 
	 loss: 27.9173, MinusLogProbMetric: 27.9173, val_loss: 28.5917, val_MinusLogProbMetric: 28.5917

Epoch 453: val_loss did not improve from 28.49342
196/196 - 41s - loss: 27.9173 - MinusLogProbMetric: 27.9173 - val_loss: 28.5917 - val_MinusLogProbMetric: 28.5917 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 454/1000
2023-10-26 06:52:23.101 
Epoch 454/1000 
	 loss: 27.9096, MinusLogProbMetric: 27.9096, val_loss: 28.5354, val_MinusLogProbMetric: 28.5354

Epoch 454: val_loss did not improve from 28.49342
196/196 - 39s - loss: 27.9096 - MinusLogProbMetric: 27.9096 - val_loss: 28.5354 - val_MinusLogProbMetric: 28.5354 - lr: 1.6667e-04 - 39s/epoch - 200ms/step
Epoch 455/1000
2023-10-26 06:53:02.931 
Epoch 455/1000 
	 loss: 27.9146, MinusLogProbMetric: 27.9146, val_loss: 28.5172, val_MinusLogProbMetric: 28.5172

Epoch 455: val_loss did not improve from 28.49342
196/196 - 40s - loss: 27.9146 - MinusLogProbMetric: 27.9146 - val_loss: 28.5172 - val_MinusLogProbMetric: 28.5172 - lr: 1.6667e-04 - 40s/epoch - 203ms/step
Epoch 456/1000
2023-10-26 06:53:43.776 
Epoch 456/1000 
	 loss: 27.8978, MinusLogProbMetric: 27.8978, val_loss: 28.6448, val_MinusLogProbMetric: 28.6448

Epoch 456: val_loss did not improve from 28.49342
196/196 - 41s - loss: 27.8978 - MinusLogProbMetric: 27.8978 - val_loss: 28.6448 - val_MinusLogProbMetric: 28.6448 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 457/1000
2023-10-26 06:54:23.504 
Epoch 457/1000 
	 loss: 27.9365, MinusLogProbMetric: 27.9365, val_loss: 28.4877, val_MinusLogProbMetric: 28.4877

Epoch 457: val_loss improved from 28.49342 to 28.48767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 41s - loss: 27.9365 - MinusLogProbMetric: 27.9365 - val_loss: 28.4877 - val_MinusLogProbMetric: 28.4877 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 458/1000
2023-10-26 06:55:06.780 
Epoch 458/1000 
	 loss: 27.9220, MinusLogProbMetric: 27.9220, val_loss: 28.5783, val_MinusLogProbMetric: 28.5783

Epoch 458: val_loss did not improve from 28.48767
196/196 - 42s - loss: 27.9220 - MinusLogProbMetric: 27.9220 - val_loss: 28.5783 - val_MinusLogProbMetric: 28.5783 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 459/1000
2023-10-26 06:55:45.922 
Epoch 459/1000 
	 loss: 27.9142, MinusLogProbMetric: 27.9142, val_loss: 28.5920, val_MinusLogProbMetric: 28.5920

Epoch 459: val_loss did not improve from 28.48767
196/196 - 39s - loss: 27.9142 - MinusLogProbMetric: 27.9142 - val_loss: 28.5920 - val_MinusLogProbMetric: 28.5920 - lr: 1.6667e-04 - 39s/epoch - 200ms/step
Epoch 460/1000
2023-10-26 06:56:26.674 
Epoch 460/1000 
	 loss: 27.9081, MinusLogProbMetric: 27.9081, val_loss: 28.6747, val_MinusLogProbMetric: 28.6747

Epoch 460: val_loss did not improve from 28.48767
196/196 - 41s - loss: 27.9081 - MinusLogProbMetric: 27.9081 - val_loss: 28.6747 - val_MinusLogProbMetric: 28.6747 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 461/1000
2023-10-26 06:57:05.636 
Epoch 461/1000 
	 loss: 27.9045, MinusLogProbMetric: 27.9045, val_loss: 28.4339, val_MinusLogProbMetric: 28.4339

Epoch 461: val_loss improved from 28.48767 to 28.43386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 40s - loss: 27.9045 - MinusLogProbMetric: 27.9045 - val_loss: 28.4339 - val_MinusLogProbMetric: 28.4339 - lr: 1.6667e-04 - 40s/epoch - 202ms/step
Epoch 462/1000
2023-10-26 06:57:43.668 
Epoch 462/1000 
	 loss: 27.8959, MinusLogProbMetric: 27.8959, val_loss: 28.5635, val_MinusLogProbMetric: 28.5635

Epoch 462: val_loss did not improve from 28.43386
196/196 - 37s - loss: 27.8959 - MinusLogProbMetric: 27.8959 - val_loss: 28.5635 - val_MinusLogProbMetric: 28.5635 - lr: 1.6667e-04 - 37s/epoch - 190ms/step
Epoch 463/1000
2023-10-26 06:58:24.569 
Epoch 463/1000 
	 loss: 27.9096, MinusLogProbMetric: 27.9096, val_loss: 28.4102, val_MinusLogProbMetric: 28.4102

Epoch 463: val_loss improved from 28.43386 to 28.41016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.9096 - MinusLogProbMetric: 27.9096 - val_loss: 28.4102 - val_MinusLogProbMetric: 28.4102 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 464/1000
2023-10-26 06:59:07.194 
Epoch 464/1000 
	 loss: 27.9035, MinusLogProbMetric: 27.9035, val_loss: 28.5344, val_MinusLogProbMetric: 28.5344

Epoch 464: val_loss did not improve from 28.41016
196/196 - 42s - loss: 27.9035 - MinusLogProbMetric: 27.9035 - val_loss: 28.5344 - val_MinusLogProbMetric: 28.5344 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 465/1000
2023-10-26 06:59:48.793 
Epoch 465/1000 
	 loss: 27.9044, MinusLogProbMetric: 27.9044, val_loss: 28.6253, val_MinusLogProbMetric: 28.6253

Epoch 465: val_loss did not improve from 28.41016
196/196 - 42s - loss: 27.9044 - MinusLogProbMetric: 27.9044 - val_loss: 28.6253 - val_MinusLogProbMetric: 28.6253 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 466/1000
2023-10-26 07:00:31.226 
Epoch 466/1000 
	 loss: 27.9331, MinusLogProbMetric: 27.9331, val_loss: 28.5114, val_MinusLogProbMetric: 28.5114

Epoch 466: val_loss did not improve from 28.41016
196/196 - 42s - loss: 27.9331 - MinusLogProbMetric: 27.9331 - val_loss: 28.5114 - val_MinusLogProbMetric: 28.5114 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 467/1000
2023-10-26 07:01:13.480 
Epoch 467/1000 
	 loss: 27.9032, MinusLogProbMetric: 27.9032, val_loss: 28.5514, val_MinusLogProbMetric: 28.5514

Epoch 467: val_loss did not improve from 28.41016
196/196 - 42s - loss: 27.9032 - MinusLogProbMetric: 27.9032 - val_loss: 28.5514 - val_MinusLogProbMetric: 28.5514 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 468/1000
2023-10-26 07:01:56.067 
Epoch 468/1000 
	 loss: 27.9008, MinusLogProbMetric: 27.9008, val_loss: 28.4004, val_MinusLogProbMetric: 28.4004

Epoch 468: val_loss improved from 28.41016 to 28.40044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 27.9008 - MinusLogProbMetric: 27.9008 - val_loss: 28.4004 - val_MinusLogProbMetric: 28.4004 - lr: 1.6667e-04 - 43s/epoch - 221ms/step
Epoch 469/1000
2023-10-26 07:02:38.870 
Epoch 469/1000 
	 loss: 27.9058, MinusLogProbMetric: 27.9058, val_loss: 28.6812, val_MinusLogProbMetric: 28.6812

Epoch 469: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9058 - MinusLogProbMetric: 27.9058 - val_loss: 28.6812 - val_MinusLogProbMetric: 28.6812 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 470/1000
2023-10-26 07:03:21.032 
Epoch 470/1000 
	 loss: 27.9028, MinusLogProbMetric: 27.9028, val_loss: 28.5910, val_MinusLogProbMetric: 28.5910

Epoch 470: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9028 - MinusLogProbMetric: 27.9028 - val_loss: 28.5910 - val_MinusLogProbMetric: 28.5910 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 471/1000
2023-10-26 07:04:03.171 
Epoch 471/1000 
	 loss: 27.9138, MinusLogProbMetric: 27.9138, val_loss: 28.4865, val_MinusLogProbMetric: 28.4865

Epoch 471: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9138 - MinusLogProbMetric: 27.9138 - val_loss: 28.4865 - val_MinusLogProbMetric: 28.4865 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 472/1000
2023-10-26 07:04:45.252 
Epoch 472/1000 
	 loss: 27.8908, MinusLogProbMetric: 27.8908, val_loss: 28.4715, val_MinusLogProbMetric: 28.4715

Epoch 472: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8908 - MinusLogProbMetric: 27.8908 - val_loss: 28.4715 - val_MinusLogProbMetric: 28.4715 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 473/1000
2023-10-26 07:05:27.204 
Epoch 473/1000 
	 loss: 27.9173, MinusLogProbMetric: 27.9173, val_loss: 28.5211, val_MinusLogProbMetric: 28.5211

Epoch 473: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9173 - MinusLogProbMetric: 27.9173 - val_loss: 28.5211 - val_MinusLogProbMetric: 28.5211 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 474/1000
2023-10-26 07:06:08.861 
Epoch 474/1000 
	 loss: 27.8990, MinusLogProbMetric: 27.8990, val_loss: 28.5255, val_MinusLogProbMetric: 28.5255

Epoch 474: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8990 - MinusLogProbMetric: 27.8990 - val_loss: 28.5255 - val_MinusLogProbMetric: 28.5255 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 475/1000
2023-10-26 07:06:50.819 
Epoch 475/1000 
	 loss: 27.8722, MinusLogProbMetric: 27.8722, val_loss: 28.6682, val_MinusLogProbMetric: 28.6682

Epoch 475: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8722 - MinusLogProbMetric: 27.8722 - val_loss: 28.6682 - val_MinusLogProbMetric: 28.6682 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 476/1000
2023-10-26 07:07:32.842 
Epoch 476/1000 
	 loss: 27.8920, MinusLogProbMetric: 27.8920, val_loss: 28.5564, val_MinusLogProbMetric: 28.5564

Epoch 476: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8920 - MinusLogProbMetric: 27.8920 - val_loss: 28.5564 - val_MinusLogProbMetric: 28.5564 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 477/1000
2023-10-26 07:08:14.934 
Epoch 477/1000 
	 loss: 27.8764, MinusLogProbMetric: 27.8764, val_loss: 28.6109, val_MinusLogProbMetric: 28.6109

Epoch 477: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8764 - MinusLogProbMetric: 27.8764 - val_loss: 28.6109 - val_MinusLogProbMetric: 28.6109 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 478/1000
2023-10-26 07:08:57.115 
Epoch 478/1000 
	 loss: 27.8836, MinusLogProbMetric: 27.8836, val_loss: 28.6228, val_MinusLogProbMetric: 28.6228

Epoch 478: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8836 - MinusLogProbMetric: 27.8836 - val_loss: 28.6228 - val_MinusLogProbMetric: 28.6228 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 479/1000
2023-10-26 07:09:38.976 
Epoch 479/1000 
	 loss: 27.9122, MinusLogProbMetric: 27.9122, val_loss: 28.5317, val_MinusLogProbMetric: 28.5317

Epoch 479: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9122 - MinusLogProbMetric: 27.9122 - val_loss: 28.5317 - val_MinusLogProbMetric: 28.5317 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 480/1000
2023-10-26 07:10:21.251 
Epoch 480/1000 
	 loss: 27.8786, MinusLogProbMetric: 27.8786, val_loss: 28.4409, val_MinusLogProbMetric: 28.4409

Epoch 480: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8786 - MinusLogProbMetric: 27.8786 - val_loss: 28.4409 - val_MinusLogProbMetric: 28.4409 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 481/1000
2023-10-26 07:11:03.064 
Epoch 481/1000 
	 loss: 27.8937, MinusLogProbMetric: 27.8937, val_loss: 28.4772, val_MinusLogProbMetric: 28.4772

Epoch 481: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8937 - MinusLogProbMetric: 27.8937 - val_loss: 28.4772 - val_MinusLogProbMetric: 28.4772 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 482/1000
2023-10-26 07:11:45.120 
Epoch 482/1000 
	 loss: 27.8824, MinusLogProbMetric: 27.8824, val_loss: 28.6135, val_MinusLogProbMetric: 28.6135

Epoch 482: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8824 - MinusLogProbMetric: 27.8824 - val_loss: 28.6135 - val_MinusLogProbMetric: 28.6135 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 483/1000
2023-10-26 07:12:27.062 
Epoch 483/1000 
	 loss: 27.8951, MinusLogProbMetric: 27.8951, val_loss: 28.4928, val_MinusLogProbMetric: 28.4928

Epoch 483: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8951 - MinusLogProbMetric: 27.8951 - val_loss: 28.4928 - val_MinusLogProbMetric: 28.4928 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 484/1000
2023-10-26 07:13:08.975 
Epoch 484/1000 
	 loss: 27.8941, MinusLogProbMetric: 27.8941, val_loss: 28.5020, val_MinusLogProbMetric: 28.5020

Epoch 484: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8941 - MinusLogProbMetric: 27.8941 - val_loss: 28.5020 - val_MinusLogProbMetric: 28.5020 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 485/1000
2023-10-26 07:13:51.037 
Epoch 485/1000 
	 loss: 27.8995, MinusLogProbMetric: 27.8995, val_loss: 28.5465, val_MinusLogProbMetric: 28.5465

Epoch 485: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8995 - MinusLogProbMetric: 27.8995 - val_loss: 28.5465 - val_MinusLogProbMetric: 28.5465 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 486/1000
2023-10-26 07:14:33.194 
Epoch 486/1000 
	 loss: 27.9153, MinusLogProbMetric: 27.9153, val_loss: 28.6542, val_MinusLogProbMetric: 28.6542

Epoch 486: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9153 - MinusLogProbMetric: 27.9153 - val_loss: 28.6542 - val_MinusLogProbMetric: 28.6542 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 487/1000
2023-10-26 07:15:15.146 
Epoch 487/1000 
	 loss: 27.8722, MinusLogProbMetric: 27.8722, val_loss: 28.4798, val_MinusLogProbMetric: 28.4798

Epoch 487: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8722 - MinusLogProbMetric: 27.8722 - val_loss: 28.4798 - val_MinusLogProbMetric: 28.4798 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 488/1000
2023-10-26 07:15:57.305 
Epoch 488/1000 
	 loss: 27.9056, MinusLogProbMetric: 27.9056, val_loss: 28.5623, val_MinusLogProbMetric: 28.5623

Epoch 488: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9056 - MinusLogProbMetric: 27.9056 - val_loss: 28.5623 - val_MinusLogProbMetric: 28.5623 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 489/1000
2023-10-26 07:16:39.363 
Epoch 489/1000 
	 loss: 27.8690, MinusLogProbMetric: 27.8690, val_loss: 28.5040, val_MinusLogProbMetric: 28.5040

Epoch 489: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8690 - MinusLogProbMetric: 27.8690 - val_loss: 28.5040 - val_MinusLogProbMetric: 28.5040 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 490/1000
2023-10-26 07:17:21.535 
Epoch 490/1000 
	 loss: 27.8732, MinusLogProbMetric: 27.8732, val_loss: 28.6509, val_MinusLogProbMetric: 28.6509

Epoch 490: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8732 - MinusLogProbMetric: 27.8732 - val_loss: 28.6509 - val_MinusLogProbMetric: 28.6509 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 491/1000
2023-10-26 07:18:03.527 
Epoch 491/1000 
	 loss: 27.8771, MinusLogProbMetric: 27.8771, val_loss: 28.8477, val_MinusLogProbMetric: 28.8477

Epoch 491: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8771 - MinusLogProbMetric: 27.8771 - val_loss: 28.8477 - val_MinusLogProbMetric: 28.8477 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 492/1000
2023-10-26 07:18:45.501 
Epoch 492/1000 
	 loss: 27.9162, MinusLogProbMetric: 27.9162, val_loss: 28.4350, val_MinusLogProbMetric: 28.4350

Epoch 492: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.9162 - MinusLogProbMetric: 27.9162 - val_loss: 28.4350 - val_MinusLogProbMetric: 28.4350 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 493/1000
2023-10-26 07:19:27.450 
Epoch 493/1000 
	 loss: 27.8735, MinusLogProbMetric: 27.8735, val_loss: 28.5574, val_MinusLogProbMetric: 28.5574

Epoch 493: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8735 - MinusLogProbMetric: 27.8735 - val_loss: 28.5574 - val_MinusLogProbMetric: 28.5574 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 494/1000
2023-10-26 07:20:09.585 
Epoch 494/1000 
	 loss: 27.8975, MinusLogProbMetric: 27.8975, val_loss: 28.6718, val_MinusLogProbMetric: 28.6718

Epoch 494: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8975 - MinusLogProbMetric: 27.8975 - val_loss: 28.6718 - val_MinusLogProbMetric: 28.6718 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 495/1000
2023-10-26 07:20:51.973 
Epoch 495/1000 
	 loss: 27.8739, MinusLogProbMetric: 27.8739, val_loss: 28.6314, val_MinusLogProbMetric: 28.6314

Epoch 495: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8739 - MinusLogProbMetric: 27.8739 - val_loss: 28.6314 - val_MinusLogProbMetric: 28.6314 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 496/1000
2023-10-26 07:21:33.805 
Epoch 496/1000 
	 loss: 27.8874, MinusLogProbMetric: 27.8874, val_loss: 28.5537, val_MinusLogProbMetric: 28.5537

Epoch 496: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8874 - MinusLogProbMetric: 27.8874 - val_loss: 28.5537 - val_MinusLogProbMetric: 28.5537 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 497/1000
2023-10-26 07:22:15.748 
Epoch 497/1000 
	 loss: 27.8769, MinusLogProbMetric: 27.8769, val_loss: 28.5348, val_MinusLogProbMetric: 28.5348

Epoch 497: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8769 - MinusLogProbMetric: 27.8769 - val_loss: 28.5348 - val_MinusLogProbMetric: 28.5348 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 498/1000
2023-10-26 07:22:57.521 
Epoch 498/1000 
	 loss: 27.8928, MinusLogProbMetric: 27.8928, val_loss: 28.4532, val_MinusLogProbMetric: 28.4532

Epoch 498: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8928 - MinusLogProbMetric: 27.8928 - val_loss: 28.4532 - val_MinusLogProbMetric: 28.4532 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 499/1000
2023-10-26 07:23:39.854 
Epoch 499/1000 
	 loss: 27.8680, MinusLogProbMetric: 27.8680, val_loss: 28.4566, val_MinusLogProbMetric: 28.4566

Epoch 499: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8680 - MinusLogProbMetric: 27.8680 - val_loss: 28.4566 - val_MinusLogProbMetric: 28.4566 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 500/1000
2023-10-26 07:24:22.006 
Epoch 500/1000 
	 loss: 27.8539, MinusLogProbMetric: 27.8539, val_loss: 28.6481, val_MinusLogProbMetric: 28.6481

Epoch 500: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8539 - MinusLogProbMetric: 27.8539 - val_loss: 28.6481 - val_MinusLogProbMetric: 28.6481 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 501/1000
2023-10-26 07:25:04.156 
Epoch 501/1000 
	 loss: 27.8692, MinusLogProbMetric: 27.8692, val_loss: 28.4966, val_MinusLogProbMetric: 28.4966

Epoch 501: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8692 - MinusLogProbMetric: 27.8692 - val_loss: 28.4966 - val_MinusLogProbMetric: 28.4966 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 502/1000
2023-10-26 07:25:46.199 
Epoch 502/1000 
	 loss: 27.8826, MinusLogProbMetric: 27.8826, val_loss: 28.5222, val_MinusLogProbMetric: 28.5222

Epoch 502: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8826 - MinusLogProbMetric: 27.8826 - val_loss: 28.5222 - val_MinusLogProbMetric: 28.5222 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 503/1000
2023-10-26 07:26:28.161 
Epoch 503/1000 
	 loss: 27.8634, MinusLogProbMetric: 27.8634, val_loss: 28.5368, val_MinusLogProbMetric: 28.5368

Epoch 503: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8634 - MinusLogProbMetric: 27.8634 - val_loss: 28.5368 - val_MinusLogProbMetric: 28.5368 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 504/1000
2023-10-26 07:27:10.322 
Epoch 504/1000 
	 loss: 27.8908, MinusLogProbMetric: 27.8908, val_loss: 28.5426, val_MinusLogProbMetric: 28.5426

Epoch 504: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8908 - MinusLogProbMetric: 27.8908 - val_loss: 28.5426 - val_MinusLogProbMetric: 28.5426 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 505/1000
2023-10-26 07:27:52.406 
Epoch 505/1000 
	 loss: 27.8821, MinusLogProbMetric: 27.8821, val_loss: 28.5164, val_MinusLogProbMetric: 28.5164

Epoch 505: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8821 - MinusLogProbMetric: 27.8821 - val_loss: 28.5164 - val_MinusLogProbMetric: 28.5164 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 506/1000
2023-10-26 07:28:34.212 
Epoch 506/1000 
	 loss: 27.8640, MinusLogProbMetric: 27.8640, val_loss: 28.5774, val_MinusLogProbMetric: 28.5774

Epoch 506: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8640 - MinusLogProbMetric: 27.8640 - val_loss: 28.5774 - val_MinusLogProbMetric: 28.5774 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 507/1000
2023-10-26 07:29:16.078 
Epoch 507/1000 
	 loss: 27.8682, MinusLogProbMetric: 27.8682, val_loss: 28.6189, val_MinusLogProbMetric: 28.6189

Epoch 507: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8682 - MinusLogProbMetric: 27.8682 - val_loss: 28.6189 - val_MinusLogProbMetric: 28.6189 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 508/1000
2023-10-26 07:29:57.463 
Epoch 508/1000 
	 loss: 27.8367, MinusLogProbMetric: 27.8367, val_loss: 28.6836, val_MinusLogProbMetric: 28.6836

Epoch 508: val_loss did not improve from 28.40044
196/196 - 41s - loss: 27.8367 - MinusLogProbMetric: 27.8367 - val_loss: 28.6836 - val_MinusLogProbMetric: 28.6836 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 509/1000
2023-10-26 07:30:38.192 
Epoch 509/1000 
	 loss: 27.8814, MinusLogProbMetric: 27.8814, val_loss: 28.5948, val_MinusLogProbMetric: 28.5948

Epoch 509: val_loss did not improve from 28.40044
196/196 - 41s - loss: 27.8814 - MinusLogProbMetric: 27.8814 - val_loss: 28.5948 - val_MinusLogProbMetric: 28.5948 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 510/1000
2023-10-26 07:31:18.192 
Epoch 510/1000 
	 loss: 27.8608, MinusLogProbMetric: 27.8608, val_loss: 28.4818, val_MinusLogProbMetric: 28.4818

Epoch 510: val_loss did not improve from 28.40044
196/196 - 40s - loss: 27.8608 - MinusLogProbMetric: 27.8608 - val_loss: 28.4818 - val_MinusLogProbMetric: 28.4818 - lr: 1.6667e-04 - 40s/epoch - 204ms/step
Epoch 511/1000
2023-10-26 07:31:57.640 
Epoch 511/1000 
	 loss: 27.8599, MinusLogProbMetric: 27.8599, val_loss: 28.5163, val_MinusLogProbMetric: 28.5163

Epoch 511: val_loss did not improve from 28.40044
196/196 - 39s - loss: 27.8599 - MinusLogProbMetric: 27.8599 - val_loss: 28.5163 - val_MinusLogProbMetric: 28.5163 - lr: 1.6667e-04 - 39s/epoch - 201ms/step
Epoch 512/1000
2023-10-26 07:32:39.129 
Epoch 512/1000 
	 loss: 27.8494, MinusLogProbMetric: 27.8494, val_loss: 28.7887, val_MinusLogProbMetric: 28.7887

Epoch 512: val_loss did not improve from 28.40044
196/196 - 41s - loss: 27.8494 - MinusLogProbMetric: 27.8494 - val_loss: 28.7887 - val_MinusLogProbMetric: 28.7887 - lr: 1.6667e-04 - 41s/epoch - 212ms/step
Epoch 513/1000
2023-10-26 07:33:21.716 
Epoch 513/1000 
	 loss: 27.8612, MinusLogProbMetric: 27.8612, val_loss: 28.5825, val_MinusLogProbMetric: 28.5825

Epoch 513: val_loss did not improve from 28.40044
196/196 - 43s - loss: 27.8612 - MinusLogProbMetric: 27.8612 - val_loss: 28.5825 - val_MinusLogProbMetric: 28.5825 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 514/1000
2023-10-26 07:34:03.426 
Epoch 514/1000 
	 loss: 27.8780, MinusLogProbMetric: 27.8780, val_loss: 28.5340, val_MinusLogProbMetric: 28.5340

Epoch 514: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8780 - MinusLogProbMetric: 27.8780 - val_loss: 28.5340 - val_MinusLogProbMetric: 28.5340 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 515/1000
2023-10-26 07:34:45.944 
Epoch 515/1000 
	 loss: 27.8569, MinusLogProbMetric: 27.8569, val_loss: 28.6151, val_MinusLogProbMetric: 28.6151

Epoch 515: val_loss did not improve from 28.40044
196/196 - 43s - loss: 27.8569 - MinusLogProbMetric: 27.8569 - val_loss: 28.6151 - val_MinusLogProbMetric: 28.6151 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 516/1000
2023-10-26 07:35:28.518 
Epoch 516/1000 
	 loss: 27.8729, MinusLogProbMetric: 27.8729, val_loss: 28.5371, val_MinusLogProbMetric: 28.5371

Epoch 516: val_loss did not improve from 28.40044
196/196 - 43s - loss: 27.8729 - MinusLogProbMetric: 27.8729 - val_loss: 28.5371 - val_MinusLogProbMetric: 28.5371 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 517/1000
2023-10-26 07:36:11.048 
Epoch 517/1000 
	 loss: 27.8504, MinusLogProbMetric: 27.8504, val_loss: 28.4507, val_MinusLogProbMetric: 28.4507

Epoch 517: val_loss did not improve from 28.40044
196/196 - 43s - loss: 27.8504 - MinusLogProbMetric: 27.8504 - val_loss: 28.4507 - val_MinusLogProbMetric: 28.4507 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 518/1000
2023-10-26 07:36:53.032 
Epoch 518/1000 
	 loss: 27.8169, MinusLogProbMetric: 27.8169, val_loss: 28.5764, val_MinusLogProbMetric: 28.5764

Epoch 518: val_loss did not improve from 28.40044
196/196 - 42s - loss: 27.8169 - MinusLogProbMetric: 27.8169 - val_loss: 28.5764 - val_MinusLogProbMetric: 28.5764 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 519/1000
2023-10-26 07:37:35.492 
Epoch 519/1000 
	 loss: 27.6978, MinusLogProbMetric: 27.6978, val_loss: 28.3872, val_MinusLogProbMetric: 28.3872

Epoch 519: val_loss improved from 28.40044 to 28.38720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 27.6978 - MinusLogProbMetric: 27.6978 - val_loss: 28.3872 - val_MinusLogProbMetric: 28.3872 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 520/1000
2023-10-26 07:38:18.046 
Epoch 520/1000 
	 loss: 27.6960, MinusLogProbMetric: 27.6960, val_loss: 28.3682, val_MinusLogProbMetric: 28.3682

Epoch 520: val_loss improved from 28.38720 to 28.36820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.6960 - MinusLogProbMetric: 27.6960 - val_loss: 28.3682 - val_MinusLogProbMetric: 28.3682 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 521/1000
2023-10-26 07:39:00.840 
Epoch 521/1000 
	 loss: 27.6981, MinusLogProbMetric: 27.6981, val_loss: 28.4105, val_MinusLogProbMetric: 28.4105

Epoch 521: val_loss did not improve from 28.36820
196/196 - 42s - loss: 27.6981 - MinusLogProbMetric: 27.6981 - val_loss: 28.4105 - val_MinusLogProbMetric: 28.4105 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 522/1000
2023-10-26 07:39:43.047 
Epoch 522/1000 
	 loss: 27.6884, MinusLogProbMetric: 27.6884, val_loss: 28.4006, val_MinusLogProbMetric: 28.4006

Epoch 522: val_loss did not improve from 28.36820
196/196 - 42s - loss: 27.6884 - MinusLogProbMetric: 27.6884 - val_loss: 28.4006 - val_MinusLogProbMetric: 28.4006 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 523/1000
2023-10-26 07:40:25.263 
Epoch 523/1000 
	 loss: 27.6923, MinusLogProbMetric: 27.6923, val_loss: 28.3966, val_MinusLogProbMetric: 28.3966

Epoch 523: val_loss did not improve from 28.36820
196/196 - 42s - loss: 27.6923 - MinusLogProbMetric: 27.6923 - val_loss: 28.3966 - val_MinusLogProbMetric: 28.3966 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 524/1000
2023-10-26 07:41:07.672 
Epoch 524/1000 
	 loss: 27.6958, MinusLogProbMetric: 27.6958, val_loss: 28.3296, val_MinusLogProbMetric: 28.3296

Epoch 524: val_loss improved from 28.36820 to 28.32963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 27.6958 - MinusLogProbMetric: 27.6958 - val_loss: 28.3296 - val_MinusLogProbMetric: 28.3296 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 525/1000
2023-10-26 07:41:50.676 
Epoch 525/1000 
	 loss: 27.6871, MinusLogProbMetric: 27.6871, val_loss: 28.4259, val_MinusLogProbMetric: 28.4259

Epoch 525: val_loss did not improve from 28.32963
196/196 - 42s - loss: 27.6871 - MinusLogProbMetric: 27.6871 - val_loss: 28.4259 - val_MinusLogProbMetric: 28.4259 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 526/1000
2023-10-26 07:42:33.141 
Epoch 526/1000 
	 loss: 27.7038, MinusLogProbMetric: 27.7038, val_loss: 28.3640, val_MinusLogProbMetric: 28.3640

Epoch 526: val_loss did not improve from 28.32963
196/196 - 42s - loss: 27.7038 - MinusLogProbMetric: 27.7038 - val_loss: 28.3640 - val_MinusLogProbMetric: 28.3640 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 527/1000
2023-10-26 07:43:16.055 
Epoch 527/1000 
	 loss: 27.6944, MinusLogProbMetric: 27.6944, val_loss: 28.5247, val_MinusLogProbMetric: 28.5247

Epoch 527: val_loss did not improve from 28.32963
196/196 - 43s - loss: 27.6944 - MinusLogProbMetric: 27.6944 - val_loss: 28.5247 - val_MinusLogProbMetric: 28.5247 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 528/1000
2023-10-26 07:43:58.119 
Epoch 528/1000 
	 loss: 27.6975, MinusLogProbMetric: 27.6975, val_loss: 28.3415, val_MinusLogProbMetric: 28.3415

Epoch 528: val_loss did not improve from 28.32963
196/196 - 42s - loss: 27.6975 - MinusLogProbMetric: 27.6975 - val_loss: 28.3415 - val_MinusLogProbMetric: 28.3415 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 529/1000
2023-10-26 07:44:40.586 
Epoch 529/1000 
	 loss: 27.6774, MinusLogProbMetric: 27.6774, val_loss: 28.3221, val_MinusLogProbMetric: 28.3221

Epoch 529: val_loss improved from 28.32963 to 28.32207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 27.6774 - MinusLogProbMetric: 27.6774 - val_loss: 28.3221 - val_MinusLogProbMetric: 28.3221 - lr: 8.3333e-05 - 43s/epoch - 221ms/step
Epoch 530/1000
2023-10-26 07:45:23.436 
Epoch 530/1000 
	 loss: 27.6823, MinusLogProbMetric: 27.6823, val_loss: 28.3265, val_MinusLogProbMetric: 28.3265

Epoch 530: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6823 - MinusLogProbMetric: 27.6823 - val_loss: 28.3265 - val_MinusLogProbMetric: 28.3265 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 531/1000
2023-10-26 07:46:05.717 
Epoch 531/1000 
	 loss: 27.6816, MinusLogProbMetric: 27.6816, val_loss: 28.4486, val_MinusLogProbMetric: 28.4486

Epoch 531: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6816 - MinusLogProbMetric: 27.6816 - val_loss: 28.4486 - val_MinusLogProbMetric: 28.4486 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 532/1000
2023-10-26 07:46:48.009 
Epoch 532/1000 
	 loss: 27.7035, MinusLogProbMetric: 27.7035, val_loss: 28.3700, val_MinusLogProbMetric: 28.3700

Epoch 532: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.7035 - MinusLogProbMetric: 27.7035 - val_loss: 28.3700 - val_MinusLogProbMetric: 28.3700 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 533/1000
2023-10-26 07:47:30.669 
Epoch 533/1000 
	 loss: 27.6921, MinusLogProbMetric: 27.6921, val_loss: 28.3546, val_MinusLogProbMetric: 28.3546

Epoch 533: val_loss did not improve from 28.32207
196/196 - 43s - loss: 27.6921 - MinusLogProbMetric: 27.6921 - val_loss: 28.3546 - val_MinusLogProbMetric: 28.3546 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 534/1000
2023-10-26 07:48:12.905 
Epoch 534/1000 
	 loss: 27.6839, MinusLogProbMetric: 27.6839, val_loss: 28.3820, val_MinusLogProbMetric: 28.3820

Epoch 534: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6839 - MinusLogProbMetric: 27.6839 - val_loss: 28.3820 - val_MinusLogProbMetric: 28.3820 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 535/1000
2023-10-26 07:48:55.333 
Epoch 535/1000 
	 loss: 27.6938, MinusLogProbMetric: 27.6938, val_loss: 28.4130, val_MinusLogProbMetric: 28.4130

Epoch 535: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6938 - MinusLogProbMetric: 27.6938 - val_loss: 28.4130 - val_MinusLogProbMetric: 28.4130 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 536/1000
2023-10-26 07:49:37.574 
Epoch 536/1000 
	 loss: 27.6870, MinusLogProbMetric: 27.6870, val_loss: 28.3823, val_MinusLogProbMetric: 28.3823

Epoch 536: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6870 - MinusLogProbMetric: 27.6870 - val_loss: 28.3823 - val_MinusLogProbMetric: 28.3823 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 537/1000
2023-10-26 07:50:19.245 
Epoch 537/1000 
	 loss: 27.6956, MinusLogProbMetric: 27.6956, val_loss: 28.3885, val_MinusLogProbMetric: 28.3885

Epoch 537: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6956 - MinusLogProbMetric: 27.6956 - val_loss: 28.3885 - val_MinusLogProbMetric: 28.3885 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 538/1000
2023-10-26 07:50:59.454 
Epoch 538/1000 
	 loss: 27.6816, MinusLogProbMetric: 27.6816, val_loss: 28.3649, val_MinusLogProbMetric: 28.3649

Epoch 538: val_loss did not improve from 28.32207
196/196 - 40s - loss: 27.6816 - MinusLogProbMetric: 27.6816 - val_loss: 28.3649 - val_MinusLogProbMetric: 28.3649 - lr: 8.3333e-05 - 40s/epoch - 205ms/step
Epoch 539/1000
2023-10-26 07:51:38.024 
Epoch 539/1000 
	 loss: 27.6915, MinusLogProbMetric: 27.6915, val_loss: 28.3283, val_MinusLogProbMetric: 28.3283

Epoch 539: val_loss did not improve from 28.32207
196/196 - 39s - loss: 27.6915 - MinusLogProbMetric: 27.6915 - val_loss: 28.3283 - val_MinusLogProbMetric: 28.3283 - lr: 8.3333e-05 - 39s/epoch - 197ms/step
Epoch 540/1000
2023-10-26 07:52:18.060 
Epoch 540/1000 
	 loss: 27.6932, MinusLogProbMetric: 27.6932, val_loss: 28.3525, val_MinusLogProbMetric: 28.3525

Epoch 540: val_loss did not improve from 28.32207
196/196 - 40s - loss: 27.6932 - MinusLogProbMetric: 27.6932 - val_loss: 28.3525 - val_MinusLogProbMetric: 28.3525 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 541/1000
2023-10-26 07:52:59.358 
Epoch 541/1000 
	 loss: 27.6904, MinusLogProbMetric: 27.6904, val_loss: 28.3987, val_MinusLogProbMetric: 28.3987

Epoch 541: val_loss did not improve from 28.32207
196/196 - 41s - loss: 27.6904 - MinusLogProbMetric: 27.6904 - val_loss: 28.3987 - val_MinusLogProbMetric: 28.3987 - lr: 8.3333e-05 - 41s/epoch - 211ms/step
Epoch 542/1000
2023-10-26 07:53:41.189 
Epoch 542/1000 
	 loss: 27.6947, MinusLogProbMetric: 27.6947, val_loss: 28.4276, val_MinusLogProbMetric: 28.4276

Epoch 542: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6947 - MinusLogProbMetric: 27.6947 - val_loss: 28.4276 - val_MinusLogProbMetric: 28.4276 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 543/1000
2023-10-26 07:54:23.707 
Epoch 543/1000 
	 loss: 27.6905, MinusLogProbMetric: 27.6905, val_loss: 28.3252, val_MinusLogProbMetric: 28.3252

Epoch 543: val_loss did not improve from 28.32207
196/196 - 43s - loss: 27.6905 - MinusLogProbMetric: 27.6905 - val_loss: 28.3252 - val_MinusLogProbMetric: 28.3252 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 544/1000
2023-10-26 07:55:05.041 
Epoch 544/1000 
	 loss: 27.6821, MinusLogProbMetric: 27.6821, val_loss: 28.3274, val_MinusLogProbMetric: 28.3274

Epoch 544: val_loss did not improve from 28.32207
196/196 - 41s - loss: 27.6821 - MinusLogProbMetric: 27.6821 - val_loss: 28.3274 - val_MinusLogProbMetric: 28.3274 - lr: 8.3333e-05 - 41s/epoch - 211ms/step
Epoch 545/1000
2023-10-26 07:55:46.919 
Epoch 545/1000 
	 loss: 27.6870, MinusLogProbMetric: 27.6870, val_loss: 28.3294, val_MinusLogProbMetric: 28.3294

Epoch 545: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6870 - MinusLogProbMetric: 27.6870 - val_loss: 28.3294 - val_MinusLogProbMetric: 28.3294 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 546/1000
2023-10-26 07:56:28.619 
Epoch 546/1000 
	 loss: 27.6859, MinusLogProbMetric: 27.6859, val_loss: 28.3522, val_MinusLogProbMetric: 28.3522

Epoch 546: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6859 - MinusLogProbMetric: 27.6859 - val_loss: 28.3522 - val_MinusLogProbMetric: 28.3522 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 547/1000
2023-10-26 07:57:10.593 
Epoch 547/1000 
	 loss: 27.6820, MinusLogProbMetric: 27.6820, val_loss: 28.3625, val_MinusLogProbMetric: 28.3625

Epoch 547: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6820 - MinusLogProbMetric: 27.6820 - val_loss: 28.3625 - val_MinusLogProbMetric: 28.3625 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 548/1000
2023-10-26 07:57:52.661 
Epoch 548/1000 
	 loss: 27.6751, MinusLogProbMetric: 27.6751, val_loss: 28.3437, val_MinusLogProbMetric: 28.3437

Epoch 548: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6751 - MinusLogProbMetric: 27.6751 - val_loss: 28.3437 - val_MinusLogProbMetric: 28.3437 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 549/1000
2023-10-26 07:58:26.860 
Epoch 549/1000 
	 loss: 27.6897, MinusLogProbMetric: 27.6897, val_loss: 28.3620, val_MinusLogProbMetric: 28.3620

Epoch 549: val_loss did not improve from 28.32207
196/196 - 34s - loss: 27.6897 - MinusLogProbMetric: 27.6897 - val_loss: 28.3620 - val_MinusLogProbMetric: 28.3620 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 550/1000
2023-10-26 07:59:00.713 
Epoch 550/1000 
	 loss: 27.6714, MinusLogProbMetric: 27.6714, val_loss: 28.3914, val_MinusLogProbMetric: 28.3914

Epoch 550: val_loss did not improve from 28.32207
196/196 - 34s - loss: 27.6714 - MinusLogProbMetric: 27.6714 - val_loss: 28.3914 - val_MinusLogProbMetric: 28.3914 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 551/1000
2023-10-26 07:59:34.319 
Epoch 551/1000 
	 loss: 27.6774, MinusLogProbMetric: 27.6774, val_loss: 28.3798, val_MinusLogProbMetric: 28.3798

Epoch 551: val_loss did not improve from 28.32207
196/196 - 34s - loss: 27.6774 - MinusLogProbMetric: 27.6774 - val_loss: 28.3798 - val_MinusLogProbMetric: 28.3798 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 552/1000
2023-10-26 08:00:09.449 
Epoch 552/1000 
	 loss: 27.6832, MinusLogProbMetric: 27.6832, val_loss: 28.3833, val_MinusLogProbMetric: 28.3833

Epoch 552: val_loss did not improve from 28.32207
196/196 - 35s - loss: 27.6832 - MinusLogProbMetric: 27.6832 - val_loss: 28.3833 - val_MinusLogProbMetric: 28.3833 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 553/1000
2023-10-26 08:00:49.835 
Epoch 553/1000 
	 loss: 27.6703, MinusLogProbMetric: 27.6703, val_loss: 28.3647, val_MinusLogProbMetric: 28.3647

Epoch 553: val_loss did not improve from 28.32207
196/196 - 40s - loss: 27.6703 - MinusLogProbMetric: 27.6703 - val_loss: 28.3647 - val_MinusLogProbMetric: 28.3647 - lr: 8.3333e-05 - 40s/epoch - 206ms/step
Epoch 554/1000
2023-10-26 08:01:32.611 
Epoch 554/1000 
	 loss: 27.6692, MinusLogProbMetric: 27.6692, val_loss: 28.3613, val_MinusLogProbMetric: 28.3613

Epoch 554: val_loss did not improve from 28.32207
196/196 - 43s - loss: 27.6692 - MinusLogProbMetric: 27.6692 - val_loss: 28.3613 - val_MinusLogProbMetric: 28.3613 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 555/1000
2023-10-26 08:02:12.692 
Epoch 555/1000 
	 loss: 27.6882, MinusLogProbMetric: 27.6882, val_loss: 28.3302, val_MinusLogProbMetric: 28.3302

Epoch 555: val_loss did not improve from 28.32207
196/196 - 40s - loss: 27.6882 - MinusLogProbMetric: 27.6882 - val_loss: 28.3302 - val_MinusLogProbMetric: 28.3302 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 556/1000
2023-10-26 08:02:55.010 
Epoch 556/1000 
	 loss: 27.6842, MinusLogProbMetric: 27.6842, val_loss: 28.3438, val_MinusLogProbMetric: 28.3438

Epoch 556: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6842 - MinusLogProbMetric: 27.6842 - val_loss: 28.3438 - val_MinusLogProbMetric: 28.3438 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 557/1000
2023-10-26 08:03:37.599 
Epoch 557/1000 
	 loss: 27.6814, MinusLogProbMetric: 27.6814, val_loss: 28.4260, val_MinusLogProbMetric: 28.4260

Epoch 557: val_loss did not improve from 28.32207
196/196 - 43s - loss: 27.6814 - MinusLogProbMetric: 27.6814 - val_loss: 28.4260 - val_MinusLogProbMetric: 28.4260 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 558/1000
2023-10-26 08:04:20.094 
Epoch 558/1000 
	 loss: 27.6699, MinusLogProbMetric: 27.6699, val_loss: 28.4171, val_MinusLogProbMetric: 28.4171

Epoch 558: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6699 - MinusLogProbMetric: 27.6699 - val_loss: 28.4171 - val_MinusLogProbMetric: 28.4171 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 559/1000
2023-10-26 08:05:02.348 
Epoch 559/1000 
	 loss: 27.6738, MinusLogProbMetric: 27.6738, val_loss: 28.3456, val_MinusLogProbMetric: 28.3456

Epoch 559: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6738 - MinusLogProbMetric: 27.6738 - val_loss: 28.3456 - val_MinusLogProbMetric: 28.3456 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 560/1000
2023-10-26 08:05:44.869 
Epoch 560/1000 
	 loss: 27.6789, MinusLogProbMetric: 27.6789, val_loss: 28.3846, val_MinusLogProbMetric: 28.3846

Epoch 560: val_loss did not improve from 28.32207
196/196 - 43s - loss: 27.6789 - MinusLogProbMetric: 27.6789 - val_loss: 28.3846 - val_MinusLogProbMetric: 28.3846 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 561/1000
2023-10-26 08:06:26.956 
Epoch 561/1000 
	 loss: 27.6781, MinusLogProbMetric: 27.6781, val_loss: 28.3389, val_MinusLogProbMetric: 28.3389

Epoch 561: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6781 - MinusLogProbMetric: 27.6781 - val_loss: 28.3389 - val_MinusLogProbMetric: 28.3389 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 562/1000
2023-10-26 08:07:09.561 
Epoch 562/1000 
	 loss: 27.6814, MinusLogProbMetric: 27.6814, val_loss: 28.3500, val_MinusLogProbMetric: 28.3500

Epoch 562: val_loss did not improve from 28.32207
196/196 - 43s - loss: 27.6814 - MinusLogProbMetric: 27.6814 - val_loss: 28.3500 - val_MinusLogProbMetric: 28.3500 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 563/1000
2023-10-26 08:07:51.847 
Epoch 563/1000 
	 loss: 27.6701, MinusLogProbMetric: 27.6701, val_loss: 28.3922, val_MinusLogProbMetric: 28.3922

Epoch 563: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6701 - MinusLogProbMetric: 27.6701 - val_loss: 28.3922 - val_MinusLogProbMetric: 28.3922 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 564/1000
2023-10-26 08:08:34.405 
Epoch 564/1000 
	 loss: 27.6702, MinusLogProbMetric: 27.6702, val_loss: 28.3338, val_MinusLogProbMetric: 28.3338

Epoch 564: val_loss did not improve from 28.32207
196/196 - 43s - loss: 27.6702 - MinusLogProbMetric: 27.6702 - val_loss: 28.3338 - val_MinusLogProbMetric: 28.3338 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 565/1000
2023-10-26 08:09:16.831 
Epoch 565/1000 
	 loss: 27.6758, MinusLogProbMetric: 27.6758, val_loss: 28.3416, val_MinusLogProbMetric: 28.3416

Epoch 565: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6758 - MinusLogProbMetric: 27.6758 - val_loss: 28.3416 - val_MinusLogProbMetric: 28.3416 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 566/1000
2023-10-26 08:09:58.841 
Epoch 566/1000 
	 loss: 27.6720, MinusLogProbMetric: 27.6720, val_loss: 28.3981, val_MinusLogProbMetric: 28.3981

Epoch 566: val_loss did not improve from 28.32207
196/196 - 42s - loss: 27.6720 - MinusLogProbMetric: 27.6720 - val_loss: 28.3981 - val_MinusLogProbMetric: 28.3981 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 567/1000
2023-10-26 08:10:32.597 
Epoch 567/1000 
	 loss: 27.6873, MinusLogProbMetric: 27.6873, val_loss: 28.3807, val_MinusLogProbMetric: 28.3807

Epoch 567: val_loss did not improve from 28.32207
196/196 - 34s - loss: 27.6873 - MinusLogProbMetric: 27.6873 - val_loss: 28.3807 - val_MinusLogProbMetric: 28.3807 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 568/1000
2023-10-26 08:11:08.159 
Epoch 568/1000 
	 loss: 27.6669, MinusLogProbMetric: 27.6669, val_loss: 28.3640, val_MinusLogProbMetric: 28.3640

Epoch 568: val_loss did not improve from 28.32207
196/196 - 36s - loss: 27.6669 - MinusLogProbMetric: 27.6669 - val_loss: 28.3640 - val_MinusLogProbMetric: 28.3640 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 569/1000
2023-10-26 08:11:44.206 
Epoch 569/1000 
	 loss: 27.6730, MinusLogProbMetric: 27.6730, val_loss: 28.4015, val_MinusLogProbMetric: 28.4015

Epoch 569: val_loss did not improve from 28.32207
196/196 - 36s - loss: 27.6730 - MinusLogProbMetric: 27.6730 - val_loss: 28.4015 - val_MinusLogProbMetric: 28.4015 - lr: 8.3333e-05 - 36s/epoch - 184ms/step
Epoch 570/1000
2023-10-26 08:12:19.831 
Epoch 570/1000 
	 loss: 27.6657, MinusLogProbMetric: 27.6657, val_loss: 28.3366, val_MinusLogProbMetric: 28.3366

Epoch 570: val_loss did not improve from 28.32207
196/196 - 36s - loss: 27.6657 - MinusLogProbMetric: 27.6657 - val_loss: 28.3366 - val_MinusLogProbMetric: 28.3366 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 571/1000
2023-10-26 08:12:56.511 
Epoch 571/1000 
	 loss: 27.6716, MinusLogProbMetric: 27.6716, val_loss: 28.3886, val_MinusLogProbMetric: 28.3886

Epoch 571: val_loss did not improve from 28.32207
196/196 - 37s - loss: 27.6716 - MinusLogProbMetric: 27.6716 - val_loss: 28.3886 - val_MinusLogProbMetric: 28.3886 - lr: 8.3333e-05 - 37s/epoch - 187ms/step
Epoch 572/1000
2023-10-26 08:13:32.982 
Epoch 572/1000 
	 loss: 27.6737, MinusLogProbMetric: 27.6737, val_loss: 28.3444, val_MinusLogProbMetric: 28.3444

Epoch 572: val_loss did not improve from 28.32207
196/196 - 36s - loss: 27.6737 - MinusLogProbMetric: 27.6737 - val_loss: 28.3444 - val_MinusLogProbMetric: 28.3444 - lr: 8.3333e-05 - 36s/epoch - 186ms/step
Epoch 573/1000
2023-10-26 08:14:10.039 
Epoch 573/1000 
	 loss: 27.6746, MinusLogProbMetric: 27.6746, val_loss: 28.3757, val_MinusLogProbMetric: 28.3757

Epoch 573: val_loss did not improve from 28.32207
196/196 - 37s - loss: 27.6746 - MinusLogProbMetric: 27.6746 - val_loss: 28.3757 - val_MinusLogProbMetric: 28.3757 - lr: 8.3333e-05 - 37s/epoch - 189ms/step
Epoch 574/1000
2023-10-26 08:14:49.712 
Epoch 574/1000 
	 loss: 27.6695, MinusLogProbMetric: 27.6695, val_loss: 28.4290, val_MinusLogProbMetric: 28.4290

Epoch 574: val_loss did not improve from 28.32207
196/196 - 40s - loss: 27.6695 - MinusLogProbMetric: 27.6695 - val_loss: 28.4290 - val_MinusLogProbMetric: 28.4290 - lr: 8.3333e-05 - 40s/epoch - 202ms/step
Epoch 575/1000
2023-10-26 08:15:23.739 
Epoch 575/1000 
	 loss: 27.6698, MinusLogProbMetric: 27.6698, val_loss: 28.3272, val_MinusLogProbMetric: 28.3272

Epoch 575: val_loss did not improve from 28.32207
196/196 - 34s - loss: 27.6698 - MinusLogProbMetric: 27.6698 - val_loss: 28.3272 - val_MinusLogProbMetric: 28.3272 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 576/1000
2023-10-26 08:16:03.790 
Epoch 576/1000 
	 loss: 27.6672, MinusLogProbMetric: 27.6672, val_loss: 28.3477, val_MinusLogProbMetric: 28.3477

Epoch 576: val_loss did not improve from 28.32207
196/196 - 40s - loss: 27.6672 - MinusLogProbMetric: 27.6672 - val_loss: 28.3477 - val_MinusLogProbMetric: 28.3477 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 577/1000
2023-10-26 08:16:40.456 
Epoch 577/1000 
	 loss: 27.6706, MinusLogProbMetric: 27.6706, val_loss: 28.3414, val_MinusLogProbMetric: 28.3414

Epoch 577: val_loss did not improve from 28.32207
196/196 - 37s - loss: 27.6706 - MinusLogProbMetric: 27.6706 - val_loss: 28.3414 - val_MinusLogProbMetric: 28.3414 - lr: 8.3333e-05 - 37s/epoch - 187ms/step
Epoch 578/1000
2023-10-26 08:17:14.358 
Epoch 578/1000 
	 loss: 27.6752, MinusLogProbMetric: 27.6752, val_loss: 28.3236, val_MinusLogProbMetric: 28.3236

Epoch 578: val_loss did not improve from 28.32207
196/196 - 34s - loss: 27.6752 - MinusLogProbMetric: 27.6752 - val_loss: 28.3236 - val_MinusLogProbMetric: 28.3236 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 579/1000
2023-10-26 08:17:54.975 
Epoch 579/1000 
	 loss: 27.6747, MinusLogProbMetric: 27.6747, val_loss: 28.3537, val_MinusLogProbMetric: 28.3537

Epoch 579: val_loss did not improve from 28.32207
196/196 - 41s - loss: 27.6747 - MinusLogProbMetric: 27.6747 - val_loss: 28.3537 - val_MinusLogProbMetric: 28.3537 - lr: 8.3333e-05 - 41s/epoch - 207ms/step
Epoch 580/1000
2023-10-26 08:18:31.392 
Epoch 580/1000 
	 loss: 27.6051, MinusLogProbMetric: 27.6051, val_loss: 28.3243, val_MinusLogProbMetric: 28.3243

Epoch 580: val_loss did not improve from 28.32207
196/196 - 36s - loss: 27.6051 - MinusLogProbMetric: 27.6051 - val_loss: 28.3243 - val_MinusLogProbMetric: 28.3243 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 581/1000
2023-10-26 08:19:08.818 
Epoch 581/1000 
	 loss: 27.5969, MinusLogProbMetric: 27.5969, val_loss: 28.3159, val_MinusLogProbMetric: 28.3159

Epoch 581: val_loss improved from 28.32207 to 28.31590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 38s - loss: 27.5969 - MinusLogProbMetric: 27.5969 - val_loss: 28.3159 - val_MinusLogProbMetric: 28.3159 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 582/1000
2023-10-26 08:19:47.773 
Epoch 582/1000 
	 loss: 27.6019, MinusLogProbMetric: 27.6019, val_loss: 28.2807, val_MinusLogProbMetric: 28.2807

Epoch 582: val_loss improved from 28.31590 to 28.28068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 39s - loss: 27.6019 - MinusLogProbMetric: 27.6019 - val_loss: 28.2807 - val_MinusLogProbMetric: 28.2807 - lr: 4.1667e-05 - 39s/epoch - 198ms/step
Epoch 583/1000
2023-10-26 08:20:22.451 
Epoch 583/1000 
	 loss: 27.6047, MinusLogProbMetric: 27.6047, val_loss: 28.2872, val_MinusLogProbMetric: 28.2872

Epoch 583: val_loss did not improve from 28.28068
196/196 - 34s - loss: 27.6047 - MinusLogProbMetric: 27.6047 - val_loss: 28.2872 - val_MinusLogProbMetric: 28.2872 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 584/1000
2023-10-26 08:21:04.321 
Epoch 584/1000 
	 loss: 27.5985, MinusLogProbMetric: 27.5985, val_loss: 28.2835, val_MinusLogProbMetric: 28.2835

Epoch 584: val_loss did not improve from 28.28068
196/196 - 42s - loss: 27.5985 - MinusLogProbMetric: 27.5985 - val_loss: 28.2835 - val_MinusLogProbMetric: 28.2835 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 585/1000
2023-10-26 08:21:38.488 
Epoch 585/1000 
	 loss: 27.6002, MinusLogProbMetric: 27.6002, val_loss: 28.2897, val_MinusLogProbMetric: 28.2897

Epoch 585: val_loss did not improve from 28.28068
196/196 - 34s - loss: 27.6002 - MinusLogProbMetric: 27.6002 - val_loss: 28.2897 - val_MinusLogProbMetric: 28.2897 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 586/1000
2023-10-26 08:22:13.236 
Epoch 586/1000 
	 loss: 27.6028, MinusLogProbMetric: 27.6028, val_loss: 28.2746, val_MinusLogProbMetric: 28.2746

Epoch 586: val_loss improved from 28.28068 to 28.27461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 35s - loss: 27.6028 - MinusLogProbMetric: 27.6028 - val_loss: 28.2746 - val_MinusLogProbMetric: 28.2746 - lr: 4.1667e-05 - 35s/epoch - 181ms/step
Epoch 587/1000
2023-10-26 08:22:53.305 
Epoch 587/1000 
	 loss: 27.5952, MinusLogProbMetric: 27.5952, val_loss: 28.3095, val_MinusLogProbMetric: 28.3095

Epoch 587: val_loss did not improve from 28.27461
196/196 - 39s - loss: 27.5952 - MinusLogProbMetric: 27.5952 - val_loss: 28.3095 - val_MinusLogProbMetric: 28.3095 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 588/1000
2023-10-26 08:23:28.391 
Epoch 588/1000 
	 loss: 27.5995, MinusLogProbMetric: 27.5995, val_loss: 28.2994, val_MinusLogProbMetric: 28.2994

Epoch 588: val_loss did not improve from 28.27461
196/196 - 35s - loss: 27.5995 - MinusLogProbMetric: 27.5995 - val_loss: 28.2994 - val_MinusLogProbMetric: 28.2994 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 589/1000
2023-10-26 08:24:04.893 
Epoch 589/1000 
	 loss: 27.5976, MinusLogProbMetric: 27.5976, val_loss: 28.2879, val_MinusLogProbMetric: 28.2879

Epoch 589: val_loss did not improve from 28.27461
196/196 - 36s - loss: 27.5976 - MinusLogProbMetric: 27.5976 - val_loss: 28.2879 - val_MinusLogProbMetric: 28.2879 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 590/1000
2023-10-26 08:24:41.866 
Epoch 590/1000 
	 loss: 27.6014, MinusLogProbMetric: 27.6014, val_loss: 28.2987, val_MinusLogProbMetric: 28.2987

Epoch 590: val_loss did not improve from 28.27461
196/196 - 37s - loss: 27.6014 - MinusLogProbMetric: 27.6014 - val_loss: 28.2987 - val_MinusLogProbMetric: 28.2987 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 591/1000
2023-10-26 08:25:16.993 
Epoch 591/1000 
	 loss: 27.5960, MinusLogProbMetric: 27.5960, val_loss: 28.2714, val_MinusLogProbMetric: 28.2714

Epoch 591: val_loss improved from 28.27461 to 28.27141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 36s - loss: 27.5960 - MinusLogProbMetric: 27.5960 - val_loss: 28.2714 - val_MinusLogProbMetric: 28.2714 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 592/1000
2023-10-26 08:25:58.344 
Epoch 592/1000 
	 loss: 27.6030, MinusLogProbMetric: 27.6030, val_loss: 28.2914, val_MinusLogProbMetric: 28.2914

Epoch 592: val_loss did not improve from 28.27141
196/196 - 40s - loss: 27.6030 - MinusLogProbMetric: 27.6030 - val_loss: 28.2914 - val_MinusLogProbMetric: 28.2914 - lr: 4.1667e-05 - 40s/epoch - 207ms/step
Epoch 593/1000
2023-10-26 08:26:32.853 
Epoch 593/1000 
	 loss: 27.5976, MinusLogProbMetric: 27.5976, val_loss: 28.2813, val_MinusLogProbMetric: 28.2813

Epoch 593: val_loss did not improve from 28.27141
196/196 - 35s - loss: 27.5976 - MinusLogProbMetric: 27.5976 - val_loss: 28.2813 - val_MinusLogProbMetric: 28.2813 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 594/1000
2023-10-26 08:27:12.349 
Epoch 594/1000 
	 loss: 27.5955, MinusLogProbMetric: 27.5955, val_loss: 28.3043, val_MinusLogProbMetric: 28.3043

Epoch 594: val_loss did not improve from 28.27141
196/196 - 39s - loss: 27.5955 - MinusLogProbMetric: 27.5955 - val_loss: 28.3043 - val_MinusLogProbMetric: 28.3043 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 595/1000
2023-10-26 08:27:49.839 
Epoch 595/1000 
	 loss: 27.6009, MinusLogProbMetric: 27.6009, val_loss: 28.2755, val_MinusLogProbMetric: 28.2755

Epoch 595: val_loss did not improve from 28.27141
196/196 - 37s - loss: 27.6009 - MinusLogProbMetric: 27.6009 - val_loss: 28.2755 - val_MinusLogProbMetric: 28.2755 - lr: 4.1667e-05 - 37s/epoch - 191ms/step
Epoch 596/1000
2023-10-26 08:28:23.678 
Epoch 596/1000 
	 loss: 27.5946, MinusLogProbMetric: 27.5946, val_loss: 28.3045, val_MinusLogProbMetric: 28.3045

Epoch 596: val_loss did not improve from 28.27141
196/196 - 34s - loss: 27.5946 - MinusLogProbMetric: 27.5946 - val_loss: 28.3045 - val_MinusLogProbMetric: 28.3045 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 597/1000
2023-10-26 08:29:04.158 
Epoch 597/1000 
	 loss: 27.5961, MinusLogProbMetric: 27.5961, val_loss: 28.2820, val_MinusLogProbMetric: 28.2820

Epoch 597: val_loss did not improve from 28.27141
196/196 - 40s - loss: 27.5961 - MinusLogProbMetric: 27.5961 - val_loss: 28.2820 - val_MinusLogProbMetric: 28.2820 - lr: 4.1667e-05 - 40s/epoch - 207ms/step
Epoch 598/1000
2023-10-26 08:29:44.808 
Epoch 598/1000 
	 loss: 27.5979, MinusLogProbMetric: 27.5979, val_loss: 28.2933, val_MinusLogProbMetric: 28.2933

Epoch 598: val_loss did not improve from 28.27141
196/196 - 41s - loss: 27.5979 - MinusLogProbMetric: 27.5979 - val_loss: 28.2933 - val_MinusLogProbMetric: 28.2933 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 599/1000
2023-10-26 08:30:26.182 
Epoch 599/1000 
	 loss: 27.5936, MinusLogProbMetric: 27.5936, val_loss: 28.2695, val_MinusLogProbMetric: 28.2695

Epoch 599: val_loss improved from 28.27141 to 28.26947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5936 - MinusLogProbMetric: 27.5936 - val_loss: 28.2695 - val_MinusLogProbMetric: 28.2695 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 600/1000
2023-10-26 08:31:08.968 
Epoch 600/1000 
	 loss: 27.5936, MinusLogProbMetric: 27.5936, val_loss: 28.2788, val_MinusLogProbMetric: 28.2788

Epoch 600: val_loss did not improve from 28.26947
196/196 - 42s - loss: 27.5936 - MinusLogProbMetric: 27.5936 - val_loss: 28.2788 - val_MinusLogProbMetric: 28.2788 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 601/1000
2023-10-26 08:31:50.447 
Epoch 601/1000 
	 loss: 27.5991, MinusLogProbMetric: 27.5991, val_loss: 28.2949, val_MinusLogProbMetric: 28.2949

Epoch 601: val_loss did not improve from 28.26947
196/196 - 41s - loss: 27.5991 - MinusLogProbMetric: 27.5991 - val_loss: 28.2949 - val_MinusLogProbMetric: 28.2949 - lr: 4.1667e-05 - 41s/epoch - 212ms/step
Epoch 602/1000
2023-10-26 08:32:32.106 
Epoch 602/1000 
	 loss: 27.5943, MinusLogProbMetric: 27.5943, val_loss: 28.2668, val_MinusLogProbMetric: 28.2668

Epoch 602: val_loss improved from 28.26947 to 28.26681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5943 - MinusLogProbMetric: 27.5943 - val_loss: 28.2668 - val_MinusLogProbMetric: 28.2668 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 603/1000
2023-10-26 08:33:14.022 
Epoch 603/1000 
	 loss: 27.5967, MinusLogProbMetric: 27.5967, val_loss: 28.2895, val_MinusLogProbMetric: 28.2895

Epoch 603: val_loss did not improve from 28.26681
196/196 - 41s - loss: 27.5967 - MinusLogProbMetric: 27.5967 - val_loss: 28.2895 - val_MinusLogProbMetric: 28.2895 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 604/1000
2023-10-26 08:33:55.329 
Epoch 604/1000 
	 loss: 27.5942, MinusLogProbMetric: 27.5942, val_loss: 28.3088, val_MinusLogProbMetric: 28.3088

Epoch 604: val_loss did not improve from 28.26681
196/196 - 41s - loss: 27.5942 - MinusLogProbMetric: 27.5942 - val_loss: 28.3088 - val_MinusLogProbMetric: 28.3088 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 605/1000
2023-10-26 08:34:36.863 
Epoch 605/1000 
	 loss: 27.5966, MinusLogProbMetric: 27.5966, val_loss: 28.3292, val_MinusLogProbMetric: 28.3292

Epoch 605: val_loss did not improve from 28.26681
196/196 - 42s - loss: 27.5966 - MinusLogProbMetric: 27.5966 - val_loss: 28.3292 - val_MinusLogProbMetric: 28.3292 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 606/1000
2023-10-26 08:35:18.215 
Epoch 606/1000 
	 loss: 27.6010, MinusLogProbMetric: 27.6010, val_loss: 28.3036, val_MinusLogProbMetric: 28.3036

Epoch 606: val_loss did not improve from 28.26681
196/196 - 41s - loss: 27.6010 - MinusLogProbMetric: 27.6010 - val_loss: 28.3036 - val_MinusLogProbMetric: 28.3036 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 607/1000
2023-10-26 08:36:00.005 
Epoch 607/1000 
	 loss: 27.5937, MinusLogProbMetric: 27.5937, val_loss: 28.2835, val_MinusLogProbMetric: 28.2835

Epoch 607: val_loss did not improve from 28.26681
196/196 - 42s - loss: 27.5937 - MinusLogProbMetric: 27.5937 - val_loss: 28.2835 - val_MinusLogProbMetric: 28.2835 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 608/1000
2023-10-26 08:36:41.636 
Epoch 608/1000 
	 loss: 27.5990, MinusLogProbMetric: 27.5990, val_loss: 28.2645, val_MinusLogProbMetric: 28.2645

Epoch 608: val_loss improved from 28.26681 to 28.26455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5990 - MinusLogProbMetric: 27.5990 - val_loss: 28.2645 - val_MinusLogProbMetric: 28.2645 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 609/1000
2023-10-26 08:37:23.617 
Epoch 609/1000 
	 loss: 27.5937, MinusLogProbMetric: 27.5937, val_loss: 28.2843, val_MinusLogProbMetric: 28.2843

Epoch 609: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5937 - MinusLogProbMetric: 27.5937 - val_loss: 28.2843 - val_MinusLogProbMetric: 28.2843 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 610/1000
2023-10-26 08:38:05.278 
Epoch 610/1000 
	 loss: 27.5930, MinusLogProbMetric: 27.5930, val_loss: 28.3068, val_MinusLogProbMetric: 28.3068

Epoch 610: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5930 - MinusLogProbMetric: 27.5930 - val_loss: 28.3068 - val_MinusLogProbMetric: 28.3068 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 611/1000
2023-10-26 08:38:47.096 
Epoch 611/1000 
	 loss: 27.5956, MinusLogProbMetric: 27.5956, val_loss: 28.2976, val_MinusLogProbMetric: 28.2976

Epoch 611: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5956 - MinusLogProbMetric: 27.5956 - val_loss: 28.2976 - val_MinusLogProbMetric: 28.2976 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 612/1000
2023-10-26 08:39:29.163 
Epoch 612/1000 
	 loss: 27.5982, MinusLogProbMetric: 27.5982, val_loss: 28.2704, val_MinusLogProbMetric: 28.2704

Epoch 612: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5982 - MinusLogProbMetric: 27.5982 - val_loss: 28.2704 - val_MinusLogProbMetric: 28.2704 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 613/1000
2023-10-26 08:40:10.699 
Epoch 613/1000 
	 loss: 27.5929, MinusLogProbMetric: 27.5929, val_loss: 28.2939, val_MinusLogProbMetric: 28.2939

Epoch 613: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5929 - MinusLogProbMetric: 27.5929 - val_loss: 28.2939 - val_MinusLogProbMetric: 28.2939 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 614/1000
2023-10-26 08:40:51.674 
Epoch 614/1000 
	 loss: 27.5935, MinusLogProbMetric: 27.5935, val_loss: 28.2667, val_MinusLogProbMetric: 28.2667

Epoch 614: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5935 - MinusLogProbMetric: 27.5935 - val_loss: 28.2667 - val_MinusLogProbMetric: 28.2667 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 615/1000
2023-10-26 08:41:32.983 
Epoch 615/1000 
	 loss: 27.6001, MinusLogProbMetric: 27.6001, val_loss: 28.2909, val_MinusLogProbMetric: 28.2909

Epoch 615: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.6001 - MinusLogProbMetric: 27.6001 - val_loss: 28.2909 - val_MinusLogProbMetric: 28.2909 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 616/1000
2023-10-26 08:42:14.312 
Epoch 616/1000 
	 loss: 27.5925, MinusLogProbMetric: 27.5925, val_loss: 28.2882, val_MinusLogProbMetric: 28.2882

Epoch 616: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5925 - MinusLogProbMetric: 27.5925 - val_loss: 28.2882 - val_MinusLogProbMetric: 28.2882 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 617/1000
2023-10-26 08:42:55.888 
Epoch 617/1000 
	 loss: 27.5915, MinusLogProbMetric: 27.5915, val_loss: 28.3040, val_MinusLogProbMetric: 28.3040

Epoch 617: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5915 - MinusLogProbMetric: 27.5915 - val_loss: 28.3040 - val_MinusLogProbMetric: 28.3040 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 618/1000
2023-10-26 08:43:37.453 
Epoch 618/1000 
	 loss: 27.5959, MinusLogProbMetric: 27.5959, val_loss: 28.2846, val_MinusLogProbMetric: 28.2846

Epoch 618: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5959 - MinusLogProbMetric: 27.5959 - val_loss: 28.2846 - val_MinusLogProbMetric: 28.2846 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 619/1000
2023-10-26 08:44:18.774 
Epoch 619/1000 
	 loss: 27.5932, MinusLogProbMetric: 27.5932, val_loss: 28.3383, val_MinusLogProbMetric: 28.3383

Epoch 619: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5932 - MinusLogProbMetric: 27.5932 - val_loss: 28.3383 - val_MinusLogProbMetric: 28.3383 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 620/1000
2023-10-26 08:45:00.621 
Epoch 620/1000 
	 loss: 27.5984, MinusLogProbMetric: 27.5984, val_loss: 28.2889, val_MinusLogProbMetric: 28.2889

Epoch 620: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5984 - MinusLogProbMetric: 27.5984 - val_loss: 28.2889 - val_MinusLogProbMetric: 28.2889 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 621/1000
2023-10-26 08:45:41.658 
Epoch 621/1000 
	 loss: 27.5932, MinusLogProbMetric: 27.5932, val_loss: 28.2911, val_MinusLogProbMetric: 28.2911

Epoch 621: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5932 - MinusLogProbMetric: 27.5932 - val_loss: 28.2911 - val_MinusLogProbMetric: 28.2911 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 622/1000
2023-10-26 08:46:23.448 
Epoch 622/1000 
	 loss: 27.5906, MinusLogProbMetric: 27.5906, val_loss: 28.2848, val_MinusLogProbMetric: 28.2848

Epoch 622: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5906 - MinusLogProbMetric: 27.5906 - val_loss: 28.2848 - val_MinusLogProbMetric: 28.2848 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 623/1000
2023-10-26 08:47:05.018 
Epoch 623/1000 
	 loss: 27.5869, MinusLogProbMetric: 27.5869, val_loss: 28.2832, val_MinusLogProbMetric: 28.2832

Epoch 623: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5869 - MinusLogProbMetric: 27.5869 - val_loss: 28.2832 - val_MinusLogProbMetric: 28.2832 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 624/1000
2023-10-26 08:47:46.680 
Epoch 624/1000 
	 loss: 27.5914, MinusLogProbMetric: 27.5914, val_loss: 28.3062, val_MinusLogProbMetric: 28.3062

Epoch 624: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5914 - MinusLogProbMetric: 27.5914 - val_loss: 28.3062 - val_MinusLogProbMetric: 28.3062 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 625/1000
2023-10-26 08:48:27.913 
Epoch 625/1000 
	 loss: 27.5899, MinusLogProbMetric: 27.5899, val_loss: 28.2992, val_MinusLogProbMetric: 28.2992

Epoch 625: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5899 - MinusLogProbMetric: 27.5899 - val_loss: 28.2992 - val_MinusLogProbMetric: 28.2992 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 626/1000
2023-10-26 08:49:09.767 
Epoch 626/1000 
	 loss: 27.5951, MinusLogProbMetric: 27.5951, val_loss: 28.3097, val_MinusLogProbMetric: 28.3097

Epoch 626: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5951 - MinusLogProbMetric: 27.5951 - val_loss: 28.3097 - val_MinusLogProbMetric: 28.3097 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 627/1000
2023-10-26 08:49:51.465 
Epoch 627/1000 
	 loss: 27.5925, MinusLogProbMetric: 27.5925, val_loss: 28.3413, val_MinusLogProbMetric: 28.3413

Epoch 627: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5925 - MinusLogProbMetric: 27.5925 - val_loss: 28.3413 - val_MinusLogProbMetric: 28.3413 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 628/1000
2023-10-26 08:50:32.645 
Epoch 628/1000 
	 loss: 27.5939, MinusLogProbMetric: 27.5939, val_loss: 28.2682, val_MinusLogProbMetric: 28.2682

Epoch 628: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5939 - MinusLogProbMetric: 27.5939 - val_loss: 28.2682 - val_MinusLogProbMetric: 28.2682 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 629/1000
2023-10-26 08:51:14.207 
Epoch 629/1000 
	 loss: 27.5889, MinusLogProbMetric: 27.5889, val_loss: 28.3027, val_MinusLogProbMetric: 28.3027

Epoch 629: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5889 - MinusLogProbMetric: 27.5889 - val_loss: 28.3027 - val_MinusLogProbMetric: 28.3027 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 630/1000
2023-10-26 08:51:55.577 
Epoch 630/1000 
	 loss: 27.5867, MinusLogProbMetric: 27.5867, val_loss: 28.2970, val_MinusLogProbMetric: 28.2970

Epoch 630: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5867 - MinusLogProbMetric: 27.5867 - val_loss: 28.2970 - val_MinusLogProbMetric: 28.2970 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 631/1000
2023-10-26 08:52:37.711 
Epoch 631/1000 
	 loss: 27.5933, MinusLogProbMetric: 27.5933, val_loss: 28.2827, val_MinusLogProbMetric: 28.2827

Epoch 631: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5933 - MinusLogProbMetric: 27.5933 - val_loss: 28.2827 - val_MinusLogProbMetric: 28.2827 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 632/1000
2023-10-26 08:53:19.643 
Epoch 632/1000 
	 loss: 27.5882, MinusLogProbMetric: 27.5882, val_loss: 28.2844, val_MinusLogProbMetric: 28.2844

Epoch 632: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5882 - MinusLogProbMetric: 27.5882 - val_loss: 28.2844 - val_MinusLogProbMetric: 28.2844 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 633/1000
2023-10-26 08:54:01.680 
Epoch 633/1000 
	 loss: 27.5980, MinusLogProbMetric: 27.5980, val_loss: 28.2952, val_MinusLogProbMetric: 28.2952

Epoch 633: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5980 - MinusLogProbMetric: 27.5980 - val_loss: 28.2952 - val_MinusLogProbMetric: 28.2952 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 634/1000
2023-10-26 08:54:43.453 
Epoch 634/1000 
	 loss: 27.5897, MinusLogProbMetric: 27.5897, val_loss: 28.2837, val_MinusLogProbMetric: 28.2837

Epoch 634: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5897 - MinusLogProbMetric: 27.5897 - val_loss: 28.2837 - val_MinusLogProbMetric: 28.2837 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 635/1000
2023-10-26 08:55:25.160 
Epoch 635/1000 
	 loss: 27.5889, MinusLogProbMetric: 27.5889, val_loss: 28.2704, val_MinusLogProbMetric: 28.2704

Epoch 635: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5889 - MinusLogProbMetric: 27.5889 - val_loss: 28.2704 - val_MinusLogProbMetric: 28.2704 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 636/1000
2023-10-26 08:56:06.828 
Epoch 636/1000 
	 loss: 27.5900, MinusLogProbMetric: 27.5900, val_loss: 28.2662, val_MinusLogProbMetric: 28.2662

Epoch 636: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5900 - MinusLogProbMetric: 27.5900 - val_loss: 28.2662 - val_MinusLogProbMetric: 28.2662 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 637/1000
2023-10-26 08:56:48.531 
Epoch 637/1000 
	 loss: 27.5900, MinusLogProbMetric: 27.5900, val_loss: 28.3070, val_MinusLogProbMetric: 28.3070

Epoch 637: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5900 - MinusLogProbMetric: 27.5900 - val_loss: 28.3070 - val_MinusLogProbMetric: 28.3070 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 638/1000
2023-10-26 08:57:30.256 
Epoch 638/1000 
	 loss: 27.5880, MinusLogProbMetric: 27.5880, val_loss: 28.2795, val_MinusLogProbMetric: 28.2795

Epoch 638: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5880 - MinusLogProbMetric: 27.5880 - val_loss: 28.2795 - val_MinusLogProbMetric: 28.2795 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 639/1000
2023-10-26 08:58:12.091 
Epoch 639/1000 
	 loss: 27.5837, MinusLogProbMetric: 27.5837, val_loss: 28.3020, val_MinusLogProbMetric: 28.3020

Epoch 639: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5837 - MinusLogProbMetric: 27.5837 - val_loss: 28.3020 - val_MinusLogProbMetric: 28.3020 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 640/1000
2023-10-26 08:58:53.017 
Epoch 640/1000 
	 loss: 27.5878, MinusLogProbMetric: 27.5878, val_loss: 28.3011, val_MinusLogProbMetric: 28.3011

Epoch 640: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5878 - MinusLogProbMetric: 27.5878 - val_loss: 28.3011 - val_MinusLogProbMetric: 28.3011 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 641/1000
2023-10-26 08:59:34.559 
Epoch 641/1000 
	 loss: 27.5877, MinusLogProbMetric: 27.5877, val_loss: 28.2709, val_MinusLogProbMetric: 28.2709

Epoch 641: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5877 - MinusLogProbMetric: 27.5877 - val_loss: 28.2709 - val_MinusLogProbMetric: 28.2709 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 642/1000
2023-10-26 09:00:15.731 
Epoch 642/1000 
	 loss: 27.5845, MinusLogProbMetric: 27.5845, val_loss: 28.2901, val_MinusLogProbMetric: 28.2901

Epoch 642: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5845 - MinusLogProbMetric: 27.5845 - val_loss: 28.2901 - val_MinusLogProbMetric: 28.2901 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 643/1000
2023-10-26 09:00:56.900 
Epoch 643/1000 
	 loss: 27.5837, MinusLogProbMetric: 27.5837, val_loss: 28.2710, val_MinusLogProbMetric: 28.2710

Epoch 643: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5837 - MinusLogProbMetric: 27.5837 - val_loss: 28.2710 - val_MinusLogProbMetric: 28.2710 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 644/1000
2023-10-26 09:01:37.730 
Epoch 644/1000 
	 loss: 27.5917, MinusLogProbMetric: 27.5917, val_loss: 28.2887, val_MinusLogProbMetric: 28.2887

Epoch 644: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5917 - MinusLogProbMetric: 27.5917 - val_loss: 28.2887 - val_MinusLogProbMetric: 28.2887 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 645/1000
2023-10-26 09:02:19.136 
Epoch 645/1000 
	 loss: 27.5836, MinusLogProbMetric: 27.5836, val_loss: 28.2787, val_MinusLogProbMetric: 28.2787

Epoch 645: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5836 - MinusLogProbMetric: 27.5836 - val_loss: 28.2787 - val_MinusLogProbMetric: 28.2787 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 646/1000
2023-10-26 09:03:00.317 
Epoch 646/1000 
	 loss: 27.5842, MinusLogProbMetric: 27.5842, val_loss: 28.2890, val_MinusLogProbMetric: 28.2890

Epoch 646: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5842 - MinusLogProbMetric: 27.5842 - val_loss: 28.2890 - val_MinusLogProbMetric: 28.2890 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 647/1000
2023-10-26 09:03:41.738 
Epoch 647/1000 
	 loss: 27.5849, MinusLogProbMetric: 27.5849, val_loss: 28.2720, val_MinusLogProbMetric: 28.2720

Epoch 647: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5849 - MinusLogProbMetric: 27.5849 - val_loss: 28.2720 - val_MinusLogProbMetric: 28.2720 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 648/1000
2023-10-26 09:04:23.374 
Epoch 648/1000 
	 loss: 27.5811, MinusLogProbMetric: 27.5811, val_loss: 28.2928, val_MinusLogProbMetric: 28.2928

Epoch 648: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5811 - MinusLogProbMetric: 27.5811 - val_loss: 28.2928 - val_MinusLogProbMetric: 28.2928 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 649/1000
2023-10-26 09:05:05.186 
Epoch 649/1000 
	 loss: 27.5835, MinusLogProbMetric: 27.5835, val_loss: 28.2930, val_MinusLogProbMetric: 28.2930

Epoch 649: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5835 - MinusLogProbMetric: 27.5835 - val_loss: 28.2930 - val_MinusLogProbMetric: 28.2930 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 650/1000
2023-10-26 09:05:46.268 
Epoch 650/1000 
	 loss: 27.5875, MinusLogProbMetric: 27.5875, val_loss: 28.2731, val_MinusLogProbMetric: 28.2731

Epoch 650: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5875 - MinusLogProbMetric: 27.5875 - val_loss: 28.2731 - val_MinusLogProbMetric: 28.2731 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 651/1000
2023-10-26 09:06:27.834 
Epoch 651/1000 
	 loss: 27.5840, MinusLogProbMetric: 27.5840, val_loss: 28.2868, val_MinusLogProbMetric: 28.2868

Epoch 651: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5840 - MinusLogProbMetric: 27.5840 - val_loss: 28.2868 - val_MinusLogProbMetric: 28.2868 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 652/1000
2023-10-26 09:07:09.265 
Epoch 652/1000 
	 loss: 27.5861, MinusLogProbMetric: 27.5861, val_loss: 28.3004, val_MinusLogProbMetric: 28.3004

Epoch 652: val_loss did not improve from 28.26455
196/196 - 41s - loss: 27.5861 - MinusLogProbMetric: 27.5861 - val_loss: 28.3004 - val_MinusLogProbMetric: 28.3004 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 653/1000
2023-10-26 09:07:51.030 
Epoch 653/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 28.2806, val_MinusLogProbMetric: 28.2806

Epoch 653: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 28.2806 - val_MinusLogProbMetric: 28.2806 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 654/1000
2023-10-26 09:08:32.636 
Epoch 654/1000 
	 loss: 27.5835, MinusLogProbMetric: 27.5835, val_loss: 28.2841, val_MinusLogProbMetric: 28.2841

Epoch 654: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5835 - MinusLogProbMetric: 27.5835 - val_loss: 28.2841 - val_MinusLogProbMetric: 28.2841 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 655/1000
2023-10-26 09:09:14.287 
Epoch 655/1000 
	 loss: 27.5814, MinusLogProbMetric: 27.5814, val_loss: 28.2810, val_MinusLogProbMetric: 28.2810

Epoch 655: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5814 - MinusLogProbMetric: 27.5814 - val_loss: 28.2810 - val_MinusLogProbMetric: 28.2810 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 656/1000
2023-10-26 09:09:55.890 
Epoch 656/1000 
	 loss: 27.5809, MinusLogProbMetric: 27.5809, val_loss: 28.2853, val_MinusLogProbMetric: 28.2853

Epoch 656: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5809 - MinusLogProbMetric: 27.5809 - val_loss: 28.2853 - val_MinusLogProbMetric: 28.2853 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 657/1000
2023-10-26 09:10:37.709 
Epoch 657/1000 
	 loss: 27.5874, MinusLogProbMetric: 27.5874, val_loss: 28.3031, val_MinusLogProbMetric: 28.3031

Epoch 657: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5874 - MinusLogProbMetric: 27.5874 - val_loss: 28.3031 - val_MinusLogProbMetric: 28.3031 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 658/1000
2023-10-26 09:11:19.387 
Epoch 658/1000 
	 loss: 27.5806, MinusLogProbMetric: 27.5806, val_loss: 28.2853, val_MinusLogProbMetric: 28.2853

Epoch 658: val_loss did not improve from 28.26455
196/196 - 42s - loss: 27.5806 - MinusLogProbMetric: 27.5806 - val_loss: 28.2853 - val_MinusLogProbMetric: 28.2853 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 659/1000
2023-10-26 09:12:00.769 
Epoch 659/1000 
	 loss: 27.5525, MinusLogProbMetric: 27.5525, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 659: val_loss improved from 28.26455 to 28.25804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5525 - MinusLogProbMetric: 27.5525 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 660/1000
2023-10-26 09:12:43.109 
Epoch 660/1000 
	 loss: 27.5508, MinusLogProbMetric: 27.5508, val_loss: 28.2641, val_MinusLogProbMetric: 28.2641

Epoch 660: val_loss did not improve from 28.25804
196/196 - 42s - loss: 27.5508 - MinusLogProbMetric: 27.5508 - val_loss: 28.2641 - val_MinusLogProbMetric: 28.2641 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 661/1000
2023-10-26 09:13:24.857 
Epoch 661/1000 
	 loss: 27.5512, MinusLogProbMetric: 27.5512, val_loss: 28.2826, val_MinusLogProbMetric: 28.2826

Epoch 661: val_loss did not improve from 28.25804
196/196 - 42s - loss: 27.5512 - MinusLogProbMetric: 27.5512 - val_loss: 28.2826 - val_MinusLogProbMetric: 28.2826 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 662/1000
2023-10-26 09:14:06.637 
Epoch 662/1000 
	 loss: 27.5502, MinusLogProbMetric: 27.5502, val_loss: 28.2544, val_MinusLogProbMetric: 28.2544

Epoch 662: val_loss improved from 28.25804 to 28.25435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5502 - MinusLogProbMetric: 27.5502 - val_loss: 28.2544 - val_MinusLogProbMetric: 28.2544 - lr: 2.0833e-05 - 42s/epoch - 217ms/step
Epoch 663/1000
2023-10-26 09:14:49.204 
Epoch 663/1000 
	 loss: 27.5494, MinusLogProbMetric: 27.5494, val_loss: 28.2596, val_MinusLogProbMetric: 28.2596

Epoch 663: val_loss did not improve from 28.25435
196/196 - 42s - loss: 27.5494 - MinusLogProbMetric: 27.5494 - val_loss: 28.2596 - val_MinusLogProbMetric: 28.2596 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 664/1000
2023-10-26 09:15:31.135 
Epoch 664/1000 
	 loss: 27.5517, MinusLogProbMetric: 27.5517, val_loss: 28.2752, val_MinusLogProbMetric: 28.2752

Epoch 664: val_loss did not improve from 28.25435
196/196 - 42s - loss: 27.5517 - MinusLogProbMetric: 27.5517 - val_loss: 28.2752 - val_MinusLogProbMetric: 28.2752 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 665/1000
2023-10-26 09:16:12.885 
Epoch 665/1000 
	 loss: 27.5492, MinusLogProbMetric: 27.5492, val_loss: 28.2682, val_MinusLogProbMetric: 28.2682

Epoch 665: val_loss did not improve from 28.25435
196/196 - 42s - loss: 27.5492 - MinusLogProbMetric: 27.5492 - val_loss: 28.2682 - val_MinusLogProbMetric: 28.2682 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 666/1000
2023-10-26 09:16:54.845 
Epoch 666/1000 
	 loss: 27.5487, MinusLogProbMetric: 27.5487, val_loss: 28.2612, val_MinusLogProbMetric: 28.2612

Epoch 666: val_loss did not improve from 28.25435
196/196 - 42s - loss: 27.5487 - MinusLogProbMetric: 27.5487 - val_loss: 28.2612 - val_MinusLogProbMetric: 28.2612 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 667/1000
2023-10-26 09:17:36.649 
Epoch 667/1000 
	 loss: 27.5503, MinusLogProbMetric: 27.5503, val_loss: 28.2615, val_MinusLogProbMetric: 28.2615

Epoch 667: val_loss did not improve from 28.25435
196/196 - 42s - loss: 27.5503 - MinusLogProbMetric: 27.5503 - val_loss: 28.2615 - val_MinusLogProbMetric: 28.2615 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 668/1000
2023-10-26 09:18:18.155 
Epoch 668/1000 
	 loss: 27.5506, MinusLogProbMetric: 27.5506, val_loss: 28.2611, val_MinusLogProbMetric: 28.2611

Epoch 668: val_loss did not improve from 28.25435
196/196 - 42s - loss: 27.5506 - MinusLogProbMetric: 27.5506 - val_loss: 28.2611 - val_MinusLogProbMetric: 28.2611 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 669/1000
2023-10-26 09:18:58.542 
Epoch 669/1000 
	 loss: 27.5510, MinusLogProbMetric: 27.5510, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 669: val_loss did not improve from 28.25435
196/196 - 40s - loss: 27.5510 - MinusLogProbMetric: 27.5510 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 2.0833e-05 - 40s/epoch - 206ms/step
Epoch 670/1000
2023-10-26 09:19:40.207 
Epoch 670/1000 
	 loss: 27.5519, MinusLogProbMetric: 27.5519, val_loss: 28.2648, val_MinusLogProbMetric: 28.2648

Epoch 670: val_loss did not improve from 28.25435
196/196 - 42s - loss: 27.5519 - MinusLogProbMetric: 27.5519 - val_loss: 28.2648 - val_MinusLogProbMetric: 28.2648 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 671/1000
2023-10-26 09:20:21.775 
Epoch 671/1000 
	 loss: 27.5489, MinusLogProbMetric: 27.5489, val_loss: 28.2523, val_MinusLogProbMetric: 28.2523

Epoch 671: val_loss improved from 28.25435 to 28.25225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5489 - MinusLogProbMetric: 27.5489 - val_loss: 28.2523 - val_MinusLogProbMetric: 28.2523 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 672/1000
2023-10-26 09:21:03.792 
Epoch 672/1000 
	 loss: 27.5520, MinusLogProbMetric: 27.5520, val_loss: 28.2565, val_MinusLogProbMetric: 28.2565

Epoch 672: val_loss did not improve from 28.25225
196/196 - 41s - loss: 27.5520 - MinusLogProbMetric: 27.5520 - val_loss: 28.2565 - val_MinusLogProbMetric: 28.2565 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 673/1000
2023-10-26 09:21:44.257 
Epoch 673/1000 
	 loss: 27.5488, MinusLogProbMetric: 27.5488, val_loss: 28.2596, val_MinusLogProbMetric: 28.2596

Epoch 673: val_loss did not improve from 28.25225
196/196 - 40s - loss: 27.5488 - MinusLogProbMetric: 27.5488 - val_loss: 28.2596 - val_MinusLogProbMetric: 28.2596 - lr: 2.0833e-05 - 40s/epoch - 206ms/step
Epoch 674/1000
2023-10-26 09:22:25.931 
Epoch 674/1000 
	 loss: 27.5529, MinusLogProbMetric: 27.5529, val_loss: 28.2464, val_MinusLogProbMetric: 28.2464

Epoch 674: val_loss improved from 28.25225 to 28.24641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5529 - MinusLogProbMetric: 27.5529 - val_loss: 28.2464 - val_MinusLogProbMetric: 28.2464 - lr: 2.0833e-05 - 42s/epoch - 217ms/step
Epoch 675/1000
2023-10-26 09:23:08.236 
Epoch 675/1000 
	 loss: 27.5500, MinusLogProbMetric: 27.5500, val_loss: 28.2495, val_MinusLogProbMetric: 28.2495

Epoch 675: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5500 - MinusLogProbMetric: 27.5500 - val_loss: 28.2495 - val_MinusLogProbMetric: 28.2495 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 676/1000
2023-10-26 09:23:49.877 
Epoch 676/1000 
	 loss: 27.5476, MinusLogProbMetric: 27.5476, val_loss: 28.2554, val_MinusLogProbMetric: 28.2554

Epoch 676: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5476 - MinusLogProbMetric: 27.5476 - val_loss: 28.2554 - val_MinusLogProbMetric: 28.2554 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 677/1000
2023-10-26 09:24:31.391 
Epoch 677/1000 
	 loss: 27.5527, MinusLogProbMetric: 27.5527, val_loss: 28.2525, val_MinusLogProbMetric: 28.2525

Epoch 677: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5527 - MinusLogProbMetric: 27.5527 - val_loss: 28.2525 - val_MinusLogProbMetric: 28.2525 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 678/1000
2023-10-26 09:25:13.119 
Epoch 678/1000 
	 loss: 27.5534, MinusLogProbMetric: 27.5534, val_loss: 28.2571, val_MinusLogProbMetric: 28.2571

Epoch 678: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5534 - MinusLogProbMetric: 27.5534 - val_loss: 28.2571 - val_MinusLogProbMetric: 28.2571 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 679/1000
2023-10-26 09:25:54.473 
Epoch 679/1000 
	 loss: 27.5504, MinusLogProbMetric: 27.5504, val_loss: 28.2585, val_MinusLogProbMetric: 28.2585

Epoch 679: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5504 - MinusLogProbMetric: 27.5504 - val_loss: 28.2585 - val_MinusLogProbMetric: 28.2585 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 680/1000
2023-10-26 09:26:35.248 
Epoch 680/1000 
	 loss: 27.5489, MinusLogProbMetric: 27.5489, val_loss: 28.2738, val_MinusLogProbMetric: 28.2738

Epoch 680: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5489 - MinusLogProbMetric: 27.5489 - val_loss: 28.2738 - val_MinusLogProbMetric: 28.2738 - lr: 2.0833e-05 - 41s/epoch - 208ms/step
Epoch 681/1000
2023-10-26 09:27:16.663 
Epoch 681/1000 
	 loss: 27.5522, MinusLogProbMetric: 27.5522, val_loss: 28.2557, val_MinusLogProbMetric: 28.2557

Epoch 681: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5522 - MinusLogProbMetric: 27.5522 - val_loss: 28.2557 - val_MinusLogProbMetric: 28.2557 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 682/1000
2023-10-26 09:27:58.278 
Epoch 682/1000 
	 loss: 27.5486, MinusLogProbMetric: 27.5486, val_loss: 28.2524, val_MinusLogProbMetric: 28.2524

Epoch 682: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5486 - MinusLogProbMetric: 27.5486 - val_loss: 28.2524 - val_MinusLogProbMetric: 28.2524 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 683/1000
2023-10-26 09:28:39.712 
Epoch 683/1000 
	 loss: 27.5503, MinusLogProbMetric: 27.5503, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 683: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5503 - MinusLogProbMetric: 27.5503 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 684/1000
2023-10-26 09:29:21.448 
Epoch 684/1000 
	 loss: 27.5478, MinusLogProbMetric: 27.5478, val_loss: 28.2535, val_MinusLogProbMetric: 28.2535

Epoch 684: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5478 - MinusLogProbMetric: 27.5478 - val_loss: 28.2535 - val_MinusLogProbMetric: 28.2535 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 685/1000
2023-10-26 09:30:02.005 
Epoch 685/1000 
	 loss: 27.5479, MinusLogProbMetric: 27.5479, val_loss: 28.2597, val_MinusLogProbMetric: 28.2597

Epoch 685: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5479 - MinusLogProbMetric: 27.5479 - val_loss: 28.2597 - val_MinusLogProbMetric: 28.2597 - lr: 2.0833e-05 - 41s/epoch - 207ms/step
Epoch 686/1000
2023-10-26 09:30:43.367 
Epoch 686/1000 
	 loss: 27.5474, MinusLogProbMetric: 27.5474, val_loss: 28.2719, val_MinusLogProbMetric: 28.2719

Epoch 686: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5474 - MinusLogProbMetric: 27.5474 - val_loss: 28.2719 - val_MinusLogProbMetric: 28.2719 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 687/1000
2023-10-26 09:31:24.785 
Epoch 687/1000 
	 loss: 27.5499, MinusLogProbMetric: 27.5499, val_loss: 28.2503, val_MinusLogProbMetric: 28.2503

Epoch 687: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5499 - MinusLogProbMetric: 27.5499 - val_loss: 28.2503 - val_MinusLogProbMetric: 28.2503 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 688/1000
2023-10-26 09:32:06.508 
Epoch 688/1000 
	 loss: 27.5486, MinusLogProbMetric: 27.5486, val_loss: 28.2547, val_MinusLogProbMetric: 28.2547

Epoch 688: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5486 - MinusLogProbMetric: 27.5486 - val_loss: 28.2547 - val_MinusLogProbMetric: 28.2547 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 689/1000
2023-10-26 09:32:47.815 
Epoch 689/1000 
	 loss: 27.5486, MinusLogProbMetric: 27.5486, val_loss: 28.2623, val_MinusLogProbMetric: 28.2623

Epoch 689: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5486 - MinusLogProbMetric: 27.5486 - val_loss: 28.2623 - val_MinusLogProbMetric: 28.2623 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 690/1000
2023-10-26 09:33:29.563 
Epoch 690/1000 
	 loss: 27.5481, MinusLogProbMetric: 27.5481, val_loss: 28.2509, val_MinusLogProbMetric: 28.2509

Epoch 690: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5481 - MinusLogProbMetric: 27.5481 - val_loss: 28.2509 - val_MinusLogProbMetric: 28.2509 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 691/1000
2023-10-26 09:34:10.561 
Epoch 691/1000 
	 loss: 27.5475, MinusLogProbMetric: 27.5475, val_loss: 28.2509, val_MinusLogProbMetric: 28.2509

Epoch 691: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5475 - MinusLogProbMetric: 27.5475 - val_loss: 28.2509 - val_MinusLogProbMetric: 28.2509 - lr: 2.0833e-05 - 41s/epoch - 209ms/step
Epoch 692/1000
2023-10-26 09:34:52.124 
Epoch 692/1000 
	 loss: 27.5493, MinusLogProbMetric: 27.5493, val_loss: 28.2508, val_MinusLogProbMetric: 28.2508

Epoch 692: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5493 - MinusLogProbMetric: 27.5493 - val_loss: 28.2508 - val_MinusLogProbMetric: 28.2508 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 693/1000
2023-10-26 09:35:33.802 
Epoch 693/1000 
	 loss: 27.5472, MinusLogProbMetric: 27.5472, val_loss: 28.2579, val_MinusLogProbMetric: 28.2579

Epoch 693: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5472 - MinusLogProbMetric: 27.5472 - val_loss: 28.2579 - val_MinusLogProbMetric: 28.2579 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 694/1000
2023-10-26 09:36:15.513 
Epoch 694/1000 
	 loss: 27.5502, MinusLogProbMetric: 27.5502, val_loss: 28.2558, val_MinusLogProbMetric: 28.2558

Epoch 694: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5502 - MinusLogProbMetric: 27.5502 - val_loss: 28.2558 - val_MinusLogProbMetric: 28.2558 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 695/1000
2023-10-26 09:36:57.273 
Epoch 695/1000 
	 loss: 27.5477, MinusLogProbMetric: 27.5477, val_loss: 28.2560, val_MinusLogProbMetric: 28.2560

Epoch 695: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5477 - MinusLogProbMetric: 27.5477 - val_loss: 28.2560 - val_MinusLogProbMetric: 28.2560 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 696/1000
2023-10-26 09:37:38.994 
Epoch 696/1000 
	 loss: 27.5500, MinusLogProbMetric: 27.5500, val_loss: 28.2599, val_MinusLogProbMetric: 28.2599

Epoch 696: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5500 - MinusLogProbMetric: 27.5500 - val_loss: 28.2599 - val_MinusLogProbMetric: 28.2599 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 697/1000
2023-10-26 09:38:20.167 
Epoch 697/1000 
	 loss: 27.5483, MinusLogProbMetric: 27.5483, val_loss: 28.2616, val_MinusLogProbMetric: 28.2616

Epoch 697: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5483 - MinusLogProbMetric: 27.5483 - val_loss: 28.2616 - val_MinusLogProbMetric: 28.2616 - lr: 2.0833e-05 - 41s/epoch - 210ms/step
Epoch 698/1000
2023-10-26 09:39:01.301 
Epoch 698/1000 
	 loss: 27.5488, MinusLogProbMetric: 27.5488, val_loss: 28.2552, val_MinusLogProbMetric: 28.2552

Epoch 698: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5488 - MinusLogProbMetric: 27.5488 - val_loss: 28.2552 - val_MinusLogProbMetric: 28.2552 - lr: 2.0833e-05 - 41s/epoch - 210ms/step
Epoch 699/1000
2023-10-26 09:39:42.831 
Epoch 699/1000 
	 loss: 27.5506, MinusLogProbMetric: 27.5506, val_loss: 28.2504, val_MinusLogProbMetric: 28.2504

Epoch 699: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5506 - MinusLogProbMetric: 27.5506 - val_loss: 28.2504 - val_MinusLogProbMetric: 28.2504 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 700/1000
2023-10-26 09:40:23.395 
Epoch 700/1000 
	 loss: 27.5475, MinusLogProbMetric: 27.5475, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 700: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5475 - MinusLogProbMetric: 27.5475 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 2.0833e-05 - 41s/epoch - 207ms/step
Epoch 701/1000
2023-10-26 09:41:05.374 
Epoch 701/1000 
	 loss: 27.5487, MinusLogProbMetric: 27.5487, val_loss: 28.2497, val_MinusLogProbMetric: 28.2497

Epoch 701: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5487 - MinusLogProbMetric: 27.5487 - val_loss: 28.2497 - val_MinusLogProbMetric: 28.2497 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 702/1000
2023-10-26 09:41:46.893 
Epoch 702/1000 
	 loss: 27.5454, MinusLogProbMetric: 27.5454, val_loss: 28.2593, val_MinusLogProbMetric: 28.2593

Epoch 702: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5454 - MinusLogProbMetric: 27.5454 - val_loss: 28.2593 - val_MinusLogProbMetric: 28.2593 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 703/1000
2023-10-26 09:42:28.415 
Epoch 703/1000 
	 loss: 27.5502, MinusLogProbMetric: 27.5502, val_loss: 28.2516, val_MinusLogProbMetric: 28.2516

Epoch 703: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5502 - MinusLogProbMetric: 27.5502 - val_loss: 28.2516 - val_MinusLogProbMetric: 28.2516 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 704/1000
2023-10-26 09:43:10.346 
Epoch 704/1000 
	 loss: 27.5466, MinusLogProbMetric: 27.5466, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 704: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5466 - MinusLogProbMetric: 27.5466 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 705/1000
2023-10-26 09:43:52.334 
Epoch 705/1000 
	 loss: 27.5457, MinusLogProbMetric: 27.5457, val_loss: 28.2583, val_MinusLogProbMetric: 28.2583

Epoch 705: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5457 - MinusLogProbMetric: 27.5457 - val_loss: 28.2583 - val_MinusLogProbMetric: 28.2583 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 706/1000
2023-10-26 09:44:33.875 
Epoch 706/1000 
	 loss: 27.5474, MinusLogProbMetric: 27.5474, val_loss: 28.2594, val_MinusLogProbMetric: 28.2594

Epoch 706: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5474 - MinusLogProbMetric: 27.5474 - val_loss: 28.2594 - val_MinusLogProbMetric: 28.2594 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 707/1000
2023-10-26 09:45:15.093 
Epoch 707/1000 
	 loss: 27.5456, MinusLogProbMetric: 27.5456, val_loss: 28.2606, val_MinusLogProbMetric: 28.2606

Epoch 707: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5456 - MinusLogProbMetric: 27.5456 - val_loss: 28.2606 - val_MinusLogProbMetric: 28.2606 - lr: 2.0833e-05 - 41s/epoch - 210ms/step
Epoch 708/1000
2023-10-26 09:45:56.693 
Epoch 708/1000 
	 loss: 27.5475, MinusLogProbMetric: 27.5475, val_loss: 28.2557, val_MinusLogProbMetric: 28.2557

Epoch 708: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5475 - MinusLogProbMetric: 27.5475 - val_loss: 28.2557 - val_MinusLogProbMetric: 28.2557 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 709/1000
2023-10-26 09:46:38.115 
Epoch 709/1000 
	 loss: 27.5465, MinusLogProbMetric: 27.5465, val_loss: 28.2619, val_MinusLogProbMetric: 28.2619

Epoch 709: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5465 - MinusLogProbMetric: 27.5465 - val_loss: 28.2619 - val_MinusLogProbMetric: 28.2619 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 710/1000
2023-10-26 09:47:19.893 
Epoch 710/1000 
	 loss: 27.5462, MinusLogProbMetric: 27.5462, val_loss: 28.2570, val_MinusLogProbMetric: 28.2570

Epoch 710: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5462 - MinusLogProbMetric: 27.5462 - val_loss: 28.2570 - val_MinusLogProbMetric: 28.2570 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 711/1000
2023-10-26 09:48:01.318 
Epoch 711/1000 
	 loss: 27.5475, MinusLogProbMetric: 27.5475, val_loss: 28.2599, val_MinusLogProbMetric: 28.2599

Epoch 711: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5475 - MinusLogProbMetric: 27.5475 - val_loss: 28.2599 - val_MinusLogProbMetric: 28.2599 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 712/1000
2023-10-26 09:48:42.791 
Epoch 712/1000 
	 loss: 27.5479, MinusLogProbMetric: 27.5479, val_loss: 28.2728, val_MinusLogProbMetric: 28.2728

Epoch 712: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5479 - MinusLogProbMetric: 27.5479 - val_loss: 28.2728 - val_MinusLogProbMetric: 28.2728 - lr: 2.0833e-05 - 41s/epoch - 212ms/step
Epoch 713/1000
2023-10-26 09:49:24.364 
Epoch 713/1000 
	 loss: 27.5479, MinusLogProbMetric: 27.5479, val_loss: 28.2677, val_MinusLogProbMetric: 28.2677

Epoch 713: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5479 - MinusLogProbMetric: 27.5479 - val_loss: 28.2677 - val_MinusLogProbMetric: 28.2677 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 714/1000
2023-10-26 09:50:06.296 
Epoch 714/1000 
	 loss: 27.5478, MinusLogProbMetric: 27.5478, val_loss: 28.2503, val_MinusLogProbMetric: 28.2503

Epoch 714: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5478 - MinusLogProbMetric: 27.5478 - val_loss: 28.2503 - val_MinusLogProbMetric: 28.2503 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 715/1000
2023-10-26 09:50:47.997 
Epoch 715/1000 
	 loss: 27.5448, MinusLogProbMetric: 27.5448, val_loss: 28.2465, val_MinusLogProbMetric: 28.2465

Epoch 715: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5448 - MinusLogProbMetric: 27.5448 - val_loss: 28.2465 - val_MinusLogProbMetric: 28.2465 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 716/1000
2023-10-26 09:51:29.310 
Epoch 716/1000 
	 loss: 27.5428, MinusLogProbMetric: 27.5428, val_loss: 28.2632, val_MinusLogProbMetric: 28.2632

Epoch 716: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5428 - MinusLogProbMetric: 27.5428 - val_loss: 28.2632 - val_MinusLogProbMetric: 28.2632 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 717/1000
2023-10-26 09:52:11.062 
Epoch 717/1000 
	 loss: 27.5451, MinusLogProbMetric: 27.5451, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 717: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5451 - MinusLogProbMetric: 27.5451 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 718/1000
2023-10-26 09:52:52.092 
Epoch 718/1000 
	 loss: 27.5434, MinusLogProbMetric: 27.5434, val_loss: 28.2633, val_MinusLogProbMetric: 28.2633

Epoch 718: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5434 - MinusLogProbMetric: 27.5434 - val_loss: 28.2633 - val_MinusLogProbMetric: 28.2633 - lr: 2.0833e-05 - 41s/epoch - 209ms/step
Epoch 719/1000
2023-10-26 09:53:33.421 
Epoch 719/1000 
	 loss: 27.5439, MinusLogProbMetric: 27.5439, val_loss: 28.2506, val_MinusLogProbMetric: 28.2506

Epoch 719: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5439 - MinusLogProbMetric: 27.5439 - val_loss: 28.2506 - val_MinusLogProbMetric: 28.2506 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 720/1000
2023-10-26 09:54:15.398 
Epoch 720/1000 
	 loss: 27.5427, MinusLogProbMetric: 27.5427, val_loss: 28.2652, val_MinusLogProbMetric: 28.2652

Epoch 720: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5427 - MinusLogProbMetric: 27.5427 - val_loss: 28.2652 - val_MinusLogProbMetric: 28.2652 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 721/1000
2023-10-26 09:54:56.923 
Epoch 721/1000 
	 loss: 27.5457, MinusLogProbMetric: 27.5457, val_loss: 28.2487, val_MinusLogProbMetric: 28.2487

Epoch 721: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5457 - MinusLogProbMetric: 27.5457 - val_loss: 28.2487 - val_MinusLogProbMetric: 28.2487 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 722/1000
2023-10-26 09:55:37.685 
Epoch 722/1000 
	 loss: 27.5451, MinusLogProbMetric: 27.5451, val_loss: 28.2586, val_MinusLogProbMetric: 28.2586

Epoch 722: val_loss did not improve from 28.24641
196/196 - 41s - loss: 27.5451 - MinusLogProbMetric: 27.5451 - val_loss: 28.2586 - val_MinusLogProbMetric: 28.2586 - lr: 2.0833e-05 - 41s/epoch - 208ms/step
Epoch 723/1000
2023-10-26 09:56:19.277 
Epoch 723/1000 
	 loss: 27.5431, MinusLogProbMetric: 27.5431, val_loss: 28.2623, val_MinusLogProbMetric: 28.2623

Epoch 723: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5431 - MinusLogProbMetric: 27.5431 - val_loss: 28.2623 - val_MinusLogProbMetric: 28.2623 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 724/1000
2023-10-26 09:57:01.046 
Epoch 724/1000 
	 loss: 27.5458, MinusLogProbMetric: 27.5458, val_loss: 28.2592, val_MinusLogProbMetric: 28.2592

Epoch 724: val_loss did not improve from 28.24641
196/196 - 42s - loss: 27.5458 - MinusLogProbMetric: 27.5458 - val_loss: 28.2592 - val_MinusLogProbMetric: 28.2592 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 725/1000
2023-10-26 09:57:42.419 
Epoch 725/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.2438, val_MinusLogProbMetric: 28.2438

Epoch 725: val_loss improved from 28.24641 to 28.24382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.2438 - val_MinusLogProbMetric: 28.2438 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 726/1000
2023-10-26 09:58:24.743 
Epoch 726/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.2508, val_MinusLogProbMetric: 28.2508

Epoch 726: val_loss did not improve from 28.24382
196/196 - 42s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.2508 - val_MinusLogProbMetric: 28.2508 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 727/1000
2023-10-26 09:59:06.167 
Epoch 727/1000 
	 loss: 27.5285, MinusLogProbMetric: 27.5285, val_loss: 28.2418, val_MinusLogProbMetric: 28.2418

Epoch 727: val_loss improved from 28.24382 to 28.24182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5285 - MinusLogProbMetric: 27.5285 - val_loss: 28.2418 - val_MinusLogProbMetric: 28.2418 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 728/1000
2023-10-26 09:59:48.225 
Epoch 728/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.2529, val_MinusLogProbMetric: 28.2529

Epoch 728: val_loss did not improve from 28.24182
196/196 - 41s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.2529 - val_MinusLogProbMetric: 28.2529 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 729/1000
2023-10-26 10:00:28.635 
Epoch 729/1000 
	 loss: 27.5284, MinusLogProbMetric: 27.5284, val_loss: 28.2410, val_MinusLogProbMetric: 28.2410

Epoch 729: val_loss improved from 28.24182 to 28.24098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 41s - loss: 27.5284 - MinusLogProbMetric: 27.5284 - val_loss: 28.2410 - val_MinusLogProbMetric: 28.2410 - lr: 1.0417e-05 - 41s/epoch - 210ms/step
Epoch 730/1000
2023-10-26 10:01:01.727 
Epoch 730/1000 
	 loss: 27.5272, MinusLogProbMetric: 27.5272, val_loss: 28.2430, val_MinusLogProbMetric: 28.2430

Epoch 730: val_loss did not improve from 28.24098
196/196 - 32s - loss: 27.5272 - MinusLogProbMetric: 27.5272 - val_loss: 28.2430 - val_MinusLogProbMetric: 28.2430 - lr: 1.0417e-05 - 32s/epoch - 165ms/step
Epoch 731/1000
2023-10-26 10:01:34.475 
Epoch 731/1000 
	 loss: 27.5280, MinusLogProbMetric: 27.5280, val_loss: 28.2402, val_MinusLogProbMetric: 28.2402

Epoch 731: val_loss improved from 28.24098 to 28.24016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 33s - loss: 27.5280 - MinusLogProbMetric: 27.5280 - val_loss: 28.2402 - val_MinusLogProbMetric: 28.2402 - lr: 1.0417e-05 - 33s/epoch - 170ms/step
Epoch 732/1000
2023-10-26 10:02:10.329 
Epoch 732/1000 
	 loss: 27.5287, MinusLogProbMetric: 27.5287, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 732: val_loss did not improve from 28.24016
196/196 - 35s - loss: 27.5287 - MinusLogProbMetric: 27.5287 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 1.0417e-05 - 35s/epoch - 180ms/step
Epoch 733/1000
2023-10-26 10:02:50.086 
Epoch 733/1000 
	 loss: 27.5280, MinusLogProbMetric: 27.5280, val_loss: 28.2449, val_MinusLogProbMetric: 28.2449

Epoch 733: val_loss did not improve from 28.24016
196/196 - 40s - loss: 27.5280 - MinusLogProbMetric: 27.5280 - val_loss: 28.2449 - val_MinusLogProbMetric: 28.2449 - lr: 1.0417e-05 - 40s/epoch - 203ms/step
Epoch 734/1000
2023-10-26 10:03:31.645 
Epoch 734/1000 
	 loss: 27.5290, MinusLogProbMetric: 27.5290, val_loss: 28.2424, val_MinusLogProbMetric: 28.2424

Epoch 734: val_loss did not improve from 28.24016
196/196 - 42s - loss: 27.5290 - MinusLogProbMetric: 27.5290 - val_loss: 28.2424 - val_MinusLogProbMetric: 28.2424 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 735/1000
2023-10-26 10:04:11.183 
Epoch 735/1000 
	 loss: 27.5281, MinusLogProbMetric: 27.5281, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 735: val_loss did not improve from 28.24016
196/196 - 40s - loss: 27.5281 - MinusLogProbMetric: 27.5281 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 1.0417e-05 - 40s/epoch - 202ms/step
Epoch 736/1000
2023-10-26 10:04:52.907 
Epoch 736/1000 
	 loss: 27.5276, MinusLogProbMetric: 27.5276, val_loss: 28.2382, val_MinusLogProbMetric: 28.2382

Epoch 736: val_loss improved from 28.24016 to 28.23825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5276 - MinusLogProbMetric: 27.5276 - val_loss: 28.2382 - val_MinusLogProbMetric: 28.2382 - lr: 1.0417e-05 - 42s/epoch - 217ms/step
Epoch 737/1000
2023-10-26 10:05:35.190 
Epoch 737/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.2487, val_MinusLogProbMetric: 28.2487

Epoch 737: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.2487 - val_MinusLogProbMetric: 28.2487 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 738/1000
2023-10-26 10:06:17.090 
Epoch 738/1000 
	 loss: 27.5280, MinusLogProbMetric: 27.5280, val_loss: 28.2438, val_MinusLogProbMetric: 28.2438

Epoch 738: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5280 - MinusLogProbMetric: 27.5280 - val_loss: 28.2438 - val_MinusLogProbMetric: 28.2438 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 739/1000
2023-10-26 10:06:58.808 
Epoch 739/1000 
	 loss: 27.5278, MinusLogProbMetric: 27.5278, val_loss: 28.2460, val_MinusLogProbMetric: 28.2460

Epoch 739: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5278 - MinusLogProbMetric: 27.5278 - val_loss: 28.2460 - val_MinusLogProbMetric: 28.2460 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 740/1000
2023-10-26 10:07:40.958 
Epoch 740/1000 
	 loss: 27.5272, MinusLogProbMetric: 27.5272, val_loss: 28.2403, val_MinusLogProbMetric: 28.2403

Epoch 740: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5272 - MinusLogProbMetric: 27.5272 - val_loss: 28.2403 - val_MinusLogProbMetric: 28.2403 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 741/1000
2023-10-26 10:08:22.734 
Epoch 741/1000 
	 loss: 27.5285, MinusLogProbMetric: 27.5285, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 741: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5285 - MinusLogProbMetric: 27.5285 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 742/1000
2023-10-26 10:09:04.353 
Epoch 742/1000 
	 loss: 27.5290, MinusLogProbMetric: 27.5290, val_loss: 28.2443, val_MinusLogProbMetric: 28.2443

Epoch 742: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5290 - MinusLogProbMetric: 27.5290 - val_loss: 28.2443 - val_MinusLogProbMetric: 28.2443 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 743/1000
2023-10-26 10:09:44.253 
Epoch 743/1000 
	 loss: 27.5284, MinusLogProbMetric: 27.5284, val_loss: 28.2422, val_MinusLogProbMetric: 28.2422

Epoch 743: val_loss did not improve from 28.23825
196/196 - 40s - loss: 27.5284 - MinusLogProbMetric: 27.5284 - val_loss: 28.2422 - val_MinusLogProbMetric: 28.2422 - lr: 1.0417e-05 - 40s/epoch - 204ms/step
Epoch 744/1000
2023-10-26 10:10:25.770 
Epoch 744/1000 
	 loss: 27.5279, MinusLogProbMetric: 27.5279, val_loss: 28.2446, val_MinusLogProbMetric: 28.2446

Epoch 744: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5279 - MinusLogProbMetric: 27.5279 - val_loss: 28.2446 - val_MinusLogProbMetric: 28.2446 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 745/1000
2023-10-26 10:11:07.702 
Epoch 745/1000 
	 loss: 27.5268, MinusLogProbMetric: 27.5268, val_loss: 28.2430, val_MinusLogProbMetric: 28.2430

Epoch 745: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5268 - MinusLogProbMetric: 27.5268 - val_loss: 28.2430 - val_MinusLogProbMetric: 28.2430 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 746/1000
2023-10-26 10:11:49.744 
Epoch 746/1000 
	 loss: 27.5269, MinusLogProbMetric: 27.5269, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 746: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5269 - MinusLogProbMetric: 27.5269 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 747/1000
2023-10-26 10:12:31.483 
Epoch 747/1000 
	 loss: 27.5279, MinusLogProbMetric: 27.5279, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 747: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5279 - MinusLogProbMetric: 27.5279 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 748/1000
2023-10-26 10:13:13.120 
Epoch 748/1000 
	 loss: 27.5269, MinusLogProbMetric: 27.5269, val_loss: 28.2416, val_MinusLogProbMetric: 28.2416

Epoch 748: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5269 - MinusLogProbMetric: 27.5269 - val_loss: 28.2416 - val_MinusLogProbMetric: 28.2416 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 749/1000
2023-10-26 10:13:54.810 
Epoch 749/1000 
	 loss: 27.5269, MinusLogProbMetric: 27.5269, val_loss: 28.2402, val_MinusLogProbMetric: 28.2402

Epoch 749: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5269 - MinusLogProbMetric: 27.5269 - val_loss: 28.2402 - val_MinusLogProbMetric: 28.2402 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 750/1000
2023-10-26 10:14:36.652 
Epoch 750/1000 
	 loss: 27.5264, MinusLogProbMetric: 27.5264, val_loss: 28.2433, val_MinusLogProbMetric: 28.2433

Epoch 750: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5264 - MinusLogProbMetric: 27.5264 - val_loss: 28.2433 - val_MinusLogProbMetric: 28.2433 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 751/1000
2023-10-26 10:15:17.528 
Epoch 751/1000 
	 loss: 27.5263, MinusLogProbMetric: 27.5263, val_loss: 28.2420, val_MinusLogProbMetric: 28.2420

Epoch 751: val_loss did not improve from 28.23825
196/196 - 41s - loss: 27.5263 - MinusLogProbMetric: 27.5263 - val_loss: 28.2420 - val_MinusLogProbMetric: 28.2420 - lr: 1.0417e-05 - 41s/epoch - 209ms/step
Epoch 752/1000
2023-10-26 10:15:59.407 
Epoch 752/1000 
	 loss: 27.5247, MinusLogProbMetric: 27.5247, val_loss: 28.2411, val_MinusLogProbMetric: 28.2411

Epoch 752: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5247 - MinusLogProbMetric: 27.5247 - val_loss: 28.2411 - val_MinusLogProbMetric: 28.2411 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 753/1000
2023-10-26 10:16:40.742 
Epoch 753/1000 
	 loss: 27.5274, MinusLogProbMetric: 27.5274, val_loss: 28.2389, val_MinusLogProbMetric: 28.2389

Epoch 753: val_loss did not improve from 28.23825
196/196 - 41s - loss: 27.5274 - MinusLogProbMetric: 27.5274 - val_loss: 28.2389 - val_MinusLogProbMetric: 28.2389 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 754/1000
2023-10-26 10:17:22.269 
Epoch 754/1000 
	 loss: 27.5267, MinusLogProbMetric: 27.5267, val_loss: 28.2428, val_MinusLogProbMetric: 28.2428

Epoch 754: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5267 - MinusLogProbMetric: 27.5267 - val_loss: 28.2428 - val_MinusLogProbMetric: 28.2428 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 755/1000
2023-10-26 10:18:03.161 
Epoch 755/1000 
	 loss: 27.5265, MinusLogProbMetric: 27.5265, val_loss: 28.2432, val_MinusLogProbMetric: 28.2432

Epoch 755: val_loss did not improve from 28.23825
196/196 - 41s - loss: 27.5265 - MinusLogProbMetric: 27.5265 - val_loss: 28.2432 - val_MinusLogProbMetric: 28.2432 - lr: 1.0417e-05 - 41s/epoch - 209ms/step
Epoch 756/1000
2023-10-26 10:18:44.891 
Epoch 756/1000 
	 loss: 27.5274, MinusLogProbMetric: 27.5274, val_loss: 28.2457, val_MinusLogProbMetric: 28.2457

Epoch 756: val_loss did not improve from 28.23825
196/196 - 42s - loss: 27.5274 - MinusLogProbMetric: 27.5274 - val_loss: 28.2457 - val_MinusLogProbMetric: 28.2457 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 757/1000
2023-10-26 10:19:23.927 
Epoch 757/1000 
	 loss: 27.5269, MinusLogProbMetric: 27.5269, val_loss: 28.2446, val_MinusLogProbMetric: 28.2446

Epoch 757: val_loss did not improve from 28.23825
196/196 - 39s - loss: 27.5269 - MinusLogProbMetric: 27.5269 - val_loss: 28.2446 - val_MinusLogProbMetric: 28.2446 - lr: 1.0417e-05 - 39s/epoch - 199ms/step
Epoch 758/1000
2023-10-26 10:20:04.459 
Epoch 758/1000 
	 loss: 27.5274, MinusLogProbMetric: 27.5274, val_loss: 28.2411, val_MinusLogProbMetric: 28.2411

Epoch 758: val_loss did not improve from 28.23825
196/196 - 41s - loss: 27.5274 - MinusLogProbMetric: 27.5274 - val_loss: 28.2411 - val_MinusLogProbMetric: 28.2411 - lr: 1.0417e-05 - 41s/epoch - 207ms/step
Epoch 759/1000
2023-10-26 10:20:45.970 
Epoch 759/1000 
	 loss: 27.5263, MinusLogProbMetric: 27.5263, val_loss: 28.2373, val_MinusLogProbMetric: 28.2373

Epoch 759: val_loss improved from 28.23825 to 28.23728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5263 - MinusLogProbMetric: 27.5263 - val_loss: 28.2373 - val_MinusLogProbMetric: 28.2373 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 760/1000
2023-10-26 10:21:28.059 
Epoch 760/1000 
	 loss: 27.5262, MinusLogProbMetric: 27.5262, val_loss: 28.2394, val_MinusLogProbMetric: 28.2394

Epoch 760: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5262 - MinusLogProbMetric: 27.5262 - val_loss: 28.2394 - val_MinusLogProbMetric: 28.2394 - lr: 1.0417e-05 - 41s/epoch - 212ms/step
Epoch 761/1000
2023-10-26 10:22:09.875 
Epoch 761/1000 
	 loss: 27.5272, MinusLogProbMetric: 27.5272, val_loss: 28.2383, val_MinusLogProbMetric: 28.2383

Epoch 761: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5272 - MinusLogProbMetric: 27.5272 - val_loss: 28.2383 - val_MinusLogProbMetric: 28.2383 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 762/1000
2023-10-26 10:22:51.626 
Epoch 762/1000 
	 loss: 27.5252, MinusLogProbMetric: 27.5252, val_loss: 28.2393, val_MinusLogProbMetric: 28.2393

Epoch 762: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5252 - MinusLogProbMetric: 27.5252 - val_loss: 28.2393 - val_MinusLogProbMetric: 28.2393 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 763/1000
2023-10-26 10:23:33.111 
Epoch 763/1000 
	 loss: 27.5259, MinusLogProbMetric: 27.5259, val_loss: 28.2490, val_MinusLogProbMetric: 28.2490

Epoch 763: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5259 - MinusLogProbMetric: 27.5259 - val_loss: 28.2490 - val_MinusLogProbMetric: 28.2490 - lr: 1.0417e-05 - 41s/epoch - 212ms/step
Epoch 764/1000
2023-10-26 10:24:15.043 
Epoch 764/1000 
	 loss: 27.5270, MinusLogProbMetric: 27.5270, val_loss: 28.2406, val_MinusLogProbMetric: 28.2406

Epoch 764: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5270 - MinusLogProbMetric: 27.5270 - val_loss: 28.2406 - val_MinusLogProbMetric: 28.2406 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 765/1000
2023-10-26 10:24:56.333 
Epoch 765/1000 
	 loss: 27.5272, MinusLogProbMetric: 27.5272, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 765: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5272 - MinusLogProbMetric: 27.5272 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 766/1000
2023-10-26 10:25:38.443 
Epoch 766/1000 
	 loss: 27.5274, MinusLogProbMetric: 27.5274, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 766: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5274 - MinusLogProbMetric: 27.5274 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 767/1000
2023-10-26 10:26:20.459 
Epoch 767/1000 
	 loss: 27.5270, MinusLogProbMetric: 27.5270, val_loss: 28.2435, val_MinusLogProbMetric: 28.2435

Epoch 767: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5270 - MinusLogProbMetric: 27.5270 - val_loss: 28.2435 - val_MinusLogProbMetric: 28.2435 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 768/1000
2023-10-26 10:27:02.170 
Epoch 768/1000 
	 loss: 27.5268, MinusLogProbMetric: 27.5268, val_loss: 28.2378, val_MinusLogProbMetric: 28.2378

Epoch 768: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5268 - MinusLogProbMetric: 27.5268 - val_loss: 28.2378 - val_MinusLogProbMetric: 28.2378 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 769/1000
2023-10-26 10:27:44.359 
Epoch 769/1000 
	 loss: 27.5249, MinusLogProbMetric: 27.5249, val_loss: 28.2460, val_MinusLogProbMetric: 28.2460

Epoch 769: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5249 - MinusLogProbMetric: 27.5249 - val_loss: 28.2460 - val_MinusLogProbMetric: 28.2460 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 770/1000
2023-10-26 10:28:26.272 
Epoch 770/1000 
	 loss: 27.5255, MinusLogProbMetric: 27.5255, val_loss: 28.2462, val_MinusLogProbMetric: 28.2462

Epoch 770: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5255 - MinusLogProbMetric: 27.5255 - val_loss: 28.2462 - val_MinusLogProbMetric: 28.2462 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 771/1000
2023-10-26 10:29:08.062 
Epoch 771/1000 
	 loss: 27.5260, MinusLogProbMetric: 27.5260, val_loss: 28.2443, val_MinusLogProbMetric: 28.2443

Epoch 771: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5260 - MinusLogProbMetric: 27.5260 - val_loss: 28.2443 - val_MinusLogProbMetric: 28.2443 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 772/1000
2023-10-26 10:29:49.087 
Epoch 772/1000 
	 loss: 27.5264, MinusLogProbMetric: 27.5264, val_loss: 28.2384, val_MinusLogProbMetric: 28.2384

Epoch 772: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5264 - MinusLogProbMetric: 27.5264 - val_loss: 28.2384 - val_MinusLogProbMetric: 28.2384 - lr: 1.0417e-05 - 41s/epoch - 209ms/step
Epoch 773/1000
2023-10-26 10:30:30.179 
Epoch 773/1000 
	 loss: 27.5250, MinusLogProbMetric: 27.5250, val_loss: 28.2422, val_MinusLogProbMetric: 28.2422

Epoch 773: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5250 - MinusLogProbMetric: 27.5250 - val_loss: 28.2422 - val_MinusLogProbMetric: 28.2422 - lr: 1.0417e-05 - 41s/epoch - 210ms/step
Epoch 774/1000
2023-10-26 10:31:12.136 
Epoch 774/1000 
	 loss: 27.5260, MinusLogProbMetric: 27.5260, val_loss: 28.2405, val_MinusLogProbMetric: 28.2405

Epoch 774: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5260 - MinusLogProbMetric: 27.5260 - val_loss: 28.2405 - val_MinusLogProbMetric: 28.2405 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 775/1000
2023-10-26 10:31:54.099 
Epoch 775/1000 
	 loss: 27.5260, MinusLogProbMetric: 27.5260, val_loss: 28.2447, val_MinusLogProbMetric: 28.2447

Epoch 775: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5260 - MinusLogProbMetric: 27.5260 - val_loss: 28.2447 - val_MinusLogProbMetric: 28.2447 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 776/1000
2023-10-26 10:32:35.916 
Epoch 776/1000 
	 loss: 27.5258, MinusLogProbMetric: 27.5258, val_loss: 28.2440, val_MinusLogProbMetric: 28.2440

Epoch 776: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5258 - MinusLogProbMetric: 27.5258 - val_loss: 28.2440 - val_MinusLogProbMetric: 28.2440 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 777/1000
2023-10-26 10:33:17.416 
Epoch 777/1000 
	 loss: 27.5265, MinusLogProbMetric: 27.5265, val_loss: 28.2422, val_MinusLogProbMetric: 28.2422

Epoch 777: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5265 - MinusLogProbMetric: 27.5265 - val_loss: 28.2422 - val_MinusLogProbMetric: 28.2422 - lr: 1.0417e-05 - 41s/epoch - 212ms/step
Epoch 778/1000
2023-10-26 10:33:59.221 
Epoch 778/1000 
	 loss: 27.5256, MinusLogProbMetric: 27.5256, val_loss: 28.2431, val_MinusLogProbMetric: 28.2431

Epoch 778: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5256 - MinusLogProbMetric: 27.5256 - val_loss: 28.2431 - val_MinusLogProbMetric: 28.2431 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 779/1000
2023-10-26 10:34:40.789 
Epoch 779/1000 
	 loss: 27.5258, MinusLogProbMetric: 27.5258, val_loss: 28.2431, val_MinusLogProbMetric: 28.2431

Epoch 779: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5258 - MinusLogProbMetric: 27.5258 - val_loss: 28.2431 - val_MinusLogProbMetric: 28.2431 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 780/1000
2023-10-26 10:35:22.331 
Epoch 780/1000 
	 loss: 27.5256, MinusLogProbMetric: 27.5256, val_loss: 28.2445, val_MinusLogProbMetric: 28.2445

Epoch 780: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5256 - MinusLogProbMetric: 27.5256 - val_loss: 28.2445 - val_MinusLogProbMetric: 28.2445 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 781/1000
2023-10-26 10:36:03.783 
Epoch 781/1000 
	 loss: 27.5272, MinusLogProbMetric: 27.5272, val_loss: 28.2513, val_MinusLogProbMetric: 28.2513

Epoch 781: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5272 - MinusLogProbMetric: 27.5272 - val_loss: 28.2513 - val_MinusLogProbMetric: 28.2513 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 782/1000
2023-10-26 10:36:45.391 
Epoch 782/1000 
	 loss: 27.5251, MinusLogProbMetric: 27.5251, val_loss: 28.2417, val_MinusLogProbMetric: 28.2417

Epoch 782: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5251 - MinusLogProbMetric: 27.5251 - val_loss: 28.2417 - val_MinusLogProbMetric: 28.2417 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 783/1000
2023-10-26 10:37:27.269 
Epoch 783/1000 
	 loss: 27.5253, MinusLogProbMetric: 27.5253, val_loss: 28.2410, val_MinusLogProbMetric: 28.2410

Epoch 783: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5253 - MinusLogProbMetric: 27.5253 - val_loss: 28.2410 - val_MinusLogProbMetric: 28.2410 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 784/1000
2023-10-26 10:38:09.001 
Epoch 784/1000 
	 loss: 27.5262, MinusLogProbMetric: 27.5262, val_loss: 28.2392, val_MinusLogProbMetric: 28.2392

Epoch 784: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5262 - MinusLogProbMetric: 27.5262 - val_loss: 28.2392 - val_MinusLogProbMetric: 28.2392 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 785/1000
2023-10-26 10:38:50.370 
Epoch 785/1000 
	 loss: 27.5252, MinusLogProbMetric: 27.5252, val_loss: 28.2404, val_MinusLogProbMetric: 28.2404

Epoch 785: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5252 - MinusLogProbMetric: 27.5252 - val_loss: 28.2404 - val_MinusLogProbMetric: 28.2404 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 786/1000
2023-10-26 10:39:31.792 
Epoch 786/1000 
	 loss: 27.5243, MinusLogProbMetric: 27.5243, val_loss: 28.2388, val_MinusLogProbMetric: 28.2388

Epoch 786: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5243 - MinusLogProbMetric: 27.5243 - val_loss: 28.2388 - val_MinusLogProbMetric: 28.2388 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 787/1000
2023-10-26 10:40:13.607 
Epoch 787/1000 
	 loss: 27.5263, MinusLogProbMetric: 27.5263, val_loss: 28.2479, val_MinusLogProbMetric: 28.2479

Epoch 787: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5263 - MinusLogProbMetric: 27.5263 - val_loss: 28.2479 - val_MinusLogProbMetric: 28.2479 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 788/1000
2023-10-26 10:40:54.878 
Epoch 788/1000 
	 loss: 27.5248, MinusLogProbMetric: 27.5248, val_loss: 28.2432, val_MinusLogProbMetric: 28.2432

Epoch 788: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5248 - MinusLogProbMetric: 27.5248 - val_loss: 28.2432 - val_MinusLogProbMetric: 28.2432 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 789/1000
2023-10-26 10:41:36.175 
Epoch 789/1000 
	 loss: 27.5242, MinusLogProbMetric: 27.5242, val_loss: 28.2400, val_MinusLogProbMetric: 28.2400

Epoch 789: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5242 - MinusLogProbMetric: 27.5242 - val_loss: 28.2400 - val_MinusLogProbMetric: 28.2400 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 790/1000
2023-10-26 10:42:18.002 
Epoch 790/1000 
	 loss: 27.5244, MinusLogProbMetric: 27.5244, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 790: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5244 - MinusLogProbMetric: 27.5244 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 791/1000
2023-10-26 10:42:59.376 
Epoch 791/1000 
	 loss: 27.5258, MinusLogProbMetric: 27.5258, val_loss: 28.2385, val_MinusLogProbMetric: 28.2385

Epoch 791: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5258 - MinusLogProbMetric: 27.5258 - val_loss: 28.2385 - val_MinusLogProbMetric: 28.2385 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 792/1000
2023-10-26 10:43:40.642 
Epoch 792/1000 
	 loss: 27.5234, MinusLogProbMetric: 27.5234, val_loss: 28.2398, val_MinusLogProbMetric: 28.2398

Epoch 792: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5234 - MinusLogProbMetric: 27.5234 - val_loss: 28.2398 - val_MinusLogProbMetric: 28.2398 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 793/1000
2023-10-26 10:44:21.339 
Epoch 793/1000 
	 loss: 27.5241, MinusLogProbMetric: 27.5241, val_loss: 28.2439, val_MinusLogProbMetric: 28.2439

Epoch 793: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5241 - MinusLogProbMetric: 27.5241 - val_loss: 28.2439 - val_MinusLogProbMetric: 28.2439 - lr: 1.0417e-05 - 41s/epoch - 208ms/step
Epoch 794/1000
2023-10-26 10:45:02.678 
Epoch 794/1000 
	 loss: 27.5250, MinusLogProbMetric: 27.5250, val_loss: 28.2382, val_MinusLogProbMetric: 28.2382

Epoch 794: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5250 - MinusLogProbMetric: 27.5250 - val_loss: 28.2382 - val_MinusLogProbMetric: 28.2382 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 795/1000
2023-10-26 10:45:43.732 
Epoch 795/1000 
	 loss: 27.5248, MinusLogProbMetric: 27.5248, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 795: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5248 - MinusLogProbMetric: 27.5248 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 1.0417e-05 - 41s/epoch - 209ms/step
Epoch 796/1000
2023-10-26 10:46:24.813 
Epoch 796/1000 
	 loss: 27.5248, MinusLogProbMetric: 27.5248, val_loss: 28.2457, val_MinusLogProbMetric: 28.2457

Epoch 796: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5248 - MinusLogProbMetric: 27.5248 - val_loss: 28.2457 - val_MinusLogProbMetric: 28.2457 - lr: 1.0417e-05 - 41s/epoch - 210ms/step
Epoch 797/1000
2023-10-26 10:47:06.705 
Epoch 797/1000 
	 loss: 27.5257, MinusLogProbMetric: 27.5257, val_loss: 28.2383, val_MinusLogProbMetric: 28.2383

Epoch 797: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5257 - MinusLogProbMetric: 27.5257 - val_loss: 28.2383 - val_MinusLogProbMetric: 28.2383 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 798/1000
2023-10-26 10:47:47.540 
Epoch 798/1000 
	 loss: 27.5240, MinusLogProbMetric: 27.5240, val_loss: 28.2398, val_MinusLogProbMetric: 28.2398

Epoch 798: val_loss did not improve from 28.23728
196/196 - 41s - loss: 27.5240 - MinusLogProbMetric: 27.5240 - val_loss: 28.2398 - val_MinusLogProbMetric: 28.2398 - lr: 1.0417e-05 - 41s/epoch - 208ms/step
Epoch 799/1000
2023-10-26 10:48:29.338 
Epoch 799/1000 
	 loss: 27.5245, MinusLogProbMetric: 27.5245, val_loss: 28.2434, val_MinusLogProbMetric: 28.2434

Epoch 799: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5245 - MinusLogProbMetric: 27.5245 - val_loss: 28.2434 - val_MinusLogProbMetric: 28.2434 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 800/1000
2023-10-26 10:49:11.097 
Epoch 800/1000 
	 loss: 27.5253, MinusLogProbMetric: 27.5253, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 800: val_loss did not improve from 28.23728
196/196 - 42s - loss: 27.5253 - MinusLogProbMetric: 27.5253 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 801/1000
2023-10-26 10:49:52.715 
Epoch 801/1000 
	 loss: 27.5256, MinusLogProbMetric: 27.5256, val_loss: 28.2372, val_MinusLogProbMetric: 28.2372

Epoch 801: val_loss improved from 28.23728 to 28.23720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5256 - MinusLogProbMetric: 27.5256 - val_loss: 28.2372 - val_MinusLogProbMetric: 28.2372 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 802/1000
2023-10-26 10:50:34.892 
Epoch 802/1000 
	 loss: 27.5243, MinusLogProbMetric: 27.5243, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 802: val_loss did not improve from 28.23720
196/196 - 42s - loss: 27.5243 - MinusLogProbMetric: 27.5243 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 803/1000
2023-10-26 10:51:16.497 
Epoch 803/1000 
	 loss: 27.5239, MinusLogProbMetric: 27.5239, val_loss: 28.2410, val_MinusLogProbMetric: 28.2410

Epoch 803: val_loss did not improve from 28.23720
196/196 - 42s - loss: 27.5239 - MinusLogProbMetric: 27.5239 - val_loss: 28.2410 - val_MinusLogProbMetric: 28.2410 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 804/1000
2023-10-26 10:51:57.548 
Epoch 804/1000 
	 loss: 27.5248, MinusLogProbMetric: 27.5248, val_loss: 28.2416, val_MinusLogProbMetric: 28.2416

Epoch 804: val_loss did not improve from 28.23720
196/196 - 41s - loss: 27.5248 - MinusLogProbMetric: 27.5248 - val_loss: 28.2416 - val_MinusLogProbMetric: 28.2416 - lr: 1.0417e-05 - 41s/epoch - 209ms/step
Epoch 805/1000
2023-10-26 10:52:39.229 
Epoch 805/1000 
	 loss: 27.5229, MinusLogProbMetric: 27.5229, val_loss: 28.2437, val_MinusLogProbMetric: 28.2437

Epoch 805: val_loss did not improve from 28.23720
196/196 - 42s - loss: 27.5229 - MinusLogProbMetric: 27.5229 - val_loss: 28.2437 - val_MinusLogProbMetric: 28.2437 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 806/1000
2023-10-26 10:53:21.177 
Epoch 806/1000 
	 loss: 27.5244, MinusLogProbMetric: 27.5244, val_loss: 28.2383, val_MinusLogProbMetric: 28.2383

Epoch 806: val_loss did not improve from 28.23720
196/196 - 42s - loss: 27.5244 - MinusLogProbMetric: 27.5244 - val_loss: 28.2383 - val_MinusLogProbMetric: 28.2383 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 807/1000
2023-10-26 10:54:02.758 
Epoch 807/1000 
	 loss: 27.5245, MinusLogProbMetric: 27.5245, val_loss: 28.2483, val_MinusLogProbMetric: 28.2483

Epoch 807: val_loss did not improve from 28.23720
196/196 - 42s - loss: 27.5245 - MinusLogProbMetric: 27.5245 - val_loss: 28.2483 - val_MinusLogProbMetric: 28.2483 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 808/1000
2023-10-26 10:54:44.714 
Epoch 808/1000 
	 loss: 27.5239, MinusLogProbMetric: 27.5239, val_loss: 28.2417, val_MinusLogProbMetric: 28.2417

Epoch 808: val_loss did not improve from 28.23720
196/196 - 42s - loss: 27.5239 - MinusLogProbMetric: 27.5239 - val_loss: 28.2417 - val_MinusLogProbMetric: 28.2417 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 809/1000
2023-10-26 10:55:26.456 
Epoch 809/1000 
	 loss: 27.5254, MinusLogProbMetric: 27.5254, val_loss: 28.2398, val_MinusLogProbMetric: 28.2398

Epoch 809: val_loss did not improve from 28.23720
196/196 - 42s - loss: 27.5254 - MinusLogProbMetric: 27.5254 - val_loss: 28.2398 - val_MinusLogProbMetric: 28.2398 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 810/1000
2023-10-26 10:56:08.405 
Epoch 810/1000 
	 loss: 27.5166, MinusLogProbMetric: 27.5166, val_loss: 28.2342, val_MinusLogProbMetric: 28.2342

Epoch 810: val_loss improved from 28.23720 to 28.23421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 43s - loss: 27.5166 - MinusLogProbMetric: 27.5166 - val_loss: 28.2342 - val_MinusLogProbMetric: 28.2342 - lr: 5.2083e-06 - 43s/epoch - 218ms/step
Epoch 811/1000
2023-10-26 10:56:50.374 
Epoch 811/1000 
	 loss: 27.5158, MinusLogProbMetric: 27.5158, val_loss: 28.2342, val_MinusLogProbMetric: 28.2342

Epoch 811: val_loss improved from 28.23421 to 28.23416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5158 - MinusLogProbMetric: 27.5158 - val_loss: 28.2342 - val_MinusLogProbMetric: 28.2342 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 812/1000
2023-10-26 10:57:32.524 
Epoch 812/1000 
	 loss: 27.5163, MinusLogProbMetric: 27.5163, val_loss: 28.2344, val_MinusLogProbMetric: 28.2344

Epoch 812: val_loss did not improve from 28.23416
196/196 - 42s - loss: 27.5163 - MinusLogProbMetric: 27.5163 - val_loss: 28.2344 - val_MinusLogProbMetric: 28.2344 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 813/1000
2023-10-26 10:58:13.987 
Epoch 813/1000 
	 loss: 27.5154, MinusLogProbMetric: 27.5154, val_loss: 28.2330, val_MinusLogProbMetric: 28.2330

Epoch 813: val_loss improved from 28.23416 to 28.23296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5154 - MinusLogProbMetric: 27.5154 - val_loss: 28.2330 - val_MinusLogProbMetric: 28.2330 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 814/1000
2023-10-26 10:58:55.702 
Epoch 814/1000 
	 loss: 27.5163, MinusLogProbMetric: 27.5163, val_loss: 28.2334, val_MinusLogProbMetric: 28.2334

Epoch 814: val_loss did not improve from 28.23296
196/196 - 41s - loss: 27.5163 - MinusLogProbMetric: 27.5163 - val_loss: 28.2334 - val_MinusLogProbMetric: 28.2334 - lr: 5.2083e-06 - 41s/epoch - 209ms/step
Epoch 815/1000
2023-10-26 10:59:37.342 
Epoch 815/1000 
	 loss: 27.5166, MinusLogProbMetric: 27.5166, val_loss: 28.2322, val_MinusLogProbMetric: 28.2322

Epoch 815: val_loss improved from 28.23296 to 28.23225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5166 - MinusLogProbMetric: 27.5166 - val_loss: 28.2322 - val_MinusLogProbMetric: 28.2322 - lr: 5.2083e-06 - 42s/epoch - 216ms/step
Epoch 816/1000
2023-10-26 11:00:19.750 
Epoch 816/1000 
	 loss: 27.5161, MinusLogProbMetric: 27.5161, val_loss: 28.2367, val_MinusLogProbMetric: 28.2367

Epoch 816: val_loss did not improve from 28.23225
196/196 - 42s - loss: 27.5161 - MinusLogProbMetric: 27.5161 - val_loss: 28.2367 - val_MinusLogProbMetric: 28.2367 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 817/1000
2023-10-26 11:01:01.192 
Epoch 817/1000 
	 loss: 27.5162, MinusLogProbMetric: 27.5162, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 817: val_loss did not improve from 28.23225
196/196 - 41s - loss: 27.5162 - MinusLogProbMetric: 27.5162 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 818/1000
2023-10-26 11:01:43.094 
Epoch 818/1000 
	 loss: 27.5156, MinusLogProbMetric: 27.5156, val_loss: 28.2350, val_MinusLogProbMetric: 28.2350

Epoch 818: val_loss did not improve from 28.23225
196/196 - 42s - loss: 27.5156 - MinusLogProbMetric: 27.5156 - val_loss: 28.2350 - val_MinusLogProbMetric: 28.2350 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 819/1000
2023-10-26 11:02:24.597 
Epoch 819/1000 
	 loss: 27.5159, MinusLogProbMetric: 27.5159, val_loss: 28.2306, val_MinusLogProbMetric: 28.2306

Epoch 819: val_loss improved from 28.23225 to 28.23057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5159 - MinusLogProbMetric: 27.5159 - val_loss: 28.2306 - val_MinusLogProbMetric: 28.2306 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 820/1000
2023-10-26 11:03:06.868 
Epoch 820/1000 
	 loss: 27.5154, MinusLogProbMetric: 27.5154, val_loss: 28.2329, val_MinusLogProbMetric: 28.2329

Epoch 820: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5154 - MinusLogProbMetric: 27.5154 - val_loss: 28.2329 - val_MinusLogProbMetric: 28.2329 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 821/1000
2023-10-26 11:03:48.977 
Epoch 821/1000 
	 loss: 27.5159, MinusLogProbMetric: 27.5159, val_loss: 28.2351, val_MinusLogProbMetric: 28.2351

Epoch 821: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5159 - MinusLogProbMetric: 27.5159 - val_loss: 28.2351 - val_MinusLogProbMetric: 28.2351 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 822/1000
2023-10-26 11:04:30.747 
Epoch 822/1000 
	 loss: 27.5158, MinusLogProbMetric: 27.5158, val_loss: 28.2335, val_MinusLogProbMetric: 28.2335

Epoch 822: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5158 - MinusLogProbMetric: 27.5158 - val_loss: 28.2335 - val_MinusLogProbMetric: 28.2335 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 823/1000
2023-10-26 11:05:11.993 
Epoch 823/1000 
	 loss: 27.5163, MinusLogProbMetric: 27.5163, val_loss: 28.2369, val_MinusLogProbMetric: 28.2369

Epoch 823: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5163 - MinusLogProbMetric: 27.5163 - val_loss: 28.2369 - val_MinusLogProbMetric: 28.2369 - lr: 5.2083e-06 - 41s/epoch - 210ms/step
Epoch 824/1000
2023-10-26 11:05:53.545 
Epoch 824/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.2352, val_MinusLogProbMetric: 28.2352

Epoch 824: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.2352 - val_MinusLogProbMetric: 28.2352 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 825/1000
2023-10-26 11:06:35.168 
Epoch 825/1000 
	 loss: 27.5155, MinusLogProbMetric: 27.5155, val_loss: 28.2348, val_MinusLogProbMetric: 28.2348

Epoch 825: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5155 - MinusLogProbMetric: 27.5155 - val_loss: 28.2348 - val_MinusLogProbMetric: 28.2348 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 826/1000
2023-10-26 11:07:17.073 
Epoch 826/1000 
	 loss: 27.5158, MinusLogProbMetric: 27.5158, val_loss: 28.2392, val_MinusLogProbMetric: 28.2392

Epoch 826: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5158 - MinusLogProbMetric: 27.5158 - val_loss: 28.2392 - val_MinusLogProbMetric: 28.2392 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 827/1000
2023-10-26 11:07:58.921 
Epoch 827/1000 
	 loss: 27.5149, MinusLogProbMetric: 27.5149, val_loss: 28.2363, val_MinusLogProbMetric: 28.2363

Epoch 827: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5149 - MinusLogProbMetric: 27.5149 - val_loss: 28.2363 - val_MinusLogProbMetric: 28.2363 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 828/1000
2023-10-26 11:08:40.813 
Epoch 828/1000 
	 loss: 27.5147, MinusLogProbMetric: 27.5147, val_loss: 28.2331, val_MinusLogProbMetric: 28.2331

Epoch 828: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5147 - MinusLogProbMetric: 27.5147 - val_loss: 28.2331 - val_MinusLogProbMetric: 28.2331 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 829/1000
2023-10-26 11:09:22.829 
Epoch 829/1000 
	 loss: 27.5159, MinusLogProbMetric: 27.5159, val_loss: 28.2378, val_MinusLogProbMetric: 28.2378

Epoch 829: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5159 - MinusLogProbMetric: 27.5159 - val_loss: 28.2378 - val_MinusLogProbMetric: 28.2378 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 830/1000
2023-10-26 11:10:04.885 
Epoch 830/1000 
	 loss: 27.5156, MinusLogProbMetric: 27.5156, val_loss: 28.2336, val_MinusLogProbMetric: 28.2336

Epoch 830: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5156 - MinusLogProbMetric: 27.5156 - val_loss: 28.2336 - val_MinusLogProbMetric: 28.2336 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 831/1000
2023-10-26 11:10:46.730 
Epoch 831/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 831: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 832/1000
2023-10-26 11:11:28.618 
Epoch 832/1000 
	 loss: 27.5157, MinusLogProbMetric: 27.5157, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 832: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5157 - MinusLogProbMetric: 27.5157 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 833/1000
2023-10-26 11:12:10.375 
Epoch 833/1000 
	 loss: 27.5151, MinusLogProbMetric: 27.5151, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 833: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5151 - MinusLogProbMetric: 27.5151 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 834/1000
2023-10-26 11:12:51.503 
Epoch 834/1000 
	 loss: 27.5154, MinusLogProbMetric: 27.5154, val_loss: 28.2370, val_MinusLogProbMetric: 28.2370

Epoch 834: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5154 - MinusLogProbMetric: 27.5154 - val_loss: 28.2370 - val_MinusLogProbMetric: 28.2370 - lr: 5.2083e-06 - 41s/epoch - 210ms/step
Epoch 835/1000
2023-10-26 11:13:32.804 
Epoch 835/1000 
	 loss: 27.5146, MinusLogProbMetric: 27.5146, val_loss: 28.2366, val_MinusLogProbMetric: 28.2366

Epoch 835: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5146 - MinusLogProbMetric: 27.5146 - val_loss: 28.2366 - val_MinusLogProbMetric: 28.2366 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 836/1000
2023-10-26 11:14:14.408 
Epoch 836/1000 
	 loss: 27.5149, MinusLogProbMetric: 27.5149, val_loss: 28.2356, val_MinusLogProbMetric: 28.2356

Epoch 836: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5149 - MinusLogProbMetric: 27.5149 - val_loss: 28.2356 - val_MinusLogProbMetric: 28.2356 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 837/1000
2023-10-26 11:14:55.249 
Epoch 837/1000 
	 loss: 27.5148, MinusLogProbMetric: 27.5148, val_loss: 28.2387, val_MinusLogProbMetric: 28.2387

Epoch 837: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5148 - MinusLogProbMetric: 27.5148 - val_loss: 28.2387 - val_MinusLogProbMetric: 28.2387 - lr: 5.2083e-06 - 41s/epoch - 208ms/step
Epoch 838/1000
2023-10-26 11:15:36.561 
Epoch 838/1000 
	 loss: 27.5151, MinusLogProbMetric: 27.5151, val_loss: 28.2335, val_MinusLogProbMetric: 28.2335

Epoch 838: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5151 - MinusLogProbMetric: 27.5151 - val_loss: 28.2335 - val_MinusLogProbMetric: 28.2335 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 839/1000
2023-10-26 11:16:17.903 
Epoch 839/1000 
	 loss: 27.5146, MinusLogProbMetric: 27.5146, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 839: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5146 - MinusLogProbMetric: 27.5146 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 840/1000
2023-10-26 11:16:59.913 
Epoch 840/1000 
	 loss: 27.5146, MinusLogProbMetric: 27.5146, val_loss: 28.2341, val_MinusLogProbMetric: 28.2341

Epoch 840: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5146 - MinusLogProbMetric: 27.5146 - val_loss: 28.2341 - val_MinusLogProbMetric: 28.2341 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 841/1000
2023-10-26 11:17:41.555 
Epoch 841/1000 
	 loss: 27.5139, MinusLogProbMetric: 27.5139, val_loss: 28.2373, val_MinusLogProbMetric: 28.2373

Epoch 841: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5139 - MinusLogProbMetric: 27.5139 - val_loss: 28.2373 - val_MinusLogProbMetric: 28.2373 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 842/1000
2023-10-26 11:18:23.338 
Epoch 842/1000 
	 loss: 27.5150, MinusLogProbMetric: 27.5150, val_loss: 28.2389, val_MinusLogProbMetric: 28.2389

Epoch 842: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5150 - MinusLogProbMetric: 27.5150 - val_loss: 28.2389 - val_MinusLogProbMetric: 28.2389 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 843/1000
2023-10-26 11:19:05.256 
Epoch 843/1000 
	 loss: 27.5145, MinusLogProbMetric: 27.5145, val_loss: 28.2341, val_MinusLogProbMetric: 28.2341

Epoch 843: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5145 - MinusLogProbMetric: 27.5145 - val_loss: 28.2341 - val_MinusLogProbMetric: 28.2341 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 844/1000
2023-10-26 11:19:47.054 
Epoch 844/1000 
	 loss: 27.5153, MinusLogProbMetric: 27.5153, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 844: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5153 - MinusLogProbMetric: 27.5153 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 845/1000
2023-10-26 11:20:29.371 
Epoch 845/1000 
	 loss: 27.5143, MinusLogProbMetric: 27.5143, val_loss: 28.2336, val_MinusLogProbMetric: 28.2336

Epoch 845: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5143 - MinusLogProbMetric: 27.5143 - val_loss: 28.2336 - val_MinusLogProbMetric: 28.2336 - lr: 5.2083e-06 - 42s/epoch - 216ms/step
Epoch 846/1000
2023-10-26 11:21:10.810 
Epoch 846/1000 
	 loss: 27.5144, MinusLogProbMetric: 27.5144, val_loss: 28.2328, val_MinusLogProbMetric: 28.2328

Epoch 846: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5144 - MinusLogProbMetric: 27.5144 - val_loss: 28.2328 - val_MinusLogProbMetric: 28.2328 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 847/1000
2023-10-26 11:21:51.966 
Epoch 847/1000 
	 loss: 27.5142, MinusLogProbMetric: 27.5142, val_loss: 28.2334, val_MinusLogProbMetric: 28.2334

Epoch 847: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5142 - MinusLogProbMetric: 27.5142 - val_loss: 28.2334 - val_MinusLogProbMetric: 28.2334 - lr: 5.2083e-06 - 41s/epoch - 210ms/step
Epoch 848/1000
2023-10-26 11:22:33.775 
Epoch 848/1000 
	 loss: 27.5150, MinusLogProbMetric: 27.5150, val_loss: 28.2335, val_MinusLogProbMetric: 28.2335

Epoch 848: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5150 - MinusLogProbMetric: 27.5150 - val_loss: 28.2335 - val_MinusLogProbMetric: 28.2335 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 849/1000
2023-10-26 11:23:15.757 
Epoch 849/1000 
	 loss: 27.5146, MinusLogProbMetric: 27.5146, val_loss: 28.2354, val_MinusLogProbMetric: 28.2354

Epoch 849: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5146 - MinusLogProbMetric: 27.5146 - val_loss: 28.2354 - val_MinusLogProbMetric: 28.2354 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 850/1000
2023-10-26 11:23:57.468 
Epoch 850/1000 
	 loss: 27.5147, MinusLogProbMetric: 27.5147, val_loss: 28.2342, val_MinusLogProbMetric: 28.2342

Epoch 850: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5147 - MinusLogProbMetric: 27.5147 - val_loss: 28.2342 - val_MinusLogProbMetric: 28.2342 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 851/1000
2023-10-26 11:24:39.419 
Epoch 851/1000 
	 loss: 27.5146, MinusLogProbMetric: 27.5146, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 851: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5146 - MinusLogProbMetric: 27.5146 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 852/1000
2023-10-26 11:25:20.871 
Epoch 852/1000 
	 loss: 27.5145, MinusLogProbMetric: 27.5145, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 852: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5145 - MinusLogProbMetric: 27.5145 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 853/1000
2023-10-26 11:26:02.535 
Epoch 853/1000 
	 loss: 27.5149, MinusLogProbMetric: 27.5149, val_loss: 28.2345, val_MinusLogProbMetric: 28.2345

Epoch 853: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5149 - MinusLogProbMetric: 27.5149 - val_loss: 28.2345 - val_MinusLogProbMetric: 28.2345 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 854/1000
2023-10-26 11:26:43.833 
Epoch 854/1000 
	 loss: 27.5142, MinusLogProbMetric: 27.5142, val_loss: 28.2347, val_MinusLogProbMetric: 28.2347

Epoch 854: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5142 - MinusLogProbMetric: 27.5142 - val_loss: 28.2347 - val_MinusLogProbMetric: 28.2347 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 855/1000
2023-10-26 11:27:25.985 
Epoch 855/1000 
	 loss: 27.5154, MinusLogProbMetric: 27.5154, val_loss: 28.2337, val_MinusLogProbMetric: 28.2337

Epoch 855: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5154 - MinusLogProbMetric: 27.5154 - val_loss: 28.2337 - val_MinusLogProbMetric: 28.2337 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 856/1000
2023-10-26 11:28:07.909 
Epoch 856/1000 
	 loss: 27.5146, MinusLogProbMetric: 27.5146, val_loss: 28.2330, val_MinusLogProbMetric: 28.2330

Epoch 856: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5146 - MinusLogProbMetric: 27.5146 - val_loss: 28.2330 - val_MinusLogProbMetric: 28.2330 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 857/1000
2023-10-26 11:28:49.252 
Epoch 857/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.2349, val_MinusLogProbMetric: 28.2349

Epoch 857: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.2349 - val_MinusLogProbMetric: 28.2349 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 858/1000
2023-10-26 11:29:30.412 
Epoch 858/1000 
	 loss: 27.5144, MinusLogProbMetric: 27.5144, val_loss: 28.2365, val_MinusLogProbMetric: 28.2365

Epoch 858: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5144 - MinusLogProbMetric: 27.5144 - val_loss: 28.2365 - val_MinusLogProbMetric: 28.2365 - lr: 5.2083e-06 - 41s/epoch - 210ms/step
Epoch 859/1000
2023-10-26 11:30:10.306 
Epoch 859/1000 
	 loss: 27.5149, MinusLogProbMetric: 27.5149, val_loss: 28.2375, val_MinusLogProbMetric: 28.2375

Epoch 859: val_loss did not improve from 28.23057
196/196 - 40s - loss: 27.5149 - MinusLogProbMetric: 27.5149 - val_loss: 28.2375 - val_MinusLogProbMetric: 28.2375 - lr: 5.2083e-06 - 40s/epoch - 204ms/step
Epoch 860/1000
2023-10-26 11:30:52.099 
Epoch 860/1000 
	 loss: 27.5157, MinusLogProbMetric: 27.5157, val_loss: 28.2339, val_MinusLogProbMetric: 28.2339

Epoch 860: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5157 - MinusLogProbMetric: 27.5157 - val_loss: 28.2339 - val_MinusLogProbMetric: 28.2339 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 861/1000
2023-10-26 11:31:33.628 
Epoch 861/1000 
	 loss: 27.5145, MinusLogProbMetric: 27.5145, val_loss: 28.2360, val_MinusLogProbMetric: 28.2360

Epoch 861: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5145 - MinusLogProbMetric: 27.5145 - val_loss: 28.2360 - val_MinusLogProbMetric: 28.2360 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 862/1000
2023-10-26 11:32:15.028 
Epoch 862/1000 
	 loss: 27.5144, MinusLogProbMetric: 27.5144, val_loss: 28.2409, val_MinusLogProbMetric: 28.2409

Epoch 862: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5144 - MinusLogProbMetric: 27.5144 - val_loss: 28.2409 - val_MinusLogProbMetric: 28.2409 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 863/1000
2023-10-26 11:32:56.687 
Epoch 863/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.2337, val_MinusLogProbMetric: 28.2337

Epoch 863: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.2337 - val_MinusLogProbMetric: 28.2337 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 864/1000
2023-10-26 11:33:38.084 
Epoch 864/1000 
	 loss: 27.5155, MinusLogProbMetric: 27.5155, val_loss: 28.2337, val_MinusLogProbMetric: 28.2337

Epoch 864: val_loss did not improve from 28.23057
196/196 - 41s - loss: 27.5155 - MinusLogProbMetric: 27.5155 - val_loss: 28.2337 - val_MinusLogProbMetric: 28.2337 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 865/1000
2023-10-26 11:34:19.982 
Epoch 865/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.2351, val_MinusLogProbMetric: 28.2351

Epoch 865: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.2351 - val_MinusLogProbMetric: 28.2351 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 866/1000
2023-10-26 11:35:01.992 
Epoch 866/1000 
	 loss: 27.5149, MinusLogProbMetric: 27.5149, val_loss: 28.2342, val_MinusLogProbMetric: 28.2342

Epoch 866: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5149 - MinusLogProbMetric: 27.5149 - val_loss: 28.2342 - val_MinusLogProbMetric: 28.2342 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 867/1000
2023-10-26 11:35:44.021 
Epoch 867/1000 
	 loss: 27.5149, MinusLogProbMetric: 27.5149, val_loss: 28.2410, val_MinusLogProbMetric: 28.2410

Epoch 867: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5149 - MinusLogProbMetric: 27.5149 - val_loss: 28.2410 - val_MinusLogProbMetric: 28.2410 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 868/1000
2023-10-26 11:36:25.966 
Epoch 868/1000 
	 loss: 27.5142, MinusLogProbMetric: 27.5142, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 868: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5142 - MinusLogProbMetric: 27.5142 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 869/1000
2023-10-26 11:37:07.605 
Epoch 869/1000 
	 loss: 27.5151, MinusLogProbMetric: 27.5151, val_loss: 28.2330, val_MinusLogProbMetric: 28.2330

Epoch 869: val_loss did not improve from 28.23057
196/196 - 42s - loss: 27.5151 - MinusLogProbMetric: 27.5151 - val_loss: 28.2330 - val_MinusLogProbMetric: 28.2330 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 870/1000
2023-10-26 11:37:49.164 
Epoch 870/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2292, val_MinusLogProbMetric: 28.2292

Epoch 870: val_loss improved from 28.23057 to 28.22923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_372/weights/best_weights.h5
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2292 - val_MinusLogProbMetric: 28.2292 - lr: 2.6042e-06 - 42s/epoch - 216ms/step
Epoch 871/1000
2023-10-26 11:38:31.478 
Epoch 871/1000 
	 loss: 27.5104, MinusLogProbMetric: 27.5104, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 871: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5104 - MinusLogProbMetric: 27.5104 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 872/1000
2023-10-26 11:39:13.432 
Epoch 872/1000 
	 loss: 27.5106, MinusLogProbMetric: 27.5106, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 872: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5106 - MinusLogProbMetric: 27.5106 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 873/1000
2023-10-26 11:39:55.214 
Epoch 873/1000 
	 loss: 27.5103, MinusLogProbMetric: 27.5103, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 873: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5103 - MinusLogProbMetric: 27.5103 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 874/1000
2023-10-26 11:40:37.282 
Epoch 874/1000 
	 loss: 27.5107, MinusLogProbMetric: 27.5107, val_loss: 28.2304, val_MinusLogProbMetric: 28.2304

Epoch 874: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5107 - MinusLogProbMetric: 27.5107 - val_loss: 28.2304 - val_MinusLogProbMetric: 28.2304 - lr: 2.6042e-06 - 42s/epoch - 215ms/step
Epoch 875/1000
2023-10-26 11:41:19.022 
Epoch 875/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2324, val_MinusLogProbMetric: 28.2324

Epoch 875: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2324 - val_MinusLogProbMetric: 28.2324 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 876/1000
2023-10-26 11:42:01.299 
Epoch 876/1000 
	 loss: 27.5118, MinusLogProbMetric: 27.5118, val_loss: 28.2327, val_MinusLogProbMetric: 28.2327

Epoch 876: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5118 - MinusLogProbMetric: 27.5118 - val_loss: 28.2327 - val_MinusLogProbMetric: 28.2327 - lr: 2.6042e-06 - 42s/epoch - 216ms/step
Epoch 877/1000
2023-10-26 11:42:42.928 
Epoch 877/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 877: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 878/1000
2023-10-26 11:43:24.840 
Epoch 878/1000 
	 loss: 27.5102, MinusLogProbMetric: 27.5102, val_loss: 28.2325, val_MinusLogProbMetric: 28.2325

Epoch 878: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5102 - MinusLogProbMetric: 27.5102 - val_loss: 28.2325 - val_MinusLogProbMetric: 28.2325 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 879/1000
2023-10-26 11:44:06.629 
Epoch 879/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2313, val_MinusLogProbMetric: 28.2313

Epoch 879: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2313 - val_MinusLogProbMetric: 28.2313 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 880/1000
2023-10-26 11:44:48.097 
Epoch 880/1000 
	 loss: 27.5105, MinusLogProbMetric: 27.5105, val_loss: 28.2301, val_MinusLogProbMetric: 28.2301

Epoch 880: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5105 - MinusLogProbMetric: 27.5105 - val_loss: 28.2301 - val_MinusLogProbMetric: 28.2301 - lr: 2.6042e-06 - 41s/epoch - 212ms/step
Epoch 881/1000
2023-10-26 11:45:29.640 
Epoch 881/1000 
	 loss: 27.5103, MinusLogProbMetric: 27.5103, val_loss: 28.2336, val_MinusLogProbMetric: 28.2336

Epoch 881: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5103 - MinusLogProbMetric: 27.5103 - val_loss: 28.2336 - val_MinusLogProbMetric: 28.2336 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 882/1000
2023-10-26 11:46:11.157 
Epoch 882/1000 
	 loss: 27.5104, MinusLogProbMetric: 27.5104, val_loss: 28.2323, val_MinusLogProbMetric: 28.2323

Epoch 882: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5104 - MinusLogProbMetric: 27.5104 - val_loss: 28.2323 - val_MinusLogProbMetric: 28.2323 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 883/1000
2023-10-26 11:46:53.120 
Epoch 883/1000 
	 loss: 27.5102, MinusLogProbMetric: 27.5102, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 883: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5102 - MinusLogProbMetric: 27.5102 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 884/1000
2023-10-26 11:47:34.648 
Epoch 884/1000 
	 loss: 27.5106, MinusLogProbMetric: 27.5106, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 884: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5106 - MinusLogProbMetric: 27.5106 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 885/1000
2023-10-26 11:48:16.714 
Epoch 885/1000 
	 loss: 27.5109, MinusLogProbMetric: 27.5109, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 885: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5109 - MinusLogProbMetric: 27.5109 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 2.6042e-06 - 42s/epoch - 215ms/step
Epoch 886/1000
2023-10-26 11:48:58.400 
Epoch 886/1000 
	 loss: 27.5103, MinusLogProbMetric: 27.5103, val_loss: 28.2327, val_MinusLogProbMetric: 28.2327

Epoch 886: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5103 - MinusLogProbMetric: 27.5103 - val_loss: 28.2327 - val_MinusLogProbMetric: 28.2327 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 887/1000
2023-10-26 11:49:40.138 
Epoch 887/1000 
	 loss: 27.5101, MinusLogProbMetric: 27.5101, val_loss: 28.2349, val_MinusLogProbMetric: 28.2349

Epoch 887: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5101 - MinusLogProbMetric: 27.5101 - val_loss: 28.2349 - val_MinusLogProbMetric: 28.2349 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 888/1000
2023-10-26 11:50:21.956 
Epoch 888/1000 
	 loss: 27.5106, MinusLogProbMetric: 27.5106, val_loss: 28.2340, val_MinusLogProbMetric: 28.2340

Epoch 888: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5106 - MinusLogProbMetric: 27.5106 - val_loss: 28.2340 - val_MinusLogProbMetric: 28.2340 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 889/1000
2023-10-26 11:51:03.601 
Epoch 889/1000 
	 loss: 27.5097, MinusLogProbMetric: 27.5097, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 889: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5097 - MinusLogProbMetric: 27.5097 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 890/1000
2023-10-26 11:51:45.336 
Epoch 890/1000 
	 loss: 27.5107, MinusLogProbMetric: 27.5107, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 890: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5107 - MinusLogProbMetric: 27.5107 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 891/1000
2023-10-26 11:52:26.804 
Epoch 891/1000 
	 loss: 27.5104, MinusLogProbMetric: 27.5104, val_loss: 28.2329, val_MinusLogProbMetric: 28.2329

Epoch 891: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5104 - MinusLogProbMetric: 27.5104 - val_loss: 28.2329 - val_MinusLogProbMetric: 28.2329 - lr: 2.6042e-06 - 41s/epoch - 212ms/step
Epoch 892/1000
2023-10-26 11:53:08.515 
Epoch 892/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2310, val_MinusLogProbMetric: 28.2310

Epoch 892: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2310 - val_MinusLogProbMetric: 28.2310 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 893/1000
2023-10-26 11:53:50.010 
Epoch 893/1000 
	 loss: 27.5105, MinusLogProbMetric: 27.5105, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 893: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5105 - MinusLogProbMetric: 27.5105 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 2.6042e-06 - 41s/epoch - 212ms/step
Epoch 894/1000
2023-10-26 11:54:31.606 
Epoch 894/1000 
	 loss: 27.5097, MinusLogProbMetric: 27.5097, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 894: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5097 - MinusLogProbMetric: 27.5097 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 895/1000
2023-10-26 11:55:13.547 
Epoch 895/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2329, val_MinusLogProbMetric: 28.2329

Epoch 895: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2329 - val_MinusLogProbMetric: 28.2329 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 896/1000
2023-10-26 11:55:55.063 
Epoch 896/1000 
	 loss: 27.5099, MinusLogProbMetric: 27.5099, val_loss: 28.2313, val_MinusLogProbMetric: 28.2313

Epoch 896: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5099 - MinusLogProbMetric: 27.5099 - val_loss: 28.2313 - val_MinusLogProbMetric: 28.2313 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 897/1000
2023-10-26 11:56:37.443 
Epoch 897/1000 
	 loss: 27.5103, MinusLogProbMetric: 27.5103, val_loss: 28.2302, val_MinusLogProbMetric: 28.2302

Epoch 897: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5103 - MinusLogProbMetric: 27.5103 - val_loss: 28.2302 - val_MinusLogProbMetric: 28.2302 - lr: 2.6042e-06 - 42s/epoch - 216ms/step
Epoch 898/1000
2023-10-26 11:57:19.373 
Epoch 898/1000 
	 loss: 27.5105, MinusLogProbMetric: 27.5105, val_loss: 28.2300, val_MinusLogProbMetric: 28.2300

Epoch 898: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5105 - MinusLogProbMetric: 27.5105 - val_loss: 28.2300 - val_MinusLogProbMetric: 28.2300 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 899/1000
2023-10-26 11:58:01.522 
Epoch 899/1000 
	 loss: 27.5097, MinusLogProbMetric: 27.5097, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 899: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5097 - MinusLogProbMetric: 27.5097 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 2.6042e-06 - 42s/epoch - 215ms/step
Epoch 900/1000
2023-10-26 11:58:42.744 
Epoch 900/1000 
	 loss: 27.5099, MinusLogProbMetric: 27.5099, val_loss: 28.2302, val_MinusLogProbMetric: 28.2302

Epoch 900: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5099 - MinusLogProbMetric: 27.5099 - val_loss: 28.2302 - val_MinusLogProbMetric: 28.2302 - lr: 2.6042e-06 - 41s/epoch - 210ms/step
Epoch 901/1000
2023-10-26 11:59:23.405 
Epoch 901/1000 
	 loss: 27.5095, MinusLogProbMetric: 27.5095, val_loss: 28.2322, val_MinusLogProbMetric: 28.2322

Epoch 901: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5095 - MinusLogProbMetric: 27.5095 - val_loss: 28.2322 - val_MinusLogProbMetric: 28.2322 - lr: 2.6042e-06 - 41s/epoch - 207ms/step
Epoch 902/1000
2023-10-26 12:00:04.515 
Epoch 902/1000 
	 loss: 27.5096, MinusLogProbMetric: 27.5096, val_loss: 28.2321, val_MinusLogProbMetric: 28.2321

Epoch 902: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5096 - MinusLogProbMetric: 27.5096 - val_loss: 28.2321 - val_MinusLogProbMetric: 28.2321 - lr: 2.6042e-06 - 41s/epoch - 210ms/step
Epoch 903/1000
2023-10-26 12:00:45.952 
Epoch 903/1000 
	 loss: 27.5101, MinusLogProbMetric: 27.5101, val_loss: 28.2312, val_MinusLogProbMetric: 28.2312

Epoch 903: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5101 - MinusLogProbMetric: 27.5101 - val_loss: 28.2312 - val_MinusLogProbMetric: 28.2312 - lr: 2.6042e-06 - 41s/epoch - 211ms/step
Epoch 904/1000
2023-10-26 12:01:27.683 
Epoch 904/1000 
	 loss: 27.5105, MinusLogProbMetric: 27.5105, val_loss: 28.2337, val_MinusLogProbMetric: 28.2337

Epoch 904: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5105 - MinusLogProbMetric: 27.5105 - val_loss: 28.2337 - val_MinusLogProbMetric: 28.2337 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 905/1000
2023-10-26 12:02:09.338 
Epoch 905/1000 
	 loss: 27.5095, MinusLogProbMetric: 27.5095, val_loss: 28.2330, val_MinusLogProbMetric: 28.2330

Epoch 905: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5095 - MinusLogProbMetric: 27.5095 - val_loss: 28.2330 - val_MinusLogProbMetric: 28.2330 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 906/1000
2023-10-26 12:02:50.954 
Epoch 906/1000 
	 loss: 27.5103, MinusLogProbMetric: 27.5103, val_loss: 28.2343, val_MinusLogProbMetric: 28.2343

Epoch 906: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5103 - MinusLogProbMetric: 27.5103 - val_loss: 28.2343 - val_MinusLogProbMetric: 28.2343 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 907/1000
2023-10-26 12:03:32.621 
Epoch 907/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2323, val_MinusLogProbMetric: 28.2323

Epoch 907: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2323 - val_MinusLogProbMetric: 28.2323 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 908/1000
2023-10-26 12:04:14.156 
Epoch 908/1000 
	 loss: 27.5098, MinusLogProbMetric: 27.5098, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 908: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5098 - MinusLogProbMetric: 27.5098 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 909/1000
2023-10-26 12:04:55.979 
Epoch 909/1000 
	 loss: 27.5094, MinusLogProbMetric: 27.5094, val_loss: 28.2305, val_MinusLogProbMetric: 28.2305

Epoch 909: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5094 - MinusLogProbMetric: 27.5094 - val_loss: 28.2305 - val_MinusLogProbMetric: 28.2305 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 910/1000
2023-10-26 12:05:37.340 
Epoch 910/1000 
	 loss: 27.5095, MinusLogProbMetric: 27.5095, val_loss: 28.2329, val_MinusLogProbMetric: 28.2329

Epoch 910: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5095 - MinusLogProbMetric: 27.5095 - val_loss: 28.2329 - val_MinusLogProbMetric: 28.2329 - lr: 2.6042e-06 - 41s/epoch - 211ms/step
Epoch 911/1000
2023-10-26 12:06:18.816 
Epoch 911/1000 
	 loss: 27.5102, MinusLogProbMetric: 27.5102, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 911: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5102 - MinusLogProbMetric: 27.5102 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 2.6042e-06 - 41s/epoch - 212ms/step
Epoch 912/1000
2023-10-26 12:07:00.770 
Epoch 912/1000 
	 loss: 27.5092, MinusLogProbMetric: 27.5092, val_loss: 28.2316, val_MinusLogProbMetric: 28.2316

Epoch 912: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5092 - MinusLogProbMetric: 27.5092 - val_loss: 28.2316 - val_MinusLogProbMetric: 28.2316 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 913/1000
2023-10-26 12:07:42.785 
Epoch 913/1000 
	 loss: 27.5101, MinusLogProbMetric: 27.5101, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 913: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5101 - MinusLogProbMetric: 27.5101 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 914/1000
2023-10-26 12:08:23.975 
Epoch 914/1000 
	 loss: 27.5093, MinusLogProbMetric: 27.5093, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 914: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5093 - MinusLogProbMetric: 27.5093 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 2.6042e-06 - 41s/epoch - 210ms/step
Epoch 915/1000
2023-10-26 12:09:05.559 
Epoch 915/1000 
	 loss: 27.5093, MinusLogProbMetric: 27.5093, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 915: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5093 - MinusLogProbMetric: 27.5093 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 916/1000
2023-10-26 12:09:47.416 
Epoch 916/1000 
	 loss: 27.5095, MinusLogProbMetric: 27.5095, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 916: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5095 - MinusLogProbMetric: 27.5095 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 917/1000
2023-10-26 12:10:29.131 
Epoch 917/1000 
	 loss: 27.5097, MinusLogProbMetric: 27.5097, val_loss: 28.2329, val_MinusLogProbMetric: 28.2329

Epoch 917: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5097 - MinusLogProbMetric: 27.5097 - val_loss: 28.2329 - val_MinusLogProbMetric: 28.2329 - lr: 2.6042e-06 - 42s/epoch - 213ms/step
Epoch 918/1000
2023-10-26 12:11:10.771 
Epoch 918/1000 
	 loss: 27.5100, MinusLogProbMetric: 27.5100, val_loss: 28.2345, val_MinusLogProbMetric: 28.2345

Epoch 918: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5100 - MinusLogProbMetric: 27.5100 - val_loss: 28.2345 - val_MinusLogProbMetric: 28.2345 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 919/1000
2023-10-26 12:11:51.847 
Epoch 919/1000 
	 loss: 27.5097, MinusLogProbMetric: 27.5097, val_loss: 28.2314, val_MinusLogProbMetric: 28.2314

Epoch 919: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5097 - MinusLogProbMetric: 27.5097 - val_loss: 28.2314 - val_MinusLogProbMetric: 28.2314 - lr: 2.6042e-06 - 41s/epoch - 210ms/step
Epoch 920/1000
2023-10-26 12:12:33.223 
Epoch 920/1000 
	 loss: 27.5099, MinusLogProbMetric: 27.5099, val_loss: 28.2340, val_MinusLogProbMetric: 28.2340

Epoch 920: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5099 - MinusLogProbMetric: 27.5099 - val_loss: 28.2340 - val_MinusLogProbMetric: 28.2340 - lr: 2.6042e-06 - 41s/epoch - 211ms/step
Epoch 921/1000
2023-10-26 12:13:13.776 
Epoch 921/1000 
	 loss: 27.5079, MinusLogProbMetric: 27.5079, val_loss: 28.2311, val_MinusLogProbMetric: 28.2311

Epoch 921: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5079 - MinusLogProbMetric: 27.5079 - val_loss: 28.2311 - val_MinusLogProbMetric: 28.2311 - lr: 1.3021e-06 - 41s/epoch - 207ms/step
Epoch 922/1000
2023-10-26 12:13:55.708 
Epoch 922/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.2301, val_MinusLogProbMetric: 28.2301

Epoch 922: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.2301 - val_MinusLogProbMetric: 28.2301 - lr: 1.3021e-06 - 42s/epoch - 214ms/step
Epoch 923/1000
2023-10-26 12:14:37.249 
Epoch 923/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.2301, val_MinusLogProbMetric: 28.2301

Epoch 923: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.2301 - val_MinusLogProbMetric: 28.2301 - lr: 1.3021e-06 - 42s/epoch - 212ms/step
Epoch 924/1000
2023-10-26 12:15:18.663 
Epoch 924/1000 
	 loss: 27.5071, MinusLogProbMetric: 27.5071, val_loss: 28.2303, val_MinusLogProbMetric: 28.2303

Epoch 924: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5071 - MinusLogProbMetric: 27.5071 - val_loss: 28.2303 - val_MinusLogProbMetric: 28.2303 - lr: 1.3021e-06 - 41s/epoch - 211ms/step
Epoch 925/1000
2023-10-26 12:16:00.165 
Epoch 925/1000 
	 loss: 27.5070, MinusLogProbMetric: 27.5070, val_loss: 28.2294, val_MinusLogProbMetric: 28.2294

Epoch 925: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5070 - MinusLogProbMetric: 27.5070 - val_loss: 28.2294 - val_MinusLogProbMetric: 28.2294 - lr: 1.3021e-06 - 41s/epoch - 212ms/step
Epoch 926/1000
2023-10-26 12:16:41.220 
Epoch 926/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.2299, val_MinusLogProbMetric: 28.2299

Epoch 926: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.2299 - val_MinusLogProbMetric: 28.2299 - lr: 1.3021e-06 - 41s/epoch - 209ms/step
Epoch 927/1000
2023-10-26 12:17:22.856 
Epoch 927/1000 
	 loss: 27.5070, MinusLogProbMetric: 27.5070, val_loss: 28.2325, val_MinusLogProbMetric: 28.2325

Epoch 927: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5070 - MinusLogProbMetric: 27.5070 - val_loss: 28.2325 - val_MinusLogProbMetric: 28.2325 - lr: 1.3021e-06 - 42s/epoch - 212ms/step
Epoch 928/1000
2023-10-26 12:18:03.935 
Epoch 928/1000 
	 loss: 27.5076, MinusLogProbMetric: 27.5076, val_loss: 28.2311, val_MinusLogProbMetric: 28.2311

Epoch 928: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5076 - MinusLogProbMetric: 27.5076 - val_loss: 28.2311 - val_MinusLogProbMetric: 28.2311 - lr: 1.3021e-06 - 41s/epoch - 210ms/step
Epoch 929/1000
2023-10-26 12:18:45.649 
Epoch 929/1000 
	 loss: 27.5077, MinusLogProbMetric: 27.5077, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 929: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5077 - MinusLogProbMetric: 27.5077 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 1.3021e-06 - 42s/epoch - 213ms/step
Epoch 930/1000
2023-10-26 12:19:27.465 
Epoch 930/1000 
	 loss: 27.5073, MinusLogProbMetric: 27.5073, val_loss: 28.2306, val_MinusLogProbMetric: 28.2306

Epoch 930: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5073 - MinusLogProbMetric: 27.5073 - val_loss: 28.2306 - val_MinusLogProbMetric: 28.2306 - lr: 1.3021e-06 - 42s/epoch - 213ms/step
Epoch 931/1000
2023-10-26 12:20:09.038 
Epoch 931/1000 
	 loss: 27.5070, MinusLogProbMetric: 27.5070, val_loss: 28.2321, val_MinusLogProbMetric: 28.2321

Epoch 931: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5070 - MinusLogProbMetric: 27.5070 - val_loss: 28.2321 - val_MinusLogProbMetric: 28.2321 - lr: 1.3021e-06 - 42s/epoch - 212ms/step
Epoch 932/1000
2023-10-26 12:20:51.121 
Epoch 932/1000 
	 loss: 27.5070, MinusLogProbMetric: 27.5070, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 932: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5070 - MinusLogProbMetric: 27.5070 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 1.3021e-06 - 42s/epoch - 215ms/step
Epoch 933/1000
2023-10-26 12:21:32.776 
Epoch 933/1000 
	 loss: 27.5078, MinusLogProbMetric: 27.5078, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 933: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5078 - MinusLogProbMetric: 27.5078 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 1.3021e-06 - 42s/epoch - 212ms/step
Epoch 934/1000
2023-10-26 12:22:14.448 
Epoch 934/1000 
	 loss: 27.5074, MinusLogProbMetric: 27.5074, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 934: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5074 - MinusLogProbMetric: 27.5074 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 1.3021e-06 - 42s/epoch - 213ms/step
Epoch 935/1000
2023-10-26 12:22:56.443 
Epoch 935/1000 
	 loss: 27.5074, MinusLogProbMetric: 27.5074, val_loss: 28.2315, val_MinusLogProbMetric: 28.2315

Epoch 935: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5074 - MinusLogProbMetric: 27.5074 - val_loss: 28.2315 - val_MinusLogProbMetric: 28.2315 - lr: 1.3021e-06 - 42s/epoch - 214ms/step
Epoch 936/1000
2023-10-26 12:23:38.085 
Epoch 936/1000 
	 loss: 27.5074, MinusLogProbMetric: 27.5074, val_loss: 28.2323, val_MinusLogProbMetric: 28.2323

Epoch 936: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5074 - MinusLogProbMetric: 27.5074 - val_loss: 28.2323 - val_MinusLogProbMetric: 28.2323 - lr: 1.3021e-06 - 42s/epoch - 212ms/step
Epoch 937/1000
2023-10-26 12:24:19.987 
Epoch 937/1000 
	 loss: 27.5076, MinusLogProbMetric: 27.5076, val_loss: 28.2299, val_MinusLogProbMetric: 28.2299

Epoch 937: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5076 - MinusLogProbMetric: 27.5076 - val_loss: 28.2299 - val_MinusLogProbMetric: 28.2299 - lr: 1.3021e-06 - 42s/epoch - 214ms/step
Epoch 938/1000
2023-10-26 12:25:01.395 
Epoch 938/1000 
	 loss: 27.5067, MinusLogProbMetric: 27.5067, val_loss: 28.2322, val_MinusLogProbMetric: 28.2322

Epoch 938: val_loss did not improve from 28.22923
196/196 - 41s - loss: 27.5067 - MinusLogProbMetric: 27.5067 - val_loss: 28.2322 - val_MinusLogProbMetric: 28.2322 - lr: 1.3021e-06 - 41s/epoch - 211ms/step
Epoch 939/1000
2023-10-26 12:25:43.277 
Epoch 939/1000 
	 loss: 27.5075, MinusLogProbMetric: 27.5075, val_loss: 28.2322, val_MinusLogProbMetric: 28.2322

Epoch 939: val_loss did not improve from 28.22923
196/196 - 42s - loss: 27.5075 - MinusLogProbMetric: 27.5075 - val_loss: 28.2322 - val_MinusLogProbMetric: 28.2322 - lr: 1.3021e-06 - 42s/epoch - 214ms/step
Epoch 940/1000
2023-10-26 12:26:19.526 
Epoch 940/1000 
	 loss: 27.5071, MinusLogProbMetric: 27.5071, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 940: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5071 - MinusLogProbMetric: 27.5071 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 1.3021e-06 - 36s/epoch - 185ms/step
Epoch 941/1000
2023-10-26 12:26:55.509 
Epoch 941/1000 
	 loss: 27.5070, MinusLogProbMetric: 27.5070, val_loss: 28.2311, val_MinusLogProbMetric: 28.2311

Epoch 941: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5070 - MinusLogProbMetric: 27.5070 - val_loss: 28.2311 - val_MinusLogProbMetric: 28.2311 - lr: 1.3021e-06 - 36s/epoch - 184ms/step
Epoch 942/1000
2023-10-26 12:27:34.920 
Epoch 942/1000 
	 loss: 27.5073, MinusLogProbMetric: 27.5073, val_loss: 28.2306, val_MinusLogProbMetric: 28.2306

Epoch 942: val_loss did not improve from 28.22923
196/196 - 39s - loss: 27.5073 - MinusLogProbMetric: 27.5073 - val_loss: 28.2306 - val_MinusLogProbMetric: 28.2306 - lr: 1.3021e-06 - 39s/epoch - 201ms/step
Epoch 943/1000
2023-10-26 12:28:13.903 
Epoch 943/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 943: val_loss did not improve from 28.22923
196/196 - 39s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 1.3021e-06 - 39s/epoch - 199ms/step
Epoch 944/1000
2023-10-26 12:28:49.348 
Epoch 944/1000 
	 loss: 27.5069, MinusLogProbMetric: 27.5069, val_loss: 28.2326, val_MinusLogProbMetric: 28.2326

Epoch 944: val_loss did not improve from 28.22923
196/196 - 35s - loss: 27.5069 - MinusLogProbMetric: 27.5069 - val_loss: 28.2326 - val_MinusLogProbMetric: 28.2326 - lr: 1.3021e-06 - 35s/epoch - 181ms/step
Epoch 945/1000
2023-10-26 12:29:25.529 
Epoch 945/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 945: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 1.3021e-06 - 36s/epoch - 185ms/step
Epoch 946/1000
2023-10-26 12:30:05.608 
Epoch 946/1000 
	 loss: 27.5069, MinusLogProbMetric: 27.5069, val_loss: 28.2295, val_MinusLogProbMetric: 28.2295

Epoch 946: val_loss did not improve from 28.22923
196/196 - 40s - loss: 27.5069 - MinusLogProbMetric: 27.5069 - val_loss: 28.2295 - val_MinusLogProbMetric: 28.2295 - lr: 1.3021e-06 - 40s/epoch - 204ms/step
Epoch 947/1000
2023-10-26 12:30:43.177 
Epoch 947/1000 
	 loss: 27.5077, MinusLogProbMetric: 27.5077, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 947: val_loss did not improve from 28.22923
196/196 - 38s - loss: 27.5077 - MinusLogProbMetric: 27.5077 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 1.3021e-06 - 38s/epoch - 192ms/step
Epoch 948/1000
2023-10-26 12:31:18.842 
Epoch 948/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 948: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 1.3021e-06 - 36s/epoch - 182ms/step
Epoch 949/1000
2023-10-26 12:31:55.881 
Epoch 949/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.2306, val_MinusLogProbMetric: 28.2306

Epoch 949: val_loss did not improve from 28.22923
196/196 - 37s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.2306 - val_MinusLogProbMetric: 28.2306 - lr: 1.3021e-06 - 37s/epoch - 189ms/step
Epoch 950/1000
2023-10-26 12:32:36.155 
Epoch 950/1000 
	 loss: 27.5075, MinusLogProbMetric: 27.5075, val_loss: 28.2311, val_MinusLogProbMetric: 28.2311

Epoch 950: val_loss did not improve from 28.22923
196/196 - 40s - loss: 27.5075 - MinusLogProbMetric: 27.5075 - val_loss: 28.2311 - val_MinusLogProbMetric: 28.2311 - lr: 1.3021e-06 - 40s/epoch - 205ms/step
Epoch 951/1000
2023-10-26 12:33:11.922 
Epoch 951/1000 
	 loss: 27.5069, MinusLogProbMetric: 27.5069, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 951: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5069 - MinusLogProbMetric: 27.5069 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 1.3021e-06 - 36s/epoch - 182ms/step
Epoch 952/1000
2023-10-26 12:33:47.310 
Epoch 952/1000 
	 loss: 27.5073, MinusLogProbMetric: 27.5073, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 952: val_loss did not improve from 28.22923
196/196 - 35s - loss: 27.5073 - MinusLogProbMetric: 27.5073 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 1.3021e-06 - 35s/epoch - 181ms/step
Epoch 953/1000
2023-10-26 12:34:25.265 
Epoch 953/1000 
	 loss: 27.5067, MinusLogProbMetric: 27.5067, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 953: val_loss did not improve from 28.22923
196/196 - 38s - loss: 27.5067 - MinusLogProbMetric: 27.5067 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 1.3021e-06 - 38s/epoch - 194ms/step
Epoch 954/1000
2023-10-26 12:35:03.994 
Epoch 954/1000 
	 loss: 27.5074, MinusLogProbMetric: 27.5074, val_loss: 28.2315, val_MinusLogProbMetric: 28.2315

Epoch 954: val_loss did not improve from 28.22923
196/196 - 39s - loss: 27.5074 - MinusLogProbMetric: 27.5074 - val_loss: 28.2315 - val_MinusLogProbMetric: 28.2315 - lr: 1.3021e-06 - 39s/epoch - 198ms/step
Epoch 955/1000
2023-10-26 12:35:39.308 
Epoch 955/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 955: val_loss did not improve from 28.22923
196/196 - 35s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 1.3021e-06 - 35s/epoch - 180ms/step
Epoch 956/1000
2023-10-26 12:36:15.049 
Epoch 956/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 956: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 1.3021e-06 - 36s/epoch - 182ms/step
Epoch 957/1000
2023-10-26 12:36:55.050 
Epoch 957/1000 
	 loss: 27.5071, MinusLogProbMetric: 27.5071, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 957: val_loss did not improve from 28.22923
196/196 - 40s - loss: 27.5071 - MinusLogProbMetric: 27.5071 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 1.3021e-06 - 40s/epoch - 204ms/step
Epoch 958/1000
2023-10-26 12:37:30.952 
Epoch 958/1000 
	 loss: 27.5077, MinusLogProbMetric: 27.5077, val_loss: 28.2312, val_MinusLogProbMetric: 28.2312

Epoch 958: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5077 - MinusLogProbMetric: 27.5077 - val_loss: 28.2312 - val_MinusLogProbMetric: 28.2312 - lr: 1.3021e-06 - 36s/epoch - 183ms/step
Epoch 959/1000
2023-10-26 12:38:06.063 
Epoch 959/1000 
	 loss: 27.5065, MinusLogProbMetric: 27.5065, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 959: val_loss did not improve from 28.22923
196/196 - 35s - loss: 27.5065 - MinusLogProbMetric: 27.5065 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 1.3021e-06 - 35s/epoch - 179ms/step
Epoch 960/1000
2023-10-26 12:38:41.670 
Epoch 960/1000 
	 loss: 27.5071, MinusLogProbMetric: 27.5071, val_loss: 28.2303, val_MinusLogProbMetric: 28.2303

Epoch 960: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5071 - MinusLogProbMetric: 27.5071 - val_loss: 28.2303 - val_MinusLogProbMetric: 28.2303 - lr: 1.3021e-06 - 36s/epoch - 182ms/step
Epoch 961/1000
2023-10-26 12:39:21.546 
Epoch 961/1000 
	 loss: 27.5069, MinusLogProbMetric: 27.5069, val_loss: 28.2310, val_MinusLogProbMetric: 28.2310

Epoch 961: val_loss did not improve from 28.22923
196/196 - 40s - loss: 27.5069 - MinusLogProbMetric: 27.5069 - val_loss: 28.2310 - val_MinusLogProbMetric: 28.2310 - lr: 1.3021e-06 - 40s/epoch - 203ms/step
Epoch 962/1000
2023-10-26 12:39:57.114 
Epoch 962/1000 
	 loss: 27.5071, MinusLogProbMetric: 27.5071, val_loss: 28.2314, val_MinusLogProbMetric: 28.2314

Epoch 962: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5071 - MinusLogProbMetric: 27.5071 - val_loss: 28.2314 - val_MinusLogProbMetric: 28.2314 - lr: 1.3021e-06 - 36s/epoch - 181ms/step
Epoch 963/1000
2023-10-26 12:40:32.366 
Epoch 963/1000 
	 loss: 27.5069, MinusLogProbMetric: 27.5069, val_loss: 28.2310, val_MinusLogProbMetric: 28.2310

Epoch 963: val_loss did not improve from 28.22923
196/196 - 35s - loss: 27.5069 - MinusLogProbMetric: 27.5069 - val_loss: 28.2310 - val_MinusLogProbMetric: 28.2310 - lr: 1.3021e-06 - 35s/epoch - 180ms/step
Epoch 964/1000
2023-10-26 12:41:08.535 
Epoch 964/1000 
	 loss: 27.5066, MinusLogProbMetric: 27.5066, val_loss: 28.2312, val_MinusLogProbMetric: 28.2312

Epoch 964: val_loss did not improve from 28.22923
196/196 - 36s - loss: 27.5066 - MinusLogProbMetric: 27.5066 - val_loss: 28.2312 - val_MinusLogProbMetric: 28.2312 - lr: 1.3021e-06 - 36s/epoch - 185ms/step
Epoch 965/1000
2023-10-26 12:41:48.001 
Epoch 965/1000 
	 loss: 27.5072, MinusLogProbMetric: 27.5072, val_loss: 28.2305, val_MinusLogProbMetric: 28.2305

Epoch 965: val_loss did not improve from 28.22923
196/196 - 39s - loss: 27.5072 - MinusLogProbMetric: 27.5072 - val_loss: 28.2305 - val_MinusLogProbMetric: 28.2305 - lr: 1.3021e-06 - 39s/epoch - 201ms/step
Epoch 966/1000
2023-10-26 12:42:23.307 
Epoch 966/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.2305, val_MinusLogProbMetric: 28.2305

Epoch 966: val_loss did not improve from 28.22923
196/196 - 35s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.2305 - val_MinusLogProbMetric: 28.2305 - lr: 1.3021e-06 - 35s/epoch - 180ms/step
Epoch 967/1000
2023-10-26 12:42:58.805 
Epoch 967/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.2316, val_MinusLogProbMetric: 28.2316

Epoch 967: val_loss did not improve from 28.22923
196/196 - 35s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.2316 - val_MinusLogProbMetric: 28.2316 - lr: 1.3021e-06 - 35s/epoch - 181ms/step
Epoch 968/1000
2023-10-26 12:43:37.756 
Epoch 968/1000 
	 loss: 27.5070, MinusLogProbMetric: 27.5070, val_loss: 28.2315, val_MinusLogProbMetric: 28.2315

Epoch 968: val_loss did not improve from 28.22923
196/196 - 39s - loss: 27.5070 - MinusLogProbMetric: 27.5070 - val_loss: 28.2315 - val_MinusLogProbMetric: 28.2315 - lr: 1.3021e-06 - 39s/epoch - 199ms/step
Epoch 969/1000
2023-10-26 12:44:15.978 
Epoch 969/1000 
	 loss: 27.5071, MinusLogProbMetric: 27.5071, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 969: val_loss did not improve from 28.22923
196/196 - 38s - loss: 27.5071 - MinusLogProbMetric: 27.5071 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 1.3021e-06 - 38s/epoch - 195ms/step
Epoch 970/1000
2023-10-26 12:44:51.400 
Epoch 970/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 970: val_loss did not improve from 28.22923
Restoring model weights from the end of the best epoch: 870.
196/196 - 36s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 1.3021e-06 - 36s/epoch - 183ms/step
Epoch 970: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 721.
Model trained in 39954.43 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.74 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.01 s.
===========
Run 372/720 done in 40047.69 s.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

===========
Generating train data for run 382.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_34"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_35 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7ff4043c41c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4043925f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4043925f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff550286a70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4045fc1c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4045fc730>, <keras.callbacks.ModelCheckpoint object at 0x7ff4045fc7f0>, <keras.callbacks.EarlyStopping object at 0x7ff4045fca60>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4045fca90>, <keras.callbacks.TerminateOnNaN object at 0x7ff4045fc6d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_382/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 12:45:00.344982
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 5: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:47:01.888 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6490.9883, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 121s - loss: nan - MinusLogProbMetric: 6490.9883 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 121s/epoch - 619ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 382.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_45"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_46 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7ff4d7018ac0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d6dc5e40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d6dc5e40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4d6c580d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4d6cc0520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4d6cc0a90>, <keras.callbacks.ModelCheckpoint object at 0x7ff4d6cc0b50>, <keras.callbacks.EarlyStopping object at 0x7ff4d6cc0dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4d6cc0df0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4d6cc0a30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_382/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 12:47:10.676942
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 32: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:49:18.339 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4891.6787, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 4891.6787 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 127s/epoch - 650ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 382.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_56"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_57 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7ff3cc77ddb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff6704e7d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff6704e7d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff48c6da560>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff48c66d930>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff48c66dea0>, <keras.callbacks.ModelCheckpoint object at 0x7ff48c66df60>, <keras.callbacks.EarlyStopping object at 0x7ff48c66e1d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff48c66e200>, <keras.callbacks.TerminateOnNaN object at 0x7ff48c66de40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_382/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 12:49:25.743154
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-26 12:52:18.354 
Epoch 1/1000 
	 loss: 3406.3416, MinusLogProbMetric: 3406.3416, val_loss: 2245.8777, val_MinusLogProbMetric: 2245.8777

Epoch 1: val_loss improved from inf to 2245.87769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 173s - loss: 3406.3416 - MinusLogProbMetric: 3406.3416 - val_loss: 2245.8777 - val_MinusLogProbMetric: 2245.8777 - lr: 1.1111e-04 - 173s/epoch - 883ms/step
Epoch 2/1000
2023-10-26 12:53:18.701 
Epoch 2/1000 
	 loss: 1340.3073, MinusLogProbMetric: 1340.3073, val_loss: 1176.1401, val_MinusLogProbMetric: 1176.1401

Epoch 2: val_loss improved from 2245.87769 to 1176.14014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 1340.3073 - MinusLogProbMetric: 1340.3073 - val_loss: 1176.1401 - val_MinusLogProbMetric: 1176.1401 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 3/1000
2023-10-26 12:54:16.321 
Epoch 3/1000 
	 loss: 770.0291, MinusLogProbMetric: 770.0291, val_loss: 646.9012, val_MinusLogProbMetric: 646.9012

Epoch 3: val_loss improved from 1176.14014 to 646.90125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 770.0291 - MinusLogProbMetric: 770.0291 - val_loss: 646.9012 - val_MinusLogProbMetric: 646.9012 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 4/1000
2023-10-26 12:55:10.894 
Epoch 4/1000 
	 loss: 604.1732, MinusLogProbMetric: 604.1732, val_loss: 540.4530, val_MinusLogProbMetric: 540.4530

Epoch 4: val_loss improved from 646.90125 to 540.45300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 604.1732 - MinusLogProbMetric: 604.1732 - val_loss: 540.4530 - val_MinusLogProbMetric: 540.4530 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 5/1000
2023-10-26 12:56:10.119 
Epoch 5/1000 
	 loss: 505.4627, MinusLogProbMetric: 505.4627, val_loss: 564.4124, val_MinusLogProbMetric: 564.4124

Epoch 5: val_loss did not improve from 540.45300
196/196 - 58s - loss: 505.4627 - MinusLogProbMetric: 505.4627 - val_loss: 564.4124 - val_MinusLogProbMetric: 564.4124 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 6/1000
2023-10-26 12:57:07.473 
Epoch 6/1000 
	 loss: 466.8578, MinusLogProbMetric: 466.8578, val_loss: 434.3405, val_MinusLogProbMetric: 434.3405

Epoch 6: val_loss improved from 540.45300 to 434.34045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 466.8578 - MinusLogProbMetric: 466.8578 - val_loss: 434.3405 - val_MinusLogProbMetric: 434.3405 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 130: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:57:45.702 
Epoch 7/1000 
	 loss: nan, MinusLogProbMetric: 671.9819, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 7: val_loss did not improve from 434.34045
196/196 - 37s - loss: nan - MinusLogProbMetric: 671.9819 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 37s/epoch - 191ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 382.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_67"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_68 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7ff3542f17b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff360711000>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff360711000>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff48c71eb30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff37c6e9e10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff37c6e9240>, <keras.callbacks.ModelCheckpoint object at 0x7ff37c6e9fc0>, <keras.callbacks.EarlyStopping object at 0x7ff37c6e8bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff37c6e8640>, <keras.callbacks.TerminateOnNaN object at 0x7ff37c6e8ca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 12:57:53.875969
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-26 13:00:46.393 
Epoch 1/1000 
	 loss: 504.1668, MinusLogProbMetric: 504.1668, val_loss: 887.3851, val_MinusLogProbMetric: 887.3851

Epoch 1: val_loss improved from inf to 887.38507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 173s - loss: 504.1668 - MinusLogProbMetric: 504.1668 - val_loss: 887.3851 - val_MinusLogProbMetric: 887.3851 - lr: 3.7037e-05 - 173s/epoch - 882ms/step
Epoch 2/1000
2023-10-26 13:01:48.048 
Epoch 2/1000 
	 loss: 628.9623, MinusLogProbMetric: 628.9623, val_loss: 507.7768, val_MinusLogProbMetric: 507.7768

Epoch 2: val_loss improved from 887.38507 to 507.77679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 628.9623 - MinusLogProbMetric: 628.9623 - val_loss: 507.7768 - val_MinusLogProbMetric: 507.7768 - lr: 3.7037e-05 - 61s/epoch - 314ms/step
Epoch 3/1000
2023-10-26 13:02:45.319 
Epoch 3/1000 
	 loss: 521.6215, MinusLogProbMetric: 521.6215, val_loss: 456.2101, val_MinusLogProbMetric: 456.2101

Epoch 3: val_loss improved from 507.77679 to 456.21014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 521.6215 - MinusLogProbMetric: 521.6215 - val_loss: 456.2101 - val_MinusLogProbMetric: 456.2101 - lr: 3.7037e-05 - 57s/epoch - 292ms/step
Epoch 4/1000
2023-10-26 13:03:41.351 
Epoch 4/1000 
	 loss: 420.4906, MinusLogProbMetric: 420.4906, val_loss: 419.8463, val_MinusLogProbMetric: 419.8463

Epoch 4: val_loss improved from 456.21014 to 419.84634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 420.4906 - MinusLogProbMetric: 420.4906 - val_loss: 419.8463 - val_MinusLogProbMetric: 419.8463 - lr: 3.7037e-05 - 56s/epoch - 286ms/step
Epoch 5/1000
2023-10-26 13:04:42.965 
Epoch 5/1000 
	 loss: 389.7030, MinusLogProbMetric: 389.7030, val_loss: 345.1104, val_MinusLogProbMetric: 345.1104

Epoch 5: val_loss improved from 419.84634 to 345.11035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 62s - loss: 389.7030 - MinusLogProbMetric: 389.7030 - val_loss: 345.1104 - val_MinusLogProbMetric: 345.1104 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 6/1000
2023-10-26 13:05:42.329 
Epoch 6/1000 
	 loss: 320.2314, MinusLogProbMetric: 320.2314, val_loss: 294.3949, val_MinusLogProbMetric: 294.3949

Epoch 6: val_loss improved from 345.11035 to 294.39487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 320.2314 - MinusLogProbMetric: 320.2314 - val_loss: 294.3949 - val_MinusLogProbMetric: 294.3949 - lr: 3.7037e-05 - 59s/epoch - 302ms/step
Epoch 7/1000
2023-10-26 13:06:37.359 
Epoch 7/1000 
	 loss: 278.6063, MinusLogProbMetric: 278.6063, val_loss: 262.0337, val_MinusLogProbMetric: 262.0337

Epoch 7: val_loss improved from 294.39487 to 262.03366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 278.6063 - MinusLogProbMetric: 278.6063 - val_loss: 262.0337 - val_MinusLogProbMetric: 262.0337 - lr: 3.7037e-05 - 55s/epoch - 281ms/step
Epoch 8/1000
2023-10-26 13:07:37.127 
Epoch 8/1000 
	 loss: 269.1693, MinusLogProbMetric: 269.1693, val_loss: 247.2079, val_MinusLogProbMetric: 247.2079

Epoch 8: val_loss improved from 262.03366 to 247.20789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 269.1693 - MinusLogProbMetric: 269.1693 - val_loss: 247.2079 - val_MinusLogProbMetric: 247.2079 - lr: 3.7037e-05 - 60s/epoch - 305ms/step
Epoch 9/1000
2023-10-26 13:08:36.465 
Epoch 9/1000 
	 loss: 235.8328, MinusLogProbMetric: 235.8328, val_loss: 226.8414, val_MinusLogProbMetric: 226.8414

Epoch 9: val_loss improved from 247.20789 to 226.84142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 235.8328 - MinusLogProbMetric: 235.8328 - val_loss: 226.8414 - val_MinusLogProbMetric: 226.8414 - lr: 3.7037e-05 - 59s/epoch - 303ms/step
Epoch 10/1000
2023-10-26 13:09:32.089 
Epoch 10/1000 
	 loss: 228.2375, MinusLogProbMetric: 228.2375, val_loss: 221.0973, val_MinusLogProbMetric: 221.0973

Epoch 10: val_loss improved from 226.84142 to 221.09731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 228.2375 - MinusLogProbMetric: 228.2375 - val_loss: 221.0973 - val_MinusLogProbMetric: 221.0973 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 11/1000
2023-10-26 13:10:28.683 
Epoch 11/1000 
	 loss: 270.6304, MinusLogProbMetric: 270.6304, val_loss: 281.1545, val_MinusLogProbMetric: 281.1545

Epoch 11: val_loss did not improve from 221.09731
196/196 - 56s - loss: 270.6304 - MinusLogProbMetric: 270.6304 - val_loss: 281.1545 - val_MinusLogProbMetric: 281.1545 - lr: 3.7037e-05 - 56s/epoch - 285ms/step
Epoch 12/1000
2023-10-26 13:11:25.440 
Epoch 12/1000 
	 loss: 244.5707, MinusLogProbMetric: 244.5707, val_loss: 229.0667, val_MinusLogProbMetric: 229.0667

Epoch 12: val_loss did not improve from 221.09731
196/196 - 57s - loss: 244.5707 - MinusLogProbMetric: 244.5707 - val_loss: 229.0667 - val_MinusLogProbMetric: 229.0667 - lr: 3.7037e-05 - 57s/epoch - 290ms/step
Epoch 13/1000
2023-10-26 13:12:19.523 
Epoch 13/1000 
	 loss: 217.3114, MinusLogProbMetric: 217.3114, val_loss: 217.5374, val_MinusLogProbMetric: 217.5374

Epoch 13: val_loss improved from 221.09731 to 217.53745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 55s - loss: 217.3114 - MinusLogProbMetric: 217.3114 - val_loss: 217.5374 - val_MinusLogProbMetric: 217.5374 - lr: 3.7037e-05 - 55s/epoch - 280ms/step
Epoch 14/1000
2023-10-26 13:13:17.268 
Epoch 14/1000 
	 loss: 214.7832, MinusLogProbMetric: 214.7832, val_loss: 204.2752, val_MinusLogProbMetric: 204.2752

Epoch 14: val_loss improved from 217.53745 to 204.27524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 214.7832 - MinusLogProbMetric: 214.7832 - val_loss: 204.2752 - val_MinusLogProbMetric: 204.2752 - lr: 3.7037e-05 - 58s/epoch - 296ms/step
Epoch 15/1000
2023-10-26 13:14:18.289 
Epoch 15/1000 
	 loss: 197.2326, MinusLogProbMetric: 197.2326, val_loss: 187.7860, val_MinusLogProbMetric: 187.7860

Epoch 15: val_loss improved from 204.27524 to 187.78598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 61s - loss: 197.2326 - MinusLogProbMetric: 197.2326 - val_loss: 187.7860 - val_MinusLogProbMetric: 187.7860 - lr: 3.7037e-05 - 61s/epoch - 310ms/step
Epoch 16/1000
2023-10-26 13:15:13.755 
Epoch 16/1000 
	 loss: 626.2390, MinusLogProbMetric: 626.2390, val_loss: 567.8193, val_MinusLogProbMetric: 567.8193

Epoch 16: val_loss did not improve from 187.78598
196/196 - 55s - loss: 626.2390 - MinusLogProbMetric: 626.2390 - val_loss: 567.8193 - val_MinusLogProbMetric: 567.8193 - lr: 3.7037e-05 - 55s/epoch - 279ms/step
Epoch 17/1000
2023-10-26 13:16:12.002 
Epoch 17/1000 
	 loss: 529.8643, MinusLogProbMetric: 529.8643, val_loss: 419.6204, val_MinusLogProbMetric: 419.6204

Epoch 17: val_loss did not improve from 187.78598
196/196 - 58s - loss: 529.8643 - MinusLogProbMetric: 529.8643 - val_loss: 419.6204 - val_MinusLogProbMetric: 419.6204 - lr: 3.7037e-05 - 58s/epoch - 297ms/step
Epoch 18/1000
2023-10-26 13:17:11.843 
Epoch 18/1000 
	 loss: 357.3369, MinusLogProbMetric: 357.3369, val_loss: 330.0260, val_MinusLogProbMetric: 330.0260

Epoch 18: val_loss did not improve from 187.78598
196/196 - 60s - loss: 357.3369 - MinusLogProbMetric: 357.3369 - val_loss: 330.0260 - val_MinusLogProbMetric: 330.0260 - lr: 3.7037e-05 - 60s/epoch - 305ms/step
Epoch 19/1000
2023-10-26 13:18:05.769 
Epoch 19/1000 
	 loss: 310.2354, MinusLogProbMetric: 310.2354, val_loss: 290.4168, val_MinusLogProbMetric: 290.4168

Epoch 19: val_loss did not improve from 187.78598
196/196 - 54s - loss: 310.2354 - MinusLogProbMetric: 310.2354 - val_loss: 290.4168 - val_MinusLogProbMetric: 290.4168 - lr: 3.7037e-05 - 54s/epoch - 275ms/step
Epoch 20/1000
2023-10-26 13:19:03.138 
Epoch 20/1000 
	 loss: 284.0761, MinusLogProbMetric: 284.0761, val_loss: 274.7827, val_MinusLogProbMetric: 274.7827

Epoch 20: val_loss did not improve from 187.78598
196/196 - 57s - loss: 284.0761 - MinusLogProbMetric: 284.0761 - val_loss: 274.7827 - val_MinusLogProbMetric: 274.7827 - lr: 3.7037e-05 - 57s/epoch - 293ms/step
Epoch 21/1000
2023-10-26 13:20:03.591 
Epoch 21/1000 
	 loss: 260.0792, MinusLogProbMetric: 260.0792, val_loss: 249.1902, val_MinusLogProbMetric: 249.1902

Epoch 21: val_loss did not improve from 187.78598
196/196 - 60s - loss: 260.0792 - MinusLogProbMetric: 260.0792 - val_loss: 249.1902 - val_MinusLogProbMetric: 249.1902 - lr: 3.7037e-05 - 60s/epoch - 308ms/step
Epoch 22/1000
2023-10-26 13:20:58.091 
Epoch 22/1000 
	 loss: 244.9802, MinusLogProbMetric: 244.9802, val_loss: 238.8538, val_MinusLogProbMetric: 238.8538

Epoch 22: val_loss did not improve from 187.78598
196/196 - 54s - loss: 244.9802 - MinusLogProbMetric: 244.9802 - val_loss: 238.8538 - val_MinusLogProbMetric: 238.8538 - lr: 3.7037e-05 - 54s/epoch - 278ms/step
Epoch 23/1000
2023-10-26 13:21:53.756 
Epoch 23/1000 
	 loss: 229.7065, MinusLogProbMetric: 229.7065, val_loss: 223.0225, val_MinusLogProbMetric: 223.0225

Epoch 23: val_loss did not improve from 187.78598
196/196 - 56s - loss: 229.7065 - MinusLogProbMetric: 229.7065 - val_loss: 223.0225 - val_MinusLogProbMetric: 223.0225 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 24/1000
2023-10-26 13:22:55.514 
Epoch 24/1000 
	 loss: 227.7843, MinusLogProbMetric: 227.7843, val_loss: 220.5656, val_MinusLogProbMetric: 220.5656

Epoch 24: val_loss did not improve from 187.78598
196/196 - 62s - loss: 227.7843 - MinusLogProbMetric: 227.7843 - val_loss: 220.5656 - val_MinusLogProbMetric: 220.5656 - lr: 3.7037e-05 - 62s/epoch - 315ms/step
Epoch 25/1000
2023-10-26 13:23:49.216 
Epoch 25/1000 
	 loss: 219.2809, MinusLogProbMetric: 219.2809, val_loss: 222.5284, val_MinusLogProbMetric: 222.5284

Epoch 25: val_loss did not improve from 187.78598
196/196 - 54s - loss: 219.2809 - MinusLogProbMetric: 219.2809 - val_loss: 222.5284 - val_MinusLogProbMetric: 222.5284 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 26/1000
2023-10-26 13:24:43.792 
Epoch 26/1000 
	 loss: 211.9688, MinusLogProbMetric: 211.9688, val_loss: 203.2515, val_MinusLogProbMetric: 203.2515

Epoch 26: val_loss did not improve from 187.78598
196/196 - 55s - loss: 211.9688 - MinusLogProbMetric: 211.9688 - val_loss: 203.2515 - val_MinusLogProbMetric: 203.2515 - lr: 3.7037e-05 - 55s/epoch - 278ms/step
Epoch 27/1000
2023-10-26 13:25:44.774 
Epoch 27/1000 
	 loss: 200.4154, MinusLogProbMetric: 200.4154, val_loss: 203.1870, val_MinusLogProbMetric: 203.1870

Epoch 27: val_loss did not improve from 187.78598
196/196 - 61s - loss: 200.4154 - MinusLogProbMetric: 200.4154 - val_loss: 203.1870 - val_MinusLogProbMetric: 203.1870 - lr: 3.7037e-05 - 61s/epoch - 311ms/step
Epoch 28/1000
2023-10-26 13:26:41.265 
Epoch 28/1000 
	 loss: 194.8183, MinusLogProbMetric: 194.8183, val_loss: 189.6834, val_MinusLogProbMetric: 189.6834

Epoch 28: val_loss did not improve from 187.78598
196/196 - 56s - loss: 194.8183 - MinusLogProbMetric: 194.8183 - val_loss: 189.6834 - val_MinusLogProbMetric: 189.6834 - lr: 3.7037e-05 - 56s/epoch - 288ms/step
Epoch 29/1000
2023-10-26 13:27:35.597 
Epoch 29/1000 
	 loss: 193.4577, MinusLogProbMetric: 193.4577, val_loss: 188.1124, val_MinusLogProbMetric: 188.1124

Epoch 29: val_loss did not improve from 187.78598
196/196 - 54s - loss: 193.4577 - MinusLogProbMetric: 193.4577 - val_loss: 188.1124 - val_MinusLogProbMetric: 188.1124 - lr: 3.7037e-05 - 54s/epoch - 277ms/step
Epoch 30/1000
2023-10-26 13:28:36.359 
Epoch 30/1000 
	 loss: 264.9351, MinusLogProbMetric: 264.9351, val_loss: 359.4078, val_MinusLogProbMetric: 359.4078

Epoch 30: val_loss did not improve from 187.78598
196/196 - 61s - loss: 264.9351 - MinusLogProbMetric: 264.9351 - val_loss: 359.4078 - val_MinusLogProbMetric: 359.4078 - lr: 3.7037e-05 - 61s/epoch - 310ms/step
Epoch 31/1000
2023-10-26 13:29:34.536 
Epoch 31/1000 
	 loss: 312.7436, MinusLogProbMetric: 312.7436, val_loss: 272.4887, val_MinusLogProbMetric: 272.4887

Epoch 31: val_loss did not improve from 187.78598
196/196 - 58s - loss: 312.7436 - MinusLogProbMetric: 312.7436 - val_loss: 272.4887 - val_MinusLogProbMetric: 272.4887 - lr: 3.7037e-05 - 58s/epoch - 297ms/step
Epoch 32/1000
2023-10-26 13:30:28.706 
Epoch 32/1000 
	 loss: 231.1011, MinusLogProbMetric: 231.1011, val_loss: 221.8754, val_MinusLogProbMetric: 221.8754

Epoch 32: val_loss did not improve from 187.78598
196/196 - 54s - loss: 231.1011 - MinusLogProbMetric: 231.1011 - val_loss: 221.8754 - val_MinusLogProbMetric: 221.8754 - lr: 3.7037e-05 - 54s/epoch - 276ms/step
Epoch 33/1000
2023-10-26 13:31:25.722 
Epoch 33/1000 
	 loss: 210.1749, MinusLogProbMetric: 210.1749, val_loss: 202.7841, val_MinusLogProbMetric: 202.7841

Epoch 33: val_loss did not improve from 187.78598
196/196 - 57s - loss: 210.1749 - MinusLogProbMetric: 210.1749 - val_loss: 202.7841 - val_MinusLogProbMetric: 202.7841 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 34/1000
2023-10-26 13:32:24.601 
Epoch 34/1000 
	 loss: 198.2894, MinusLogProbMetric: 198.2894, val_loss: 196.7079, val_MinusLogProbMetric: 196.7079

Epoch 34: val_loss did not improve from 187.78598
196/196 - 59s - loss: 198.2894 - MinusLogProbMetric: 198.2894 - val_loss: 196.7079 - val_MinusLogProbMetric: 196.7079 - lr: 3.7037e-05 - 59s/epoch - 300ms/step
Epoch 35/1000
2023-10-26 13:33:18.252 
Epoch 35/1000 
	 loss: 190.2745, MinusLogProbMetric: 190.2745, val_loss: 185.1324, val_MinusLogProbMetric: 185.1324

Epoch 35: val_loss improved from 187.78598 to 185.13239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 54s - loss: 190.2745 - MinusLogProbMetric: 190.2745 - val_loss: 185.1324 - val_MinusLogProbMetric: 185.1324 - lr: 3.7037e-05 - 54s/epoch - 278ms/step
Epoch 36/1000
2023-10-26 13:34:15.037 
Epoch 36/1000 
	 loss: 182.1479, MinusLogProbMetric: 182.1479, val_loss: 179.8472, val_MinusLogProbMetric: 179.8472

Epoch 36: val_loss improved from 185.13239 to 179.84720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 182.1479 - MinusLogProbMetric: 182.1479 - val_loss: 179.8472 - val_MinusLogProbMetric: 179.8472 - lr: 3.7037e-05 - 57s/epoch - 290ms/step
Epoch 37/1000
2023-10-26 13:35:14.914 
Epoch 37/1000 
	 loss: 175.2238, MinusLogProbMetric: 175.2238, val_loss: 180.1443, val_MinusLogProbMetric: 180.1443

Epoch 37: val_loss did not improve from 179.84720
196/196 - 59s - loss: 175.2238 - MinusLogProbMetric: 175.2238 - val_loss: 180.1443 - val_MinusLogProbMetric: 180.1443 - lr: 3.7037e-05 - 59s/epoch - 301ms/step
Epoch 38/1000
2023-10-26 13:36:09.667 
Epoch 38/1000 
	 loss: 174.9426, MinusLogProbMetric: 174.9426, val_loss: 167.3577, val_MinusLogProbMetric: 167.3577

Epoch 38: val_loss improved from 179.84720 to 167.35770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 174.9426 - MinusLogProbMetric: 174.9426 - val_loss: 167.3577 - val_MinusLogProbMetric: 167.3577 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 39/1000
2023-10-26 13:37:05.661 
Epoch 39/1000 
	 loss: 165.7392, MinusLogProbMetric: 165.7392, val_loss: 162.4923, val_MinusLogProbMetric: 162.4923

Epoch 39: val_loss improved from 167.35770 to 162.49231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 56s - loss: 165.7392 - MinusLogProbMetric: 165.7392 - val_loss: 162.4923 - val_MinusLogProbMetric: 162.4923 - lr: 3.7037e-05 - 56s/epoch - 285ms/step
Epoch 40/1000
2023-10-26 13:38:09.576 
Epoch 40/1000 
	 loss: 181.2571, MinusLogProbMetric: 181.2571, val_loss: 185.1201, val_MinusLogProbMetric: 185.1201

Epoch 40: val_loss did not improve from 162.49231
196/196 - 63s - loss: 181.2571 - MinusLogProbMetric: 181.2571 - val_loss: 185.1201 - val_MinusLogProbMetric: 185.1201 - lr: 3.7037e-05 - 63s/epoch - 322ms/step
Epoch 41/1000
2023-10-26 13:39:07.688 
Epoch 41/1000 
	 loss: 176.1354, MinusLogProbMetric: 176.1354, val_loss: 169.8891, val_MinusLogProbMetric: 169.8891

Epoch 41: val_loss did not improve from 162.49231
196/196 - 58s - loss: 176.1354 - MinusLogProbMetric: 176.1354 - val_loss: 169.8891 - val_MinusLogProbMetric: 169.8891 - lr: 3.7037e-05 - 58s/epoch - 296ms/step
Epoch 42/1000
2023-10-26 13:40:03.176 
Epoch 42/1000 
	 loss: 165.1431, MinusLogProbMetric: 165.1431, val_loss: 164.4773, val_MinusLogProbMetric: 164.4773

Epoch 42: val_loss did not improve from 162.49231
196/196 - 55s - loss: 165.1431 - MinusLogProbMetric: 165.1431 - val_loss: 164.4773 - val_MinusLogProbMetric: 164.4773 - lr: 3.7037e-05 - 55s/epoch - 283ms/step
Epoch 43/1000
2023-10-26 13:41:05.282 
Epoch 43/1000 
	 loss: 159.3826, MinusLogProbMetric: 159.3826, val_loss: 156.0390, val_MinusLogProbMetric: 156.0390

Epoch 43: val_loss improved from 162.49231 to 156.03905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 159.3826 - MinusLogProbMetric: 159.3826 - val_loss: 156.0390 - val_MinusLogProbMetric: 156.0390 - lr: 3.7037e-05 - 63s/epoch - 321ms/step
Epoch 44/1000
2023-10-26 13:42:07.043 
Epoch 44/1000 
	 loss: 162.3933, MinusLogProbMetric: 162.3933, val_loss: 213.3897, val_MinusLogProbMetric: 213.3897

Epoch 44: val_loss did not improve from 156.03905
196/196 - 61s - loss: 162.3933 - MinusLogProbMetric: 162.3933 - val_loss: 213.3897 - val_MinusLogProbMetric: 213.3897 - lr: 3.7037e-05 - 61s/epoch - 311ms/step
Epoch 45/1000
2023-10-26 13:43:03.681 
Epoch 45/1000 
	 loss: 165.2091, MinusLogProbMetric: 165.2091, val_loss: 155.8981, val_MinusLogProbMetric: 155.8981

Epoch 45: val_loss improved from 156.03905 to 155.89809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 165.2091 - MinusLogProbMetric: 165.2091 - val_loss: 155.8981 - val_MinusLogProbMetric: 155.8981 - lr: 3.7037e-05 - 58s/epoch - 294ms/step
Epoch 46/1000
2023-10-26 13:44:03.576 
Epoch 46/1000 
	 loss: 153.2939, MinusLogProbMetric: 153.2939, val_loss: 150.3394, val_MinusLogProbMetric: 150.3394

Epoch 46: val_loss improved from 155.89809 to 150.33936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 153.2939 - MinusLogProbMetric: 153.2939 - val_loss: 150.3394 - val_MinusLogProbMetric: 150.3394 - lr: 3.7037e-05 - 60s/epoch - 305ms/step
Epoch 47/1000
2023-10-26 13:45:06.290 
Epoch 47/1000 
	 loss: 147.4672, MinusLogProbMetric: 147.4672, val_loss: 145.1292, val_MinusLogProbMetric: 145.1292

Epoch 47: val_loss improved from 150.33936 to 145.12917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 63s - loss: 147.4672 - MinusLogProbMetric: 147.4672 - val_loss: 145.1292 - val_MinusLogProbMetric: 145.1292 - lr: 3.7037e-05 - 63s/epoch - 320ms/step
Epoch 48/1000
2023-10-26 13:46:03.575 
Epoch 48/1000 
	 loss: 143.1015, MinusLogProbMetric: 143.1015, val_loss: 141.6157, val_MinusLogProbMetric: 141.6157

Epoch 48: val_loss improved from 145.12917 to 141.61571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 143.1015 - MinusLogProbMetric: 143.1015 - val_loss: 141.6157 - val_MinusLogProbMetric: 141.6157 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 49/1000
2023-10-26 13:47:00.628 
Epoch 49/1000 
	 loss: 140.4008, MinusLogProbMetric: 140.4008, val_loss: 139.2099, val_MinusLogProbMetric: 139.2099

Epoch 49: val_loss improved from 141.61571 to 139.20995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 140.4008 - MinusLogProbMetric: 140.4008 - val_loss: 139.2099 - val_MinusLogProbMetric: 139.2099 - lr: 3.7037e-05 - 57s/epoch - 292ms/step
Epoch 50/1000
2023-10-26 13:48:06.906 
Epoch 50/1000 
	 loss: 137.5104, MinusLogProbMetric: 137.5104, val_loss: 136.3179, val_MinusLogProbMetric: 136.3179

Epoch 50: val_loss improved from 139.20995 to 136.31787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 137.5104 - MinusLogProbMetric: 137.5104 - val_loss: 136.3179 - val_MinusLogProbMetric: 136.3179 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 51/1000
2023-10-26 13:49:06.417 
Epoch 51/1000 
	 loss: 257.0717, MinusLogProbMetric: 257.0717, val_loss: 211.2935, val_MinusLogProbMetric: 211.2935

Epoch 51: val_loss did not improve from 136.31787
196/196 - 59s - loss: 257.0717 - MinusLogProbMetric: 257.0717 - val_loss: 211.2935 - val_MinusLogProbMetric: 211.2935 - lr: 3.7037e-05 - 59s/epoch - 299ms/step
Epoch 52/1000
2023-10-26 13:50:02.028 
Epoch 52/1000 
	 loss: 195.3662, MinusLogProbMetric: 195.3662, val_loss: 184.2369, val_MinusLogProbMetric: 184.2369

Epoch 52: val_loss did not improve from 136.31787
196/196 - 56s - loss: 195.3662 - MinusLogProbMetric: 195.3662 - val_loss: 184.2369 - val_MinusLogProbMetric: 184.2369 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 53/1000
2023-10-26 13:51:02.233 
Epoch 53/1000 
	 loss: 177.0191, MinusLogProbMetric: 177.0191, val_loss: 171.0653, val_MinusLogProbMetric: 171.0653

Epoch 53: val_loss did not improve from 136.31787
196/196 - 60s - loss: 177.0191 - MinusLogProbMetric: 177.0191 - val_loss: 171.0653 - val_MinusLogProbMetric: 171.0653 - lr: 3.7037e-05 - 60s/epoch - 307ms/step
Epoch 54/1000
2023-10-26 13:52:03.847 
Epoch 54/1000 
	 loss: 166.4720, MinusLogProbMetric: 166.4720, val_loss: 160.7982, val_MinusLogProbMetric: 160.7982

Epoch 54: val_loss did not improve from 136.31787
196/196 - 62s - loss: 166.4720 - MinusLogProbMetric: 166.4720 - val_loss: 160.7982 - val_MinusLogProbMetric: 160.7982 - lr: 3.7037e-05 - 62s/epoch - 314ms/step
Epoch 55/1000
2023-10-26 13:52:59.373 
Epoch 55/1000 
	 loss: 164.7484, MinusLogProbMetric: 164.7484, val_loss: 170.3200, val_MinusLogProbMetric: 170.3200

Epoch 55: val_loss did not improve from 136.31787
196/196 - 56s - loss: 164.7484 - MinusLogProbMetric: 164.7484 - val_loss: 170.3200 - val_MinusLogProbMetric: 170.3200 - lr: 3.7037e-05 - 56s/epoch - 283ms/step
Epoch 56/1000
2023-10-26 13:53:54.755 
Epoch 56/1000 
	 loss: 157.1555, MinusLogProbMetric: 157.1555, val_loss: 150.8541, val_MinusLogProbMetric: 150.8541

Epoch 56: val_loss did not improve from 136.31787
196/196 - 55s - loss: 157.1555 - MinusLogProbMetric: 157.1555 - val_loss: 150.8541 - val_MinusLogProbMetric: 150.8541 - lr: 3.7037e-05 - 55s/epoch - 283ms/step
Epoch 57/1000
2023-10-26 13:54:59.893 
Epoch 57/1000 
	 loss: 148.2283, MinusLogProbMetric: 148.2283, val_loss: 145.3073, val_MinusLogProbMetric: 145.3073

Epoch 57: val_loss did not improve from 136.31787
196/196 - 65s - loss: 148.2283 - MinusLogProbMetric: 148.2283 - val_loss: 145.3073 - val_MinusLogProbMetric: 145.3073 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 58/1000
2023-10-26 13:55:55.638 
Epoch 58/1000 
	 loss: 142.9117, MinusLogProbMetric: 142.9117, val_loss: 140.8730, val_MinusLogProbMetric: 140.8730

Epoch 58: val_loss did not improve from 136.31787
196/196 - 56s - loss: 142.9117 - MinusLogProbMetric: 142.9117 - val_loss: 140.8730 - val_MinusLogProbMetric: 140.8730 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 59/1000
2023-10-26 13:56:52.431 
Epoch 59/1000 
	 loss: 138.7657, MinusLogProbMetric: 138.7657, val_loss: 136.4000, val_MinusLogProbMetric: 136.4000

Epoch 59: val_loss did not improve from 136.31787
196/196 - 57s - loss: 138.7657 - MinusLogProbMetric: 138.7657 - val_loss: 136.4000 - val_MinusLogProbMetric: 136.4000 - lr: 3.7037e-05 - 57s/epoch - 290ms/step
Epoch 60/1000
2023-10-26 13:57:55.072 
Epoch 60/1000 
	 loss: 136.6135, MinusLogProbMetric: 136.6135, val_loss: 133.9900, val_MinusLogProbMetric: 133.9900

Epoch 60: val_loss improved from 136.31787 to 133.98999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 64s - loss: 136.6135 - MinusLogProbMetric: 136.6135 - val_loss: 133.9900 - val_MinusLogProbMetric: 133.9900 - lr: 3.7037e-05 - 64s/epoch - 325ms/step
Epoch 61/1000
2023-10-26 13:58:55.249 
Epoch 61/1000 
	 loss: 132.2161, MinusLogProbMetric: 132.2161, val_loss: 130.7075, val_MinusLogProbMetric: 130.7075

Epoch 61: val_loss improved from 133.98999 to 130.70753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 132.2161 - MinusLogProbMetric: 132.2161 - val_loss: 130.7075 - val_MinusLogProbMetric: 130.7075 - lr: 3.7037e-05 - 60s/epoch - 306ms/step
Epoch 62/1000
2023-10-26 13:59:55.425 
Epoch 62/1000 
	 loss: 129.1775, MinusLogProbMetric: 129.1775, val_loss: 128.0466, val_MinusLogProbMetric: 128.0466

Epoch 62: val_loss improved from 130.70753 to 128.04660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 60s - loss: 129.1775 - MinusLogProbMetric: 129.1775 - val_loss: 128.0466 - val_MinusLogProbMetric: 128.0466 - lr: 3.7037e-05 - 60s/epoch - 308ms/step
Epoch 63/1000
2023-10-26 14:00:52.776 
Epoch 63/1000 
	 loss: 126.8327, MinusLogProbMetric: 126.8327, val_loss: 125.5509, val_MinusLogProbMetric: 125.5509

Epoch 63: val_loss improved from 128.04660 to 125.55089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 57s - loss: 126.8327 - MinusLogProbMetric: 126.8327 - val_loss: 125.5509 - val_MinusLogProbMetric: 125.5509 - lr: 3.7037e-05 - 57s/epoch - 293ms/step
Epoch 64/1000
2023-10-26 14:01:51.996 
Epoch 64/1000 
	 loss: 124.3512, MinusLogProbMetric: 124.3512, val_loss: 123.3280, val_MinusLogProbMetric: 123.3280

Epoch 64: val_loss improved from 125.55089 to 123.32796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 59s - loss: 124.3512 - MinusLogProbMetric: 124.3512 - val_loss: 123.3280 - val_MinusLogProbMetric: 123.3280 - lr: 3.7037e-05 - 59s/epoch - 303ms/step
Epoch 65/1000
2023-10-26 14:02:58.695 
Epoch 65/1000 
	 loss: 122.2548, MinusLogProbMetric: 122.2548, val_loss: 121.0669, val_MinusLogProbMetric: 121.0669

Epoch 65: val_loss improved from 123.32796 to 121.06694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 122.2548 - MinusLogProbMetric: 122.2548 - val_loss: 121.0669 - val_MinusLogProbMetric: 121.0669 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 66/1000
2023-10-26 14:04:04.791 
Epoch 66/1000 
	 loss: 120.4588, MinusLogProbMetric: 120.4588, val_loss: 119.1002, val_MinusLogProbMetric: 119.1002

Epoch 66: val_loss improved from 121.06694 to 119.10018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 120.4588 - MinusLogProbMetric: 120.4588 - val_loss: 119.1002 - val_MinusLogProbMetric: 119.1002 - lr: 3.7037e-05 - 66s/epoch - 337ms/step
Epoch 67/1000
2023-10-26 14:05:11.449 
Epoch 67/1000 
	 loss: 117.7873, MinusLogProbMetric: 117.7873, val_loss: 116.6907, val_MinusLogProbMetric: 116.6907

Epoch 67: val_loss improved from 119.10018 to 116.69074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 117.7873 - MinusLogProbMetric: 117.7873 - val_loss: 116.6907 - val_MinusLogProbMetric: 116.6907 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 68/1000
2023-10-26 14:06:18.236 
Epoch 68/1000 
	 loss: 115.7591, MinusLogProbMetric: 115.7591, val_loss: 114.9637, val_MinusLogProbMetric: 114.9637

Epoch 68: val_loss improved from 116.69074 to 114.96371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 115.7591 - MinusLogProbMetric: 115.7591 - val_loss: 114.9637 - val_MinusLogProbMetric: 114.9637 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 69/1000
2023-10-26 14:07:24.138 
Epoch 69/1000 
	 loss: 123.8875, MinusLogProbMetric: 123.8875, val_loss: 116.8298, val_MinusLogProbMetric: 116.8298

Epoch 69: val_loss did not improve from 114.96371
196/196 - 65s - loss: 123.8875 - MinusLogProbMetric: 123.8875 - val_loss: 116.8298 - val_MinusLogProbMetric: 116.8298 - lr: 3.7037e-05 - 65s/epoch - 332ms/step
Epoch 70/1000
2023-10-26 14:08:29.268 
Epoch 70/1000 
	 loss: 115.2090, MinusLogProbMetric: 115.2090, val_loss: 114.0419, val_MinusLogProbMetric: 114.0419

Epoch 70: val_loss improved from 114.96371 to 114.04185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 115.2090 - MinusLogProbMetric: 115.2090 - val_loss: 114.0419 - val_MinusLogProbMetric: 114.0419 - lr: 3.7037e-05 - 66s/epoch - 338ms/step
Epoch 71/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 174: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 14:09:28.057 
Epoch 71/1000 
	 loss: nan, MinusLogProbMetric: 118.5358, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 71: val_loss did not improve from 114.04185
196/196 - 58s - loss: nan - MinusLogProbMetric: 118.5358 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 58s/epoch - 295ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 382.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_78"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_79 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7ff7d03ef5b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4f4247f70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4f4247f70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff670384700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4f46f1600>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4f46f1b70>, <keras.callbacks.ModelCheckpoint object at 0x7ff4f46f1c30>, <keras.callbacks.EarlyStopping object at 0x7ff4f46f1ea0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4f46f1ed0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4f46f1b10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 14:09:38.313330
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-26 14:12:47.282 
Epoch 1/1000 
	 loss: 137.7068, MinusLogProbMetric: 137.7068, val_loss: 119.3112, val_MinusLogProbMetric: 119.3112

Epoch 1: val_loss improved from inf to 119.31118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 190s - loss: 137.7068 - MinusLogProbMetric: 137.7068 - val_loss: 119.3112 - val_MinusLogProbMetric: 119.3112 - lr: 1.2346e-05 - 190s/epoch - 968ms/step
Epoch 2/1000
2023-10-26 14:13:52.990 
Epoch 2/1000 
	 loss: 113.3356, MinusLogProbMetric: 113.3356, val_loss: 109.3773, val_MinusLogProbMetric: 109.3773

Epoch 2: val_loss improved from 119.31118 to 109.37727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 113.3356 - MinusLogProbMetric: 113.3356 - val_loss: 109.3773 - val_MinusLogProbMetric: 109.3773 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 3/1000
2023-10-26 14:14:58.496 
Epoch 3/1000 
	 loss: 109.4607, MinusLogProbMetric: 109.4607, val_loss: 106.6526, val_MinusLogProbMetric: 106.6526

Epoch 3: val_loss improved from 109.37727 to 106.65262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 109.4607 - MinusLogProbMetric: 109.4607 - val_loss: 106.6526 - val_MinusLogProbMetric: 106.6526 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 4/1000
2023-10-26 14:16:03.408 
Epoch 4/1000 
	 loss: 105.6074, MinusLogProbMetric: 105.6074, val_loss: 104.2704, val_MinusLogProbMetric: 104.2704

Epoch 4: val_loss improved from 106.65262 to 104.27039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 105.6074 - MinusLogProbMetric: 105.6074 - val_loss: 104.2704 - val_MinusLogProbMetric: 104.2704 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 5/1000
2023-10-26 14:17:08.945 
Epoch 5/1000 
	 loss: 102.3879, MinusLogProbMetric: 102.3879, val_loss: 101.2208, val_MinusLogProbMetric: 101.2208

Epoch 5: val_loss improved from 104.27039 to 101.22080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 102.3879 - MinusLogProbMetric: 102.3879 - val_loss: 101.2208 - val_MinusLogProbMetric: 101.2208 - lr: 1.2346e-05 - 66s/epoch - 334ms/step
Epoch 6/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 62: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 14:17:33.696 
Epoch 6/1000 
	 loss: nan, MinusLogProbMetric: 106.2958, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 6: val_loss did not improve from 101.22080
196/196 - 24s - loss: nan - MinusLogProbMetric: 106.2958 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 24s/epoch - 122ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 382.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_89"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_90 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7ff6f028aad0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4ac23bd90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4ac23bd90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff7d0267070>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4ac26e500>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4ac26ea70>, <keras.callbacks.ModelCheckpoint object at 0x7ff4ac26eb30>, <keras.callbacks.EarlyStopping object at 0x7ff4ac26eda0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4ac26edd0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4ac26ea10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 14:17:44.679785
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-26 14:20:56.050 
Epoch 1/1000 
	 loss: 99.5558, MinusLogProbMetric: 99.5558, val_loss: 98.3690, val_MinusLogProbMetric: 98.3690

Epoch 1: val_loss improved from inf to 98.36904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 192s - loss: 99.5558 - MinusLogProbMetric: 99.5558 - val_loss: 98.3690 - val_MinusLogProbMetric: 98.3690 - lr: 4.1152e-06 - 192s/epoch - 979ms/step
Epoch 2/1000
2023-10-26 14:22:01.941 
Epoch 2/1000 
	 loss: 95.9179, MinusLogProbMetric: 95.9179, val_loss: 94.5125, val_MinusLogProbMetric: 94.5125

Epoch 2: val_loss improved from 98.36904 to 94.51247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 95.9179 - MinusLogProbMetric: 95.9179 - val_loss: 94.5125 - val_MinusLogProbMetric: 94.5125 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 3/1000
2023-10-26 14:23:09.037 
Epoch 3/1000 
	 loss: 93.2421, MinusLogProbMetric: 93.2421, val_loss: 91.9327, val_MinusLogProbMetric: 91.9327

Epoch 3: val_loss improved from 94.51247 to 91.93269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 93.2421 - MinusLogProbMetric: 93.2421 - val_loss: 91.9327 - val_MinusLogProbMetric: 91.9327 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 4/1000
2023-10-26 14:24:14.933 
Epoch 4/1000 
	 loss: 91.0426, MinusLogProbMetric: 91.0426, val_loss: 89.5652, val_MinusLogProbMetric: 89.5652

Epoch 4: val_loss improved from 91.93269 to 89.56519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 91.0426 - MinusLogProbMetric: 91.0426 - val_loss: 89.5652 - val_MinusLogProbMetric: 89.5652 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 5/1000
2023-10-26 14:25:21.313 
Epoch 5/1000 
	 loss: 90.0814, MinusLogProbMetric: 90.0814, val_loss: 89.1158, val_MinusLogProbMetric: 89.1158

Epoch 5: val_loss improved from 89.56519 to 89.11579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 90.0814 - MinusLogProbMetric: 90.0814 - val_loss: 89.1158 - val_MinusLogProbMetric: 89.1158 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 6/1000
2023-10-26 14:26:27.247 
Epoch 6/1000 
	 loss: 92.0764, MinusLogProbMetric: 92.0764, val_loss: 88.9098, val_MinusLogProbMetric: 88.9098

Epoch 6: val_loss improved from 89.11579 to 88.90979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 92.0764 - MinusLogProbMetric: 92.0764 - val_loss: 88.9098 - val_MinusLogProbMetric: 88.9098 - lr: 4.1152e-06 - 66s/epoch - 336ms/step
Epoch 7/1000
2023-10-26 14:27:32.445 
Epoch 7/1000 
	 loss: 87.1711, MinusLogProbMetric: 87.1711, val_loss: 86.1781, val_MinusLogProbMetric: 86.1781

Epoch 7: val_loss improved from 88.90979 to 86.17814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 87.1711 - MinusLogProbMetric: 87.1711 - val_loss: 86.1781 - val_MinusLogProbMetric: 86.1781 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 8/1000
2023-10-26 14:28:37.537 
Epoch 8/1000 
	 loss: 86.3937, MinusLogProbMetric: 86.3937, val_loss: 86.4579, val_MinusLogProbMetric: 86.4579

Epoch 8: val_loss did not improve from 86.17814
196/196 - 64s - loss: 86.3937 - MinusLogProbMetric: 86.3937 - val_loss: 86.4579 - val_MinusLogProbMetric: 86.4579 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 9/1000
2023-10-26 14:29:42.634 
Epoch 9/1000 
	 loss: 85.4392, MinusLogProbMetric: 85.4392, val_loss: 84.2816, val_MinusLogProbMetric: 84.2816

Epoch 9: val_loss improved from 86.17814 to 84.28156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 85.4392 - MinusLogProbMetric: 85.4392 - val_loss: 84.2816 - val_MinusLogProbMetric: 84.2816 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 10/1000
2023-10-26 14:30:48.921 
Epoch 10/1000 
	 loss: 83.5715, MinusLogProbMetric: 83.5715, val_loss: 83.1728, val_MinusLogProbMetric: 83.1728

Epoch 10: val_loss improved from 84.28156 to 83.17284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 83.5715 - MinusLogProbMetric: 83.5715 - val_loss: 83.1728 - val_MinusLogProbMetric: 83.1728 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 11/1000
2023-10-26 14:31:55.272 
Epoch 11/1000 
	 loss: 82.7118, MinusLogProbMetric: 82.7118, val_loss: 82.9432, val_MinusLogProbMetric: 82.9432

Epoch 11: val_loss improved from 83.17284 to 82.94318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 82.7118 - MinusLogProbMetric: 82.7118 - val_loss: 82.9432 - val_MinusLogProbMetric: 82.9432 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 12/1000
2023-10-26 14:32:59.505 
Epoch 12/1000 
	 loss: 81.8834, MinusLogProbMetric: 81.8834, val_loss: 81.9427, val_MinusLogProbMetric: 81.9427

Epoch 12: val_loss improved from 82.94318 to 81.94270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 81.8834 - MinusLogProbMetric: 81.8834 - val_loss: 81.9427 - val_MinusLogProbMetric: 81.9427 - lr: 4.1152e-06 - 65s/epoch - 329ms/step
Epoch 13/1000
2023-10-26 14:34:05.684 
Epoch 13/1000 
	 loss: 81.0203, MinusLogProbMetric: 81.0203, val_loss: 82.2474, val_MinusLogProbMetric: 82.2474

Epoch 13: val_loss did not improve from 81.94270
196/196 - 65s - loss: 81.0203 - MinusLogProbMetric: 81.0203 - val_loss: 82.2474 - val_MinusLogProbMetric: 82.2474 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 14/1000
2023-10-26 14:35:10.200 
Epoch 14/1000 
	 loss: 79.9706, MinusLogProbMetric: 79.9706, val_loss: 79.5605, val_MinusLogProbMetric: 79.5605

Epoch 14: val_loss improved from 81.94270 to 79.56052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 79.9706 - MinusLogProbMetric: 79.9706 - val_loss: 79.5605 - val_MinusLogProbMetric: 79.5605 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 15/1000
2023-10-26 14:36:15.703 
Epoch 15/1000 
	 loss: 79.5767, MinusLogProbMetric: 79.5767, val_loss: 79.0206, val_MinusLogProbMetric: 79.0206

Epoch 15: val_loss improved from 79.56052 to 79.02061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 79.5767 - MinusLogProbMetric: 79.5767 - val_loss: 79.0206 - val_MinusLogProbMetric: 79.0206 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 16/1000
2023-10-26 14:37:20.757 
Epoch 16/1000 
	 loss: 78.5305, MinusLogProbMetric: 78.5305, val_loss: 78.1292, val_MinusLogProbMetric: 78.1292

Epoch 16: val_loss improved from 79.02061 to 78.12925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 78.5305 - MinusLogProbMetric: 78.5305 - val_loss: 78.1292 - val_MinusLogProbMetric: 78.1292 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 17/1000
2023-10-26 14:38:26.714 
Epoch 17/1000 
	 loss: 78.4491, MinusLogProbMetric: 78.4491, val_loss: 78.1695, val_MinusLogProbMetric: 78.1695

Epoch 17: val_loss did not improve from 78.12925
196/196 - 65s - loss: 78.4491 - MinusLogProbMetric: 78.4491 - val_loss: 78.1695 - val_MinusLogProbMetric: 78.1695 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 18/1000
2023-10-26 14:39:32.140 
Epoch 18/1000 
	 loss: 77.0394, MinusLogProbMetric: 77.0394, val_loss: 76.9513, val_MinusLogProbMetric: 76.9513

Epoch 18: val_loss improved from 78.12925 to 76.95129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 77.0394 - MinusLogProbMetric: 77.0394 - val_loss: 76.9513 - val_MinusLogProbMetric: 76.9513 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 19/1000
2023-10-26 14:40:37.321 
Epoch 19/1000 
	 loss: 76.4202, MinusLogProbMetric: 76.4202, val_loss: 76.1100, val_MinusLogProbMetric: 76.1100

Epoch 19: val_loss improved from 76.95129 to 76.10999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 76.4202 - MinusLogProbMetric: 76.4202 - val_loss: 76.1100 - val_MinusLogProbMetric: 76.1100 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 20/1000
2023-10-26 14:41:42.235 
Epoch 20/1000 
	 loss: 75.6041, MinusLogProbMetric: 75.6041, val_loss: 75.5607, val_MinusLogProbMetric: 75.5607

Epoch 20: val_loss improved from 76.10999 to 75.56067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 65s - loss: 75.6041 - MinusLogProbMetric: 75.6041 - val_loss: 75.5607 - val_MinusLogProbMetric: 75.5607 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 21/1000
2023-10-26 14:42:47.262 
Epoch 21/1000 
	 loss: 77.6959, MinusLogProbMetric: 77.6959, val_loss: 76.7560, val_MinusLogProbMetric: 76.7560

Epoch 21: val_loss did not improve from 75.56067
196/196 - 64s - loss: 77.6959 - MinusLogProbMetric: 77.6959 - val_loss: 76.7560 - val_MinusLogProbMetric: 76.7560 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 22/1000
2023-10-26 14:43:52.681 
Epoch 22/1000 
	 loss: 76.0932, MinusLogProbMetric: 76.0932, val_loss: 84.6469, val_MinusLogProbMetric: 84.6469

Epoch 22: val_loss did not improve from 75.56067
196/196 - 65s - loss: 76.0932 - MinusLogProbMetric: 76.0932 - val_loss: 84.6469 - val_MinusLogProbMetric: 84.6469 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 23/1000
2023-10-26 14:44:56.938 
Epoch 23/1000 
	 loss: 90.5575, MinusLogProbMetric: 90.5575, val_loss: 78.8052, val_MinusLogProbMetric: 78.8052

Epoch 23: val_loss did not improve from 75.56067
196/196 - 64s - loss: 90.5575 - MinusLogProbMetric: 90.5575 - val_loss: 78.8052 - val_MinusLogProbMetric: 78.8052 - lr: 4.1152e-06 - 64s/epoch - 328ms/step
Epoch 24/1000
2023-10-26 14:46:02.441 
Epoch 24/1000 
	 loss: 77.3124, MinusLogProbMetric: 77.3124, val_loss: 76.3761, val_MinusLogProbMetric: 76.3761

Epoch 24: val_loss did not improve from 75.56067
196/196 - 65s - loss: 77.3124 - MinusLogProbMetric: 77.3124 - val_loss: 76.3761 - val_MinusLogProbMetric: 76.3761 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 25/1000
2023-10-26 14:47:07.110 
Epoch 25/1000 
	 loss: 75.4793, MinusLogProbMetric: 75.4793, val_loss: 75.0906, val_MinusLogProbMetric: 75.0906

Epoch 25: val_loss improved from 75.56067 to 75.09064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 75.4793 - MinusLogProbMetric: 75.4793 - val_loss: 75.0906 - val_MinusLogProbMetric: 75.0906 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 26/1000
2023-10-26 14:48:13.796 
Epoch 26/1000 
	 loss: 74.3969, MinusLogProbMetric: 74.3969, val_loss: 74.4620, val_MinusLogProbMetric: 74.4620

Epoch 26: val_loss improved from 75.09064 to 74.46200, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 74.3969 - MinusLogProbMetric: 74.3969 - val_loss: 74.4620 - val_MinusLogProbMetric: 74.4620 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 27/1000
2023-10-26 14:49:20.635 
Epoch 27/1000 
	 loss: 73.6470, MinusLogProbMetric: 73.6470, val_loss: 73.7639, val_MinusLogProbMetric: 73.7639

Epoch 27: val_loss improved from 74.46200 to 73.76391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 73.6470 - MinusLogProbMetric: 73.6470 - val_loss: 73.7639 - val_MinusLogProbMetric: 73.7639 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 28/1000
2023-10-26 14:50:27.356 
Epoch 28/1000 
	 loss: 73.3910, MinusLogProbMetric: 73.3910, val_loss: 73.1110, val_MinusLogProbMetric: 73.1110

Epoch 28: val_loss improved from 73.76391 to 73.11098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 73.3910 - MinusLogProbMetric: 73.3910 - val_loss: 73.1110 - val_MinusLogProbMetric: 73.1110 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 29/1000
2023-10-26 14:51:33.618 
Epoch 29/1000 
	 loss: 74.7843, MinusLogProbMetric: 74.7843, val_loss: 72.5954, val_MinusLogProbMetric: 72.5954

Epoch 29: val_loss improved from 73.11098 to 72.59541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 74.7843 - MinusLogProbMetric: 74.7843 - val_loss: 72.5954 - val_MinusLogProbMetric: 72.5954 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 30/1000
2023-10-26 14:52:40.481 
Epoch 30/1000 
	 loss: 72.1480, MinusLogProbMetric: 72.1480, val_loss: 72.0850, val_MinusLogProbMetric: 72.0850

Epoch 30: val_loss improved from 72.59541 to 72.08504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 72.1480 - MinusLogProbMetric: 72.1480 - val_loss: 72.0850 - val_MinusLogProbMetric: 72.0850 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 31/1000
2023-10-26 14:53:47.191 
Epoch 31/1000 
	 loss: 71.7093, MinusLogProbMetric: 71.7093, val_loss: 71.7037, val_MinusLogProbMetric: 71.7037

Epoch 31: val_loss improved from 72.08504 to 71.70366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 71.7093 - MinusLogProbMetric: 71.7093 - val_loss: 71.7037 - val_MinusLogProbMetric: 71.7037 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 32/1000
2023-10-26 14:54:52.720 
Epoch 32/1000 
	 loss: 71.1558, MinusLogProbMetric: 71.1558, val_loss: 71.1061, val_MinusLogProbMetric: 71.1061

Epoch 32: val_loss improved from 71.70366 to 71.10612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 71.1558 - MinusLogProbMetric: 71.1558 - val_loss: 71.1061 - val_MinusLogProbMetric: 71.1061 - lr: 4.1152e-06 - 66s/epoch - 334ms/step
Epoch 33/1000
2023-10-26 14:55:58.281 
Epoch 33/1000 
	 loss: 73.9215, MinusLogProbMetric: 73.9215, val_loss: 73.5007, val_MinusLogProbMetric: 73.5007

Epoch 33: val_loss did not improve from 71.10612
196/196 - 65s - loss: 73.9215 - MinusLogProbMetric: 73.9215 - val_loss: 73.5007 - val_MinusLogProbMetric: 73.5007 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 34/1000
2023-10-26 14:57:03.467 
Epoch 34/1000 
	 loss: 71.5675, MinusLogProbMetric: 71.5675, val_loss: 70.9308, val_MinusLogProbMetric: 70.9308

Epoch 34: val_loss improved from 71.10612 to 70.93082, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 71.5675 - MinusLogProbMetric: 71.5675 - val_loss: 70.9308 - val_MinusLogProbMetric: 70.9308 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 35/1000
2023-10-26 14:58:09.982 
Epoch 35/1000 
	 loss: 72.3811, MinusLogProbMetric: 72.3811, val_loss: 71.5290, val_MinusLogProbMetric: 71.5290

Epoch 35: val_loss did not improve from 70.93082
196/196 - 65s - loss: 72.3811 - MinusLogProbMetric: 72.3811 - val_loss: 71.5290 - val_MinusLogProbMetric: 71.5290 - lr: 4.1152e-06 - 65s/epoch - 334ms/step
Epoch 36/1000
2023-10-26 14:59:14.993 
Epoch 36/1000 
	 loss: 71.8693, MinusLogProbMetric: 71.8693, val_loss: 71.1709, val_MinusLogProbMetric: 71.1709

Epoch 36: val_loss did not improve from 70.93082
196/196 - 65s - loss: 71.8693 - MinusLogProbMetric: 71.8693 - val_loss: 71.1709 - val_MinusLogProbMetric: 71.1709 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 37/1000
2023-10-26 15:00:19.939 
Epoch 37/1000 
	 loss: 70.6264, MinusLogProbMetric: 70.6264, val_loss: 70.6622, val_MinusLogProbMetric: 70.6622

Epoch 37: val_loss improved from 70.93082 to 70.66224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 70.6264 - MinusLogProbMetric: 70.6264 - val_loss: 70.6622 - val_MinusLogProbMetric: 70.6622 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 38/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 162: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:01:16.415 
Epoch 38/1000 
	 loss: nan, MinusLogProbMetric: 102.7700, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 38: val_loss did not improve from 70.66224
196/196 - 55s - loss: nan - MinusLogProbMetric: 102.7700 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 55s/epoch - 283ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 382.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_100"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_101 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7ff36c43dc00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff5506838b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff5506838b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff32d5cff70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4f44d7ac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4f44d7fd0>, <keras.callbacks.ModelCheckpoint object at 0x7ff4f442c130>, <keras.callbacks.EarlyStopping object at 0x7ff4f442c3a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4f442c3d0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4f442c040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 15:01:27.203998
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-26 15:04:37.342 
Epoch 1/1000 
	 loss: 69.9089, MinusLogProbMetric: 69.9089, val_loss: 69.6226, val_MinusLogProbMetric: 69.6226

Epoch 1: val_loss improved from inf to 69.62257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 191s - loss: 69.9089 - MinusLogProbMetric: 69.9089 - val_loss: 69.6226 - val_MinusLogProbMetric: 69.6226 - lr: 1.3717e-06 - 191s/epoch - 974ms/step
Epoch 2/1000
2023-10-26 15:05:43.643 
Epoch 2/1000 
	 loss: 69.3495, MinusLogProbMetric: 69.3495, val_loss: 69.3275, val_MinusLogProbMetric: 69.3275

Epoch 2: val_loss improved from 69.62257 to 69.32755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 69.3495 - MinusLogProbMetric: 69.3495 - val_loss: 69.3275 - val_MinusLogProbMetric: 69.3275 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 3/1000
2023-10-26 15:06:50.161 
Epoch 3/1000 
	 loss: 68.9479, MinusLogProbMetric: 68.9479, val_loss: 68.7113, val_MinusLogProbMetric: 68.7113

Epoch 3: val_loss improved from 69.32755 to 68.71126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 68.9479 - MinusLogProbMetric: 68.9479 - val_loss: 68.7113 - val_MinusLogProbMetric: 68.7113 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 4/1000
2023-10-26 15:07:56.638 
Epoch 4/1000 
	 loss: 68.6384, MinusLogProbMetric: 68.6384, val_loss: 68.3397, val_MinusLogProbMetric: 68.3397

Epoch 4: val_loss improved from 68.71126 to 68.33974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 68.6384 - MinusLogProbMetric: 68.6384 - val_loss: 68.3397 - val_MinusLogProbMetric: 68.3397 - lr: 1.3717e-06 - 66s/epoch - 338ms/step
Epoch 5/1000
2023-10-26 15:09:02.354 
Epoch 5/1000 
	 loss: 68.0663, MinusLogProbMetric: 68.0663, val_loss: 67.8425, val_MinusLogProbMetric: 67.8425

Epoch 5: val_loss improved from 68.33974 to 67.84245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 68.0663 - MinusLogProbMetric: 68.0663 - val_loss: 67.8425 - val_MinusLogProbMetric: 67.8425 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 6/1000
2023-10-26 15:09:59.967 
Epoch 6/1000 
	 loss: 68.3173, MinusLogProbMetric: 68.3173, val_loss: 67.7283, val_MinusLogProbMetric: 67.7283

Epoch 6: val_loss improved from 67.84245 to 67.72830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 58s - loss: 68.3173 - MinusLogProbMetric: 68.3173 - val_loss: 67.7283 - val_MinusLogProbMetric: 67.7283 - lr: 1.3717e-06 - 58s/epoch - 294ms/step
Epoch 7/1000
2023-10-26 15:11:06.332 
Epoch 7/1000 
	 loss: 67.2667, MinusLogProbMetric: 67.2667, val_loss: 67.1546, val_MinusLogProbMetric: 67.1546

Epoch 7: val_loss improved from 67.72830 to 67.15465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 67.2667 - MinusLogProbMetric: 67.2667 - val_loss: 67.1546 - val_MinusLogProbMetric: 67.1546 - lr: 1.3717e-06 - 66s/epoch - 339ms/step
Epoch 8/1000
2023-10-26 15:12:12.922 
Epoch 8/1000 
	 loss: 67.1959, MinusLogProbMetric: 67.1959, val_loss: 67.0085, val_MinusLogProbMetric: 67.0085

Epoch 8: val_loss improved from 67.15465 to 67.00853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 67.1959 - MinusLogProbMetric: 67.1959 - val_loss: 67.0085 - val_MinusLogProbMetric: 67.0085 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 9/1000
2023-10-26 15:13:19.878 
Epoch 9/1000 
	 loss: 73.2400, MinusLogProbMetric: 73.2400, val_loss: 69.8415, val_MinusLogProbMetric: 69.8415

Epoch 9: val_loss did not improve from 67.00853
196/196 - 66s - loss: 73.2400 - MinusLogProbMetric: 73.2400 - val_loss: 69.8415 - val_MinusLogProbMetric: 69.8415 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 10/1000
2023-10-26 15:14:25.456 
Epoch 10/1000 
	 loss: 67.9953, MinusLogProbMetric: 67.9953, val_loss: 67.1964, val_MinusLogProbMetric: 67.1964

Epoch 10: val_loss did not improve from 67.00853
196/196 - 66s - loss: 67.9953 - MinusLogProbMetric: 67.9953 - val_loss: 67.1964 - val_MinusLogProbMetric: 67.1964 - lr: 1.3717e-06 - 66s/epoch - 335ms/step
Epoch 11/1000
2023-10-26 15:15:30.686 
Epoch 11/1000 
	 loss: 66.7103, MinusLogProbMetric: 66.7103, val_loss: 66.6941, val_MinusLogProbMetric: 66.6941

Epoch 11: val_loss improved from 67.00853 to 66.69415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 66s - loss: 66.7103 - MinusLogProbMetric: 66.7103 - val_loss: 66.6941 - val_MinusLogProbMetric: 66.6941 - lr: 1.3717e-06 - 66s/epoch - 337ms/step
Epoch 12/1000
2023-10-26 15:16:38.048 
Epoch 12/1000 
	 loss: 66.4656, MinusLogProbMetric: 66.4656, val_loss: 66.4197, val_MinusLogProbMetric: 66.4197

Epoch 12: val_loss improved from 66.69415 to 66.41965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 66.4656 - MinusLogProbMetric: 66.4656 - val_loss: 66.4197 - val_MinusLogProbMetric: 66.4197 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 13/1000
2023-10-26 15:17:45.240 
Epoch 13/1000 
	 loss: 66.1218, MinusLogProbMetric: 66.1218, val_loss: 66.3107, val_MinusLogProbMetric: 66.3107

Epoch 13: val_loss improved from 66.41965 to 66.31069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 66.1218 - MinusLogProbMetric: 66.1218 - val_loss: 66.3107 - val_MinusLogProbMetric: 66.3107 - lr: 1.3717e-06 - 67s/epoch - 343ms/step
Epoch 14/1000
2023-10-26 15:18:52.954 
Epoch 14/1000 
	 loss: 65.9603, MinusLogProbMetric: 65.9603, val_loss: 66.0384, val_MinusLogProbMetric: 66.0384

Epoch 14: val_loss improved from 66.31069 to 66.03839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 68s - loss: 65.9603 - MinusLogProbMetric: 65.9603 - val_loss: 66.0384 - val_MinusLogProbMetric: 66.0384 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 15/1000
2023-10-26 15:19:59.950 
Epoch 15/1000 
	 loss: 65.6908, MinusLogProbMetric: 65.6908, val_loss: 65.8161, val_MinusLogProbMetric: 65.8161

Epoch 15: val_loss improved from 66.03839 to 65.81612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 65.6908 - MinusLogProbMetric: 65.6908 - val_loss: 65.8161 - val_MinusLogProbMetric: 65.8161 - lr: 1.3717e-06 - 67s/epoch - 342ms/step
Epoch 16/1000
2023-10-26 15:21:07.599 
Epoch 16/1000 
	 loss: 65.5360, MinusLogProbMetric: 65.5360, val_loss: 65.9713, val_MinusLogProbMetric: 65.9713

Epoch 16: val_loss did not improve from 65.81612
196/196 - 67s - loss: 65.5360 - MinusLogProbMetric: 65.5360 - val_loss: 65.9713 - val_MinusLogProbMetric: 65.9713 - lr: 1.3717e-06 - 67s/epoch - 340ms/step
Epoch 17/1000
2023-10-26 15:22:13.971 
Epoch 17/1000 
	 loss: 65.4625, MinusLogProbMetric: 65.4625, val_loss: 65.5306, val_MinusLogProbMetric: 65.5306

Epoch 17: val_loss improved from 65.81612 to 65.53056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 65.4625 - MinusLogProbMetric: 65.4625 - val_loss: 65.5306 - val_MinusLogProbMetric: 65.5306 - lr: 1.3717e-06 - 67s/epoch - 344ms/step
Epoch 18/1000
2023-10-26 15:23:21.736 
Epoch 18/1000 
	 loss: 65.1895, MinusLogProbMetric: 65.1895, val_loss: 65.2897, val_MinusLogProbMetric: 65.2897

Epoch 18: val_loss improved from 65.53056 to 65.28973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 68s - loss: 65.1895 - MinusLogProbMetric: 65.1895 - val_loss: 65.2897 - val_MinusLogProbMetric: 65.2897 - lr: 1.3717e-06 - 68s/epoch - 345ms/step
Epoch 19/1000
2023-10-26 15:24:29.548 
Epoch 19/1000 
	 loss: 65.0137, MinusLogProbMetric: 65.0137, val_loss: 65.1861, val_MinusLogProbMetric: 65.1861

Epoch 19: val_loss improved from 65.28973 to 65.18606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 68s - loss: 65.0137 - MinusLogProbMetric: 65.0137 - val_loss: 65.1861 - val_MinusLogProbMetric: 65.1861 - lr: 1.3717e-06 - 68s/epoch - 346ms/step
Epoch 20/1000
2023-10-26 15:25:36.831 
Epoch 20/1000 
	 loss: 64.8926, MinusLogProbMetric: 64.8926, val_loss: 64.9231, val_MinusLogProbMetric: 64.9231

Epoch 20: val_loss improved from 65.18606 to 64.92307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 67s - loss: 64.8926 - MinusLogProbMetric: 64.8926 - val_loss: 64.9231 - val_MinusLogProbMetric: 64.9231 - lr: 1.3717e-06 - 67s/epoch - 343ms/step
Epoch 21/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 49: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:25:57.145 
Epoch 21/1000 
	 loss: nan, MinusLogProbMetric: 64.8055, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 21: val_loss did not improve from 64.92307
196/196 - 19s - loss: nan - MinusLogProbMetric: 64.8055 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 19s/epoch - 99ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 382.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_111"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_112 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7ff51c5f50c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3242f7d30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3242f7d30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff3485b6bc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff51c588b50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff51c5890c0>, <keras.callbacks.ModelCheckpoint object at 0x7ff51c589180>, <keras.callbacks.EarlyStopping object at 0x7ff51c5893f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff51c589420>, <keras.callbacks.TerminateOnNaN object at 0x7ff51c589060>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 15:26:08.637095
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
2023-10-26 15:29:18.078 
Epoch 1/1000 
	 loss: 64.9305, MinusLogProbMetric: 64.9305, val_loss: 65.0499, val_MinusLogProbMetric: 65.0499

Epoch 1: val_loss improved from inf to 65.04987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5
196/196 - 190s - loss: 64.9305 - MinusLogProbMetric: 64.9305 - val_loss: 65.0499 - val_MinusLogProbMetric: 65.0499 - lr: 4.5725e-07 - 190s/epoch - 970ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 13: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:29:27.918 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 66.0997, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 65.04987
196/196 - 9s - loss: nan - MinusLogProbMetric: 66.0997 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 9s/epoch - 44ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 382.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_122"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_123 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7ff62840a500>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff75023c640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff75023c640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4d7523dc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4d4ae7610>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4d4ae7b80>, <keras.callbacks.ModelCheckpoint object at 0x7ff4d4ae7c40>, <keras.callbacks.EarlyStopping object at 0x7ff4d4ae7eb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4d4ae7ee0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4d4ae7b20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 15:29:38.752664
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 20: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:31:52.797 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 65.0350, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 65.0350 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 134s/epoch - 682ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 382.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_133"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_134 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7ff32d615840>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff6f0335ab0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff6f0335ab0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff38c5d2d40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff3342184c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff334218a30>, <keras.callbacks.ModelCheckpoint object at 0x7ff334218af0>, <keras.callbacks.EarlyStopping object at 0x7ff334218d60>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff334218d90>, <keras.callbacks.TerminateOnNaN object at 0x7ff3342189d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 15:32:03.477533
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 20: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:34:19.228 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 65.0152, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 65.0152 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 135s/epoch - 691ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 382.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_382/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_382
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_144"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_145 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7ff670157df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff5786c70a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff5786c70a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff305c83280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff31c420d30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_382/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff31c421570>, <keras.callbacks.ModelCheckpoint object at 0x7ff31c420eb0>, <keras.callbacks.EarlyStopping object at 0x7ff31c4215d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff31c420fd0>, <keras.callbacks.TerminateOnNaN object at 0x7ff31c4213f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 382/720 with hyperparameters:
timestamp = 2023-10-26 15:34:31.111683
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 20: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:36:52.999 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 65.1278, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 65.1278 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 142s/epoch - 722ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 382/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

===========
Generating train data for run 387.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_150"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_151 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7ff8284c90f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff6006e1450>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff6006e1450>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff5785dfdf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4d4a718a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4d4a72050>, <keras.callbacks.ModelCheckpoint object at 0x7ff4d4a72740>, <keras.callbacks.EarlyStopping object at 0x7ff4d4a71030>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4d4a73c70>, <keras.callbacks.TerminateOnNaN object at 0x7ff4d4a731f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:36:59.746978
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:38:18.918 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6288.2095, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 79s - loss: nan - MinusLogProbMetric: 6288.2095 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 79s/epoch - 403ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 387.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_156"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_157 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7ff4d60abac0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d69cbd00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d69cbd00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff21c192f80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4d5f12ad0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4d5f13040>, <keras.callbacks.ModelCheckpoint object at 0x7ff4d5f13100>, <keras.callbacks.EarlyStopping object at 0x7ff4d5f13370>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4d5f133a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4d5f12fe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:38:26.052779
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:39:44.317 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6291.5903, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 78s - loss: nan - MinusLogProbMetric: 6291.5903 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 78s/epoch - 398ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 387.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_162"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_163 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7ff4d5c60220>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d5309e40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d5309e40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff14a4be7d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4d5260160>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4d52606d0>, <keras.callbacks.ModelCheckpoint object at 0x7ff4d5260790>, <keras.callbacks.EarlyStopping object at 0x7ff4d5260a00>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4d5260a30>, <keras.callbacks.TerminateOnNaN object at 0x7ff4d5260670>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:39:50.771758
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 8: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:41:12.400 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6243.7656, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 81s - loss: nan - MinusLogProbMetric: 6243.7656 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 81s/epoch - 416ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 3.703703703703703e-05.
===========
Generating train data for run 387.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_168"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_169 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7ff21c79be80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff17cf18df0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff17cf18df0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4f4203070>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff21c75f670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff21c75fbe0>, <keras.callbacks.ModelCheckpoint object at 0x7ff21c75fca0>, <keras.callbacks.EarlyStopping object at 0x7ff21c75ff10>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff21c75ff40>, <keras.callbacks.TerminateOnNaN object at 0x7ff21c75fb80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:41:18.218791
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:42:40.945 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6374.7354, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 83s - loss: nan - MinusLogProbMetric: 6374.7354 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 83s/epoch - 421ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.2345679012345677e-05.
===========
Generating train data for run 387.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_174"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_175 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7ff4d695aec0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff14a4d33d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff14a4d33d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4d6958880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff5e82660b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff5e8264d30>, <keras.callbacks.ModelCheckpoint object at 0x7ff5e8264280>, <keras.callbacks.EarlyStopping object at 0x7ff5e8264580>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff5e8266260>, <keras.callbacks.TerminateOnNaN object at 0x7ff5e8267bb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:42:47.280190
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 13: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:44:06.539 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6403.1143, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 79s - loss: nan - MinusLogProbMetric: 6403.1143 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 79s/epoch - 403ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 4.115226337448558e-06.
===========
Generating train data for run 387.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_180"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_181 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7ff4afd30670>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4af7998a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4af7998a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff305027fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4af30c580>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4af30caf0>, <keras.callbacks.ModelCheckpoint object at 0x7ff4af30cbb0>, <keras.callbacks.EarlyStopping object at 0x7ff4af30ce20>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4af30ce50>, <keras.callbacks.TerminateOnNaN object at 0x7ff4af30ca90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:44:12.918188
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:45:39.023 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6435.3828, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 86s - loss: nan - MinusLogProbMetric: 6435.3828 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 86s/epoch - 438ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.3717421124828526e-06.
===========
Generating train data for run 387.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7ff14aa82ef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff14ac4d4e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff14ac4d4e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff31c39ed10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1b605dfc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff1b605e530>, <keras.callbacks.ModelCheckpoint object at 0x7ff1b605e5f0>, <keras.callbacks.EarlyStopping object at 0x7ff1b605e860>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1b605e890>, <keras.callbacks.TerminateOnNaN object at 0x7ff1b605e4d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:45:45.421985
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:47:04.095 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6449.4604, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 79s - loss: nan - MinusLogProbMetric: 6449.4604 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 79s/epoch - 401ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 4.572473708276175e-07.
===========
Generating train data for run 387.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_192"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_193 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7ff143027e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1b4757f10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1b4757f10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff14b69bd00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff31dd1b700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff31dd1bc70>, <keras.callbacks.ModelCheckpoint object at 0x7ff31dd1bd30>, <keras.callbacks.EarlyStopping object at 0x7ff31dd1bfa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff31dd1bfd0>, <keras.callbacks.TerminateOnNaN object at 0x7ff31dd1bc10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:47:09.413639
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 19: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:48:40.570 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6450.6484, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 91s - loss: nan - MinusLogProbMetric: 6450.6484 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 91s/epoch - 465ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.524157902758725e-07.
===========
Generating train data for run 387.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_198"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_199 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7ff30ceffca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff30456cc70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff30456cc70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff30d10e920>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff30ce6ae30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff30ce6b3a0>, <keras.callbacks.ModelCheckpoint object at 0x7ff30ce6b460>, <keras.callbacks.EarlyStopping object at 0x7ff30ce6b6d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff30ce6b700>, <keras.callbacks.TerminateOnNaN object at 0x7ff30ce6b340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:48:46.844418
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 33: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:50:11.715 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6451.2051, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 85s - loss: nan - MinusLogProbMetric: 6451.2051 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 85s/epoch - 433ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 5.0805263425290834e-08.
===========
Generating train data for run 387.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_204"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_205 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7ff1330b8850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff13195bdf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff13195bdf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff1b701dff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff131eb0fd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff131eb1540>, <keras.callbacks.ModelCheckpoint object at 0x7ff131eb1600>, <keras.callbacks.EarlyStopping object at 0x7ff131eb1870>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff131eb18a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff131eb14e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:50:18.521457
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 33: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:51:47.711 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6451.9287, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 89s - loss: nan - MinusLogProbMetric: 6451.9287 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 89s/epoch - 454ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.6935087808430278e-08.
===========
Generating train data for run 387.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_387/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_387
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_210"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_211 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7ff30427bb80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff31db52170>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff31db52170>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff14380bfd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff21c3eb430>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_387/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff21c3eb9a0>, <keras.callbacks.ModelCheckpoint object at 0x7ff21c3eba60>, <keras.callbacks.EarlyStopping object at 0x7ff21c3ebcd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff21c3ebd00>, <keras.callbacks.TerminateOnNaN object at 0x7ff21c3eb940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_387/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 387/720 with hyperparameters:
timestamp = 2023-10-26 15:51:54.585628
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 33: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 15:53:20.829 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6452.4282, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 86s - loss: nan - MinusLogProbMetric: 6452.4282 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 86s/epoch - 439ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 5.645029269476759e-09.
===========
Run 387/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 388.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_388/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_388/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_388/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_388
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_216"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_217 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7ff143a48850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff132ff4610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff132ff4610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff31d23de10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1433c1240>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff1433c17b0>, <keras.callbacks.ModelCheckpoint object at 0x7ff1433c1870>, <keras.callbacks.EarlyStopping object at 0x7ff1433c1ae0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1433c1b10>, <keras.callbacks.TerminateOnNaN object at 0x7ff1433c1750>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_388/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 388/720 with hyperparameters:
timestamp = 2023-10-26 15:53:27.835757
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
2023-10-26 15:55:41.227 
Epoch 1/1000 
	 loss: 776.0137, MinusLogProbMetric: 776.0137, val_loss: 209.2547, val_MinusLogProbMetric: 209.2547

Epoch 1: val_loss improved from inf to 209.25470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 134s - loss: 776.0137 - MinusLogProbMetric: 776.0137 - val_loss: 209.2547 - val_MinusLogProbMetric: 209.2547 - lr: 0.0010 - 134s/epoch - 683ms/step
Epoch 2/1000
2023-10-26 15:56:25.362 
Epoch 2/1000 
	 loss: 153.4785, MinusLogProbMetric: 153.4785, val_loss: 145.0733, val_MinusLogProbMetric: 145.0733

Epoch 2: val_loss improved from 209.25470 to 145.07330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 153.4785 - MinusLogProbMetric: 153.4785 - val_loss: 145.0733 - val_MinusLogProbMetric: 145.0733 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 3/1000
2023-10-26 15:57:09.901 
Epoch 3/1000 
	 loss: 114.8931, MinusLogProbMetric: 114.8931, val_loss: 92.4355, val_MinusLogProbMetric: 92.4355

Epoch 3: val_loss improved from 145.07330 to 92.43546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 45s - loss: 114.8931 - MinusLogProbMetric: 114.8931 - val_loss: 92.4355 - val_MinusLogProbMetric: 92.4355 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 4/1000
2023-10-26 15:57:53.306 
Epoch 4/1000 
	 loss: 88.4342, MinusLogProbMetric: 88.4342, val_loss: 83.1941, val_MinusLogProbMetric: 83.1941

Epoch 4: val_loss improved from 92.43546 to 83.19407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 88.4342 - MinusLogProbMetric: 88.4342 - val_loss: 83.1941 - val_MinusLogProbMetric: 83.1941 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 5/1000
2023-10-26 15:58:29.952 
Epoch 5/1000 
	 loss: 73.6400, MinusLogProbMetric: 73.6400, val_loss: 65.7711, val_MinusLogProbMetric: 65.7711

Epoch 5: val_loss improved from 83.19407 to 65.77110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 37s - loss: 73.6400 - MinusLogProbMetric: 73.6400 - val_loss: 65.7711 - val_MinusLogProbMetric: 65.7711 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 6/1000
2023-10-26 15:59:05.115 
Epoch 6/1000 
	 loss: 64.1877, MinusLogProbMetric: 64.1877, val_loss: 60.8259, val_MinusLogProbMetric: 60.8259

Epoch 6: val_loss improved from 65.77110 to 60.82592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 35s - loss: 64.1877 - MinusLogProbMetric: 64.1877 - val_loss: 60.8259 - val_MinusLogProbMetric: 60.8259 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 7/1000
2023-10-26 15:59:42.792 
Epoch 7/1000 
	 loss: 58.6563, MinusLogProbMetric: 58.6563, val_loss: 57.3057, val_MinusLogProbMetric: 57.3057

Epoch 7: val_loss improved from 60.82592 to 57.30569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 38s - loss: 58.6563 - MinusLogProbMetric: 58.6563 - val_loss: 57.3057 - val_MinusLogProbMetric: 57.3057 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 8/1000
2023-10-26 16:00:21.348 
Epoch 8/1000 
	 loss: 54.9317, MinusLogProbMetric: 54.9317, val_loss: 53.4772, val_MinusLogProbMetric: 53.4772

Epoch 8: val_loss improved from 57.30569 to 53.47721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 39s - loss: 54.9317 - MinusLogProbMetric: 54.9317 - val_loss: 53.4772 - val_MinusLogProbMetric: 53.4772 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 9/1000
2023-10-26 16:01:02.493 
Epoch 9/1000 
	 loss: 52.3512, MinusLogProbMetric: 52.3512, val_loss: 58.7566, val_MinusLogProbMetric: 58.7566

Epoch 9: val_loss did not improve from 53.47721
196/196 - 40s - loss: 52.3512 - MinusLogProbMetric: 52.3512 - val_loss: 58.7566 - val_MinusLogProbMetric: 58.7566 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 10/1000
2023-10-26 16:01:43.785 
Epoch 10/1000 
	 loss: 53.5309, MinusLogProbMetric: 53.5309, val_loss: 54.6943, val_MinusLogProbMetric: 54.6943

Epoch 10: val_loss did not improve from 53.47721
196/196 - 41s - loss: 53.5309 - MinusLogProbMetric: 53.5309 - val_loss: 54.6943 - val_MinusLogProbMetric: 54.6943 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 11/1000
2023-10-26 16:02:20.183 
Epoch 11/1000 
	 loss: 53.6280, MinusLogProbMetric: 53.6280, val_loss: 48.0884, val_MinusLogProbMetric: 48.0884

Epoch 11: val_loss improved from 53.47721 to 48.08843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 37s - loss: 53.6280 - MinusLogProbMetric: 53.6280 - val_loss: 48.0884 - val_MinusLogProbMetric: 48.0884 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 12/1000
2023-10-26 16:02:56.979 
Epoch 12/1000 
	 loss: 46.8770, MinusLogProbMetric: 46.8770, val_loss: 44.6606, val_MinusLogProbMetric: 44.6606

Epoch 12: val_loss improved from 48.08843 to 44.66059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 37s - loss: 46.8770 - MinusLogProbMetric: 46.8770 - val_loss: 44.6606 - val_MinusLogProbMetric: 44.6606 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 13/1000
2023-10-26 16:03:37.918 
Epoch 13/1000 
	 loss: 46.2814, MinusLogProbMetric: 46.2814, val_loss: 45.9057, val_MinusLogProbMetric: 45.9057

Epoch 13: val_loss did not improve from 44.66059
196/196 - 40s - loss: 46.2814 - MinusLogProbMetric: 46.2814 - val_loss: 45.9057 - val_MinusLogProbMetric: 45.9057 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 14/1000
2023-10-26 16:04:17.292 
Epoch 14/1000 
	 loss: 45.8238, MinusLogProbMetric: 45.8238, val_loss: 44.2924, val_MinusLogProbMetric: 44.2924

Epoch 14: val_loss improved from 44.66059 to 44.29242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 40s - loss: 45.8238 - MinusLogProbMetric: 45.8238 - val_loss: 44.2924 - val_MinusLogProbMetric: 44.2924 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 15/1000
2023-10-26 16:04:54.143 
Epoch 15/1000 
	 loss: 44.0751, MinusLogProbMetric: 44.0751, val_loss: 44.2067, val_MinusLogProbMetric: 44.2067

Epoch 15: val_loss improved from 44.29242 to 44.20671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 37s - loss: 44.0751 - MinusLogProbMetric: 44.0751 - val_loss: 44.2067 - val_MinusLogProbMetric: 44.2067 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 16/1000
2023-10-26 16:05:31.653 
Epoch 16/1000 
	 loss: 43.8251, MinusLogProbMetric: 43.8251, val_loss: 44.9108, val_MinusLogProbMetric: 44.9108

Epoch 16: val_loss did not improve from 44.20671
196/196 - 37s - loss: 43.8251 - MinusLogProbMetric: 43.8251 - val_loss: 44.9108 - val_MinusLogProbMetric: 44.9108 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 17/1000
2023-10-26 16:06:12.195 
Epoch 17/1000 
	 loss: 42.7007, MinusLogProbMetric: 42.7007, val_loss: 41.6440, val_MinusLogProbMetric: 41.6440

Epoch 17: val_loss improved from 44.20671 to 41.64405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 41s - loss: 42.7007 - MinusLogProbMetric: 42.7007 - val_loss: 41.6440 - val_MinusLogProbMetric: 41.6440 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 18/1000
2023-10-26 16:06:56.278 
Epoch 18/1000 
	 loss: 42.2276, MinusLogProbMetric: 42.2276, val_loss: 42.6653, val_MinusLogProbMetric: 42.6653

Epoch 18: val_loss did not improve from 41.64405
196/196 - 43s - loss: 42.2276 - MinusLogProbMetric: 42.2276 - val_loss: 42.6653 - val_MinusLogProbMetric: 42.6653 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 19/1000
2023-10-26 16:07:36.209 
Epoch 19/1000 
	 loss: 46.0073, MinusLogProbMetric: 46.0073, val_loss: 41.4252, val_MinusLogProbMetric: 41.4252

Epoch 19: val_loss improved from 41.64405 to 41.42516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 41s - loss: 46.0073 - MinusLogProbMetric: 46.0073 - val_loss: 41.4252 - val_MinusLogProbMetric: 41.4252 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 20/1000
2023-10-26 16:08:15.688 
Epoch 20/1000 
	 loss: 41.1812, MinusLogProbMetric: 41.1812, val_loss: 41.7902, val_MinusLogProbMetric: 41.7902

Epoch 20: val_loss did not improve from 41.42516
196/196 - 39s - loss: 41.1812 - MinusLogProbMetric: 41.1812 - val_loss: 41.7902 - val_MinusLogProbMetric: 41.7902 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 21/1000
2023-10-26 16:08:58.374 
Epoch 21/1000 
	 loss: 40.3536, MinusLogProbMetric: 40.3536, val_loss: 42.1458, val_MinusLogProbMetric: 42.1458

Epoch 21: val_loss did not improve from 41.42516
196/196 - 43s - loss: 40.3536 - MinusLogProbMetric: 40.3536 - val_loss: 42.1458 - val_MinusLogProbMetric: 42.1458 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 22/1000
2023-10-26 16:09:41.209 
Epoch 22/1000 
	 loss: 40.6439, MinusLogProbMetric: 40.6439, val_loss: 38.8190, val_MinusLogProbMetric: 38.8190

Epoch 22: val_loss improved from 41.42516 to 38.81895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 40.6439 - MinusLogProbMetric: 40.6439 - val_loss: 38.8190 - val_MinusLogProbMetric: 38.8190 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 23/1000
2023-10-26 16:10:24.105 
Epoch 23/1000 
	 loss: 39.8842, MinusLogProbMetric: 39.8842, val_loss: 41.1626, val_MinusLogProbMetric: 41.1626

Epoch 23: val_loss did not improve from 38.81895
196/196 - 42s - loss: 39.8842 - MinusLogProbMetric: 39.8842 - val_loss: 41.1626 - val_MinusLogProbMetric: 41.1626 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 24/1000
2023-10-26 16:11:02.686 
Epoch 24/1000 
	 loss: 39.5743, MinusLogProbMetric: 39.5743, val_loss: 40.9404, val_MinusLogProbMetric: 40.9404

Epoch 24: val_loss did not improve from 38.81895
196/196 - 39s - loss: 39.5743 - MinusLogProbMetric: 39.5743 - val_loss: 40.9404 - val_MinusLogProbMetric: 40.9404 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 25/1000
2023-10-26 16:11:45.657 
Epoch 25/1000 
	 loss: 39.5500, MinusLogProbMetric: 39.5500, val_loss: 38.6238, val_MinusLogProbMetric: 38.6238

Epoch 25: val_loss improved from 38.81895 to 38.62382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 39.5500 - MinusLogProbMetric: 39.5500 - val_loss: 38.6238 - val_MinusLogProbMetric: 38.6238 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 26/1000
2023-10-26 16:12:29.627 
Epoch 26/1000 
	 loss: 39.8529, MinusLogProbMetric: 39.8529, val_loss: 37.7239, val_MinusLogProbMetric: 37.7239

Epoch 26: val_loss improved from 38.62382 to 37.72392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 39.8529 - MinusLogProbMetric: 39.8529 - val_loss: 37.7239 - val_MinusLogProbMetric: 37.7239 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 27/1000
2023-10-26 16:13:12.159 
Epoch 27/1000 
	 loss: 39.1382, MinusLogProbMetric: 39.1382, val_loss: 39.5794, val_MinusLogProbMetric: 39.5794

Epoch 27: val_loss did not improve from 37.72392
196/196 - 42s - loss: 39.1382 - MinusLogProbMetric: 39.1382 - val_loss: 39.5794 - val_MinusLogProbMetric: 39.5794 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 28/1000
2023-10-26 16:13:54.270 
Epoch 28/1000 
	 loss: 39.0402, MinusLogProbMetric: 39.0402, val_loss: 39.0665, val_MinusLogProbMetric: 39.0665

Epoch 28: val_loss did not improve from 37.72392
196/196 - 42s - loss: 39.0402 - MinusLogProbMetric: 39.0402 - val_loss: 39.0665 - val_MinusLogProbMetric: 39.0665 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 29/1000
2023-10-26 16:14:32.532 
Epoch 29/1000 
	 loss: 38.5332, MinusLogProbMetric: 38.5332, val_loss: 37.8632, val_MinusLogProbMetric: 37.8632

Epoch 29: val_loss did not improve from 37.72392
196/196 - 38s - loss: 38.5332 - MinusLogProbMetric: 38.5332 - val_loss: 37.8632 - val_MinusLogProbMetric: 37.8632 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 30/1000
2023-10-26 16:15:14.892 
Epoch 30/1000 
	 loss: 38.1984, MinusLogProbMetric: 38.1984, val_loss: 38.7374, val_MinusLogProbMetric: 38.7374

Epoch 30: val_loss did not improve from 37.72392
196/196 - 42s - loss: 38.1984 - MinusLogProbMetric: 38.1984 - val_loss: 38.7374 - val_MinusLogProbMetric: 38.7374 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 31/1000
2023-10-26 16:15:57.943 
Epoch 31/1000 
	 loss: 38.3168, MinusLogProbMetric: 38.3168, val_loss: 36.8718, val_MinusLogProbMetric: 36.8718

Epoch 31: val_loss improved from 37.72392 to 36.87181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 38.3168 - MinusLogProbMetric: 38.3168 - val_loss: 36.8718 - val_MinusLogProbMetric: 36.8718 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 32/1000
2023-10-26 16:16:41.231 
Epoch 32/1000 
	 loss: 37.4808, MinusLogProbMetric: 37.4808, val_loss: 36.6454, val_MinusLogProbMetric: 36.6454

Epoch 32: val_loss improved from 36.87181 to 36.64537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 37.4808 - MinusLogProbMetric: 37.4808 - val_loss: 36.6454 - val_MinusLogProbMetric: 36.6454 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 33/1000
2023-10-26 16:17:21.664 
Epoch 33/1000 
	 loss: 38.1022, MinusLogProbMetric: 38.1022, val_loss: 38.0712, val_MinusLogProbMetric: 38.0712

Epoch 33: val_loss did not improve from 36.64537
196/196 - 40s - loss: 38.1022 - MinusLogProbMetric: 38.1022 - val_loss: 38.0712 - val_MinusLogProbMetric: 38.0712 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 34/1000
2023-10-26 16:18:02.523 
Epoch 34/1000 
	 loss: 38.0873, MinusLogProbMetric: 38.0873, val_loss: 42.6416, val_MinusLogProbMetric: 42.6416

Epoch 34: val_loss did not improve from 36.64537
196/196 - 41s - loss: 38.0873 - MinusLogProbMetric: 38.0873 - val_loss: 42.6416 - val_MinusLogProbMetric: 42.6416 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 35/1000
2023-10-26 16:18:44.824 
Epoch 35/1000 
	 loss: 37.4578, MinusLogProbMetric: 37.4578, val_loss: 35.3787, val_MinusLogProbMetric: 35.3787

Epoch 35: val_loss improved from 36.64537 to 35.37865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 37.4578 - MinusLogProbMetric: 37.4578 - val_loss: 35.3787 - val_MinusLogProbMetric: 35.3787 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 36/1000
2023-10-26 16:19:27.168 
Epoch 36/1000 
	 loss: 37.5651, MinusLogProbMetric: 37.5651, val_loss: 36.4948, val_MinusLogProbMetric: 36.4948

Epoch 36: val_loss did not improve from 35.37865
196/196 - 42s - loss: 37.5651 - MinusLogProbMetric: 37.5651 - val_loss: 36.4948 - val_MinusLogProbMetric: 36.4948 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 37/1000
2023-10-26 16:20:09.596 
Epoch 37/1000 
	 loss: 37.3132, MinusLogProbMetric: 37.3132, val_loss: 39.1987, val_MinusLogProbMetric: 39.1987

Epoch 37: val_loss did not improve from 35.37865
196/196 - 42s - loss: 37.3132 - MinusLogProbMetric: 37.3132 - val_loss: 39.1987 - val_MinusLogProbMetric: 39.1987 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 38/1000
2023-10-26 16:20:50.121 
Epoch 38/1000 
	 loss: 37.1093, MinusLogProbMetric: 37.1093, val_loss: 38.0236, val_MinusLogProbMetric: 38.0236

Epoch 38: val_loss did not improve from 35.37865
196/196 - 41s - loss: 37.1093 - MinusLogProbMetric: 37.1093 - val_loss: 38.0236 - val_MinusLogProbMetric: 38.0236 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 39/1000
2023-10-26 16:21:33.037 
Epoch 39/1000 
	 loss: 36.8457, MinusLogProbMetric: 36.8457, val_loss: 38.6634, val_MinusLogProbMetric: 38.6634

Epoch 39: val_loss did not improve from 35.37865
196/196 - 43s - loss: 36.8457 - MinusLogProbMetric: 36.8457 - val_loss: 38.6634 - val_MinusLogProbMetric: 38.6634 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 40/1000
2023-10-26 16:22:15.357 
Epoch 40/1000 
	 loss: 37.0713, MinusLogProbMetric: 37.0713, val_loss: 37.3933, val_MinusLogProbMetric: 37.3933

Epoch 40: val_loss did not improve from 35.37865
196/196 - 42s - loss: 37.0713 - MinusLogProbMetric: 37.0713 - val_loss: 37.3933 - val_MinusLogProbMetric: 37.3933 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 41/1000
2023-10-26 16:22:55.869 
Epoch 41/1000 
	 loss: 36.9376, MinusLogProbMetric: 36.9376, val_loss: 35.6338, val_MinusLogProbMetric: 35.6338

Epoch 41: val_loss did not improve from 35.37865
196/196 - 41s - loss: 36.9376 - MinusLogProbMetric: 36.9376 - val_loss: 35.6338 - val_MinusLogProbMetric: 35.6338 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 42/1000
2023-10-26 16:23:34.348 
Epoch 42/1000 
	 loss: 36.3167, MinusLogProbMetric: 36.3167, val_loss: 37.7439, val_MinusLogProbMetric: 37.7439

Epoch 42: val_loss did not improve from 35.37865
196/196 - 38s - loss: 36.3167 - MinusLogProbMetric: 36.3167 - val_loss: 37.7439 - val_MinusLogProbMetric: 37.7439 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 43/1000
2023-10-26 16:24:16.643 
Epoch 43/1000 
	 loss: 36.1265, MinusLogProbMetric: 36.1265, val_loss: 35.2160, val_MinusLogProbMetric: 35.2160

Epoch 43: val_loss improved from 35.37865 to 35.21600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 36.1265 - MinusLogProbMetric: 36.1265 - val_loss: 35.2160 - val_MinusLogProbMetric: 35.2160 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 44/1000
2023-10-26 16:24:54.908 
Epoch 44/1000 
	 loss: 36.0606, MinusLogProbMetric: 36.0606, val_loss: 36.1347, val_MinusLogProbMetric: 36.1347

Epoch 44: val_loss did not improve from 35.21600
196/196 - 38s - loss: 36.0606 - MinusLogProbMetric: 36.0606 - val_loss: 36.1347 - val_MinusLogProbMetric: 36.1347 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 45/1000
2023-10-26 16:25:31.324 
Epoch 45/1000 
	 loss: 35.9149, MinusLogProbMetric: 35.9149, val_loss: 36.6844, val_MinusLogProbMetric: 36.6844

Epoch 45: val_loss did not improve from 35.21600
196/196 - 36s - loss: 35.9149 - MinusLogProbMetric: 35.9149 - val_loss: 36.6844 - val_MinusLogProbMetric: 36.6844 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 46/1000
2023-10-26 16:26:12.230 
Epoch 46/1000 
	 loss: 36.2941, MinusLogProbMetric: 36.2941, val_loss: 36.2764, val_MinusLogProbMetric: 36.2764

Epoch 46: val_loss did not improve from 35.21600
196/196 - 41s - loss: 36.2941 - MinusLogProbMetric: 36.2941 - val_loss: 36.2764 - val_MinusLogProbMetric: 36.2764 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 47/1000
2023-10-26 16:26:55.185 
Epoch 47/1000 
	 loss: 36.0751, MinusLogProbMetric: 36.0751, val_loss: 35.7975, val_MinusLogProbMetric: 35.7975

Epoch 47: val_loss did not improve from 35.21600
196/196 - 43s - loss: 36.0751 - MinusLogProbMetric: 36.0751 - val_loss: 35.7975 - val_MinusLogProbMetric: 35.7975 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 48/1000
2023-10-26 16:27:35.324 
Epoch 48/1000 
	 loss: 35.8738, MinusLogProbMetric: 35.8738, val_loss: 35.0542, val_MinusLogProbMetric: 35.0542

Epoch 48: val_loss improved from 35.21600 to 35.05416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 41s - loss: 35.8738 - MinusLogProbMetric: 35.8738 - val_loss: 35.0542 - val_MinusLogProbMetric: 35.0542 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 49/1000
2023-10-26 16:28:18.388 
Epoch 49/1000 
	 loss: 35.8432, MinusLogProbMetric: 35.8432, val_loss: 35.4478, val_MinusLogProbMetric: 35.4478

Epoch 49: val_loss did not improve from 35.05416
196/196 - 42s - loss: 35.8432 - MinusLogProbMetric: 35.8432 - val_loss: 35.4478 - val_MinusLogProbMetric: 35.4478 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 50/1000
2023-10-26 16:29:01.104 
Epoch 50/1000 
	 loss: 35.5674, MinusLogProbMetric: 35.5674, val_loss: 34.7699, val_MinusLogProbMetric: 34.7699

Epoch 50: val_loss improved from 35.05416 to 34.76987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 35.5674 - MinusLogProbMetric: 35.5674 - val_loss: 34.7699 - val_MinusLogProbMetric: 34.7699 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 51/1000
2023-10-26 16:29:40.754 
Epoch 51/1000 
	 loss: 35.3632, MinusLogProbMetric: 35.3632, val_loss: 36.5712, val_MinusLogProbMetric: 36.5712

Epoch 51: val_loss did not improve from 34.76987
196/196 - 39s - loss: 35.3632 - MinusLogProbMetric: 35.3632 - val_loss: 36.5712 - val_MinusLogProbMetric: 36.5712 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 52/1000
2023-10-26 16:30:21.105 
Epoch 52/1000 
	 loss: 35.2315, MinusLogProbMetric: 35.2315, val_loss: 36.1646, val_MinusLogProbMetric: 36.1646

Epoch 52: val_loss did not improve from 34.76987
196/196 - 40s - loss: 35.2315 - MinusLogProbMetric: 35.2315 - val_loss: 36.1646 - val_MinusLogProbMetric: 36.1646 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 53/1000
2023-10-26 16:31:04.053 
Epoch 53/1000 
	 loss: 35.8164, MinusLogProbMetric: 35.8164, val_loss: 36.3095, val_MinusLogProbMetric: 36.3095

Epoch 53: val_loss did not improve from 34.76987
196/196 - 43s - loss: 35.8164 - MinusLogProbMetric: 35.8164 - val_loss: 36.3095 - val_MinusLogProbMetric: 36.3095 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 54/1000
2023-10-26 16:31:44.309 
Epoch 54/1000 
	 loss: 35.3072, MinusLogProbMetric: 35.3072, val_loss: 37.8767, val_MinusLogProbMetric: 37.8767

Epoch 54: val_loss did not improve from 34.76987
196/196 - 40s - loss: 35.3072 - MinusLogProbMetric: 35.3072 - val_loss: 37.8767 - val_MinusLogProbMetric: 37.8767 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 55/1000
2023-10-26 16:32:26.621 
Epoch 55/1000 
	 loss: 35.3640, MinusLogProbMetric: 35.3640, val_loss: 33.8541, val_MinusLogProbMetric: 33.8541

Epoch 55: val_loss improved from 34.76987 to 33.85410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 35.3640 - MinusLogProbMetric: 35.3640 - val_loss: 33.8541 - val_MinusLogProbMetric: 33.8541 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 56/1000
2023-10-26 16:33:10.064 
Epoch 56/1000 
	 loss: 35.1844, MinusLogProbMetric: 35.1844, val_loss: 36.3577, val_MinusLogProbMetric: 36.3577

Epoch 56: val_loss did not improve from 33.85410
196/196 - 43s - loss: 35.1844 - MinusLogProbMetric: 35.1844 - val_loss: 36.3577 - val_MinusLogProbMetric: 36.3577 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 57/1000
2023-10-26 16:33:45.191 
Epoch 57/1000 
	 loss: 34.9842, MinusLogProbMetric: 34.9842, val_loss: 35.1875, val_MinusLogProbMetric: 35.1875

Epoch 57: val_loss did not improve from 33.85410
196/196 - 35s - loss: 34.9842 - MinusLogProbMetric: 34.9842 - val_loss: 35.1875 - val_MinusLogProbMetric: 35.1875 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 58/1000
2023-10-26 16:34:28.332 
Epoch 58/1000 
	 loss: 35.4608, MinusLogProbMetric: 35.4608, val_loss: 34.9336, val_MinusLogProbMetric: 34.9336

Epoch 58: val_loss did not improve from 33.85410
196/196 - 43s - loss: 35.4608 - MinusLogProbMetric: 35.4608 - val_loss: 34.9336 - val_MinusLogProbMetric: 34.9336 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 59/1000
2023-10-26 16:35:22.777 
Epoch 59/1000 
	 loss: 34.8709, MinusLogProbMetric: 34.8709, val_loss: 35.1327, val_MinusLogProbMetric: 35.1327

Epoch 59: val_loss did not improve from 33.85410
196/196 - 54s - loss: 34.8709 - MinusLogProbMetric: 34.8709 - val_loss: 35.1327 - val_MinusLogProbMetric: 35.1327 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 60/1000
2023-10-26 16:36:05.674 
Epoch 60/1000 
	 loss: 34.8952, MinusLogProbMetric: 34.8952, val_loss: 33.7315, val_MinusLogProbMetric: 33.7315

Epoch 60: val_loss improved from 33.85410 to 33.73152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 34.8952 - MinusLogProbMetric: 34.8952 - val_loss: 33.7315 - val_MinusLogProbMetric: 33.7315 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 61/1000
2023-10-26 16:36:50.009 
Epoch 61/1000 
	 loss: 35.1634, MinusLogProbMetric: 35.1634, val_loss: 36.1142, val_MinusLogProbMetric: 36.1142

Epoch 61: val_loss did not improve from 33.73152
196/196 - 44s - loss: 35.1634 - MinusLogProbMetric: 35.1634 - val_loss: 36.1142 - val_MinusLogProbMetric: 36.1142 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 62/1000
2023-10-26 16:37:30.305 
Epoch 62/1000 
	 loss: 34.7285, MinusLogProbMetric: 34.7285, val_loss: 36.0168, val_MinusLogProbMetric: 36.0168

Epoch 62: val_loss did not improve from 33.73152
196/196 - 40s - loss: 34.7285 - MinusLogProbMetric: 34.7285 - val_loss: 36.0168 - val_MinusLogProbMetric: 36.0168 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 63/1000
2023-10-26 16:38:07.788 
Epoch 63/1000 
	 loss: 34.8714, MinusLogProbMetric: 34.8714, val_loss: 35.4664, val_MinusLogProbMetric: 35.4664

Epoch 63: val_loss did not improve from 33.73152
196/196 - 37s - loss: 34.8714 - MinusLogProbMetric: 34.8714 - val_loss: 35.4664 - val_MinusLogProbMetric: 35.4664 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 64/1000
2023-10-26 16:38:47.399 
Epoch 64/1000 
	 loss: 34.4966, MinusLogProbMetric: 34.4966, val_loss: 35.2466, val_MinusLogProbMetric: 35.2466

Epoch 64: val_loss did not improve from 33.73152
196/196 - 40s - loss: 34.4966 - MinusLogProbMetric: 34.4966 - val_loss: 35.2466 - val_MinusLogProbMetric: 35.2466 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 65/1000
2023-10-26 16:39:30.385 
Epoch 65/1000 
	 loss: 34.8359, MinusLogProbMetric: 34.8359, val_loss: 35.4047, val_MinusLogProbMetric: 35.4047

Epoch 65: val_loss did not improve from 33.73152
196/196 - 43s - loss: 34.8359 - MinusLogProbMetric: 34.8359 - val_loss: 35.4047 - val_MinusLogProbMetric: 35.4047 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 66/1000
2023-10-26 16:40:12.137 
Epoch 66/1000 
	 loss: 34.2977, MinusLogProbMetric: 34.2977, val_loss: 33.3017, val_MinusLogProbMetric: 33.3017

Epoch 66: val_loss improved from 33.73152 to 33.30170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 42s - loss: 34.2977 - MinusLogProbMetric: 34.2977 - val_loss: 33.3017 - val_MinusLogProbMetric: 33.3017 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 67/1000
2023-10-26 16:40:54.823 
Epoch 67/1000 
	 loss: 34.5030, MinusLogProbMetric: 34.5030, val_loss: 35.2691, val_MinusLogProbMetric: 35.2691

Epoch 67: val_loss did not improve from 33.30170
196/196 - 42s - loss: 34.5030 - MinusLogProbMetric: 34.5030 - val_loss: 35.2691 - val_MinusLogProbMetric: 35.2691 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 68/1000
2023-10-26 16:41:35.013 
Epoch 68/1000 
	 loss: 34.0680, MinusLogProbMetric: 34.0680, val_loss: 33.8686, val_MinusLogProbMetric: 33.8686

Epoch 68: val_loss did not improve from 33.30170
196/196 - 40s - loss: 34.0680 - MinusLogProbMetric: 34.0680 - val_loss: 33.8686 - val_MinusLogProbMetric: 33.8686 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 69/1000
2023-10-26 16:42:17.089 
Epoch 69/1000 
	 loss: 34.4873, MinusLogProbMetric: 34.4873, val_loss: 33.8341, val_MinusLogProbMetric: 33.8341

Epoch 69: val_loss did not improve from 33.30170
196/196 - 42s - loss: 34.4873 - MinusLogProbMetric: 34.4873 - val_loss: 33.8341 - val_MinusLogProbMetric: 33.8341 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 70/1000
2023-10-26 16:42:58.877 
Epoch 70/1000 
	 loss: 34.0135, MinusLogProbMetric: 34.0135, val_loss: 35.3792, val_MinusLogProbMetric: 35.3792

Epoch 70: val_loss did not improve from 33.30170
196/196 - 42s - loss: 34.0135 - MinusLogProbMetric: 34.0135 - val_loss: 35.3792 - val_MinusLogProbMetric: 35.3792 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 71/1000
2023-10-26 16:43:41.692 
Epoch 71/1000 
	 loss: 34.1983, MinusLogProbMetric: 34.1983, val_loss: 33.9752, val_MinusLogProbMetric: 33.9752

Epoch 71: val_loss did not improve from 33.30170
196/196 - 43s - loss: 34.1983 - MinusLogProbMetric: 34.1983 - val_loss: 33.9752 - val_MinusLogProbMetric: 33.9752 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 72/1000
2023-10-26 16:44:24.114 
Epoch 72/1000 
	 loss: 34.2686, MinusLogProbMetric: 34.2686, val_loss: 38.1003, val_MinusLogProbMetric: 38.1003

Epoch 72: val_loss did not improve from 33.30170
196/196 - 42s - loss: 34.2686 - MinusLogProbMetric: 34.2686 - val_loss: 38.1003 - val_MinusLogProbMetric: 38.1003 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 73/1000
2023-10-26 16:45:04.546 
Epoch 73/1000 
	 loss: 34.2348, MinusLogProbMetric: 34.2348, val_loss: 33.1692, val_MinusLogProbMetric: 33.1692

Epoch 73: val_loss improved from 33.30170 to 33.16918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 41s - loss: 34.2348 - MinusLogProbMetric: 34.2348 - val_loss: 33.1692 - val_MinusLogProbMetric: 33.1692 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 74/1000
2023-10-26 16:45:47.269 
Epoch 74/1000 
	 loss: 33.8207, MinusLogProbMetric: 33.8207, val_loss: 33.3930, val_MinusLogProbMetric: 33.3930

Epoch 74: val_loss did not improve from 33.16918
196/196 - 42s - loss: 33.8207 - MinusLogProbMetric: 33.8207 - val_loss: 33.3930 - val_MinusLogProbMetric: 33.3930 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 75/1000
2023-10-26 16:46:28.729 
Epoch 75/1000 
	 loss: 33.9697, MinusLogProbMetric: 33.9697, val_loss: 34.3753, val_MinusLogProbMetric: 34.3753

Epoch 75: val_loss did not improve from 33.16918
196/196 - 41s - loss: 33.9697 - MinusLogProbMetric: 33.9697 - val_loss: 34.3753 - val_MinusLogProbMetric: 34.3753 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 76/1000
2023-10-26 16:47:08.018 
Epoch 76/1000 
	 loss: 33.7705, MinusLogProbMetric: 33.7705, val_loss: 33.0703, val_MinusLogProbMetric: 33.0703

Epoch 76: val_loss improved from 33.16918 to 33.07032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 40s - loss: 33.7705 - MinusLogProbMetric: 33.7705 - val_loss: 33.0703 - val_MinusLogProbMetric: 33.0703 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 77/1000
2023-10-26 16:47:51.058 
Epoch 77/1000 
	 loss: 33.6165, MinusLogProbMetric: 33.6165, val_loss: 35.6844, val_MinusLogProbMetric: 35.6844

Epoch 77: val_loss did not improve from 33.07032
196/196 - 42s - loss: 33.6165 - MinusLogProbMetric: 33.6165 - val_loss: 35.6844 - val_MinusLogProbMetric: 35.6844 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 78/1000
2023-10-26 16:48:33.918 
Epoch 78/1000 
	 loss: 33.5169, MinusLogProbMetric: 33.5169, val_loss: 33.7511, val_MinusLogProbMetric: 33.7511

Epoch 78: val_loss did not improve from 33.07032
196/196 - 43s - loss: 33.5169 - MinusLogProbMetric: 33.5169 - val_loss: 33.7511 - val_MinusLogProbMetric: 33.7511 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 79/1000
2023-10-26 16:49:13.499 
Epoch 79/1000 
	 loss: 33.3643, MinusLogProbMetric: 33.3643, val_loss: 33.2864, val_MinusLogProbMetric: 33.2864

Epoch 79: val_loss did not improve from 33.07032
196/196 - 40s - loss: 33.3643 - MinusLogProbMetric: 33.3643 - val_loss: 33.2864 - val_MinusLogProbMetric: 33.2864 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 80/1000
2023-10-26 16:49:52.232 
Epoch 80/1000 
	 loss: 33.5964, MinusLogProbMetric: 33.5964, val_loss: 32.6919, val_MinusLogProbMetric: 32.6919

Epoch 80: val_loss improved from 33.07032 to 32.69187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 39s - loss: 33.5964 - MinusLogProbMetric: 33.5964 - val_loss: 32.6919 - val_MinusLogProbMetric: 32.6919 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 81/1000
2023-10-26 16:50:30.277 
Epoch 81/1000 
	 loss: 33.5372, MinusLogProbMetric: 33.5372, val_loss: 33.4592, val_MinusLogProbMetric: 33.4592

Epoch 81: val_loss did not improve from 32.69187
196/196 - 37s - loss: 33.5372 - MinusLogProbMetric: 33.5372 - val_loss: 33.4592 - val_MinusLogProbMetric: 33.4592 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 82/1000
2023-10-26 16:51:10.644 
Epoch 82/1000 
	 loss: 33.3402, MinusLogProbMetric: 33.3402, val_loss: 33.3828, val_MinusLogProbMetric: 33.3828

Epoch 82: val_loss did not improve from 32.69187
196/196 - 40s - loss: 33.3402 - MinusLogProbMetric: 33.3402 - val_loss: 33.3828 - val_MinusLogProbMetric: 33.3828 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 83/1000
2023-10-26 16:51:52.617 
Epoch 83/1000 
	 loss: 33.2811, MinusLogProbMetric: 33.2811, val_loss: 33.1229, val_MinusLogProbMetric: 33.1229

Epoch 83: val_loss did not improve from 32.69187
196/196 - 42s - loss: 33.2811 - MinusLogProbMetric: 33.2811 - val_loss: 33.1229 - val_MinusLogProbMetric: 33.1229 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 84/1000
2023-10-26 16:52:33.313 
Epoch 84/1000 
	 loss: 33.0846, MinusLogProbMetric: 33.0846, val_loss: 33.0257, val_MinusLogProbMetric: 33.0257

Epoch 84: val_loss did not improve from 32.69187
196/196 - 41s - loss: 33.0846 - MinusLogProbMetric: 33.0846 - val_loss: 33.0257 - val_MinusLogProbMetric: 33.0257 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 85/1000
2023-10-26 16:53:10.997 
Epoch 85/1000 
	 loss: 32.9116, MinusLogProbMetric: 32.9116, val_loss: 33.2767, val_MinusLogProbMetric: 33.2767

Epoch 85: val_loss did not improve from 32.69187
196/196 - 38s - loss: 32.9116 - MinusLogProbMetric: 32.9116 - val_loss: 33.2767 - val_MinusLogProbMetric: 33.2767 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 86/1000
2023-10-26 16:53:47.717 
Epoch 86/1000 
	 loss: 33.0142, MinusLogProbMetric: 33.0142, val_loss: 32.7267, val_MinusLogProbMetric: 32.7267

Epoch 86: val_loss did not improve from 32.69187
196/196 - 37s - loss: 33.0142 - MinusLogProbMetric: 33.0142 - val_loss: 32.7267 - val_MinusLogProbMetric: 32.7267 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 87/1000
2023-10-26 16:54:25.384 
Epoch 87/1000 
	 loss: 32.8226, MinusLogProbMetric: 32.8226, val_loss: 33.8113, val_MinusLogProbMetric: 33.8113

Epoch 87: val_loss did not improve from 32.69187
196/196 - 38s - loss: 32.8226 - MinusLogProbMetric: 32.8226 - val_loss: 33.8113 - val_MinusLogProbMetric: 33.8113 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 88/1000
2023-10-26 16:55:08.177 
Epoch 88/1000 
	 loss: 33.0031, MinusLogProbMetric: 33.0031, val_loss: 35.0627, val_MinusLogProbMetric: 35.0627

Epoch 88: val_loss did not improve from 32.69187
196/196 - 43s - loss: 33.0031 - MinusLogProbMetric: 33.0031 - val_loss: 35.0627 - val_MinusLogProbMetric: 35.0627 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 89/1000
2023-10-26 16:55:50.866 
Epoch 89/1000 
	 loss: 32.8703, MinusLogProbMetric: 32.8703, val_loss: 32.5492, val_MinusLogProbMetric: 32.5492

Epoch 89: val_loss improved from 32.69187 to 32.54919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 32.8703 - MinusLogProbMetric: 32.8703 - val_loss: 32.5492 - val_MinusLogProbMetric: 32.5492 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 90/1000
2023-10-26 16:56:34.474 
Epoch 90/1000 
	 loss: 32.7858, MinusLogProbMetric: 32.7858, val_loss: 33.5467, val_MinusLogProbMetric: 33.5467

Epoch 90: val_loss did not improve from 32.54919
196/196 - 43s - loss: 32.7858 - MinusLogProbMetric: 32.7858 - val_loss: 33.5467 - val_MinusLogProbMetric: 33.5467 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 91/1000
2023-10-26 16:57:15.839 
Epoch 91/1000 
	 loss: 32.6621, MinusLogProbMetric: 32.6621, val_loss: 32.2792, val_MinusLogProbMetric: 32.2792

Epoch 91: val_loss improved from 32.54919 to 32.27921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 42s - loss: 32.6621 - MinusLogProbMetric: 32.6621 - val_loss: 32.2792 - val_MinusLogProbMetric: 32.2792 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 92/1000
2023-10-26 16:57:58.704 
Epoch 92/1000 
	 loss: 32.7792, MinusLogProbMetric: 32.7792, val_loss: 32.4156, val_MinusLogProbMetric: 32.4156

Epoch 92: val_loss did not improve from 32.27921
196/196 - 42s - loss: 32.7792 - MinusLogProbMetric: 32.7792 - val_loss: 32.4156 - val_MinusLogProbMetric: 32.4156 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 93/1000
2023-10-26 16:58:41.567 
Epoch 93/1000 
	 loss: 32.6109, MinusLogProbMetric: 32.6109, val_loss: 32.3724, val_MinusLogProbMetric: 32.3724

Epoch 93: val_loss did not improve from 32.27921
196/196 - 43s - loss: 32.6109 - MinusLogProbMetric: 32.6109 - val_loss: 32.3724 - val_MinusLogProbMetric: 32.3724 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 94/1000
2023-10-26 16:59:24.793 
Epoch 94/1000 
	 loss: 32.3245, MinusLogProbMetric: 32.3245, val_loss: 33.2132, val_MinusLogProbMetric: 33.2132

Epoch 94: val_loss did not improve from 32.27921
196/196 - 43s - loss: 32.3245 - MinusLogProbMetric: 32.3245 - val_loss: 33.2132 - val_MinusLogProbMetric: 33.2132 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 95/1000
2023-10-26 17:00:06.387 
Epoch 95/1000 
	 loss: 32.9600, MinusLogProbMetric: 32.9600, val_loss: 32.0632, val_MinusLogProbMetric: 32.0632

Epoch 95: val_loss improved from 32.27921 to 32.06320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 42s - loss: 32.9600 - MinusLogProbMetric: 32.9600 - val_loss: 32.0632 - val_MinusLogProbMetric: 32.0632 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 96/1000
2023-10-26 17:00:47.928 
Epoch 96/1000 
	 loss: 32.1799, MinusLogProbMetric: 32.1799, val_loss: 32.7528, val_MinusLogProbMetric: 32.7528

Epoch 96: val_loss did not improve from 32.06320
196/196 - 41s - loss: 32.1799 - MinusLogProbMetric: 32.1799 - val_loss: 32.7528 - val_MinusLogProbMetric: 32.7528 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 97/1000
2023-10-26 17:01:30.099 
Epoch 97/1000 
	 loss: 32.4842, MinusLogProbMetric: 32.4842, val_loss: 34.7130, val_MinusLogProbMetric: 34.7130

Epoch 97: val_loss did not improve from 32.06320
196/196 - 42s - loss: 32.4842 - MinusLogProbMetric: 32.4842 - val_loss: 34.7130 - val_MinusLogProbMetric: 34.7130 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 98/1000
2023-10-26 17:02:10.922 
Epoch 98/1000 
	 loss: 32.3117, MinusLogProbMetric: 32.3117, val_loss: 32.2356, val_MinusLogProbMetric: 32.2356

Epoch 98: val_loss did not improve from 32.06320
196/196 - 41s - loss: 32.3117 - MinusLogProbMetric: 32.3117 - val_loss: 32.2356 - val_MinusLogProbMetric: 32.2356 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 99/1000
2023-10-26 17:02:51.116 
Epoch 99/1000 
	 loss: 32.5050, MinusLogProbMetric: 32.5050, val_loss: 31.9867, val_MinusLogProbMetric: 31.9867

Epoch 99: val_loss improved from 32.06320 to 31.98669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 41s - loss: 32.5050 - MinusLogProbMetric: 32.5050 - val_loss: 31.9867 - val_MinusLogProbMetric: 31.9867 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 100/1000
2023-10-26 17:03:34.806 
Epoch 100/1000 
	 loss: 32.1631, MinusLogProbMetric: 32.1631, val_loss: 33.4053, val_MinusLogProbMetric: 33.4053

Epoch 100: val_loss did not improve from 31.98669
196/196 - 43s - loss: 32.1631 - MinusLogProbMetric: 32.1631 - val_loss: 33.4053 - val_MinusLogProbMetric: 33.4053 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 101/1000
2023-10-26 17:04:16.376 
Epoch 101/1000 
	 loss: 32.3220, MinusLogProbMetric: 32.3220, val_loss: 32.9098, val_MinusLogProbMetric: 32.9098

Epoch 101: val_loss did not improve from 31.98669
196/196 - 42s - loss: 32.3220 - MinusLogProbMetric: 32.3220 - val_loss: 32.9098 - val_MinusLogProbMetric: 32.9098 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 102/1000
2023-10-26 17:04:54.961 
Epoch 102/1000 
	 loss: 32.3384, MinusLogProbMetric: 32.3384, val_loss: 33.7485, val_MinusLogProbMetric: 33.7485

Epoch 102: val_loss did not improve from 31.98669
196/196 - 39s - loss: 32.3384 - MinusLogProbMetric: 32.3384 - val_loss: 33.7485 - val_MinusLogProbMetric: 33.7485 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 103/1000
2023-10-26 17:05:35.543 
Epoch 103/1000 
	 loss: 32.1753, MinusLogProbMetric: 32.1753, val_loss: 32.5953, val_MinusLogProbMetric: 32.5953

Epoch 103: val_loss did not improve from 31.98669
196/196 - 41s - loss: 32.1753 - MinusLogProbMetric: 32.1753 - val_loss: 32.5953 - val_MinusLogProbMetric: 32.5953 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 104/1000
2023-10-26 17:06:17.327 
Epoch 104/1000 
	 loss: 32.4258, MinusLogProbMetric: 32.4258, val_loss: 31.9058, val_MinusLogProbMetric: 31.9058

Epoch 104: val_loss improved from 31.98669 to 31.90576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 42s - loss: 32.4258 - MinusLogProbMetric: 32.4258 - val_loss: 31.9058 - val_MinusLogProbMetric: 31.9058 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 105/1000
2023-10-26 17:06:58.638 
Epoch 105/1000 
	 loss: 31.8176, MinusLogProbMetric: 31.8176, val_loss: 31.6768, val_MinusLogProbMetric: 31.6768

Epoch 105: val_loss improved from 31.90576 to 31.67680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 41s - loss: 31.8176 - MinusLogProbMetric: 31.8176 - val_loss: 31.6768 - val_MinusLogProbMetric: 31.6768 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 106/1000
2023-10-26 17:07:42.479 
Epoch 106/1000 
	 loss: 32.0571, MinusLogProbMetric: 32.0571, val_loss: 32.1229, val_MinusLogProbMetric: 32.1229

Epoch 106: val_loss did not improve from 31.67680
196/196 - 43s - loss: 32.0571 - MinusLogProbMetric: 32.0571 - val_loss: 32.1229 - val_MinusLogProbMetric: 32.1229 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 107/1000
2023-10-26 17:08:23.987 
Epoch 107/1000 
	 loss: 31.8460, MinusLogProbMetric: 31.8460, val_loss: 33.2344, val_MinusLogProbMetric: 33.2344

Epoch 107: val_loss did not improve from 31.67680
196/196 - 42s - loss: 31.8460 - MinusLogProbMetric: 31.8460 - val_loss: 33.2344 - val_MinusLogProbMetric: 33.2344 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 108/1000
2023-10-26 17:09:06.387 
Epoch 108/1000 
	 loss: 32.1178, MinusLogProbMetric: 32.1178, val_loss: 32.6042, val_MinusLogProbMetric: 32.6042

Epoch 108: val_loss did not improve from 31.67680
196/196 - 42s - loss: 32.1178 - MinusLogProbMetric: 32.1178 - val_loss: 32.6042 - val_MinusLogProbMetric: 32.6042 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 109/1000
2023-10-26 17:09:48.192 
Epoch 109/1000 
	 loss: 32.1760, MinusLogProbMetric: 32.1760, val_loss: 32.1070, val_MinusLogProbMetric: 32.1070

Epoch 109: val_loss did not improve from 31.67680
196/196 - 42s - loss: 32.1760 - MinusLogProbMetric: 32.1760 - val_loss: 32.1070 - val_MinusLogProbMetric: 32.1070 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 110/1000
2023-10-26 17:10:30.168 
Epoch 110/1000 
	 loss: 31.9820, MinusLogProbMetric: 31.9820, val_loss: 32.9207, val_MinusLogProbMetric: 32.9207

Epoch 110: val_loss did not improve from 31.67680
196/196 - 42s - loss: 31.9820 - MinusLogProbMetric: 31.9820 - val_loss: 32.9207 - val_MinusLogProbMetric: 32.9207 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 111/1000
2023-10-26 17:11:13.246 
Epoch 111/1000 
	 loss: 32.0471, MinusLogProbMetric: 32.0471, val_loss: 32.0987, val_MinusLogProbMetric: 32.0987

Epoch 111: val_loss did not improve from 31.67680
196/196 - 43s - loss: 32.0471 - MinusLogProbMetric: 32.0471 - val_loss: 32.0987 - val_MinusLogProbMetric: 32.0987 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 112/1000
2023-10-26 17:11:55.915 
Epoch 112/1000 
	 loss: 31.7252, MinusLogProbMetric: 31.7252, val_loss: 33.0260, val_MinusLogProbMetric: 33.0260

Epoch 112: val_loss did not improve from 31.67680
196/196 - 43s - loss: 31.7252 - MinusLogProbMetric: 31.7252 - val_loss: 33.0260 - val_MinusLogProbMetric: 33.0260 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 113/1000
2023-10-26 17:12:35.436 
Epoch 113/1000 
	 loss: 31.6522, MinusLogProbMetric: 31.6522, val_loss: 32.0956, val_MinusLogProbMetric: 32.0956

Epoch 113: val_loss did not improve from 31.67680
196/196 - 40s - loss: 31.6522 - MinusLogProbMetric: 31.6522 - val_loss: 32.0956 - val_MinusLogProbMetric: 32.0956 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 114/1000
2023-10-26 17:13:14.764 
Epoch 114/1000 
	 loss: 32.0326, MinusLogProbMetric: 32.0326, val_loss: 32.0586, val_MinusLogProbMetric: 32.0586

Epoch 114: val_loss did not improve from 31.67680
196/196 - 39s - loss: 32.0326 - MinusLogProbMetric: 32.0326 - val_loss: 32.0586 - val_MinusLogProbMetric: 32.0586 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 115/1000
2023-10-26 17:13:54.251 
Epoch 115/1000 
	 loss: 31.6046, MinusLogProbMetric: 31.6046, val_loss: 31.3691, val_MinusLogProbMetric: 31.3691

Epoch 115: val_loss improved from 31.67680 to 31.36908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 40s - loss: 31.6046 - MinusLogProbMetric: 31.6046 - val_loss: 31.3691 - val_MinusLogProbMetric: 31.3691 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 116/1000
2023-10-26 17:14:37.904 
Epoch 116/1000 
	 loss: 31.7901, MinusLogProbMetric: 31.7901, val_loss: 32.3568, val_MinusLogProbMetric: 32.3568

Epoch 116: val_loss did not improve from 31.36908
196/196 - 43s - loss: 31.7901 - MinusLogProbMetric: 31.7901 - val_loss: 32.3568 - val_MinusLogProbMetric: 32.3568 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 117/1000
2023-10-26 17:15:16.898 
Epoch 117/1000 
	 loss: 31.7231, MinusLogProbMetric: 31.7231, val_loss: 31.6587, val_MinusLogProbMetric: 31.6587

Epoch 117: val_loss did not improve from 31.36908
196/196 - 39s - loss: 31.7231 - MinusLogProbMetric: 31.7231 - val_loss: 31.6587 - val_MinusLogProbMetric: 31.6587 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 118/1000
2023-10-26 17:15:58.233 
Epoch 118/1000 
	 loss: 32.1898, MinusLogProbMetric: 32.1898, val_loss: 33.0865, val_MinusLogProbMetric: 33.0865

Epoch 118: val_loss did not improve from 31.36908
196/196 - 41s - loss: 32.1898 - MinusLogProbMetric: 32.1898 - val_loss: 33.0865 - val_MinusLogProbMetric: 33.0865 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 119/1000
2023-10-26 17:16:40.788 
Epoch 119/1000 
	 loss: 31.6041, MinusLogProbMetric: 31.6041, val_loss: 32.1549, val_MinusLogProbMetric: 32.1549

Epoch 119: val_loss did not improve from 31.36908
196/196 - 43s - loss: 31.6041 - MinusLogProbMetric: 31.6041 - val_loss: 32.1549 - val_MinusLogProbMetric: 32.1549 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 120/1000
2023-10-26 17:17:22.234 
Epoch 120/1000 
	 loss: 31.5884, MinusLogProbMetric: 31.5884, val_loss: 32.7556, val_MinusLogProbMetric: 32.7556

Epoch 120: val_loss did not improve from 31.36908
196/196 - 41s - loss: 31.5884 - MinusLogProbMetric: 31.5884 - val_loss: 32.7556 - val_MinusLogProbMetric: 32.7556 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 121/1000
2023-10-26 17:18:04.102 
Epoch 121/1000 
	 loss: 32.0448, MinusLogProbMetric: 32.0448, val_loss: 32.5963, val_MinusLogProbMetric: 32.5963

Epoch 121: val_loss did not improve from 31.36908
196/196 - 42s - loss: 32.0448 - MinusLogProbMetric: 32.0448 - val_loss: 32.5963 - val_MinusLogProbMetric: 32.5963 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 122/1000
2023-10-26 17:18:43.514 
Epoch 122/1000 
	 loss: 31.6346, MinusLogProbMetric: 31.6346, val_loss: 31.9251, val_MinusLogProbMetric: 31.9251

Epoch 122: val_loss did not improve from 31.36908
196/196 - 39s - loss: 31.6346 - MinusLogProbMetric: 31.6346 - val_loss: 31.9251 - val_MinusLogProbMetric: 31.9251 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 123/1000
2023-10-26 17:19:21.055 
Epoch 123/1000 
	 loss: 31.3618, MinusLogProbMetric: 31.3618, val_loss: 33.1043, val_MinusLogProbMetric: 33.1043

Epoch 123: val_loss did not improve from 31.36908
196/196 - 38s - loss: 31.3618 - MinusLogProbMetric: 31.3618 - val_loss: 33.1043 - val_MinusLogProbMetric: 33.1043 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 124/1000
2023-10-26 17:20:00.835 
Epoch 124/1000 
	 loss: 31.6676, MinusLogProbMetric: 31.6676, val_loss: 31.8760, val_MinusLogProbMetric: 31.8760

Epoch 124: val_loss did not improve from 31.36908
196/196 - 40s - loss: 31.6676 - MinusLogProbMetric: 31.6676 - val_loss: 31.8760 - val_MinusLogProbMetric: 31.8760 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 125/1000
2023-10-26 17:20:42.515 
Epoch 125/1000 
	 loss: 31.9089, MinusLogProbMetric: 31.9089, val_loss: 31.7995, val_MinusLogProbMetric: 31.7995

Epoch 125: val_loss did not improve from 31.36908
196/196 - 42s - loss: 31.9089 - MinusLogProbMetric: 31.9089 - val_loss: 31.7995 - val_MinusLogProbMetric: 31.7995 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 126/1000
2023-10-26 17:21:24.655 
Epoch 126/1000 
	 loss: 31.4076, MinusLogProbMetric: 31.4076, val_loss: 31.4429, val_MinusLogProbMetric: 31.4429

Epoch 126: val_loss did not improve from 31.36908
196/196 - 42s - loss: 31.4076 - MinusLogProbMetric: 31.4076 - val_loss: 31.4429 - val_MinusLogProbMetric: 31.4429 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 127/1000
2023-10-26 17:22:04.819 
Epoch 127/1000 
	 loss: 31.5166, MinusLogProbMetric: 31.5166, val_loss: 32.1792, val_MinusLogProbMetric: 32.1792

Epoch 127: val_loss did not improve from 31.36908
196/196 - 40s - loss: 31.5166 - MinusLogProbMetric: 31.5166 - val_loss: 32.1792 - val_MinusLogProbMetric: 32.1792 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 128/1000
2023-10-26 17:22:47.929 
Epoch 128/1000 
	 loss: 31.5732, MinusLogProbMetric: 31.5732, val_loss: 32.1005, val_MinusLogProbMetric: 32.1005

Epoch 128: val_loss did not improve from 31.36908
196/196 - 43s - loss: 31.5732 - MinusLogProbMetric: 31.5732 - val_loss: 32.1005 - val_MinusLogProbMetric: 32.1005 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 129/1000
2023-10-26 17:23:30.611 
Epoch 129/1000 
	 loss: 31.8942, MinusLogProbMetric: 31.8942, val_loss: 31.7648, val_MinusLogProbMetric: 31.7648

Epoch 129: val_loss did not improve from 31.36908
196/196 - 43s - loss: 31.8942 - MinusLogProbMetric: 31.8942 - val_loss: 31.7648 - val_MinusLogProbMetric: 31.7648 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 130/1000
2023-10-26 17:24:09.749 
Epoch 130/1000 
	 loss: 31.4677, MinusLogProbMetric: 31.4677, val_loss: 33.3055, val_MinusLogProbMetric: 33.3055

Epoch 130: val_loss did not improve from 31.36908
196/196 - 39s - loss: 31.4677 - MinusLogProbMetric: 31.4677 - val_loss: 33.3055 - val_MinusLogProbMetric: 33.3055 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 131/1000
2023-10-26 17:24:52.195 
Epoch 131/1000 
	 loss: 31.6474, MinusLogProbMetric: 31.6474, val_loss: 31.2524, val_MinusLogProbMetric: 31.2524

Epoch 131: val_loss improved from 31.36908 to 31.25236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 31.6474 - MinusLogProbMetric: 31.6474 - val_loss: 31.2524 - val_MinusLogProbMetric: 31.2524 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 132/1000
2023-10-26 17:25:34.252 
Epoch 132/1000 
	 loss: 31.6474, MinusLogProbMetric: 31.6474, val_loss: 32.4632, val_MinusLogProbMetric: 32.4632

Epoch 132: val_loss did not improve from 31.25236
196/196 - 41s - loss: 31.6474 - MinusLogProbMetric: 31.6474 - val_loss: 32.4632 - val_MinusLogProbMetric: 32.4632 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 133/1000
2023-10-26 17:26:15.788 
Epoch 133/1000 
	 loss: 31.0418, MinusLogProbMetric: 31.0418, val_loss: 32.4971, val_MinusLogProbMetric: 32.4971

Epoch 133: val_loss did not improve from 31.25236
196/196 - 42s - loss: 31.0418 - MinusLogProbMetric: 31.0418 - val_loss: 32.4971 - val_MinusLogProbMetric: 32.4971 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 134/1000
2023-10-26 17:26:57.135 
Epoch 134/1000 
	 loss: 31.6166, MinusLogProbMetric: 31.6166, val_loss: 31.5782, val_MinusLogProbMetric: 31.5782

Epoch 134: val_loss did not improve from 31.25236
196/196 - 41s - loss: 31.6166 - MinusLogProbMetric: 31.6166 - val_loss: 31.5782 - val_MinusLogProbMetric: 31.5782 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 135/1000
2023-10-26 17:27:37.927 
Epoch 135/1000 
	 loss: 31.4339, MinusLogProbMetric: 31.4339, val_loss: 31.0434, val_MinusLogProbMetric: 31.0434

Epoch 135: val_loss improved from 31.25236 to 31.04340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 42s - loss: 31.4339 - MinusLogProbMetric: 31.4339 - val_loss: 31.0434 - val_MinusLogProbMetric: 31.0434 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 136/1000
2023-10-26 17:28:19.777 
Epoch 136/1000 
	 loss: 31.2826, MinusLogProbMetric: 31.2826, val_loss: 32.2764, val_MinusLogProbMetric: 32.2764

Epoch 136: val_loss did not improve from 31.04340
196/196 - 41s - loss: 31.2826 - MinusLogProbMetric: 31.2826 - val_loss: 32.2764 - val_MinusLogProbMetric: 32.2764 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 137/1000
2023-10-26 17:29:02.692 
Epoch 137/1000 
	 loss: 31.2933, MinusLogProbMetric: 31.2933, val_loss: 30.6977, val_MinusLogProbMetric: 30.6977

Epoch 137: val_loss improved from 31.04340 to 30.69772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 31.2933 - MinusLogProbMetric: 31.2933 - val_loss: 30.6977 - val_MinusLogProbMetric: 30.6977 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 138/1000
2023-10-26 17:29:45.058 
Epoch 138/1000 
	 loss: 31.2683, MinusLogProbMetric: 31.2683, val_loss: 30.9823, val_MinusLogProbMetric: 30.9823

Epoch 138: val_loss did not improve from 30.69772
196/196 - 42s - loss: 31.2683 - MinusLogProbMetric: 31.2683 - val_loss: 30.9823 - val_MinusLogProbMetric: 30.9823 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 139/1000
2023-10-26 17:30:27.498 
Epoch 139/1000 
	 loss: 31.2991, MinusLogProbMetric: 31.2991, val_loss: 31.3224, val_MinusLogProbMetric: 31.3224

Epoch 139: val_loss did not improve from 30.69772
196/196 - 42s - loss: 31.2991 - MinusLogProbMetric: 31.2991 - val_loss: 31.3224 - val_MinusLogProbMetric: 31.3224 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 140/1000
2023-10-26 17:31:09.537 
Epoch 140/1000 
	 loss: 31.2320, MinusLogProbMetric: 31.2320, val_loss: 31.2823, val_MinusLogProbMetric: 31.2823

Epoch 140: val_loss did not improve from 30.69772
196/196 - 42s - loss: 31.2320 - MinusLogProbMetric: 31.2320 - val_loss: 31.2823 - val_MinusLogProbMetric: 31.2823 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 141/1000
2023-10-26 17:31:52.117 
Epoch 141/1000 
	 loss: 31.2450, MinusLogProbMetric: 31.2450, val_loss: 32.0285, val_MinusLogProbMetric: 32.0285

Epoch 141: val_loss did not improve from 30.69772
196/196 - 43s - loss: 31.2450 - MinusLogProbMetric: 31.2450 - val_loss: 32.0285 - val_MinusLogProbMetric: 32.0285 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 142/1000
2023-10-26 17:32:34.595 
Epoch 142/1000 
	 loss: 31.3123, MinusLogProbMetric: 31.3123, val_loss: 31.9256, val_MinusLogProbMetric: 31.9256

Epoch 142: val_loss did not improve from 30.69772
196/196 - 42s - loss: 31.3123 - MinusLogProbMetric: 31.3123 - val_loss: 31.9256 - val_MinusLogProbMetric: 31.9256 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 143/1000
2023-10-26 17:33:17.453 
Epoch 143/1000 
	 loss: 31.0525, MinusLogProbMetric: 31.0525, val_loss: 31.4438, val_MinusLogProbMetric: 31.4438

Epoch 143: val_loss did not improve from 30.69772
196/196 - 43s - loss: 31.0525 - MinusLogProbMetric: 31.0525 - val_loss: 31.4438 - val_MinusLogProbMetric: 31.4438 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 144/1000
2023-10-26 17:33:58.376 
Epoch 144/1000 
	 loss: 30.9929, MinusLogProbMetric: 30.9929, val_loss: 31.0200, val_MinusLogProbMetric: 31.0200

Epoch 144: val_loss did not improve from 30.69772
196/196 - 41s - loss: 30.9929 - MinusLogProbMetric: 30.9929 - val_loss: 31.0200 - val_MinusLogProbMetric: 31.0200 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 145/1000
2023-10-26 17:34:38.603 
Epoch 145/1000 
	 loss: 31.5380, MinusLogProbMetric: 31.5380, val_loss: 31.1024, val_MinusLogProbMetric: 31.1024

Epoch 145: val_loss did not improve from 30.69772
196/196 - 40s - loss: 31.5380 - MinusLogProbMetric: 31.5380 - val_loss: 31.1024 - val_MinusLogProbMetric: 31.1024 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 146/1000
2023-10-26 17:35:20.783 
Epoch 146/1000 
	 loss: 30.9810, MinusLogProbMetric: 30.9810, val_loss: 31.8560, val_MinusLogProbMetric: 31.8560

Epoch 146: val_loss did not improve from 30.69772
196/196 - 42s - loss: 30.9810 - MinusLogProbMetric: 30.9810 - val_loss: 31.8560 - val_MinusLogProbMetric: 31.8560 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 147/1000
2023-10-26 17:36:02.869 
Epoch 147/1000 
	 loss: 31.3732, MinusLogProbMetric: 31.3732, val_loss: 32.4490, val_MinusLogProbMetric: 32.4490

Epoch 147: val_loss did not improve from 30.69772
196/196 - 42s - loss: 31.3732 - MinusLogProbMetric: 31.3732 - val_loss: 32.4490 - val_MinusLogProbMetric: 32.4490 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 148/1000
2023-10-26 17:36:45.551 
Epoch 148/1000 
	 loss: 31.0287, MinusLogProbMetric: 31.0287, val_loss: 31.3764, val_MinusLogProbMetric: 31.3764

Epoch 148: val_loss did not improve from 30.69772
196/196 - 43s - loss: 31.0287 - MinusLogProbMetric: 31.0287 - val_loss: 31.3764 - val_MinusLogProbMetric: 31.3764 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 149/1000
2023-10-26 17:37:28.763 
Epoch 149/1000 
	 loss: 31.2722, MinusLogProbMetric: 31.2722, val_loss: 31.3298, val_MinusLogProbMetric: 31.3298

Epoch 149: val_loss did not improve from 30.69772
196/196 - 43s - loss: 31.2722 - MinusLogProbMetric: 31.2722 - val_loss: 31.3298 - val_MinusLogProbMetric: 31.3298 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 150/1000
2023-10-26 17:38:11.391 
Epoch 150/1000 
	 loss: 30.9897, MinusLogProbMetric: 30.9897, val_loss: 34.2120, val_MinusLogProbMetric: 34.2120

Epoch 150: val_loss did not improve from 30.69772
196/196 - 43s - loss: 30.9897 - MinusLogProbMetric: 30.9897 - val_loss: 34.2120 - val_MinusLogProbMetric: 34.2120 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 151/1000
2023-10-26 17:38:54.206 
Epoch 151/1000 
	 loss: 31.2220, MinusLogProbMetric: 31.2220, val_loss: 30.6513, val_MinusLogProbMetric: 30.6513

Epoch 151: val_loss improved from 30.69772 to 30.65131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 31.2220 - MinusLogProbMetric: 31.2220 - val_loss: 30.6513 - val_MinusLogProbMetric: 30.6513 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 152/1000
2023-10-26 17:39:37.934 
Epoch 152/1000 
	 loss: 31.0030, MinusLogProbMetric: 31.0030, val_loss: 32.0296, val_MinusLogProbMetric: 32.0296

Epoch 152: val_loss did not improve from 30.65131
196/196 - 43s - loss: 31.0030 - MinusLogProbMetric: 31.0030 - val_loss: 32.0296 - val_MinusLogProbMetric: 32.0296 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 153/1000
2023-10-26 17:40:20.759 
Epoch 153/1000 
	 loss: 30.9952, MinusLogProbMetric: 30.9952, val_loss: 31.5587, val_MinusLogProbMetric: 31.5587

Epoch 153: val_loss did not improve from 30.65131
196/196 - 43s - loss: 30.9952 - MinusLogProbMetric: 30.9952 - val_loss: 31.5587 - val_MinusLogProbMetric: 31.5587 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 154/1000
2023-10-26 17:41:03.389 
Epoch 154/1000 
	 loss: 30.8174, MinusLogProbMetric: 30.8174, val_loss: 32.1227, val_MinusLogProbMetric: 32.1227

Epoch 154: val_loss did not improve from 30.65131
196/196 - 43s - loss: 30.8174 - MinusLogProbMetric: 30.8174 - val_loss: 32.1227 - val_MinusLogProbMetric: 32.1227 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 155/1000
2023-10-26 17:41:46.311 
Epoch 155/1000 
	 loss: 30.8523, MinusLogProbMetric: 30.8523, val_loss: 31.3370, val_MinusLogProbMetric: 31.3370

Epoch 155: val_loss did not improve from 30.65131
196/196 - 43s - loss: 30.8523 - MinusLogProbMetric: 30.8523 - val_loss: 31.3370 - val_MinusLogProbMetric: 31.3370 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 156/1000
2023-10-26 17:42:29.367 
Epoch 156/1000 
	 loss: 30.9796, MinusLogProbMetric: 30.9796, val_loss: 31.0737, val_MinusLogProbMetric: 31.0737

Epoch 156: val_loss did not improve from 30.65131
196/196 - 43s - loss: 30.9796 - MinusLogProbMetric: 30.9796 - val_loss: 31.0737 - val_MinusLogProbMetric: 31.0737 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 157/1000
2023-10-26 17:43:12.557 
Epoch 157/1000 
	 loss: 31.1739, MinusLogProbMetric: 31.1739, val_loss: 31.4123, val_MinusLogProbMetric: 31.4123

Epoch 157: val_loss did not improve from 30.65131
196/196 - 43s - loss: 31.1739 - MinusLogProbMetric: 31.1739 - val_loss: 31.4123 - val_MinusLogProbMetric: 31.4123 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 158/1000
2023-10-26 17:43:51.491 
Epoch 158/1000 
	 loss: 30.7482, MinusLogProbMetric: 30.7482, val_loss: 31.9996, val_MinusLogProbMetric: 31.9996

Epoch 158: val_loss did not improve from 30.65131
196/196 - 39s - loss: 30.7482 - MinusLogProbMetric: 30.7482 - val_loss: 31.9996 - val_MinusLogProbMetric: 31.9996 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 159/1000
2023-10-26 17:44:30.768 
Epoch 159/1000 
	 loss: 31.1834, MinusLogProbMetric: 31.1834, val_loss: 31.5039, val_MinusLogProbMetric: 31.5039

Epoch 159: val_loss did not improve from 30.65131
196/196 - 39s - loss: 31.1834 - MinusLogProbMetric: 31.1834 - val_loss: 31.5039 - val_MinusLogProbMetric: 31.5039 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 160/1000
2023-10-26 17:45:10.615 
Epoch 160/1000 
	 loss: 30.9522, MinusLogProbMetric: 30.9522, val_loss: 32.0457, val_MinusLogProbMetric: 32.0457

Epoch 160: val_loss did not improve from 30.65131
196/196 - 40s - loss: 30.9522 - MinusLogProbMetric: 30.9522 - val_loss: 32.0457 - val_MinusLogProbMetric: 32.0457 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 161/1000
2023-10-26 17:45:53.378 
Epoch 161/1000 
	 loss: 30.8855, MinusLogProbMetric: 30.8855, val_loss: 31.2712, val_MinusLogProbMetric: 31.2712

Epoch 161: val_loss did not improve from 30.65131
196/196 - 43s - loss: 30.8855 - MinusLogProbMetric: 30.8855 - val_loss: 31.2712 - val_MinusLogProbMetric: 31.2712 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 162/1000
2023-10-26 17:46:36.277 
Epoch 162/1000 
	 loss: 30.7096, MinusLogProbMetric: 30.7096, val_loss: 34.0744, val_MinusLogProbMetric: 34.0744

Epoch 162: val_loss did not improve from 30.65131
196/196 - 43s - loss: 30.7096 - MinusLogProbMetric: 30.7096 - val_loss: 34.0744 - val_MinusLogProbMetric: 34.0744 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 163/1000
2023-10-26 17:47:19.635 
Epoch 163/1000 
	 loss: 30.8353, MinusLogProbMetric: 30.8353, val_loss: 30.3921, val_MinusLogProbMetric: 30.3921

Epoch 163: val_loss improved from 30.65131 to 30.39215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 30.8353 - MinusLogProbMetric: 30.8353 - val_loss: 30.3921 - val_MinusLogProbMetric: 30.3921 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 164/1000
2023-10-26 17:48:02.840 
Epoch 164/1000 
	 loss: 30.7851, MinusLogProbMetric: 30.7851, val_loss: 30.8336, val_MinusLogProbMetric: 30.8336

Epoch 164: val_loss did not improve from 30.39215
196/196 - 42s - loss: 30.7851 - MinusLogProbMetric: 30.7851 - val_loss: 30.8336 - val_MinusLogProbMetric: 30.8336 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 165/1000
2023-10-26 17:48:45.827 
Epoch 165/1000 
	 loss: 33.9419, MinusLogProbMetric: 33.9419, val_loss: 35.1789, val_MinusLogProbMetric: 35.1789

Epoch 165: val_loss did not improve from 30.39215
196/196 - 43s - loss: 33.9419 - MinusLogProbMetric: 33.9419 - val_loss: 35.1789 - val_MinusLogProbMetric: 35.1789 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 166/1000
2023-10-26 17:49:28.933 
Epoch 166/1000 
	 loss: 32.2500, MinusLogProbMetric: 32.2500, val_loss: 31.6757, val_MinusLogProbMetric: 31.6757

Epoch 166: val_loss did not improve from 30.39215
196/196 - 43s - loss: 32.2500 - MinusLogProbMetric: 32.2500 - val_loss: 31.6757 - val_MinusLogProbMetric: 31.6757 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 167/1000
2023-10-26 17:50:11.655 
Epoch 167/1000 
	 loss: 31.4870, MinusLogProbMetric: 31.4870, val_loss: 31.0931, val_MinusLogProbMetric: 31.0931

Epoch 167: val_loss did not improve from 30.39215
196/196 - 43s - loss: 31.4870 - MinusLogProbMetric: 31.4870 - val_loss: 31.0931 - val_MinusLogProbMetric: 31.0931 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 168/1000
2023-10-26 17:50:54.552 
Epoch 168/1000 
	 loss: 31.3564, MinusLogProbMetric: 31.3564, val_loss: 32.7554, val_MinusLogProbMetric: 32.7554

Epoch 168: val_loss did not improve from 30.39215
196/196 - 43s - loss: 31.3564 - MinusLogProbMetric: 31.3564 - val_loss: 32.7554 - val_MinusLogProbMetric: 32.7554 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 169/1000
2023-10-26 17:51:37.314 
Epoch 169/1000 
	 loss: 31.2789, MinusLogProbMetric: 31.2789, val_loss: 31.5746, val_MinusLogProbMetric: 31.5746

Epoch 169: val_loss did not improve from 30.39215
196/196 - 43s - loss: 31.2789 - MinusLogProbMetric: 31.2789 - val_loss: 31.5746 - val_MinusLogProbMetric: 31.5746 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 170/1000
2023-10-26 17:52:20.054 
Epoch 170/1000 
	 loss: 31.3023, MinusLogProbMetric: 31.3023, val_loss: 31.9818, val_MinusLogProbMetric: 31.9818

Epoch 170: val_loss did not improve from 30.39215
196/196 - 43s - loss: 31.3023 - MinusLogProbMetric: 31.3023 - val_loss: 31.9818 - val_MinusLogProbMetric: 31.9818 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 171/1000
2023-10-26 17:53:03.289 
Epoch 171/1000 
	 loss: 31.0563, MinusLogProbMetric: 31.0563, val_loss: 30.7833, val_MinusLogProbMetric: 30.7833

Epoch 171: val_loss did not improve from 30.39215
196/196 - 43s - loss: 31.0563 - MinusLogProbMetric: 31.0563 - val_loss: 30.7833 - val_MinusLogProbMetric: 30.7833 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 172/1000
2023-10-26 17:53:46.042 
Epoch 172/1000 
	 loss: 30.9996, MinusLogProbMetric: 30.9996, val_loss: 30.3291, val_MinusLogProbMetric: 30.3291

Epoch 172: val_loss improved from 30.39215 to 30.32907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 30.9996 - MinusLogProbMetric: 30.9996 - val_loss: 30.3291 - val_MinusLogProbMetric: 30.3291 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 173/1000
2023-10-26 17:54:29.515 
Epoch 173/1000 
	 loss: 30.9817, MinusLogProbMetric: 30.9817, val_loss: 30.5186, val_MinusLogProbMetric: 30.5186

Epoch 173: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.9817 - MinusLogProbMetric: 30.9817 - val_loss: 30.5186 - val_MinusLogProbMetric: 30.5186 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 174/1000
2023-10-26 17:55:12.598 
Epoch 174/1000 
	 loss: 31.0128, MinusLogProbMetric: 31.0128, val_loss: 31.3166, val_MinusLogProbMetric: 31.3166

Epoch 174: val_loss did not improve from 30.32907
196/196 - 43s - loss: 31.0128 - MinusLogProbMetric: 31.0128 - val_loss: 31.3166 - val_MinusLogProbMetric: 31.3166 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 175/1000
2023-10-26 17:55:55.808 
Epoch 175/1000 
	 loss: 30.7742, MinusLogProbMetric: 30.7742, val_loss: 31.1743, val_MinusLogProbMetric: 31.1743

Epoch 175: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.7742 - MinusLogProbMetric: 30.7742 - val_loss: 31.1743 - val_MinusLogProbMetric: 31.1743 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 176/1000
2023-10-26 17:56:39.168 
Epoch 176/1000 
	 loss: 30.9577, MinusLogProbMetric: 30.9577, val_loss: 32.2214, val_MinusLogProbMetric: 32.2214

Epoch 176: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.9577 - MinusLogProbMetric: 30.9577 - val_loss: 32.2214 - val_MinusLogProbMetric: 32.2214 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 177/1000
2023-10-26 17:57:22.241 
Epoch 177/1000 
	 loss: 31.0569, MinusLogProbMetric: 31.0569, val_loss: 31.6242, val_MinusLogProbMetric: 31.6242

Epoch 177: val_loss did not improve from 30.32907
196/196 - 43s - loss: 31.0569 - MinusLogProbMetric: 31.0569 - val_loss: 31.6242 - val_MinusLogProbMetric: 31.6242 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 178/1000
2023-10-26 17:58:04.721 
Epoch 178/1000 
	 loss: 30.7746, MinusLogProbMetric: 30.7746, val_loss: 30.9012, val_MinusLogProbMetric: 30.9012

Epoch 178: val_loss did not improve from 30.32907
196/196 - 42s - loss: 30.7746 - MinusLogProbMetric: 30.7746 - val_loss: 30.9012 - val_MinusLogProbMetric: 30.9012 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 179/1000
2023-10-26 17:58:47.891 
Epoch 179/1000 
	 loss: 30.5574, MinusLogProbMetric: 30.5574, val_loss: 30.4645, val_MinusLogProbMetric: 30.4645

Epoch 179: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.5574 - MinusLogProbMetric: 30.5574 - val_loss: 30.4645 - val_MinusLogProbMetric: 30.4645 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 180/1000
2023-10-26 17:59:30.953 
Epoch 180/1000 
	 loss: 30.5862, MinusLogProbMetric: 30.5862, val_loss: 30.9051, val_MinusLogProbMetric: 30.9051

Epoch 180: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.5862 - MinusLogProbMetric: 30.5862 - val_loss: 30.9051 - val_MinusLogProbMetric: 30.9051 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 181/1000
2023-10-26 18:00:13.793 
Epoch 181/1000 
	 loss: 30.8218, MinusLogProbMetric: 30.8218, val_loss: 30.8960, val_MinusLogProbMetric: 30.8960

Epoch 181: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.8218 - MinusLogProbMetric: 30.8218 - val_loss: 30.8960 - val_MinusLogProbMetric: 30.8960 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 182/1000
2023-10-26 18:00:56.889 
Epoch 182/1000 
	 loss: 30.5065, MinusLogProbMetric: 30.5065, val_loss: 31.0372, val_MinusLogProbMetric: 31.0372

Epoch 182: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.5065 - MinusLogProbMetric: 30.5065 - val_loss: 31.0372 - val_MinusLogProbMetric: 31.0372 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 183/1000
2023-10-26 18:01:39.789 
Epoch 183/1000 
	 loss: 30.3683, MinusLogProbMetric: 30.3683, val_loss: 31.4027, val_MinusLogProbMetric: 31.4027

Epoch 183: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.3683 - MinusLogProbMetric: 30.3683 - val_loss: 31.4027 - val_MinusLogProbMetric: 31.4027 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 184/1000
2023-10-26 18:02:22.695 
Epoch 184/1000 
	 loss: 30.5163, MinusLogProbMetric: 30.5163, val_loss: 30.8057, val_MinusLogProbMetric: 30.8057

Epoch 184: val_loss did not improve from 30.32907
196/196 - 43s - loss: 30.5163 - MinusLogProbMetric: 30.5163 - val_loss: 30.8057 - val_MinusLogProbMetric: 30.8057 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 185/1000
2023-10-26 18:03:05.591 
Epoch 185/1000 
	 loss: 30.6322, MinusLogProbMetric: 30.6322, val_loss: 30.3050, val_MinusLogProbMetric: 30.3050

Epoch 185: val_loss improved from 30.32907 to 30.30499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 30.6322 - MinusLogProbMetric: 30.6322 - val_loss: 30.3050 - val_MinusLogProbMetric: 30.3050 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 186/1000
2023-10-26 18:03:49.398 
Epoch 186/1000 
	 loss: 30.4023, MinusLogProbMetric: 30.4023, val_loss: 30.3500, val_MinusLogProbMetric: 30.3500

Epoch 186: val_loss did not improve from 30.30499
196/196 - 43s - loss: 30.4023 - MinusLogProbMetric: 30.4023 - val_loss: 30.3500 - val_MinusLogProbMetric: 30.3500 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 187/1000
2023-10-26 18:04:32.238 
Epoch 187/1000 
	 loss: 30.4364, MinusLogProbMetric: 30.4364, val_loss: 30.3805, val_MinusLogProbMetric: 30.3805

Epoch 187: val_loss did not improve from 30.30499
196/196 - 43s - loss: 30.4364 - MinusLogProbMetric: 30.4364 - val_loss: 30.3805 - val_MinusLogProbMetric: 30.3805 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 188/1000
2023-10-26 18:05:14.653 
Epoch 188/1000 
	 loss: 30.4109, MinusLogProbMetric: 30.4109, val_loss: 30.2257, val_MinusLogProbMetric: 30.2257

Epoch 188: val_loss improved from 30.30499 to 30.22571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 30.4109 - MinusLogProbMetric: 30.4109 - val_loss: 30.2257 - val_MinusLogProbMetric: 30.2257 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 189/1000
2023-10-26 18:05:56.290 
Epoch 189/1000 
	 loss: 30.7164, MinusLogProbMetric: 30.7164, val_loss: 30.0397, val_MinusLogProbMetric: 30.0397

Epoch 189: val_loss improved from 30.22571 to 30.03973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 42s - loss: 30.7164 - MinusLogProbMetric: 30.7164 - val_loss: 30.0397 - val_MinusLogProbMetric: 30.0397 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 190/1000
2023-10-26 18:06:38.646 
Epoch 190/1000 
	 loss: 30.2413, MinusLogProbMetric: 30.2413, val_loss: 31.2480, val_MinusLogProbMetric: 31.2480

Epoch 190: val_loss did not improve from 30.03973
196/196 - 41s - loss: 30.2413 - MinusLogProbMetric: 30.2413 - val_loss: 31.2480 - val_MinusLogProbMetric: 31.2480 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 191/1000
2023-10-26 18:07:17.137 
Epoch 191/1000 
	 loss: 30.6163, MinusLogProbMetric: 30.6163, val_loss: 31.7128, val_MinusLogProbMetric: 31.7128

Epoch 191: val_loss did not improve from 30.03973
196/196 - 38s - loss: 30.6163 - MinusLogProbMetric: 30.6163 - val_loss: 31.7128 - val_MinusLogProbMetric: 31.7128 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 192/1000
2023-10-26 18:07:57.311 
Epoch 192/1000 
	 loss: 30.3647, MinusLogProbMetric: 30.3647, val_loss: 31.3216, val_MinusLogProbMetric: 31.3216

Epoch 192: val_loss did not improve from 30.03973
196/196 - 40s - loss: 30.3647 - MinusLogProbMetric: 30.3647 - val_loss: 31.3216 - val_MinusLogProbMetric: 31.3216 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 193/1000
2023-10-26 18:08:38.742 
Epoch 193/1000 
	 loss: 30.3293, MinusLogProbMetric: 30.3293, val_loss: 30.3530, val_MinusLogProbMetric: 30.3530

Epoch 193: val_loss did not improve from 30.03973
196/196 - 41s - loss: 30.3293 - MinusLogProbMetric: 30.3293 - val_loss: 30.3530 - val_MinusLogProbMetric: 30.3530 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 194/1000
2023-10-26 18:09:21.137 
Epoch 194/1000 
	 loss: 30.5821, MinusLogProbMetric: 30.5821, val_loss: 30.3348, val_MinusLogProbMetric: 30.3348

Epoch 194: val_loss did not improve from 30.03973
196/196 - 42s - loss: 30.5821 - MinusLogProbMetric: 30.5821 - val_loss: 30.3348 - val_MinusLogProbMetric: 30.3348 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 195/1000
2023-10-26 18:10:03.836 
Epoch 195/1000 
	 loss: 30.3446, MinusLogProbMetric: 30.3446, val_loss: 31.4157, val_MinusLogProbMetric: 31.4157

Epoch 195: val_loss did not improve from 30.03973
196/196 - 43s - loss: 30.3446 - MinusLogProbMetric: 30.3446 - val_loss: 31.4157 - val_MinusLogProbMetric: 31.4157 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 196/1000
2023-10-26 18:10:46.478 
Epoch 196/1000 
	 loss: 30.3083, MinusLogProbMetric: 30.3083, val_loss: 30.9961, val_MinusLogProbMetric: 30.9961

Epoch 196: val_loss did not improve from 30.03973
196/196 - 43s - loss: 30.3083 - MinusLogProbMetric: 30.3083 - val_loss: 30.9961 - val_MinusLogProbMetric: 30.9961 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 197/1000
2023-10-26 18:11:29.477 
Epoch 197/1000 
	 loss: 30.3005, MinusLogProbMetric: 30.3005, val_loss: 29.9582, val_MinusLogProbMetric: 29.9582

Epoch 197: val_loss improved from 30.03973 to 29.95819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 30.3005 - MinusLogProbMetric: 30.3005 - val_loss: 29.9582 - val_MinusLogProbMetric: 29.9582 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 198/1000
2023-10-26 18:12:12.892 
Epoch 198/1000 
	 loss: 30.2865, MinusLogProbMetric: 30.2865, val_loss: 30.8579, val_MinusLogProbMetric: 30.8579

Epoch 198: val_loss did not improve from 29.95819
196/196 - 43s - loss: 30.2865 - MinusLogProbMetric: 30.2865 - val_loss: 30.8579 - val_MinusLogProbMetric: 30.8579 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 199/1000
2023-10-26 18:12:55.608 
Epoch 199/1000 
	 loss: 30.3173, MinusLogProbMetric: 30.3173, val_loss: 31.5457, val_MinusLogProbMetric: 31.5457

Epoch 199: val_loss did not improve from 29.95819
196/196 - 43s - loss: 30.3173 - MinusLogProbMetric: 30.3173 - val_loss: 31.5457 - val_MinusLogProbMetric: 31.5457 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 200/1000
2023-10-26 18:13:38.685 
Epoch 200/1000 
	 loss: 30.4359, MinusLogProbMetric: 30.4359, val_loss: 31.3265, val_MinusLogProbMetric: 31.3265

Epoch 200: val_loss did not improve from 29.95819
196/196 - 43s - loss: 30.4359 - MinusLogProbMetric: 30.4359 - val_loss: 31.3265 - val_MinusLogProbMetric: 31.3265 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 201/1000
2023-10-26 18:14:21.657 
Epoch 201/1000 
	 loss: 30.1469, MinusLogProbMetric: 30.1469, val_loss: 30.1474, val_MinusLogProbMetric: 30.1474

Epoch 201: val_loss did not improve from 29.95819
196/196 - 43s - loss: 30.1469 - MinusLogProbMetric: 30.1469 - val_loss: 30.1474 - val_MinusLogProbMetric: 30.1474 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 202/1000
2023-10-26 18:15:04.290 
Epoch 202/1000 
	 loss: 30.2392, MinusLogProbMetric: 30.2392, val_loss: 29.7181, val_MinusLogProbMetric: 29.7181

Epoch 202: val_loss improved from 29.95819 to 29.71807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 30.2392 - MinusLogProbMetric: 30.2392 - val_loss: 29.7181 - val_MinusLogProbMetric: 29.7181 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 203/1000
2023-10-26 18:15:47.995 
Epoch 203/1000 
	 loss: 30.2796, MinusLogProbMetric: 30.2796, val_loss: 30.3856, val_MinusLogProbMetric: 30.3856

Epoch 203: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.2796 - MinusLogProbMetric: 30.2796 - val_loss: 30.3856 - val_MinusLogProbMetric: 30.3856 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 204/1000
2023-10-26 18:16:29.015 
Epoch 204/1000 
	 loss: 30.2009, MinusLogProbMetric: 30.2009, val_loss: 30.1685, val_MinusLogProbMetric: 30.1685

Epoch 204: val_loss did not improve from 29.71807
196/196 - 41s - loss: 30.2009 - MinusLogProbMetric: 30.2009 - val_loss: 30.1685 - val_MinusLogProbMetric: 30.1685 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 205/1000
2023-10-26 18:17:05.279 
Epoch 205/1000 
	 loss: 30.2342, MinusLogProbMetric: 30.2342, val_loss: 30.4340, val_MinusLogProbMetric: 30.4340

Epoch 205: val_loss did not improve from 29.71807
196/196 - 36s - loss: 30.2342 - MinusLogProbMetric: 30.2342 - val_loss: 30.4340 - val_MinusLogProbMetric: 30.4340 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 206/1000
2023-10-26 18:17:48.519 
Epoch 206/1000 
	 loss: 30.4111, MinusLogProbMetric: 30.4111, val_loss: 30.2731, val_MinusLogProbMetric: 30.2731

Epoch 206: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.4111 - MinusLogProbMetric: 30.4111 - val_loss: 30.2731 - val_MinusLogProbMetric: 30.2731 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 207/1000
2023-10-26 18:18:27.243 
Epoch 207/1000 
	 loss: 30.0773, MinusLogProbMetric: 30.0773, val_loss: 30.7800, val_MinusLogProbMetric: 30.7800

Epoch 207: val_loss did not improve from 29.71807
196/196 - 39s - loss: 30.0773 - MinusLogProbMetric: 30.0773 - val_loss: 30.7800 - val_MinusLogProbMetric: 30.7800 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 208/1000
2023-10-26 18:19:04.686 
Epoch 208/1000 
	 loss: 30.4113, MinusLogProbMetric: 30.4113, val_loss: 32.8045, val_MinusLogProbMetric: 32.8045

Epoch 208: val_loss did not improve from 29.71807
196/196 - 37s - loss: 30.4113 - MinusLogProbMetric: 30.4113 - val_loss: 32.8045 - val_MinusLogProbMetric: 32.8045 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 209/1000
2023-10-26 18:19:46.694 
Epoch 209/1000 
	 loss: 30.2621, MinusLogProbMetric: 30.2621, val_loss: 30.3833, val_MinusLogProbMetric: 30.3833

Epoch 209: val_loss did not improve from 29.71807
196/196 - 42s - loss: 30.2621 - MinusLogProbMetric: 30.2621 - val_loss: 30.3833 - val_MinusLogProbMetric: 30.3833 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 210/1000
2023-10-26 18:20:30.150 
Epoch 210/1000 
	 loss: 30.2110, MinusLogProbMetric: 30.2110, val_loss: 30.2021, val_MinusLogProbMetric: 30.2021

Epoch 210: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.2110 - MinusLogProbMetric: 30.2110 - val_loss: 30.2021 - val_MinusLogProbMetric: 30.2021 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 211/1000
2023-10-26 18:21:13.239 
Epoch 211/1000 
	 loss: 30.0589, MinusLogProbMetric: 30.0589, val_loss: 30.9748, val_MinusLogProbMetric: 30.9748

Epoch 211: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0589 - MinusLogProbMetric: 30.0589 - val_loss: 30.9748 - val_MinusLogProbMetric: 30.9748 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 212/1000
2023-10-26 18:21:56.522 
Epoch 212/1000 
	 loss: 30.2927, MinusLogProbMetric: 30.2927, val_loss: 30.1680, val_MinusLogProbMetric: 30.1680

Epoch 212: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.2927 - MinusLogProbMetric: 30.2927 - val_loss: 30.1680 - val_MinusLogProbMetric: 30.1680 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 213/1000
2023-10-26 18:22:39.623 
Epoch 213/1000 
	 loss: 30.3399, MinusLogProbMetric: 30.3399, val_loss: 30.4791, val_MinusLogProbMetric: 30.4791

Epoch 213: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.3399 - MinusLogProbMetric: 30.3399 - val_loss: 30.4791 - val_MinusLogProbMetric: 30.4791 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 214/1000
2023-10-26 18:23:22.747 
Epoch 214/1000 
	 loss: 29.9674, MinusLogProbMetric: 29.9674, val_loss: 29.9692, val_MinusLogProbMetric: 29.9692

Epoch 214: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9674 - MinusLogProbMetric: 29.9674 - val_loss: 29.9692 - val_MinusLogProbMetric: 29.9692 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 215/1000
2023-10-26 18:24:05.959 
Epoch 215/1000 
	 loss: 30.0335, MinusLogProbMetric: 30.0335, val_loss: 29.8535, val_MinusLogProbMetric: 29.8535

Epoch 215: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0335 - MinusLogProbMetric: 30.0335 - val_loss: 29.8535 - val_MinusLogProbMetric: 29.8535 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 216/1000
2023-10-26 18:24:49.611 
Epoch 216/1000 
	 loss: 30.1441, MinusLogProbMetric: 30.1441, val_loss: 31.0182, val_MinusLogProbMetric: 31.0182

Epoch 216: val_loss did not improve from 29.71807
196/196 - 44s - loss: 30.1441 - MinusLogProbMetric: 30.1441 - val_loss: 31.0182 - val_MinusLogProbMetric: 31.0182 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 217/1000
2023-10-26 18:25:33.030 
Epoch 217/1000 
	 loss: 29.8454, MinusLogProbMetric: 29.8454, val_loss: 30.6681, val_MinusLogProbMetric: 30.6681

Epoch 217: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.8454 - MinusLogProbMetric: 29.8454 - val_loss: 30.6681 - val_MinusLogProbMetric: 30.6681 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 218/1000
2023-10-26 18:26:16.198 
Epoch 218/1000 
	 loss: 30.3598, MinusLogProbMetric: 30.3598, val_loss: 29.9068, val_MinusLogProbMetric: 29.9068

Epoch 218: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.3598 - MinusLogProbMetric: 30.3598 - val_loss: 29.9068 - val_MinusLogProbMetric: 29.9068 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 219/1000
2023-10-26 18:26:59.172 
Epoch 219/1000 
	 loss: 30.0124, MinusLogProbMetric: 30.0124, val_loss: 29.9728, val_MinusLogProbMetric: 29.9728

Epoch 219: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0124 - MinusLogProbMetric: 30.0124 - val_loss: 29.9728 - val_MinusLogProbMetric: 29.9728 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 220/1000
2023-10-26 18:27:42.287 
Epoch 220/1000 
	 loss: 30.0256, MinusLogProbMetric: 30.0256, val_loss: 30.0604, val_MinusLogProbMetric: 30.0604

Epoch 220: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0256 - MinusLogProbMetric: 30.0256 - val_loss: 30.0604 - val_MinusLogProbMetric: 30.0604 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 221/1000
2023-10-26 18:28:25.498 
Epoch 221/1000 
	 loss: 30.0008, MinusLogProbMetric: 30.0008, val_loss: 31.1476, val_MinusLogProbMetric: 31.1476

Epoch 221: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0008 - MinusLogProbMetric: 30.0008 - val_loss: 31.1476 - val_MinusLogProbMetric: 31.1476 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 222/1000
2023-10-26 18:29:08.848 
Epoch 222/1000 
	 loss: 30.2261, MinusLogProbMetric: 30.2261, val_loss: 30.5582, val_MinusLogProbMetric: 30.5582

Epoch 222: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.2261 - MinusLogProbMetric: 30.2261 - val_loss: 30.5582 - val_MinusLogProbMetric: 30.5582 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 223/1000
2023-10-26 18:29:52.201 
Epoch 223/1000 
	 loss: 30.2736, MinusLogProbMetric: 30.2736, val_loss: 29.8601, val_MinusLogProbMetric: 29.8601

Epoch 223: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.2736 - MinusLogProbMetric: 30.2736 - val_loss: 29.8601 - val_MinusLogProbMetric: 29.8601 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 224/1000
2023-10-26 18:30:35.248 
Epoch 224/1000 
	 loss: 30.0241, MinusLogProbMetric: 30.0241, val_loss: 30.1198, val_MinusLogProbMetric: 30.1198

Epoch 224: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0241 - MinusLogProbMetric: 30.0241 - val_loss: 30.1198 - val_MinusLogProbMetric: 30.1198 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 225/1000
2023-10-26 18:31:18.008 
Epoch 225/1000 
	 loss: 29.9429, MinusLogProbMetric: 29.9429, val_loss: 29.9591, val_MinusLogProbMetric: 29.9591

Epoch 225: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9429 - MinusLogProbMetric: 29.9429 - val_loss: 29.9591 - val_MinusLogProbMetric: 29.9591 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 226/1000
2023-10-26 18:32:01.260 
Epoch 226/1000 
	 loss: 30.2361, MinusLogProbMetric: 30.2361, val_loss: 31.1574, val_MinusLogProbMetric: 31.1574

Epoch 226: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.2361 - MinusLogProbMetric: 30.2361 - val_loss: 31.1574 - val_MinusLogProbMetric: 31.1574 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 227/1000
2023-10-26 18:32:44.507 
Epoch 227/1000 
	 loss: 30.0414, MinusLogProbMetric: 30.0414, val_loss: 31.4090, val_MinusLogProbMetric: 31.4090

Epoch 227: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0414 - MinusLogProbMetric: 30.0414 - val_loss: 31.4090 - val_MinusLogProbMetric: 31.4090 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 228/1000
2023-10-26 18:33:27.628 
Epoch 228/1000 
	 loss: 29.9425, MinusLogProbMetric: 29.9425, val_loss: 30.1097, val_MinusLogProbMetric: 30.1097

Epoch 228: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9425 - MinusLogProbMetric: 29.9425 - val_loss: 30.1097 - val_MinusLogProbMetric: 30.1097 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 229/1000
2023-10-26 18:34:11.144 
Epoch 229/1000 
	 loss: 30.0495, MinusLogProbMetric: 30.0495, val_loss: 30.0062, val_MinusLogProbMetric: 30.0062

Epoch 229: val_loss did not improve from 29.71807
196/196 - 44s - loss: 30.0495 - MinusLogProbMetric: 30.0495 - val_loss: 30.0062 - val_MinusLogProbMetric: 30.0062 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 230/1000
2023-10-26 18:34:54.191 
Epoch 230/1000 
	 loss: 29.9679, MinusLogProbMetric: 29.9679, val_loss: 30.2051, val_MinusLogProbMetric: 30.2051

Epoch 230: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9679 - MinusLogProbMetric: 29.9679 - val_loss: 30.2051 - val_MinusLogProbMetric: 30.2051 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 231/1000
2023-10-26 18:35:37.606 
Epoch 231/1000 
	 loss: 29.9654, MinusLogProbMetric: 29.9654, val_loss: 31.0071, val_MinusLogProbMetric: 31.0071

Epoch 231: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9654 - MinusLogProbMetric: 29.9654 - val_loss: 31.0071 - val_MinusLogProbMetric: 31.0071 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 232/1000
2023-10-26 18:36:21.258 
Epoch 232/1000 
	 loss: 29.8707, MinusLogProbMetric: 29.8707, val_loss: 30.9919, val_MinusLogProbMetric: 30.9919

Epoch 232: val_loss did not improve from 29.71807
196/196 - 44s - loss: 29.8707 - MinusLogProbMetric: 29.8707 - val_loss: 30.9919 - val_MinusLogProbMetric: 30.9919 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 233/1000
2023-10-26 18:37:04.728 
Epoch 233/1000 
	 loss: 29.9633, MinusLogProbMetric: 29.9633, val_loss: 31.0419, val_MinusLogProbMetric: 31.0419

Epoch 233: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9633 - MinusLogProbMetric: 29.9633 - val_loss: 31.0419 - val_MinusLogProbMetric: 31.0419 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 234/1000
2023-10-26 18:37:48.352 
Epoch 234/1000 
	 loss: 29.9539, MinusLogProbMetric: 29.9539, val_loss: 30.2619, val_MinusLogProbMetric: 30.2619

Epoch 234: val_loss did not improve from 29.71807
196/196 - 44s - loss: 29.9539 - MinusLogProbMetric: 29.9539 - val_loss: 30.2619 - val_MinusLogProbMetric: 30.2619 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 235/1000
2023-10-26 18:38:31.376 
Epoch 235/1000 
	 loss: 29.8354, MinusLogProbMetric: 29.8354, val_loss: 31.1659, val_MinusLogProbMetric: 31.1659

Epoch 235: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.8354 - MinusLogProbMetric: 29.8354 - val_loss: 31.1659 - val_MinusLogProbMetric: 31.1659 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 236/1000
2023-10-26 18:39:14.469 
Epoch 236/1000 
	 loss: 29.7479, MinusLogProbMetric: 29.7479, val_loss: 30.3392, val_MinusLogProbMetric: 30.3392

Epoch 236: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.7479 - MinusLogProbMetric: 29.7479 - val_loss: 30.3392 - val_MinusLogProbMetric: 30.3392 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 237/1000
2023-10-26 18:39:57.550 
Epoch 237/1000 
	 loss: 30.0896, MinusLogProbMetric: 30.0896, val_loss: 29.8341, val_MinusLogProbMetric: 29.8341

Epoch 237: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0896 - MinusLogProbMetric: 30.0896 - val_loss: 29.8341 - val_MinusLogProbMetric: 29.8341 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 238/1000
2023-10-26 18:40:40.511 
Epoch 238/1000 
	 loss: 29.8427, MinusLogProbMetric: 29.8427, val_loss: 31.6110, val_MinusLogProbMetric: 31.6110

Epoch 238: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.8427 - MinusLogProbMetric: 29.8427 - val_loss: 31.6110 - val_MinusLogProbMetric: 31.6110 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 239/1000
2023-10-26 18:41:23.752 
Epoch 239/1000 
	 loss: 30.0080, MinusLogProbMetric: 30.0080, val_loss: 29.8583, val_MinusLogProbMetric: 29.8583

Epoch 239: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.0080 - MinusLogProbMetric: 30.0080 - val_loss: 29.8583 - val_MinusLogProbMetric: 29.8583 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 240/1000
2023-10-26 18:42:06.919 
Epoch 240/1000 
	 loss: 29.7561, MinusLogProbMetric: 29.7561, val_loss: 30.9282, val_MinusLogProbMetric: 30.9282

Epoch 240: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.7561 - MinusLogProbMetric: 29.7561 - val_loss: 30.9282 - val_MinusLogProbMetric: 30.9282 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 241/1000
2023-10-26 18:42:50.333 
Epoch 241/1000 
	 loss: 29.9101, MinusLogProbMetric: 29.9101, val_loss: 30.6153, val_MinusLogProbMetric: 30.6153

Epoch 241: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9101 - MinusLogProbMetric: 29.9101 - val_loss: 30.6153 - val_MinusLogProbMetric: 30.6153 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 242/1000
2023-10-26 18:43:33.951 
Epoch 242/1000 
	 loss: 29.7483, MinusLogProbMetric: 29.7483, val_loss: 30.4547, val_MinusLogProbMetric: 30.4547

Epoch 242: val_loss did not improve from 29.71807
196/196 - 44s - loss: 29.7483 - MinusLogProbMetric: 29.7483 - val_loss: 30.4547 - val_MinusLogProbMetric: 30.4547 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 243/1000
2023-10-26 18:44:17.196 
Epoch 243/1000 
	 loss: 30.1381, MinusLogProbMetric: 30.1381, val_loss: 29.9231, val_MinusLogProbMetric: 29.9231

Epoch 243: val_loss did not improve from 29.71807
196/196 - 43s - loss: 30.1381 - MinusLogProbMetric: 30.1381 - val_loss: 29.9231 - val_MinusLogProbMetric: 29.9231 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 244/1000
2023-10-26 18:45:00.098 
Epoch 244/1000 
	 loss: 29.8363, MinusLogProbMetric: 29.8363, val_loss: 30.9330, val_MinusLogProbMetric: 30.9330

Epoch 244: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.8363 - MinusLogProbMetric: 29.8363 - val_loss: 30.9330 - val_MinusLogProbMetric: 30.9330 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 245/1000
2023-10-26 18:45:43.185 
Epoch 245/1000 
	 loss: 29.7975, MinusLogProbMetric: 29.7975, val_loss: 30.7927, val_MinusLogProbMetric: 30.7927

Epoch 245: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.7975 - MinusLogProbMetric: 29.7975 - val_loss: 30.7927 - val_MinusLogProbMetric: 30.7927 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 246/1000
2023-10-26 18:46:26.237 
Epoch 246/1000 
	 loss: 29.9547, MinusLogProbMetric: 29.9547, val_loss: 30.4041, val_MinusLogProbMetric: 30.4041

Epoch 246: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.9547 - MinusLogProbMetric: 29.9547 - val_loss: 30.4041 - val_MinusLogProbMetric: 30.4041 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 247/1000
2023-10-26 18:47:09.466 
Epoch 247/1000 
	 loss: 29.7580, MinusLogProbMetric: 29.7580, val_loss: 30.1099, val_MinusLogProbMetric: 30.1099

Epoch 247: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.7580 - MinusLogProbMetric: 29.7580 - val_loss: 30.1099 - val_MinusLogProbMetric: 30.1099 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 248/1000
2023-10-26 18:47:52.688 
Epoch 248/1000 
	 loss: 29.8034, MinusLogProbMetric: 29.8034, val_loss: 29.8485, val_MinusLogProbMetric: 29.8485

Epoch 248: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.8034 - MinusLogProbMetric: 29.8034 - val_loss: 29.8485 - val_MinusLogProbMetric: 29.8485 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 249/1000
2023-10-26 18:48:36.384 
Epoch 249/1000 
	 loss: 29.7223, MinusLogProbMetric: 29.7223, val_loss: 31.9612, val_MinusLogProbMetric: 31.9612

Epoch 249: val_loss did not improve from 29.71807
196/196 - 44s - loss: 29.7223 - MinusLogProbMetric: 29.7223 - val_loss: 31.9612 - val_MinusLogProbMetric: 31.9612 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 250/1000
2023-10-26 18:49:19.727 
Epoch 250/1000 
	 loss: 29.8853, MinusLogProbMetric: 29.8853, val_loss: 30.5093, val_MinusLogProbMetric: 30.5093

Epoch 250: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.8853 - MinusLogProbMetric: 29.8853 - val_loss: 30.5093 - val_MinusLogProbMetric: 30.5093 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 251/1000
2023-10-26 18:50:02.761 
Epoch 251/1000 
	 loss: 29.6845, MinusLogProbMetric: 29.6845, val_loss: 30.0605, val_MinusLogProbMetric: 30.0605

Epoch 251: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.6845 - MinusLogProbMetric: 29.6845 - val_loss: 30.0605 - val_MinusLogProbMetric: 30.0605 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 252/1000
2023-10-26 18:50:45.920 
Epoch 252/1000 
	 loss: 29.7809, MinusLogProbMetric: 29.7809, val_loss: 31.5321, val_MinusLogProbMetric: 31.5321

Epoch 252: val_loss did not improve from 29.71807
196/196 - 43s - loss: 29.7809 - MinusLogProbMetric: 29.7809 - val_loss: 31.5321 - val_MinusLogProbMetric: 31.5321 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 253/1000
2023-10-26 18:51:29.222 
Epoch 253/1000 
	 loss: 28.6712, MinusLogProbMetric: 28.6712, val_loss: 29.0796, val_MinusLogProbMetric: 29.0796

Epoch 253: val_loss improved from 29.71807 to 29.07962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 28.6712 - MinusLogProbMetric: 28.6712 - val_loss: 29.0796 - val_MinusLogProbMetric: 29.0796 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 254/1000
2023-10-26 18:52:13.539 
Epoch 254/1000 
	 loss: 28.6004, MinusLogProbMetric: 28.6004, val_loss: 29.1935, val_MinusLogProbMetric: 29.1935

Epoch 254: val_loss did not improve from 29.07962
196/196 - 44s - loss: 28.6004 - MinusLogProbMetric: 28.6004 - val_loss: 29.1935 - val_MinusLogProbMetric: 29.1935 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 255/1000
2023-10-26 18:52:56.968 
Epoch 255/1000 
	 loss: 28.6741, MinusLogProbMetric: 28.6741, val_loss: 29.1600, val_MinusLogProbMetric: 29.1600

Epoch 255: val_loss did not improve from 29.07962
196/196 - 43s - loss: 28.6741 - MinusLogProbMetric: 28.6741 - val_loss: 29.1600 - val_MinusLogProbMetric: 29.1600 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 256/1000
2023-10-26 18:53:39.299 
Epoch 256/1000 
	 loss: 28.6782, MinusLogProbMetric: 28.6782, val_loss: 29.0722, val_MinusLogProbMetric: 29.0722

Epoch 256: val_loss improved from 29.07962 to 29.07220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 43s - loss: 28.6782 - MinusLogProbMetric: 28.6782 - val_loss: 29.0722 - val_MinusLogProbMetric: 29.0722 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 257/1000
2023-10-26 18:54:23.315 
Epoch 257/1000 
	 loss: 28.5598, MinusLogProbMetric: 28.5598, val_loss: 29.5204, val_MinusLogProbMetric: 29.5204

Epoch 257: val_loss did not improve from 29.07220
196/196 - 43s - loss: 28.5598 - MinusLogProbMetric: 28.5598 - val_loss: 29.5204 - val_MinusLogProbMetric: 29.5204 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 258/1000
2023-10-26 18:55:06.877 
Epoch 258/1000 
	 loss: 28.6251, MinusLogProbMetric: 28.6251, val_loss: 29.6243, val_MinusLogProbMetric: 29.6243

Epoch 258: val_loss did not improve from 29.07220
196/196 - 44s - loss: 28.6251 - MinusLogProbMetric: 28.6251 - val_loss: 29.6243 - val_MinusLogProbMetric: 29.6243 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 259/1000
2023-10-26 18:55:50.241 
Epoch 259/1000 
	 loss: 28.6477, MinusLogProbMetric: 28.6477, val_loss: 29.1890, val_MinusLogProbMetric: 29.1890

Epoch 259: val_loss did not improve from 29.07220
196/196 - 43s - loss: 28.6477 - MinusLogProbMetric: 28.6477 - val_loss: 29.1890 - val_MinusLogProbMetric: 29.1890 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 260/1000
2023-10-26 18:56:33.936 
Epoch 260/1000 
	 loss: 28.7730, MinusLogProbMetric: 28.7730, val_loss: 29.4611, val_MinusLogProbMetric: 29.4611

Epoch 260: val_loss did not improve from 29.07220
196/196 - 44s - loss: 28.7730 - MinusLogProbMetric: 28.7730 - val_loss: 29.4611 - val_MinusLogProbMetric: 29.4611 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 261/1000
2023-10-26 18:57:17.316 
Epoch 261/1000 
	 loss: 28.6167, MinusLogProbMetric: 28.6167, val_loss: 29.1012, val_MinusLogProbMetric: 29.1012

Epoch 261: val_loss did not improve from 29.07220
196/196 - 43s - loss: 28.6167 - MinusLogProbMetric: 28.6167 - val_loss: 29.1012 - val_MinusLogProbMetric: 29.1012 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 262/1000
2023-10-26 18:58:01.081 
Epoch 262/1000 
	 loss: 28.6658, MinusLogProbMetric: 28.6658, val_loss: 29.1086, val_MinusLogProbMetric: 29.1086

Epoch 262: val_loss did not improve from 29.07220
196/196 - 44s - loss: 28.6658 - MinusLogProbMetric: 28.6658 - val_loss: 29.1086 - val_MinusLogProbMetric: 29.1086 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 263/1000
2023-10-26 18:58:44.809 
Epoch 263/1000 
	 loss: 28.6274, MinusLogProbMetric: 28.6274, val_loss: 29.3145, val_MinusLogProbMetric: 29.3145

Epoch 263: val_loss did not improve from 29.07220
196/196 - 44s - loss: 28.6274 - MinusLogProbMetric: 28.6274 - val_loss: 29.3145 - val_MinusLogProbMetric: 29.3145 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 264/1000
2023-10-26 18:59:27.932 
Epoch 264/1000 
	 loss: 28.5782, MinusLogProbMetric: 28.5782, val_loss: 29.2868, val_MinusLogProbMetric: 29.2868

Epoch 264: val_loss did not improve from 29.07220
196/196 - 43s - loss: 28.5782 - MinusLogProbMetric: 28.5782 - val_loss: 29.2868 - val_MinusLogProbMetric: 29.2868 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 265/1000
2023-10-26 19:00:11.110 
Epoch 265/1000 
	 loss: 28.7433, MinusLogProbMetric: 28.7433, val_loss: 28.9153, val_MinusLogProbMetric: 28.9153

Epoch 265: val_loss improved from 29.07220 to 28.91527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 28.7433 - MinusLogProbMetric: 28.7433 - val_loss: 28.9153 - val_MinusLogProbMetric: 28.9153 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 266/1000
2023-10-26 19:00:54.870 
Epoch 266/1000 
	 loss: 28.6864, MinusLogProbMetric: 28.6864, val_loss: 28.9744, val_MinusLogProbMetric: 28.9744

Epoch 266: val_loss did not improve from 28.91527
196/196 - 43s - loss: 28.6864 - MinusLogProbMetric: 28.6864 - val_loss: 28.9744 - val_MinusLogProbMetric: 28.9744 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 267/1000
2023-10-26 19:01:38.196 
Epoch 267/1000 
	 loss: 28.6623, MinusLogProbMetric: 28.6623, val_loss: 29.1452, val_MinusLogProbMetric: 29.1452

Epoch 267: val_loss did not improve from 28.91527
196/196 - 43s - loss: 28.6623 - MinusLogProbMetric: 28.6623 - val_loss: 29.1452 - val_MinusLogProbMetric: 29.1452 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 268/1000
2023-10-26 19:02:21.630 
Epoch 268/1000 
	 loss: 28.6354, MinusLogProbMetric: 28.6354, val_loss: 28.8516, val_MinusLogProbMetric: 28.8516

Epoch 268: val_loss improved from 28.91527 to 28.85162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 28.6354 - MinusLogProbMetric: 28.6354 - val_loss: 28.8516 - val_MinusLogProbMetric: 28.8516 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 269/1000
2023-10-26 19:03:05.764 
Epoch 269/1000 
	 loss: 28.6323, MinusLogProbMetric: 28.6323, val_loss: 29.0675, val_MinusLogProbMetric: 29.0675

Epoch 269: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6323 - MinusLogProbMetric: 28.6323 - val_loss: 29.0675 - val_MinusLogProbMetric: 29.0675 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 270/1000
2023-10-26 19:03:49.012 
Epoch 270/1000 
	 loss: 28.6780, MinusLogProbMetric: 28.6780, val_loss: 29.6972, val_MinusLogProbMetric: 29.6972

Epoch 270: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6780 - MinusLogProbMetric: 28.6780 - val_loss: 29.6972 - val_MinusLogProbMetric: 29.6972 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 271/1000
2023-10-26 19:04:32.059 
Epoch 271/1000 
	 loss: 28.6602, MinusLogProbMetric: 28.6602, val_loss: 29.1637, val_MinusLogProbMetric: 29.1637

Epoch 271: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6602 - MinusLogProbMetric: 28.6602 - val_loss: 29.1637 - val_MinusLogProbMetric: 29.1637 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 272/1000
2023-10-26 19:05:15.728 
Epoch 272/1000 
	 loss: 28.6672, MinusLogProbMetric: 28.6672, val_loss: 29.1243, val_MinusLogProbMetric: 29.1243

Epoch 272: val_loss did not improve from 28.85162
196/196 - 44s - loss: 28.6672 - MinusLogProbMetric: 28.6672 - val_loss: 29.1243 - val_MinusLogProbMetric: 29.1243 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 273/1000
2023-10-26 19:05:59.125 
Epoch 273/1000 
	 loss: 28.5807, MinusLogProbMetric: 28.5807, val_loss: 28.9137, val_MinusLogProbMetric: 28.9137

Epoch 273: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5807 - MinusLogProbMetric: 28.5807 - val_loss: 28.9137 - val_MinusLogProbMetric: 28.9137 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 274/1000
2023-10-26 19:06:40.946 
Epoch 274/1000 
	 loss: 28.6131, MinusLogProbMetric: 28.6131, val_loss: 29.1618, val_MinusLogProbMetric: 29.1618

Epoch 274: val_loss did not improve from 28.85162
196/196 - 42s - loss: 28.6131 - MinusLogProbMetric: 28.6131 - val_loss: 29.1618 - val_MinusLogProbMetric: 29.1618 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 275/1000
2023-10-26 19:07:21.918 
Epoch 275/1000 
	 loss: 28.6395, MinusLogProbMetric: 28.6395, val_loss: 29.8331, val_MinusLogProbMetric: 29.8331

Epoch 275: val_loss did not improve from 28.85162
196/196 - 41s - loss: 28.6395 - MinusLogProbMetric: 28.6395 - val_loss: 29.8331 - val_MinusLogProbMetric: 29.8331 - lr: 5.0000e-04 - 41s/epoch - 209ms/step
Epoch 276/1000
2023-10-26 19:08:03.692 
Epoch 276/1000 
	 loss: 28.6060, MinusLogProbMetric: 28.6060, val_loss: 28.8785, val_MinusLogProbMetric: 28.8785

Epoch 276: val_loss did not improve from 28.85162
196/196 - 42s - loss: 28.6060 - MinusLogProbMetric: 28.6060 - val_loss: 28.8785 - val_MinusLogProbMetric: 28.8785 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 277/1000
2023-10-26 19:08:42.495 
Epoch 277/1000 
	 loss: 28.5957, MinusLogProbMetric: 28.5957, val_loss: 28.9887, val_MinusLogProbMetric: 28.9887

Epoch 277: val_loss did not improve from 28.85162
196/196 - 39s - loss: 28.5957 - MinusLogProbMetric: 28.5957 - val_loss: 28.9887 - val_MinusLogProbMetric: 28.9887 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 278/1000
2023-10-26 19:09:22.995 
Epoch 278/1000 
	 loss: 28.6368, MinusLogProbMetric: 28.6368, val_loss: 29.7216, val_MinusLogProbMetric: 29.7216

Epoch 278: val_loss did not improve from 28.85162
196/196 - 40s - loss: 28.6368 - MinusLogProbMetric: 28.6368 - val_loss: 29.7216 - val_MinusLogProbMetric: 29.7216 - lr: 5.0000e-04 - 40s/epoch - 207ms/step
Epoch 279/1000
2023-10-26 19:10:06.272 
Epoch 279/1000 
	 loss: 28.6588, MinusLogProbMetric: 28.6588, val_loss: 29.0330, val_MinusLogProbMetric: 29.0330

Epoch 279: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6588 - MinusLogProbMetric: 28.6588 - val_loss: 29.0330 - val_MinusLogProbMetric: 29.0330 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 280/1000
2023-10-26 19:10:49.644 
Epoch 280/1000 
	 loss: 28.6142, MinusLogProbMetric: 28.6142, val_loss: 28.9150, val_MinusLogProbMetric: 28.9150

Epoch 280: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6142 - MinusLogProbMetric: 28.6142 - val_loss: 28.9150 - val_MinusLogProbMetric: 28.9150 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 281/1000
2023-10-26 19:11:32.055 
Epoch 281/1000 
	 loss: 28.5716, MinusLogProbMetric: 28.5716, val_loss: 29.0494, val_MinusLogProbMetric: 29.0494

Epoch 281: val_loss did not improve from 28.85162
196/196 - 42s - loss: 28.5716 - MinusLogProbMetric: 28.5716 - val_loss: 29.0494 - val_MinusLogProbMetric: 29.0494 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 282/1000
2023-10-26 19:12:15.441 
Epoch 282/1000 
	 loss: 28.6151, MinusLogProbMetric: 28.6151, val_loss: 29.0363, val_MinusLogProbMetric: 29.0363

Epoch 282: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6151 - MinusLogProbMetric: 28.6151 - val_loss: 29.0363 - val_MinusLogProbMetric: 29.0363 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 283/1000
2023-10-26 19:12:58.269 
Epoch 283/1000 
	 loss: 28.5341, MinusLogProbMetric: 28.5341, val_loss: 29.2421, val_MinusLogProbMetric: 29.2421

Epoch 283: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5341 - MinusLogProbMetric: 28.5341 - val_loss: 29.2421 - val_MinusLogProbMetric: 29.2421 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 284/1000
2023-10-26 19:13:41.201 
Epoch 284/1000 
	 loss: 28.6144, MinusLogProbMetric: 28.6144, val_loss: 28.9700, val_MinusLogProbMetric: 28.9700

Epoch 284: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6144 - MinusLogProbMetric: 28.6144 - val_loss: 28.9700 - val_MinusLogProbMetric: 28.9700 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 285/1000
2023-10-26 19:14:24.253 
Epoch 285/1000 
	 loss: 28.5411, MinusLogProbMetric: 28.5411, val_loss: 29.4876, val_MinusLogProbMetric: 29.4876

Epoch 285: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5411 - MinusLogProbMetric: 28.5411 - val_loss: 29.4876 - val_MinusLogProbMetric: 29.4876 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 286/1000
2023-10-26 19:15:07.081 
Epoch 286/1000 
	 loss: 28.7052, MinusLogProbMetric: 28.7052, val_loss: 29.0683, val_MinusLogProbMetric: 29.0683

Epoch 286: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.7052 - MinusLogProbMetric: 28.7052 - val_loss: 29.0683 - val_MinusLogProbMetric: 29.0683 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 287/1000
2023-10-26 19:15:49.803 
Epoch 287/1000 
	 loss: 28.6793, MinusLogProbMetric: 28.6793, val_loss: 29.3586, val_MinusLogProbMetric: 29.3586

Epoch 287: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6793 - MinusLogProbMetric: 28.6793 - val_loss: 29.3586 - val_MinusLogProbMetric: 29.3586 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 288/1000
2023-10-26 19:16:32.741 
Epoch 288/1000 
	 loss: 28.5722, MinusLogProbMetric: 28.5722, val_loss: 29.7540, val_MinusLogProbMetric: 29.7540

Epoch 288: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5722 - MinusLogProbMetric: 28.5722 - val_loss: 29.7540 - val_MinusLogProbMetric: 29.7540 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 289/1000
2023-10-26 19:17:15.507 
Epoch 289/1000 
	 loss: 28.6079, MinusLogProbMetric: 28.6079, val_loss: 29.2034, val_MinusLogProbMetric: 29.2034

Epoch 289: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6079 - MinusLogProbMetric: 28.6079 - val_loss: 29.2034 - val_MinusLogProbMetric: 29.2034 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 290/1000
2023-10-26 19:17:58.713 
Epoch 290/1000 
	 loss: 28.5196, MinusLogProbMetric: 28.5196, val_loss: 29.0553, val_MinusLogProbMetric: 29.0553

Epoch 290: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5196 - MinusLogProbMetric: 28.5196 - val_loss: 29.0553 - val_MinusLogProbMetric: 29.0553 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 291/1000
2023-10-26 19:18:42.167 
Epoch 291/1000 
	 loss: 28.6240, MinusLogProbMetric: 28.6240, val_loss: 29.0963, val_MinusLogProbMetric: 29.0963

Epoch 291: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6240 - MinusLogProbMetric: 28.6240 - val_loss: 29.0963 - val_MinusLogProbMetric: 29.0963 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 292/1000
2023-10-26 19:19:25.196 
Epoch 292/1000 
	 loss: 28.5566, MinusLogProbMetric: 28.5566, val_loss: 29.0347, val_MinusLogProbMetric: 29.0347

Epoch 292: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5566 - MinusLogProbMetric: 28.5566 - val_loss: 29.0347 - val_MinusLogProbMetric: 29.0347 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 293/1000
2023-10-26 19:20:08.583 
Epoch 293/1000 
	 loss: 28.5740, MinusLogProbMetric: 28.5740, val_loss: 29.0800, val_MinusLogProbMetric: 29.0800

Epoch 293: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5740 - MinusLogProbMetric: 28.5740 - val_loss: 29.0800 - val_MinusLogProbMetric: 29.0800 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 294/1000
2023-10-26 19:20:51.756 
Epoch 294/1000 
	 loss: 28.5727, MinusLogProbMetric: 28.5727, val_loss: 28.8787, val_MinusLogProbMetric: 28.8787

Epoch 294: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5727 - MinusLogProbMetric: 28.5727 - val_loss: 28.8787 - val_MinusLogProbMetric: 28.8787 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 295/1000
2023-10-26 19:21:34.718 
Epoch 295/1000 
	 loss: 28.5373, MinusLogProbMetric: 28.5373, val_loss: 29.0759, val_MinusLogProbMetric: 29.0759

Epoch 295: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5373 - MinusLogProbMetric: 28.5373 - val_loss: 29.0759 - val_MinusLogProbMetric: 29.0759 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 296/1000
2023-10-26 19:22:17.642 
Epoch 296/1000 
	 loss: 28.6141, MinusLogProbMetric: 28.6141, val_loss: 29.7738, val_MinusLogProbMetric: 29.7738

Epoch 296: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.6141 - MinusLogProbMetric: 28.6141 - val_loss: 29.7738 - val_MinusLogProbMetric: 29.7738 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 297/1000
2023-10-26 19:23:00.266 
Epoch 297/1000 
	 loss: 28.4995, MinusLogProbMetric: 28.4995, val_loss: 29.1988, val_MinusLogProbMetric: 29.1988

Epoch 297: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.4995 - MinusLogProbMetric: 28.4995 - val_loss: 29.1988 - val_MinusLogProbMetric: 29.1988 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 298/1000
2023-10-26 19:23:43.266 
Epoch 298/1000 
	 loss: 28.5093, MinusLogProbMetric: 28.5093, val_loss: 29.4160, val_MinusLogProbMetric: 29.4160

Epoch 298: val_loss did not improve from 28.85162
196/196 - 43s - loss: 28.5093 - MinusLogProbMetric: 28.5093 - val_loss: 29.4160 - val_MinusLogProbMetric: 29.4160 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 299/1000
2023-10-26 19:24:26.353 
Epoch 299/1000 
	 loss: 28.5856, MinusLogProbMetric: 28.5856, val_loss: 28.7867, val_MinusLogProbMetric: 28.7867

Epoch 299: val_loss improved from 28.85162 to 28.78671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 28.5856 - MinusLogProbMetric: 28.5856 - val_loss: 28.7867 - val_MinusLogProbMetric: 28.7867 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 300/1000
2023-10-26 19:25:10.477 
Epoch 300/1000 
	 loss: 28.5302, MinusLogProbMetric: 28.5302, val_loss: 29.0335, val_MinusLogProbMetric: 29.0335

Epoch 300: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5302 - MinusLogProbMetric: 28.5302 - val_loss: 29.0335 - val_MinusLogProbMetric: 29.0335 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 301/1000
2023-10-26 19:25:53.984 
Epoch 301/1000 
	 loss: 28.5417, MinusLogProbMetric: 28.5417, val_loss: 29.0337, val_MinusLogProbMetric: 29.0337

Epoch 301: val_loss did not improve from 28.78671
196/196 - 44s - loss: 28.5417 - MinusLogProbMetric: 28.5417 - val_loss: 29.0337 - val_MinusLogProbMetric: 29.0337 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 302/1000
2023-10-26 19:26:36.538 
Epoch 302/1000 
	 loss: 28.6403, MinusLogProbMetric: 28.6403, val_loss: 29.2839, val_MinusLogProbMetric: 29.2839

Epoch 302: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.6403 - MinusLogProbMetric: 28.6403 - val_loss: 29.2839 - val_MinusLogProbMetric: 29.2839 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 303/1000
2023-10-26 19:27:19.549 
Epoch 303/1000 
	 loss: 28.6182, MinusLogProbMetric: 28.6182, val_loss: 29.6781, val_MinusLogProbMetric: 29.6781

Epoch 303: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.6182 - MinusLogProbMetric: 28.6182 - val_loss: 29.6781 - val_MinusLogProbMetric: 29.6781 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 304/1000
2023-10-26 19:28:02.446 
Epoch 304/1000 
	 loss: 28.5159, MinusLogProbMetric: 28.5159, val_loss: 29.1498, val_MinusLogProbMetric: 29.1498

Epoch 304: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5159 - MinusLogProbMetric: 28.5159 - val_loss: 29.1498 - val_MinusLogProbMetric: 29.1498 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 305/1000
2023-10-26 19:28:45.538 
Epoch 305/1000 
	 loss: 28.5590, MinusLogProbMetric: 28.5590, val_loss: 28.9902, val_MinusLogProbMetric: 28.9902

Epoch 305: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5590 - MinusLogProbMetric: 28.5590 - val_loss: 28.9902 - val_MinusLogProbMetric: 28.9902 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 306/1000
2023-10-26 19:29:28.454 
Epoch 306/1000 
	 loss: 28.4528, MinusLogProbMetric: 28.4528, val_loss: 29.2133, val_MinusLogProbMetric: 29.2133

Epoch 306: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.4528 - MinusLogProbMetric: 28.4528 - val_loss: 29.2133 - val_MinusLogProbMetric: 29.2133 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 307/1000
2023-10-26 19:30:08.230 
Epoch 307/1000 
	 loss: 28.5091, MinusLogProbMetric: 28.5091, val_loss: 29.1051, val_MinusLogProbMetric: 29.1051

Epoch 307: val_loss did not improve from 28.78671
196/196 - 40s - loss: 28.5091 - MinusLogProbMetric: 28.5091 - val_loss: 29.1051 - val_MinusLogProbMetric: 29.1051 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 308/1000
2023-10-26 19:30:44.759 
Epoch 308/1000 
	 loss: 28.5369, MinusLogProbMetric: 28.5369, val_loss: 30.3673, val_MinusLogProbMetric: 30.3673

Epoch 308: val_loss did not improve from 28.78671
196/196 - 37s - loss: 28.5369 - MinusLogProbMetric: 28.5369 - val_loss: 30.3673 - val_MinusLogProbMetric: 30.3673 - lr: 5.0000e-04 - 37s/epoch - 186ms/step
Epoch 309/1000
2023-10-26 19:31:27.451 
Epoch 309/1000 
	 loss: 28.5487, MinusLogProbMetric: 28.5487, val_loss: 29.3450, val_MinusLogProbMetric: 29.3450

Epoch 309: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5487 - MinusLogProbMetric: 28.5487 - val_loss: 29.3450 - val_MinusLogProbMetric: 29.3450 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 310/1000
2023-10-26 19:32:10.059 
Epoch 310/1000 
	 loss: 28.5455, MinusLogProbMetric: 28.5455, val_loss: 29.2215, val_MinusLogProbMetric: 29.2215

Epoch 310: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5455 - MinusLogProbMetric: 28.5455 - val_loss: 29.2215 - val_MinusLogProbMetric: 29.2215 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 311/1000
2023-10-26 19:32:53.075 
Epoch 311/1000 
	 loss: 28.5517, MinusLogProbMetric: 28.5517, val_loss: 29.0553, val_MinusLogProbMetric: 29.0553

Epoch 311: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5517 - MinusLogProbMetric: 28.5517 - val_loss: 29.0553 - val_MinusLogProbMetric: 29.0553 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 312/1000
2023-10-26 19:33:36.378 
Epoch 312/1000 
	 loss: 28.6419, MinusLogProbMetric: 28.6419, val_loss: 29.1629, val_MinusLogProbMetric: 29.1629

Epoch 312: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.6419 - MinusLogProbMetric: 28.6419 - val_loss: 29.1629 - val_MinusLogProbMetric: 29.1629 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 313/1000
2023-10-26 19:34:19.415 
Epoch 313/1000 
	 loss: 28.4539, MinusLogProbMetric: 28.4539, val_loss: 28.9781, val_MinusLogProbMetric: 28.9781

Epoch 313: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.4539 - MinusLogProbMetric: 28.4539 - val_loss: 28.9781 - val_MinusLogProbMetric: 28.9781 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 314/1000
2023-10-26 19:35:02.919 
Epoch 314/1000 
	 loss: 28.5441, MinusLogProbMetric: 28.5441, val_loss: 28.9448, val_MinusLogProbMetric: 28.9448

Epoch 314: val_loss did not improve from 28.78671
196/196 - 44s - loss: 28.5441 - MinusLogProbMetric: 28.5441 - val_loss: 28.9448 - val_MinusLogProbMetric: 28.9448 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 315/1000
2023-10-26 19:35:46.295 
Epoch 315/1000 
	 loss: 28.4893, MinusLogProbMetric: 28.4893, val_loss: 28.9220, val_MinusLogProbMetric: 28.9220

Epoch 315: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.4893 - MinusLogProbMetric: 28.4893 - val_loss: 28.9220 - val_MinusLogProbMetric: 28.9220 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 316/1000
2023-10-26 19:36:29.551 
Epoch 316/1000 
	 loss: 28.5193, MinusLogProbMetric: 28.5193, val_loss: 29.7200, val_MinusLogProbMetric: 29.7200

Epoch 316: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5193 - MinusLogProbMetric: 28.5193 - val_loss: 29.7200 - val_MinusLogProbMetric: 29.7200 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 317/1000
2023-10-26 19:37:12.814 
Epoch 317/1000 
	 loss: 28.5335, MinusLogProbMetric: 28.5335, val_loss: 28.9146, val_MinusLogProbMetric: 28.9146

Epoch 317: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5335 - MinusLogProbMetric: 28.5335 - val_loss: 28.9146 - val_MinusLogProbMetric: 28.9146 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 318/1000
2023-10-26 19:37:56.122 
Epoch 318/1000 
	 loss: 28.5358, MinusLogProbMetric: 28.5358, val_loss: 28.9913, val_MinusLogProbMetric: 28.9913

Epoch 318: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5358 - MinusLogProbMetric: 28.5358 - val_loss: 28.9913 - val_MinusLogProbMetric: 28.9913 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 319/1000
2023-10-26 19:38:39.076 
Epoch 319/1000 
	 loss: 28.5331, MinusLogProbMetric: 28.5331, val_loss: 29.3205, val_MinusLogProbMetric: 29.3205

Epoch 319: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.5331 - MinusLogProbMetric: 28.5331 - val_loss: 29.3205 - val_MinusLogProbMetric: 29.3205 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 320/1000
2023-10-26 19:39:21.892 
Epoch 320/1000 
	 loss: 28.4988, MinusLogProbMetric: 28.4988, val_loss: 29.3148, val_MinusLogProbMetric: 29.3148

Epoch 320: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.4988 - MinusLogProbMetric: 28.4988 - val_loss: 29.3148 - val_MinusLogProbMetric: 29.3148 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 321/1000
2023-10-26 19:40:05.159 
Epoch 321/1000 
	 loss: 28.6942, MinusLogProbMetric: 28.6942, val_loss: 28.8502, val_MinusLogProbMetric: 28.8502

Epoch 321: val_loss did not improve from 28.78671
196/196 - 43s - loss: 28.6942 - MinusLogProbMetric: 28.6942 - val_loss: 28.8502 - val_MinusLogProbMetric: 28.8502 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 322/1000
2023-10-26 19:40:48.170 
Epoch 322/1000 
	 loss: 28.6206, MinusLogProbMetric: 28.6206, val_loss: 28.6977, val_MinusLogProbMetric: 28.6977

Epoch 322: val_loss improved from 28.78671 to 28.69774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 28.6206 - MinusLogProbMetric: 28.6206 - val_loss: 28.6977 - val_MinusLogProbMetric: 28.6977 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 323/1000
2023-10-26 19:41:32.027 
Epoch 323/1000 
	 loss: 28.5729, MinusLogProbMetric: 28.5729, val_loss: 28.9628, val_MinusLogProbMetric: 28.9628

Epoch 323: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.5729 - MinusLogProbMetric: 28.5729 - val_loss: 28.9628 - val_MinusLogProbMetric: 28.9628 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 324/1000
2023-10-26 19:42:15.733 
Epoch 324/1000 
	 loss: 28.5470, MinusLogProbMetric: 28.5470, val_loss: 28.8598, val_MinusLogProbMetric: 28.8598

Epoch 324: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5470 - MinusLogProbMetric: 28.5470 - val_loss: 28.8598 - val_MinusLogProbMetric: 28.8598 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 325/1000
2023-10-26 19:42:58.691 
Epoch 325/1000 
	 loss: 28.5197, MinusLogProbMetric: 28.5197, val_loss: 29.4073, val_MinusLogProbMetric: 29.4073

Epoch 325: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.5197 - MinusLogProbMetric: 28.5197 - val_loss: 29.4073 - val_MinusLogProbMetric: 29.4073 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 326/1000
2023-10-26 19:43:41.879 
Epoch 326/1000 
	 loss: 28.5094, MinusLogProbMetric: 28.5094, val_loss: 29.7825, val_MinusLogProbMetric: 29.7825

Epoch 326: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.5094 - MinusLogProbMetric: 28.5094 - val_loss: 29.7825 - val_MinusLogProbMetric: 29.7825 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 327/1000
2023-10-26 19:44:25.420 
Epoch 327/1000 
	 loss: 28.5998, MinusLogProbMetric: 28.5998, val_loss: 29.1605, val_MinusLogProbMetric: 29.1605

Epoch 327: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5998 - MinusLogProbMetric: 28.5998 - val_loss: 29.1605 - val_MinusLogProbMetric: 29.1605 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 328/1000
2023-10-26 19:45:08.944 
Epoch 328/1000 
	 loss: 28.5438, MinusLogProbMetric: 28.5438, val_loss: 29.0427, val_MinusLogProbMetric: 29.0427

Epoch 328: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5438 - MinusLogProbMetric: 28.5438 - val_loss: 29.0427 - val_MinusLogProbMetric: 29.0427 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 329/1000
2023-10-26 19:45:52.363 
Epoch 329/1000 
	 loss: 28.4943, MinusLogProbMetric: 28.4943, val_loss: 29.4610, val_MinusLogProbMetric: 29.4610

Epoch 329: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4943 - MinusLogProbMetric: 28.4943 - val_loss: 29.4610 - val_MinusLogProbMetric: 29.4610 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 330/1000
2023-10-26 19:46:35.738 
Epoch 330/1000 
	 loss: 28.4276, MinusLogProbMetric: 28.4276, val_loss: 29.1090, val_MinusLogProbMetric: 29.1090

Epoch 330: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4276 - MinusLogProbMetric: 28.4276 - val_loss: 29.1090 - val_MinusLogProbMetric: 29.1090 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 331/1000
2023-10-26 19:47:18.794 
Epoch 331/1000 
	 loss: 28.4883, MinusLogProbMetric: 28.4883, val_loss: 29.4629, val_MinusLogProbMetric: 29.4629

Epoch 331: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4883 - MinusLogProbMetric: 28.4883 - val_loss: 29.4629 - val_MinusLogProbMetric: 29.4629 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 332/1000
2023-10-26 19:48:02.512 
Epoch 332/1000 
	 loss: 28.5217, MinusLogProbMetric: 28.5217, val_loss: 28.9344, val_MinusLogProbMetric: 28.9344

Epoch 332: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5217 - MinusLogProbMetric: 28.5217 - val_loss: 28.9344 - val_MinusLogProbMetric: 28.9344 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 333/1000
2023-10-26 19:48:45.652 
Epoch 333/1000 
	 loss: 28.5524, MinusLogProbMetric: 28.5524, val_loss: 29.0847, val_MinusLogProbMetric: 29.0847

Epoch 333: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.5524 - MinusLogProbMetric: 28.5524 - val_loss: 29.0847 - val_MinusLogProbMetric: 29.0847 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 334/1000
2023-10-26 19:49:28.600 
Epoch 334/1000 
	 loss: 28.4947, MinusLogProbMetric: 28.4947, val_loss: 29.3900, val_MinusLogProbMetric: 29.3900

Epoch 334: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4947 - MinusLogProbMetric: 28.4947 - val_loss: 29.3900 - val_MinusLogProbMetric: 29.3900 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 335/1000
2023-10-26 19:50:11.569 
Epoch 335/1000 
	 loss: 28.4794, MinusLogProbMetric: 28.4794, val_loss: 29.3680, val_MinusLogProbMetric: 29.3680

Epoch 335: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4794 - MinusLogProbMetric: 28.4794 - val_loss: 29.3680 - val_MinusLogProbMetric: 29.3680 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 336/1000
2023-10-26 19:50:54.544 
Epoch 336/1000 
	 loss: 28.4522, MinusLogProbMetric: 28.4522, val_loss: 28.9160, val_MinusLogProbMetric: 28.9160

Epoch 336: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4522 - MinusLogProbMetric: 28.4522 - val_loss: 28.9160 - val_MinusLogProbMetric: 28.9160 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 337/1000
2023-10-26 19:51:38.100 
Epoch 337/1000 
	 loss: 28.5747, MinusLogProbMetric: 28.5747, val_loss: 29.4000, val_MinusLogProbMetric: 29.4000

Epoch 337: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5747 - MinusLogProbMetric: 28.5747 - val_loss: 29.4000 - val_MinusLogProbMetric: 29.4000 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 338/1000
2023-10-26 19:52:21.276 
Epoch 338/1000 
	 loss: 28.3990, MinusLogProbMetric: 28.3990, val_loss: 29.1931, val_MinusLogProbMetric: 29.1931

Epoch 338: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.3990 - MinusLogProbMetric: 28.3990 - val_loss: 29.1931 - val_MinusLogProbMetric: 29.1931 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 339/1000
2023-10-26 19:53:04.537 
Epoch 339/1000 
	 loss: 28.4653, MinusLogProbMetric: 28.4653, val_loss: 29.5376, val_MinusLogProbMetric: 29.5376

Epoch 339: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4653 - MinusLogProbMetric: 28.4653 - val_loss: 29.5376 - val_MinusLogProbMetric: 29.5376 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 340/1000
2023-10-26 19:53:48.039 
Epoch 340/1000 
	 loss: 28.5501, MinusLogProbMetric: 28.5501, val_loss: 29.2018, val_MinusLogProbMetric: 29.2018

Epoch 340: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5501 - MinusLogProbMetric: 28.5501 - val_loss: 29.2018 - val_MinusLogProbMetric: 29.2018 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 341/1000
2023-10-26 19:54:31.562 
Epoch 341/1000 
	 loss: 28.5559, MinusLogProbMetric: 28.5559, val_loss: 29.3934, val_MinusLogProbMetric: 29.3934

Epoch 341: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5559 - MinusLogProbMetric: 28.5559 - val_loss: 29.3934 - val_MinusLogProbMetric: 29.3934 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 342/1000
2023-10-26 19:55:14.612 
Epoch 342/1000 
	 loss: 28.4825, MinusLogProbMetric: 28.4825, val_loss: 29.1250, val_MinusLogProbMetric: 29.1250

Epoch 342: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4825 - MinusLogProbMetric: 28.4825 - val_loss: 29.1250 - val_MinusLogProbMetric: 29.1250 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 343/1000
2023-10-26 19:55:58.076 
Epoch 343/1000 
	 loss: 28.4957, MinusLogProbMetric: 28.4957, val_loss: 29.0749, val_MinusLogProbMetric: 29.0749

Epoch 343: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4957 - MinusLogProbMetric: 28.4957 - val_loss: 29.0749 - val_MinusLogProbMetric: 29.0749 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 344/1000
2023-10-26 19:56:41.171 
Epoch 344/1000 
	 loss: 28.5473, MinusLogProbMetric: 28.5473, val_loss: 28.8467, val_MinusLogProbMetric: 28.8467

Epoch 344: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.5473 - MinusLogProbMetric: 28.5473 - val_loss: 28.8467 - val_MinusLogProbMetric: 28.8467 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 345/1000
2023-10-26 19:57:24.255 
Epoch 345/1000 
	 loss: 28.4741, MinusLogProbMetric: 28.4741, val_loss: 29.6908, val_MinusLogProbMetric: 29.6908

Epoch 345: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4741 - MinusLogProbMetric: 28.4741 - val_loss: 29.6908 - val_MinusLogProbMetric: 29.6908 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 346/1000
2023-10-26 19:58:07.298 
Epoch 346/1000 
	 loss: 28.5124, MinusLogProbMetric: 28.5124, val_loss: 29.1124, val_MinusLogProbMetric: 29.1124

Epoch 346: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.5124 - MinusLogProbMetric: 28.5124 - val_loss: 29.1124 - val_MinusLogProbMetric: 29.1124 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 347/1000
2023-10-26 19:58:50.692 
Epoch 347/1000 
	 loss: 28.4807, MinusLogProbMetric: 28.4807, val_loss: 28.9502, val_MinusLogProbMetric: 28.9502

Epoch 347: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4807 - MinusLogProbMetric: 28.4807 - val_loss: 28.9502 - val_MinusLogProbMetric: 28.9502 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 348/1000
2023-10-26 19:59:33.739 
Epoch 348/1000 
	 loss: 28.4134, MinusLogProbMetric: 28.4134, val_loss: 29.0968, val_MinusLogProbMetric: 29.0968

Epoch 348: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4134 - MinusLogProbMetric: 28.4134 - val_loss: 29.0968 - val_MinusLogProbMetric: 29.0968 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 349/1000
2023-10-26 20:00:16.574 
Epoch 349/1000 
	 loss: 28.4043, MinusLogProbMetric: 28.4043, val_loss: 29.3888, val_MinusLogProbMetric: 29.3888

Epoch 349: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4043 - MinusLogProbMetric: 28.4043 - val_loss: 29.3888 - val_MinusLogProbMetric: 29.3888 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 350/1000
2023-10-26 20:00:59.990 
Epoch 350/1000 
	 loss: 28.5873, MinusLogProbMetric: 28.5873, val_loss: 28.7582, val_MinusLogProbMetric: 28.7582

Epoch 350: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.5873 - MinusLogProbMetric: 28.5873 - val_loss: 28.7582 - val_MinusLogProbMetric: 28.7582 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 351/1000
2023-10-26 20:01:43.537 
Epoch 351/1000 
	 loss: 28.4942, MinusLogProbMetric: 28.4942, val_loss: 29.2165, val_MinusLogProbMetric: 29.2165

Epoch 351: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.4942 - MinusLogProbMetric: 28.4942 - val_loss: 29.2165 - val_MinusLogProbMetric: 29.2165 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 352/1000
2023-10-26 20:02:26.730 
Epoch 352/1000 
	 loss: 28.4093, MinusLogProbMetric: 28.4093, val_loss: 29.1286, val_MinusLogProbMetric: 29.1286

Epoch 352: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4093 - MinusLogProbMetric: 28.4093 - val_loss: 29.1286 - val_MinusLogProbMetric: 29.1286 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 353/1000
2023-10-26 20:03:09.847 
Epoch 353/1000 
	 loss: 28.4949, MinusLogProbMetric: 28.4949, val_loss: 29.0131, val_MinusLogProbMetric: 29.0131

Epoch 353: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4949 - MinusLogProbMetric: 28.4949 - val_loss: 29.0131 - val_MinusLogProbMetric: 29.0131 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 354/1000
2023-10-26 20:03:53.606 
Epoch 354/1000 
	 loss: 28.4633, MinusLogProbMetric: 28.4633, val_loss: 29.7581, val_MinusLogProbMetric: 29.7581

Epoch 354: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.4633 - MinusLogProbMetric: 28.4633 - val_loss: 29.7581 - val_MinusLogProbMetric: 29.7581 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 355/1000
2023-10-26 20:04:37.147 
Epoch 355/1000 
	 loss: 28.5255, MinusLogProbMetric: 28.5255, val_loss: 29.7351, val_MinusLogProbMetric: 29.7351

Epoch 355: val_loss did not improve from 28.69774
196/196 - 44s - loss: 28.5255 - MinusLogProbMetric: 28.5255 - val_loss: 29.7351 - val_MinusLogProbMetric: 29.7351 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 356/1000
2023-10-26 20:05:20.280 
Epoch 356/1000 
	 loss: 28.4350, MinusLogProbMetric: 28.4350, val_loss: 28.9307, val_MinusLogProbMetric: 28.9307

Epoch 356: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.4350 - MinusLogProbMetric: 28.4350 - val_loss: 28.9307 - val_MinusLogProbMetric: 28.9307 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 357/1000
2023-10-26 20:06:03.718 
Epoch 357/1000 
	 loss: 28.3853, MinusLogProbMetric: 28.3853, val_loss: 28.9995, val_MinusLogProbMetric: 28.9995

Epoch 357: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.3853 - MinusLogProbMetric: 28.3853 - val_loss: 28.9995 - val_MinusLogProbMetric: 28.9995 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 358/1000
2023-10-26 20:06:46.957 
Epoch 358/1000 
	 loss: 28.3786, MinusLogProbMetric: 28.3786, val_loss: 28.7198, val_MinusLogProbMetric: 28.7198

Epoch 358: val_loss did not improve from 28.69774
196/196 - 43s - loss: 28.3786 - MinusLogProbMetric: 28.3786 - val_loss: 28.7198 - val_MinusLogProbMetric: 28.7198 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 359/1000
2023-10-26 20:07:30.301 
Epoch 359/1000 
	 loss: 28.3835, MinusLogProbMetric: 28.3835, val_loss: 28.6426, val_MinusLogProbMetric: 28.6426

Epoch 359: val_loss improved from 28.69774 to 28.64261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 28.3835 - MinusLogProbMetric: 28.3835 - val_loss: 28.6426 - val_MinusLogProbMetric: 28.6426 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 360/1000
2023-10-26 20:08:14.451 
Epoch 360/1000 
	 loss: 28.4119, MinusLogProbMetric: 28.4119, val_loss: 29.1460, val_MinusLogProbMetric: 29.1460

Epoch 360: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4119 - MinusLogProbMetric: 28.4119 - val_loss: 29.1460 - val_MinusLogProbMetric: 29.1460 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 361/1000
2023-10-26 20:08:57.727 
Epoch 361/1000 
	 loss: 28.4252, MinusLogProbMetric: 28.4252, val_loss: 29.6977, val_MinusLogProbMetric: 29.6977

Epoch 361: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4252 - MinusLogProbMetric: 28.4252 - val_loss: 29.6977 - val_MinusLogProbMetric: 29.6977 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 362/1000
2023-10-26 20:09:41.164 
Epoch 362/1000 
	 loss: 28.4970, MinusLogProbMetric: 28.4970, val_loss: 29.7150, val_MinusLogProbMetric: 29.7150

Epoch 362: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4970 - MinusLogProbMetric: 28.4970 - val_loss: 29.7150 - val_MinusLogProbMetric: 29.7150 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 363/1000
2023-10-26 20:10:24.448 
Epoch 363/1000 
	 loss: 28.4898, MinusLogProbMetric: 28.4898, val_loss: 28.9510, val_MinusLogProbMetric: 28.9510

Epoch 363: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4898 - MinusLogProbMetric: 28.4898 - val_loss: 28.9510 - val_MinusLogProbMetric: 28.9510 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 364/1000
2023-10-26 20:11:07.530 
Epoch 364/1000 
	 loss: 28.4901, MinusLogProbMetric: 28.4901, val_loss: 28.8837, val_MinusLogProbMetric: 28.8837

Epoch 364: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4901 - MinusLogProbMetric: 28.4901 - val_loss: 28.8837 - val_MinusLogProbMetric: 28.8837 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 365/1000
2023-10-26 20:11:50.740 
Epoch 365/1000 
	 loss: 28.4443, MinusLogProbMetric: 28.4443, val_loss: 29.3616, val_MinusLogProbMetric: 29.3616

Epoch 365: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4443 - MinusLogProbMetric: 28.4443 - val_loss: 29.3616 - val_MinusLogProbMetric: 29.3616 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 366/1000
2023-10-26 20:12:33.738 
Epoch 366/1000 
	 loss: 28.4464, MinusLogProbMetric: 28.4464, val_loss: 29.0350, val_MinusLogProbMetric: 29.0350

Epoch 366: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4464 - MinusLogProbMetric: 28.4464 - val_loss: 29.0350 - val_MinusLogProbMetric: 29.0350 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 367/1000
2023-10-26 20:13:17.055 
Epoch 367/1000 
	 loss: 28.3534, MinusLogProbMetric: 28.3534, val_loss: 29.4017, val_MinusLogProbMetric: 29.4017

Epoch 367: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3534 - MinusLogProbMetric: 28.3534 - val_loss: 29.4017 - val_MinusLogProbMetric: 29.4017 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 368/1000
2023-10-26 20:14:00.022 
Epoch 368/1000 
	 loss: 28.4985, MinusLogProbMetric: 28.4985, val_loss: 29.1645, val_MinusLogProbMetric: 29.1645

Epoch 368: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4985 - MinusLogProbMetric: 28.4985 - val_loss: 29.1645 - val_MinusLogProbMetric: 29.1645 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 369/1000
2023-10-26 20:14:43.274 
Epoch 369/1000 
	 loss: 28.3731, MinusLogProbMetric: 28.3731, val_loss: 28.8913, val_MinusLogProbMetric: 28.8913

Epoch 369: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3731 - MinusLogProbMetric: 28.3731 - val_loss: 28.8913 - val_MinusLogProbMetric: 28.8913 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 370/1000
2023-10-26 20:15:26.715 
Epoch 370/1000 
	 loss: 28.4343, MinusLogProbMetric: 28.4343, val_loss: 29.0398, val_MinusLogProbMetric: 29.0398

Epoch 370: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4343 - MinusLogProbMetric: 28.4343 - val_loss: 29.0398 - val_MinusLogProbMetric: 29.0398 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 371/1000
2023-10-26 20:16:10.512 
Epoch 371/1000 
	 loss: 28.3331, MinusLogProbMetric: 28.3331, val_loss: 29.0314, val_MinusLogProbMetric: 29.0314

Epoch 371: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.3331 - MinusLogProbMetric: 28.3331 - val_loss: 29.0314 - val_MinusLogProbMetric: 29.0314 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 372/1000
2023-10-26 20:16:53.838 
Epoch 372/1000 
	 loss: 28.4281, MinusLogProbMetric: 28.4281, val_loss: 29.2822, val_MinusLogProbMetric: 29.2822

Epoch 372: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4281 - MinusLogProbMetric: 28.4281 - val_loss: 29.2822 - val_MinusLogProbMetric: 29.2822 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 373/1000
2023-10-26 20:17:37.244 
Epoch 373/1000 
	 loss: 28.4540, MinusLogProbMetric: 28.4540, val_loss: 28.7972, val_MinusLogProbMetric: 28.7972

Epoch 373: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4540 - MinusLogProbMetric: 28.4540 - val_loss: 28.7972 - val_MinusLogProbMetric: 28.7972 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 374/1000
2023-10-26 20:18:20.629 
Epoch 374/1000 
	 loss: 28.3999, MinusLogProbMetric: 28.3999, val_loss: 29.0220, val_MinusLogProbMetric: 29.0220

Epoch 374: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3999 - MinusLogProbMetric: 28.3999 - val_loss: 29.0220 - val_MinusLogProbMetric: 29.0220 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 375/1000
2023-10-26 20:19:03.729 
Epoch 375/1000 
	 loss: 28.4311, MinusLogProbMetric: 28.4311, val_loss: 28.8175, val_MinusLogProbMetric: 28.8175

Epoch 375: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4311 - MinusLogProbMetric: 28.4311 - val_loss: 28.8175 - val_MinusLogProbMetric: 28.8175 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 376/1000
2023-10-26 20:19:47.121 
Epoch 376/1000 
	 loss: 28.3579, MinusLogProbMetric: 28.3579, val_loss: 28.9372, val_MinusLogProbMetric: 28.9372

Epoch 376: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3579 - MinusLogProbMetric: 28.3579 - val_loss: 28.9372 - val_MinusLogProbMetric: 28.9372 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 377/1000
2023-10-26 20:20:30.419 
Epoch 377/1000 
	 loss: 28.4049, MinusLogProbMetric: 28.4049, val_loss: 28.9539, val_MinusLogProbMetric: 28.9539

Epoch 377: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4049 - MinusLogProbMetric: 28.4049 - val_loss: 28.9539 - val_MinusLogProbMetric: 28.9539 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 378/1000
2023-10-26 20:21:13.891 
Epoch 378/1000 
	 loss: 28.4019, MinusLogProbMetric: 28.4019, val_loss: 29.0817, val_MinusLogProbMetric: 29.0817

Epoch 378: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4019 - MinusLogProbMetric: 28.4019 - val_loss: 29.0817 - val_MinusLogProbMetric: 29.0817 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 379/1000
2023-10-26 20:21:57.046 
Epoch 379/1000 
	 loss: 28.4513, MinusLogProbMetric: 28.4513, val_loss: 30.0984, val_MinusLogProbMetric: 30.0984

Epoch 379: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4513 - MinusLogProbMetric: 28.4513 - val_loss: 30.0984 - val_MinusLogProbMetric: 30.0984 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 380/1000
2023-10-26 20:22:40.029 
Epoch 380/1000 
	 loss: 28.4264, MinusLogProbMetric: 28.4264, val_loss: 28.7660, val_MinusLogProbMetric: 28.7660

Epoch 380: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4264 - MinusLogProbMetric: 28.4264 - val_loss: 28.7660 - val_MinusLogProbMetric: 28.7660 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 381/1000
2023-10-26 20:23:23.689 
Epoch 381/1000 
	 loss: 28.3790, MinusLogProbMetric: 28.3790, val_loss: 29.2317, val_MinusLogProbMetric: 29.2317

Epoch 381: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.3790 - MinusLogProbMetric: 28.3790 - val_loss: 29.2317 - val_MinusLogProbMetric: 29.2317 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 382/1000
2023-10-26 20:24:06.997 
Epoch 382/1000 
	 loss: 28.4798, MinusLogProbMetric: 28.4798, val_loss: 29.0692, val_MinusLogProbMetric: 29.0692

Epoch 382: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4798 - MinusLogProbMetric: 28.4798 - val_loss: 29.0692 - val_MinusLogProbMetric: 29.0692 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 383/1000
2023-10-26 20:24:50.638 
Epoch 383/1000 
	 loss: 28.3718, MinusLogProbMetric: 28.3718, val_loss: 29.4571, val_MinusLogProbMetric: 29.4571

Epoch 383: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.3718 - MinusLogProbMetric: 28.3718 - val_loss: 29.4571 - val_MinusLogProbMetric: 29.4571 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 384/1000
2023-10-26 20:25:33.716 
Epoch 384/1000 
	 loss: 28.5228, MinusLogProbMetric: 28.5228, val_loss: 29.0541, val_MinusLogProbMetric: 29.0541

Epoch 384: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.5228 - MinusLogProbMetric: 28.5228 - val_loss: 29.0541 - val_MinusLogProbMetric: 29.0541 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 385/1000
2023-10-26 20:26:17.539 
Epoch 385/1000 
	 loss: 28.4481, MinusLogProbMetric: 28.4481, val_loss: 29.4583, val_MinusLogProbMetric: 29.4583

Epoch 385: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.4481 - MinusLogProbMetric: 28.4481 - val_loss: 29.4583 - val_MinusLogProbMetric: 29.4583 - lr: 5.0000e-04 - 44s/epoch - 224ms/step
Epoch 386/1000
2023-10-26 20:27:00.937 
Epoch 386/1000 
	 loss: 28.4466, MinusLogProbMetric: 28.4466, val_loss: 28.9981, val_MinusLogProbMetric: 28.9981

Epoch 386: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4466 - MinusLogProbMetric: 28.4466 - val_loss: 28.9981 - val_MinusLogProbMetric: 28.9981 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 387/1000
2023-10-26 20:27:43.945 
Epoch 387/1000 
	 loss: 28.4362, MinusLogProbMetric: 28.4362, val_loss: 29.3446, val_MinusLogProbMetric: 29.3446

Epoch 387: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4362 - MinusLogProbMetric: 28.4362 - val_loss: 29.3446 - val_MinusLogProbMetric: 29.3446 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 388/1000
2023-10-26 20:28:26.616 
Epoch 388/1000 
	 loss: 28.4462, MinusLogProbMetric: 28.4462, val_loss: 28.9869, val_MinusLogProbMetric: 28.9869

Epoch 388: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4462 - MinusLogProbMetric: 28.4462 - val_loss: 28.9869 - val_MinusLogProbMetric: 28.9869 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 389/1000
2023-10-26 20:29:09.807 
Epoch 389/1000 
	 loss: 28.3599, MinusLogProbMetric: 28.3599, val_loss: 28.7899, val_MinusLogProbMetric: 28.7899

Epoch 389: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3599 - MinusLogProbMetric: 28.3599 - val_loss: 28.7899 - val_MinusLogProbMetric: 28.7899 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 390/1000
2023-10-26 20:29:52.639 
Epoch 390/1000 
	 loss: 28.4184, MinusLogProbMetric: 28.4184, val_loss: 29.0079, val_MinusLogProbMetric: 29.0079

Epoch 390: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4184 - MinusLogProbMetric: 28.4184 - val_loss: 29.0079 - val_MinusLogProbMetric: 29.0079 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 391/1000
2023-10-26 20:30:35.923 
Epoch 391/1000 
	 loss: 28.4872, MinusLogProbMetric: 28.4872, val_loss: 28.8455, val_MinusLogProbMetric: 28.8455

Epoch 391: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4872 - MinusLogProbMetric: 28.4872 - val_loss: 28.8455 - val_MinusLogProbMetric: 28.8455 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 392/1000
2023-10-26 20:31:19.246 
Epoch 392/1000 
	 loss: 28.4147, MinusLogProbMetric: 28.4147, val_loss: 28.9334, val_MinusLogProbMetric: 28.9334

Epoch 392: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4147 - MinusLogProbMetric: 28.4147 - val_loss: 28.9334 - val_MinusLogProbMetric: 28.9334 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 393/1000
2023-10-26 20:32:02.269 
Epoch 393/1000 
	 loss: 28.3575, MinusLogProbMetric: 28.3575, val_loss: 29.2886, val_MinusLogProbMetric: 29.2886

Epoch 393: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3575 - MinusLogProbMetric: 28.3575 - val_loss: 29.2886 - val_MinusLogProbMetric: 29.2886 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 394/1000
2023-10-26 20:32:45.598 
Epoch 394/1000 
	 loss: 28.4375, MinusLogProbMetric: 28.4375, val_loss: 28.8804, val_MinusLogProbMetric: 28.8804

Epoch 394: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4375 - MinusLogProbMetric: 28.4375 - val_loss: 28.8804 - val_MinusLogProbMetric: 28.8804 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 395/1000
2023-10-26 20:33:29.191 
Epoch 395/1000 
	 loss: 28.3702, MinusLogProbMetric: 28.3702, val_loss: 29.1489, val_MinusLogProbMetric: 29.1489

Epoch 395: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.3702 - MinusLogProbMetric: 28.3702 - val_loss: 29.1489 - val_MinusLogProbMetric: 29.1489 - lr: 5.0000e-04 - 44s/epoch - 222ms/step
Epoch 396/1000
2023-10-26 20:34:12.473 
Epoch 396/1000 
	 loss: 28.3980, MinusLogProbMetric: 28.3980, val_loss: 29.1121, val_MinusLogProbMetric: 29.1121

Epoch 396: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3980 - MinusLogProbMetric: 28.3980 - val_loss: 29.1121 - val_MinusLogProbMetric: 29.1121 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 397/1000
2023-10-26 20:34:55.623 
Epoch 397/1000 
	 loss: 28.4087, MinusLogProbMetric: 28.4087, val_loss: 28.6910, val_MinusLogProbMetric: 28.6910

Epoch 397: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4087 - MinusLogProbMetric: 28.4087 - val_loss: 28.6910 - val_MinusLogProbMetric: 28.6910 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 398/1000
2023-10-26 20:35:38.880 
Epoch 398/1000 
	 loss: 28.3574, MinusLogProbMetric: 28.3574, val_loss: 28.8681, val_MinusLogProbMetric: 28.8681

Epoch 398: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3574 - MinusLogProbMetric: 28.3574 - val_loss: 28.8681 - val_MinusLogProbMetric: 28.8681 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 399/1000
2023-10-26 20:36:22.629 
Epoch 399/1000 
	 loss: 28.4501, MinusLogProbMetric: 28.4501, val_loss: 29.0369, val_MinusLogProbMetric: 29.0369

Epoch 399: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.4501 - MinusLogProbMetric: 28.4501 - val_loss: 29.0369 - val_MinusLogProbMetric: 29.0369 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 400/1000
2023-10-26 20:37:06.286 
Epoch 400/1000 
	 loss: 28.3942, MinusLogProbMetric: 28.3942, val_loss: 28.6626, val_MinusLogProbMetric: 28.6626

Epoch 400: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.3942 - MinusLogProbMetric: 28.3942 - val_loss: 28.6626 - val_MinusLogProbMetric: 28.6626 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 401/1000
2023-10-26 20:37:49.962 
Epoch 401/1000 
	 loss: 28.4216, MinusLogProbMetric: 28.4216, val_loss: 29.0986, val_MinusLogProbMetric: 29.0986

Epoch 401: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.4216 - MinusLogProbMetric: 28.4216 - val_loss: 29.0986 - val_MinusLogProbMetric: 29.0986 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 402/1000
2023-10-26 20:38:33.623 
Epoch 402/1000 
	 loss: 28.3737, MinusLogProbMetric: 28.3737, val_loss: 28.7267, val_MinusLogProbMetric: 28.7267

Epoch 402: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.3737 - MinusLogProbMetric: 28.3737 - val_loss: 28.7267 - val_MinusLogProbMetric: 28.7267 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 403/1000
2023-10-26 20:39:16.938 
Epoch 403/1000 
	 loss: 28.3433, MinusLogProbMetric: 28.3433, val_loss: 29.0696, val_MinusLogProbMetric: 29.0696

Epoch 403: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3433 - MinusLogProbMetric: 28.3433 - val_loss: 29.0696 - val_MinusLogProbMetric: 29.0696 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 404/1000
2023-10-26 20:40:00.681 
Epoch 404/1000 
	 loss: 28.3444, MinusLogProbMetric: 28.3444, val_loss: 29.0400, val_MinusLogProbMetric: 29.0400

Epoch 404: val_loss did not improve from 28.64261
196/196 - 44s - loss: 28.3444 - MinusLogProbMetric: 28.3444 - val_loss: 29.0400 - val_MinusLogProbMetric: 29.0400 - lr: 5.0000e-04 - 44s/epoch - 223ms/step
Epoch 405/1000
2023-10-26 20:40:43.969 
Epoch 405/1000 
	 loss: 28.3757, MinusLogProbMetric: 28.3757, val_loss: 29.1986, val_MinusLogProbMetric: 29.1986

Epoch 405: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3757 - MinusLogProbMetric: 28.3757 - val_loss: 29.1986 - val_MinusLogProbMetric: 29.1986 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 406/1000
2023-10-26 20:41:27.396 
Epoch 406/1000 
	 loss: 28.4653, MinusLogProbMetric: 28.4653, val_loss: 28.8529, val_MinusLogProbMetric: 28.8529

Epoch 406: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.4653 - MinusLogProbMetric: 28.4653 - val_loss: 28.8529 - val_MinusLogProbMetric: 28.8529 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 407/1000
2023-10-26 20:42:10.435 
Epoch 407/1000 
	 loss: 28.2976, MinusLogProbMetric: 28.2976, val_loss: 28.8361, val_MinusLogProbMetric: 28.8361

Epoch 407: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.2976 - MinusLogProbMetric: 28.2976 - val_loss: 28.8361 - val_MinusLogProbMetric: 28.8361 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 408/1000
2023-10-26 20:42:53.925 
Epoch 408/1000 
	 loss: 28.3675, MinusLogProbMetric: 28.3675, val_loss: 28.9464, val_MinusLogProbMetric: 28.9464

Epoch 408: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3675 - MinusLogProbMetric: 28.3675 - val_loss: 28.9464 - val_MinusLogProbMetric: 28.9464 - lr: 5.0000e-04 - 43s/epoch - 222ms/step
Epoch 409/1000
2023-10-26 20:43:37.185 
Epoch 409/1000 
	 loss: 28.3358, MinusLogProbMetric: 28.3358, val_loss: 29.4574, val_MinusLogProbMetric: 29.4574

Epoch 409: val_loss did not improve from 28.64261
196/196 - 43s - loss: 28.3358 - MinusLogProbMetric: 28.3358 - val_loss: 29.4574 - val_MinusLogProbMetric: 29.4574 - lr: 5.0000e-04 - 43s/epoch - 221ms/step
Epoch 410/1000
2023-10-26 20:44:20.314 
Epoch 410/1000 
	 loss: 27.9128, MinusLogProbMetric: 27.9128, val_loss: 28.4757, val_MinusLogProbMetric: 28.4757

Epoch 410: val_loss improved from 28.64261 to 28.47566, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.9128 - MinusLogProbMetric: 27.9128 - val_loss: 28.4757 - val_MinusLogProbMetric: 28.4757 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 411/1000
2023-10-26 20:45:04.587 
Epoch 411/1000 
	 loss: 27.8502, MinusLogProbMetric: 27.8502, val_loss: 28.4200, val_MinusLogProbMetric: 28.4200

Epoch 411: val_loss improved from 28.47566 to 28.42000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.8502 - MinusLogProbMetric: 27.8502 - val_loss: 28.4200 - val_MinusLogProbMetric: 28.4200 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 412/1000
2023-10-26 20:45:48.546 
Epoch 412/1000 
	 loss: 27.8757, MinusLogProbMetric: 27.8757, val_loss: 28.5592, val_MinusLogProbMetric: 28.5592

Epoch 412: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8757 - MinusLogProbMetric: 27.8757 - val_loss: 28.5592 - val_MinusLogProbMetric: 28.5592 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 413/1000
2023-10-26 20:46:32.169 
Epoch 413/1000 
	 loss: 27.8544, MinusLogProbMetric: 27.8544, val_loss: 28.7382, val_MinusLogProbMetric: 28.7382

Epoch 413: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8544 - MinusLogProbMetric: 27.8544 - val_loss: 28.7382 - val_MinusLogProbMetric: 28.7382 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 414/1000
2023-10-26 20:47:15.648 
Epoch 414/1000 
	 loss: 27.9027, MinusLogProbMetric: 27.9027, val_loss: 28.5840, val_MinusLogProbMetric: 28.5840

Epoch 414: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9027 - MinusLogProbMetric: 27.9027 - val_loss: 28.5840 - val_MinusLogProbMetric: 28.5840 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 415/1000
2023-10-26 20:47:59.656 
Epoch 415/1000 
	 loss: 27.8690, MinusLogProbMetric: 27.8690, val_loss: 28.5306, val_MinusLogProbMetric: 28.5306

Epoch 415: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8690 - MinusLogProbMetric: 27.8690 - val_loss: 28.5306 - val_MinusLogProbMetric: 28.5306 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 416/1000
2023-10-26 20:48:42.983 
Epoch 416/1000 
	 loss: 27.8816, MinusLogProbMetric: 27.8816, val_loss: 28.6258, val_MinusLogProbMetric: 28.6258

Epoch 416: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8816 - MinusLogProbMetric: 27.8816 - val_loss: 28.6258 - val_MinusLogProbMetric: 28.6258 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 417/1000
2023-10-26 20:49:26.402 
Epoch 417/1000 
	 loss: 27.8826, MinusLogProbMetric: 27.8826, val_loss: 28.4295, val_MinusLogProbMetric: 28.4295

Epoch 417: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8826 - MinusLogProbMetric: 27.8826 - val_loss: 28.4295 - val_MinusLogProbMetric: 28.4295 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 418/1000
2023-10-26 20:50:09.805 
Epoch 418/1000 
	 loss: 27.8903, MinusLogProbMetric: 27.8903, val_loss: 28.7352, val_MinusLogProbMetric: 28.7352

Epoch 418: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8903 - MinusLogProbMetric: 27.8903 - val_loss: 28.7352 - val_MinusLogProbMetric: 28.7352 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 419/1000
2023-10-26 20:50:53.162 
Epoch 419/1000 
	 loss: 27.8838, MinusLogProbMetric: 27.8838, val_loss: 28.6681, val_MinusLogProbMetric: 28.6681

Epoch 419: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8838 - MinusLogProbMetric: 27.8838 - val_loss: 28.6681 - val_MinusLogProbMetric: 28.6681 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 420/1000
2023-10-26 20:51:36.510 
Epoch 420/1000 
	 loss: 27.8736, MinusLogProbMetric: 27.8736, val_loss: 28.4430, val_MinusLogProbMetric: 28.4430

Epoch 420: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8736 - MinusLogProbMetric: 27.8736 - val_loss: 28.4430 - val_MinusLogProbMetric: 28.4430 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 421/1000
2023-10-26 20:52:20.097 
Epoch 421/1000 
	 loss: 27.9138, MinusLogProbMetric: 27.9138, val_loss: 28.5246, val_MinusLogProbMetric: 28.5246

Epoch 421: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.9138 - MinusLogProbMetric: 27.9138 - val_loss: 28.5246 - val_MinusLogProbMetric: 28.5246 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 422/1000
2023-10-26 20:53:03.493 
Epoch 422/1000 
	 loss: 27.9112, MinusLogProbMetric: 27.9112, val_loss: 28.5503, val_MinusLogProbMetric: 28.5503

Epoch 422: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9112 - MinusLogProbMetric: 27.9112 - val_loss: 28.5503 - val_MinusLogProbMetric: 28.5503 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 423/1000
2023-10-26 20:53:46.848 
Epoch 423/1000 
	 loss: 27.8628, MinusLogProbMetric: 27.8628, val_loss: 28.5842, val_MinusLogProbMetric: 28.5842

Epoch 423: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8628 - MinusLogProbMetric: 27.8628 - val_loss: 28.5842 - val_MinusLogProbMetric: 28.5842 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 424/1000
2023-10-26 20:54:30.135 
Epoch 424/1000 
	 loss: 27.8599, MinusLogProbMetric: 27.8599, val_loss: 28.5552, val_MinusLogProbMetric: 28.5552

Epoch 424: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8599 - MinusLogProbMetric: 27.8599 - val_loss: 28.5552 - val_MinusLogProbMetric: 28.5552 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 425/1000
2023-10-26 20:55:13.074 
Epoch 425/1000 
	 loss: 27.9073, MinusLogProbMetric: 27.9073, val_loss: 28.5208, val_MinusLogProbMetric: 28.5208

Epoch 425: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9073 - MinusLogProbMetric: 27.9073 - val_loss: 28.5208 - val_MinusLogProbMetric: 28.5208 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 426/1000
2023-10-26 20:55:56.368 
Epoch 426/1000 
	 loss: 27.8694, MinusLogProbMetric: 27.8694, val_loss: 28.6469, val_MinusLogProbMetric: 28.6469

Epoch 426: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8694 - MinusLogProbMetric: 27.8694 - val_loss: 28.6469 - val_MinusLogProbMetric: 28.6469 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 427/1000
2023-10-26 20:56:40.467 
Epoch 427/1000 
	 loss: 27.8862, MinusLogProbMetric: 27.8862, val_loss: 28.6592, val_MinusLogProbMetric: 28.6592

Epoch 427: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8862 - MinusLogProbMetric: 27.8862 - val_loss: 28.6592 - val_MinusLogProbMetric: 28.6592 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 428/1000
2023-10-26 20:57:24.116 
Epoch 428/1000 
	 loss: 27.8556, MinusLogProbMetric: 27.8556, val_loss: 28.4983, val_MinusLogProbMetric: 28.4983

Epoch 428: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8556 - MinusLogProbMetric: 27.8556 - val_loss: 28.4983 - val_MinusLogProbMetric: 28.4983 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 429/1000
2023-10-26 20:58:07.404 
Epoch 429/1000 
	 loss: 27.8904, MinusLogProbMetric: 27.8904, val_loss: 28.5772, val_MinusLogProbMetric: 28.5772

Epoch 429: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8904 - MinusLogProbMetric: 27.8904 - val_loss: 28.5772 - val_MinusLogProbMetric: 28.5772 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 430/1000
2023-10-26 20:58:50.477 
Epoch 430/1000 
	 loss: 27.9105, MinusLogProbMetric: 27.9105, val_loss: 28.4283, val_MinusLogProbMetric: 28.4283

Epoch 430: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9105 - MinusLogProbMetric: 27.9105 - val_loss: 28.4283 - val_MinusLogProbMetric: 28.4283 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 431/1000
2023-10-26 20:59:33.470 
Epoch 431/1000 
	 loss: 27.8814, MinusLogProbMetric: 27.8814, val_loss: 29.2962, val_MinusLogProbMetric: 29.2962

Epoch 431: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8814 - MinusLogProbMetric: 27.8814 - val_loss: 29.2962 - val_MinusLogProbMetric: 29.2962 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 432/1000
2023-10-26 21:00:16.652 
Epoch 432/1000 
	 loss: 27.9106, MinusLogProbMetric: 27.9106, val_loss: 28.5613, val_MinusLogProbMetric: 28.5613

Epoch 432: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9106 - MinusLogProbMetric: 27.9106 - val_loss: 28.5613 - val_MinusLogProbMetric: 28.5613 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 433/1000
2023-10-26 21:01:00.775 
Epoch 433/1000 
	 loss: 27.8773, MinusLogProbMetric: 27.8773, val_loss: 28.4821, val_MinusLogProbMetric: 28.4821

Epoch 433: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8773 - MinusLogProbMetric: 27.8773 - val_loss: 28.4821 - val_MinusLogProbMetric: 28.4821 - lr: 2.5000e-04 - 44s/epoch - 225ms/step
Epoch 434/1000
2023-10-26 21:01:44.692 
Epoch 434/1000 
	 loss: 27.8783, MinusLogProbMetric: 27.8783, val_loss: 28.9493, val_MinusLogProbMetric: 28.9493

Epoch 434: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8783 - MinusLogProbMetric: 27.8783 - val_loss: 28.9493 - val_MinusLogProbMetric: 28.9493 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 435/1000
2023-10-26 21:02:27.963 
Epoch 435/1000 
	 loss: 27.9245, MinusLogProbMetric: 27.9245, val_loss: 28.4409, val_MinusLogProbMetric: 28.4409

Epoch 435: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9245 - MinusLogProbMetric: 27.9245 - val_loss: 28.4409 - val_MinusLogProbMetric: 28.4409 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 436/1000
2023-10-26 21:03:11.586 
Epoch 436/1000 
	 loss: 27.8927, MinusLogProbMetric: 27.8927, val_loss: 28.5754, val_MinusLogProbMetric: 28.5754

Epoch 436: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8927 - MinusLogProbMetric: 27.8927 - val_loss: 28.5754 - val_MinusLogProbMetric: 28.5754 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 437/1000
2023-10-26 21:03:54.736 
Epoch 437/1000 
	 loss: 27.9052, MinusLogProbMetric: 27.9052, val_loss: 28.9207, val_MinusLogProbMetric: 28.9207

Epoch 437: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9052 - MinusLogProbMetric: 27.9052 - val_loss: 28.9207 - val_MinusLogProbMetric: 28.9207 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 438/1000
2023-10-26 21:04:37.771 
Epoch 438/1000 
	 loss: 27.9105, MinusLogProbMetric: 27.9105, val_loss: 28.8498, val_MinusLogProbMetric: 28.8498

Epoch 438: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9105 - MinusLogProbMetric: 27.9105 - val_loss: 28.8498 - val_MinusLogProbMetric: 28.8498 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 439/1000
2023-10-26 21:05:21.590 
Epoch 439/1000 
	 loss: 27.9159, MinusLogProbMetric: 27.9159, val_loss: 28.4744, val_MinusLogProbMetric: 28.4744

Epoch 439: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.9159 - MinusLogProbMetric: 27.9159 - val_loss: 28.4744 - val_MinusLogProbMetric: 28.4744 - lr: 2.5000e-04 - 44s/epoch - 224ms/step
Epoch 440/1000
2023-10-26 21:06:04.861 
Epoch 440/1000 
	 loss: 27.8729, MinusLogProbMetric: 27.8729, val_loss: 28.5391, val_MinusLogProbMetric: 28.5391

Epoch 440: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8729 - MinusLogProbMetric: 27.8729 - val_loss: 28.5391 - val_MinusLogProbMetric: 28.5391 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 441/1000
2023-10-26 21:06:48.050 
Epoch 441/1000 
	 loss: 27.9224, MinusLogProbMetric: 27.9224, val_loss: 28.4633, val_MinusLogProbMetric: 28.4633

Epoch 441: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9224 - MinusLogProbMetric: 27.9224 - val_loss: 28.4633 - val_MinusLogProbMetric: 28.4633 - lr: 2.5000e-04 - 43s/epoch - 220ms/step
Epoch 442/1000
2023-10-26 21:07:31.335 
Epoch 442/1000 
	 loss: 27.8428, MinusLogProbMetric: 27.8428, val_loss: 28.4894, val_MinusLogProbMetric: 28.4894

Epoch 442: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8428 - MinusLogProbMetric: 27.8428 - val_loss: 28.4894 - val_MinusLogProbMetric: 28.4894 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 443/1000
2023-10-26 21:08:14.980 
Epoch 443/1000 
	 loss: 27.8374, MinusLogProbMetric: 27.8374, val_loss: 28.7281, val_MinusLogProbMetric: 28.7281

Epoch 443: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8374 - MinusLogProbMetric: 27.8374 - val_loss: 28.7281 - val_MinusLogProbMetric: 28.7281 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 444/1000
2023-10-26 21:08:58.317 
Epoch 444/1000 
	 loss: 27.9049, MinusLogProbMetric: 27.9049, val_loss: 28.8741, val_MinusLogProbMetric: 28.8741

Epoch 444: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9049 - MinusLogProbMetric: 27.9049 - val_loss: 28.8741 - val_MinusLogProbMetric: 28.8741 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 445/1000
2023-10-26 21:09:41.698 
Epoch 445/1000 
	 loss: 27.9145, MinusLogProbMetric: 27.9145, val_loss: 28.4414, val_MinusLogProbMetric: 28.4414

Epoch 445: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9145 - MinusLogProbMetric: 27.9145 - val_loss: 28.4414 - val_MinusLogProbMetric: 28.4414 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 446/1000
2023-10-26 21:10:25.225 
Epoch 446/1000 
	 loss: 27.8612, MinusLogProbMetric: 27.8612, val_loss: 28.4225, val_MinusLogProbMetric: 28.4225

Epoch 446: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8612 - MinusLogProbMetric: 27.8612 - val_loss: 28.4225 - val_MinusLogProbMetric: 28.4225 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 447/1000
2023-10-26 21:11:08.626 
Epoch 447/1000 
	 loss: 27.8681, MinusLogProbMetric: 27.8681, val_loss: 28.4831, val_MinusLogProbMetric: 28.4831

Epoch 447: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8681 - MinusLogProbMetric: 27.8681 - val_loss: 28.4831 - val_MinusLogProbMetric: 28.4831 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 448/1000
2023-10-26 21:11:52.190 
Epoch 448/1000 
	 loss: 27.8775, MinusLogProbMetric: 27.8775, val_loss: 28.4743, val_MinusLogProbMetric: 28.4743

Epoch 448: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8775 - MinusLogProbMetric: 27.8775 - val_loss: 28.4743 - val_MinusLogProbMetric: 28.4743 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 449/1000
2023-10-26 21:12:35.866 
Epoch 449/1000 
	 loss: 27.8860, MinusLogProbMetric: 27.8860, val_loss: 28.6010, val_MinusLogProbMetric: 28.6010

Epoch 449: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8860 - MinusLogProbMetric: 27.8860 - val_loss: 28.6010 - val_MinusLogProbMetric: 28.6010 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 450/1000
2023-10-26 21:13:19.332 
Epoch 450/1000 
	 loss: 27.8533, MinusLogProbMetric: 27.8533, val_loss: 28.5493, val_MinusLogProbMetric: 28.5493

Epoch 450: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8533 - MinusLogProbMetric: 27.8533 - val_loss: 28.5493 - val_MinusLogProbMetric: 28.5493 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 451/1000
2023-10-26 21:14:02.697 
Epoch 451/1000 
	 loss: 27.8816, MinusLogProbMetric: 27.8816, val_loss: 28.4503, val_MinusLogProbMetric: 28.4503

Epoch 451: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8816 - MinusLogProbMetric: 27.8816 - val_loss: 28.4503 - val_MinusLogProbMetric: 28.4503 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 452/1000
2023-10-26 21:14:46.478 
Epoch 452/1000 
	 loss: 27.8486, MinusLogProbMetric: 27.8486, val_loss: 28.5428, val_MinusLogProbMetric: 28.5428

Epoch 452: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8486 - MinusLogProbMetric: 27.8486 - val_loss: 28.5428 - val_MinusLogProbMetric: 28.5428 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 453/1000
2023-10-26 21:15:29.956 
Epoch 453/1000 
	 loss: 27.8488, MinusLogProbMetric: 27.8488, val_loss: 28.6733, val_MinusLogProbMetric: 28.6733

Epoch 453: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8488 - MinusLogProbMetric: 27.8488 - val_loss: 28.6733 - val_MinusLogProbMetric: 28.6733 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 454/1000
2023-10-26 21:16:13.521 
Epoch 454/1000 
	 loss: 27.9017, MinusLogProbMetric: 27.9017, val_loss: 28.4565, val_MinusLogProbMetric: 28.4565

Epoch 454: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.9017 - MinusLogProbMetric: 27.9017 - val_loss: 28.4565 - val_MinusLogProbMetric: 28.4565 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 455/1000
2023-10-26 21:16:57.029 
Epoch 455/1000 
	 loss: 27.8908, MinusLogProbMetric: 27.8908, val_loss: 28.7885, val_MinusLogProbMetric: 28.7885

Epoch 455: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8908 - MinusLogProbMetric: 27.8908 - val_loss: 28.7885 - val_MinusLogProbMetric: 28.7885 - lr: 2.5000e-04 - 44s/epoch - 222ms/step
Epoch 456/1000
2023-10-26 21:17:39.934 
Epoch 456/1000 
	 loss: 27.8938, MinusLogProbMetric: 27.8938, val_loss: 28.6800, val_MinusLogProbMetric: 28.6800

Epoch 456: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8938 - MinusLogProbMetric: 27.8938 - val_loss: 28.6800 - val_MinusLogProbMetric: 28.6800 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 457/1000
2023-10-26 21:18:23.317 
Epoch 457/1000 
	 loss: 27.8394, MinusLogProbMetric: 27.8394, val_loss: 28.7912, val_MinusLogProbMetric: 28.7912

Epoch 457: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8394 - MinusLogProbMetric: 27.8394 - val_loss: 28.7912 - val_MinusLogProbMetric: 28.7912 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 458/1000
2023-10-26 21:19:06.653 
Epoch 458/1000 
	 loss: 27.8778, MinusLogProbMetric: 27.8778, val_loss: 28.8550, val_MinusLogProbMetric: 28.8550

Epoch 458: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.8778 - MinusLogProbMetric: 27.8778 - val_loss: 28.8550 - val_MinusLogProbMetric: 28.8550 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 459/1000
2023-10-26 21:19:49.907 
Epoch 459/1000 
	 loss: 27.9017, MinusLogProbMetric: 27.9017, val_loss: 28.8173, val_MinusLogProbMetric: 28.8173

Epoch 459: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9017 - MinusLogProbMetric: 27.9017 - val_loss: 28.8173 - val_MinusLogProbMetric: 28.8173 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 460/1000
2023-10-26 21:20:33.375 
Epoch 460/1000 
	 loss: 27.9061, MinusLogProbMetric: 27.9061, val_loss: 28.5698, val_MinusLogProbMetric: 28.5698

Epoch 460: val_loss did not improve from 28.42000
196/196 - 43s - loss: 27.9061 - MinusLogProbMetric: 27.9061 - val_loss: 28.5698 - val_MinusLogProbMetric: 28.5698 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 461/1000
2023-10-26 21:21:17.148 
Epoch 461/1000 
	 loss: 27.8791, MinusLogProbMetric: 27.8791, val_loss: 28.4865, val_MinusLogProbMetric: 28.4865

Epoch 461: val_loss did not improve from 28.42000
196/196 - 44s - loss: 27.8791 - MinusLogProbMetric: 27.8791 - val_loss: 28.4865 - val_MinusLogProbMetric: 28.4865 - lr: 2.5000e-04 - 44s/epoch - 223ms/step
Epoch 462/1000
2023-10-26 21:22:00.866 
Epoch 462/1000 
	 loss: 27.7001, MinusLogProbMetric: 27.7001, val_loss: 28.3393, val_MinusLogProbMetric: 28.3393

Epoch 462: val_loss improved from 28.42000 to 28.33928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 45s - loss: 27.7001 - MinusLogProbMetric: 27.7001 - val_loss: 28.3393 - val_MinusLogProbMetric: 28.3393 - lr: 1.2500e-04 - 45s/epoch - 228ms/step
Epoch 463/1000
2023-10-26 21:22:45.177 
Epoch 463/1000 
	 loss: 27.6649, MinusLogProbMetric: 27.6649, val_loss: 28.3228, val_MinusLogProbMetric: 28.3228

Epoch 463: val_loss improved from 28.33928 to 28.32283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.6649 - MinusLogProbMetric: 27.6649 - val_loss: 28.3228 - val_MinusLogProbMetric: 28.3228 - lr: 1.2500e-04 - 44s/epoch - 226ms/step
Epoch 464/1000
2023-10-26 21:23:29.728 
Epoch 464/1000 
	 loss: 27.6763, MinusLogProbMetric: 27.6763, val_loss: 28.3791, val_MinusLogProbMetric: 28.3791

Epoch 464: val_loss did not improve from 28.32283
196/196 - 44s - loss: 27.6763 - MinusLogProbMetric: 27.6763 - val_loss: 28.3791 - val_MinusLogProbMetric: 28.3791 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 465/1000
2023-10-26 21:24:13.017 
Epoch 465/1000 
	 loss: 27.7084, MinusLogProbMetric: 27.7084, val_loss: 28.3490, val_MinusLogProbMetric: 28.3490

Epoch 465: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.7084 - MinusLogProbMetric: 27.7084 - val_loss: 28.3490 - val_MinusLogProbMetric: 28.3490 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 466/1000
2023-10-26 21:24:56.211 
Epoch 466/1000 
	 loss: 27.6862, MinusLogProbMetric: 27.6862, val_loss: 28.3898, val_MinusLogProbMetric: 28.3898

Epoch 466: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.6862 - MinusLogProbMetric: 27.6862 - val_loss: 28.3898 - val_MinusLogProbMetric: 28.3898 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 467/1000
2023-10-26 21:25:39.267 
Epoch 467/1000 
	 loss: 27.6698, MinusLogProbMetric: 27.6698, val_loss: 28.3716, val_MinusLogProbMetric: 28.3716

Epoch 467: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.6698 - MinusLogProbMetric: 27.6698 - val_loss: 28.3716 - val_MinusLogProbMetric: 28.3716 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 468/1000
2023-10-26 21:26:22.157 
Epoch 468/1000 
	 loss: 27.6787, MinusLogProbMetric: 27.6787, val_loss: 28.5077, val_MinusLogProbMetric: 28.5077

Epoch 468: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.6787 - MinusLogProbMetric: 27.6787 - val_loss: 28.5077 - val_MinusLogProbMetric: 28.5077 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 469/1000
2023-10-26 21:27:05.671 
Epoch 469/1000 
	 loss: 27.6856, MinusLogProbMetric: 27.6856, val_loss: 28.3360, val_MinusLogProbMetric: 28.3360

Epoch 469: val_loss did not improve from 28.32283
196/196 - 44s - loss: 27.6856 - MinusLogProbMetric: 27.6856 - val_loss: 28.3360 - val_MinusLogProbMetric: 28.3360 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 470/1000
2023-10-26 21:27:48.896 
Epoch 470/1000 
	 loss: 27.6706, MinusLogProbMetric: 27.6706, val_loss: 28.4086, val_MinusLogProbMetric: 28.4086

Epoch 470: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.6706 - MinusLogProbMetric: 27.6706 - val_loss: 28.4086 - val_MinusLogProbMetric: 28.4086 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 471/1000
2023-10-26 21:28:31.829 
Epoch 471/1000 
	 loss: 27.7056, MinusLogProbMetric: 27.7056, val_loss: 28.4428, val_MinusLogProbMetric: 28.4428

Epoch 471: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.7056 - MinusLogProbMetric: 27.7056 - val_loss: 28.4428 - val_MinusLogProbMetric: 28.4428 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 472/1000
2023-10-26 21:29:15.183 
Epoch 472/1000 
	 loss: 27.6896, MinusLogProbMetric: 27.6896, val_loss: 28.3450, val_MinusLogProbMetric: 28.3450

Epoch 472: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.6896 - MinusLogProbMetric: 27.6896 - val_loss: 28.3450 - val_MinusLogProbMetric: 28.3450 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 473/1000
2023-10-26 21:29:58.651 
Epoch 473/1000 
	 loss: 27.6783, MinusLogProbMetric: 27.6783, val_loss: 28.4729, val_MinusLogProbMetric: 28.4729

Epoch 473: val_loss did not improve from 28.32283
196/196 - 43s - loss: 27.6783 - MinusLogProbMetric: 27.6783 - val_loss: 28.4729 - val_MinusLogProbMetric: 28.4729 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 474/1000
2023-10-26 21:30:41.993 
Epoch 474/1000 
	 loss: 27.6636, MinusLogProbMetric: 27.6636, val_loss: 28.3187, val_MinusLogProbMetric: 28.3187

Epoch 474: val_loss improved from 28.32283 to 28.31868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.6636 - MinusLogProbMetric: 27.6636 - val_loss: 28.3187 - val_MinusLogProbMetric: 28.3187 - lr: 1.2500e-04 - 44s/epoch - 225ms/step
Epoch 475/1000
2023-10-26 21:31:26.396 
Epoch 475/1000 
	 loss: 27.6686, MinusLogProbMetric: 27.6686, val_loss: 28.4556, val_MinusLogProbMetric: 28.4556

Epoch 475: val_loss did not improve from 28.31868
196/196 - 44s - loss: 27.6686 - MinusLogProbMetric: 27.6686 - val_loss: 28.4556 - val_MinusLogProbMetric: 28.4556 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 476/1000
2023-10-26 21:32:10.077 
Epoch 476/1000 
	 loss: 27.6806, MinusLogProbMetric: 27.6806, val_loss: 28.3289, val_MinusLogProbMetric: 28.3289

Epoch 476: val_loss did not improve from 28.31868
196/196 - 44s - loss: 27.6806 - MinusLogProbMetric: 27.6806 - val_loss: 28.3289 - val_MinusLogProbMetric: 28.3289 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 477/1000
2023-10-26 21:32:53.950 
Epoch 477/1000 
	 loss: 27.6867, MinusLogProbMetric: 27.6867, val_loss: 28.4350, val_MinusLogProbMetric: 28.4350

Epoch 477: val_loss did not improve from 28.31868
196/196 - 44s - loss: 27.6867 - MinusLogProbMetric: 27.6867 - val_loss: 28.4350 - val_MinusLogProbMetric: 28.4350 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 478/1000
2023-10-26 21:33:37.258 
Epoch 478/1000 
	 loss: 27.7006, MinusLogProbMetric: 27.7006, val_loss: 28.3268, val_MinusLogProbMetric: 28.3268

Epoch 478: val_loss did not improve from 28.31868
196/196 - 43s - loss: 27.7006 - MinusLogProbMetric: 27.7006 - val_loss: 28.3268 - val_MinusLogProbMetric: 28.3268 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 479/1000
2023-10-26 21:34:20.548 
Epoch 479/1000 
	 loss: 27.6730, MinusLogProbMetric: 27.6730, val_loss: 28.3356, val_MinusLogProbMetric: 28.3356

Epoch 479: val_loss did not improve from 28.31868
196/196 - 43s - loss: 27.6730 - MinusLogProbMetric: 27.6730 - val_loss: 28.3356 - val_MinusLogProbMetric: 28.3356 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 480/1000
2023-10-26 21:35:04.017 
Epoch 480/1000 
	 loss: 27.6735, MinusLogProbMetric: 27.6735, val_loss: 28.3818, val_MinusLogProbMetric: 28.3818

Epoch 480: val_loss did not improve from 28.31868
196/196 - 43s - loss: 27.6735 - MinusLogProbMetric: 27.6735 - val_loss: 28.3818 - val_MinusLogProbMetric: 28.3818 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 481/1000
2023-10-26 21:35:47.432 
Epoch 481/1000 
	 loss: 27.6594, MinusLogProbMetric: 27.6594, val_loss: 28.3459, val_MinusLogProbMetric: 28.3459

Epoch 481: val_loss did not improve from 28.31868
196/196 - 43s - loss: 27.6594 - MinusLogProbMetric: 27.6594 - val_loss: 28.3459 - val_MinusLogProbMetric: 28.3459 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 482/1000
2023-10-26 21:36:30.670 
Epoch 482/1000 
	 loss: 27.6648, MinusLogProbMetric: 27.6648, val_loss: 28.3129, val_MinusLogProbMetric: 28.3129

Epoch 482: val_loss improved from 28.31868 to 28.31295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.6648 - MinusLogProbMetric: 27.6648 - val_loss: 28.3129 - val_MinusLogProbMetric: 28.3129 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 483/1000
2023-10-26 21:37:14.770 
Epoch 483/1000 
	 loss: 27.6711, MinusLogProbMetric: 27.6711, val_loss: 28.6325, val_MinusLogProbMetric: 28.6325

Epoch 483: val_loss did not improve from 28.31295
196/196 - 43s - loss: 27.6711 - MinusLogProbMetric: 27.6711 - val_loss: 28.6325 - val_MinusLogProbMetric: 28.6325 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 484/1000
2023-10-26 21:37:58.084 
Epoch 484/1000 
	 loss: 27.6814, MinusLogProbMetric: 27.6814, val_loss: 28.4374, val_MinusLogProbMetric: 28.4374

Epoch 484: val_loss did not improve from 28.31295
196/196 - 43s - loss: 27.6814 - MinusLogProbMetric: 27.6814 - val_loss: 28.4374 - val_MinusLogProbMetric: 28.4374 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 485/1000
2023-10-26 21:38:41.379 
Epoch 485/1000 
	 loss: 27.6757, MinusLogProbMetric: 27.6757, val_loss: 28.4163, val_MinusLogProbMetric: 28.4163

Epoch 485: val_loss did not improve from 28.31295
196/196 - 43s - loss: 27.6757 - MinusLogProbMetric: 27.6757 - val_loss: 28.4163 - val_MinusLogProbMetric: 28.4163 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 486/1000
2023-10-26 21:39:24.965 
Epoch 486/1000 
	 loss: 27.6700, MinusLogProbMetric: 27.6700, val_loss: 28.5254, val_MinusLogProbMetric: 28.5254

Epoch 486: val_loss did not improve from 28.31295
196/196 - 44s - loss: 27.6700 - MinusLogProbMetric: 27.6700 - val_loss: 28.5254 - val_MinusLogProbMetric: 28.5254 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 487/1000
2023-10-26 21:40:08.500 
Epoch 487/1000 
	 loss: 27.6619, MinusLogProbMetric: 27.6619, val_loss: 28.3649, val_MinusLogProbMetric: 28.3649

Epoch 487: val_loss did not improve from 28.31295
196/196 - 44s - loss: 27.6619 - MinusLogProbMetric: 27.6619 - val_loss: 28.3649 - val_MinusLogProbMetric: 28.3649 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 488/1000
2023-10-26 21:40:52.015 
Epoch 488/1000 
	 loss: 27.6773, MinusLogProbMetric: 27.6773, val_loss: 28.4829, val_MinusLogProbMetric: 28.4829

Epoch 488: val_loss did not improve from 28.31295
196/196 - 44s - loss: 27.6773 - MinusLogProbMetric: 27.6773 - val_loss: 28.4829 - val_MinusLogProbMetric: 28.4829 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 489/1000
2023-10-26 21:41:35.186 
Epoch 489/1000 
	 loss: 27.6729, MinusLogProbMetric: 27.6729, val_loss: 28.3925, val_MinusLogProbMetric: 28.3925

Epoch 489: val_loss did not improve from 28.31295
196/196 - 43s - loss: 27.6729 - MinusLogProbMetric: 27.6729 - val_loss: 28.3925 - val_MinusLogProbMetric: 28.3925 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 490/1000
2023-10-26 21:42:18.412 
Epoch 490/1000 
	 loss: 27.6873, MinusLogProbMetric: 27.6873, val_loss: 28.3479, val_MinusLogProbMetric: 28.3479

Epoch 490: val_loss did not improve from 28.31295
196/196 - 43s - loss: 27.6873 - MinusLogProbMetric: 27.6873 - val_loss: 28.3479 - val_MinusLogProbMetric: 28.3479 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 491/1000
2023-10-26 21:43:02.001 
Epoch 491/1000 
	 loss: 27.6684, MinusLogProbMetric: 27.6684, val_loss: 28.3512, val_MinusLogProbMetric: 28.3512

Epoch 491: val_loss did not improve from 28.31295
196/196 - 44s - loss: 27.6684 - MinusLogProbMetric: 27.6684 - val_loss: 28.3512 - val_MinusLogProbMetric: 28.3512 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 492/1000
2023-10-26 21:43:45.304 
Epoch 492/1000 
	 loss: 27.6997, MinusLogProbMetric: 27.6997, val_loss: 28.5014, val_MinusLogProbMetric: 28.5014

Epoch 492: val_loss did not improve from 28.31295
196/196 - 43s - loss: 27.6997 - MinusLogProbMetric: 27.6997 - val_loss: 28.5014 - val_MinusLogProbMetric: 28.5014 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 493/1000
2023-10-26 21:44:28.623 
Epoch 493/1000 
	 loss: 27.6688, MinusLogProbMetric: 27.6688, val_loss: 28.3170, val_MinusLogProbMetric: 28.3170

Epoch 493: val_loss did not improve from 28.31295
196/196 - 43s - loss: 27.6688 - MinusLogProbMetric: 27.6688 - val_loss: 28.3170 - val_MinusLogProbMetric: 28.3170 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 494/1000
2023-10-26 21:45:11.832 
Epoch 494/1000 
	 loss: 27.6790, MinusLogProbMetric: 27.6790, val_loss: 28.3023, val_MinusLogProbMetric: 28.3023

Epoch 494: val_loss improved from 28.31295 to 28.30231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.6790 - MinusLogProbMetric: 27.6790 - val_loss: 28.3023 - val_MinusLogProbMetric: 28.3023 - lr: 1.2500e-04 - 44s/epoch - 225ms/step
Epoch 495/1000
2023-10-26 21:45:56.016 
Epoch 495/1000 
	 loss: 27.6540, MinusLogProbMetric: 27.6540, val_loss: 28.3643, val_MinusLogProbMetric: 28.3643

Epoch 495: val_loss did not improve from 28.30231
196/196 - 43s - loss: 27.6540 - MinusLogProbMetric: 27.6540 - val_loss: 28.3643 - val_MinusLogProbMetric: 28.3643 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 496/1000
2023-10-26 21:46:39.390 
Epoch 496/1000 
	 loss: 27.6830, MinusLogProbMetric: 27.6830, val_loss: 28.4154, val_MinusLogProbMetric: 28.4154

Epoch 496: val_loss did not improve from 28.30231
196/196 - 43s - loss: 27.6830 - MinusLogProbMetric: 27.6830 - val_loss: 28.4154 - val_MinusLogProbMetric: 28.4154 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 497/1000
2023-10-26 21:47:22.607 
Epoch 497/1000 
	 loss: 27.6742, MinusLogProbMetric: 27.6742, val_loss: 28.3307, val_MinusLogProbMetric: 28.3307

Epoch 497: val_loss did not improve from 28.30231
196/196 - 43s - loss: 27.6742 - MinusLogProbMetric: 27.6742 - val_loss: 28.3307 - val_MinusLogProbMetric: 28.3307 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 498/1000
2023-10-26 21:48:05.859 
Epoch 498/1000 
	 loss: 27.6634, MinusLogProbMetric: 27.6634, val_loss: 28.3199, val_MinusLogProbMetric: 28.3199

Epoch 498: val_loss did not improve from 28.30231
196/196 - 43s - loss: 27.6634 - MinusLogProbMetric: 27.6634 - val_loss: 28.3199 - val_MinusLogProbMetric: 28.3199 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 499/1000
2023-10-26 21:48:49.325 
Epoch 499/1000 
	 loss: 27.6765, MinusLogProbMetric: 27.6765, val_loss: 28.3019, val_MinusLogProbMetric: 28.3019

Epoch 499: val_loss improved from 28.30231 to 28.30194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.6765 - MinusLogProbMetric: 27.6765 - val_loss: 28.3019 - val_MinusLogProbMetric: 28.3019 - lr: 1.2500e-04 - 44s/epoch - 225ms/step
Epoch 500/1000
2023-10-26 21:49:33.170 
Epoch 500/1000 
	 loss: 27.6633, MinusLogProbMetric: 27.6633, val_loss: 28.3965, val_MinusLogProbMetric: 28.3965

Epoch 500: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6633 - MinusLogProbMetric: 27.6633 - val_loss: 28.3965 - val_MinusLogProbMetric: 28.3965 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 501/1000
2023-10-26 21:50:16.068 
Epoch 501/1000 
	 loss: 27.6711, MinusLogProbMetric: 27.6711, val_loss: 28.3976, val_MinusLogProbMetric: 28.3976

Epoch 501: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6711 - MinusLogProbMetric: 27.6711 - val_loss: 28.3976 - val_MinusLogProbMetric: 28.3976 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 502/1000
2023-10-26 21:50:59.390 
Epoch 502/1000 
	 loss: 27.6645, MinusLogProbMetric: 27.6645, val_loss: 28.4048, val_MinusLogProbMetric: 28.4048

Epoch 502: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6645 - MinusLogProbMetric: 27.6645 - val_loss: 28.4048 - val_MinusLogProbMetric: 28.4048 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 503/1000
2023-10-26 21:51:42.662 
Epoch 503/1000 
	 loss: 27.6697, MinusLogProbMetric: 27.6697, val_loss: 28.4466, val_MinusLogProbMetric: 28.4466

Epoch 503: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6697 - MinusLogProbMetric: 27.6697 - val_loss: 28.4466 - val_MinusLogProbMetric: 28.4466 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 504/1000
2023-10-26 21:52:25.934 
Epoch 504/1000 
	 loss: 27.6590, MinusLogProbMetric: 27.6590, val_loss: 28.3541, val_MinusLogProbMetric: 28.3541

Epoch 504: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6590 - MinusLogProbMetric: 27.6590 - val_loss: 28.3541 - val_MinusLogProbMetric: 28.3541 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 505/1000
2023-10-26 21:53:09.361 
Epoch 505/1000 
	 loss: 27.6571, MinusLogProbMetric: 27.6571, val_loss: 28.3818, val_MinusLogProbMetric: 28.3818

Epoch 505: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6571 - MinusLogProbMetric: 27.6571 - val_loss: 28.3818 - val_MinusLogProbMetric: 28.3818 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 506/1000
2023-10-26 21:53:52.894 
Epoch 506/1000 
	 loss: 27.6622, MinusLogProbMetric: 27.6622, val_loss: 28.3602, val_MinusLogProbMetric: 28.3602

Epoch 506: val_loss did not improve from 28.30194
196/196 - 44s - loss: 27.6622 - MinusLogProbMetric: 27.6622 - val_loss: 28.3602 - val_MinusLogProbMetric: 28.3602 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 507/1000
2023-10-26 21:54:36.373 
Epoch 507/1000 
	 loss: 27.6666, MinusLogProbMetric: 27.6666, val_loss: 28.4541, val_MinusLogProbMetric: 28.4541

Epoch 507: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6666 - MinusLogProbMetric: 27.6666 - val_loss: 28.4541 - val_MinusLogProbMetric: 28.4541 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 508/1000
2023-10-26 21:55:19.611 
Epoch 508/1000 
	 loss: 27.6622, MinusLogProbMetric: 27.6622, val_loss: 28.3455, val_MinusLogProbMetric: 28.3455

Epoch 508: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6622 - MinusLogProbMetric: 27.6622 - val_loss: 28.3455 - val_MinusLogProbMetric: 28.3455 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 509/1000
2023-10-26 21:56:03.180 
Epoch 509/1000 
	 loss: 27.6816, MinusLogProbMetric: 27.6816, val_loss: 28.3651, val_MinusLogProbMetric: 28.3651

Epoch 509: val_loss did not improve from 28.30194
196/196 - 44s - loss: 27.6816 - MinusLogProbMetric: 27.6816 - val_loss: 28.3651 - val_MinusLogProbMetric: 28.3651 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 510/1000
2023-10-26 21:56:46.294 
Epoch 510/1000 
	 loss: 27.6505, MinusLogProbMetric: 27.6505, val_loss: 28.3992, val_MinusLogProbMetric: 28.3992

Epoch 510: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6505 - MinusLogProbMetric: 27.6505 - val_loss: 28.3992 - val_MinusLogProbMetric: 28.3992 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 511/1000
2023-10-26 21:57:30.216 
Epoch 511/1000 
	 loss: 27.6593, MinusLogProbMetric: 27.6593, val_loss: 28.4249, val_MinusLogProbMetric: 28.4249

Epoch 511: val_loss did not improve from 28.30194
196/196 - 44s - loss: 27.6593 - MinusLogProbMetric: 27.6593 - val_loss: 28.4249 - val_MinusLogProbMetric: 28.4249 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 512/1000
2023-10-26 21:58:07.663 
Epoch 512/1000 
	 loss: 27.6640, MinusLogProbMetric: 27.6640, val_loss: 28.3855, val_MinusLogProbMetric: 28.3855

Epoch 512: val_loss did not improve from 28.30194
196/196 - 37s - loss: 27.6640 - MinusLogProbMetric: 27.6640 - val_loss: 28.3855 - val_MinusLogProbMetric: 28.3855 - lr: 1.2500e-04 - 37s/epoch - 191ms/step
Epoch 513/1000
2023-10-26 21:58:44.972 
Epoch 513/1000 
	 loss: 27.6660, MinusLogProbMetric: 27.6660, val_loss: 28.4196, val_MinusLogProbMetric: 28.4196

Epoch 513: val_loss did not improve from 28.30194
196/196 - 37s - loss: 27.6660 - MinusLogProbMetric: 27.6660 - val_loss: 28.4196 - val_MinusLogProbMetric: 28.4196 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 514/1000
2023-10-26 21:59:26.145 
Epoch 514/1000 
	 loss: 27.6536, MinusLogProbMetric: 27.6536, val_loss: 28.3481, val_MinusLogProbMetric: 28.3481

Epoch 514: val_loss did not improve from 28.30194
196/196 - 41s - loss: 27.6536 - MinusLogProbMetric: 27.6536 - val_loss: 28.3481 - val_MinusLogProbMetric: 28.3481 - lr: 1.2500e-04 - 41s/epoch - 210ms/step
Epoch 515/1000
2023-10-26 22:00:09.147 
Epoch 515/1000 
	 loss: 27.6550, MinusLogProbMetric: 27.6550, val_loss: 28.4361, val_MinusLogProbMetric: 28.4361

Epoch 515: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6550 - MinusLogProbMetric: 27.6550 - val_loss: 28.4361 - val_MinusLogProbMetric: 28.4361 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 516/1000
2023-10-26 22:00:51.534 
Epoch 516/1000 
	 loss: 27.6521, MinusLogProbMetric: 27.6521, val_loss: 28.3292, val_MinusLogProbMetric: 28.3292

Epoch 516: val_loss did not improve from 28.30194
196/196 - 42s - loss: 27.6521 - MinusLogProbMetric: 27.6521 - val_loss: 28.3292 - val_MinusLogProbMetric: 28.3292 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 517/1000
2023-10-26 22:01:34.483 
Epoch 517/1000 
	 loss: 27.6761, MinusLogProbMetric: 27.6761, val_loss: 28.5905, val_MinusLogProbMetric: 28.5905

Epoch 517: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6761 - MinusLogProbMetric: 27.6761 - val_loss: 28.5905 - val_MinusLogProbMetric: 28.5905 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 518/1000
2023-10-26 22:02:17.474 
Epoch 518/1000 
	 loss: 27.6596, MinusLogProbMetric: 27.6596, val_loss: 28.4359, val_MinusLogProbMetric: 28.4359

Epoch 518: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6596 - MinusLogProbMetric: 27.6596 - val_loss: 28.4359 - val_MinusLogProbMetric: 28.4359 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 519/1000
2023-10-26 22:03:00.693 
Epoch 519/1000 
	 loss: 27.6462, MinusLogProbMetric: 27.6462, val_loss: 28.3967, val_MinusLogProbMetric: 28.3967

Epoch 519: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6462 - MinusLogProbMetric: 27.6462 - val_loss: 28.3967 - val_MinusLogProbMetric: 28.3967 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 520/1000
2023-10-26 22:03:43.478 
Epoch 520/1000 
	 loss: 27.6625, MinusLogProbMetric: 27.6625, val_loss: 28.3611, val_MinusLogProbMetric: 28.3611

Epoch 520: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6625 - MinusLogProbMetric: 27.6625 - val_loss: 28.3611 - val_MinusLogProbMetric: 28.3611 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 521/1000
2023-10-26 22:04:26.231 
Epoch 521/1000 
	 loss: 27.6580, MinusLogProbMetric: 27.6580, val_loss: 28.5100, val_MinusLogProbMetric: 28.5100

Epoch 521: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6580 - MinusLogProbMetric: 27.6580 - val_loss: 28.5100 - val_MinusLogProbMetric: 28.5100 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 522/1000
2023-10-26 22:05:09.118 
Epoch 522/1000 
	 loss: 27.6526, MinusLogProbMetric: 27.6526, val_loss: 28.3549, val_MinusLogProbMetric: 28.3549

Epoch 522: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6526 - MinusLogProbMetric: 27.6526 - val_loss: 28.3549 - val_MinusLogProbMetric: 28.3549 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 523/1000
2023-10-26 22:05:52.357 
Epoch 523/1000 
	 loss: 27.6698, MinusLogProbMetric: 27.6698, val_loss: 28.3235, val_MinusLogProbMetric: 28.3235

Epoch 523: val_loss did not improve from 28.30194
196/196 - 43s - loss: 27.6698 - MinusLogProbMetric: 27.6698 - val_loss: 28.3235 - val_MinusLogProbMetric: 28.3235 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 524/1000
2023-10-26 22:06:35.464 
Epoch 524/1000 
	 loss: 27.6528, MinusLogProbMetric: 27.6528, val_loss: 28.2873, val_MinusLogProbMetric: 28.2873

Epoch 524: val_loss improved from 28.30194 to 28.28731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.6528 - MinusLogProbMetric: 27.6528 - val_loss: 28.2873 - val_MinusLogProbMetric: 28.2873 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 525/1000
2023-10-26 22:07:19.063 
Epoch 525/1000 
	 loss: 27.6565, MinusLogProbMetric: 27.6565, val_loss: 28.3081, val_MinusLogProbMetric: 28.3081

Epoch 525: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6565 - MinusLogProbMetric: 27.6565 - val_loss: 28.3081 - val_MinusLogProbMetric: 28.3081 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 526/1000
2023-10-26 22:08:02.100 
Epoch 526/1000 
	 loss: 27.6637, MinusLogProbMetric: 27.6637, val_loss: 28.4690, val_MinusLogProbMetric: 28.4690

Epoch 526: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6637 - MinusLogProbMetric: 27.6637 - val_loss: 28.4690 - val_MinusLogProbMetric: 28.4690 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 527/1000
2023-10-26 22:08:45.466 
Epoch 527/1000 
	 loss: 27.6567, MinusLogProbMetric: 27.6567, val_loss: 28.3779, val_MinusLogProbMetric: 28.3779

Epoch 527: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6567 - MinusLogProbMetric: 27.6567 - val_loss: 28.3779 - val_MinusLogProbMetric: 28.3779 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 528/1000
2023-10-26 22:09:28.724 
Epoch 528/1000 
	 loss: 27.6652, MinusLogProbMetric: 27.6652, val_loss: 28.3757, val_MinusLogProbMetric: 28.3757

Epoch 528: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6652 - MinusLogProbMetric: 27.6652 - val_loss: 28.3757 - val_MinusLogProbMetric: 28.3757 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 529/1000
2023-10-26 22:10:12.253 
Epoch 529/1000 
	 loss: 27.6598, MinusLogProbMetric: 27.6598, val_loss: 28.4939, val_MinusLogProbMetric: 28.4939

Epoch 529: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6598 - MinusLogProbMetric: 27.6598 - val_loss: 28.4939 - val_MinusLogProbMetric: 28.4939 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 530/1000
2023-10-26 22:10:55.658 
Epoch 530/1000 
	 loss: 27.6595, MinusLogProbMetric: 27.6595, val_loss: 28.3225, val_MinusLogProbMetric: 28.3225

Epoch 530: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6595 - MinusLogProbMetric: 27.6595 - val_loss: 28.3225 - val_MinusLogProbMetric: 28.3225 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 531/1000
2023-10-26 22:11:39.070 
Epoch 531/1000 
	 loss: 27.6616, MinusLogProbMetric: 27.6616, val_loss: 28.3471, val_MinusLogProbMetric: 28.3471

Epoch 531: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6616 - MinusLogProbMetric: 27.6616 - val_loss: 28.3471 - val_MinusLogProbMetric: 28.3471 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 532/1000
2023-10-26 22:12:22.343 
Epoch 532/1000 
	 loss: 27.6456, MinusLogProbMetric: 27.6456, val_loss: 28.4201, val_MinusLogProbMetric: 28.4201

Epoch 532: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6456 - MinusLogProbMetric: 27.6456 - val_loss: 28.4201 - val_MinusLogProbMetric: 28.4201 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 533/1000
2023-10-26 22:13:05.770 
Epoch 533/1000 
	 loss: 27.6536, MinusLogProbMetric: 27.6536, val_loss: 28.3409, val_MinusLogProbMetric: 28.3409

Epoch 533: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6536 - MinusLogProbMetric: 27.6536 - val_loss: 28.3409 - val_MinusLogProbMetric: 28.3409 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 534/1000
2023-10-26 22:13:49.221 
Epoch 534/1000 
	 loss: 27.6525, MinusLogProbMetric: 27.6525, val_loss: 28.3860, val_MinusLogProbMetric: 28.3860

Epoch 534: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6525 - MinusLogProbMetric: 27.6525 - val_loss: 28.3860 - val_MinusLogProbMetric: 28.3860 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 535/1000
2023-10-26 22:14:32.593 
Epoch 535/1000 
	 loss: 27.6791, MinusLogProbMetric: 27.6791, val_loss: 28.3666, val_MinusLogProbMetric: 28.3666

Epoch 535: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6791 - MinusLogProbMetric: 27.6791 - val_loss: 28.3666 - val_MinusLogProbMetric: 28.3666 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 536/1000
2023-10-26 22:15:15.647 
Epoch 536/1000 
	 loss: 27.6583, MinusLogProbMetric: 27.6583, val_loss: 28.3642, val_MinusLogProbMetric: 28.3642

Epoch 536: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6583 - MinusLogProbMetric: 27.6583 - val_loss: 28.3642 - val_MinusLogProbMetric: 28.3642 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 537/1000
2023-10-26 22:15:58.747 
Epoch 537/1000 
	 loss: 27.6379, MinusLogProbMetric: 27.6379, val_loss: 28.4490, val_MinusLogProbMetric: 28.4490

Epoch 537: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6379 - MinusLogProbMetric: 27.6379 - val_loss: 28.4490 - val_MinusLogProbMetric: 28.4490 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 538/1000
2023-10-26 22:16:42.081 
Epoch 538/1000 
	 loss: 27.6628, MinusLogProbMetric: 27.6628, val_loss: 28.3632, val_MinusLogProbMetric: 28.3632

Epoch 538: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6628 - MinusLogProbMetric: 27.6628 - val_loss: 28.3632 - val_MinusLogProbMetric: 28.3632 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 539/1000
2023-10-26 22:17:25.251 
Epoch 539/1000 
	 loss: 27.6499, MinusLogProbMetric: 27.6499, val_loss: 28.3617, val_MinusLogProbMetric: 28.3617

Epoch 539: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6499 - MinusLogProbMetric: 27.6499 - val_loss: 28.3617 - val_MinusLogProbMetric: 28.3617 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 540/1000
2023-10-26 22:18:08.575 
Epoch 540/1000 
	 loss: 27.6659, MinusLogProbMetric: 27.6659, val_loss: 28.3112, val_MinusLogProbMetric: 28.3112

Epoch 540: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6659 - MinusLogProbMetric: 27.6659 - val_loss: 28.3112 - val_MinusLogProbMetric: 28.3112 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 541/1000
2023-10-26 22:18:51.979 
Epoch 541/1000 
	 loss: 27.6622, MinusLogProbMetric: 27.6622, val_loss: 28.4262, val_MinusLogProbMetric: 28.4262

Epoch 541: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6622 - MinusLogProbMetric: 27.6622 - val_loss: 28.4262 - val_MinusLogProbMetric: 28.4262 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 542/1000
2023-10-26 22:19:35.309 
Epoch 542/1000 
	 loss: 27.6598, MinusLogProbMetric: 27.6598, val_loss: 28.3374, val_MinusLogProbMetric: 28.3374

Epoch 542: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6598 - MinusLogProbMetric: 27.6598 - val_loss: 28.3374 - val_MinusLogProbMetric: 28.3374 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 543/1000
2023-10-26 22:20:18.479 
Epoch 543/1000 
	 loss: 27.6461, MinusLogProbMetric: 27.6461, val_loss: 28.3619, val_MinusLogProbMetric: 28.3619

Epoch 543: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6461 - MinusLogProbMetric: 27.6461 - val_loss: 28.3619 - val_MinusLogProbMetric: 28.3619 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 544/1000
2023-10-26 22:21:01.560 
Epoch 544/1000 
	 loss: 27.6508, MinusLogProbMetric: 27.6508, val_loss: 28.3542, val_MinusLogProbMetric: 28.3542

Epoch 544: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6508 - MinusLogProbMetric: 27.6508 - val_loss: 28.3542 - val_MinusLogProbMetric: 28.3542 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 545/1000
2023-10-26 22:21:44.754 
Epoch 545/1000 
	 loss: 27.6747, MinusLogProbMetric: 27.6747, val_loss: 28.3380, val_MinusLogProbMetric: 28.3380

Epoch 545: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6747 - MinusLogProbMetric: 27.6747 - val_loss: 28.3380 - val_MinusLogProbMetric: 28.3380 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 546/1000
2023-10-26 22:22:28.202 
Epoch 546/1000 
	 loss: 27.6463, MinusLogProbMetric: 27.6463, val_loss: 28.3081, val_MinusLogProbMetric: 28.3081

Epoch 546: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6463 - MinusLogProbMetric: 27.6463 - val_loss: 28.3081 - val_MinusLogProbMetric: 28.3081 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 547/1000
2023-10-26 22:23:11.313 
Epoch 547/1000 
	 loss: 27.6457, MinusLogProbMetric: 27.6457, val_loss: 28.3179, val_MinusLogProbMetric: 28.3179

Epoch 547: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6457 - MinusLogProbMetric: 27.6457 - val_loss: 28.3179 - val_MinusLogProbMetric: 28.3179 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 548/1000
2023-10-26 22:23:54.614 
Epoch 548/1000 
	 loss: 27.6481, MinusLogProbMetric: 27.6481, val_loss: 28.4614, val_MinusLogProbMetric: 28.4614

Epoch 548: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6481 - MinusLogProbMetric: 27.6481 - val_loss: 28.4614 - val_MinusLogProbMetric: 28.4614 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 549/1000
2023-10-26 22:24:37.997 
Epoch 549/1000 
	 loss: 27.6728, MinusLogProbMetric: 27.6728, val_loss: 28.3901, val_MinusLogProbMetric: 28.3901

Epoch 549: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6728 - MinusLogProbMetric: 27.6728 - val_loss: 28.3901 - val_MinusLogProbMetric: 28.3901 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 550/1000
2023-10-26 22:25:20.896 
Epoch 550/1000 
	 loss: 27.6605, MinusLogProbMetric: 27.6605, val_loss: 28.4325, val_MinusLogProbMetric: 28.4325

Epoch 550: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6605 - MinusLogProbMetric: 27.6605 - val_loss: 28.4325 - val_MinusLogProbMetric: 28.4325 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 551/1000
2023-10-26 22:26:04.366 
Epoch 551/1000 
	 loss: 27.6605, MinusLogProbMetric: 27.6605, val_loss: 28.3459, val_MinusLogProbMetric: 28.3459

Epoch 551: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6605 - MinusLogProbMetric: 27.6605 - val_loss: 28.3459 - val_MinusLogProbMetric: 28.3459 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 552/1000
2023-10-26 22:26:47.812 
Epoch 552/1000 
	 loss: 27.6651, MinusLogProbMetric: 27.6651, val_loss: 28.3632, val_MinusLogProbMetric: 28.3632

Epoch 552: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6651 - MinusLogProbMetric: 27.6651 - val_loss: 28.3632 - val_MinusLogProbMetric: 28.3632 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 553/1000
2023-10-26 22:27:30.644 
Epoch 553/1000 
	 loss: 27.6810, MinusLogProbMetric: 27.6810, val_loss: 28.3426, val_MinusLogProbMetric: 28.3426

Epoch 553: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6810 - MinusLogProbMetric: 27.6810 - val_loss: 28.3426 - val_MinusLogProbMetric: 28.3426 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 554/1000
2023-10-26 22:28:13.623 
Epoch 554/1000 
	 loss: 27.6655, MinusLogProbMetric: 27.6655, val_loss: 28.3504, val_MinusLogProbMetric: 28.3504

Epoch 554: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6655 - MinusLogProbMetric: 27.6655 - val_loss: 28.3504 - val_MinusLogProbMetric: 28.3504 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 555/1000
2023-10-26 22:28:56.390 
Epoch 555/1000 
	 loss: 27.6527, MinusLogProbMetric: 27.6527, val_loss: 28.3197, val_MinusLogProbMetric: 28.3197

Epoch 555: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6527 - MinusLogProbMetric: 27.6527 - val_loss: 28.3197 - val_MinusLogProbMetric: 28.3197 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 556/1000
2023-10-26 22:29:39.985 
Epoch 556/1000 
	 loss: 27.6533, MinusLogProbMetric: 27.6533, val_loss: 28.3672, val_MinusLogProbMetric: 28.3672

Epoch 556: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6533 - MinusLogProbMetric: 27.6533 - val_loss: 28.3672 - val_MinusLogProbMetric: 28.3672 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 557/1000
2023-10-26 22:30:23.565 
Epoch 557/1000 
	 loss: 27.6456, MinusLogProbMetric: 27.6456, val_loss: 28.2981, val_MinusLogProbMetric: 28.2981

Epoch 557: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6456 - MinusLogProbMetric: 27.6456 - val_loss: 28.2981 - val_MinusLogProbMetric: 28.2981 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 558/1000
2023-10-26 22:31:06.992 
Epoch 558/1000 
	 loss: 27.6613, MinusLogProbMetric: 27.6613, val_loss: 28.5238, val_MinusLogProbMetric: 28.5238

Epoch 558: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6613 - MinusLogProbMetric: 27.6613 - val_loss: 28.5238 - val_MinusLogProbMetric: 28.5238 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 559/1000
2023-10-26 22:31:50.426 
Epoch 559/1000 
	 loss: 27.6611, MinusLogProbMetric: 27.6611, val_loss: 28.4193, val_MinusLogProbMetric: 28.4193

Epoch 559: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6611 - MinusLogProbMetric: 27.6611 - val_loss: 28.4193 - val_MinusLogProbMetric: 28.4193 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 560/1000
2023-10-26 22:32:33.815 
Epoch 560/1000 
	 loss: 27.6593, MinusLogProbMetric: 27.6593, val_loss: 28.3370, val_MinusLogProbMetric: 28.3370

Epoch 560: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6593 - MinusLogProbMetric: 27.6593 - val_loss: 28.3370 - val_MinusLogProbMetric: 28.3370 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 561/1000
2023-10-26 22:33:17.454 
Epoch 561/1000 
	 loss: 27.6633, MinusLogProbMetric: 27.6633, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 561: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6633 - MinusLogProbMetric: 27.6633 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 562/1000
2023-10-26 22:34:00.640 
Epoch 562/1000 
	 loss: 27.6384, MinusLogProbMetric: 27.6384, val_loss: 28.3850, val_MinusLogProbMetric: 28.3850

Epoch 562: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6384 - MinusLogProbMetric: 27.6384 - val_loss: 28.3850 - val_MinusLogProbMetric: 28.3850 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 563/1000
2023-10-26 22:34:44.148 
Epoch 563/1000 
	 loss: 27.6525, MinusLogProbMetric: 27.6525, val_loss: 28.4863, val_MinusLogProbMetric: 28.4863

Epoch 563: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6525 - MinusLogProbMetric: 27.6525 - val_loss: 28.4863 - val_MinusLogProbMetric: 28.4863 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 564/1000
2023-10-26 22:35:27.618 
Epoch 564/1000 
	 loss: 27.6371, MinusLogProbMetric: 27.6371, val_loss: 28.4579, val_MinusLogProbMetric: 28.4579

Epoch 564: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6371 - MinusLogProbMetric: 27.6371 - val_loss: 28.4579 - val_MinusLogProbMetric: 28.4579 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 565/1000
2023-10-26 22:36:10.950 
Epoch 565/1000 
	 loss: 27.6505, MinusLogProbMetric: 27.6505, val_loss: 28.3231, val_MinusLogProbMetric: 28.3231

Epoch 565: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6505 - MinusLogProbMetric: 27.6505 - val_loss: 28.3231 - val_MinusLogProbMetric: 28.3231 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 566/1000
2023-10-26 22:36:54.402 
Epoch 566/1000 
	 loss: 27.6492, MinusLogProbMetric: 27.6492, val_loss: 28.3662, val_MinusLogProbMetric: 28.3662

Epoch 566: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6492 - MinusLogProbMetric: 27.6492 - val_loss: 28.3662 - val_MinusLogProbMetric: 28.3662 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 567/1000
2023-10-26 22:37:37.945 
Epoch 567/1000 
	 loss: 27.6490, MinusLogProbMetric: 27.6490, val_loss: 28.4130, val_MinusLogProbMetric: 28.4130

Epoch 567: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6490 - MinusLogProbMetric: 27.6490 - val_loss: 28.4130 - val_MinusLogProbMetric: 28.4130 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 568/1000
2023-10-26 22:38:21.157 
Epoch 568/1000 
	 loss: 27.6455, MinusLogProbMetric: 27.6455, val_loss: 28.3806, val_MinusLogProbMetric: 28.3806

Epoch 568: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6455 - MinusLogProbMetric: 27.6455 - val_loss: 28.3806 - val_MinusLogProbMetric: 28.3806 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 569/1000
2023-10-26 22:39:05.130 
Epoch 569/1000 
	 loss: 27.6440, MinusLogProbMetric: 27.6440, val_loss: 28.3479, val_MinusLogProbMetric: 28.3479

Epoch 569: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6440 - MinusLogProbMetric: 27.6440 - val_loss: 28.3479 - val_MinusLogProbMetric: 28.3479 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 570/1000
2023-10-26 22:39:48.550 
Epoch 570/1000 
	 loss: 27.6559, MinusLogProbMetric: 27.6559, val_loss: 28.4026, val_MinusLogProbMetric: 28.4026

Epoch 570: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6559 - MinusLogProbMetric: 27.6559 - val_loss: 28.4026 - val_MinusLogProbMetric: 28.4026 - lr: 1.2500e-04 - 43s/epoch - 222ms/step
Epoch 571/1000
2023-10-26 22:40:31.816 
Epoch 571/1000 
	 loss: 27.6582, MinusLogProbMetric: 27.6582, val_loss: 28.3649, val_MinusLogProbMetric: 28.3649

Epoch 571: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6582 - MinusLogProbMetric: 27.6582 - val_loss: 28.3649 - val_MinusLogProbMetric: 28.3649 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 572/1000
2023-10-26 22:41:15.345 
Epoch 572/1000 
	 loss: 27.6591, MinusLogProbMetric: 27.6591, val_loss: 28.4675, val_MinusLogProbMetric: 28.4675

Epoch 572: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6591 - MinusLogProbMetric: 27.6591 - val_loss: 28.4675 - val_MinusLogProbMetric: 28.4675 - lr: 1.2500e-04 - 44s/epoch - 222ms/step
Epoch 573/1000
2023-10-26 22:41:59.161 
Epoch 573/1000 
	 loss: 27.6399, MinusLogProbMetric: 27.6399, val_loss: 28.3970, val_MinusLogProbMetric: 28.3970

Epoch 573: val_loss did not improve from 28.28731
196/196 - 44s - loss: 27.6399 - MinusLogProbMetric: 27.6399 - val_loss: 28.3970 - val_MinusLogProbMetric: 28.3970 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 574/1000
2023-10-26 22:42:42.526 
Epoch 574/1000 
	 loss: 27.6471, MinusLogProbMetric: 27.6471, val_loss: 28.5277, val_MinusLogProbMetric: 28.5277

Epoch 574: val_loss did not improve from 28.28731
196/196 - 43s - loss: 27.6471 - MinusLogProbMetric: 27.6471 - val_loss: 28.5277 - val_MinusLogProbMetric: 28.5277 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 575/1000
2023-10-26 22:43:26.047 
Epoch 575/1000 
	 loss: 27.5724, MinusLogProbMetric: 27.5724, val_loss: 28.2744, val_MinusLogProbMetric: 28.2744

Epoch 575: val_loss improved from 28.28731 to 28.27437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.5724 - MinusLogProbMetric: 27.5724 - val_loss: 28.2744 - val_MinusLogProbMetric: 28.2744 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 576/1000
2023-10-26 22:44:10.107 
Epoch 576/1000 
	 loss: 27.5585, MinusLogProbMetric: 27.5585, val_loss: 28.2781, val_MinusLogProbMetric: 28.2781

Epoch 576: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5585 - MinusLogProbMetric: 27.5585 - val_loss: 28.2781 - val_MinusLogProbMetric: 28.2781 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 577/1000
2023-10-26 22:44:53.496 
Epoch 577/1000 
	 loss: 27.5707, MinusLogProbMetric: 27.5707, val_loss: 28.2839, val_MinusLogProbMetric: 28.2839

Epoch 577: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5707 - MinusLogProbMetric: 27.5707 - val_loss: 28.2839 - val_MinusLogProbMetric: 28.2839 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 578/1000
2023-10-26 22:45:36.437 
Epoch 578/1000 
	 loss: 27.5708, MinusLogProbMetric: 27.5708, val_loss: 28.3269, val_MinusLogProbMetric: 28.3269

Epoch 578: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5708 - MinusLogProbMetric: 27.5708 - val_loss: 28.3269 - val_MinusLogProbMetric: 28.3269 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 579/1000
2023-10-26 22:46:19.452 
Epoch 579/1000 
	 loss: 27.5674, MinusLogProbMetric: 27.5674, val_loss: 28.2952, val_MinusLogProbMetric: 28.2952

Epoch 579: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5674 - MinusLogProbMetric: 27.5674 - val_loss: 28.2952 - val_MinusLogProbMetric: 28.2952 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 580/1000
2023-10-26 22:47:03.049 
Epoch 580/1000 
	 loss: 27.5696, MinusLogProbMetric: 27.5696, val_loss: 28.2891, val_MinusLogProbMetric: 28.2891

Epoch 580: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5696 - MinusLogProbMetric: 27.5696 - val_loss: 28.2891 - val_MinusLogProbMetric: 28.2891 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 581/1000
2023-10-26 22:47:46.608 
Epoch 581/1000 
	 loss: 27.5687, MinusLogProbMetric: 27.5687, val_loss: 28.3230, val_MinusLogProbMetric: 28.3230

Epoch 581: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5687 - MinusLogProbMetric: 27.5687 - val_loss: 28.3230 - val_MinusLogProbMetric: 28.3230 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 582/1000
2023-10-26 22:48:29.689 
Epoch 582/1000 
	 loss: 27.5594, MinusLogProbMetric: 27.5594, val_loss: 28.3128, val_MinusLogProbMetric: 28.3128

Epoch 582: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5594 - MinusLogProbMetric: 27.5594 - val_loss: 28.3128 - val_MinusLogProbMetric: 28.3128 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 583/1000
2023-10-26 22:49:12.707 
Epoch 583/1000 
	 loss: 27.5610, MinusLogProbMetric: 27.5610, val_loss: 28.3036, val_MinusLogProbMetric: 28.3036

Epoch 583: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5610 - MinusLogProbMetric: 27.5610 - val_loss: 28.3036 - val_MinusLogProbMetric: 28.3036 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 584/1000
2023-10-26 22:49:55.968 
Epoch 584/1000 
	 loss: 27.5700, MinusLogProbMetric: 27.5700, val_loss: 28.3480, val_MinusLogProbMetric: 28.3480

Epoch 584: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5700 - MinusLogProbMetric: 27.5700 - val_loss: 28.3480 - val_MinusLogProbMetric: 28.3480 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 585/1000
2023-10-26 22:50:38.492 
Epoch 585/1000 
	 loss: 27.5654, MinusLogProbMetric: 27.5654, val_loss: 28.2859, val_MinusLogProbMetric: 28.2859

Epoch 585: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5654 - MinusLogProbMetric: 27.5654 - val_loss: 28.2859 - val_MinusLogProbMetric: 28.2859 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 586/1000
2023-10-26 22:51:21.846 
Epoch 586/1000 
	 loss: 27.5666, MinusLogProbMetric: 27.5666, val_loss: 28.2775, val_MinusLogProbMetric: 28.2775

Epoch 586: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5666 - MinusLogProbMetric: 27.5666 - val_loss: 28.2775 - val_MinusLogProbMetric: 28.2775 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 587/1000
2023-10-26 22:52:04.924 
Epoch 587/1000 
	 loss: 27.5694, MinusLogProbMetric: 27.5694, val_loss: 28.2856, val_MinusLogProbMetric: 28.2856

Epoch 587: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5694 - MinusLogProbMetric: 27.5694 - val_loss: 28.2856 - val_MinusLogProbMetric: 28.2856 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 588/1000
2023-10-26 22:52:48.106 
Epoch 588/1000 
	 loss: 27.5638, MinusLogProbMetric: 27.5638, val_loss: 28.3444, val_MinusLogProbMetric: 28.3444

Epoch 588: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5638 - MinusLogProbMetric: 27.5638 - val_loss: 28.3444 - val_MinusLogProbMetric: 28.3444 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 589/1000
2023-10-26 22:53:31.410 
Epoch 589/1000 
	 loss: 27.5724, MinusLogProbMetric: 27.5724, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 589: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5724 - MinusLogProbMetric: 27.5724 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 590/1000
2023-10-26 22:54:14.847 
Epoch 590/1000 
	 loss: 27.5588, MinusLogProbMetric: 27.5588, val_loss: 28.2754, val_MinusLogProbMetric: 28.2754

Epoch 590: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5588 - MinusLogProbMetric: 27.5588 - val_loss: 28.2754 - val_MinusLogProbMetric: 28.2754 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 591/1000
2023-10-26 22:54:57.732 
Epoch 591/1000 
	 loss: 27.5753, MinusLogProbMetric: 27.5753, val_loss: 28.3578, val_MinusLogProbMetric: 28.3578

Epoch 591: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5753 - MinusLogProbMetric: 27.5753 - val_loss: 28.3578 - val_MinusLogProbMetric: 28.3578 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 592/1000
2023-10-26 22:55:40.756 
Epoch 592/1000 
	 loss: 27.5632, MinusLogProbMetric: 27.5632, val_loss: 28.2786, val_MinusLogProbMetric: 28.2786

Epoch 592: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5632 - MinusLogProbMetric: 27.5632 - val_loss: 28.2786 - val_MinusLogProbMetric: 28.2786 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 593/1000
2023-10-26 22:56:23.876 
Epoch 593/1000 
	 loss: 27.5702, MinusLogProbMetric: 27.5702, val_loss: 28.3355, val_MinusLogProbMetric: 28.3355

Epoch 593: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5702 - MinusLogProbMetric: 27.5702 - val_loss: 28.3355 - val_MinusLogProbMetric: 28.3355 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 594/1000
2023-10-26 22:57:07.390 
Epoch 594/1000 
	 loss: 27.5651, MinusLogProbMetric: 27.5651, val_loss: 28.2780, val_MinusLogProbMetric: 28.2780

Epoch 594: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5651 - MinusLogProbMetric: 27.5651 - val_loss: 28.2780 - val_MinusLogProbMetric: 28.2780 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 595/1000
2023-10-26 22:57:50.649 
Epoch 595/1000 
	 loss: 27.5687, MinusLogProbMetric: 27.5687, val_loss: 28.3133, val_MinusLogProbMetric: 28.3133

Epoch 595: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5687 - MinusLogProbMetric: 27.5687 - val_loss: 28.3133 - val_MinusLogProbMetric: 28.3133 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 596/1000
2023-10-26 22:58:34.065 
Epoch 596/1000 
	 loss: 27.5698, MinusLogProbMetric: 27.5698, val_loss: 28.3173, val_MinusLogProbMetric: 28.3173

Epoch 596: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5698 - MinusLogProbMetric: 27.5698 - val_loss: 28.3173 - val_MinusLogProbMetric: 28.3173 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 597/1000
2023-10-26 22:59:17.500 
Epoch 597/1000 
	 loss: 27.5672, MinusLogProbMetric: 27.5672, val_loss: 28.3170, val_MinusLogProbMetric: 28.3170

Epoch 597: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5672 - MinusLogProbMetric: 27.5672 - val_loss: 28.3170 - val_MinusLogProbMetric: 28.3170 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 598/1000
2023-10-26 23:00:00.928 
Epoch 598/1000 
	 loss: 27.5775, MinusLogProbMetric: 27.5775, val_loss: 28.3325, val_MinusLogProbMetric: 28.3325

Epoch 598: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5775 - MinusLogProbMetric: 27.5775 - val_loss: 28.3325 - val_MinusLogProbMetric: 28.3325 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 599/1000
2023-10-26 23:00:44.576 
Epoch 599/1000 
	 loss: 27.5723, MinusLogProbMetric: 27.5723, val_loss: 28.3071, val_MinusLogProbMetric: 28.3071

Epoch 599: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5723 - MinusLogProbMetric: 27.5723 - val_loss: 28.3071 - val_MinusLogProbMetric: 28.3071 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 600/1000
2023-10-26 23:01:27.885 
Epoch 600/1000 
	 loss: 27.5605, MinusLogProbMetric: 27.5605, val_loss: 28.2873, val_MinusLogProbMetric: 28.2873

Epoch 600: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5605 - MinusLogProbMetric: 27.5605 - val_loss: 28.2873 - val_MinusLogProbMetric: 28.2873 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 601/1000
2023-10-26 23:02:11.099 
Epoch 601/1000 
	 loss: 27.5637, MinusLogProbMetric: 27.5637, val_loss: 28.3447, val_MinusLogProbMetric: 28.3447

Epoch 601: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5637 - MinusLogProbMetric: 27.5637 - val_loss: 28.3447 - val_MinusLogProbMetric: 28.3447 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 602/1000
2023-10-26 23:02:54.551 
Epoch 602/1000 
	 loss: 27.5669, MinusLogProbMetric: 27.5669, val_loss: 28.2942, val_MinusLogProbMetric: 28.2942

Epoch 602: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5669 - MinusLogProbMetric: 27.5669 - val_loss: 28.2942 - val_MinusLogProbMetric: 28.2942 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 603/1000
2023-10-26 23:03:38.116 
Epoch 603/1000 
	 loss: 27.5707, MinusLogProbMetric: 27.5707, val_loss: 28.2951, val_MinusLogProbMetric: 28.2951

Epoch 603: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5707 - MinusLogProbMetric: 27.5707 - val_loss: 28.2951 - val_MinusLogProbMetric: 28.2951 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 604/1000
2023-10-26 23:04:21.163 
Epoch 604/1000 
	 loss: 27.5699, MinusLogProbMetric: 27.5699, val_loss: 28.2983, val_MinusLogProbMetric: 28.2983

Epoch 604: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5699 - MinusLogProbMetric: 27.5699 - val_loss: 28.2983 - val_MinusLogProbMetric: 28.2983 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 605/1000
2023-10-26 23:05:04.677 
Epoch 605/1000 
	 loss: 27.5659, MinusLogProbMetric: 27.5659, val_loss: 28.3638, val_MinusLogProbMetric: 28.3638

Epoch 605: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5659 - MinusLogProbMetric: 27.5659 - val_loss: 28.3638 - val_MinusLogProbMetric: 28.3638 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 606/1000
2023-10-26 23:05:48.193 
Epoch 606/1000 
	 loss: 27.5659, MinusLogProbMetric: 27.5659, val_loss: 28.3337, val_MinusLogProbMetric: 28.3337

Epoch 606: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5659 - MinusLogProbMetric: 27.5659 - val_loss: 28.3337 - val_MinusLogProbMetric: 28.3337 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 607/1000
2023-10-26 23:06:31.790 
Epoch 607/1000 
	 loss: 27.5671, MinusLogProbMetric: 27.5671, val_loss: 28.3106, val_MinusLogProbMetric: 28.3106

Epoch 607: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5671 - MinusLogProbMetric: 27.5671 - val_loss: 28.3106 - val_MinusLogProbMetric: 28.3106 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 608/1000
2023-10-26 23:07:15.410 
Epoch 608/1000 
	 loss: 27.5571, MinusLogProbMetric: 27.5571, val_loss: 28.3326, val_MinusLogProbMetric: 28.3326

Epoch 608: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5571 - MinusLogProbMetric: 27.5571 - val_loss: 28.3326 - val_MinusLogProbMetric: 28.3326 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 609/1000
2023-10-26 23:07:58.693 
Epoch 609/1000 
	 loss: 27.5642, MinusLogProbMetric: 27.5642, val_loss: 28.3127, val_MinusLogProbMetric: 28.3127

Epoch 609: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5642 - MinusLogProbMetric: 27.5642 - val_loss: 28.3127 - val_MinusLogProbMetric: 28.3127 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 610/1000
2023-10-26 23:08:42.074 
Epoch 610/1000 
	 loss: 27.5696, MinusLogProbMetric: 27.5696, val_loss: 28.2929, val_MinusLogProbMetric: 28.2929

Epoch 610: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5696 - MinusLogProbMetric: 27.5696 - val_loss: 28.2929 - val_MinusLogProbMetric: 28.2929 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 611/1000
2023-10-26 23:09:24.876 
Epoch 611/1000 
	 loss: 27.5683, MinusLogProbMetric: 27.5683, val_loss: 28.3733, val_MinusLogProbMetric: 28.3733

Epoch 611: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5683 - MinusLogProbMetric: 27.5683 - val_loss: 28.3733 - val_MinusLogProbMetric: 28.3733 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 612/1000
2023-10-26 23:10:07.778 
Epoch 612/1000 
	 loss: 27.5718, MinusLogProbMetric: 27.5718, val_loss: 28.2765, val_MinusLogProbMetric: 28.2765

Epoch 612: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5718 - MinusLogProbMetric: 27.5718 - val_loss: 28.2765 - val_MinusLogProbMetric: 28.2765 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 613/1000
2023-10-26 23:10:50.947 
Epoch 613/1000 
	 loss: 27.5717, MinusLogProbMetric: 27.5717, val_loss: 28.3538, val_MinusLogProbMetric: 28.3538

Epoch 613: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5717 - MinusLogProbMetric: 27.5717 - val_loss: 28.3538 - val_MinusLogProbMetric: 28.3538 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 614/1000
2023-10-26 23:11:34.090 
Epoch 614/1000 
	 loss: 27.5580, MinusLogProbMetric: 27.5580, val_loss: 28.2876, val_MinusLogProbMetric: 28.2876

Epoch 614: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5580 - MinusLogProbMetric: 27.5580 - val_loss: 28.2876 - val_MinusLogProbMetric: 28.2876 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 615/1000
2023-10-26 23:12:17.496 
Epoch 615/1000 
	 loss: 27.5582, MinusLogProbMetric: 27.5582, val_loss: 28.2806, val_MinusLogProbMetric: 28.2806

Epoch 615: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5582 - MinusLogProbMetric: 27.5582 - val_loss: 28.2806 - val_MinusLogProbMetric: 28.2806 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 616/1000
2023-10-26 23:13:00.628 
Epoch 616/1000 
	 loss: 27.5647, MinusLogProbMetric: 27.5647, val_loss: 28.3449, val_MinusLogProbMetric: 28.3449

Epoch 616: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5647 - MinusLogProbMetric: 27.5647 - val_loss: 28.3449 - val_MinusLogProbMetric: 28.3449 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 617/1000
2023-10-26 23:13:44.132 
Epoch 617/1000 
	 loss: 27.5685, MinusLogProbMetric: 27.5685, val_loss: 28.2934, val_MinusLogProbMetric: 28.2934

Epoch 617: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5685 - MinusLogProbMetric: 27.5685 - val_loss: 28.2934 - val_MinusLogProbMetric: 28.2934 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 618/1000
2023-10-26 23:14:27.503 
Epoch 618/1000 
	 loss: 27.5575, MinusLogProbMetric: 27.5575, val_loss: 28.3343, val_MinusLogProbMetric: 28.3343

Epoch 618: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5575 - MinusLogProbMetric: 27.5575 - val_loss: 28.3343 - val_MinusLogProbMetric: 28.3343 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 619/1000
2023-10-26 23:15:11.198 
Epoch 619/1000 
	 loss: 27.5563, MinusLogProbMetric: 27.5563, val_loss: 28.3269, val_MinusLogProbMetric: 28.3269

Epoch 619: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5563 - MinusLogProbMetric: 27.5563 - val_loss: 28.3269 - val_MinusLogProbMetric: 28.3269 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 620/1000
2023-10-26 23:15:54.697 
Epoch 620/1000 
	 loss: 27.5589, MinusLogProbMetric: 27.5589, val_loss: 28.3392, val_MinusLogProbMetric: 28.3392

Epoch 620: val_loss did not improve from 28.27437
196/196 - 43s - loss: 27.5589 - MinusLogProbMetric: 27.5589 - val_loss: 28.3392 - val_MinusLogProbMetric: 28.3392 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 621/1000
2023-10-26 23:16:38.341 
Epoch 621/1000 
	 loss: 27.5562, MinusLogProbMetric: 27.5562, val_loss: 28.2785, val_MinusLogProbMetric: 28.2785

Epoch 621: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5562 - MinusLogProbMetric: 27.5562 - val_loss: 28.2785 - val_MinusLogProbMetric: 28.2785 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 622/1000
2023-10-26 23:17:21.903 
Epoch 622/1000 
	 loss: 27.5634, MinusLogProbMetric: 27.5634, val_loss: 28.3222, val_MinusLogProbMetric: 28.3222

Epoch 622: val_loss did not improve from 28.27437
196/196 - 44s - loss: 27.5634 - MinusLogProbMetric: 27.5634 - val_loss: 28.3222 - val_MinusLogProbMetric: 28.3222 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 623/1000
2023-10-26 23:18:05.235 
Epoch 623/1000 
	 loss: 27.5580, MinusLogProbMetric: 27.5580, val_loss: 28.2706, val_MinusLogProbMetric: 28.2706

Epoch 623: val_loss improved from 28.27437 to 28.27064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.5580 - MinusLogProbMetric: 27.5580 - val_loss: 28.2706 - val_MinusLogProbMetric: 28.2706 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 624/1000
2023-10-26 23:18:49.442 
Epoch 624/1000 
	 loss: 27.5585, MinusLogProbMetric: 27.5585, val_loss: 28.2871, val_MinusLogProbMetric: 28.2871

Epoch 624: val_loss did not improve from 28.27064
196/196 - 43s - loss: 27.5585 - MinusLogProbMetric: 27.5585 - val_loss: 28.2871 - val_MinusLogProbMetric: 28.2871 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 625/1000
2023-10-26 23:19:32.867 
Epoch 625/1000 
	 loss: 27.5585, MinusLogProbMetric: 27.5585, val_loss: 28.3431, val_MinusLogProbMetric: 28.3431

Epoch 625: val_loss did not improve from 28.27064
196/196 - 43s - loss: 27.5585 - MinusLogProbMetric: 27.5585 - val_loss: 28.3431 - val_MinusLogProbMetric: 28.3431 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 626/1000
2023-10-26 23:20:16.665 
Epoch 626/1000 
	 loss: 27.5591, MinusLogProbMetric: 27.5591, val_loss: 28.2895, val_MinusLogProbMetric: 28.2895

Epoch 626: val_loss did not improve from 28.27064
196/196 - 44s - loss: 27.5591 - MinusLogProbMetric: 27.5591 - val_loss: 28.2895 - val_MinusLogProbMetric: 28.2895 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 627/1000
2023-10-26 23:21:00.090 
Epoch 627/1000 
	 loss: 27.5632, MinusLogProbMetric: 27.5632, val_loss: 28.3581, val_MinusLogProbMetric: 28.3581

Epoch 627: val_loss did not improve from 28.27064
196/196 - 43s - loss: 27.5632 - MinusLogProbMetric: 27.5632 - val_loss: 28.3581 - val_MinusLogProbMetric: 28.3581 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 628/1000
2023-10-26 23:21:43.494 
Epoch 628/1000 
	 loss: 27.5555, MinusLogProbMetric: 27.5555, val_loss: 28.2846, val_MinusLogProbMetric: 28.2846

Epoch 628: val_loss did not improve from 28.27064
196/196 - 43s - loss: 27.5555 - MinusLogProbMetric: 27.5555 - val_loss: 28.2846 - val_MinusLogProbMetric: 28.2846 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 629/1000
2023-10-26 23:22:26.835 
Epoch 629/1000 
	 loss: 27.5664, MinusLogProbMetric: 27.5664, val_loss: 28.2978, val_MinusLogProbMetric: 28.2978

Epoch 629: val_loss did not improve from 28.27064
196/196 - 43s - loss: 27.5664 - MinusLogProbMetric: 27.5664 - val_loss: 28.2978 - val_MinusLogProbMetric: 28.2978 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 630/1000
2023-10-26 23:23:10.273 
Epoch 630/1000 
	 loss: 27.5639, MinusLogProbMetric: 27.5639, val_loss: 28.2909, val_MinusLogProbMetric: 28.2909

Epoch 630: val_loss did not improve from 28.27064
196/196 - 43s - loss: 27.5639 - MinusLogProbMetric: 27.5639 - val_loss: 28.2909 - val_MinusLogProbMetric: 28.2909 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 631/1000
2023-10-26 23:23:53.126 
Epoch 631/1000 
	 loss: 27.5629, MinusLogProbMetric: 27.5629, val_loss: 28.2925, val_MinusLogProbMetric: 28.2925

Epoch 631: val_loss did not improve from 28.27064
196/196 - 43s - loss: 27.5629 - MinusLogProbMetric: 27.5629 - val_loss: 28.2925 - val_MinusLogProbMetric: 28.2925 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 632/1000
2023-10-26 23:24:36.681 
Epoch 632/1000 
	 loss: 27.5555, MinusLogProbMetric: 27.5555, val_loss: 28.3090, val_MinusLogProbMetric: 28.3090

Epoch 632: val_loss did not improve from 28.27064
196/196 - 44s - loss: 27.5555 - MinusLogProbMetric: 27.5555 - val_loss: 28.3090 - val_MinusLogProbMetric: 28.3090 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 633/1000
2023-10-26 23:25:20.008 
Epoch 633/1000 
	 loss: 27.5562, MinusLogProbMetric: 27.5562, val_loss: 28.2648, val_MinusLogProbMetric: 28.2648

Epoch 633: val_loss improved from 28.27064 to 28.26481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.5562 - MinusLogProbMetric: 27.5562 - val_loss: 28.2648 - val_MinusLogProbMetric: 28.2648 - lr: 6.2500e-05 - 44s/epoch - 225ms/step
Epoch 634/1000
2023-10-26 23:26:04.128 
Epoch 634/1000 
	 loss: 27.5602, MinusLogProbMetric: 27.5602, val_loss: 28.2584, val_MinusLogProbMetric: 28.2584

Epoch 634: val_loss improved from 28.26481 to 28.25838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.5602 - MinusLogProbMetric: 27.5602 - val_loss: 28.2584 - val_MinusLogProbMetric: 28.2584 - lr: 6.2500e-05 - 44s/epoch - 226ms/step
Epoch 635/1000
2023-10-26 23:26:48.287 
Epoch 635/1000 
	 loss: 27.5521, MinusLogProbMetric: 27.5521, val_loss: 28.2671, val_MinusLogProbMetric: 28.2671

Epoch 635: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5521 - MinusLogProbMetric: 27.5521 - val_loss: 28.2671 - val_MinusLogProbMetric: 28.2671 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 636/1000
2023-10-26 23:27:31.616 
Epoch 636/1000 
	 loss: 27.5500, MinusLogProbMetric: 27.5500, val_loss: 28.2856, val_MinusLogProbMetric: 28.2856

Epoch 636: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5500 - MinusLogProbMetric: 27.5500 - val_loss: 28.2856 - val_MinusLogProbMetric: 28.2856 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 637/1000
2023-10-26 23:28:15.565 
Epoch 637/1000 
	 loss: 27.5541, MinusLogProbMetric: 27.5541, val_loss: 28.2841, val_MinusLogProbMetric: 28.2841

Epoch 637: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5541 - MinusLogProbMetric: 27.5541 - val_loss: 28.2841 - val_MinusLogProbMetric: 28.2841 - lr: 6.2500e-05 - 44s/epoch - 224ms/step
Epoch 638/1000
2023-10-26 23:28:59.315 
Epoch 638/1000 
	 loss: 27.5594, MinusLogProbMetric: 27.5594, val_loss: 28.2700, val_MinusLogProbMetric: 28.2700

Epoch 638: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5594 - MinusLogProbMetric: 27.5594 - val_loss: 28.2700 - val_MinusLogProbMetric: 28.2700 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 639/1000
2023-10-26 23:29:42.754 
Epoch 639/1000 
	 loss: 27.5546, MinusLogProbMetric: 27.5546, val_loss: 28.3439, val_MinusLogProbMetric: 28.3439

Epoch 639: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5546 - MinusLogProbMetric: 27.5546 - val_loss: 28.3439 - val_MinusLogProbMetric: 28.3439 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 640/1000
2023-10-26 23:30:26.091 
Epoch 640/1000 
	 loss: 27.5571, MinusLogProbMetric: 27.5571, val_loss: 28.2933, val_MinusLogProbMetric: 28.2933

Epoch 640: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5571 - MinusLogProbMetric: 27.5571 - val_loss: 28.2933 - val_MinusLogProbMetric: 28.2933 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 641/1000
2023-10-26 23:31:09.171 
Epoch 641/1000 
	 loss: 27.5504, MinusLogProbMetric: 27.5504, val_loss: 28.2737, val_MinusLogProbMetric: 28.2737

Epoch 641: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5504 - MinusLogProbMetric: 27.5504 - val_loss: 28.2737 - val_MinusLogProbMetric: 28.2737 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 642/1000
2023-10-26 23:31:52.337 
Epoch 642/1000 
	 loss: 27.5547, MinusLogProbMetric: 27.5547, val_loss: 28.3085, val_MinusLogProbMetric: 28.3085

Epoch 642: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5547 - MinusLogProbMetric: 27.5547 - val_loss: 28.3085 - val_MinusLogProbMetric: 28.3085 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 643/1000
2023-10-26 23:32:35.536 
Epoch 643/1000 
	 loss: 27.5465, MinusLogProbMetric: 27.5465, val_loss: 28.2893, val_MinusLogProbMetric: 28.2893

Epoch 643: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5465 - MinusLogProbMetric: 27.5465 - val_loss: 28.2893 - val_MinusLogProbMetric: 28.2893 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 644/1000
2023-10-26 23:33:18.486 
Epoch 644/1000 
	 loss: 27.5490, MinusLogProbMetric: 27.5490, val_loss: 28.2901, val_MinusLogProbMetric: 28.2901

Epoch 644: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5490 - MinusLogProbMetric: 27.5490 - val_loss: 28.2901 - val_MinusLogProbMetric: 28.2901 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 645/1000
2023-10-26 23:34:01.600 
Epoch 645/1000 
	 loss: 27.5583, MinusLogProbMetric: 27.5583, val_loss: 28.3205, val_MinusLogProbMetric: 28.3205

Epoch 645: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5583 - MinusLogProbMetric: 27.5583 - val_loss: 28.3205 - val_MinusLogProbMetric: 28.3205 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 646/1000
2023-10-26 23:34:44.842 
Epoch 646/1000 
	 loss: 27.5590, MinusLogProbMetric: 27.5590, val_loss: 28.3074, val_MinusLogProbMetric: 28.3074

Epoch 646: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5590 - MinusLogProbMetric: 27.5590 - val_loss: 28.3074 - val_MinusLogProbMetric: 28.3074 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 647/1000
2023-10-26 23:35:27.975 
Epoch 647/1000 
	 loss: 27.5571, MinusLogProbMetric: 27.5571, val_loss: 28.2623, val_MinusLogProbMetric: 28.2623

Epoch 647: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5571 - MinusLogProbMetric: 27.5571 - val_loss: 28.2623 - val_MinusLogProbMetric: 28.2623 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 648/1000
2023-10-26 23:36:11.089 
Epoch 648/1000 
	 loss: 27.5575, MinusLogProbMetric: 27.5575, val_loss: 28.3059, val_MinusLogProbMetric: 28.3059

Epoch 648: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5575 - MinusLogProbMetric: 27.5575 - val_loss: 28.3059 - val_MinusLogProbMetric: 28.3059 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 649/1000
2023-10-26 23:36:54.199 
Epoch 649/1000 
	 loss: 27.5530, MinusLogProbMetric: 27.5530, val_loss: 28.3170, val_MinusLogProbMetric: 28.3170

Epoch 649: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5530 - MinusLogProbMetric: 27.5530 - val_loss: 28.3170 - val_MinusLogProbMetric: 28.3170 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 650/1000
2023-10-26 23:37:37.466 
Epoch 650/1000 
	 loss: 27.5543, MinusLogProbMetric: 27.5543, val_loss: 28.3402, val_MinusLogProbMetric: 28.3402

Epoch 650: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5543 - MinusLogProbMetric: 27.5543 - val_loss: 28.3402 - val_MinusLogProbMetric: 28.3402 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 651/1000
2023-10-26 23:38:20.789 
Epoch 651/1000 
	 loss: 27.5629, MinusLogProbMetric: 27.5629, val_loss: 28.2831, val_MinusLogProbMetric: 28.2831

Epoch 651: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5629 - MinusLogProbMetric: 27.5629 - val_loss: 28.2831 - val_MinusLogProbMetric: 28.2831 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 652/1000
2023-10-26 23:39:04.428 
Epoch 652/1000 
	 loss: 27.5606, MinusLogProbMetric: 27.5606, val_loss: 28.2864, val_MinusLogProbMetric: 28.2864

Epoch 652: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5606 - MinusLogProbMetric: 27.5606 - val_loss: 28.2864 - val_MinusLogProbMetric: 28.2864 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 653/1000
2023-10-26 23:39:47.889 
Epoch 653/1000 
	 loss: 27.5536, MinusLogProbMetric: 27.5536, val_loss: 28.3336, val_MinusLogProbMetric: 28.3336

Epoch 653: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5536 - MinusLogProbMetric: 27.5536 - val_loss: 28.3336 - val_MinusLogProbMetric: 28.3336 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 654/1000
2023-10-26 23:40:31.336 
Epoch 654/1000 
	 loss: 27.5588, MinusLogProbMetric: 27.5588, val_loss: 28.2865, val_MinusLogProbMetric: 28.2865

Epoch 654: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5588 - MinusLogProbMetric: 27.5588 - val_loss: 28.2865 - val_MinusLogProbMetric: 28.2865 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 655/1000
2023-10-26 23:41:14.963 
Epoch 655/1000 
	 loss: 27.5512, MinusLogProbMetric: 27.5512, val_loss: 28.2903, val_MinusLogProbMetric: 28.2903

Epoch 655: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5512 - MinusLogProbMetric: 27.5512 - val_loss: 28.2903 - val_MinusLogProbMetric: 28.2903 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 656/1000
2023-10-26 23:41:58.820 
Epoch 656/1000 
	 loss: 27.5698, MinusLogProbMetric: 27.5698, val_loss: 28.3340, val_MinusLogProbMetric: 28.3340

Epoch 656: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5698 - MinusLogProbMetric: 27.5698 - val_loss: 28.3340 - val_MinusLogProbMetric: 28.3340 - lr: 6.2500e-05 - 44s/epoch - 224ms/step
Epoch 657/1000
2023-10-26 23:42:42.011 
Epoch 657/1000 
	 loss: 27.5613, MinusLogProbMetric: 27.5613, val_loss: 28.3330, val_MinusLogProbMetric: 28.3330

Epoch 657: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5613 - MinusLogProbMetric: 27.5613 - val_loss: 28.3330 - val_MinusLogProbMetric: 28.3330 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 658/1000
2023-10-26 23:43:25.311 
Epoch 658/1000 
	 loss: 27.5617, MinusLogProbMetric: 27.5617, val_loss: 28.2882, val_MinusLogProbMetric: 28.2882

Epoch 658: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5617 - MinusLogProbMetric: 27.5617 - val_loss: 28.2882 - val_MinusLogProbMetric: 28.2882 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 659/1000
2023-10-26 23:44:08.779 
Epoch 659/1000 
	 loss: 27.5567, MinusLogProbMetric: 27.5567, val_loss: 28.3127, val_MinusLogProbMetric: 28.3127

Epoch 659: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5567 - MinusLogProbMetric: 27.5567 - val_loss: 28.3127 - val_MinusLogProbMetric: 28.3127 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 660/1000
2023-10-26 23:44:52.511 
Epoch 660/1000 
	 loss: 27.5545, MinusLogProbMetric: 27.5545, val_loss: 28.3197, val_MinusLogProbMetric: 28.3197

Epoch 660: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5545 - MinusLogProbMetric: 27.5545 - val_loss: 28.3197 - val_MinusLogProbMetric: 28.3197 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 661/1000
2023-10-26 23:45:36.008 
Epoch 661/1000 
	 loss: 27.5573, MinusLogProbMetric: 27.5573, val_loss: 28.2622, val_MinusLogProbMetric: 28.2622

Epoch 661: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5573 - MinusLogProbMetric: 27.5573 - val_loss: 28.2622 - val_MinusLogProbMetric: 28.2622 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 662/1000
2023-10-26 23:46:18.724 
Epoch 662/1000 
	 loss: 27.5473, MinusLogProbMetric: 27.5473, val_loss: 28.2887, val_MinusLogProbMetric: 28.2887

Epoch 662: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5473 - MinusLogProbMetric: 27.5473 - val_loss: 28.2887 - val_MinusLogProbMetric: 28.2887 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 663/1000
2023-10-26 23:47:02.455 
Epoch 663/1000 
	 loss: 27.5597, MinusLogProbMetric: 27.5597, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 663: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5597 - MinusLogProbMetric: 27.5597 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 664/1000
2023-10-26 23:47:45.955 
Epoch 664/1000 
	 loss: 27.5424, MinusLogProbMetric: 27.5424, val_loss: 28.2622, val_MinusLogProbMetric: 28.2622

Epoch 664: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5424 - MinusLogProbMetric: 27.5424 - val_loss: 28.2622 - val_MinusLogProbMetric: 28.2622 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 665/1000
2023-10-26 23:48:29.347 
Epoch 665/1000 
	 loss: 27.5516, MinusLogProbMetric: 27.5516, val_loss: 28.3037, val_MinusLogProbMetric: 28.3037

Epoch 665: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5516 - MinusLogProbMetric: 27.5516 - val_loss: 28.3037 - val_MinusLogProbMetric: 28.3037 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 666/1000
2023-10-26 23:49:12.606 
Epoch 666/1000 
	 loss: 27.5488, MinusLogProbMetric: 27.5488, val_loss: 28.3757, val_MinusLogProbMetric: 28.3757

Epoch 666: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5488 - MinusLogProbMetric: 27.5488 - val_loss: 28.3757 - val_MinusLogProbMetric: 28.3757 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 667/1000
2023-10-26 23:49:55.564 
Epoch 667/1000 
	 loss: 27.5626, MinusLogProbMetric: 27.5626, val_loss: 28.3079, val_MinusLogProbMetric: 28.3079

Epoch 667: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5626 - MinusLogProbMetric: 27.5626 - val_loss: 28.3079 - val_MinusLogProbMetric: 28.3079 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 668/1000
2023-10-26 23:50:38.666 
Epoch 668/1000 
	 loss: 27.5420, MinusLogProbMetric: 27.5420, val_loss: 28.2809, val_MinusLogProbMetric: 28.2809

Epoch 668: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5420 - MinusLogProbMetric: 27.5420 - val_loss: 28.2809 - val_MinusLogProbMetric: 28.2809 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 669/1000
2023-10-26 23:51:22.177 
Epoch 669/1000 
	 loss: 27.5477, MinusLogProbMetric: 27.5477, val_loss: 28.2772, val_MinusLogProbMetric: 28.2772

Epoch 669: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5477 - MinusLogProbMetric: 27.5477 - val_loss: 28.2772 - val_MinusLogProbMetric: 28.2772 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 670/1000
2023-10-26 23:52:05.351 
Epoch 670/1000 
	 loss: 27.5477, MinusLogProbMetric: 27.5477, val_loss: 28.2699, val_MinusLogProbMetric: 28.2699

Epoch 670: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5477 - MinusLogProbMetric: 27.5477 - val_loss: 28.2699 - val_MinusLogProbMetric: 28.2699 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 671/1000
2023-10-26 23:52:48.800 
Epoch 671/1000 
	 loss: 27.5434, MinusLogProbMetric: 27.5434, val_loss: 28.3028, val_MinusLogProbMetric: 28.3028

Epoch 671: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5434 - MinusLogProbMetric: 27.5434 - val_loss: 28.3028 - val_MinusLogProbMetric: 28.3028 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 672/1000
2023-10-26 23:53:31.787 
Epoch 672/1000 
	 loss: 27.5456, MinusLogProbMetric: 27.5456, val_loss: 28.2825, val_MinusLogProbMetric: 28.2825

Epoch 672: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5456 - MinusLogProbMetric: 27.5456 - val_loss: 28.2825 - val_MinusLogProbMetric: 28.2825 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 673/1000
2023-10-26 23:54:15.328 
Epoch 673/1000 
	 loss: 27.5432, MinusLogProbMetric: 27.5432, val_loss: 28.2656, val_MinusLogProbMetric: 28.2656

Epoch 673: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5432 - MinusLogProbMetric: 27.5432 - val_loss: 28.2656 - val_MinusLogProbMetric: 28.2656 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 674/1000
2023-10-26 23:54:59.012 
Epoch 674/1000 
	 loss: 27.5540, MinusLogProbMetric: 27.5540, val_loss: 28.2691, val_MinusLogProbMetric: 28.2691

Epoch 674: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5540 - MinusLogProbMetric: 27.5540 - val_loss: 28.2691 - val_MinusLogProbMetric: 28.2691 - lr: 6.2500e-05 - 44s/epoch - 223ms/step
Epoch 675/1000
2023-10-26 23:55:42.553 
Epoch 675/1000 
	 loss: 27.5327, MinusLogProbMetric: 27.5327, val_loss: 28.3679, val_MinusLogProbMetric: 28.3679

Epoch 675: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5327 - MinusLogProbMetric: 27.5327 - val_loss: 28.3679 - val_MinusLogProbMetric: 28.3679 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 676/1000
2023-10-26 23:56:25.928 
Epoch 676/1000 
	 loss: 27.5509, MinusLogProbMetric: 27.5509, val_loss: 28.3339, val_MinusLogProbMetric: 28.3339

Epoch 676: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5509 - MinusLogProbMetric: 27.5509 - val_loss: 28.3339 - val_MinusLogProbMetric: 28.3339 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 677/1000
2023-10-26 23:57:09.408 
Epoch 677/1000 
	 loss: 27.5502, MinusLogProbMetric: 27.5502, val_loss: 28.3074, val_MinusLogProbMetric: 28.3074

Epoch 677: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5502 - MinusLogProbMetric: 27.5502 - val_loss: 28.3074 - val_MinusLogProbMetric: 28.3074 - lr: 6.2500e-05 - 43s/epoch - 222ms/step
Epoch 678/1000
2023-10-26 23:57:52.522 
Epoch 678/1000 
	 loss: 27.5414, MinusLogProbMetric: 27.5414, val_loss: 28.2660, val_MinusLogProbMetric: 28.2660

Epoch 678: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5414 - MinusLogProbMetric: 27.5414 - val_loss: 28.2660 - val_MinusLogProbMetric: 28.2660 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 679/1000
2023-10-26 23:58:35.513 
Epoch 679/1000 
	 loss: 27.5418, MinusLogProbMetric: 27.5418, val_loss: 28.4147, val_MinusLogProbMetric: 28.4147

Epoch 679: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5418 - MinusLogProbMetric: 27.5418 - val_loss: 28.4147 - val_MinusLogProbMetric: 28.4147 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 680/1000
2023-10-26 23:59:18.622 
Epoch 680/1000 
	 loss: 27.5399, MinusLogProbMetric: 27.5399, val_loss: 28.2890, val_MinusLogProbMetric: 28.2890

Epoch 680: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5399 - MinusLogProbMetric: 27.5399 - val_loss: 28.2890 - val_MinusLogProbMetric: 28.2890 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 681/1000
2023-10-27 00:00:01.713 
Epoch 681/1000 
	 loss: 27.5398, MinusLogProbMetric: 27.5398, val_loss: 28.2991, val_MinusLogProbMetric: 28.2991

Epoch 681: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5398 - MinusLogProbMetric: 27.5398 - val_loss: 28.2991 - val_MinusLogProbMetric: 28.2991 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 682/1000
2023-10-27 00:00:44.998 
Epoch 682/1000 
	 loss: 27.5493, MinusLogProbMetric: 27.5493, val_loss: 28.3512, val_MinusLogProbMetric: 28.3512

Epoch 682: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5493 - MinusLogProbMetric: 27.5493 - val_loss: 28.3512 - val_MinusLogProbMetric: 28.3512 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 683/1000
2023-10-27 00:01:28.222 
Epoch 683/1000 
	 loss: 27.5405, MinusLogProbMetric: 27.5405, val_loss: 28.2873, val_MinusLogProbMetric: 28.2873

Epoch 683: val_loss did not improve from 28.25838
196/196 - 43s - loss: 27.5405 - MinusLogProbMetric: 27.5405 - val_loss: 28.2873 - val_MinusLogProbMetric: 28.2873 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 684/1000
2023-10-27 00:02:12.111 
Epoch 684/1000 
	 loss: 27.5373, MinusLogProbMetric: 27.5373, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 684: val_loss did not improve from 28.25838
196/196 - 44s - loss: 27.5373 - MinusLogProbMetric: 27.5373 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 6.2500e-05 - 44s/epoch - 224ms/step
Epoch 685/1000
2023-10-27 00:02:55.458 
Epoch 685/1000 
	 loss: 27.5115, MinusLogProbMetric: 27.5115, val_loss: 28.2375, val_MinusLogProbMetric: 28.2375

Epoch 685: val_loss improved from 28.25838 to 28.23754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.5115 - MinusLogProbMetric: 27.5115 - val_loss: 28.2375 - val_MinusLogProbMetric: 28.2375 - lr: 3.1250e-05 - 44s/epoch - 225ms/step
Epoch 686/1000
2023-10-27 00:03:39.486 
Epoch 686/1000 
	 loss: 27.5097, MinusLogProbMetric: 27.5097, val_loss: 28.2666, val_MinusLogProbMetric: 28.2666

Epoch 686: val_loss did not improve from 28.23754
196/196 - 43s - loss: 27.5097 - MinusLogProbMetric: 27.5097 - val_loss: 28.2666 - val_MinusLogProbMetric: 28.2666 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 687/1000
2023-10-27 00:04:23.006 
Epoch 687/1000 
	 loss: 27.5112, MinusLogProbMetric: 27.5112, val_loss: 28.2546, val_MinusLogProbMetric: 28.2546

Epoch 687: val_loss did not improve from 28.23754
196/196 - 44s - loss: 27.5112 - MinusLogProbMetric: 27.5112 - val_loss: 28.2546 - val_MinusLogProbMetric: 28.2546 - lr: 3.1250e-05 - 44s/epoch - 222ms/step
Epoch 688/1000
2023-10-27 00:05:06.288 
Epoch 688/1000 
	 loss: 27.5081, MinusLogProbMetric: 27.5081, val_loss: 28.2403, val_MinusLogProbMetric: 28.2403

Epoch 688: val_loss did not improve from 28.23754
196/196 - 43s - loss: 27.5081 - MinusLogProbMetric: 27.5081 - val_loss: 28.2403 - val_MinusLogProbMetric: 28.2403 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 689/1000
2023-10-27 00:05:49.998 
Epoch 689/1000 
	 loss: 27.5130, MinusLogProbMetric: 27.5130, val_loss: 28.2630, val_MinusLogProbMetric: 28.2630

Epoch 689: val_loss did not improve from 28.23754
196/196 - 44s - loss: 27.5130 - MinusLogProbMetric: 27.5130 - val_loss: 28.2630 - val_MinusLogProbMetric: 28.2630 - lr: 3.1250e-05 - 44s/epoch - 223ms/step
Epoch 690/1000
2023-10-27 00:06:32.954 
Epoch 690/1000 
	 loss: 27.5075, MinusLogProbMetric: 27.5075, val_loss: 28.2461, val_MinusLogProbMetric: 28.2461

Epoch 690: val_loss did not improve from 28.23754
196/196 - 43s - loss: 27.5075 - MinusLogProbMetric: 27.5075 - val_loss: 28.2461 - val_MinusLogProbMetric: 28.2461 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 691/1000
2023-10-27 00:07:15.911 
Epoch 691/1000 
	 loss: 27.5065, MinusLogProbMetric: 27.5065, val_loss: 28.2543, val_MinusLogProbMetric: 28.2543

Epoch 691: val_loss did not improve from 28.23754
196/196 - 43s - loss: 27.5065 - MinusLogProbMetric: 27.5065 - val_loss: 28.2543 - val_MinusLogProbMetric: 28.2543 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 692/1000
2023-10-27 00:07:59.772 
Epoch 692/1000 
	 loss: 27.5095, MinusLogProbMetric: 27.5095, val_loss: 28.2502, val_MinusLogProbMetric: 28.2502

Epoch 692: val_loss did not improve from 28.23754
196/196 - 44s - loss: 27.5095 - MinusLogProbMetric: 27.5095 - val_loss: 28.2502 - val_MinusLogProbMetric: 28.2502 - lr: 3.1250e-05 - 44s/epoch - 224ms/step
Epoch 693/1000
2023-10-27 00:08:43.458 
Epoch 693/1000 
	 loss: 27.5121, MinusLogProbMetric: 27.5121, val_loss: 28.2537, val_MinusLogProbMetric: 28.2537

Epoch 693: val_loss did not improve from 28.23754
196/196 - 44s - loss: 27.5121 - MinusLogProbMetric: 27.5121 - val_loss: 28.2537 - val_MinusLogProbMetric: 28.2537 - lr: 3.1250e-05 - 44s/epoch - 223ms/step
Epoch 694/1000
2023-10-27 00:09:27.051 
Epoch 694/1000 
	 loss: 27.5065, MinusLogProbMetric: 27.5065, val_loss: 28.2362, val_MinusLogProbMetric: 28.2362

Epoch 694: val_loss improved from 28.23754 to 28.23625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.5065 - MinusLogProbMetric: 27.5065 - val_loss: 28.2362 - val_MinusLogProbMetric: 28.2362 - lr: 3.1250e-05 - 44s/epoch - 226ms/step
Epoch 695/1000
2023-10-27 00:10:11.249 
Epoch 695/1000 
	 loss: 27.5080, MinusLogProbMetric: 27.5080, val_loss: 28.2604, val_MinusLogProbMetric: 28.2604

Epoch 695: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5080 - MinusLogProbMetric: 27.5080 - val_loss: 28.2604 - val_MinusLogProbMetric: 28.2604 - lr: 3.1250e-05 - 43s/epoch - 222ms/step
Epoch 696/1000
2023-10-27 00:10:54.782 
Epoch 696/1000 
	 loss: 27.5130, MinusLogProbMetric: 27.5130, val_loss: 28.2512, val_MinusLogProbMetric: 28.2512

Epoch 696: val_loss did not improve from 28.23625
196/196 - 44s - loss: 27.5130 - MinusLogProbMetric: 27.5130 - val_loss: 28.2512 - val_MinusLogProbMetric: 28.2512 - lr: 3.1250e-05 - 44s/epoch - 222ms/step
Epoch 697/1000
2023-10-27 00:11:38.307 
Epoch 697/1000 
	 loss: 27.5082, MinusLogProbMetric: 27.5082, val_loss: 28.2603, val_MinusLogProbMetric: 28.2603

Epoch 697: val_loss did not improve from 28.23625
196/196 - 44s - loss: 27.5082 - MinusLogProbMetric: 27.5082 - val_loss: 28.2603 - val_MinusLogProbMetric: 28.2603 - lr: 3.1250e-05 - 44s/epoch - 222ms/step
Epoch 698/1000
2023-10-27 00:12:21.396 
Epoch 698/1000 
	 loss: 27.5073, MinusLogProbMetric: 27.5073, val_loss: 28.2579, val_MinusLogProbMetric: 28.2579

Epoch 698: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5073 - MinusLogProbMetric: 27.5073 - val_loss: 28.2579 - val_MinusLogProbMetric: 28.2579 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 699/1000
2023-10-27 00:13:04.823 
Epoch 699/1000 
	 loss: 27.5067, MinusLogProbMetric: 27.5067, val_loss: 28.2515, val_MinusLogProbMetric: 28.2515

Epoch 699: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5067 - MinusLogProbMetric: 27.5067 - val_loss: 28.2515 - val_MinusLogProbMetric: 28.2515 - lr: 3.1250e-05 - 43s/epoch - 222ms/step
Epoch 700/1000
2023-10-27 00:13:47.877 
Epoch 700/1000 
	 loss: 27.5107, MinusLogProbMetric: 27.5107, val_loss: 28.2445, val_MinusLogProbMetric: 28.2445

Epoch 700: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5107 - MinusLogProbMetric: 27.5107 - val_loss: 28.2445 - val_MinusLogProbMetric: 28.2445 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 701/1000
2023-10-27 00:14:31.019 
Epoch 701/1000 
	 loss: 27.5081, MinusLogProbMetric: 27.5081, val_loss: 28.2715, val_MinusLogProbMetric: 28.2715

Epoch 701: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5081 - MinusLogProbMetric: 27.5081 - val_loss: 28.2715 - val_MinusLogProbMetric: 28.2715 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 702/1000
2023-10-27 00:15:08.945 
Epoch 702/1000 
	 loss: 27.5129, MinusLogProbMetric: 27.5129, val_loss: 28.2459, val_MinusLogProbMetric: 28.2459

Epoch 702: val_loss did not improve from 28.23625
196/196 - 38s - loss: 27.5129 - MinusLogProbMetric: 27.5129 - val_loss: 28.2459 - val_MinusLogProbMetric: 28.2459 - lr: 3.1250e-05 - 38s/epoch - 193ms/step
Epoch 703/1000
2023-10-27 00:15:51.203 
Epoch 703/1000 
	 loss: 27.5102, MinusLogProbMetric: 27.5102, val_loss: 28.2598, val_MinusLogProbMetric: 28.2598

Epoch 703: val_loss did not improve from 28.23625
196/196 - 42s - loss: 27.5102 - MinusLogProbMetric: 27.5102 - val_loss: 28.2598 - val_MinusLogProbMetric: 28.2598 - lr: 3.1250e-05 - 42s/epoch - 216ms/step
Epoch 704/1000
2023-10-27 00:16:34.087 
Epoch 704/1000 
	 loss: 27.5126, MinusLogProbMetric: 27.5126, val_loss: 28.2561, val_MinusLogProbMetric: 28.2561

Epoch 704: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5126 - MinusLogProbMetric: 27.5126 - val_loss: 28.2561 - val_MinusLogProbMetric: 28.2561 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 705/1000
2023-10-27 00:17:16.933 
Epoch 705/1000 
	 loss: 27.5083, MinusLogProbMetric: 27.5083, val_loss: 28.2373, val_MinusLogProbMetric: 28.2373

Epoch 705: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5083 - MinusLogProbMetric: 27.5083 - val_loss: 28.2373 - val_MinusLogProbMetric: 28.2373 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 706/1000
2023-10-27 00:17:59.914 
Epoch 706/1000 
	 loss: 27.5074, MinusLogProbMetric: 27.5074, val_loss: 28.2741, val_MinusLogProbMetric: 28.2741

Epoch 706: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5074 - MinusLogProbMetric: 27.5074 - val_loss: 28.2741 - val_MinusLogProbMetric: 28.2741 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 707/1000
2023-10-27 00:18:42.792 
Epoch 707/1000 
	 loss: 27.5075, MinusLogProbMetric: 27.5075, val_loss: 28.2583, val_MinusLogProbMetric: 28.2583

Epoch 707: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5075 - MinusLogProbMetric: 27.5075 - val_loss: 28.2583 - val_MinusLogProbMetric: 28.2583 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 708/1000
2023-10-27 00:19:25.733 
Epoch 708/1000 
	 loss: 27.5111, MinusLogProbMetric: 27.5111, val_loss: 28.2575, val_MinusLogProbMetric: 28.2575

Epoch 708: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5111 - MinusLogProbMetric: 27.5111 - val_loss: 28.2575 - val_MinusLogProbMetric: 28.2575 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 709/1000
2023-10-27 00:20:08.644 
Epoch 709/1000 
	 loss: 27.5087, MinusLogProbMetric: 27.5087, val_loss: 28.2478, val_MinusLogProbMetric: 28.2478

Epoch 709: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5087 - MinusLogProbMetric: 27.5087 - val_loss: 28.2478 - val_MinusLogProbMetric: 28.2478 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 710/1000
2023-10-27 00:20:51.991 
Epoch 710/1000 
	 loss: 27.5091, MinusLogProbMetric: 27.5091, val_loss: 28.2565, val_MinusLogProbMetric: 28.2565

Epoch 710: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5091 - MinusLogProbMetric: 27.5091 - val_loss: 28.2565 - val_MinusLogProbMetric: 28.2565 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 711/1000
2023-10-27 00:21:35.559 
Epoch 711/1000 
	 loss: 27.5061, MinusLogProbMetric: 27.5061, val_loss: 28.2436, val_MinusLogProbMetric: 28.2436

Epoch 711: val_loss did not improve from 28.23625
196/196 - 44s - loss: 27.5061 - MinusLogProbMetric: 27.5061 - val_loss: 28.2436 - val_MinusLogProbMetric: 28.2436 - lr: 3.1250e-05 - 44s/epoch - 222ms/step
Epoch 712/1000
2023-10-27 00:22:18.907 
Epoch 712/1000 
	 loss: 27.5111, MinusLogProbMetric: 27.5111, val_loss: 28.2529, val_MinusLogProbMetric: 28.2529

Epoch 712: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5111 - MinusLogProbMetric: 27.5111 - val_loss: 28.2529 - val_MinusLogProbMetric: 28.2529 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 713/1000
2023-10-27 00:23:01.956 
Epoch 713/1000 
	 loss: 27.5103, MinusLogProbMetric: 27.5103, val_loss: 28.2499, val_MinusLogProbMetric: 28.2499

Epoch 713: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5103 - MinusLogProbMetric: 27.5103 - val_loss: 28.2499 - val_MinusLogProbMetric: 28.2499 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 714/1000
2023-10-27 00:23:45.470 
Epoch 714/1000 
	 loss: 27.5071, MinusLogProbMetric: 27.5071, val_loss: 28.2700, val_MinusLogProbMetric: 28.2700

Epoch 714: val_loss did not improve from 28.23625
196/196 - 44s - loss: 27.5071 - MinusLogProbMetric: 27.5071 - val_loss: 28.2700 - val_MinusLogProbMetric: 28.2700 - lr: 3.1250e-05 - 44s/epoch - 222ms/step
Epoch 715/1000
2023-10-27 00:24:28.076 
Epoch 715/1000 
	 loss: 27.5085, MinusLogProbMetric: 27.5085, val_loss: 28.2617, val_MinusLogProbMetric: 28.2617

Epoch 715: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5085 - MinusLogProbMetric: 27.5085 - val_loss: 28.2617 - val_MinusLogProbMetric: 28.2617 - lr: 3.1250e-05 - 43s/epoch - 217ms/step
Epoch 716/1000
2023-10-27 00:25:11.196 
Epoch 716/1000 
	 loss: 27.5119, MinusLogProbMetric: 27.5119, val_loss: 28.2468, val_MinusLogProbMetric: 28.2468

Epoch 716: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5119 - MinusLogProbMetric: 27.5119 - val_loss: 28.2468 - val_MinusLogProbMetric: 28.2468 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 717/1000
2023-10-27 00:25:54.656 
Epoch 717/1000 
	 loss: 27.5089, MinusLogProbMetric: 27.5089, val_loss: 28.2713, val_MinusLogProbMetric: 28.2713

Epoch 717: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5089 - MinusLogProbMetric: 27.5089 - val_loss: 28.2713 - val_MinusLogProbMetric: 28.2713 - lr: 3.1250e-05 - 43s/epoch - 222ms/step
Epoch 718/1000
2023-10-27 00:26:37.824 
Epoch 718/1000 
	 loss: 27.5116, MinusLogProbMetric: 27.5116, val_loss: 28.2585, val_MinusLogProbMetric: 28.2585

Epoch 718: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5116 - MinusLogProbMetric: 27.5116 - val_loss: 28.2585 - val_MinusLogProbMetric: 28.2585 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 719/1000
2023-10-27 00:27:20.690 
Epoch 719/1000 
	 loss: 27.5048, MinusLogProbMetric: 27.5048, val_loss: 28.2588, val_MinusLogProbMetric: 28.2588

Epoch 719: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5048 - MinusLogProbMetric: 27.5048 - val_loss: 28.2588 - val_MinusLogProbMetric: 28.2588 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 720/1000
2023-10-27 00:28:03.608 
Epoch 720/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 28.2468, val_MinusLogProbMetric: 28.2468

Epoch 720: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 28.2468 - val_MinusLogProbMetric: 28.2468 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 721/1000
2023-10-27 00:28:46.775 
Epoch 721/1000 
	 loss: 27.5105, MinusLogProbMetric: 27.5105, val_loss: 28.2500, val_MinusLogProbMetric: 28.2500

Epoch 721: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5105 - MinusLogProbMetric: 27.5105 - val_loss: 28.2500 - val_MinusLogProbMetric: 28.2500 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 722/1000
2023-10-27 00:29:29.730 
Epoch 722/1000 
	 loss: 27.5093, MinusLogProbMetric: 27.5093, val_loss: 28.2596, val_MinusLogProbMetric: 28.2596

Epoch 722: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5093 - MinusLogProbMetric: 27.5093 - val_loss: 28.2596 - val_MinusLogProbMetric: 28.2596 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 723/1000
2023-10-27 00:30:12.882 
Epoch 723/1000 
	 loss: 27.5111, MinusLogProbMetric: 27.5111, val_loss: 28.2645, val_MinusLogProbMetric: 28.2645

Epoch 723: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5111 - MinusLogProbMetric: 27.5111 - val_loss: 28.2645 - val_MinusLogProbMetric: 28.2645 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 724/1000
2023-10-27 00:30:55.869 
Epoch 724/1000 
	 loss: 27.5066, MinusLogProbMetric: 27.5066, val_loss: 28.2616, val_MinusLogProbMetric: 28.2616

Epoch 724: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5066 - MinusLogProbMetric: 27.5066 - val_loss: 28.2616 - val_MinusLogProbMetric: 28.2616 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 725/1000
2023-10-27 00:31:38.802 
Epoch 725/1000 
	 loss: 27.5107, MinusLogProbMetric: 27.5107, val_loss: 28.2695, val_MinusLogProbMetric: 28.2695

Epoch 725: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5107 - MinusLogProbMetric: 27.5107 - val_loss: 28.2695 - val_MinusLogProbMetric: 28.2695 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 726/1000
2023-10-27 00:32:21.747 
Epoch 726/1000 
	 loss: 27.5093, MinusLogProbMetric: 27.5093, val_loss: 28.2626, val_MinusLogProbMetric: 28.2626

Epoch 726: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5093 - MinusLogProbMetric: 27.5093 - val_loss: 28.2626 - val_MinusLogProbMetric: 28.2626 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 727/1000
2023-10-27 00:33:04.984 
Epoch 727/1000 
	 loss: 27.5055, MinusLogProbMetric: 27.5055, val_loss: 28.2662, val_MinusLogProbMetric: 28.2662

Epoch 727: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5055 - MinusLogProbMetric: 27.5055 - val_loss: 28.2662 - val_MinusLogProbMetric: 28.2662 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 728/1000
2023-10-27 00:33:48.220 
Epoch 728/1000 
	 loss: 27.5067, MinusLogProbMetric: 27.5067, val_loss: 28.2707, val_MinusLogProbMetric: 28.2707

Epoch 728: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5067 - MinusLogProbMetric: 27.5067 - val_loss: 28.2707 - val_MinusLogProbMetric: 28.2707 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 729/1000
2023-10-27 00:34:31.205 
Epoch 729/1000 
	 loss: 27.5067, MinusLogProbMetric: 27.5067, val_loss: 28.2512, val_MinusLogProbMetric: 28.2512

Epoch 729: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5067 - MinusLogProbMetric: 27.5067 - val_loss: 28.2512 - val_MinusLogProbMetric: 28.2512 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 730/1000
2023-10-27 00:35:14.173 
Epoch 730/1000 
	 loss: 27.5066, MinusLogProbMetric: 27.5066, val_loss: 28.2659, val_MinusLogProbMetric: 28.2659

Epoch 730: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5066 - MinusLogProbMetric: 27.5066 - val_loss: 28.2659 - val_MinusLogProbMetric: 28.2659 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 731/1000
2023-10-27 00:35:57.133 
Epoch 731/1000 
	 loss: 27.5076, MinusLogProbMetric: 27.5076, val_loss: 28.2649, val_MinusLogProbMetric: 28.2649

Epoch 731: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5076 - MinusLogProbMetric: 27.5076 - val_loss: 28.2649 - val_MinusLogProbMetric: 28.2649 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 732/1000
2023-10-27 00:36:39.997 
Epoch 732/1000 
	 loss: 27.5041, MinusLogProbMetric: 27.5041, val_loss: 28.2489, val_MinusLogProbMetric: 28.2489

Epoch 732: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5041 - MinusLogProbMetric: 27.5041 - val_loss: 28.2489 - val_MinusLogProbMetric: 28.2489 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 733/1000
2023-10-27 00:37:23.240 
Epoch 733/1000 
	 loss: 27.5028, MinusLogProbMetric: 27.5028, val_loss: 28.2604, val_MinusLogProbMetric: 28.2604

Epoch 733: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5028 - MinusLogProbMetric: 27.5028 - val_loss: 28.2604 - val_MinusLogProbMetric: 28.2604 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 734/1000
2023-10-27 00:38:06.060 
Epoch 734/1000 
	 loss: 27.5042, MinusLogProbMetric: 27.5042, val_loss: 28.2564, val_MinusLogProbMetric: 28.2564

Epoch 734: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5042 - MinusLogProbMetric: 27.5042 - val_loss: 28.2564 - val_MinusLogProbMetric: 28.2564 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 735/1000
2023-10-27 00:38:49.030 
Epoch 735/1000 
	 loss: 27.5047, MinusLogProbMetric: 27.5047, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 735: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5047 - MinusLogProbMetric: 27.5047 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 736/1000
2023-10-27 00:39:32.464 
Epoch 736/1000 
	 loss: 27.5043, MinusLogProbMetric: 27.5043, val_loss: 28.2552, val_MinusLogProbMetric: 28.2552

Epoch 736: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5043 - MinusLogProbMetric: 27.5043 - val_loss: 28.2552 - val_MinusLogProbMetric: 28.2552 - lr: 3.1250e-05 - 43s/epoch - 222ms/step
Epoch 737/1000
2023-10-27 00:40:15.520 
Epoch 737/1000 
	 loss: 27.5039, MinusLogProbMetric: 27.5039, val_loss: 28.2508, val_MinusLogProbMetric: 28.2508

Epoch 737: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5039 - MinusLogProbMetric: 27.5039 - val_loss: 28.2508 - val_MinusLogProbMetric: 28.2508 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 738/1000
2023-10-27 00:40:58.770 
Epoch 738/1000 
	 loss: 27.5037, MinusLogProbMetric: 27.5037, val_loss: 28.2517, val_MinusLogProbMetric: 28.2517

Epoch 738: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5037 - MinusLogProbMetric: 27.5037 - val_loss: 28.2517 - val_MinusLogProbMetric: 28.2517 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 739/1000
2023-10-27 00:41:41.782 
Epoch 739/1000 
	 loss: 27.5054, MinusLogProbMetric: 27.5054, val_loss: 28.2701, val_MinusLogProbMetric: 28.2701

Epoch 739: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5054 - MinusLogProbMetric: 27.5054 - val_loss: 28.2701 - val_MinusLogProbMetric: 28.2701 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 740/1000
2023-10-27 00:42:24.609 
Epoch 740/1000 
	 loss: 27.5054, MinusLogProbMetric: 27.5054, val_loss: 28.2643, val_MinusLogProbMetric: 28.2643

Epoch 740: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5054 - MinusLogProbMetric: 27.5054 - val_loss: 28.2643 - val_MinusLogProbMetric: 28.2643 - lr: 3.1250e-05 - 43s/epoch - 218ms/step
Epoch 741/1000
2023-10-27 00:43:07.660 
Epoch 741/1000 
	 loss: 27.5044, MinusLogProbMetric: 27.5044, val_loss: 28.2487, val_MinusLogProbMetric: 28.2487

Epoch 741: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5044 - MinusLogProbMetric: 27.5044 - val_loss: 28.2487 - val_MinusLogProbMetric: 28.2487 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 742/1000
2023-10-27 00:43:50.907 
Epoch 742/1000 
	 loss: 27.5060, MinusLogProbMetric: 27.5060, val_loss: 28.2786, val_MinusLogProbMetric: 28.2786

Epoch 742: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5060 - MinusLogProbMetric: 27.5060 - val_loss: 28.2786 - val_MinusLogProbMetric: 28.2786 - lr: 3.1250e-05 - 43s/epoch - 221ms/step
Epoch 743/1000
2023-10-27 00:44:33.979 
Epoch 743/1000 
	 loss: 27.5083, MinusLogProbMetric: 27.5083, val_loss: 28.2456, val_MinusLogProbMetric: 28.2456

Epoch 743: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5083 - MinusLogProbMetric: 27.5083 - val_loss: 28.2456 - val_MinusLogProbMetric: 28.2456 - lr: 3.1250e-05 - 43s/epoch - 220ms/step
Epoch 744/1000
2023-10-27 00:45:16.839 
Epoch 744/1000 
	 loss: 27.5066, MinusLogProbMetric: 27.5066, val_loss: 28.2586, val_MinusLogProbMetric: 28.2586

Epoch 744: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.5066 - MinusLogProbMetric: 27.5066 - val_loss: 28.2586 - val_MinusLogProbMetric: 28.2586 - lr: 3.1250e-05 - 43s/epoch - 219ms/step
Epoch 745/1000
2023-10-27 00:45:59.619 
Epoch 745/1000 
	 loss: 27.4906, MinusLogProbMetric: 27.4906, val_loss: 28.2418, val_MinusLogProbMetric: 28.2418

Epoch 745: val_loss did not improve from 28.23625
196/196 - 43s - loss: 27.4906 - MinusLogProbMetric: 27.4906 - val_loss: 28.2418 - val_MinusLogProbMetric: 28.2418 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 746/1000
2023-10-27 00:46:42.943 
Epoch 746/1000 
	 loss: 27.4899, MinusLogProbMetric: 27.4899, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 746: val_loss improved from 28.23625 to 28.23186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.4899 - MinusLogProbMetric: 27.4899 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 1.5625e-05 - 44s/epoch - 226ms/step
Epoch 747/1000
2023-10-27 00:47:27.137 
Epoch 747/1000 
	 loss: 27.4914, MinusLogProbMetric: 27.4914, val_loss: 28.2450, val_MinusLogProbMetric: 28.2450

Epoch 747: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4914 - MinusLogProbMetric: 27.4914 - val_loss: 28.2450 - val_MinusLogProbMetric: 28.2450 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 748/1000
2023-10-27 00:48:10.267 
Epoch 748/1000 
	 loss: 27.4923, MinusLogProbMetric: 27.4923, val_loss: 28.2534, val_MinusLogProbMetric: 28.2534

Epoch 748: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4923 - MinusLogProbMetric: 27.4923 - val_loss: 28.2534 - val_MinusLogProbMetric: 28.2534 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 749/1000
2023-10-27 00:48:53.611 
Epoch 749/1000 
	 loss: 27.4894, MinusLogProbMetric: 27.4894, val_loss: 28.2414, val_MinusLogProbMetric: 28.2414

Epoch 749: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4894 - MinusLogProbMetric: 27.4894 - val_loss: 28.2414 - val_MinusLogProbMetric: 28.2414 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 750/1000
2023-10-27 00:49:36.556 
Epoch 750/1000 
	 loss: 27.4899, MinusLogProbMetric: 27.4899, val_loss: 28.2436, val_MinusLogProbMetric: 28.2436

Epoch 750: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4899 - MinusLogProbMetric: 27.4899 - val_loss: 28.2436 - val_MinusLogProbMetric: 28.2436 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 751/1000
2023-10-27 00:50:19.651 
Epoch 751/1000 
	 loss: 27.4906, MinusLogProbMetric: 27.4906, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 751: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4906 - MinusLogProbMetric: 27.4906 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 752/1000
2023-10-27 00:51:02.730 
Epoch 752/1000 
	 loss: 27.4886, MinusLogProbMetric: 27.4886, val_loss: 28.2431, val_MinusLogProbMetric: 28.2431

Epoch 752: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4886 - MinusLogProbMetric: 27.4886 - val_loss: 28.2431 - val_MinusLogProbMetric: 28.2431 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 753/1000
2023-10-27 00:51:45.679 
Epoch 753/1000 
	 loss: 27.4901, MinusLogProbMetric: 27.4901, val_loss: 28.2410, val_MinusLogProbMetric: 28.2410

Epoch 753: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4901 - MinusLogProbMetric: 27.4901 - val_loss: 28.2410 - val_MinusLogProbMetric: 28.2410 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 754/1000
2023-10-27 00:52:28.676 
Epoch 754/1000 
	 loss: 27.4914, MinusLogProbMetric: 27.4914, val_loss: 28.2471, val_MinusLogProbMetric: 28.2471

Epoch 754: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4914 - MinusLogProbMetric: 27.4914 - val_loss: 28.2471 - val_MinusLogProbMetric: 28.2471 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 755/1000
2023-10-27 00:53:11.574 
Epoch 755/1000 
	 loss: 27.4900, MinusLogProbMetric: 27.4900, val_loss: 28.2552, val_MinusLogProbMetric: 28.2552

Epoch 755: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4900 - MinusLogProbMetric: 27.4900 - val_loss: 28.2552 - val_MinusLogProbMetric: 28.2552 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 756/1000
2023-10-27 00:53:54.808 
Epoch 756/1000 
	 loss: 27.4913, MinusLogProbMetric: 27.4913, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 756: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4913 - MinusLogProbMetric: 27.4913 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 757/1000
2023-10-27 00:54:37.833 
Epoch 757/1000 
	 loss: 27.4895, MinusLogProbMetric: 27.4895, val_loss: 28.2523, val_MinusLogProbMetric: 28.2523

Epoch 757: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4895 - MinusLogProbMetric: 27.4895 - val_loss: 28.2523 - val_MinusLogProbMetric: 28.2523 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 758/1000
2023-10-27 00:55:21.131 
Epoch 758/1000 
	 loss: 27.4905, MinusLogProbMetric: 27.4905, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 758: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4905 - MinusLogProbMetric: 27.4905 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 759/1000
2023-10-27 00:56:03.950 
Epoch 759/1000 
	 loss: 27.4883, MinusLogProbMetric: 27.4883, val_loss: 28.2429, val_MinusLogProbMetric: 28.2429

Epoch 759: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4883 - MinusLogProbMetric: 27.4883 - val_loss: 28.2429 - val_MinusLogProbMetric: 28.2429 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 760/1000
2023-10-27 00:56:47.283 
Epoch 760/1000 
	 loss: 27.4902, MinusLogProbMetric: 27.4902, val_loss: 28.2322, val_MinusLogProbMetric: 28.2322

Epoch 760: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4902 - MinusLogProbMetric: 27.4902 - val_loss: 28.2322 - val_MinusLogProbMetric: 28.2322 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 761/1000
2023-10-27 00:57:30.587 
Epoch 761/1000 
	 loss: 27.4906, MinusLogProbMetric: 27.4906, val_loss: 28.2400, val_MinusLogProbMetric: 28.2400

Epoch 761: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4906 - MinusLogProbMetric: 27.4906 - val_loss: 28.2400 - val_MinusLogProbMetric: 28.2400 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 762/1000
2023-10-27 00:58:13.715 
Epoch 762/1000 
	 loss: 27.4901, MinusLogProbMetric: 27.4901, val_loss: 28.2531, val_MinusLogProbMetric: 28.2531

Epoch 762: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4901 - MinusLogProbMetric: 27.4901 - val_loss: 28.2531 - val_MinusLogProbMetric: 28.2531 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 763/1000
2023-10-27 00:58:56.656 
Epoch 763/1000 
	 loss: 27.4890, MinusLogProbMetric: 27.4890, val_loss: 28.2496, val_MinusLogProbMetric: 28.2496

Epoch 763: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4890 - MinusLogProbMetric: 27.4890 - val_loss: 28.2496 - val_MinusLogProbMetric: 28.2496 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 764/1000
2023-10-27 00:59:39.883 
Epoch 764/1000 
	 loss: 27.4905, MinusLogProbMetric: 27.4905, val_loss: 28.2424, val_MinusLogProbMetric: 28.2424

Epoch 764: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4905 - MinusLogProbMetric: 27.4905 - val_loss: 28.2424 - val_MinusLogProbMetric: 28.2424 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 765/1000
2023-10-27 01:00:23.009 
Epoch 765/1000 
	 loss: 27.4885, MinusLogProbMetric: 27.4885, val_loss: 28.2404, val_MinusLogProbMetric: 28.2404

Epoch 765: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4885 - MinusLogProbMetric: 27.4885 - val_loss: 28.2404 - val_MinusLogProbMetric: 28.2404 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 766/1000
2023-10-27 01:01:05.662 
Epoch 766/1000 
	 loss: 27.4891, MinusLogProbMetric: 27.4891, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 766: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4891 - MinusLogProbMetric: 27.4891 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 767/1000
2023-10-27 01:01:48.457 
Epoch 767/1000 
	 loss: 27.4887, MinusLogProbMetric: 27.4887, val_loss: 28.2361, val_MinusLogProbMetric: 28.2361

Epoch 767: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4887 - MinusLogProbMetric: 27.4887 - val_loss: 28.2361 - val_MinusLogProbMetric: 28.2361 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 768/1000
2023-10-27 01:02:31.432 
Epoch 768/1000 
	 loss: 27.4896, MinusLogProbMetric: 27.4896, val_loss: 28.2470, val_MinusLogProbMetric: 28.2470

Epoch 768: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4896 - MinusLogProbMetric: 27.4896 - val_loss: 28.2470 - val_MinusLogProbMetric: 28.2470 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 769/1000
2023-10-27 01:03:14.440 
Epoch 769/1000 
	 loss: 27.4911, MinusLogProbMetric: 27.4911, val_loss: 28.2532, val_MinusLogProbMetric: 28.2532

Epoch 769: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4911 - MinusLogProbMetric: 27.4911 - val_loss: 28.2532 - val_MinusLogProbMetric: 28.2532 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 770/1000
2023-10-27 01:03:57.474 
Epoch 770/1000 
	 loss: 27.4889, MinusLogProbMetric: 27.4889, val_loss: 28.2379, val_MinusLogProbMetric: 28.2379

Epoch 770: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4889 - MinusLogProbMetric: 27.4889 - val_loss: 28.2379 - val_MinusLogProbMetric: 28.2379 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 771/1000
2023-10-27 01:04:40.073 
Epoch 771/1000 
	 loss: 27.4893, MinusLogProbMetric: 27.4893, val_loss: 28.2368, val_MinusLogProbMetric: 28.2368

Epoch 771: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4893 - MinusLogProbMetric: 27.4893 - val_loss: 28.2368 - val_MinusLogProbMetric: 28.2368 - lr: 1.5625e-05 - 43s/epoch - 217ms/step
Epoch 772/1000
2023-10-27 01:05:23.153 
Epoch 772/1000 
	 loss: 27.4902, MinusLogProbMetric: 27.4902, val_loss: 28.2405, val_MinusLogProbMetric: 28.2405

Epoch 772: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4902 - MinusLogProbMetric: 27.4902 - val_loss: 28.2405 - val_MinusLogProbMetric: 28.2405 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 773/1000
2023-10-27 01:06:05.897 
Epoch 773/1000 
	 loss: 27.4882, MinusLogProbMetric: 27.4882, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 773: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4882 - MinusLogProbMetric: 27.4882 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 774/1000
2023-10-27 01:06:49.135 
Epoch 774/1000 
	 loss: 27.4903, MinusLogProbMetric: 27.4903, val_loss: 28.2453, val_MinusLogProbMetric: 28.2453

Epoch 774: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4903 - MinusLogProbMetric: 27.4903 - val_loss: 28.2453 - val_MinusLogProbMetric: 28.2453 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 775/1000
2023-10-27 01:07:32.117 
Epoch 775/1000 
	 loss: 27.4890, MinusLogProbMetric: 27.4890, val_loss: 28.2429, val_MinusLogProbMetric: 28.2429

Epoch 775: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4890 - MinusLogProbMetric: 27.4890 - val_loss: 28.2429 - val_MinusLogProbMetric: 28.2429 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 776/1000
2023-10-27 01:08:15.195 
Epoch 776/1000 
	 loss: 27.4915, MinusLogProbMetric: 27.4915, val_loss: 28.2504, val_MinusLogProbMetric: 28.2504

Epoch 776: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4915 - MinusLogProbMetric: 27.4915 - val_loss: 28.2504 - val_MinusLogProbMetric: 28.2504 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 777/1000
2023-10-27 01:08:58.461 
Epoch 777/1000 
	 loss: 27.4895, MinusLogProbMetric: 27.4895, val_loss: 28.2546, val_MinusLogProbMetric: 28.2546

Epoch 777: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4895 - MinusLogProbMetric: 27.4895 - val_loss: 28.2546 - val_MinusLogProbMetric: 28.2546 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 778/1000
2023-10-27 01:09:41.612 
Epoch 778/1000 
	 loss: 27.4890, MinusLogProbMetric: 27.4890, val_loss: 28.2387, val_MinusLogProbMetric: 28.2387

Epoch 778: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4890 - MinusLogProbMetric: 27.4890 - val_loss: 28.2387 - val_MinusLogProbMetric: 28.2387 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 779/1000
2023-10-27 01:10:25.194 
Epoch 779/1000 
	 loss: 27.4899, MinusLogProbMetric: 27.4899, val_loss: 28.2427, val_MinusLogProbMetric: 28.2427

Epoch 779: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4899 - MinusLogProbMetric: 27.4899 - val_loss: 28.2427 - val_MinusLogProbMetric: 28.2427 - lr: 1.5625e-05 - 44s/epoch - 222ms/step
Epoch 780/1000
2023-10-27 01:11:08.213 
Epoch 780/1000 
	 loss: 27.4917, MinusLogProbMetric: 27.4917, val_loss: 28.2543, val_MinusLogProbMetric: 28.2543

Epoch 780: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4917 - MinusLogProbMetric: 27.4917 - val_loss: 28.2543 - val_MinusLogProbMetric: 28.2543 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 781/1000
2023-10-27 01:11:51.539 
Epoch 781/1000 
	 loss: 27.4882, MinusLogProbMetric: 27.4882, val_loss: 28.2372, val_MinusLogProbMetric: 28.2372

Epoch 781: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4882 - MinusLogProbMetric: 27.4882 - val_loss: 28.2372 - val_MinusLogProbMetric: 28.2372 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 782/1000
2023-10-27 01:12:35.062 
Epoch 782/1000 
	 loss: 27.4870, MinusLogProbMetric: 27.4870, val_loss: 28.2502, val_MinusLogProbMetric: 28.2502

Epoch 782: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4870 - MinusLogProbMetric: 27.4870 - val_loss: 28.2502 - val_MinusLogProbMetric: 28.2502 - lr: 1.5625e-05 - 44s/epoch - 222ms/step
Epoch 783/1000
2023-10-27 01:13:18.181 
Epoch 783/1000 
	 loss: 27.4897, MinusLogProbMetric: 27.4897, val_loss: 28.2392, val_MinusLogProbMetric: 28.2392

Epoch 783: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4897 - MinusLogProbMetric: 27.4897 - val_loss: 28.2392 - val_MinusLogProbMetric: 28.2392 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 784/1000
2023-10-27 01:14:01.170 
Epoch 784/1000 
	 loss: 27.4893, MinusLogProbMetric: 27.4893, val_loss: 28.2479, val_MinusLogProbMetric: 28.2479

Epoch 784: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4893 - MinusLogProbMetric: 27.4893 - val_loss: 28.2479 - val_MinusLogProbMetric: 28.2479 - lr: 1.5625e-05 - 43s/epoch - 219ms/step
Epoch 785/1000
2023-10-27 01:14:44.319 
Epoch 785/1000 
	 loss: 27.4899, MinusLogProbMetric: 27.4899, val_loss: 28.2415, val_MinusLogProbMetric: 28.2415

Epoch 785: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4899 - MinusLogProbMetric: 27.4899 - val_loss: 28.2415 - val_MinusLogProbMetric: 28.2415 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 786/1000
2023-10-27 01:15:27.627 
Epoch 786/1000 
	 loss: 27.4898, MinusLogProbMetric: 27.4898, val_loss: 28.2473, val_MinusLogProbMetric: 28.2473

Epoch 786: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4898 - MinusLogProbMetric: 27.4898 - val_loss: 28.2473 - val_MinusLogProbMetric: 28.2473 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 787/1000
2023-10-27 01:16:10.342 
Epoch 787/1000 
	 loss: 27.4873, MinusLogProbMetric: 27.4873, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 787: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4873 - MinusLogProbMetric: 27.4873 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 788/1000
2023-10-27 01:16:53.441 
Epoch 788/1000 
	 loss: 27.4876, MinusLogProbMetric: 27.4876, val_loss: 28.2481, val_MinusLogProbMetric: 28.2481

Epoch 788: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4876 - MinusLogProbMetric: 27.4876 - val_loss: 28.2481 - val_MinusLogProbMetric: 28.2481 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 789/1000
2023-10-27 01:17:36.122 
Epoch 789/1000 
	 loss: 27.4882, MinusLogProbMetric: 27.4882, val_loss: 28.2486, val_MinusLogProbMetric: 28.2486

Epoch 789: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4882 - MinusLogProbMetric: 27.4882 - val_loss: 28.2486 - val_MinusLogProbMetric: 28.2486 - lr: 1.5625e-05 - 43s/epoch - 218ms/step
Epoch 790/1000
2023-10-27 01:18:19.503 
Epoch 790/1000 
	 loss: 27.4895, MinusLogProbMetric: 27.4895, val_loss: 28.2368, val_MinusLogProbMetric: 28.2368

Epoch 790: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4895 - MinusLogProbMetric: 27.4895 - val_loss: 28.2368 - val_MinusLogProbMetric: 28.2368 - lr: 1.5625e-05 - 43s/epoch - 221ms/step
Epoch 791/1000
2023-10-27 01:19:02.647 
Epoch 791/1000 
	 loss: 27.4877, MinusLogProbMetric: 27.4877, val_loss: 28.2373, val_MinusLogProbMetric: 28.2373

Epoch 791: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4877 - MinusLogProbMetric: 27.4877 - val_loss: 28.2373 - val_MinusLogProbMetric: 28.2373 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 792/1000
2023-10-27 01:19:45.750 
Epoch 792/1000 
	 loss: 27.4891, MinusLogProbMetric: 27.4891, val_loss: 28.2538, val_MinusLogProbMetric: 28.2538

Epoch 792: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4891 - MinusLogProbMetric: 27.4891 - val_loss: 28.2538 - val_MinusLogProbMetric: 28.2538 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 793/1000
2023-10-27 01:20:28.858 
Epoch 793/1000 
	 loss: 27.4900, MinusLogProbMetric: 27.4900, val_loss: 28.2412, val_MinusLogProbMetric: 28.2412

Epoch 793: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4900 - MinusLogProbMetric: 27.4900 - val_loss: 28.2412 - val_MinusLogProbMetric: 28.2412 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 794/1000
2023-10-27 01:21:11.967 
Epoch 794/1000 
	 loss: 27.4868, MinusLogProbMetric: 27.4868, val_loss: 28.2406, val_MinusLogProbMetric: 28.2406

Epoch 794: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4868 - MinusLogProbMetric: 27.4868 - val_loss: 28.2406 - val_MinusLogProbMetric: 28.2406 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 795/1000
2023-10-27 01:21:55.041 
Epoch 795/1000 
	 loss: 27.4886, MinusLogProbMetric: 27.4886, val_loss: 28.2491, val_MinusLogProbMetric: 28.2491

Epoch 795: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4886 - MinusLogProbMetric: 27.4886 - val_loss: 28.2491 - val_MinusLogProbMetric: 28.2491 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 796/1000
2023-10-27 01:22:38.196 
Epoch 796/1000 
	 loss: 27.4886, MinusLogProbMetric: 27.4886, val_loss: 28.2703, val_MinusLogProbMetric: 28.2703

Epoch 796: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4886 - MinusLogProbMetric: 27.4886 - val_loss: 28.2703 - val_MinusLogProbMetric: 28.2703 - lr: 1.5625e-05 - 43s/epoch - 220ms/step
Epoch 797/1000
2023-10-27 01:23:21.456 
Epoch 797/1000 
	 loss: 27.4838, MinusLogProbMetric: 27.4838, val_loss: 28.2375, val_MinusLogProbMetric: 28.2375

Epoch 797: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4838 - MinusLogProbMetric: 27.4838 - val_loss: 28.2375 - val_MinusLogProbMetric: 28.2375 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 798/1000
2023-10-27 01:24:04.166 
Epoch 798/1000 
	 loss: 27.4812, MinusLogProbMetric: 27.4812, val_loss: 28.2467, val_MinusLogProbMetric: 28.2467

Epoch 798: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4812 - MinusLogProbMetric: 27.4812 - val_loss: 28.2467 - val_MinusLogProbMetric: 28.2467 - lr: 7.8125e-06 - 43s/epoch - 218ms/step
Epoch 799/1000
2023-10-27 01:24:47.073 
Epoch 799/1000 
	 loss: 27.4819, MinusLogProbMetric: 27.4819, val_loss: 28.2407, val_MinusLogProbMetric: 28.2407

Epoch 799: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4819 - MinusLogProbMetric: 27.4819 - val_loss: 28.2407 - val_MinusLogProbMetric: 28.2407 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 800/1000
2023-10-27 01:25:30.249 
Epoch 800/1000 
	 loss: 27.4804, MinusLogProbMetric: 27.4804, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 800: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4804 - MinusLogProbMetric: 27.4804 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 801/1000
2023-10-27 01:26:13.274 
Epoch 801/1000 
	 loss: 27.4819, MinusLogProbMetric: 27.4819, val_loss: 28.2459, val_MinusLogProbMetric: 28.2459

Epoch 801: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4819 - MinusLogProbMetric: 27.4819 - val_loss: 28.2459 - val_MinusLogProbMetric: 28.2459 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 802/1000
2023-10-27 01:26:56.054 
Epoch 802/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 28.2403, val_MinusLogProbMetric: 28.2403

Epoch 802: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 28.2403 - val_MinusLogProbMetric: 28.2403 - lr: 7.8125e-06 - 43s/epoch - 218ms/step
Epoch 803/1000
2023-10-27 01:27:39.384 
Epoch 803/1000 
	 loss: 27.4810, MinusLogProbMetric: 27.4810, val_loss: 28.2422, val_MinusLogProbMetric: 28.2422

Epoch 803: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4810 - MinusLogProbMetric: 27.4810 - val_loss: 28.2422 - val_MinusLogProbMetric: 28.2422 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 804/1000
2023-10-27 01:28:22.694 
Epoch 804/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 28.2390, val_MinusLogProbMetric: 28.2390

Epoch 804: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 28.2390 - val_MinusLogProbMetric: 28.2390 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 805/1000
2023-10-27 01:29:05.828 
Epoch 805/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 805: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 806/1000
2023-10-27 01:29:48.770 
Epoch 806/1000 
	 loss: 27.4815, MinusLogProbMetric: 27.4815, val_loss: 28.2452, val_MinusLogProbMetric: 28.2452

Epoch 806: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4815 - MinusLogProbMetric: 27.4815 - val_loss: 28.2452 - val_MinusLogProbMetric: 28.2452 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 807/1000
2023-10-27 01:30:32.178 
Epoch 807/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 28.2400, val_MinusLogProbMetric: 28.2400

Epoch 807: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 28.2400 - val_MinusLogProbMetric: 28.2400 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 808/1000
2023-10-27 01:31:15.406 
Epoch 808/1000 
	 loss: 27.4805, MinusLogProbMetric: 27.4805, val_loss: 28.2407, val_MinusLogProbMetric: 28.2407

Epoch 808: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4805 - MinusLogProbMetric: 27.4805 - val_loss: 28.2407 - val_MinusLogProbMetric: 28.2407 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 809/1000
2023-10-27 01:31:58.243 
Epoch 809/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 28.2364, val_MinusLogProbMetric: 28.2364

Epoch 809: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 28.2364 - val_MinusLogProbMetric: 28.2364 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 810/1000
2023-10-27 01:32:41.135 
Epoch 810/1000 
	 loss: 27.4803, MinusLogProbMetric: 27.4803, val_loss: 28.2456, val_MinusLogProbMetric: 28.2456

Epoch 810: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4803 - MinusLogProbMetric: 27.4803 - val_loss: 28.2456 - val_MinusLogProbMetric: 28.2456 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 811/1000
2023-10-27 01:33:24.329 
Epoch 811/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 28.2395, val_MinusLogProbMetric: 28.2395

Epoch 811: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 28.2395 - val_MinusLogProbMetric: 28.2395 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 812/1000
2023-10-27 01:34:07.769 
Epoch 812/1000 
	 loss: 27.4810, MinusLogProbMetric: 27.4810, val_loss: 28.2359, val_MinusLogProbMetric: 28.2359

Epoch 812: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4810 - MinusLogProbMetric: 27.4810 - val_loss: 28.2359 - val_MinusLogProbMetric: 28.2359 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 813/1000
2023-10-27 01:34:50.561 
Epoch 813/1000 
	 loss: 27.4806, MinusLogProbMetric: 27.4806, val_loss: 28.2417, val_MinusLogProbMetric: 28.2417

Epoch 813: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4806 - MinusLogProbMetric: 27.4806 - val_loss: 28.2417 - val_MinusLogProbMetric: 28.2417 - lr: 7.8125e-06 - 43s/epoch - 218ms/step
Epoch 814/1000
2023-10-27 01:35:33.713 
Epoch 814/1000 
	 loss: 27.4814, MinusLogProbMetric: 27.4814, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 814: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4814 - MinusLogProbMetric: 27.4814 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 815/1000
2023-10-27 01:36:17.165 
Epoch 815/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 815: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 816/1000
2023-10-27 01:37:00.192 
Epoch 816/1000 
	 loss: 27.4810, MinusLogProbMetric: 27.4810, val_loss: 28.2370, val_MinusLogProbMetric: 28.2370

Epoch 816: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4810 - MinusLogProbMetric: 27.4810 - val_loss: 28.2370 - val_MinusLogProbMetric: 28.2370 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 817/1000
2023-10-27 01:37:43.776 
Epoch 817/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.2385, val_MinusLogProbMetric: 28.2385

Epoch 817: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.2385 - val_MinusLogProbMetric: 28.2385 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 818/1000
2023-10-27 01:38:26.981 
Epoch 818/1000 
	 loss: 27.4807, MinusLogProbMetric: 27.4807, val_loss: 28.2355, val_MinusLogProbMetric: 28.2355

Epoch 818: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4807 - MinusLogProbMetric: 27.4807 - val_loss: 28.2355 - val_MinusLogProbMetric: 28.2355 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 819/1000
2023-10-27 01:39:09.972 
Epoch 819/1000 
	 loss: 27.4799, MinusLogProbMetric: 27.4799, val_loss: 28.2340, val_MinusLogProbMetric: 28.2340

Epoch 819: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4799 - MinusLogProbMetric: 27.4799 - val_loss: 28.2340 - val_MinusLogProbMetric: 28.2340 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 820/1000
2023-10-27 01:39:53.605 
Epoch 820/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2409, val_MinusLogProbMetric: 28.2409

Epoch 820: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2409 - val_MinusLogProbMetric: 28.2409 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 821/1000
2023-10-27 01:40:37.197 
Epoch 821/1000 
	 loss: 27.4807, MinusLogProbMetric: 27.4807, val_loss: 28.2348, val_MinusLogProbMetric: 28.2348

Epoch 821: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4807 - MinusLogProbMetric: 27.4807 - val_loss: 28.2348 - val_MinusLogProbMetric: 28.2348 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 822/1000
2023-10-27 01:41:20.595 
Epoch 822/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 822: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 823/1000
2023-10-27 01:42:03.639 
Epoch 823/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 28.2367, val_MinusLogProbMetric: 28.2367

Epoch 823: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 28.2367 - val_MinusLogProbMetric: 28.2367 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 824/1000
2023-10-27 01:42:47.087 
Epoch 824/1000 
	 loss: 27.4810, MinusLogProbMetric: 27.4810, val_loss: 28.2402, val_MinusLogProbMetric: 28.2402

Epoch 824: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4810 - MinusLogProbMetric: 27.4810 - val_loss: 28.2402 - val_MinusLogProbMetric: 28.2402 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 825/1000
2023-10-27 01:43:30.238 
Epoch 825/1000 
	 loss: 27.4803, MinusLogProbMetric: 27.4803, val_loss: 28.2453, val_MinusLogProbMetric: 28.2453

Epoch 825: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4803 - MinusLogProbMetric: 27.4803 - val_loss: 28.2453 - val_MinusLogProbMetric: 28.2453 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 826/1000
2023-10-27 01:44:14.114 
Epoch 826/1000 
	 loss: 27.4805, MinusLogProbMetric: 27.4805, val_loss: 28.2387, val_MinusLogProbMetric: 28.2387

Epoch 826: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4805 - MinusLogProbMetric: 27.4805 - val_loss: 28.2387 - val_MinusLogProbMetric: 28.2387 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 827/1000
2023-10-27 01:44:57.665 
Epoch 827/1000 
	 loss: 27.4805, MinusLogProbMetric: 27.4805, val_loss: 28.2335, val_MinusLogProbMetric: 28.2335

Epoch 827: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4805 - MinusLogProbMetric: 27.4805 - val_loss: 28.2335 - val_MinusLogProbMetric: 28.2335 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 828/1000
2023-10-27 01:45:38.237 
Epoch 828/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2333, val_MinusLogProbMetric: 28.2333

Epoch 828: val_loss did not improve from 28.23186
196/196 - 41s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2333 - val_MinusLogProbMetric: 28.2333 - lr: 7.8125e-06 - 41s/epoch - 207ms/step
Epoch 829/1000
2023-10-27 01:46:21.321 
Epoch 829/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2409, val_MinusLogProbMetric: 28.2409

Epoch 829: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2409 - val_MinusLogProbMetric: 28.2409 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 830/1000
2023-10-27 01:47:02.745 
Epoch 830/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.2383, val_MinusLogProbMetric: 28.2383

Epoch 830: val_loss did not improve from 28.23186
196/196 - 41s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.2383 - val_MinusLogProbMetric: 28.2383 - lr: 7.8125e-06 - 41s/epoch - 211ms/step
Epoch 831/1000
2023-10-27 01:47:43.879 
Epoch 831/1000 
	 loss: 27.4805, MinusLogProbMetric: 27.4805, val_loss: 28.2447, val_MinusLogProbMetric: 28.2447

Epoch 831: val_loss did not improve from 28.23186
196/196 - 41s - loss: 27.4805 - MinusLogProbMetric: 27.4805 - val_loss: 28.2447 - val_MinusLogProbMetric: 28.2447 - lr: 7.8125e-06 - 41s/epoch - 210ms/step
Epoch 832/1000
2023-10-27 01:48:25.692 
Epoch 832/1000 
	 loss: 27.4808, MinusLogProbMetric: 27.4808, val_loss: 28.2334, val_MinusLogProbMetric: 28.2334

Epoch 832: val_loss did not improve from 28.23186
196/196 - 42s - loss: 27.4808 - MinusLogProbMetric: 27.4808 - val_loss: 28.2334 - val_MinusLogProbMetric: 28.2334 - lr: 7.8125e-06 - 42s/epoch - 213ms/step
Epoch 833/1000
2023-10-27 01:49:09.257 
Epoch 833/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 28.2404, val_MinusLogProbMetric: 28.2404

Epoch 833: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 28.2404 - val_MinusLogProbMetric: 28.2404 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 834/1000
2023-10-27 01:49:52.772 
Epoch 834/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 28.2405, val_MinusLogProbMetric: 28.2405

Epoch 834: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 28.2405 - val_MinusLogProbMetric: 28.2405 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 835/1000
2023-10-27 01:50:36.044 
Epoch 835/1000 
	 loss: 27.4804, MinusLogProbMetric: 27.4804, val_loss: 28.2427, val_MinusLogProbMetric: 28.2427

Epoch 835: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4804 - MinusLogProbMetric: 27.4804 - val_loss: 28.2427 - val_MinusLogProbMetric: 28.2427 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 836/1000
2023-10-27 01:51:19.766 
Epoch 836/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 28.2386, val_MinusLogProbMetric: 28.2386

Epoch 836: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 28.2386 - val_MinusLogProbMetric: 28.2386 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 837/1000
2023-10-27 01:52:03.487 
Epoch 837/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 837: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 838/1000
2023-10-27 01:52:47.274 
Epoch 838/1000 
	 loss: 27.4805, MinusLogProbMetric: 27.4805, val_loss: 28.2416, val_MinusLogProbMetric: 28.2416

Epoch 838: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4805 - MinusLogProbMetric: 27.4805 - val_loss: 28.2416 - val_MinusLogProbMetric: 28.2416 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 839/1000
2023-10-27 01:53:30.455 
Epoch 839/1000 
	 loss: 27.4796, MinusLogProbMetric: 27.4796, val_loss: 28.2368, val_MinusLogProbMetric: 28.2368

Epoch 839: val_loss did not improve from 28.23186
196/196 - 43s - loss: 27.4796 - MinusLogProbMetric: 27.4796 - val_loss: 28.2368 - val_MinusLogProbMetric: 28.2368 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 840/1000
2023-10-27 01:54:14.062 
Epoch 840/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 28.2449, val_MinusLogProbMetric: 28.2449

Epoch 840: val_loss did not improve from 28.23186
196/196 - 44s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 28.2449 - val_MinusLogProbMetric: 28.2449 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 841/1000
2023-10-27 01:54:57.483 
Epoch 841/1000 
	 loss: 27.4803, MinusLogProbMetric: 27.4803, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 841: val_loss improved from 28.23186 to 28.23167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 45s - loss: 27.4803 - MinusLogProbMetric: 27.4803 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 7.8125e-06 - 45s/epoch - 227ms/step
Epoch 842/1000
2023-10-27 01:55:42.122 
Epoch 842/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 28.2393, val_MinusLogProbMetric: 28.2393

Epoch 842: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 28.2393 - val_MinusLogProbMetric: 28.2393 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 843/1000
2023-10-27 01:56:25.773 
Epoch 843/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2361, val_MinusLogProbMetric: 28.2361

Epoch 843: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2361 - val_MinusLogProbMetric: 28.2361 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 844/1000
2023-10-27 01:57:08.762 
Epoch 844/1000 
	 loss: 27.4799, MinusLogProbMetric: 27.4799, val_loss: 28.2390, val_MinusLogProbMetric: 28.2390

Epoch 844: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4799 - MinusLogProbMetric: 27.4799 - val_loss: 28.2390 - val_MinusLogProbMetric: 28.2390 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 845/1000
2023-10-27 01:57:52.453 
Epoch 845/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.2349, val_MinusLogProbMetric: 28.2349

Epoch 845: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.2349 - val_MinusLogProbMetric: 28.2349 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 846/1000
2023-10-27 01:58:36.043 
Epoch 846/1000 
	 loss: 27.4788, MinusLogProbMetric: 27.4788, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 846: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4788 - MinusLogProbMetric: 27.4788 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 847/1000
2023-10-27 01:59:19.096 
Epoch 847/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 847: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 848/1000
2023-10-27 02:00:02.680 
Epoch 848/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.2340, val_MinusLogProbMetric: 28.2340

Epoch 848: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.2340 - val_MinusLogProbMetric: 28.2340 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 849/1000
2023-10-27 02:00:45.925 
Epoch 849/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 28.2392, val_MinusLogProbMetric: 28.2392

Epoch 849: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 28.2392 - val_MinusLogProbMetric: 28.2392 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 850/1000
2023-10-27 02:01:29.773 
Epoch 850/1000 
	 loss: 27.4786, MinusLogProbMetric: 27.4786, val_loss: 28.2437, val_MinusLogProbMetric: 28.2437

Epoch 850: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4786 - MinusLogProbMetric: 27.4786 - val_loss: 28.2437 - val_MinusLogProbMetric: 28.2437 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 851/1000
2023-10-27 02:02:13.536 
Epoch 851/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.2378, val_MinusLogProbMetric: 28.2378

Epoch 851: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.2378 - val_MinusLogProbMetric: 28.2378 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 852/1000
2023-10-27 02:02:57.594 
Epoch 852/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.2377, val_MinusLogProbMetric: 28.2377

Epoch 852: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.2377 - val_MinusLogProbMetric: 28.2377 - lr: 7.8125e-06 - 44s/epoch - 225ms/step
Epoch 853/1000
2023-10-27 02:03:37.406 
Epoch 853/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 28.2366, val_MinusLogProbMetric: 28.2366

Epoch 853: val_loss did not improve from 28.23167
196/196 - 40s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 28.2366 - val_MinusLogProbMetric: 28.2366 - lr: 7.8125e-06 - 40s/epoch - 203ms/step
Epoch 854/1000
2023-10-27 02:04:16.304 
Epoch 854/1000 
	 loss: 27.4812, MinusLogProbMetric: 27.4812, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 854: val_loss did not improve from 28.23167
196/196 - 39s - loss: 27.4812 - MinusLogProbMetric: 27.4812 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 7.8125e-06 - 39s/epoch - 198ms/step
Epoch 855/1000
2023-10-27 02:05:00.127 
Epoch 855/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.2375, val_MinusLogProbMetric: 28.2375

Epoch 855: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.2375 - val_MinusLogProbMetric: 28.2375 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 856/1000
2023-10-27 02:05:43.267 
Epoch 856/1000 
	 loss: 27.4800, MinusLogProbMetric: 27.4800, val_loss: 28.2379, val_MinusLogProbMetric: 28.2379

Epoch 856: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4800 - MinusLogProbMetric: 27.4800 - val_loss: 28.2379 - val_MinusLogProbMetric: 28.2379 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 857/1000
2023-10-27 02:06:26.756 
Epoch 857/1000 
	 loss: 27.4785, MinusLogProbMetric: 27.4785, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 857: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4785 - MinusLogProbMetric: 27.4785 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 858/1000
2023-10-27 02:07:10.071 
Epoch 858/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 28.2415, val_MinusLogProbMetric: 28.2415

Epoch 858: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 28.2415 - val_MinusLogProbMetric: 28.2415 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 859/1000
2023-10-27 02:07:53.559 
Epoch 859/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.2406, val_MinusLogProbMetric: 28.2406

Epoch 859: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.2406 - val_MinusLogProbMetric: 28.2406 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 860/1000
2023-10-27 02:08:36.882 
Epoch 860/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.2388, val_MinusLogProbMetric: 28.2388

Epoch 860: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.2388 - val_MinusLogProbMetric: 28.2388 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 861/1000
2023-10-27 02:09:19.758 
Epoch 861/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 28.2393, val_MinusLogProbMetric: 28.2393

Epoch 861: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 28.2393 - val_MinusLogProbMetric: 28.2393 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 862/1000
2023-10-27 02:10:03.043 
Epoch 862/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2453, val_MinusLogProbMetric: 28.2453

Epoch 862: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2453 - val_MinusLogProbMetric: 28.2453 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 863/1000
2023-10-27 02:10:46.318 
Epoch 863/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 28.2372, val_MinusLogProbMetric: 28.2372

Epoch 863: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 28.2372 - val_MinusLogProbMetric: 28.2372 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 864/1000
2023-10-27 02:11:29.989 
Epoch 864/1000 
	 loss: 27.4785, MinusLogProbMetric: 27.4785, val_loss: 28.2328, val_MinusLogProbMetric: 28.2328

Epoch 864: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4785 - MinusLogProbMetric: 27.4785 - val_loss: 28.2328 - val_MinusLogProbMetric: 28.2328 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 865/1000
2023-10-27 02:12:13.227 
Epoch 865/1000 
	 loss: 27.4796, MinusLogProbMetric: 27.4796, val_loss: 28.2492, val_MinusLogProbMetric: 28.2492

Epoch 865: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4796 - MinusLogProbMetric: 27.4796 - val_loss: 28.2492 - val_MinusLogProbMetric: 28.2492 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 866/1000
2023-10-27 02:12:57.025 
Epoch 866/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.2395, val_MinusLogProbMetric: 28.2395

Epoch 866: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.2395 - val_MinusLogProbMetric: 28.2395 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 867/1000
2023-10-27 02:13:40.885 
Epoch 867/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 867: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 868/1000
2023-10-27 02:14:24.284 
Epoch 868/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.2375, val_MinusLogProbMetric: 28.2375

Epoch 868: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.2375 - val_MinusLogProbMetric: 28.2375 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 869/1000
2023-10-27 02:15:07.925 
Epoch 869/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.2414, val_MinusLogProbMetric: 28.2414

Epoch 869: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.2414 - val_MinusLogProbMetric: 28.2414 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 870/1000
2023-10-27 02:15:51.859 
Epoch 870/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 28.2372, val_MinusLogProbMetric: 28.2372

Epoch 870: val_loss did not improve from 28.23167
196/196 - 44s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 28.2372 - val_MinusLogProbMetric: 28.2372 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 871/1000
2023-10-27 02:16:34.955 
Epoch 871/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.2336, val_MinusLogProbMetric: 28.2336

Epoch 871: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.2336 - val_MinusLogProbMetric: 28.2336 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 872/1000
2023-10-27 02:17:18.456 
Epoch 872/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2414, val_MinusLogProbMetric: 28.2414

Epoch 872: val_loss did not improve from 28.23167
196/196 - 43s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2414 - val_MinusLogProbMetric: 28.2414 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 873/1000
2023-10-27 02:18:02.396 
Epoch 873/1000 
	 loss: 27.4803, MinusLogProbMetric: 27.4803, val_loss: 28.2304, val_MinusLogProbMetric: 28.2304

Epoch 873: val_loss improved from 28.23167 to 28.23040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 45s - loss: 27.4803 - MinusLogProbMetric: 27.4803 - val_loss: 28.2304 - val_MinusLogProbMetric: 28.2304 - lr: 7.8125e-06 - 45s/epoch - 229ms/step
Epoch 874/1000
2023-10-27 02:18:46.253 
Epoch 874/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 28.2391, val_MinusLogProbMetric: 28.2391

Epoch 874: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 28.2391 - val_MinusLogProbMetric: 28.2391 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 875/1000
2023-10-27 02:19:29.547 
Epoch 875/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2412, val_MinusLogProbMetric: 28.2412

Epoch 875: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2412 - val_MinusLogProbMetric: 28.2412 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 876/1000
2023-10-27 02:20:12.985 
Epoch 876/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.2393, val_MinusLogProbMetric: 28.2393

Epoch 876: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.2393 - val_MinusLogProbMetric: 28.2393 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 877/1000
2023-10-27 02:20:56.183 
Epoch 877/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2391, val_MinusLogProbMetric: 28.2391

Epoch 877: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2391 - val_MinusLogProbMetric: 28.2391 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 878/1000
2023-10-27 02:21:39.358 
Epoch 878/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 28.2395, val_MinusLogProbMetric: 28.2395

Epoch 878: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 28.2395 - val_MinusLogProbMetric: 28.2395 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 879/1000
2023-10-27 02:22:22.397 
Epoch 879/1000 
	 loss: 27.4796, MinusLogProbMetric: 27.4796, val_loss: 28.2387, val_MinusLogProbMetric: 28.2387

Epoch 879: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4796 - MinusLogProbMetric: 27.4796 - val_loss: 28.2387 - val_MinusLogProbMetric: 28.2387 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 880/1000
2023-10-27 02:23:06.025 
Epoch 880/1000 
	 loss: 27.4785, MinusLogProbMetric: 27.4785, val_loss: 28.2376, val_MinusLogProbMetric: 28.2376

Epoch 880: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4785 - MinusLogProbMetric: 27.4785 - val_loss: 28.2376 - val_MinusLogProbMetric: 28.2376 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 881/1000
2023-10-27 02:23:49.523 
Epoch 881/1000 
	 loss: 27.4796, MinusLogProbMetric: 27.4796, val_loss: 28.2415, val_MinusLogProbMetric: 28.2415

Epoch 881: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4796 - MinusLogProbMetric: 27.4796 - val_loss: 28.2415 - val_MinusLogProbMetric: 28.2415 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 882/1000
2023-10-27 02:24:32.716 
Epoch 882/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.2425, val_MinusLogProbMetric: 28.2425

Epoch 882: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.2425 - val_MinusLogProbMetric: 28.2425 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 883/1000
2023-10-27 02:25:15.882 
Epoch 883/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 883: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 884/1000
2023-10-27 02:25:59.467 
Epoch 884/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.2349, val_MinusLogProbMetric: 28.2349

Epoch 884: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.2349 - val_MinusLogProbMetric: 28.2349 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 885/1000
2023-10-27 02:26:43.050 
Epoch 885/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.2350, val_MinusLogProbMetric: 28.2350

Epoch 885: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.2350 - val_MinusLogProbMetric: 28.2350 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 886/1000
2023-10-27 02:27:26.753 
Epoch 886/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.2390, val_MinusLogProbMetric: 28.2390

Epoch 886: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.2390 - val_MinusLogProbMetric: 28.2390 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 887/1000
2023-10-27 02:28:10.331 
Epoch 887/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.2384, val_MinusLogProbMetric: 28.2384

Epoch 887: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.2384 - val_MinusLogProbMetric: 28.2384 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 888/1000
2023-10-27 02:28:53.415 
Epoch 888/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.2390, val_MinusLogProbMetric: 28.2390

Epoch 888: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.2390 - val_MinusLogProbMetric: 28.2390 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 889/1000
2023-10-27 02:29:36.516 
Epoch 889/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.2399, val_MinusLogProbMetric: 28.2399

Epoch 889: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.2399 - val_MinusLogProbMetric: 28.2399 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 890/1000
2023-10-27 02:30:20.229 
Epoch 890/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.2494, val_MinusLogProbMetric: 28.2494

Epoch 890: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.2494 - val_MinusLogProbMetric: 28.2494 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 891/1000
2023-10-27 02:31:03.808 
Epoch 891/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2380, val_MinusLogProbMetric: 28.2380

Epoch 891: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2380 - val_MinusLogProbMetric: 28.2380 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 892/1000
2023-10-27 02:31:46.936 
Epoch 892/1000 
	 loss: 27.4803, MinusLogProbMetric: 27.4803, val_loss: 28.2403, val_MinusLogProbMetric: 28.2403

Epoch 892: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4803 - MinusLogProbMetric: 27.4803 - val_loss: 28.2403 - val_MinusLogProbMetric: 28.2403 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 893/1000
2023-10-27 02:32:30.000 
Epoch 893/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 28.2317, val_MinusLogProbMetric: 28.2317

Epoch 893: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 28.2317 - val_MinusLogProbMetric: 28.2317 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 894/1000
2023-10-27 02:33:13.289 
Epoch 894/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.2380, val_MinusLogProbMetric: 28.2380

Epoch 894: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.2380 - val_MinusLogProbMetric: 28.2380 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 895/1000
2023-10-27 02:33:56.637 
Epoch 895/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.2445, val_MinusLogProbMetric: 28.2445

Epoch 895: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.2445 - val_MinusLogProbMetric: 28.2445 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 896/1000
2023-10-27 02:34:39.974 
Epoch 896/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 896: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 897/1000
2023-10-27 02:35:23.120 
Epoch 897/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2370, val_MinusLogProbMetric: 28.2370

Epoch 897: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2370 - val_MinusLogProbMetric: 28.2370 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 898/1000
2023-10-27 02:36:06.520 
Epoch 898/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2325, val_MinusLogProbMetric: 28.2325

Epoch 898: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2325 - val_MinusLogProbMetric: 28.2325 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 899/1000
2023-10-27 02:36:49.559 
Epoch 899/1000 
	 loss: 27.4784, MinusLogProbMetric: 27.4784, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 899: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4784 - MinusLogProbMetric: 27.4784 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 900/1000
2023-10-27 02:37:32.592 
Epoch 900/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.2418, val_MinusLogProbMetric: 28.2418

Epoch 900: val_loss did not improve from 28.23040
196/196 - 43s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.2418 - val_MinusLogProbMetric: 28.2418 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 901/1000
2023-10-27 02:38:16.140 
Epoch 901/1000 
	 loss: 27.4773, MinusLogProbMetric: 27.4773, val_loss: 28.2355, val_MinusLogProbMetric: 28.2355

Epoch 901: val_loss did not improve from 28.23040
196/196 - 44s - loss: 27.4773 - MinusLogProbMetric: 27.4773 - val_loss: 28.2355 - val_MinusLogProbMetric: 28.2355 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 902/1000
2023-10-27 02:38:59.785 
Epoch 902/1000 
	 loss: 27.4780, MinusLogProbMetric: 27.4780, val_loss: 28.2295, val_MinusLogProbMetric: 28.2295

Epoch 902: val_loss improved from 28.23040 to 28.22955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 44s - loss: 27.4780 - MinusLogProbMetric: 27.4780 - val_loss: 28.2295 - val_MinusLogProbMetric: 28.2295 - lr: 7.8125e-06 - 44s/epoch - 227ms/step
Epoch 903/1000
2023-10-27 02:39:43.619 
Epoch 903/1000 
	 loss: 27.4788, MinusLogProbMetric: 27.4788, val_loss: 28.2377, val_MinusLogProbMetric: 28.2377

Epoch 903: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4788 - MinusLogProbMetric: 27.4788 - val_loss: 28.2377 - val_MinusLogProbMetric: 28.2377 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 904/1000
2023-10-27 02:40:26.696 
Epoch 904/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.2426, val_MinusLogProbMetric: 28.2426

Epoch 904: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.2426 - val_MinusLogProbMetric: 28.2426 - lr: 7.8125e-06 - 43s/epoch - 220ms/step
Epoch 905/1000
2023-10-27 02:41:09.536 
Epoch 905/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 28.2363, val_MinusLogProbMetric: 28.2363

Epoch 905: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 28.2363 - val_MinusLogProbMetric: 28.2363 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 906/1000
2023-10-27 02:41:52.768 
Epoch 906/1000 
	 loss: 27.4777, MinusLogProbMetric: 27.4777, val_loss: 28.2363, val_MinusLogProbMetric: 28.2363

Epoch 906: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4777 - MinusLogProbMetric: 27.4777 - val_loss: 28.2363 - val_MinusLogProbMetric: 28.2363 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 907/1000
2023-10-27 02:42:36.313 
Epoch 907/1000 
	 loss: 27.4784, MinusLogProbMetric: 27.4784, val_loss: 28.2357, val_MinusLogProbMetric: 28.2357

Epoch 907: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4784 - MinusLogProbMetric: 27.4784 - val_loss: 28.2357 - val_MinusLogProbMetric: 28.2357 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 908/1000
2023-10-27 02:43:19.782 
Epoch 908/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.2425, val_MinusLogProbMetric: 28.2425

Epoch 908: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.2425 - val_MinusLogProbMetric: 28.2425 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 909/1000
2023-10-27 02:44:03.721 
Epoch 909/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.2402, val_MinusLogProbMetric: 28.2402

Epoch 909: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.2402 - val_MinusLogProbMetric: 28.2402 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 910/1000
2023-10-27 02:44:47.590 
Epoch 910/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 28.2435, val_MinusLogProbMetric: 28.2435

Epoch 910: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 28.2435 - val_MinusLogProbMetric: 28.2435 - lr: 7.8125e-06 - 44s/epoch - 224ms/step
Epoch 911/1000
2023-10-27 02:45:31.123 
Epoch 911/1000 
	 loss: 27.4779, MinusLogProbMetric: 27.4779, val_loss: 28.2370, val_MinusLogProbMetric: 28.2370

Epoch 911: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4779 - MinusLogProbMetric: 27.4779 - val_loss: 28.2370 - val_MinusLogProbMetric: 28.2370 - lr: 7.8125e-06 - 44s/epoch - 222ms/step
Epoch 912/1000
2023-10-27 02:46:14.752 
Epoch 912/1000 
	 loss: 27.4779, MinusLogProbMetric: 27.4779, val_loss: 28.2441, val_MinusLogProbMetric: 28.2441

Epoch 912: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4779 - MinusLogProbMetric: 27.4779 - val_loss: 28.2441 - val_MinusLogProbMetric: 28.2441 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 913/1000
2023-10-27 02:46:58.221 
Epoch 913/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2347, val_MinusLogProbMetric: 28.2347

Epoch 913: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2347 - val_MinusLogProbMetric: 28.2347 - lr: 7.8125e-06 - 43s/epoch - 222ms/step
Epoch 914/1000
2023-10-27 02:47:40.937 
Epoch 914/1000 
	 loss: 27.4778, MinusLogProbMetric: 27.4778, val_loss: 28.2368, val_MinusLogProbMetric: 28.2368

Epoch 914: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4778 - MinusLogProbMetric: 27.4778 - val_loss: 28.2368 - val_MinusLogProbMetric: 28.2368 - lr: 7.8125e-06 - 43s/epoch - 218ms/step
Epoch 915/1000
2023-10-27 02:48:24.588 
Epoch 915/1000 
	 loss: 27.4786, MinusLogProbMetric: 27.4786, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 915: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4786 - MinusLogProbMetric: 27.4786 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 916/1000
2023-10-27 02:49:07.845 
Epoch 916/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 28.2386, val_MinusLogProbMetric: 28.2386

Epoch 916: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 28.2386 - val_MinusLogProbMetric: 28.2386 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 917/1000
2023-10-27 02:49:51.186 
Epoch 917/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2363, val_MinusLogProbMetric: 28.2363

Epoch 917: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2363 - val_MinusLogProbMetric: 28.2363 - lr: 7.8125e-06 - 43s/epoch - 221ms/step
Epoch 918/1000
2023-10-27 02:50:32.117 
Epoch 918/1000 
	 loss: 27.4786, MinusLogProbMetric: 27.4786, val_loss: 28.2359, val_MinusLogProbMetric: 28.2359

Epoch 918: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4786 - MinusLogProbMetric: 27.4786 - val_loss: 28.2359 - val_MinusLogProbMetric: 28.2359 - lr: 7.8125e-06 - 41s/epoch - 209ms/step
Epoch 919/1000
2023-10-27 02:51:11.782 
Epoch 919/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2368, val_MinusLogProbMetric: 28.2368

Epoch 919: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2368 - val_MinusLogProbMetric: 28.2368 - lr: 7.8125e-06 - 40s/epoch - 202ms/step
Epoch 920/1000
2023-10-27 02:51:48.095 
Epoch 920/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2405, val_MinusLogProbMetric: 28.2405

Epoch 920: val_loss did not improve from 28.22955
196/196 - 36s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2405 - val_MinusLogProbMetric: 28.2405 - lr: 7.8125e-06 - 36s/epoch - 185ms/step
Epoch 921/1000
2023-10-27 02:52:25.006 
Epoch 921/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.2471, val_MinusLogProbMetric: 28.2471

Epoch 921: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.2471 - val_MinusLogProbMetric: 28.2471 - lr: 7.8125e-06 - 37s/epoch - 188ms/step
Epoch 922/1000
2023-10-27 02:53:02.540 
Epoch 922/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 28.2376, val_MinusLogProbMetric: 28.2376

Epoch 922: val_loss did not improve from 28.22955
196/196 - 38s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 28.2376 - val_MinusLogProbMetric: 28.2376 - lr: 7.8125e-06 - 38s/epoch - 191ms/step
Epoch 923/1000
2023-10-27 02:53:46.333 
Epoch 923/1000 
	 loss: 27.4777, MinusLogProbMetric: 27.4777, val_loss: 28.2398, val_MinusLogProbMetric: 28.2398

Epoch 923: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4777 - MinusLogProbMetric: 27.4777 - val_loss: 28.2398 - val_MinusLogProbMetric: 28.2398 - lr: 7.8125e-06 - 44s/epoch - 223ms/step
Epoch 924/1000
2023-10-27 02:54:26.367 
Epoch 924/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 28.2339, val_MinusLogProbMetric: 28.2339

Epoch 924: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 28.2339 - val_MinusLogProbMetric: 28.2339 - lr: 7.8125e-06 - 40s/epoch - 204ms/step
Epoch 925/1000
2023-10-27 02:55:03.307 
Epoch 925/1000 
	 loss: 27.4777, MinusLogProbMetric: 27.4777, val_loss: 28.2366, val_MinusLogProbMetric: 28.2366

Epoch 925: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4777 - MinusLogProbMetric: 27.4777 - val_loss: 28.2366 - val_MinusLogProbMetric: 28.2366 - lr: 7.8125e-06 - 37s/epoch - 188ms/step
Epoch 926/1000
2023-10-27 02:55:43.048 
Epoch 926/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.2413, val_MinusLogProbMetric: 28.2413

Epoch 926: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.2413 - val_MinusLogProbMetric: 28.2413 - lr: 7.8125e-06 - 40s/epoch - 203ms/step
Epoch 927/1000
2023-10-27 02:56:24.856 
Epoch 927/1000 
	 loss: 27.4785, MinusLogProbMetric: 27.4785, val_loss: 28.2339, val_MinusLogProbMetric: 28.2339

Epoch 927: val_loss did not improve from 28.22955
196/196 - 42s - loss: 27.4785 - MinusLogProbMetric: 27.4785 - val_loss: 28.2339 - val_MinusLogProbMetric: 28.2339 - lr: 7.8125e-06 - 42s/epoch - 213ms/step
Epoch 928/1000
2023-10-27 02:57:06.656 
Epoch 928/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.2391, val_MinusLogProbMetric: 28.2391

Epoch 928: val_loss did not improve from 28.22955
196/196 - 42s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.2391 - val_MinusLogProbMetric: 28.2391 - lr: 7.8125e-06 - 42s/epoch - 213ms/step
Epoch 929/1000
2023-10-27 02:57:43.847 
Epoch 929/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.2410, val_MinusLogProbMetric: 28.2410

Epoch 929: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.2410 - val_MinusLogProbMetric: 28.2410 - lr: 7.8125e-06 - 37s/epoch - 190ms/step
Epoch 930/1000
2023-10-27 02:58:25.017 
Epoch 930/1000 
	 loss: 27.4785, MinusLogProbMetric: 27.4785, val_loss: 28.2361, val_MinusLogProbMetric: 28.2361

Epoch 930: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4785 - MinusLogProbMetric: 27.4785 - val_loss: 28.2361 - val_MinusLogProbMetric: 28.2361 - lr: 7.8125e-06 - 41s/epoch - 210ms/step
Epoch 931/1000
2023-10-27 02:59:06.398 
Epoch 931/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 28.2364, val_MinusLogProbMetric: 28.2364

Epoch 931: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 28.2364 - val_MinusLogProbMetric: 28.2364 - lr: 7.8125e-06 - 41s/epoch - 211ms/step
Epoch 932/1000
2023-10-27 02:59:46.871 
Epoch 932/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 28.2388, val_MinusLogProbMetric: 28.2388

Epoch 932: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 28.2388 - val_MinusLogProbMetric: 28.2388 - lr: 7.8125e-06 - 40s/epoch - 206ms/step
Epoch 933/1000
2023-10-27 03:00:23.909 
Epoch 933/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 28.2332, val_MinusLogProbMetric: 28.2332

Epoch 933: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 28.2332 - val_MinusLogProbMetric: 28.2332 - lr: 7.8125e-06 - 37s/epoch - 189ms/step
Epoch 934/1000
2023-10-27 03:01:01.337 
Epoch 934/1000 
	 loss: 27.4788, MinusLogProbMetric: 27.4788, val_loss: 28.2370, val_MinusLogProbMetric: 28.2370

Epoch 934: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4788 - MinusLogProbMetric: 27.4788 - val_loss: 28.2370 - val_MinusLogProbMetric: 28.2370 - lr: 7.8125e-06 - 37s/epoch - 191ms/step
Epoch 935/1000
2023-10-27 03:01:42.578 
Epoch 935/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 28.2365, val_MinusLogProbMetric: 28.2365

Epoch 935: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 28.2365 - val_MinusLogProbMetric: 28.2365 - lr: 7.8125e-06 - 41s/epoch - 210ms/step
Epoch 936/1000
2023-10-27 03:02:24.009 
Epoch 936/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.2413, val_MinusLogProbMetric: 28.2413

Epoch 936: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.2413 - val_MinusLogProbMetric: 28.2413 - lr: 7.8125e-06 - 41s/epoch - 211ms/step
Epoch 937/1000
2023-10-27 03:03:02.005 
Epoch 937/1000 
	 loss: 27.4786, MinusLogProbMetric: 27.4786, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 937: val_loss did not improve from 28.22955
196/196 - 38s - loss: 27.4786 - MinusLogProbMetric: 27.4786 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 7.8125e-06 - 38s/epoch - 194ms/step
Epoch 938/1000
2023-10-27 03:03:39.838 
Epoch 938/1000 
	 loss: 27.4780, MinusLogProbMetric: 27.4780, val_loss: 28.2380, val_MinusLogProbMetric: 28.2380

Epoch 938: val_loss did not improve from 28.22955
196/196 - 38s - loss: 27.4780 - MinusLogProbMetric: 27.4780 - val_loss: 28.2380 - val_MinusLogProbMetric: 28.2380 - lr: 7.8125e-06 - 38s/epoch - 193ms/step
Epoch 939/1000
2023-10-27 03:04:20.838 
Epoch 939/1000 
	 loss: 27.4775, MinusLogProbMetric: 27.4775, val_loss: 28.2345, val_MinusLogProbMetric: 28.2345

Epoch 939: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4775 - MinusLogProbMetric: 27.4775 - val_loss: 28.2345 - val_MinusLogProbMetric: 28.2345 - lr: 7.8125e-06 - 41s/epoch - 209ms/step
Epoch 940/1000
2023-10-27 03:04:59.812 
Epoch 940/1000 
	 loss: 27.4779, MinusLogProbMetric: 27.4779, val_loss: 28.2350, val_MinusLogProbMetric: 28.2350

Epoch 940: val_loss did not improve from 28.22955
196/196 - 39s - loss: 27.4779 - MinusLogProbMetric: 27.4779 - val_loss: 28.2350 - val_MinusLogProbMetric: 28.2350 - lr: 7.8125e-06 - 39s/epoch - 199ms/step
Epoch 941/1000
2023-10-27 03:05:37.011 
Epoch 941/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.2392, val_MinusLogProbMetric: 28.2392

Epoch 941: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.2392 - val_MinusLogProbMetric: 28.2392 - lr: 7.8125e-06 - 37s/epoch - 190ms/step
Epoch 942/1000
2023-10-27 03:06:16.410 
Epoch 942/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.2447, val_MinusLogProbMetric: 28.2447

Epoch 942: val_loss did not improve from 28.22955
196/196 - 39s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.2447 - val_MinusLogProbMetric: 28.2447 - lr: 7.8125e-06 - 39s/epoch - 201ms/step
Epoch 943/1000
2023-10-27 03:06:59.422 
Epoch 943/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 28.2482, val_MinusLogProbMetric: 28.2482

Epoch 943: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 28.2482 - val_MinusLogProbMetric: 28.2482 - lr: 7.8125e-06 - 43s/epoch - 219ms/step
Epoch 944/1000
2023-10-27 03:07:38.681 
Epoch 944/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 28.2411, val_MinusLogProbMetric: 28.2411

Epoch 944: val_loss did not improve from 28.22955
196/196 - 39s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 28.2411 - val_MinusLogProbMetric: 28.2411 - lr: 7.8125e-06 - 39s/epoch - 200ms/step
Epoch 945/1000
2023-10-27 03:08:15.085 
Epoch 945/1000 
	 loss: 27.4771, MinusLogProbMetric: 27.4771, val_loss: 28.2345, val_MinusLogProbMetric: 28.2345

Epoch 945: val_loss did not improve from 28.22955
196/196 - 36s - loss: 27.4771 - MinusLogProbMetric: 27.4771 - val_loss: 28.2345 - val_MinusLogProbMetric: 28.2345 - lr: 7.8125e-06 - 36s/epoch - 186ms/step
Epoch 946/1000
2023-10-27 03:08:53.617 
Epoch 946/1000 
	 loss: 27.4789, MinusLogProbMetric: 27.4789, val_loss: 28.2348, val_MinusLogProbMetric: 28.2348

Epoch 946: val_loss did not improve from 28.22955
196/196 - 39s - loss: 27.4789 - MinusLogProbMetric: 27.4789 - val_loss: 28.2348 - val_MinusLogProbMetric: 28.2348 - lr: 7.8125e-06 - 39s/epoch - 197ms/step
Epoch 947/1000
2023-10-27 03:09:35.209 
Epoch 947/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 28.2425, val_MinusLogProbMetric: 28.2425

Epoch 947: val_loss did not improve from 28.22955
196/196 - 42s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 28.2425 - val_MinusLogProbMetric: 28.2425 - lr: 7.8125e-06 - 42s/epoch - 212ms/step
Epoch 948/1000
2023-10-27 03:10:15.469 
Epoch 948/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 28.2382, val_MinusLogProbMetric: 28.2382

Epoch 948: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 28.2382 - val_MinusLogProbMetric: 28.2382 - lr: 7.8125e-06 - 40s/epoch - 205ms/step
Epoch 949/1000
2023-10-27 03:10:51.806 
Epoch 949/1000 
	 loss: 27.4780, MinusLogProbMetric: 27.4780, val_loss: 28.2400, val_MinusLogProbMetric: 28.2400

Epoch 949: val_loss did not improve from 28.22955
196/196 - 36s - loss: 27.4780 - MinusLogProbMetric: 27.4780 - val_loss: 28.2400 - val_MinusLogProbMetric: 28.2400 - lr: 7.8125e-06 - 36s/epoch - 185ms/step
Epoch 950/1000
2023-10-27 03:11:32.401 
Epoch 950/1000 
	 loss: 27.4775, MinusLogProbMetric: 27.4775, val_loss: 28.2378, val_MinusLogProbMetric: 28.2378

Epoch 950: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4775 - MinusLogProbMetric: 27.4775 - val_loss: 28.2378 - val_MinusLogProbMetric: 28.2378 - lr: 7.8125e-06 - 41s/epoch - 207ms/step
Epoch 951/1000
2023-10-27 03:12:12.764 
Epoch 951/1000 
	 loss: 27.4763, MinusLogProbMetric: 27.4763, val_loss: 28.2369, val_MinusLogProbMetric: 28.2369

Epoch 951: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4763 - MinusLogProbMetric: 27.4763 - val_loss: 28.2369 - val_MinusLogProbMetric: 28.2369 - lr: 7.8125e-06 - 40s/epoch - 206ms/step
Epoch 952/1000
2023-10-27 03:12:49.763 
Epoch 952/1000 
	 loss: 27.4765, MinusLogProbMetric: 27.4765, val_loss: 28.2344, val_MinusLogProbMetric: 28.2344

Epoch 952: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4765 - MinusLogProbMetric: 27.4765 - val_loss: 28.2344 - val_MinusLogProbMetric: 28.2344 - lr: 7.8125e-06 - 37s/epoch - 189ms/step
Epoch 953/1000
2023-10-27 03:13:26.682 
Epoch 953/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2353, val_MinusLogProbMetric: 28.2353

Epoch 953: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2353 - val_MinusLogProbMetric: 28.2353 - lr: 3.9063e-06 - 37s/epoch - 188ms/step
Epoch 954/1000
2023-10-27 03:14:09.023 
Epoch 954/1000 
	 loss: 27.4734, MinusLogProbMetric: 27.4734, val_loss: 28.2385, val_MinusLogProbMetric: 28.2385

Epoch 954: val_loss did not improve from 28.22955
196/196 - 42s - loss: 27.4734 - MinusLogProbMetric: 27.4734 - val_loss: 28.2385 - val_MinusLogProbMetric: 28.2385 - lr: 3.9063e-06 - 42s/epoch - 216ms/step
Epoch 955/1000
2023-10-27 03:14:49.165 
Epoch 955/1000 
	 loss: 27.4737, MinusLogProbMetric: 27.4737, val_loss: 28.2350, val_MinusLogProbMetric: 28.2350

Epoch 955: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4737 - MinusLogProbMetric: 27.4737 - val_loss: 28.2350 - val_MinusLogProbMetric: 28.2350 - lr: 3.9063e-06 - 40s/epoch - 205ms/step
Epoch 956/1000
2023-10-27 03:15:25.743 
Epoch 956/1000 
	 loss: 27.4729, MinusLogProbMetric: 27.4729, val_loss: 28.2352, val_MinusLogProbMetric: 28.2352

Epoch 956: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4729 - MinusLogProbMetric: 27.4729 - val_loss: 28.2352 - val_MinusLogProbMetric: 28.2352 - lr: 3.9063e-06 - 37s/epoch - 187ms/step
Epoch 957/1000
2023-10-27 03:16:02.722 
Epoch 957/1000 
	 loss: 27.4736, MinusLogProbMetric: 27.4736, val_loss: 28.2362, val_MinusLogProbMetric: 28.2362

Epoch 957: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4736 - MinusLogProbMetric: 27.4736 - val_loss: 28.2362 - val_MinusLogProbMetric: 28.2362 - lr: 3.9063e-06 - 37s/epoch - 189ms/step
Epoch 958/1000
2023-10-27 03:16:44.645 
Epoch 958/1000 
	 loss: 27.4740, MinusLogProbMetric: 27.4740, val_loss: 28.2376, val_MinusLogProbMetric: 28.2376

Epoch 958: val_loss did not improve from 28.22955
196/196 - 42s - loss: 27.4740 - MinusLogProbMetric: 27.4740 - val_loss: 28.2376 - val_MinusLogProbMetric: 28.2376 - lr: 3.9063e-06 - 42s/epoch - 214ms/step
Epoch 959/1000
2023-10-27 03:17:22.550 
Epoch 959/1000 
	 loss: 27.4737, MinusLogProbMetric: 27.4737, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 959: val_loss did not improve from 28.22955
196/196 - 38s - loss: 27.4737 - MinusLogProbMetric: 27.4737 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 3.9063e-06 - 38s/epoch - 193ms/step
Epoch 960/1000
2023-10-27 03:17:59.501 
Epoch 960/1000 
	 loss: 27.4730, MinusLogProbMetric: 27.4730, val_loss: 28.2347, val_MinusLogProbMetric: 28.2347

Epoch 960: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4730 - MinusLogProbMetric: 27.4730 - val_loss: 28.2347 - val_MinusLogProbMetric: 28.2347 - lr: 3.9063e-06 - 37s/epoch - 189ms/step
Epoch 961/1000
2023-10-27 03:18:35.832 
Epoch 961/1000 
	 loss: 27.4733, MinusLogProbMetric: 27.4733, val_loss: 28.2329, val_MinusLogProbMetric: 28.2329

Epoch 961: val_loss did not improve from 28.22955
196/196 - 36s - loss: 27.4733 - MinusLogProbMetric: 27.4733 - val_loss: 28.2329 - val_MinusLogProbMetric: 28.2329 - lr: 3.9063e-06 - 36s/epoch - 185ms/step
Epoch 962/1000
2023-10-27 03:19:17.042 
Epoch 962/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2324, val_MinusLogProbMetric: 28.2324

Epoch 962: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2324 - val_MinusLogProbMetric: 28.2324 - lr: 3.9063e-06 - 41s/epoch - 210ms/step
Epoch 963/1000
2023-10-27 03:19:56.177 
Epoch 963/1000 
	 loss: 27.4730, MinusLogProbMetric: 27.4730, val_loss: 28.2354, val_MinusLogProbMetric: 28.2354

Epoch 963: val_loss did not improve from 28.22955
196/196 - 39s - loss: 27.4730 - MinusLogProbMetric: 27.4730 - val_loss: 28.2354 - val_MinusLogProbMetric: 28.2354 - lr: 3.9063e-06 - 39s/epoch - 200ms/step
Epoch 964/1000
2023-10-27 03:20:32.482 
Epoch 964/1000 
	 loss: 27.4734, MinusLogProbMetric: 27.4734, val_loss: 28.2339, val_MinusLogProbMetric: 28.2339

Epoch 964: val_loss did not improve from 28.22955
196/196 - 36s - loss: 27.4734 - MinusLogProbMetric: 27.4734 - val_loss: 28.2339 - val_MinusLogProbMetric: 28.2339 - lr: 3.9063e-06 - 36s/epoch - 185ms/step
Epoch 965/1000
2023-10-27 03:21:13.093 
Epoch 965/1000 
	 loss: 27.4733, MinusLogProbMetric: 27.4733, val_loss: 28.2324, val_MinusLogProbMetric: 28.2324

Epoch 965: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4733 - MinusLogProbMetric: 27.4733 - val_loss: 28.2324 - val_MinusLogProbMetric: 28.2324 - lr: 3.9063e-06 - 41s/epoch - 207ms/step
Epoch 966/1000
2023-10-27 03:21:55.445 
Epoch 966/1000 
	 loss: 27.4739, MinusLogProbMetric: 27.4739, val_loss: 28.2348, val_MinusLogProbMetric: 28.2348

Epoch 966: val_loss did not improve from 28.22955
196/196 - 42s - loss: 27.4739 - MinusLogProbMetric: 27.4739 - val_loss: 28.2348 - val_MinusLogProbMetric: 28.2348 - lr: 3.9063e-06 - 42s/epoch - 216ms/step
Epoch 967/1000
2023-10-27 03:22:31.795 
Epoch 967/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2353, val_MinusLogProbMetric: 28.2353

Epoch 967: val_loss did not improve from 28.22955
196/196 - 36s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2353 - val_MinusLogProbMetric: 28.2353 - lr: 3.9063e-06 - 36s/epoch - 185ms/step
Epoch 968/1000
2023-10-27 03:23:10.045 
Epoch 968/1000 
	 loss: 27.4737, MinusLogProbMetric: 27.4737, val_loss: 28.2341, val_MinusLogProbMetric: 28.2341

Epoch 968: val_loss did not improve from 28.22955
196/196 - 38s - loss: 27.4737 - MinusLogProbMetric: 27.4737 - val_loss: 28.2341 - val_MinusLogProbMetric: 28.2341 - lr: 3.9063e-06 - 38s/epoch - 195ms/step
Epoch 969/1000
2023-10-27 03:23:49.027 
Epoch 969/1000 
	 loss: 27.4744, MinusLogProbMetric: 27.4744, val_loss: 28.2323, val_MinusLogProbMetric: 28.2323

Epoch 969: val_loss did not improve from 28.22955
196/196 - 39s - loss: 27.4744 - MinusLogProbMetric: 27.4744 - val_loss: 28.2323 - val_MinusLogProbMetric: 28.2323 - lr: 3.9063e-06 - 39s/epoch - 199ms/step
Epoch 970/1000
2023-10-27 03:24:32.071 
Epoch 970/1000 
	 loss: 27.4734, MinusLogProbMetric: 27.4734, val_loss: 28.2328, val_MinusLogProbMetric: 28.2328

Epoch 970: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4734 - MinusLogProbMetric: 27.4734 - val_loss: 28.2328 - val_MinusLogProbMetric: 28.2328 - lr: 3.9063e-06 - 43s/epoch - 220ms/step
Epoch 971/1000
2023-10-27 03:25:13.471 
Epoch 971/1000 
	 loss: 27.4730, MinusLogProbMetric: 27.4730, val_loss: 28.2354, val_MinusLogProbMetric: 28.2354

Epoch 971: val_loss did not improve from 28.22955
196/196 - 41s - loss: 27.4730 - MinusLogProbMetric: 27.4730 - val_loss: 28.2354 - val_MinusLogProbMetric: 28.2354 - lr: 3.9063e-06 - 41s/epoch - 211ms/step
Epoch 972/1000
2023-10-27 03:25:56.312 
Epoch 972/1000 
	 loss: 27.4739, MinusLogProbMetric: 27.4739, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 972: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4739 - MinusLogProbMetric: 27.4739 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 3.9063e-06 - 43s/epoch - 219ms/step
Epoch 973/1000
2023-10-27 03:26:36.318 
Epoch 973/1000 
	 loss: 27.4730, MinusLogProbMetric: 27.4730, val_loss: 28.2372, val_MinusLogProbMetric: 28.2372

Epoch 973: val_loss did not improve from 28.22955
196/196 - 40s - loss: 27.4730 - MinusLogProbMetric: 27.4730 - val_loss: 28.2372 - val_MinusLogProbMetric: 28.2372 - lr: 3.9063e-06 - 40s/epoch - 204ms/step
Epoch 974/1000
2023-10-27 03:27:13.478 
Epoch 974/1000 
	 loss: 27.4740, MinusLogProbMetric: 27.4740, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 974: val_loss did not improve from 28.22955
196/196 - 37s - loss: 27.4740 - MinusLogProbMetric: 27.4740 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 3.9063e-06 - 37s/epoch - 190ms/step
Epoch 975/1000
2023-10-27 03:27:52.005 
Epoch 975/1000 
	 loss: 27.4738, MinusLogProbMetric: 27.4738, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 975: val_loss did not improve from 28.22955
196/196 - 39s - loss: 27.4738 - MinusLogProbMetric: 27.4738 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 3.9063e-06 - 39s/epoch - 197ms/step
Epoch 976/1000
2023-10-27 03:28:35.346 
Epoch 976/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2390, val_MinusLogProbMetric: 28.2390

Epoch 976: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2390 - val_MinusLogProbMetric: 28.2390 - lr: 3.9063e-06 - 43s/epoch - 221ms/step
Epoch 977/1000
2023-10-27 03:29:18.930 
Epoch 977/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 977: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 3.9063e-06 - 44s/epoch - 222ms/step
Epoch 978/1000
2023-10-27 03:30:02.033 
Epoch 978/1000 
	 loss: 27.4729, MinusLogProbMetric: 27.4729, val_loss: 28.2334, val_MinusLogProbMetric: 28.2334

Epoch 978: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4729 - MinusLogProbMetric: 27.4729 - val_loss: 28.2334 - val_MinusLogProbMetric: 28.2334 - lr: 3.9063e-06 - 43s/epoch - 220ms/step
Epoch 979/1000
2023-10-27 03:30:44.592 
Epoch 979/1000 
	 loss: 27.4737, MinusLogProbMetric: 27.4737, val_loss: 28.2330, val_MinusLogProbMetric: 28.2330

Epoch 979: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4737 - MinusLogProbMetric: 27.4737 - val_loss: 28.2330 - val_MinusLogProbMetric: 28.2330 - lr: 3.9063e-06 - 43s/epoch - 217ms/step
Epoch 980/1000
2023-10-27 03:31:29.381 
Epoch 980/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2396, val_MinusLogProbMetric: 28.2396

Epoch 980: val_loss did not improve from 28.22955
196/196 - 45s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2396 - val_MinusLogProbMetric: 28.2396 - lr: 3.9063e-06 - 45s/epoch - 229ms/step
Epoch 981/1000
2023-10-27 03:32:13.864 
Epoch 981/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 981: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 3.9063e-06 - 44s/epoch - 227ms/step
Epoch 982/1000
2023-10-27 03:32:58.070 
Epoch 982/1000 
	 loss: 27.4727, MinusLogProbMetric: 27.4727, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 982: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4727 - MinusLogProbMetric: 27.4727 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 983/1000
2023-10-27 03:33:41.148 
Epoch 983/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2460, val_MinusLogProbMetric: 28.2460

Epoch 983: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2460 - val_MinusLogProbMetric: 28.2460 - lr: 3.9063e-06 - 43s/epoch - 220ms/step
Epoch 984/1000
2023-10-27 03:34:23.990 
Epoch 984/1000 
	 loss: 27.4734, MinusLogProbMetric: 27.4734, val_loss: 28.2348, val_MinusLogProbMetric: 28.2348

Epoch 984: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4734 - MinusLogProbMetric: 27.4734 - val_loss: 28.2348 - val_MinusLogProbMetric: 28.2348 - lr: 3.9063e-06 - 43s/epoch - 219ms/step
Epoch 985/1000
2023-10-27 03:35:07.474 
Epoch 985/1000 
	 loss: 27.4733, MinusLogProbMetric: 27.4733, val_loss: 28.2342, val_MinusLogProbMetric: 28.2342

Epoch 985: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4733 - MinusLogProbMetric: 27.4733 - val_loss: 28.2342 - val_MinusLogProbMetric: 28.2342 - lr: 3.9063e-06 - 43s/epoch - 222ms/step
Epoch 986/1000
2023-10-27 03:35:49.835 
Epoch 986/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2299, val_MinusLogProbMetric: 28.2299

Epoch 986: val_loss did not improve from 28.22955
196/196 - 42s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2299 - val_MinusLogProbMetric: 28.2299 - lr: 3.9063e-06 - 42s/epoch - 216ms/step
Epoch 987/1000
2023-10-27 03:36:32.903 
Epoch 987/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2322, val_MinusLogProbMetric: 28.2322

Epoch 987: val_loss did not improve from 28.22955
196/196 - 43s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2322 - val_MinusLogProbMetric: 28.2322 - lr: 3.9063e-06 - 43s/epoch - 220ms/step
Epoch 988/1000
2023-10-27 03:37:17.187 
Epoch 988/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 988: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 989/1000
2023-10-27 03:38:01.926 
Epoch 989/1000 
	 loss: 27.4733, MinusLogProbMetric: 27.4733, val_loss: 28.2365, val_MinusLogProbMetric: 28.2365

Epoch 989: val_loss did not improve from 28.22955
196/196 - 45s - loss: 27.4733 - MinusLogProbMetric: 27.4733 - val_loss: 28.2365 - val_MinusLogProbMetric: 28.2365 - lr: 3.9063e-06 - 45s/epoch - 228ms/step
Epoch 990/1000
2023-10-27 03:38:46.542 
Epoch 990/1000 
	 loss: 27.4730, MinusLogProbMetric: 27.4730, val_loss: 28.2369, val_MinusLogProbMetric: 28.2369

Epoch 990: val_loss did not improve from 28.22955
196/196 - 45s - loss: 27.4730 - MinusLogProbMetric: 27.4730 - val_loss: 28.2369 - val_MinusLogProbMetric: 28.2369 - lr: 3.9063e-06 - 45s/epoch - 228ms/step
Epoch 991/1000
2023-10-27 03:39:30.138 
Epoch 991/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2321, val_MinusLogProbMetric: 28.2321

Epoch 991: val_loss did not improve from 28.22955
196/196 - 44s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2321 - val_MinusLogProbMetric: 28.2321 - lr: 3.9063e-06 - 44s/epoch - 222ms/step
Epoch 992/1000
2023-10-27 03:40:13.995 
Epoch 992/1000 
	 loss: 27.4730, MinusLogProbMetric: 27.4730, val_loss: 28.2290, val_MinusLogProbMetric: 28.2290

Epoch 992: val_loss improved from 28.22955 to 28.22905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_388/weights/best_weights.h5
196/196 - 45s - loss: 27.4730 - MinusLogProbMetric: 27.4730 - val_loss: 28.2290 - val_MinusLogProbMetric: 28.2290 - lr: 3.9063e-06 - 45s/epoch - 228ms/step
Epoch 993/1000
2023-10-27 03:40:59.274 
Epoch 993/1000 
	 loss: 27.4733, MinusLogProbMetric: 27.4733, val_loss: 28.2295, val_MinusLogProbMetric: 28.2295

Epoch 993: val_loss did not improve from 28.22905
196/196 - 44s - loss: 27.4733 - MinusLogProbMetric: 27.4733 - val_loss: 28.2295 - val_MinusLogProbMetric: 28.2295 - lr: 3.9063e-06 - 44s/epoch - 226ms/step
Epoch 994/1000
2023-10-27 03:41:43.285 
Epoch 994/1000 
	 loss: 27.4729, MinusLogProbMetric: 27.4729, val_loss: 28.2340, val_MinusLogProbMetric: 28.2340

Epoch 994: val_loss did not improve from 28.22905
196/196 - 44s - loss: 27.4729 - MinusLogProbMetric: 27.4729 - val_loss: 28.2340 - val_MinusLogProbMetric: 28.2340 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 995/1000
2023-10-27 03:42:27.215 
Epoch 995/1000 
	 loss: 27.4737, MinusLogProbMetric: 27.4737, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 995: val_loss did not improve from 28.22905
196/196 - 44s - loss: 27.4737 - MinusLogProbMetric: 27.4737 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 3.9063e-06 - 44s/epoch - 224ms/step
Epoch 996/1000
2023-10-27 03:43:11.236 
Epoch 996/1000 
	 loss: 27.4723, MinusLogProbMetric: 27.4723, val_loss: 28.2315, val_MinusLogProbMetric: 28.2315

Epoch 996: val_loss did not improve from 28.22905
196/196 - 44s - loss: 27.4723 - MinusLogProbMetric: 27.4723 - val_loss: 28.2315 - val_MinusLogProbMetric: 28.2315 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 997/1000
2023-10-27 03:43:54.576 
Epoch 997/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2340, val_MinusLogProbMetric: 28.2340

Epoch 997: val_loss did not improve from 28.22905
196/196 - 43s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2340 - val_MinusLogProbMetric: 28.2340 - lr: 3.9063e-06 - 43s/epoch - 221ms/step
Epoch 998/1000
2023-10-27 03:44:38.618 
Epoch 998/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2344, val_MinusLogProbMetric: 28.2344

Epoch 998: val_loss did not improve from 28.22905
196/196 - 44s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2344 - val_MinusLogProbMetric: 28.2344 - lr: 3.9063e-06 - 44s/epoch - 225ms/step
Epoch 999/1000
2023-10-27 03:45:22.130 
Epoch 999/1000 
	 loss: 27.4735, MinusLogProbMetric: 27.4735, val_loss: 28.2333, val_MinusLogProbMetric: 28.2333

Epoch 999: val_loss did not improve from 28.22905
196/196 - 44s - loss: 27.4735 - MinusLogProbMetric: 27.4735 - val_loss: 28.2333 - val_MinusLogProbMetric: 28.2333 - lr: 3.9063e-06 - 44s/epoch - 222ms/step
Epoch 1000/1000
2023-10-27 03:46:05.853 
Epoch 1000/1000 
	 loss: 27.4732, MinusLogProbMetric: 27.4732, val_loss: 28.2360, val_MinusLogProbMetric: 28.2360

Epoch 1000: val_loss did not improve from 28.22905
196/196 - 44s - loss: 27.4732 - MinusLogProbMetric: 27.4732 - val_loss: 28.2360 - val_MinusLogProbMetric: 28.2360 - lr: 3.9063e-06 - 44s/epoch - 223ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 926.
Model trained in 42758.14 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.05 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.29 s.
===========
Run 388/720 done in 42766.02 s.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

===========
Generating train data for run 399.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_227"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_228 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7ff14b6d64d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3058e5660>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3058e5660>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4af3163b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff348397be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff1402f0640>, <keras.callbacks.ModelCheckpoint object at 0x7ff1402f0700>, <keras.callbacks.EarlyStopping object at 0x7ff1402f0970>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1402f09a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff1402f05e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 03:46:19.066326
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:49:06.113 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 167s/epoch - 851ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 399.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_238"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_239 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7ff30c48b610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff30c4f6c80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff30c4f6c80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ffba809ef20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4f64be350>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4f64be8c0>, <keras.callbacks.ModelCheckpoint object at 0x7ff4f64be980>, <keras.callbacks.EarlyStopping object at 0x7ff4f64bebf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4f64bec20>, <keras.callbacks.TerminateOnNaN object at 0x7ff4f64be860>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 03:49:18.897849
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:52:07.505 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 168s/epoch - 859ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 399.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_249"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_250 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7ff4f61e8bb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff51d6b1b10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff51d6b1b10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff30ca98df0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff132939b70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff13293a0e0>, <keras.callbacks.ModelCheckpoint object at 0x7ff13293a1a0>, <keras.callbacks.EarlyStopping object at 0x7ff13293a410>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff13293a440>, <keras.callbacks.TerminateOnNaN object at 0x7ff13293a080>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 03:52:20.773406
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:55:08.397 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 167s/epoch - 853ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 3.703703703703703e-05.
===========
Generating train data for run 399.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_260"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_261 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7ff4f65e6a40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3155010f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3155010f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff143da7070>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4f650d330>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4f650d8a0>, <keras.callbacks.ModelCheckpoint object at 0x7ff4f650d960>, <keras.callbacks.EarlyStopping object at 0x7ff4f650dbd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4f650dc00>, <keras.callbacks.TerminateOnNaN object at 0x7ff4f650d840>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 03:55:17.683365
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 196007 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff51db7e320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:57:59.948 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 162s/epoch - 827ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.2345679012345677e-05.
===========
Generating train data for run 399.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_271"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_272 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7ffba81cab00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff31c6beec0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff31c6beec0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff314bb7280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4f65682b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff314beee30>, <keras.callbacks.ModelCheckpoint object at 0x7ff314beed10>, <keras.callbacks.EarlyStopping object at 0x7ff314befb50>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff314beed70>, <keras.callbacks.TerminateOnNaN object at 0x7ff314bee9b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 03:58:11.668261
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 196009 calls to <function Model.make_train_function.<locals>.train_function at 0x7ff51da25480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:00:46.931 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 155s/epoch - 791ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.115226337448558e-06.
===========
Generating train data for run 399.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_282"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_283 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7ff3c49f23e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4f47b9420>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4f47b9420>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff38c755420>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff38c793730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff38c790250>, <keras.callbacks.ModelCheckpoint object at 0x7ff38c791c60>, <keras.callbacks.EarlyStopping object at 0x7ff38c792b60>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff38c792fb0>, <keras.callbacks.TerminateOnNaN object at 0x7ff38c790400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 04:00:59.748711
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:03:40.949 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 161s/epoch - 822ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.3717421124828526e-06.
===========
Generating train data for run 399.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_293"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_294 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7ff4ad2f6380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1001dd480>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1001dd480>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4ad14ecb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4adad1690>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4adad1c00>, <keras.callbacks.ModelCheckpoint object at 0x7ff4adad1cc0>, <keras.callbacks.EarlyStopping object at 0x7ff4adad1f30>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4adad1f60>, <keras.callbacks.TerminateOnNaN object at 0x7ff4adad1ba0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 04:03:52.756196
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:06:37.561 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 165s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 165s/epoch - 840ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.572473708276175e-07.
===========
Generating train data for run 399.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_304"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_305 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7ffbb9976380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffbc206f520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffbc206f520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff31495e9b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff40c2294e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff40c229a50>, <keras.callbacks.ModelCheckpoint object at 0x7ff40c229b10>, <keras.callbacks.EarlyStopping object at 0x7ff40c229d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff40c229db0>, <keras.callbacks.TerminateOnNaN object at 0x7ff40c2299f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 04:06:47.923678
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:09:36.233 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 168s/epoch - 857ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.524157902758725e-07.
===========
Generating train data for run 399.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_315"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_316 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7ff4505cbf70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4502abfd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4502abfd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4f71c7340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4505c7760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4505c7cd0>, <keras.callbacks.ModelCheckpoint object at 0x7ff4505c7d90>, <keras.callbacks.EarlyStopping object at 0x7ff4505c7c70>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4505c7ca0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4505c7fa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 04:09:49.120635
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:12:37.111 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 168s/epoch - 855ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.0805263425290834e-08.
===========
Generating train data for run 399.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_326"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_327 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7ff2c15fbc40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff5a8600700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff5a8600700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff133d053f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff2c15eadd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff2c15eb340>, <keras.callbacks.ModelCheckpoint object at 0x7ff2c15eb400>, <keras.callbacks.EarlyStopping object at 0x7ff2c15eb670>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff2c15eb6a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff2c15eb2e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 04:12:47.617115
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:15:35.194 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 167s/epoch - 854ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.6935087808430278e-08.
===========
Generating train data for run 399.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_399/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_399
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_337"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_338 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7ffba7fb9270>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff5a82c6fe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff5a82c6fe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff45014bf40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff13386cf40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_399/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff13386d4b0>, <keras.callbacks.ModelCheckpoint object at 0x7ff13386d570>, <keras.callbacks.EarlyStopping object at 0x7ff13386d7e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff13386d810>, <keras.callbacks.TerminateOnNaN object at 0x7ff13386d450>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_399/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 399/720 with hyperparameters:
timestamp = 2023-10-27 04:15:46.324605
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:18:46.785 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7872.8555, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 180s - loss: nan - MinusLogProbMetric: 7872.8555 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 180s/epoch - 919ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.645029269476759e-09.
===========
Run 399/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

===========
Generating train data for run 403.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_343"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_344 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7ff51c359240>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4f47bcd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4f47bcd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff57878e560>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff450669ea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff45066add0>, <keras.callbacks.ModelCheckpoint object at 0x7ff45066ac20>, <keras.callbacks.EarlyStopping object at 0x7ff450669db0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4506de8c0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4506dd9f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:18:54.008313
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:20:11.620 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 77s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 77s/epoch - 395ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 403.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_349"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_350 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7ffb97b5e0b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffb9791bd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffb9791bd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ffb97ce7f40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ffb973e0df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ffb973e1360>, <keras.callbacks.ModelCheckpoint object at 0x7ffb973e1420>, <keras.callbacks.EarlyStopping object at 0x7ffb973e1690>, <keras.callbacks.ReduceLROnPlateau object at 0x7ffb973e16c0>, <keras.callbacks.TerminateOnNaN object at 0x7ffb973e1300>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:20:17.956145
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:21:44.343 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 86s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 86s/epoch - 439ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 403.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_355"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_356 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7ff14b490cd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffba8455240>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffba8455240>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff12167f280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1b5bf5b40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff1b5bf60b0>, <keras.callbacks.ModelCheckpoint object at 0x7ff1b5bf6170>, <keras.callbacks.EarlyStopping object at 0x7ff1b5bf63e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1b5bf6410>, <keras.callbacks.TerminateOnNaN object at 0x7ff1b5bf6050>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:21:50.953894
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:23:10.630 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 79s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 79s/epoch - 405ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 403.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_361"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_362 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7ff1230bf370>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1435918d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1435918d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff133cb58a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff14a6f1960>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff14a6f13c0>, <keras.callbacks.ModelCheckpoint object at 0x7ff14a6f07f0>, <keras.callbacks.EarlyStopping object at 0x7ff14a6f0cd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff14a6f0d60>, <keras.callbacks.TerminateOnNaN object at 0x7ff14a6f1390>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:23:15.953650
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:24:32.002 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 76s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 76s/epoch - 387ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 403.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_367"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_368 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7ffb86cc3d00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffb7e46ab00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffb7e46ab00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff451899c00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ffb7e450850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ffb7e450dc0>, <keras.callbacks.ModelCheckpoint object at 0x7ffb7e450e80>, <keras.callbacks.EarlyStopping object at 0x7ffb7e4510f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ffb7e451120>, <keras.callbacks.TerminateOnNaN object at 0x7ffb7e450d60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:24:36.935939
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:26:06.647 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 90s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 90s/epoch - 458ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 403.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_373"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_374 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7ff133a83250>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff10829c190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff10829c190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff305523850>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1b4fee200>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff1b4fee770>, <keras.callbacks.ModelCheckpoint object at 0x7ff1b4fee830>, <keras.callbacks.EarlyStopping object at 0x7ff1b4feeaa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1b4feead0>, <keras.callbacks.TerminateOnNaN object at 0x7ff1b4fee710>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:26:12.792477
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:27:38.354 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 85s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 85s/epoch - 436ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 403.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_379"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_380 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7ff48fb5ffd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff12202eaa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff12202eaa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff305f56b00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff48fba7820>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff48fba7d90>, <keras.callbacks.ModelCheckpoint object at 0x7ff48fba7e50>, <keras.callbacks.EarlyStopping object at 0x7ff48fba7fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff48fba7ee0>, <keras.callbacks.TerminateOnNaN object at 0x7ff48fba7f10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:27:45.161726
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:29:17.125 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 92s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 92s/epoch - 469ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 403.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_385"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_386 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7ff4f739abc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff103064040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff103064040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff1201e3820>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff45179ab60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff45179b520>, <keras.callbacks.ModelCheckpoint object at 0x7ff45179bb80>, <keras.callbacks.EarlyStopping object at 0x7ff451798dc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff451799a80>, <keras.callbacks.TerminateOnNaN object at 0x7ff45179b1f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:29:24.149964
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:30:43.828 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 80s/epoch - 406ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 403.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_391"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_392 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7ff142cca560>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff14bc26f50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff14bc26f50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff118f0aa70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff14a5e9de0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff14a5ea350>, <keras.callbacks.ModelCheckpoint object at 0x7ff14a5ea410>, <keras.callbacks.EarlyStopping object at 0x7ff14a5ea680>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff14a5ea6b0>, <keras.callbacks.TerminateOnNaN object at 0x7ff14a5ea2f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:30:50.767360
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:32:15.093 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 84s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 84s/epoch - 429ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 403.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_397"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_398 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7ff14a595a50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffb756f4640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffb756f4640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff100924fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ffb754233d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ffb75423940>, <keras.callbacks.ModelCheckpoint object at 0x7ffb75423a00>, <keras.callbacks.EarlyStopping object at 0x7ffb75423c70>, <keras.callbacks.ReduceLROnPlateau object at 0x7ffb75423ca0>, <keras.callbacks.TerminateOnNaN object at 0x7ffb754238e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:32:21.702301
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:33:51.982 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 90s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 90s/epoch - 460ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 403.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_403/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_403
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_403"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_404 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  1326510   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,326,510
Trainable params: 1,326,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7ff31d414d30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff31d497760>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff31d497760>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff45223bb80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1b6f27040>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_403/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff1b6f27700>, <keras.callbacks.ModelCheckpoint object at 0x7ff1b6f27c70>, <keras.callbacks.EarlyStopping object at 0x7ff1b6f26f20>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1b6f27e20>, <keras.callbacks.TerminateOnNaN object at 0x7ff1b6f27e50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_403/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 403/720 with hyperparameters:
timestamp = 2023-10-27 04:33:56.858897
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1326510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:35:12.862 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10837.4971, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 76s - loss: nan - MinusLogProbMetric: 10837.4971 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 76s/epoch - 387ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 403/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 404.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_404
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_409"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_410 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7ff3058ee4d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ffba866ba60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ffba866ba60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff1b6f4dd50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1225963e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff122596950>, <keras.callbacks.ModelCheckpoint object at 0x7ff122596a10>, <keras.callbacks.EarlyStopping object at 0x7ff122596c80>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff122596cb0>, <keras.callbacks.TerminateOnNaN object at 0x7ff1225968f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_404/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 404/720 with hyperparameters:
timestamp = 2023-10-27 04:35:18.823494
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:36:43.964 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 8600.8193, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 85s - loss: nan - MinusLogProbMetric: 8600.8193 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 85s/epoch - 434ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 404.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_404
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_415"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_416 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7ff12252a050>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff48553faf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff48553faf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff48ec3aec0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff48e61b8b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff48e61be20>, <keras.callbacks.ModelCheckpoint object at 0x7ff48e61bee0>, <keras.callbacks.EarlyStopping object at 0x7ff48e61bfa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff48e61bdc0>, <keras.callbacks.TerminateOnNaN object at 0x7ff48e61bdf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_404/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 404/720 with hyperparameters:
timestamp = 2023-10-27 04:36:50.429503
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 151: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:38:57.512 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2800.7505, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 2800.7505 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 127s/epoch - 648ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 404.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_404
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_421"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_422 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7ff119226ef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff11263ce80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff11263ce80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff4ace372b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff112f027a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff112f00730>, <keras.callbacks.ModelCheckpoint object at 0x7ff112f024a0>, <keras.callbacks.EarlyStopping object at 0x7ff112f034c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff112f02b00>, <keras.callbacks.TerminateOnNaN object at 0x7ff112f02500>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_404/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 404/720 with hyperparameters:
timestamp = 2023-10-27 04:39:04.576495
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
2023-10-27 04:41:10.577 
Epoch 1/1000 
	 loss: 3016.3958, MinusLogProbMetric: 3016.3958, val_loss: 821.7795, val_MinusLogProbMetric: 821.7795

Epoch 1: val_loss improved from inf to 821.77948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 127s - loss: 3016.3958 - MinusLogProbMetric: 3016.3958 - val_loss: 821.7795 - val_MinusLogProbMetric: 821.7795 - lr: 1.1111e-04 - 127s/epoch - 646ms/step
Epoch 2/1000
2023-10-27 04:41:55.066 
Epoch 2/1000 
	 loss: 630.2763, MinusLogProbMetric: 630.2763, val_loss: 520.8481, val_MinusLogProbMetric: 520.8481

Epoch 2: val_loss improved from 821.77948 to 520.84808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 630.2763 - MinusLogProbMetric: 630.2763 - val_loss: 520.8481 - val_MinusLogProbMetric: 520.8481 - lr: 1.1111e-04 - 44s/epoch - 227ms/step
Epoch 3/1000
2023-10-27 04:42:39.886 
Epoch 3/1000 
	 loss: 453.3943, MinusLogProbMetric: 453.3943, val_loss: 397.5880, val_MinusLogProbMetric: 397.5880

Epoch 3: val_loss improved from 520.84808 to 397.58804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 453.3943 - MinusLogProbMetric: 453.3943 - val_loss: 397.5880 - val_MinusLogProbMetric: 397.5880 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 4/1000
2023-10-27 04:43:23.792 
Epoch 4/1000 
	 loss: 373.7084, MinusLogProbMetric: 373.7084, val_loss: 358.3826, val_MinusLogProbMetric: 358.3826

Epoch 4: val_loss improved from 397.58804 to 358.38257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 373.7084 - MinusLogProbMetric: 373.7084 - val_loss: 358.3826 - val_MinusLogProbMetric: 358.3826 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 5/1000
2023-10-27 04:44:08.032 
Epoch 5/1000 
	 loss: 335.7001, MinusLogProbMetric: 335.7001, val_loss: 316.5086, val_MinusLogProbMetric: 316.5086

Epoch 5: val_loss improved from 358.38257 to 316.50858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 335.7001 - MinusLogProbMetric: 335.7001 - val_loss: 316.5086 - val_MinusLogProbMetric: 316.5086 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 6/1000
2023-10-27 04:44:52.132 
Epoch 6/1000 
	 loss: 304.2958, MinusLogProbMetric: 304.2958, val_loss: 290.8080, val_MinusLogProbMetric: 290.8080

Epoch 6: val_loss improved from 316.50858 to 290.80804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 304.2958 - MinusLogProbMetric: 304.2958 - val_loss: 290.8080 - val_MinusLogProbMetric: 290.8080 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 7/1000
2023-10-27 04:45:36.606 
Epoch 7/1000 
	 loss: 279.9545, MinusLogProbMetric: 279.9545, val_loss: 276.0309, val_MinusLogProbMetric: 276.0309

Epoch 7: val_loss improved from 290.80804 to 276.03085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 279.9545 - MinusLogProbMetric: 279.9545 - val_loss: 276.0309 - val_MinusLogProbMetric: 276.0309 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 8/1000
2023-10-27 04:46:20.724 
Epoch 8/1000 
	 loss: 260.2231, MinusLogProbMetric: 260.2231, val_loss: 252.4989, val_MinusLogProbMetric: 252.4989

Epoch 8: val_loss improved from 276.03085 to 252.49886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 260.2231 - MinusLogProbMetric: 260.2231 - val_loss: 252.4989 - val_MinusLogProbMetric: 252.4989 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 9/1000
2023-10-27 04:47:04.296 
Epoch 9/1000 
	 loss: 243.8833, MinusLogProbMetric: 243.8833, val_loss: 236.0260, val_MinusLogProbMetric: 236.0260

Epoch 9: val_loss improved from 252.49886 to 236.02599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 243.8833 - MinusLogProbMetric: 243.8833 - val_loss: 236.0260 - val_MinusLogProbMetric: 236.0260 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 10/1000
2023-10-27 04:47:48.767 
Epoch 10/1000 
	 loss: 231.2511, MinusLogProbMetric: 231.2511, val_loss: 225.9997, val_MinusLogProbMetric: 225.9997

Epoch 10: val_loss improved from 236.02599 to 225.99966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 231.2511 - MinusLogProbMetric: 231.2511 - val_loss: 225.9997 - val_MinusLogProbMetric: 225.9997 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 11/1000
2023-10-27 04:48:33.178 
Epoch 11/1000 
	 loss: 217.6454, MinusLogProbMetric: 217.6454, val_loss: 215.1404, val_MinusLogProbMetric: 215.1404

Epoch 11: val_loss improved from 225.99966 to 215.14041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 217.6454 - MinusLogProbMetric: 217.6454 - val_loss: 215.1404 - val_MinusLogProbMetric: 215.1404 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 12/1000
2023-10-27 04:49:15.393 
Epoch 12/1000 
	 loss: 206.6425, MinusLogProbMetric: 206.6425, val_loss: 202.5984, val_MinusLogProbMetric: 202.5984

Epoch 12: val_loss improved from 215.14041 to 202.59839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 206.6425 - MinusLogProbMetric: 206.6425 - val_loss: 202.5984 - val_MinusLogProbMetric: 202.5984 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 13/1000
2023-10-27 04:49:59.300 
Epoch 13/1000 
	 loss: 197.5833, MinusLogProbMetric: 197.5833, val_loss: 194.9582, val_MinusLogProbMetric: 194.9582

Epoch 13: val_loss improved from 202.59839 to 194.95824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 197.5833 - MinusLogProbMetric: 197.5833 - val_loss: 194.9582 - val_MinusLogProbMetric: 194.9582 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 14/1000
2023-10-27 04:50:42.991 
Epoch 14/1000 
	 loss: 189.5106, MinusLogProbMetric: 189.5106, val_loss: 185.0554, val_MinusLogProbMetric: 185.0554

Epoch 14: val_loss improved from 194.95824 to 185.05540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 189.5106 - MinusLogProbMetric: 189.5106 - val_loss: 185.0554 - val_MinusLogProbMetric: 185.0554 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 15/1000
2023-10-27 04:51:26.211 
Epoch 15/1000 
	 loss: 180.3228, MinusLogProbMetric: 180.3228, val_loss: 175.8915, val_MinusLogProbMetric: 175.8915

Epoch 15: val_loss improved from 185.05540 to 175.89151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 180.3228 - MinusLogProbMetric: 180.3228 - val_loss: 175.8915 - val_MinusLogProbMetric: 175.8915 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 16/1000
2023-10-27 04:52:10.117 
Epoch 16/1000 
	 loss: 173.3699, MinusLogProbMetric: 173.3699, val_loss: 170.7887, val_MinusLogProbMetric: 170.7887

Epoch 16: val_loss improved from 175.89151 to 170.78868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 173.3699 - MinusLogProbMetric: 173.3699 - val_loss: 170.7887 - val_MinusLogProbMetric: 170.7887 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 17/1000
2023-10-27 04:52:53.312 
Epoch 17/1000 
	 loss: 166.8660, MinusLogProbMetric: 166.8660, val_loss: 163.3894, val_MinusLogProbMetric: 163.3894

Epoch 17: val_loss improved from 170.78868 to 163.38943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 166.8660 - MinusLogProbMetric: 166.8660 - val_loss: 163.3894 - val_MinusLogProbMetric: 163.3894 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 18/1000
2023-10-27 04:53:37.100 
Epoch 18/1000 
	 loss: 159.9231, MinusLogProbMetric: 159.9231, val_loss: 157.1471, val_MinusLogProbMetric: 157.1471

Epoch 18: val_loss improved from 163.38943 to 157.14706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 159.9231 - MinusLogProbMetric: 159.9231 - val_loss: 157.1471 - val_MinusLogProbMetric: 157.1471 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 19/1000
2023-10-27 04:54:20.804 
Epoch 19/1000 
	 loss: 159.6847, MinusLogProbMetric: 159.6847, val_loss: 153.8749, val_MinusLogProbMetric: 153.8749

Epoch 19: val_loss improved from 157.14706 to 153.87486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 159.6847 - MinusLogProbMetric: 159.6847 - val_loss: 153.8749 - val_MinusLogProbMetric: 153.8749 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 20/1000
2023-10-27 04:55:04.547 
Epoch 20/1000 
	 loss: 149.7892, MinusLogProbMetric: 149.7892, val_loss: 147.7619, val_MinusLogProbMetric: 147.7619

Epoch 20: val_loss improved from 153.87486 to 147.76190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 149.7892 - MinusLogProbMetric: 149.7892 - val_loss: 147.7619 - val_MinusLogProbMetric: 147.7619 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 21/1000
2023-10-27 04:55:48.108 
Epoch 21/1000 
	 loss: 144.8049, MinusLogProbMetric: 144.8049, val_loss: 142.8738, val_MinusLogProbMetric: 142.8738

Epoch 21: val_loss improved from 147.76190 to 142.87378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 144.8049 - MinusLogProbMetric: 144.8049 - val_loss: 142.8738 - val_MinusLogProbMetric: 142.8738 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 22/1000
2023-10-27 04:56:32.032 
Epoch 22/1000 
	 loss: 141.1758, MinusLogProbMetric: 141.1758, val_loss: 139.2754, val_MinusLogProbMetric: 139.2754

Epoch 22: val_loss improved from 142.87378 to 139.27541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 141.1758 - MinusLogProbMetric: 141.1758 - val_loss: 139.2754 - val_MinusLogProbMetric: 139.2754 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 23/1000
2023-10-27 04:57:16.087 
Epoch 23/1000 
	 loss: 136.8045, MinusLogProbMetric: 136.8045, val_loss: 135.4305, val_MinusLogProbMetric: 135.4305

Epoch 23: val_loss improved from 139.27541 to 135.43047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 136.8045 - MinusLogProbMetric: 136.8045 - val_loss: 135.4305 - val_MinusLogProbMetric: 135.4305 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 24/1000
2023-10-27 04:58:00.066 
Epoch 24/1000 
	 loss: 133.2700, MinusLogProbMetric: 133.2700, val_loss: 132.0862, val_MinusLogProbMetric: 132.0862

Epoch 24: val_loss improved from 135.43047 to 132.08618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 133.2700 - MinusLogProbMetric: 133.2700 - val_loss: 132.0862 - val_MinusLogProbMetric: 132.0862 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 25/1000
2023-10-27 04:58:43.217 
Epoch 25/1000 
	 loss: 132.6845, MinusLogProbMetric: 132.6845, val_loss: 133.3367, val_MinusLogProbMetric: 133.3367

Epoch 25: val_loss did not improve from 132.08618
196/196 - 42s - loss: 132.6845 - MinusLogProbMetric: 132.6845 - val_loss: 133.3367 - val_MinusLogProbMetric: 133.3367 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 26/1000
2023-10-27 04:59:25.044 
Epoch 26/1000 
	 loss: 128.2681, MinusLogProbMetric: 128.2681, val_loss: 126.0576, val_MinusLogProbMetric: 126.0576

Epoch 26: val_loss improved from 132.08618 to 126.05764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 128.2681 - MinusLogProbMetric: 128.2681 - val_loss: 126.0576 - val_MinusLogProbMetric: 126.0576 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 27/1000
2023-10-27 05:00:08.811 
Epoch 27/1000 
	 loss: 124.2531, MinusLogProbMetric: 124.2531, val_loss: 123.1740, val_MinusLogProbMetric: 123.1740

Epoch 27: val_loss improved from 126.05764 to 123.17395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 124.2531 - MinusLogProbMetric: 124.2531 - val_loss: 123.1740 - val_MinusLogProbMetric: 123.1740 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 28/1000
2023-10-27 05:00:52.442 
Epoch 28/1000 
	 loss: 121.7059, MinusLogProbMetric: 121.7059, val_loss: 120.8963, val_MinusLogProbMetric: 120.8963

Epoch 28: val_loss improved from 123.17395 to 120.89630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 121.7059 - MinusLogProbMetric: 121.7059 - val_loss: 120.8963 - val_MinusLogProbMetric: 120.8963 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 29/1000
2023-10-27 05:01:36.390 
Epoch 29/1000 
	 loss: 118.7838, MinusLogProbMetric: 118.7838, val_loss: 117.6446, val_MinusLogProbMetric: 117.6446

Epoch 29: val_loss improved from 120.89630 to 117.64459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 118.7838 - MinusLogProbMetric: 118.7838 - val_loss: 117.6446 - val_MinusLogProbMetric: 117.6446 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 30/1000
2023-10-27 05:02:19.851 
Epoch 30/1000 
	 loss: 116.5663, MinusLogProbMetric: 116.5663, val_loss: 115.6801, val_MinusLogProbMetric: 115.6801

Epoch 30: val_loss improved from 117.64459 to 115.68008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 116.5663 - MinusLogProbMetric: 116.5663 - val_loss: 115.6801 - val_MinusLogProbMetric: 115.6801 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 31/1000
2023-10-27 05:03:03.499 
Epoch 31/1000 
	 loss: 114.4087, MinusLogProbMetric: 114.4087, val_loss: 113.1385, val_MinusLogProbMetric: 113.1385

Epoch 31: val_loss improved from 115.68008 to 113.13853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 114.4087 - MinusLogProbMetric: 114.4087 - val_loss: 113.1385 - val_MinusLogProbMetric: 113.1385 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 32/1000
2023-10-27 05:03:47.372 
Epoch 32/1000 
	 loss: 113.0168, MinusLogProbMetric: 113.0168, val_loss: 111.3703, val_MinusLogProbMetric: 111.3703

Epoch 32: val_loss improved from 113.13853 to 111.37029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 113.0168 - MinusLogProbMetric: 113.0168 - val_loss: 111.3703 - val_MinusLogProbMetric: 111.3703 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 33/1000
2023-10-27 05:04:31.583 
Epoch 33/1000 
	 loss: 110.2270, MinusLogProbMetric: 110.2270, val_loss: 110.3969, val_MinusLogProbMetric: 110.3969

Epoch 33: val_loss improved from 111.37029 to 110.39685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 110.2270 - MinusLogProbMetric: 110.2270 - val_loss: 110.3969 - val_MinusLogProbMetric: 110.3969 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 34/1000
2023-10-27 05:05:14.358 
Epoch 34/1000 
	 loss: 108.4657, MinusLogProbMetric: 108.4657, val_loss: 107.8136, val_MinusLogProbMetric: 107.8136

Epoch 34: val_loss improved from 110.39685 to 107.81360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 108.4657 - MinusLogProbMetric: 108.4657 - val_loss: 107.8136 - val_MinusLogProbMetric: 107.8136 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 35/1000
2023-10-27 05:05:58.083 
Epoch 35/1000 
	 loss: 106.4343, MinusLogProbMetric: 106.4343, val_loss: 105.9165, val_MinusLogProbMetric: 105.9165

Epoch 35: val_loss improved from 107.81360 to 105.91645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 106.4343 - MinusLogProbMetric: 106.4343 - val_loss: 105.9165 - val_MinusLogProbMetric: 105.9165 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 36/1000
2023-10-27 05:06:41.742 
Epoch 36/1000 
	 loss: 104.7943, MinusLogProbMetric: 104.7943, val_loss: 104.2074, val_MinusLogProbMetric: 104.2074

Epoch 36: val_loss improved from 105.91645 to 104.20739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 104.7943 - MinusLogProbMetric: 104.7943 - val_loss: 104.2074 - val_MinusLogProbMetric: 104.2074 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 37/1000
2023-10-27 05:07:26.069 
Epoch 37/1000 
	 loss: 103.2543, MinusLogProbMetric: 103.2543, val_loss: 102.4556, val_MinusLogProbMetric: 102.4556

Epoch 37: val_loss improved from 104.20739 to 102.45564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 103.2543 - MinusLogProbMetric: 103.2543 - val_loss: 102.4556 - val_MinusLogProbMetric: 102.4556 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 38/1000
2023-10-27 05:08:09.947 
Epoch 38/1000 
	 loss: 103.3290, MinusLogProbMetric: 103.3290, val_loss: 102.0802, val_MinusLogProbMetric: 102.0802

Epoch 38: val_loss improved from 102.45564 to 102.08018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 103.3290 - MinusLogProbMetric: 103.3290 - val_loss: 102.0802 - val_MinusLogProbMetric: 102.0802 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 39/1000
2023-10-27 05:08:53.414 
Epoch 39/1000 
	 loss: 100.6472, MinusLogProbMetric: 100.6472, val_loss: 100.3835, val_MinusLogProbMetric: 100.3835

Epoch 39: val_loss improved from 102.08018 to 100.38351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 100.6472 - MinusLogProbMetric: 100.6472 - val_loss: 100.3835 - val_MinusLogProbMetric: 100.3835 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 40/1000
2023-10-27 05:09:37.266 
Epoch 40/1000 
	 loss: 98.7383, MinusLogProbMetric: 98.7383, val_loss: 99.1661, val_MinusLogProbMetric: 99.1661

Epoch 40: val_loss improved from 100.38351 to 99.16607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 98.7383 - MinusLogProbMetric: 98.7383 - val_loss: 99.1661 - val_MinusLogProbMetric: 99.1661 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 41/1000
2023-10-27 05:10:20.572 
Epoch 41/1000 
	 loss: 97.4151, MinusLogProbMetric: 97.4151, val_loss: 97.3081, val_MinusLogProbMetric: 97.3081

Epoch 41: val_loss improved from 99.16607 to 97.30813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 97.4151 - MinusLogProbMetric: 97.4151 - val_loss: 97.3081 - val_MinusLogProbMetric: 97.3081 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 42/1000
2023-10-27 05:11:04.568 
Epoch 42/1000 
	 loss: 96.1780, MinusLogProbMetric: 96.1780, val_loss: 96.5312, val_MinusLogProbMetric: 96.5312

Epoch 42: val_loss improved from 97.30813 to 96.53122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 96.1780 - MinusLogProbMetric: 96.1780 - val_loss: 96.5312 - val_MinusLogProbMetric: 96.5312 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 43/1000
2023-10-27 05:11:48.254 
Epoch 43/1000 
	 loss: 95.0053, MinusLogProbMetric: 95.0053, val_loss: 95.3362, val_MinusLogProbMetric: 95.3362

Epoch 43: val_loss improved from 96.53122 to 95.33624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 95.0053 - MinusLogProbMetric: 95.0053 - val_loss: 95.3362 - val_MinusLogProbMetric: 95.3362 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 44/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:12:32.399 
Epoch 44/1000 
	 loss: 93.9194, MinusLogProbMetric: 93.9194, val_loss: 93.6630, val_MinusLogProbMetric: 93.6630

Epoch 44: val_loss improved from 95.33624 to 93.66299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 93.9194 - MinusLogProbMetric: 93.9194 - val_loss: 93.6630 - val_MinusLogProbMetric: 93.6630 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 45/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:13:16.424 
Epoch 45/1000 
	 loss: 92.7078, MinusLogProbMetric: 92.7078, val_loss: 92.5762, val_MinusLogProbMetric: 92.5762

Epoch 45: val_loss improved from 93.66299 to 92.57623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 92.7078 - MinusLogProbMetric: 92.7078 - val_loss: 92.5762 - val_MinusLogProbMetric: 92.5762 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 46/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:13:59.989 
Epoch 46/1000 
	 loss: 91.8506, MinusLogProbMetric: 91.8506, val_loss: 91.6732, val_MinusLogProbMetric: 91.6732

Epoch 46: val_loss improved from 92.57623 to 91.67324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 91.8506 - MinusLogProbMetric: 91.8506 - val_loss: 91.6732 - val_MinusLogProbMetric: 91.6732 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 47/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:14:43.852 
Epoch 47/1000 
	 loss: 90.6865, MinusLogProbMetric: 90.6865, val_loss: 90.4274, val_MinusLogProbMetric: 90.4274

Epoch 47: val_loss improved from 91.67324 to 90.42739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 90.6865 - MinusLogProbMetric: 90.6865 - val_loss: 90.4274 - val_MinusLogProbMetric: 90.4274 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 48/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:15:27.454 
Epoch 48/1000 
	 loss: 89.7681, MinusLogProbMetric: 89.7681, val_loss: 89.9549, val_MinusLogProbMetric: 89.9549

Epoch 48: val_loss improved from 90.42739 to 89.95490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 89.7681 - MinusLogProbMetric: 89.7681 - val_loss: 89.9549 - val_MinusLogProbMetric: 89.9549 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 49/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:16:09.920 
Epoch 49/1000 
	 loss: 88.8063, MinusLogProbMetric: 88.8063, val_loss: 88.8255, val_MinusLogProbMetric: 88.8255

Epoch 49: val_loss improved from 89.95490 to 88.82549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 88.8063 - MinusLogProbMetric: 88.8063 - val_loss: 88.8255 - val_MinusLogProbMetric: 88.8255 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 50/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:16:53.347 
Epoch 50/1000 
	 loss: 87.9897, MinusLogProbMetric: 87.9897, val_loss: 88.1696, val_MinusLogProbMetric: 88.1695

Epoch 50: val_loss improved from 88.82549 to 88.16958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 87.9897 - MinusLogProbMetric: 87.9897 - val_loss: 88.1696 - val_MinusLogProbMetric: 88.1695 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 51/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:17:35.640 
Epoch 51/1000 
	 loss: 87.1106, MinusLogProbMetric: 87.1106, val_loss: 86.9551, val_MinusLogProbMetric: 86.9550

Epoch 51: val_loss improved from 88.16958 to 86.95506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 87.1106 - MinusLogProbMetric: 87.1106 - val_loss: 86.9551 - val_MinusLogProbMetric: 86.9550 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 52/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:18:19.487 
Epoch 52/1000 
	 loss: 86.1943, MinusLogProbMetric: 86.1943, val_loss: 86.1889, val_MinusLogProbMetric: 86.1889

Epoch 52: val_loss improved from 86.95506 to 86.18893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 86.1943 - MinusLogProbMetric: 86.1943 - val_loss: 86.1889 - val_MinusLogProbMetric: 86.1889 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 53/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:19:03.304 
Epoch 53/1000 
	 loss: 85.3854, MinusLogProbMetric: 85.3854, val_loss: 85.3608, val_MinusLogProbMetric: 85.3608

Epoch 53: val_loss improved from 86.18893 to 85.36078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 85.3854 - MinusLogProbMetric: 85.3854 - val_loss: 85.3608 - val_MinusLogProbMetric: 85.3608 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 54/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:19:46.670 
Epoch 54/1000 
	 loss: 84.7271, MinusLogProbMetric: 84.7271, val_loss: 84.6651, val_MinusLogProbMetric: 84.6651

Epoch 54: val_loss improved from 85.36078 to 84.66508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 84.7271 - MinusLogProbMetric: 84.7271 - val_loss: 84.6651 - val_MinusLogProbMetric: 84.6651 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 55/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:20:29.747 
Epoch 55/1000 
	 loss: 83.9294, MinusLogProbMetric: 83.9294, val_loss: 83.8056, val_MinusLogProbMetric: 83.8055

Epoch 55: val_loss improved from 84.66508 to 83.80556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 83.9294 - MinusLogProbMetric: 83.9294 - val_loss: 83.8056 - val_MinusLogProbMetric: 83.8055 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 56/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:21:13.082 
Epoch 56/1000 
	 loss: 83.1041, MinusLogProbMetric: 83.1041, val_loss: 83.3261, val_MinusLogProbMetric: 83.3261

Epoch 56: val_loss improved from 83.80556 to 83.32609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 83.1041 - MinusLogProbMetric: 83.1041 - val_loss: 83.3261 - val_MinusLogProbMetric: 83.3261 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 57/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:21:56.188 
Epoch 57/1000 
	 loss: 82.7222, MinusLogProbMetric: 82.7222, val_loss: 82.6099, val_MinusLogProbMetric: 82.6099

Epoch 57: val_loss improved from 83.32609 to 82.60991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 82.7222 - MinusLogProbMetric: 82.7222 - val_loss: 82.6099 - val_MinusLogProbMetric: 82.6099 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 58/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:22:39.461 
Epoch 58/1000 
	 loss: 81.7923, MinusLogProbMetric: 81.7923, val_loss: 81.9789, val_MinusLogProbMetric: 81.9789

Epoch 58: val_loss improved from 82.60991 to 81.97888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 81.7923 - MinusLogProbMetric: 81.7923 - val_loss: 81.9789 - val_MinusLogProbMetric: 81.9789 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 59/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:23:22.782 
Epoch 59/1000 
	 loss: 81.0210, MinusLogProbMetric: 81.0210, val_loss: 81.2634, val_MinusLogProbMetric: 81.2633

Epoch 59: val_loss improved from 81.97888 to 81.26335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 81.0210 - MinusLogProbMetric: 81.0210 - val_loss: 81.2634 - val_MinusLogProbMetric: 81.2633 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 60/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:24:06.634 
Epoch 60/1000 
	 loss: 80.5277, MinusLogProbMetric: 80.5277, val_loss: 80.5342, val_MinusLogProbMetric: 80.5342

Epoch 60: val_loss improved from 81.26335 to 80.53420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 80.5277 - MinusLogProbMetric: 80.5277 - val_loss: 80.5342 - val_MinusLogProbMetric: 80.5342 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 61/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:24:50.215 
Epoch 61/1000 
	 loss: 79.9490, MinusLogProbMetric: 79.9490, val_loss: 80.2895, val_MinusLogProbMetric: 80.2895

Epoch 61: val_loss improved from 80.53420 to 80.28948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 79.9490 - MinusLogProbMetric: 79.9490 - val_loss: 80.2895 - val_MinusLogProbMetric: 80.2895 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 62/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:25:33.128 
Epoch 62/1000 
	 loss: 79.1202, MinusLogProbMetric: 79.1202, val_loss: 79.2806, val_MinusLogProbMetric: 79.2806

Epoch 62: val_loss improved from 80.28948 to 79.28063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 79.1202 - MinusLogProbMetric: 79.1202 - val_loss: 79.2806 - val_MinusLogProbMetric: 79.2806 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 63/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:26:08.478 
Epoch 63/1000 
	 loss: 78.5530, MinusLogProbMetric: 78.5530, val_loss: 78.9388, val_MinusLogProbMetric: 78.9387

Epoch 63: val_loss improved from 79.28063 to 78.93876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 35s - loss: 78.5530 - MinusLogProbMetric: 78.5530 - val_loss: 78.9388 - val_MinusLogProbMetric: 78.9387 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 64/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:26:42.199 
Epoch 64/1000 
	 loss: 78.0550, MinusLogProbMetric: 78.0550, val_loss: 78.2598, val_MinusLogProbMetric: 78.2598

Epoch 64: val_loss improved from 78.93876 to 78.25982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 34s - loss: 78.0550 - MinusLogProbMetric: 78.0550 - val_loss: 78.2598 - val_MinusLogProbMetric: 78.2598 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 65/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:27:16.676 
Epoch 65/1000 
	 loss: 77.4340, MinusLogProbMetric: 77.4340, val_loss: 77.9245, val_MinusLogProbMetric: 77.9245

Epoch 65: val_loss improved from 78.25982 to 77.92449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 35s - loss: 77.4340 - MinusLogProbMetric: 77.4340 - val_loss: 77.9245 - val_MinusLogProbMetric: 77.9245 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 66/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:27:58.794 
Epoch 66/1000 
	 loss: 76.9105, MinusLogProbMetric: 76.9105, val_loss: 77.2964, val_MinusLogProbMetric: 77.2964

Epoch 66: val_loss improved from 77.92449 to 77.29643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 76.9105 - MinusLogProbMetric: 76.9105 - val_loss: 77.2964 - val_MinusLogProbMetric: 77.2964 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 67/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:28:42.838 
Epoch 67/1000 
	 loss: 76.2566, MinusLogProbMetric: 76.2566, val_loss: 76.3478, val_MinusLogProbMetric: 76.3478

Epoch 67: val_loss improved from 77.29643 to 76.34780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 76.2566 - MinusLogProbMetric: 76.2566 - val_loss: 76.3478 - val_MinusLogProbMetric: 76.3478 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 68/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:29:25.357 
Epoch 68/1000 
	 loss: 75.7331, MinusLogProbMetric: 75.7331, val_loss: 75.9826, val_MinusLogProbMetric: 75.9826

Epoch 68: val_loss improved from 76.34780 to 75.98257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 75.7331 - MinusLogProbMetric: 75.7331 - val_loss: 75.9826 - val_MinusLogProbMetric: 75.9826 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 69/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:30:08.247 
Epoch 69/1000 
	 loss: 75.2865, MinusLogProbMetric: 75.2865, val_loss: 75.7842, val_MinusLogProbMetric: 75.7841

Epoch 69: val_loss improved from 75.98257 to 75.78416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 75.2865 - MinusLogProbMetric: 75.2865 - val_loss: 75.7842 - val_MinusLogProbMetric: 75.7841 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 70/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:30:51.756 
Epoch 70/1000 
	 loss: 74.7744, MinusLogProbMetric: 74.7744, val_loss: 74.9844, val_MinusLogProbMetric: 74.9844

Epoch 70: val_loss improved from 75.78416 to 74.98439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 74.7744 - MinusLogProbMetric: 74.7744 - val_loss: 74.9844 - val_MinusLogProbMetric: 74.9844 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 71/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:31:35.289 
Epoch 71/1000 
	 loss: 74.3672, MinusLogProbMetric: 74.3672, val_loss: 74.3573, val_MinusLogProbMetric: 74.3573

Epoch 71: val_loss improved from 74.98439 to 74.35732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 74.3672 - MinusLogProbMetric: 74.3672 - val_loss: 74.3573 - val_MinusLogProbMetric: 74.3573 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 72/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:32:18.396 
Epoch 72/1000 
	 loss: 73.8652, MinusLogProbMetric: 73.8652, val_loss: 73.9983, val_MinusLogProbMetric: 73.9983

Epoch 72: val_loss improved from 74.35732 to 73.99832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 73.8652 - MinusLogProbMetric: 73.8652 - val_loss: 73.9983 - val_MinusLogProbMetric: 73.9983 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 73/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:33:01.971 
Epoch 73/1000 
	 loss: 73.4367, MinusLogProbMetric: 73.4367, val_loss: 73.7564, val_MinusLogProbMetric: 73.7564

Epoch 73: val_loss improved from 73.99832 to 73.75638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 73.4367 - MinusLogProbMetric: 73.4367 - val_loss: 73.7564 - val_MinusLogProbMetric: 73.7564 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 74/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:33:44.938 
Epoch 74/1000 
	 loss: 72.9065, MinusLogProbMetric: 72.9065, val_loss: 73.3754, val_MinusLogProbMetric: 73.3753

Epoch 74: val_loss improved from 73.75638 to 73.37536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 72.9065 - MinusLogProbMetric: 72.9065 - val_loss: 73.3754 - val_MinusLogProbMetric: 73.3753 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 75/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:34:29.046 
Epoch 75/1000 
	 loss: 72.5897, MinusLogProbMetric: 72.5897, val_loss: 72.7349, val_MinusLogProbMetric: 72.7349

Epoch 75: val_loss improved from 73.37536 to 72.73493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 72.5897 - MinusLogProbMetric: 72.5897 - val_loss: 72.7349 - val_MinusLogProbMetric: 72.7349 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 76/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:35:13.513 
Epoch 76/1000 
	 loss: 72.0943, MinusLogProbMetric: 72.0943, val_loss: 72.5683, val_MinusLogProbMetric: 72.5683

Epoch 76: val_loss improved from 72.73493 to 72.56829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 72.0943 - MinusLogProbMetric: 72.0943 - val_loss: 72.5683 - val_MinusLogProbMetric: 72.5683 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 77/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:35:58.010 
Epoch 77/1000 
	 loss: 72.4178, MinusLogProbMetric: 72.4178, val_loss: 72.6839, val_MinusLogProbMetric: 72.6839

Epoch 77: val_loss did not improve from 72.56829
196/196 - 43s - loss: 72.4178 - MinusLogProbMetric: 72.4178 - val_loss: 72.6839 - val_MinusLogProbMetric: 72.6839 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 78/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:36:41.594 
Epoch 78/1000 
	 loss: 71.5343, MinusLogProbMetric: 71.5343, val_loss: 72.1101, val_MinusLogProbMetric: 72.1101

Epoch 78: val_loss improved from 72.56829 to 72.11007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 71.5343 - MinusLogProbMetric: 71.5343 - val_loss: 72.1101 - val_MinusLogProbMetric: 72.1101 - lr: 1.1111e-04 - 45s/epoch - 227ms/step
Epoch 79/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:37:25.987 
Epoch 79/1000 
	 loss: 70.8697, MinusLogProbMetric: 70.8697, val_loss: 71.0641, val_MinusLogProbMetric: 71.0641

Epoch 79: val_loss improved from 72.11007 to 71.06407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 70.8697 - MinusLogProbMetric: 70.8697 - val_loss: 71.0641 - val_MinusLogProbMetric: 71.0641 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 80/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:38:09.433 
Epoch 80/1000 
	 loss: 70.7054, MinusLogProbMetric: 70.7054, val_loss: 72.5484, val_MinusLogProbMetric: 72.5484

Epoch 80: val_loss did not improve from 71.06407
196/196 - 43s - loss: 70.7054 - MinusLogProbMetric: 70.7054 - val_loss: 72.5484 - val_MinusLogProbMetric: 72.5484 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 81/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:38:52.584 
Epoch 81/1000 
	 loss: 70.5072, MinusLogProbMetric: 70.5072, val_loss: 70.4569, val_MinusLogProbMetric: 70.4569

Epoch 81: val_loss improved from 71.06407 to 70.45689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 70.5072 - MinusLogProbMetric: 70.5072 - val_loss: 70.4569 - val_MinusLogProbMetric: 70.4569 - lr: 1.1111e-04 - 44s/epoch - 225ms/step
Epoch 82/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:39:36.126 
Epoch 82/1000 
	 loss: 70.5079, MinusLogProbMetric: 70.5079, val_loss: 70.0909, val_MinusLogProbMetric: 70.0909

Epoch 82: val_loss improved from 70.45689 to 70.09090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 70.5079 - MinusLogProbMetric: 70.5079 - val_loss: 70.0909 - val_MinusLogProbMetric: 70.0909 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 83/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:40:20.090 
Epoch 83/1000 
	 loss: 69.5449, MinusLogProbMetric: 69.5449, val_loss: 69.6604, val_MinusLogProbMetric: 69.6604

Epoch 83: val_loss improved from 70.09090 to 69.66042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 69.5449 - MinusLogProbMetric: 69.5449 - val_loss: 69.6604 - val_MinusLogProbMetric: 69.6604 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 84/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:41:02.552 
Epoch 84/1000 
	 loss: 69.2329, MinusLogProbMetric: 69.2329, val_loss: 69.2887, val_MinusLogProbMetric: 69.2887

Epoch 84: val_loss improved from 69.66042 to 69.28869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 69.2329 - MinusLogProbMetric: 69.2329 - val_loss: 69.2887 - val_MinusLogProbMetric: 69.2887 - lr: 1.1111e-04 - 42s/epoch - 217ms/step
Epoch 85/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:41:46.350 
Epoch 85/1000 
	 loss: 68.7270, MinusLogProbMetric: 68.7270, val_loss: 69.1350, val_MinusLogProbMetric: 69.1350

Epoch 85: val_loss improved from 69.28869 to 69.13500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 68.7270 - MinusLogProbMetric: 68.7270 - val_loss: 69.1350 - val_MinusLogProbMetric: 69.1350 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 86/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:42:30.621 
Epoch 86/1000 
	 loss: 68.4689, MinusLogProbMetric: 68.4689, val_loss: 68.8548, val_MinusLogProbMetric: 68.8548

Epoch 86: val_loss improved from 69.13500 to 68.85477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 68.4689 - MinusLogProbMetric: 68.4689 - val_loss: 68.8548 - val_MinusLogProbMetric: 68.8548 - lr: 1.1111e-04 - 44s/epoch - 226ms/step
Epoch 87/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:43:14.114 
Epoch 87/1000 
	 loss: 68.1503, MinusLogProbMetric: 68.1503, val_loss: 68.9272, val_MinusLogProbMetric: 68.9272

Epoch 87: val_loss did not improve from 68.85477
196/196 - 43s - loss: 68.1503 - MinusLogProbMetric: 68.1503 - val_loss: 68.9272 - val_MinusLogProbMetric: 68.9272 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 88/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:43:56.925 
Epoch 88/1000 
	 loss: 67.9470, MinusLogProbMetric: 67.9470, val_loss: 67.9944, val_MinusLogProbMetric: 67.9944

Epoch 88: val_loss improved from 68.85477 to 67.99442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 67.9470 - MinusLogProbMetric: 67.9470 - val_loss: 67.9944 - val_MinusLogProbMetric: 67.9944 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 89/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:44:41.321 
Epoch 89/1000 
	 loss: 67.5308, MinusLogProbMetric: 67.5308, val_loss: 68.3684, val_MinusLogProbMetric: 68.3684

Epoch 89: val_loss did not improve from 67.99442
196/196 - 44s - loss: 67.5308 - MinusLogProbMetric: 67.5308 - val_loss: 68.3684 - val_MinusLogProbMetric: 68.3684 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 90/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:45:24.684 
Epoch 90/1000 
	 loss: 67.2895, MinusLogProbMetric: 67.2895, val_loss: 68.6948, val_MinusLogProbMetric: 68.6948

Epoch 90: val_loss did not improve from 67.99442
196/196 - 43s - loss: 67.2895 - MinusLogProbMetric: 67.2895 - val_loss: 68.6948 - val_MinusLogProbMetric: 68.6948 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 91/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:46:07.777 
Epoch 91/1000 
	 loss: 67.0452, MinusLogProbMetric: 67.0452, val_loss: 67.1779, val_MinusLogProbMetric: 67.1779

Epoch 91: val_loss improved from 67.99442 to 67.17789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 67.0452 - MinusLogProbMetric: 67.0452 - val_loss: 67.1779 - val_MinusLogProbMetric: 67.1779 - lr: 1.1111e-04 - 44s/epoch - 224ms/step
Epoch 92/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:46:52.062 
Epoch 92/1000 
	 loss: 66.6938, MinusLogProbMetric: 66.6938, val_loss: 67.2852, val_MinusLogProbMetric: 67.2852

Epoch 92: val_loss did not improve from 67.17789
196/196 - 43s - loss: 66.6938 - MinusLogProbMetric: 66.6938 - val_loss: 67.2852 - val_MinusLogProbMetric: 67.2852 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 93/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:47:35.544 
Epoch 93/1000 
	 loss: 66.2855, MinusLogProbMetric: 66.2855, val_loss: 66.5028, val_MinusLogProbMetric: 66.5028

Epoch 93: val_loss improved from 67.17789 to 66.50283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 66.2855 - MinusLogProbMetric: 66.2855 - val_loss: 66.5028 - val_MinusLogProbMetric: 66.5028 - lr: 1.1111e-04 - 45s/epoch - 228ms/step
Epoch 94/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:48:19.616 
Epoch 94/1000 
	 loss: 66.4545, MinusLogProbMetric: 66.4545, val_loss: 66.7153, val_MinusLogProbMetric: 66.7153

Epoch 94: val_loss did not improve from 66.50283
196/196 - 43s - loss: 66.4545 - MinusLogProbMetric: 66.4545 - val_loss: 66.7153 - val_MinusLogProbMetric: 66.7153 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 95/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:49:02.217 
Epoch 95/1000 
	 loss: 65.8359, MinusLogProbMetric: 65.8359, val_loss: 69.5401, val_MinusLogProbMetric: 69.5401

Epoch 95: val_loss did not improve from 66.50283
196/196 - 43s - loss: 65.8359 - MinusLogProbMetric: 65.8359 - val_loss: 69.5401 - val_MinusLogProbMetric: 69.5401 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 96/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:49:44.589 
Epoch 96/1000 
	 loss: 65.6538, MinusLogProbMetric: 65.6538, val_loss: 66.3427, val_MinusLogProbMetric: 66.3427

Epoch 96: val_loss improved from 66.50283 to 66.34268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 65.6538 - MinusLogProbMetric: 65.6538 - val_loss: 66.3427 - val_MinusLogProbMetric: 66.3427 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 97/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:50:27.529 
Epoch 97/1000 
	 loss: 65.4962, MinusLogProbMetric: 65.4962, val_loss: 65.8834, val_MinusLogProbMetric: 65.8834

Epoch 97: val_loss improved from 66.34268 to 65.88345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 65.4962 - MinusLogProbMetric: 65.4962 - val_loss: 65.8834 - val_MinusLogProbMetric: 65.8834 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 98/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:51:10.902 
Epoch 98/1000 
	 loss: 64.9675, MinusLogProbMetric: 64.9675, val_loss: 65.3399, val_MinusLogProbMetric: 65.3399

Epoch 98: val_loss improved from 65.88345 to 65.33992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 64.9675 - MinusLogProbMetric: 64.9675 - val_loss: 65.3399 - val_MinusLogProbMetric: 65.3399 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 99/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:51:54.429 
Epoch 99/1000 
	 loss: 65.0029, MinusLogProbMetric: 65.0029, val_loss: 65.0695, val_MinusLogProbMetric: 65.0695

Epoch 99: val_loss improved from 65.33992 to 65.06949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 65.0029 - MinusLogProbMetric: 65.0029 - val_loss: 65.0695 - val_MinusLogProbMetric: 65.0695 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 100/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:52:37.816 
Epoch 100/1000 
	 loss: 64.4536, MinusLogProbMetric: 64.4536, val_loss: 65.1385, val_MinusLogProbMetric: 65.1385

Epoch 100: val_loss did not improve from 65.06949
196/196 - 43s - loss: 64.4536 - MinusLogProbMetric: 64.4536 - val_loss: 65.1385 - val_MinusLogProbMetric: 65.1385 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 101/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:53:20.168 
Epoch 101/1000 
	 loss: 64.3757, MinusLogProbMetric: 64.3757, val_loss: 65.1559, val_MinusLogProbMetric: 65.1559

Epoch 101: val_loss did not improve from 65.06949
196/196 - 42s - loss: 64.3757 - MinusLogProbMetric: 64.3757 - val_loss: 65.1559 - val_MinusLogProbMetric: 65.1559 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 102/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:54:02.030 
Epoch 102/1000 
	 loss: 64.5669, MinusLogProbMetric: 64.5669, val_loss: 65.2104, val_MinusLogProbMetric: 65.2104

Epoch 102: val_loss did not improve from 65.06949
196/196 - 42s - loss: 64.5669 - MinusLogProbMetric: 64.5669 - val_loss: 65.2104 - val_MinusLogProbMetric: 65.2104 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 103/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:54:44.431 
Epoch 103/1000 
	 loss: 64.1662, MinusLogProbMetric: 64.1662, val_loss: 64.2361, val_MinusLogProbMetric: 64.2361

Epoch 103: val_loss improved from 65.06949 to 64.23613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 64.1662 - MinusLogProbMetric: 64.1662 - val_loss: 64.2361 - val_MinusLogProbMetric: 64.2361 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 104/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:55:27.512 
Epoch 104/1000 
	 loss: 63.5826, MinusLogProbMetric: 63.5826, val_loss: 64.3253, val_MinusLogProbMetric: 64.3253

Epoch 104: val_loss did not improve from 64.23613
196/196 - 42s - loss: 63.5826 - MinusLogProbMetric: 63.5826 - val_loss: 64.3253 - val_MinusLogProbMetric: 64.3253 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 105/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:56:09.804 
Epoch 105/1000 
	 loss: 63.5399, MinusLogProbMetric: 63.5399, val_loss: 63.8670, val_MinusLogProbMetric: 63.8670

Epoch 105: val_loss improved from 64.23613 to 63.86704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 63.5399 - MinusLogProbMetric: 63.5399 - val_loss: 63.8670 - val_MinusLogProbMetric: 63.8670 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 106/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:56:52.647 
Epoch 106/1000 
	 loss: 63.2783, MinusLogProbMetric: 63.2783, val_loss: 64.7455, val_MinusLogProbMetric: 64.7455

Epoch 106: val_loss did not improve from 63.86704
196/196 - 42s - loss: 63.2783 - MinusLogProbMetric: 63.2783 - val_loss: 64.7455 - val_MinusLogProbMetric: 64.7455 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 107/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:57:35.276 
Epoch 107/1000 
	 loss: 63.0626, MinusLogProbMetric: 63.0626, val_loss: 63.7720, val_MinusLogProbMetric: 63.7720

Epoch 107: val_loss improved from 63.86704 to 63.77198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 63.0626 - MinusLogProbMetric: 63.0626 - val_loss: 63.7720 - val_MinusLogProbMetric: 63.7720 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 108/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:58:18.825 
Epoch 108/1000 
	 loss: 63.1278, MinusLogProbMetric: 63.1278, val_loss: 63.6489, val_MinusLogProbMetric: 63.6489

Epoch 108: val_loss improved from 63.77198 to 63.64893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 63.1278 - MinusLogProbMetric: 63.1278 - val_loss: 63.6489 - val_MinusLogProbMetric: 63.6489 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 109/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:59:02.023 
Epoch 109/1000 
	 loss: 62.6229, MinusLogProbMetric: 62.6229, val_loss: 63.0774, val_MinusLogProbMetric: 63.0774

Epoch 109: val_loss improved from 63.64893 to 63.07743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 62.6229 - MinusLogProbMetric: 62.6229 - val_loss: 63.0774 - val_MinusLogProbMetric: 63.0774 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 110/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 05:59:44.675 
Epoch 110/1000 
	 loss: 62.4621, MinusLogProbMetric: 62.4621, val_loss: 62.6322, val_MinusLogProbMetric: 62.6322

Epoch 110: val_loss improved from 63.07743 to 62.63224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 62.4621 - MinusLogProbMetric: 62.4621 - val_loss: 62.6322 - val_MinusLogProbMetric: 62.6322 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 111/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:00:27.873 
Epoch 111/1000 
	 loss: 62.6839, MinusLogProbMetric: 62.6839, val_loss: 63.7563, val_MinusLogProbMetric: 63.7563

Epoch 111: val_loss did not improve from 62.63224
196/196 - 42s - loss: 62.6839 - MinusLogProbMetric: 62.6839 - val_loss: 63.7563 - val_MinusLogProbMetric: 63.7563 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 112/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:01:09.985 
Epoch 112/1000 
	 loss: 64.1312, MinusLogProbMetric: 64.1312, val_loss: 65.4773, val_MinusLogProbMetric: 65.4773

Epoch 112: val_loss did not improve from 62.63224
196/196 - 42s - loss: 64.1312 - MinusLogProbMetric: 64.1312 - val_loss: 65.4773 - val_MinusLogProbMetric: 65.4773 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 113/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:01:52.563 
Epoch 113/1000 
	 loss: 63.5177, MinusLogProbMetric: 63.5177, val_loss: 63.3730, val_MinusLogProbMetric: 63.3730

Epoch 113: val_loss did not improve from 62.63224
196/196 - 43s - loss: 63.5177 - MinusLogProbMetric: 63.5177 - val_loss: 63.3730 - val_MinusLogProbMetric: 63.3730 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 114/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:02:35.170 
Epoch 114/1000 
	 loss: 62.6654, MinusLogProbMetric: 62.6654, val_loss: 63.1702, val_MinusLogProbMetric: 63.1702

Epoch 114: val_loss did not improve from 62.63224
196/196 - 43s - loss: 62.6654 - MinusLogProbMetric: 62.6654 - val_loss: 63.1702 - val_MinusLogProbMetric: 63.1702 - lr: 1.1111e-04 - 43s/epoch - 217ms/step
Epoch 115/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:03:17.004 
Epoch 115/1000 
	 loss: 62.5575, MinusLogProbMetric: 62.5575, val_loss: 62.9033, val_MinusLogProbMetric: 62.9033

Epoch 115: val_loss did not improve from 62.63224
196/196 - 42s - loss: 62.5575 - MinusLogProbMetric: 62.5575 - val_loss: 62.9033 - val_MinusLogProbMetric: 62.9033 - lr: 1.1111e-04 - 42s/epoch - 213ms/step
Epoch 116/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:03:59.115 
Epoch 116/1000 
	 loss: 62.1831, MinusLogProbMetric: 62.1831, val_loss: 62.2872, val_MinusLogProbMetric: 62.2872

Epoch 116: val_loss improved from 62.63224 to 62.28719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 62.1831 - MinusLogProbMetric: 62.1831 - val_loss: 62.2872 - val_MinusLogProbMetric: 62.2872 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 117/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:04:42.063 
Epoch 117/1000 
	 loss: 61.9534, MinusLogProbMetric: 61.9534, val_loss: 62.7052, val_MinusLogProbMetric: 62.7052

Epoch 117: val_loss did not improve from 62.28719
196/196 - 42s - loss: 61.9534 - MinusLogProbMetric: 61.9534 - val_loss: 62.7052 - val_MinusLogProbMetric: 62.7052 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 118/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:05:24.478 
Epoch 118/1000 
	 loss: 61.6436, MinusLogProbMetric: 61.6436, val_loss: 62.8288, val_MinusLogProbMetric: 62.8288

Epoch 118: val_loss did not improve from 62.28719
196/196 - 42s - loss: 61.6436 - MinusLogProbMetric: 61.6436 - val_loss: 62.8288 - val_MinusLogProbMetric: 62.8288 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 119/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:06:07.156 
Epoch 119/1000 
	 loss: 61.3894, MinusLogProbMetric: 61.3894, val_loss: 61.6657, val_MinusLogProbMetric: 61.6657

Epoch 119: val_loss improved from 62.28719 to 61.66573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 61.3894 - MinusLogProbMetric: 61.3894 - val_loss: 61.6657 - val_MinusLogProbMetric: 61.6657 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 120/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:06:50.221 
Epoch 120/1000 
	 loss: 61.4455, MinusLogProbMetric: 61.4455, val_loss: 63.1664, val_MinusLogProbMetric: 63.1664

Epoch 120: val_loss did not improve from 61.66573
196/196 - 42s - loss: 61.4455 - MinusLogProbMetric: 61.4455 - val_loss: 63.1664 - val_MinusLogProbMetric: 63.1664 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 121/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:07:32.800 
Epoch 121/1000 
	 loss: 60.9984, MinusLogProbMetric: 60.9984, val_loss: 61.5522, val_MinusLogProbMetric: 61.5522

Epoch 121: val_loss improved from 61.66573 to 61.55220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 60.9984 - MinusLogProbMetric: 60.9984 - val_loss: 61.5522 - val_MinusLogProbMetric: 61.5522 - lr: 1.1111e-04 - 43s/epoch - 222ms/step
Epoch 122/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:08:15.725 
Epoch 122/1000 
	 loss: 61.1087, MinusLogProbMetric: 61.1087, val_loss: 61.4311, val_MinusLogProbMetric: 61.4311

Epoch 122: val_loss improved from 61.55220 to 61.43114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 61.1087 - MinusLogProbMetric: 61.1087 - val_loss: 61.4311 - val_MinusLogProbMetric: 61.4311 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 123/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:08:57.907 
Epoch 123/1000 
	 loss: 60.6270, MinusLogProbMetric: 60.6270, val_loss: 61.6870, val_MinusLogProbMetric: 61.6870

Epoch 123: val_loss did not improve from 61.43114
196/196 - 41s - loss: 60.6270 - MinusLogProbMetric: 60.6270 - val_loss: 61.6870 - val_MinusLogProbMetric: 61.6870 - lr: 1.1111e-04 - 41s/epoch - 211ms/step
Epoch 124/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:09:40.144 
Epoch 124/1000 
	 loss: 60.4947, MinusLogProbMetric: 60.4947, val_loss: 61.5641, val_MinusLogProbMetric: 61.5641

Epoch 124: val_loss did not improve from 61.43114
196/196 - 42s - loss: 60.4947 - MinusLogProbMetric: 60.4947 - val_loss: 61.5641 - val_MinusLogProbMetric: 61.5641 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 125/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:10:22.041 
Epoch 125/1000 
	 loss: 60.4977, MinusLogProbMetric: 60.4977, val_loss: 67.3749, val_MinusLogProbMetric: 67.3749

Epoch 125: val_loss did not improve from 61.43114
196/196 - 42s - loss: 60.4977 - MinusLogProbMetric: 60.4977 - val_loss: 67.3749 - val_MinusLogProbMetric: 67.3749 - lr: 1.1111e-04 - 42s/epoch - 214ms/step
Epoch 126/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:11:04.184 
Epoch 126/1000 
	 loss: 60.8805, MinusLogProbMetric: 60.8805, val_loss: 61.7284, val_MinusLogProbMetric: 61.7284

Epoch 126: val_loss did not improve from 61.43114
196/196 - 42s - loss: 60.8805 - MinusLogProbMetric: 60.8805 - val_loss: 61.7284 - val_MinusLogProbMetric: 61.7284 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 127/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:11:46.989 
Epoch 127/1000 
	 loss: 60.1379, MinusLogProbMetric: 60.1379, val_loss: 60.3955, val_MinusLogProbMetric: 60.3955

Epoch 127: val_loss improved from 61.43114 to 60.39545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 60.1379 - MinusLogProbMetric: 60.1379 - val_loss: 60.3955 - val_MinusLogProbMetric: 60.3955 - lr: 1.1111e-04 - 44s/epoch - 223ms/step
Epoch 128/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:12:30.244 
Epoch 128/1000 
	 loss: 59.9960, MinusLogProbMetric: 59.9960, val_loss: 62.0998, val_MinusLogProbMetric: 62.0998

Epoch 128: val_loss did not improve from 60.39545
196/196 - 42s - loss: 59.9960 - MinusLogProbMetric: 59.9960 - val_loss: 62.0998 - val_MinusLogProbMetric: 62.0998 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 129/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:13:12.743 
Epoch 129/1000 
	 loss: 59.5524, MinusLogProbMetric: 59.5524, val_loss: 59.9820, val_MinusLogProbMetric: 59.9820

Epoch 129: val_loss improved from 60.39545 to 59.98201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 59.5524 - MinusLogProbMetric: 59.5524 - val_loss: 59.9820 - val_MinusLogProbMetric: 59.9820 - lr: 1.1111e-04 - 44s/epoch - 222ms/step
Epoch 130/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:13:56.710 
Epoch 130/1000 
	 loss: 60.6177, MinusLogProbMetric: 60.6177, val_loss: 60.1026, val_MinusLogProbMetric: 60.1026

Epoch 130: val_loss did not improve from 59.98201
196/196 - 43s - loss: 60.6177 - MinusLogProbMetric: 60.6177 - val_loss: 60.1026 - val_MinusLogProbMetric: 60.1026 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 131/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:14:39.124 
Epoch 131/1000 
	 loss: 59.6121, MinusLogProbMetric: 59.6121, val_loss: 60.0192, val_MinusLogProbMetric: 60.0192

Epoch 131: val_loss did not improve from 59.98201
196/196 - 42s - loss: 59.6121 - MinusLogProbMetric: 59.6121 - val_loss: 60.0192 - val_MinusLogProbMetric: 60.0192 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 132/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:15:21.951 
Epoch 132/1000 
	 loss: 59.6284, MinusLogProbMetric: 59.6284, val_loss: 60.2165, val_MinusLogProbMetric: 60.2165

Epoch 132: val_loss did not improve from 59.98201
196/196 - 43s - loss: 59.6284 - MinusLogProbMetric: 59.6284 - val_loss: 60.2165 - val_MinusLogProbMetric: 60.2165 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 133/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:16:03.204 
Epoch 133/1000 
	 loss: 59.5557, MinusLogProbMetric: 59.5557, val_loss: 59.3605, val_MinusLogProbMetric: 59.3605

Epoch 133: val_loss improved from 59.98201 to 59.36052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 59.5557 - MinusLogProbMetric: 59.5557 - val_loss: 59.3605 - val_MinusLogProbMetric: 59.3605 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 134/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:16:46.522 
Epoch 134/1000 
	 loss: 59.1127, MinusLogProbMetric: 59.1127, val_loss: 59.4717, val_MinusLogProbMetric: 59.4717

Epoch 134: val_loss did not improve from 59.36052
196/196 - 42s - loss: 59.1127 - MinusLogProbMetric: 59.1127 - val_loss: 59.4717 - val_MinusLogProbMetric: 59.4717 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 135/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:17:28.585 
Epoch 135/1000 
	 loss: 58.6306, MinusLogProbMetric: 58.6306, val_loss: 59.3811, val_MinusLogProbMetric: 59.3811

Epoch 135: val_loss did not improve from 59.36052
196/196 - 42s - loss: 58.6306 - MinusLogProbMetric: 58.6306 - val_loss: 59.3811 - val_MinusLogProbMetric: 59.3811 - lr: 1.1111e-04 - 42s/epoch - 215ms/step
Epoch 136/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:18:11.059 
Epoch 136/1000 
	 loss: 58.7602, MinusLogProbMetric: 58.7602, val_loss: 59.1633, val_MinusLogProbMetric: 59.1633

Epoch 136: val_loss improved from 59.36052 to 59.16334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 58.7602 - MinusLogProbMetric: 58.7602 - val_loss: 59.1633 - val_MinusLogProbMetric: 59.1633 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 137/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:18:54.363 
Epoch 137/1000 
	 loss: 58.9886, MinusLogProbMetric: 58.9886, val_loss: 62.9444, val_MinusLogProbMetric: 62.9445

Epoch 137: val_loss did not improve from 59.16334
196/196 - 42s - loss: 58.9886 - MinusLogProbMetric: 58.9886 - val_loss: 62.9444 - val_MinusLogProbMetric: 62.9445 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 138/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:19:36.215 
Epoch 138/1000 
	 loss: 59.0088, MinusLogProbMetric: 59.0088, val_loss: 59.1233, val_MinusLogProbMetric: 59.1233

Epoch 138: val_loss improved from 59.16334 to 59.12330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 59.0088 - MinusLogProbMetric: 59.0088 - val_loss: 59.1233 - val_MinusLogProbMetric: 59.1233 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 139/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:20:18.706 
Epoch 139/1000 
	 loss: 58.5094, MinusLogProbMetric: 58.5094, val_loss: 60.4562, val_MinusLogProbMetric: 60.4562

Epoch 139: val_loss did not improve from 59.12330
196/196 - 41s - loss: 58.5094 - MinusLogProbMetric: 58.5094 - val_loss: 60.4562 - val_MinusLogProbMetric: 60.4562 - lr: 1.1111e-04 - 41s/epoch - 212ms/step
Epoch 140/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:21:01.130 
Epoch 140/1000 
	 loss: 58.8100, MinusLogProbMetric: 58.8100, val_loss: 59.0036, val_MinusLogProbMetric: 59.0036

Epoch 140: val_loss improved from 59.12330 to 59.00362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 58.8100 - MinusLogProbMetric: 58.8100 - val_loss: 59.0036 - val_MinusLogProbMetric: 59.0036 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 141/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:21:44.415 
Epoch 141/1000 
	 loss: 58.1301, MinusLogProbMetric: 58.1301, val_loss: 58.3730, val_MinusLogProbMetric: 58.3730

Epoch 141: val_loss improved from 59.00362 to 58.37297, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 58.1301 - MinusLogProbMetric: 58.1301 - val_loss: 58.3730 - val_MinusLogProbMetric: 58.3730 - lr: 1.1111e-04 - 43s/epoch - 221ms/step
Epoch 142/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:22:27.744 
Epoch 142/1000 
	 loss: 58.2437, MinusLogProbMetric: 58.2437, val_loss: 59.7559, val_MinusLogProbMetric: 59.7559

Epoch 142: val_loss did not improve from 58.37297
196/196 - 42s - loss: 58.2437 - MinusLogProbMetric: 58.2437 - val_loss: 59.7559 - val_MinusLogProbMetric: 59.7559 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 143/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:23:09.993 
Epoch 143/1000 
	 loss: 58.6273, MinusLogProbMetric: 58.6273, val_loss: 58.4968, val_MinusLogProbMetric: 58.4968

Epoch 143: val_loss did not improve from 58.37297
196/196 - 42s - loss: 58.6273 - MinusLogProbMetric: 58.6273 - val_loss: 58.4968 - val_MinusLogProbMetric: 58.4968 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 144/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:23:52.248 
Epoch 144/1000 
	 loss: 58.1245, MinusLogProbMetric: 58.1245, val_loss: 58.7219, val_MinusLogProbMetric: 58.7220

Epoch 144: val_loss did not improve from 58.37297
196/196 - 42s - loss: 58.1245 - MinusLogProbMetric: 58.1245 - val_loss: 58.7219 - val_MinusLogProbMetric: 58.7220 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 145/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:24:35.209 
Epoch 145/1000 
	 loss: 58.0357, MinusLogProbMetric: 58.0357, val_loss: 60.7550, val_MinusLogProbMetric: 60.7550

Epoch 145: val_loss did not improve from 58.37297
196/196 - 43s - loss: 58.0357 - MinusLogProbMetric: 58.0357 - val_loss: 60.7550 - val_MinusLogProbMetric: 60.7550 - lr: 1.1111e-04 - 43s/epoch - 219ms/step
Epoch 146/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:25:17.948 
Epoch 146/1000 
	 loss: 58.1826, MinusLogProbMetric: 58.1826, val_loss: 58.4366, val_MinusLogProbMetric: 58.4366

Epoch 146: val_loss did not improve from 58.37297
196/196 - 43s - loss: 58.1826 - MinusLogProbMetric: 58.1826 - val_loss: 58.4366 - val_MinusLogProbMetric: 58.4366 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 147/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:26:01.036 
Epoch 147/1000 
	 loss: 58.2772, MinusLogProbMetric: 58.2772, val_loss: 59.6441, val_MinusLogProbMetric: 59.6441

Epoch 147: val_loss did not improve from 58.37297
196/196 - 43s - loss: 58.2772 - MinusLogProbMetric: 58.2772 - val_loss: 59.6441 - val_MinusLogProbMetric: 59.6441 - lr: 1.1111e-04 - 43s/epoch - 220ms/step
Epoch 148/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:26:43.382 
Epoch 148/1000 
	 loss: 57.8226, MinusLogProbMetric: 57.8226, val_loss: 58.7911, val_MinusLogProbMetric: 58.7911

Epoch 148: val_loss did not improve from 58.37297
196/196 - 42s - loss: 57.8226 - MinusLogProbMetric: 57.8226 - val_loss: 58.7911 - val_MinusLogProbMetric: 58.7911 - lr: 1.1111e-04 - 42s/epoch - 216ms/step
Epoch 149/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:27:26.089 
Epoch 149/1000 
	 loss: 57.5760, MinusLogProbMetric: 57.5760, val_loss: 58.5457, val_MinusLogProbMetric: 58.5457

Epoch 149: val_loss did not improve from 58.37297
196/196 - 43s - loss: 57.5760 - MinusLogProbMetric: 57.5760 - val_loss: 58.5457 - val_MinusLogProbMetric: 58.5457 - lr: 1.1111e-04 - 43s/epoch - 218ms/step
Epoch 150/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 115: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 06:27:52.237 
Epoch 150/1000 
	 loss: nan, MinusLogProbMetric: 77.7670, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 150: val_loss did not improve from 58.37297
196/196 - 26s - loss: nan - MinusLogProbMetric: 77.7670 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 26s/epoch - 133ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 404.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_404
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_427"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_428 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7ff42cd47bb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d40fe200>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4d40fe200>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff113882f80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff40f2c7fd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff40f2fc580>, <keras.callbacks.ModelCheckpoint object at 0x7ff40f2fc640>, <keras.callbacks.EarlyStopping object at 0x7ff40f2fc8b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff40f2fc8e0>, <keras.callbacks.TerminateOnNaN object at 0x7ff40f2fc520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 404/720 with hyperparameters:
timestamp = 2023-10-27 06:27:59.577153
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 06:29:19.461 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 616.5256, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 80s - loss: nan - MinusLogProbMetric: 616.5256 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 80s/epoch - 406ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 404.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_404/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_404
self.data_kwargs: {'seed': 0}
self.x_data: [[3.1988616e+00 6.4596548e+00 3.4971681e+00 ... 8.4548540e+00
  9.9900160e+00 9.2984438e+00]
 [7.2895646e+00 2.2435577e+00 7.1758137e+00 ... 1.8451822e+00
  6.6551912e-01 4.2349429e+00]
 [4.5507236e+00 8.1209593e+00 6.0352001e+00 ... 4.3117601e-01
  8.2851753e+00 2.7205688e-01]
 ...
 [6.4571786e+00 3.3359065e+00 7.2818942e+00 ... 2.8372436e+00
  1.6785768e-01 3.7896473e+00]
 [7.0386562e+00 1.5919313e+00 7.3295102e+00 ... 3.2355165e+00
  9.2371702e-03 4.3022585e+00]
 [3.2339067e+00 7.1636057e+00 3.9843028e+00 ... 8.2155190e+00
  9.6683483e+00 8.1347198e+00]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_433"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_434 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  2971950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7ff12a23ada0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff51d487100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff51d487100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff3cd912a40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff12a2594e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff12a259a50>, <keras.callbacks.ModelCheckpoint object at 0x7ff12a259b10>, <keras.callbacks.EarlyStopping object at 0x7ff12a259d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff12a259db0>, <keras.callbacks.TerminateOnNaN object at 0x7ff12a2599f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.1117342 ,  6.724983  ,  4.738387  , ...,  9.18981   ,
         9.966645  ,  9.417953  ],
       [ 3.070945  ,  7.03914   ,  3.3850236 , ...,  8.421514  ,
         9.388257  , 10.0439825 ],
       [ 3.3494005 ,  6.6208525 ,  4.071571  , ...,  8.689273  ,
         9.733164  , 10.183063  ],
       ...,
       [ 5.8936186 ,  7.0103445 ,  5.6668954 , ...,  0.10688139,
         8.324543  ,  0.35676813],
       [ 5.1697054 ,  6.843064  ,  5.9020343 , ...,  0.7361994 ,
         8.296032  , -0.01713873],
       [ 6.558123  ,  7.169092  ,  6.083838  , ...,  0.36036867,
         8.29056   , -0.10401164]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 404/720 with hyperparameters:
timestamp = 2023-10-27 06:29:26.799729
ndims = 100
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.1988616   6.459655    3.497168    2.5258198   0.5146842   0.01144499
  8.660946    4.4687414   5.3369775   9.222479    9.825627    2.0164165
  5.8595243   2.7608967   0.01777877  7.9863286   3.0862474   4.038523
  4.8480043   8.324652    6.3692403   8.858448    2.7943842   8.503383
  1.8646172   9.545225    7.280446    3.16205     9.42947     7.2309823
  2.5119016   2.2208848   5.641275    0.15324405  1.8451912   3.9893744
  3.7616286   4.3864183   1.6560541   7.271039    8.639502    0.42183757
  5.939365    1.6088052   6.496237    3.9659636   5.826187    2.751687
  2.041491    5.154163    4.2846527   9.414735    8.1881895   6.119909
  9.289563    1.0227115   5.5518694   5.898068    8.84132     3.1552873
  2.987031    1.1560615   0.7653631  10.124298    5.9141235   7.6628203
  3.1531022   6.0366597   1.2124346   4.589357    9.863814    8.792568
  3.4425902   9.722224    1.758931    9.752882    9.420302    8.509628
  5.7721124   9.332831    3.1424959   7.715396    7.1740627   0.08349583
  3.1046915   1.3896693   9.787189    4.757281    4.951331    4.9443026
  1.1299293   2.0995584   9.307955    1.7638296   4.8817134   0.51830494
  1.3658354   8.454854    9.990016    9.298444  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:31:44.195 
Epoch 1/1000 
	 loss: 66.0431, MinusLogProbMetric: 66.0431, val_loss: 57.7090, val_MinusLogProbMetric: 57.7090

Epoch 1: val_loss improved from inf to 57.70897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 138s - loss: 66.0431 - MinusLogProbMetric: 66.0431 - val_loss: 57.7090 - val_MinusLogProbMetric: 57.7090 - lr: 1.2346e-05 - 138s/epoch - 704ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:32:29.304 
Epoch 2/1000 
	 loss: 57.0418, MinusLogProbMetric: 57.0418, val_loss: 57.5270, val_MinusLogProbMetric: 57.5270

Epoch 2: val_loss improved from 57.70897 to 57.52697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 57.0418 - MinusLogProbMetric: 57.0418 - val_loss: 57.5270 - val_MinusLogProbMetric: 57.5270 - lr: 1.2346e-05 - 45s/epoch - 230ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:33:13.925 
Epoch 3/1000 
	 loss: 57.0894, MinusLogProbMetric: 57.0894, val_loss: 58.5935, val_MinusLogProbMetric: 58.5935

Epoch 3: val_loss did not improve from 57.52697
196/196 - 44s - loss: 57.0894 - MinusLogProbMetric: 57.0894 - val_loss: 58.5935 - val_MinusLogProbMetric: 58.5935 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:33:57.054 
Epoch 4/1000 
	 loss: 56.7368, MinusLogProbMetric: 56.7368, val_loss: 57.6574, val_MinusLogProbMetric: 57.6574

Epoch 4: val_loss did not improve from 57.52697
196/196 - 43s - loss: 56.7368 - MinusLogProbMetric: 56.7368 - val_loss: 57.6574 - val_MinusLogProbMetric: 57.6574 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:34:41.511 
Epoch 5/1000 
	 loss: 56.5952, MinusLogProbMetric: 56.5952, val_loss: 57.6281, val_MinusLogProbMetric: 57.6281

Epoch 5: val_loss did not improve from 57.52697
196/196 - 44s - loss: 56.5952 - MinusLogProbMetric: 56.5952 - val_loss: 57.6281 - val_MinusLogProbMetric: 57.6281 - lr: 1.2346e-05 - 44s/epoch - 227ms/step
Epoch 6/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:35:24.974 
Epoch 6/1000 
	 loss: 56.2585, MinusLogProbMetric: 56.2585, val_loss: 56.6173, val_MinusLogProbMetric: 56.6173

Epoch 6: val_loss improved from 57.52697 to 56.61726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 56.2585 - MinusLogProbMetric: 56.2585 - val_loss: 56.6173 - val_MinusLogProbMetric: 56.6173 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:36:09.489 
Epoch 7/1000 
	 loss: 56.1935, MinusLogProbMetric: 56.1935, val_loss: 56.4445, val_MinusLogProbMetric: 56.4445

Epoch 7: val_loss improved from 56.61726 to 56.44453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 56.1935 - MinusLogProbMetric: 56.1935 - val_loss: 56.4445 - val_MinusLogProbMetric: 56.4445 - lr: 1.2346e-05 - 45s/epoch - 227ms/step
Epoch 8/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:36:54.550 
Epoch 8/1000 
	 loss: 55.7475, MinusLogProbMetric: 55.7475, val_loss: 56.4278, val_MinusLogProbMetric: 56.4278

Epoch 8: val_loss improved from 56.44453 to 56.42780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 55.7475 - MinusLogProbMetric: 55.7475 - val_loss: 56.4278 - val_MinusLogProbMetric: 56.4278 - lr: 1.2346e-05 - 45s/epoch - 230ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:37:39.062 
Epoch 9/1000 
	 loss: 55.4513, MinusLogProbMetric: 55.4513, val_loss: 56.2764, val_MinusLogProbMetric: 56.2764

Epoch 9: val_loss improved from 56.42780 to 56.27642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 55.4513 - MinusLogProbMetric: 55.4513 - val_loss: 56.2764 - val_MinusLogProbMetric: 56.2764 - lr: 1.2346e-05 - 45s/epoch - 227ms/step
Epoch 10/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:38:23.518 
Epoch 10/1000 
	 loss: 55.6755, MinusLogProbMetric: 55.6755, val_loss: 56.2823, val_MinusLogProbMetric: 56.2823

Epoch 10: val_loss did not improve from 56.27642
196/196 - 44s - loss: 55.6755 - MinusLogProbMetric: 55.6755 - val_loss: 56.2823 - val_MinusLogProbMetric: 56.2823 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 11/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:39:07.454 
Epoch 11/1000 
	 loss: 55.1534, MinusLogProbMetric: 55.1534, val_loss: 56.2143, val_MinusLogProbMetric: 56.2143

Epoch 11: val_loss improved from 56.27642 to 56.21425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 55.1534 - MinusLogProbMetric: 55.1534 - val_loss: 56.2143 - val_MinusLogProbMetric: 56.2143 - lr: 1.2346e-05 - 45s/epoch - 229ms/step
Epoch 12/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:39:50.627 
Epoch 12/1000 
	 loss: 55.1108, MinusLogProbMetric: 55.1108, val_loss: 55.6145, val_MinusLogProbMetric: 55.6145

Epoch 12: val_loss improved from 56.21425 to 55.61449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 55.1108 - MinusLogProbMetric: 55.1108 - val_loss: 55.6145 - val_MinusLogProbMetric: 55.6145 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 13/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:40:35.177 
Epoch 13/1000 
	 loss: 54.8552, MinusLogProbMetric: 54.8552, val_loss: 55.5095, val_MinusLogProbMetric: 55.5095

Epoch 13: val_loss improved from 55.61449 to 55.50954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 54.8552 - MinusLogProbMetric: 54.8552 - val_loss: 55.5095 - val_MinusLogProbMetric: 55.5095 - lr: 1.2346e-05 - 45s/epoch - 227ms/step
Epoch 14/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:41:19.784 
Epoch 14/1000 
	 loss: 54.8437, MinusLogProbMetric: 54.8437, val_loss: 56.2960, val_MinusLogProbMetric: 56.2960

Epoch 14: val_loss did not improve from 55.50954
196/196 - 44s - loss: 54.8437 - MinusLogProbMetric: 54.8437 - val_loss: 56.2960 - val_MinusLogProbMetric: 56.2960 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 15/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:42:02.795 
Epoch 15/1000 
	 loss: 54.6848, MinusLogProbMetric: 54.6848, val_loss: 55.3928, val_MinusLogProbMetric: 55.3928

Epoch 15: val_loss improved from 55.50954 to 55.39275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 54.6848 - MinusLogProbMetric: 54.6848 - val_loss: 55.3928 - val_MinusLogProbMetric: 55.3928 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 16/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:42:47.155 
Epoch 16/1000 
	 loss: 54.3690, MinusLogProbMetric: 54.3690, val_loss: 55.0832, val_MinusLogProbMetric: 55.0832

Epoch 16: val_loss improved from 55.39275 to 55.08323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 54.3690 - MinusLogProbMetric: 54.3690 - val_loss: 55.0832 - val_MinusLogProbMetric: 55.0832 - lr: 1.2346e-05 - 44s/epoch - 227ms/step
Epoch 17/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:43:31.958 
Epoch 17/1000 
	 loss: 54.2085, MinusLogProbMetric: 54.2085, val_loss: 55.2754, val_MinusLogProbMetric: 55.2754

Epoch 17: val_loss did not improve from 55.08323
196/196 - 44s - loss: 54.2085 - MinusLogProbMetric: 54.2085 - val_loss: 55.2754 - val_MinusLogProbMetric: 55.2754 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 18/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:44:15.294 
Epoch 18/1000 
	 loss: 54.0663, MinusLogProbMetric: 54.0663, val_loss: 54.5662, val_MinusLogProbMetric: 54.5662

Epoch 18: val_loss improved from 55.08323 to 54.56615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 54.0663 - MinusLogProbMetric: 54.0663 - val_loss: 54.5662 - val_MinusLogProbMetric: 54.5662 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 19/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:45:00.244 
Epoch 19/1000 
	 loss: 53.9757, MinusLogProbMetric: 53.9757, val_loss: 54.8525, val_MinusLogProbMetric: 54.8525

Epoch 19: val_loss did not improve from 54.56615
196/196 - 44s - loss: 53.9757 - MinusLogProbMetric: 53.9757 - val_loss: 54.8525 - val_MinusLogProbMetric: 54.8525 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 20/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:45:43.207 
Epoch 20/1000 
	 loss: 53.9708, MinusLogProbMetric: 53.9708, val_loss: 54.4033, val_MinusLogProbMetric: 54.4033

Epoch 20: val_loss improved from 54.56615 to 54.40328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 53.9708 - MinusLogProbMetric: 53.9708 - val_loss: 54.4033 - val_MinusLogProbMetric: 54.4033 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 21/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:46:27.609 
Epoch 21/1000 
	 loss: 53.7012, MinusLogProbMetric: 53.7012, val_loss: 54.4754, val_MinusLogProbMetric: 54.4754

Epoch 21: val_loss did not improve from 54.40328
196/196 - 44s - loss: 53.7012 - MinusLogProbMetric: 53.7012 - val_loss: 54.4754 - val_MinusLogProbMetric: 54.4754 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 22/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:47:11.870 
Epoch 22/1000 
	 loss: 53.6386, MinusLogProbMetric: 53.6386, val_loss: 54.5929, val_MinusLogProbMetric: 54.5929

Epoch 22: val_loss did not improve from 54.40328
196/196 - 44s - loss: 53.6386 - MinusLogProbMetric: 53.6386 - val_loss: 54.5929 - val_MinusLogProbMetric: 54.5929 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 23/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:47:55.564 
Epoch 23/1000 
	 loss: 53.5435, MinusLogProbMetric: 53.5435, val_loss: 54.2738, val_MinusLogProbMetric: 54.2738

Epoch 23: val_loss improved from 54.40328 to 54.27377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 53.5435 - MinusLogProbMetric: 53.5435 - val_loss: 54.2738 - val_MinusLogProbMetric: 54.2738 - lr: 1.2346e-05 - 45s/epoch - 227ms/step
Epoch 24/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:48:39.949 
Epoch 24/1000 
	 loss: 53.3521, MinusLogProbMetric: 53.3521, val_loss: 53.9575, val_MinusLogProbMetric: 53.9575

Epoch 24: val_loss improved from 54.27377 to 53.95749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 53.3521 - MinusLogProbMetric: 53.3521 - val_loss: 53.9575 - val_MinusLogProbMetric: 53.9575 - lr: 1.2346e-05 - 44s/epoch - 227ms/step
Epoch 25/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:49:24.559 
Epoch 25/1000 
	 loss: 53.2460, MinusLogProbMetric: 53.2460, val_loss: 53.9370, val_MinusLogProbMetric: 53.9370

Epoch 25: val_loss improved from 53.95749 to 53.93697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 53.2460 - MinusLogProbMetric: 53.2460 - val_loss: 53.9370 - val_MinusLogProbMetric: 53.9370 - lr: 1.2346e-05 - 45s/epoch - 228ms/step
Epoch 26/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:50:09.719 
Epoch 26/1000 
	 loss: 53.3513, MinusLogProbMetric: 53.3513, val_loss: 53.8142, val_MinusLogProbMetric: 53.8142

Epoch 26: val_loss improved from 53.93697 to 53.81422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 53.3513 - MinusLogProbMetric: 53.3513 - val_loss: 53.8142 - val_MinusLogProbMetric: 53.8142 - lr: 1.2346e-05 - 45s/epoch - 230ms/step
Epoch 27/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:50:52.970 
Epoch 27/1000 
	 loss: 53.0900, MinusLogProbMetric: 53.0900, val_loss: 53.7093, val_MinusLogProbMetric: 53.7093

Epoch 27: val_loss improved from 53.81422 to 53.70930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 53.0900 - MinusLogProbMetric: 53.0900 - val_loss: 53.7093 - val_MinusLogProbMetric: 53.7093 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 28/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:51:36.490 
Epoch 28/1000 
	 loss: 52.9589, MinusLogProbMetric: 52.9589, val_loss: 53.6535, val_MinusLogProbMetric: 53.6535

Epoch 28: val_loss improved from 53.70930 to 53.65348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 52.9589 - MinusLogProbMetric: 52.9589 - val_loss: 53.6535 - val_MinusLogProbMetric: 53.6535 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 29/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:52:19.548 
Epoch 29/1000 
	 loss: 52.8094, MinusLogProbMetric: 52.8094, val_loss: 53.4921, val_MinusLogProbMetric: 53.4921

Epoch 29: val_loss improved from 53.65348 to 53.49212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 52.8094 - MinusLogProbMetric: 52.8094 - val_loss: 53.4921 - val_MinusLogProbMetric: 53.4921 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 30/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:53:03.556 
Epoch 30/1000 
	 loss: 52.7193, MinusLogProbMetric: 52.7193, val_loss: 53.0762, val_MinusLogProbMetric: 53.0763

Epoch 30: val_loss improved from 53.49212 to 53.07624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 52.7193 - MinusLogProbMetric: 52.7193 - val_loss: 53.0762 - val_MinusLogProbMetric: 53.0763 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 31/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:53:46.743 
Epoch 31/1000 
	 loss: 52.7236, MinusLogProbMetric: 52.7236, val_loss: 53.3849, val_MinusLogProbMetric: 53.3850

Epoch 31: val_loss did not improve from 53.07624
196/196 - 42s - loss: 52.7236 - MinusLogProbMetric: 52.7236 - val_loss: 53.3849 - val_MinusLogProbMetric: 53.3850 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 32/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:54:29.370 
Epoch 32/1000 
	 loss: 52.5671, MinusLogProbMetric: 52.5671, val_loss: 53.2850, val_MinusLogProbMetric: 53.2850

Epoch 32: val_loss did not improve from 53.07624
196/196 - 43s - loss: 52.5671 - MinusLogProbMetric: 52.5671 - val_loss: 53.2850 - val_MinusLogProbMetric: 53.2850 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 33/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:55:10.788 
Epoch 33/1000 
	 loss: 52.4644, MinusLogProbMetric: 52.4644, val_loss: 53.1736, val_MinusLogProbMetric: 53.1736

Epoch 33: val_loss did not improve from 53.07624
196/196 - 41s - loss: 52.4644 - MinusLogProbMetric: 52.4644 - val_loss: 53.1736 - val_MinusLogProbMetric: 53.1736 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 34/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:55:55.045 
Epoch 34/1000 
	 loss: 52.3483, MinusLogProbMetric: 52.3483, val_loss: 52.9687, val_MinusLogProbMetric: 52.9687

Epoch 34: val_loss improved from 53.07624 to 52.96869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 52.3483 - MinusLogProbMetric: 52.3483 - val_loss: 52.9687 - val_MinusLogProbMetric: 52.9687 - lr: 1.2346e-05 - 45s/epoch - 229ms/step
Epoch 35/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:56:39.306 
Epoch 35/1000 
	 loss: 52.3150, MinusLogProbMetric: 52.3150, val_loss: 52.7639, val_MinusLogProbMetric: 52.7639

Epoch 35: val_loss improved from 52.96869 to 52.76389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 52.3150 - MinusLogProbMetric: 52.3150 - val_loss: 52.7639 - val_MinusLogProbMetric: 52.7639 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 36/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:57:23.772 
Epoch 36/1000 
	 loss: 52.1211, MinusLogProbMetric: 52.1211, val_loss: 53.0503, val_MinusLogProbMetric: 53.0503

Epoch 36: val_loss did not improve from 52.76389
196/196 - 44s - loss: 52.1211 - MinusLogProbMetric: 52.1211 - val_loss: 53.0503 - val_MinusLogProbMetric: 53.0503 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 37/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:58:07.377 
Epoch 37/1000 
	 loss: 52.1890, MinusLogProbMetric: 52.1890, val_loss: 53.0191, val_MinusLogProbMetric: 53.0191

Epoch 37: val_loss did not improve from 52.76389
196/196 - 44s - loss: 52.1890 - MinusLogProbMetric: 52.1890 - val_loss: 53.0191 - val_MinusLogProbMetric: 53.0191 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 38/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:58:50.460 
Epoch 38/1000 
	 loss: 51.9664, MinusLogProbMetric: 51.9664, val_loss: 52.4944, val_MinusLogProbMetric: 52.4944

Epoch 38: val_loss improved from 52.76389 to 52.49435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 51.9664 - MinusLogProbMetric: 51.9664 - val_loss: 52.4944 - val_MinusLogProbMetric: 52.4944 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 39/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 06:59:35.076 
Epoch 39/1000 
	 loss: 51.9266, MinusLogProbMetric: 51.9266, val_loss: 52.6371, val_MinusLogProbMetric: 52.6372

Epoch 39: val_loss did not improve from 52.49435
196/196 - 44s - loss: 51.9266 - MinusLogProbMetric: 51.9266 - val_loss: 52.6371 - val_MinusLogProbMetric: 52.6372 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 40/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:00:19.324 
Epoch 40/1000 
	 loss: 51.8417, MinusLogProbMetric: 51.8417, val_loss: 53.0144, val_MinusLogProbMetric: 53.0144

Epoch 40: val_loss did not improve from 52.49435
196/196 - 44s - loss: 51.8417 - MinusLogProbMetric: 51.8417 - val_loss: 53.0144 - val_MinusLogProbMetric: 53.0144 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 41/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:01:02.908 
Epoch 41/1000 
	 loss: 51.7831, MinusLogProbMetric: 51.7831, val_loss: 52.4370, val_MinusLogProbMetric: 52.4370

Epoch 41: val_loss improved from 52.49435 to 52.43695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 51.7831 - MinusLogProbMetric: 51.7831 - val_loss: 52.4370 - val_MinusLogProbMetric: 52.4370 - lr: 1.2346e-05 - 45s/epoch - 227ms/step
Epoch 42/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:01:47.470 
Epoch 42/1000 
	 loss: 51.7260, MinusLogProbMetric: 51.7260, val_loss: 53.3633, val_MinusLogProbMetric: 53.3634

Epoch 42: val_loss did not improve from 52.43695
196/196 - 44s - loss: 51.7260 - MinusLogProbMetric: 51.7260 - val_loss: 53.3633 - val_MinusLogProbMetric: 53.3634 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 43/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:02:31.046 
Epoch 43/1000 
	 loss: 51.7119, MinusLogProbMetric: 51.7119, val_loss: 52.3827, val_MinusLogProbMetric: 52.3827

Epoch 43: val_loss improved from 52.43695 to 52.38273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 51.7119 - MinusLogProbMetric: 51.7119 - val_loss: 52.3827 - val_MinusLogProbMetric: 52.3827 - lr: 1.2346e-05 - 44s/epoch - 226ms/step
Epoch 44/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:03:15.430 
Epoch 44/1000 
	 loss: 51.4522, MinusLogProbMetric: 51.4522, val_loss: 52.3374, val_MinusLogProbMetric: 52.3374

Epoch 44: val_loss improved from 52.38273 to 52.33739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 51.4522 - MinusLogProbMetric: 51.4522 - val_loss: 52.3374 - val_MinusLogProbMetric: 52.3374 - lr: 1.2346e-05 - 44s/epoch - 227ms/step
Epoch 45/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:03:59.523 
Epoch 45/1000 
	 loss: 51.3918, MinusLogProbMetric: 51.3918, val_loss: 52.2975, val_MinusLogProbMetric: 52.2975

Epoch 45: val_loss improved from 52.33739 to 52.29746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 51.3918 - MinusLogProbMetric: 51.3918 - val_loss: 52.2975 - val_MinusLogProbMetric: 52.2975 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 46/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:04:43.496 
Epoch 46/1000 
	 loss: 51.4899, MinusLogProbMetric: 51.4899, val_loss: 52.3473, val_MinusLogProbMetric: 52.3473

Epoch 46: val_loss did not improve from 52.29746
196/196 - 43s - loss: 51.4899 - MinusLogProbMetric: 51.4899 - val_loss: 52.3473 - val_MinusLogProbMetric: 52.3473 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 47/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:05:27.012 
Epoch 47/1000 
	 loss: 51.3679, MinusLogProbMetric: 51.3679, val_loss: 52.5366, val_MinusLogProbMetric: 52.5366

Epoch 47: val_loss did not improve from 52.29746
196/196 - 44s - loss: 51.3679 - MinusLogProbMetric: 51.3679 - val_loss: 52.5366 - val_MinusLogProbMetric: 52.5366 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 48/1000
2023-10-27 07:06:08.838 
Epoch 48/1000 
	 loss: 51.3169, MinusLogProbMetric: 51.3169, val_loss: 52.1028, val_MinusLogProbMetric: 52.1028

Epoch 48: val_loss improved from 52.29746 to 52.10279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 51.3169 - MinusLogProbMetric: 51.3169 - val_loss: 52.1028 - val_MinusLogProbMetric: 52.1028 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 49/1000
2023-10-27 07:06:48.200 
Epoch 49/1000 
	 loss: 51.3484, MinusLogProbMetric: 51.3484, val_loss: 52.0066, val_MinusLogProbMetric: 52.0066

Epoch 49: val_loss improved from 52.10279 to 52.00663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 51.3484 - MinusLogProbMetric: 51.3484 - val_loss: 52.0066 - val_MinusLogProbMetric: 52.0066 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 50/1000
2023-10-27 07:07:26.508 
Epoch 50/1000 
	 loss: 51.2258, MinusLogProbMetric: 51.2258, val_loss: 51.8899, val_MinusLogProbMetric: 51.8899

Epoch 50: val_loss improved from 52.00663 to 51.88992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 38s - loss: 51.2258 - MinusLogProbMetric: 51.2258 - val_loss: 51.8899 - val_MinusLogProbMetric: 51.8899 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 51/1000
2023-10-27 07:08:05.573 
Epoch 51/1000 
	 loss: 51.3832, MinusLogProbMetric: 51.3832, val_loss: 51.7382, val_MinusLogProbMetric: 51.7382

Epoch 51: val_loss improved from 51.88992 to 51.73816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 51.3832 - MinusLogProbMetric: 51.3832 - val_loss: 51.7382 - val_MinusLogProbMetric: 51.7382 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 52/1000
2023-10-27 07:08:50.537 
Epoch 52/1000 
	 loss: 50.9950, MinusLogProbMetric: 50.9950, val_loss: 51.8286, val_MinusLogProbMetric: 51.8286

Epoch 52: val_loss did not improve from 51.73816
196/196 - 44s - loss: 50.9950 - MinusLogProbMetric: 50.9950 - val_loss: 51.8286 - val_MinusLogProbMetric: 51.8286 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 53/1000
2023-10-27 07:09:34.710 
Epoch 53/1000 
	 loss: 50.8849, MinusLogProbMetric: 50.8849, val_loss: 51.6812, val_MinusLogProbMetric: 51.6812

Epoch 53: val_loss improved from 51.73816 to 51.68118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 45s - loss: 50.8849 - MinusLogProbMetric: 50.8849 - val_loss: 51.6812 - val_MinusLogProbMetric: 51.6812 - lr: 1.2346e-05 - 45s/epoch - 230ms/step
Epoch 54/1000
2023-10-27 07:10:18.879 
Epoch 54/1000 
	 loss: 50.9287, MinusLogProbMetric: 50.9287, val_loss: 54.4701, val_MinusLogProbMetric: 54.4701

Epoch 54: val_loss did not improve from 51.68118
196/196 - 43s - loss: 50.9287 - MinusLogProbMetric: 50.9287 - val_loss: 54.4701 - val_MinusLogProbMetric: 54.4701 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 55/1000
2023-10-27 07:11:02.323 
Epoch 55/1000 
	 loss: 50.9585, MinusLogProbMetric: 50.9585, val_loss: 52.9216, val_MinusLogProbMetric: 52.9216

Epoch 55: val_loss did not improve from 51.68118
196/196 - 43s - loss: 50.9585 - MinusLogProbMetric: 50.9585 - val_loss: 52.9216 - val_MinusLogProbMetric: 52.9216 - lr: 1.2346e-05 - 43s/epoch - 222ms/step
Epoch 56/1000
2023-10-27 07:11:46.256 
Epoch 56/1000 
	 loss: 50.8291, MinusLogProbMetric: 50.8291, val_loss: 51.7402, val_MinusLogProbMetric: 51.7402

Epoch 56: val_loss did not improve from 51.68118
196/196 - 44s - loss: 50.8291 - MinusLogProbMetric: 50.8291 - val_loss: 51.7402 - val_MinusLogProbMetric: 51.7402 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 57/1000
2023-10-27 07:12:29.470 
Epoch 57/1000 
	 loss: 50.7730, MinusLogProbMetric: 50.7730, val_loss: 52.5960, val_MinusLogProbMetric: 52.5960

Epoch 57: val_loss did not improve from 51.68118
196/196 - 43s - loss: 50.7730 - MinusLogProbMetric: 50.7730 - val_loss: 52.5960 - val_MinusLogProbMetric: 52.5960 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 58/1000
2023-10-27 07:13:12.268 
Epoch 58/1000 
	 loss: 50.6976, MinusLogProbMetric: 50.6976, val_loss: 52.2610, val_MinusLogProbMetric: 52.2610

Epoch 58: val_loss did not improve from 51.68118
196/196 - 43s - loss: 50.6976 - MinusLogProbMetric: 50.6976 - val_loss: 52.2610 - val_MinusLogProbMetric: 52.2610 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 59/1000
2023-10-27 07:13:47.400 
Epoch 59/1000 
	 loss: 50.5738, MinusLogProbMetric: 50.5738, val_loss: 51.6376, val_MinusLogProbMetric: 51.6376

Epoch 59: val_loss improved from 51.68118 to 51.63760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 36s - loss: 50.5738 - MinusLogProbMetric: 50.5738 - val_loss: 51.6376 - val_MinusLogProbMetric: 51.6376 - lr: 1.2346e-05 - 36s/epoch - 182ms/step
Epoch 60/1000
2023-10-27 07:14:21.414 
Epoch 60/1000 
	 loss: 50.5830, MinusLogProbMetric: 50.5830, val_loss: 51.7438, val_MinusLogProbMetric: 51.7438

Epoch 60: val_loss did not improve from 51.63760
196/196 - 33s - loss: 50.5830 - MinusLogProbMetric: 50.5830 - val_loss: 51.7438 - val_MinusLogProbMetric: 51.7438 - lr: 1.2346e-05 - 33s/epoch - 170ms/step
Epoch 61/1000
2023-10-27 07:14:54.875 
Epoch 61/1000 
	 loss: 50.4534, MinusLogProbMetric: 50.4534, val_loss: 51.1384, val_MinusLogProbMetric: 51.1384

Epoch 61: val_loss improved from 51.63760 to 51.13841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 34s - loss: 50.4534 - MinusLogProbMetric: 50.4534 - val_loss: 51.1384 - val_MinusLogProbMetric: 51.1384 - lr: 1.2346e-05 - 34s/epoch - 174ms/step
Epoch 62/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:15:34.629 
Epoch 62/1000 
	 loss: 50.3262, MinusLogProbMetric: 50.3262, val_loss: 50.9519, val_MinusLogProbMetric: 50.9519

Epoch 62: val_loss improved from 51.13841 to 50.95185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 40s - loss: 50.3262 - MinusLogProbMetric: 50.3262 - val_loss: 50.9519 - val_MinusLogProbMetric: 50.9519 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 63/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:16:16.316 
Epoch 63/1000 
	 loss: 50.5849, MinusLogProbMetric: 50.5849, val_loss: 51.0840, val_MinusLogProbMetric: 51.0840

Epoch 63: val_loss did not improve from 50.95185
196/196 - 41s - loss: 50.5849 - MinusLogProbMetric: 50.5849 - val_loss: 51.0840 - val_MinusLogProbMetric: 51.0840 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 64/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:16:59.660 
Epoch 64/1000 
	 loss: 50.2461, MinusLogProbMetric: 50.2461, val_loss: 51.2773, val_MinusLogProbMetric: 51.2773

Epoch 64: val_loss did not improve from 50.95185
196/196 - 43s - loss: 50.2461 - MinusLogProbMetric: 50.2461 - val_loss: 51.2773 - val_MinusLogProbMetric: 51.2773 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 65/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:17:40.419 
Epoch 65/1000 
	 loss: 50.2654, MinusLogProbMetric: 50.2654, val_loss: 50.8453, val_MinusLogProbMetric: 50.8454

Epoch 65: val_loss improved from 50.95185 to 50.84535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 50.2654 - MinusLogProbMetric: 50.2654 - val_loss: 50.8453 - val_MinusLogProbMetric: 50.8454 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 66/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:18:22.845 
Epoch 66/1000 
	 loss: 50.0996, MinusLogProbMetric: 50.0996, val_loss: 51.0959, val_MinusLogProbMetric: 51.0959

Epoch 66: val_loss did not improve from 50.84535
196/196 - 42s - loss: 50.0996 - MinusLogProbMetric: 50.0996 - val_loss: 51.0959 - val_MinusLogProbMetric: 51.0959 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 67/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:19:04.646 
Epoch 67/1000 
	 loss: 50.3581, MinusLogProbMetric: 50.3581, val_loss: 50.6699, val_MinusLogProbMetric: 50.6700

Epoch 67: val_loss improved from 50.84535 to 50.66994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 50.3581 - MinusLogProbMetric: 50.3581 - val_loss: 50.6699 - val_MinusLogProbMetric: 50.6700 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 68/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:19:48.928 
Epoch 68/1000 
	 loss: 50.3203, MinusLogProbMetric: 50.3203, val_loss: 50.7950, val_MinusLogProbMetric: 50.7950

Epoch 68: val_loss did not improve from 50.66994
196/196 - 44s - loss: 50.3203 - MinusLogProbMetric: 50.3203 - val_loss: 50.7950 - val_MinusLogProbMetric: 50.7950 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 69/1000
2023-10-27 07:20:30.602 
Epoch 69/1000 
	 loss: 50.1068, MinusLogProbMetric: 50.1068, val_loss: 51.6269, val_MinusLogProbMetric: 51.6269

Epoch 69: val_loss did not improve from 50.66994
196/196 - 42s - loss: 50.1068 - MinusLogProbMetric: 50.1068 - val_loss: 51.6269 - val_MinusLogProbMetric: 51.6269 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 70/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:21:13.658 
Epoch 70/1000 
	 loss: 50.0831, MinusLogProbMetric: 50.0831, val_loss: 51.2797, val_MinusLogProbMetric: 51.2797

Epoch 70: val_loss did not improve from 50.66994
196/196 - 43s - loss: 50.0831 - MinusLogProbMetric: 50.0831 - val_loss: 51.2797 - val_MinusLogProbMetric: 51.2797 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 71/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:21:55.362 
Epoch 71/1000 
	 loss: 49.9907, MinusLogProbMetric: 49.9907, val_loss: 50.9466, val_MinusLogProbMetric: 50.9466

Epoch 71: val_loss did not improve from 50.66994
196/196 - 42s - loss: 49.9907 - MinusLogProbMetric: 49.9907 - val_loss: 50.9466 - val_MinusLogProbMetric: 50.9466 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 72/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:22:37.661 
Epoch 72/1000 
	 loss: 50.2787, MinusLogProbMetric: 50.2787, val_loss: 50.6463, val_MinusLogProbMetric: 50.6463

Epoch 72: val_loss improved from 50.66994 to 50.64629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 50.2787 - MinusLogProbMetric: 50.2787 - val_loss: 50.6463 - val_MinusLogProbMetric: 50.6463 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 73/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:23:21.284 
Epoch 73/1000 
	 loss: 49.8249, MinusLogProbMetric: 49.8249, val_loss: 50.9097, val_MinusLogProbMetric: 50.9097

Epoch 73: val_loss did not improve from 50.64629
196/196 - 43s - loss: 49.8249 - MinusLogProbMetric: 49.8249 - val_loss: 50.9097 - val_MinusLogProbMetric: 50.9097 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 74/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:24:04.133 
Epoch 74/1000 
	 loss: 50.0711, MinusLogProbMetric: 50.0711, val_loss: 50.6308, val_MinusLogProbMetric: 50.6308

Epoch 74: val_loss improved from 50.64629 to 50.63083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 50.0711 - MinusLogProbMetric: 50.0711 - val_loss: 50.6308 - val_MinusLogProbMetric: 50.6308 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 75/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:24:47.136 
Epoch 75/1000 
	 loss: 49.7409, MinusLogProbMetric: 49.7409, val_loss: 51.1043, val_MinusLogProbMetric: 51.1043

Epoch 75: val_loss did not improve from 50.63083
196/196 - 42s - loss: 49.7409 - MinusLogProbMetric: 49.7409 - val_loss: 51.1043 - val_MinusLogProbMetric: 51.1043 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 76/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:25:29.662 
Epoch 76/1000 
	 loss: 49.6772, MinusLogProbMetric: 49.6772, val_loss: 51.6136, val_MinusLogProbMetric: 51.6136

Epoch 76: val_loss did not improve from 50.63083
196/196 - 43s - loss: 49.6772 - MinusLogProbMetric: 49.6772 - val_loss: 51.6136 - val_MinusLogProbMetric: 51.6136 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 77/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:26:12.256 
Epoch 77/1000 
	 loss: 49.6402, MinusLogProbMetric: 49.6402, val_loss: 50.3113, val_MinusLogProbMetric: 50.3114

Epoch 77: val_loss improved from 50.63083 to 50.31134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 49.6402 - MinusLogProbMetric: 49.6402 - val_loss: 50.3113 - val_MinusLogProbMetric: 50.3114 - lr: 1.2346e-05 - 43s/epoch - 222ms/step
Epoch 78/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:26:56.034 
Epoch 78/1000 
	 loss: 49.6786, MinusLogProbMetric: 49.6786, val_loss: 50.7846, val_MinusLogProbMetric: 50.7846

Epoch 78: val_loss did not improve from 50.31134
196/196 - 43s - loss: 49.6786 - MinusLogProbMetric: 49.6786 - val_loss: 50.7846 - val_MinusLogProbMetric: 50.7846 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 79/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:27:38.805 
Epoch 79/1000 
	 loss: 49.4873, MinusLogProbMetric: 49.4873, val_loss: 50.5305, val_MinusLogProbMetric: 50.5305

Epoch 79: val_loss did not improve from 50.31134
196/196 - 43s - loss: 49.4873 - MinusLogProbMetric: 49.4873 - val_loss: 50.5305 - val_MinusLogProbMetric: 50.5305 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 80/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:28:21.969 
Epoch 80/1000 
	 loss: 49.7557, MinusLogProbMetric: 49.7557, val_loss: 51.0411, val_MinusLogProbMetric: 51.0411

Epoch 80: val_loss did not improve from 50.31134
196/196 - 43s - loss: 49.7557 - MinusLogProbMetric: 49.7557 - val_loss: 51.0411 - val_MinusLogProbMetric: 51.0411 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 81/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:29:04.265 
Epoch 81/1000 
	 loss: 49.3043, MinusLogProbMetric: 49.3043, val_loss: 50.1910, val_MinusLogProbMetric: 50.1911

Epoch 81: val_loss improved from 50.31134 to 50.19105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 49.3043 - MinusLogProbMetric: 49.3043 - val_loss: 50.1910 - val_MinusLogProbMetric: 50.1911 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 82/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:29:48.253 
Epoch 82/1000 
	 loss: 49.4214, MinusLogProbMetric: 49.4214, val_loss: 50.2963, val_MinusLogProbMetric: 50.2963

Epoch 82: val_loss did not improve from 50.19105
196/196 - 43s - loss: 49.4214 - MinusLogProbMetric: 49.4214 - val_loss: 50.2963 - val_MinusLogProbMetric: 50.2963 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 83/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:30:31.088 
Epoch 83/1000 
	 loss: 49.4212, MinusLogProbMetric: 49.4212, val_loss: 50.1583, val_MinusLogProbMetric: 50.1583

Epoch 83: val_loss improved from 50.19105 to 50.15830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 49.4212 - MinusLogProbMetric: 49.4212 - val_loss: 50.1583 - val_MinusLogProbMetric: 50.1583 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 84/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:31:14.826 
Epoch 84/1000 
	 loss: 49.2452, MinusLogProbMetric: 49.2452, val_loss: 50.0199, val_MinusLogProbMetric: 50.0199

Epoch 84: val_loss improved from 50.15830 to 50.01990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 49.2452 - MinusLogProbMetric: 49.2452 - val_loss: 50.0199 - val_MinusLogProbMetric: 50.0199 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 85/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:31:58.178 
Epoch 85/1000 
	 loss: 49.4850, MinusLogProbMetric: 49.4850, val_loss: 50.0235, val_MinusLogProbMetric: 50.0235

Epoch 85: val_loss did not improve from 50.01990
196/196 - 43s - loss: 49.4850 - MinusLogProbMetric: 49.4850 - val_loss: 50.0235 - val_MinusLogProbMetric: 50.0235 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 86/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:32:40.982 
Epoch 86/1000 
	 loss: 49.2815, MinusLogProbMetric: 49.2815, val_loss: 50.2227, val_MinusLogProbMetric: 50.2227

Epoch 86: val_loss did not improve from 50.01990
196/196 - 43s - loss: 49.2815 - MinusLogProbMetric: 49.2815 - val_loss: 50.2227 - val_MinusLogProbMetric: 50.2227 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 87/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:33:23.340 
Epoch 87/1000 
	 loss: 49.6027, MinusLogProbMetric: 49.6027, val_loss: 50.3176, val_MinusLogProbMetric: 50.3176

Epoch 87: val_loss did not improve from 50.01990
196/196 - 42s - loss: 49.6027 - MinusLogProbMetric: 49.6027 - val_loss: 50.3176 - val_MinusLogProbMetric: 50.3176 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 88/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:34:05.421 
Epoch 88/1000 
	 loss: 49.5466, MinusLogProbMetric: 49.5466, val_loss: 50.2408, val_MinusLogProbMetric: 50.2408

Epoch 88: val_loss did not improve from 50.01990
196/196 - 42s - loss: 49.5466 - MinusLogProbMetric: 49.5466 - val_loss: 50.2408 - val_MinusLogProbMetric: 50.2408 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 89/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:34:48.046 
Epoch 89/1000 
	 loss: 49.0852, MinusLogProbMetric: 49.0852, val_loss: 50.0156, val_MinusLogProbMetric: 50.0156

Epoch 89: val_loss improved from 50.01990 to 50.01559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 49.0852 - MinusLogProbMetric: 49.0852 - val_loss: 50.0156 - val_MinusLogProbMetric: 50.0156 - lr: 1.2346e-05 - 43s/epoch - 222ms/step
Epoch 90/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:35:31.436 
Epoch 90/1000 
	 loss: 49.0620, MinusLogProbMetric: 49.0620, val_loss: 50.1396, val_MinusLogProbMetric: 50.1396

Epoch 90: val_loss did not improve from 50.01559
196/196 - 43s - loss: 49.0620 - MinusLogProbMetric: 49.0620 - val_loss: 50.1396 - val_MinusLogProbMetric: 50.1396 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 91/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:36:14.292 
Epoch 91/1000 
	 loss: 49.0570, MinusLogProbMetric: 49.0570, val_loss: 50.5535, val_MinusLogProbMetric: 50.5535

Epoch 91: val_loss did not improve from 50.01559
196/196 - 43s - loss: 49.0570 - MinusLogProbMetric: 49.0570 - val_loss: 50.5535 - val_MinusLogProbMetric: 50.5535 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 92/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:36:56.941 
Epoch 92/1000 
	 loss: 49.1981, MinusLogProbMetric: 49.1981, val_loss: 50.3708, val_MinusLogProbMetric: 50.3708

Epoch 92: val_loss did not improve from 50.01559
196/196 - 43s - loss: 49.1981 - MinusLogProbMetric: 49.1981 - val_loss: 50.3708 - val_MinusLogProbMetric: 50.3708 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 93/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:37:39.594 
Epoch 93/1000 
	 loss: 49.0202, MinusLogProbMetric: 49.0202, val_loss: 49.9835, val_MinusLogProbMetric: 49.9835

Epoch 93: val_loss improved from 50.01559 to 49.98353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 49.0202 - MinusLogProbMetric: 49.0202 - val_loss: 49.9835 - val_MinusLogProbMetric: 49.9835 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 94/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:38:21.092 
Epoch 94/1000 
	 loss: 49.4889, MinusLogProbMetric: 49.4889, val_loss: 50.4331, val_MinusLogProbMetric: 50.4331

Epoch 94: val_loss did not improve from 49.98353
196/196 - 41s - loss: 49.4889 - MinusLogProbMetric: 49.4889 - val_loss: 50.4331 - val_MinusLogProbMetric: 50.4331 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 95/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:38:58.172 
Epoch 95/1000 
	 loss: 49.0873, MinusLogProbMetric: 49.0873, val_loss: 50.1979, val_MinusLogProbMetric: 50.1980

Epoch 95: val_loss did not improve from 49.98353
196/196 - 37s - loss: 49.0873 - MinusLogProbMetric: 49.0873 - val_loss: 50.1979 - val_MinusLogProbMetric: 50.1980 - lr: 1.2346e-05 - 37s/epoch - 189ms/step
Epoch 96/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:39:34.716 
Epoch 96/1000 
	 loss: 48.7522, MinusLogProbMetric: 48.7522, val_loss: 50.1527, val_MinusLogProbMetric: 50.1527

Epoch 96: val_loss did not improve from 49.98353
196/196 - 37s - loss: 48.7522 - MinusLogProbMetric: 48.7522 - val_loss: 50.1527 - val_MinusLogProbMetric: 50.1527 - lr: 1.2346e-05 - 37s/epoch - 186ms/step
Epoch 97/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:40:10.902 
Epoch 97/1000 
	 loss: 48.8637, MinusLogProbMetric: 48.8637, val_loss: 49.7170, val_MinusLogProbMetric: 49.7170

Epoch 97: val_loss improved from 49.98353 to 49.71698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 48.8637 - MinusLogProbMetric: 48.8637 - val_loss: 49.7170 - val_MinusLogProbMetric: 49.7170 - lr: 1.2346e-05 - 37s/epoch - 188ms/step
Epoch 98/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:40:54.723 
Epoch 98/1000 
	 loss: 48.8047, MinusLogProbMetric: 48.8047, val_loss: 49.7518, val_MinusLogProbMetric: 49.7518

Epoch 98: val_loss did not improve from 49.71698
196/196 - 43s - loss: 48.8047 - MinusLogProbMetric: 48.8047 - val_loss: 49.7518 - val_MinusLogProbMetric: 49.7518 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 99/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:41:37.937 
Epoch 99/1000 
	 loss: 48.7908, MinusLogProbMetric: 48.7908, val_loss: 49.4180, val_MinusLogProbMetric: 49.4180

Epoch 99: val_loss improved from 49.71698 to 49.41796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 48.7908 - MinusLogProbMetric: 48.7908 - val_loss: 49.4180 - val_MinusLogProbMetric: 49.4180 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 100/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:42:20.788 
Epoch 100/1000 
	 loss: 48.9213, MinusLogProbMetric: 48.9213, val_loss: 49.8056, val_MinusLogProbMetric: 49.8056

Epoch 100: val_loss did not improve from 49.41796
196/196 - 42s - loss: 48.9213 - MinusLogProbMetric: 48.9213 - val_loss: 49.8056 - val_MinusLogProbMetric: 49.8056 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 101/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:43:03.755 
Epoch 101/1000 
	 loss: 48.6819, MinusLogProbMetric: 48.6819, val_loss: 49.4057, val_MinusLogProbMetric: 49.4057

Epoch 101: val_loss improved from 49.41796 to 49.40571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 48.6819 - MinusLogProbMetric: 48.6819 - val_loss: 49.4057 - val_MinusLogProbMetric: 49.4057 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 102/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:43:47.103 
Epoch 102/1000 
	 loss: 48.6621, MinusLogProbMetric: 48.6621, val_loss: 49.7102, val_MinusLogProbMetric: 49.7102

Epoch 102: val_loss did not improve from 49.40571
196/196 - 42s - loss: 48.6621 - MinusLogProbMetric: 48.6621 - val_loss: 49.7102 - val_MinusLogProbMetric: 49.7102 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 103/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:44:29.965 
Epoch 103/1000 
	 loss: 48.5972, MinusLogProbMetric: 48.5972, val_loss: 49.6885, val_MinusLogProbMetric: 49.6885

Epoch 103: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.5972 - MinusLogProbMetric: 48.5972 - val_loss: 49.6885 - val_MinusLogProbMetric: 49.6885 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 104/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:45:12.751 
Epoch 104/1000 
	 loss: 48.5905, MinusLogProbMetric: 48.5905, val_loss: 50.2295, val_MinusLogProbMetric: 50.2295

Epoch 104: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.5905 - MinusLogProbMetric: 48.5905 - val_loss: 50.2295 - val_MinusLogProbMetric: 50.2295 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 105/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:45:55.425 
Epoch 105/1000 
	 loss: 48.8653, MinusLogProbMetric: 48.8653, val_loss: 50.1939, val_MinusLogProbMetric: 50.1939

Epoch 105: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.8653 - MinusLogProbMetric: 48.8653 - val_loss: 50.1939 - val_MinusLogProbMetric: 50.1939 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 106/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:46:38.131 
Epoch 106/1000 
	 loss: 48.5351, MinusLogProbMetric: 48.5351, val_loss: 49.5574, val_MinusLogProbMetric: 49.5574

Epoch 106: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.5351 - MinusLogProbMetric: 48.5351 - val_loss: 49.5574 - val_MinusLogProbMetric: 49.5574 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 107/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:47:20.882 
Epoch 107/1000 
	 loss: 48.5099, MinusLogProbMetric: 48.5099, val_loss: 49.6688, val_MinusLogProbMetric: 49.6688

Epoch 107: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.5099 - MinusLogProbMetric: 48.5099 - val_loss: 49.6688 - val_MinusLogProbMetric: 49.6688 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 108/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:48:03.618 
Epoch 108/1000 
	 loss: 48.4476, MinusLogProbMetric: 48.4476, val_loss: 49.6269, val_MinusLogProbMetric: 49.6269

Epoch 108: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.4476 - MinusLogProbMetric: 48.4476 - val_loss: 49.6269 - val_MinusLogProbMetric: 49.6269 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 109/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:48:46.930 
Epoch 109/1000 
	 loss: 48.4832, MinusLogProbMetric: 48.4832, val_loss: 49.7209, val_MinusLogProbMetric: 49.7209

Epoch 109: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.4832 - MinusLogProbMetric: 48.4832 - val_loss: 49.7209 - val_MinusLogProbMetric: 49.7209 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 110/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:49:29.622 
Epoch 110/1000 
	 loss: 48.5068, MinusLogProbMetric: 48.5068, val_loss: 49.4173, val_MinusLogProbMetric: 49.4174

Epoch 110: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.5068 - MinusLogProbMetric: 48.5068 - val_loss: 49.4173 - val_MinusLogProbMetric: 49.4174 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 111/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:50:12.198 
Epoch 111/1000 
	 loss: 48.3132, MinusLogProbMetric: 48.3132, val_loss: 49.9009, val_MinusLogProbMetric: 49.9009

Epoch 111: val_loss did not improve from 49.40571
196/196 - 43s - loss: 48.3132 - MinusLogProbMetric: 48.3132 - val_loss: 49.9009 - val_MinusLogProbMetric: 49.9009 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 112/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:50:53.933 
Epoch 112/1000 
	 loss: 48.3890, MinusLogProbMetric: 48.3890, val_loss: 49.3732, val_MinusLogProbMetric: 49.3732

Epoch 112: val_loss improved from 49.40571 to 49.37318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 48.3890 - MinusLogProbMetric: 48.3890 - val_loss: 49.3732 - val_MinusLogProbMetric: 49.3732 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 113/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:51:36.918 
Epoch 113/1000 
	 loss: 48.4353, MinusLogProbMetric: 48.4353, val_loss: 49.3288, val_MinusLogProbMetric: 49.3288

Epoch 113: val_loss improved from 49.37318 to 49.32878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 48.4353 - MinusLogProbMetric: 48.4353 - val_loss: 49.3288 - val_MinusLogProbMetric: 49.3288 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 114/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:52:20.783 
Epoch 114/1000 
	 loss: 48.2554, MinusLogProbMetric: 48.2554, val_loss: 49.2337, val_MinusLogProbMetric: 49.2337

Epoch 114: val_loss improved from 49.32878 to 49.23371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 48.2554 - MinusLogProbMetric: 48.2554 - val_loss: 49.2337 - val_MinusLogProbMetric: 49.2337 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 115/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:53:04.344 
Epoch 115/1000 
	 loss: 48.2450, MinusLogProbMetric: 48.2450, val_loss: 49.4503, val_MinusLogProbMetric: 49.4503

Epoch 115: val_loss did not improve from 49.23371
196/196 - 43s - loss: 48.2450 - MinusLogProbMetric: 48.2450 - val_loss: 49.4503 - val_MinusLogProbMetric: 49.4503 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 116/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:53:47.652 
Epoch 116/1000 
	 loss: 48.2241, MinusLogProbMetric: 48.2241, val_loss: 49.4481, val_MinusLogProbMetric: 49.4481

Epoch 116: val_loss did not improve from 49.23371
196/196 - 43s - loss: 48.2241 - MinusLogProbMetric: 48.2241 - val_loss: 49.4481 - val_MinusLogProbMetric: 49.4481 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 117/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:54:30.696 
Epoch 117/1000 
	 loss: 48.9656, MinusLogProbMetric: 48.9656, val_loss: 50.2481, val_MinusLogProbMetric: 50.2481

Epoch 117: val_loss did not improve from 49.23371
196/196 - 43s - loss: 48.9656 - MinusLogProbMetric: 48.9656 - val_loss: 50.2481 - val_MinusLogProbMetric: 50.2481 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 118/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:55:12.437 
Epoch 118/1000 
	 loss: 48.3553, MinusLogProbMetric: 48.3553, val_loss: 50.1620, val_MinusLogProbMetric: 50.1620

Epoch 118: val_loss did not improve from 49.23371
196/196 - 42s - loss: 48.3553 - MinusLogProbMetric: 48.3553 - val_loss: 50.1620 - val_MinusLogProbMetric: 50.1620 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 119/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:55:53.598 
Epoch 119/1000 
	 loss: 48.0676, MinusLogProbMetric: 48.0676, val_loss: 48.9921, val_MinusLogProbMetric: 48.9921

Epoch 119: val_loss improved from 49.23371 to 48.99210, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 48.0676 - MinusLogProbMetric: 48.0676 - val_loss: 48.9921 - val_MinusLogProbMetric: 48.9921 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 120/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:56:32.597 
Epoch 120/1000 
	 loss: 48.0549, MinusLogProbMetric: 48.0549, val_loss: 48.9307, val_MinusLogProbMetric: 48.9308

Epoch 120: val_loss improved from 48.99210 to 48.93074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 48.0549 - MinusLogProbMetric: 48.0549 - val_loss: 48.9307 - val_MinusLogProbMetric: 48.9308 - lr: 1.2346e-05 - 39s/epoch - 199ms/step
Epoch 121/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:57:10.100 
Epoch 121/1000 
	 loss: 47.9854, MinusLogProbMetric: 47.9854, val_loss: 48.7762, val_MinusLogProbMetric: 48.7763

Epoch 121: val_loss improved from 48.93074 to 48.77624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 47.9854 - MinusLogProbMetric: 47.9854 - val_loss: 48.7762 - val_MinusLogProbMetric: 48.7763 - lr: 1.2346e-05 - 37s/epoch - 191ms/step
Epoch 122/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:57:48.681 
Epoch 122/1000 
	 loss: 48.1003, MinusLogProbMetric: 48.1003, val_loss: 48.7043, val_MinusLogProbMetric: 48.7043

Epoch 122: val_loss improved from 48.77624 to 48.70426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 48.1003 - MinusLogProbMetric: 48.1003 - val_loss: 48.7043 - val_MinusLogProbMetric: 48.7043 - lr: 1.2346e-05 - 39s/epoch - 198ms/step
Epoch 123/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:58:31.391 
Epoch 123/1000 
	 loss: 48.0979, MinusLogProbMetric: 48.0979, val_loss: 48.9910, val_MinusLogProbMetric: 48.9910

Epoch 123: val_loss did not improve from 48.70426
196/196 - 42s - loss: 48.0979 - MinusLogProbMetric: 48.0979 - val_loss: 48.9910 - val_MinusLogProbMetric: 48.9910 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 124/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:59:09.017 
Epoch 124/1000 
	 loss: 48.1687, MinusLogProbMetric: 48.1687, val_loss: 48.9348, val_MinusLogProbMetric: 48.9349

Epoch 124: val_loss did not improve from 48.70426
196/196 - 38s - loss: 48.1687 - MinusLogProbMetric: 48.1687 - val_loss: 48.9348 - val_MinusLogProbMetric: 48.9349 - lr: 1.2346e-05 - 38s/epoch - 192ms/step
Epoch 125/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 07:59:46.367 
Epoch 125/1000 
	 loss: 48.0476, MinusLogProbMetric: 48.0476, val_loss: 50.4199, val_MinusLogProbMetric: 50.4199

Epoch 125: val_loss did not improve from 48.70426
196/196 - 37s - loss: 48.0476 - MinusLogProbMetric: 48.0476 - val_loss: 50.4199 - val_MinusLogProbMetric: 50.4199 - lr: 1.2346e-05 - 37s/epoch - 191ms/step
Epoch 126/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:00:25.573 
Epoch 126/1000 
	 loss: 48.2182, MinusLogProbMetric: 48.2182, val_loss: 50.0056, val_MinusLogProbMetric: 50.0056

Epoch 126: val_loss did not improve from 48.70426
196/196 - 39s - loss: 48.2182 - MinusLogProbMetric: 48.2182 - val_loss: 50.0056 - val_MinusLogProbMetric: 50.0056 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 127/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:01:04.534 
Epoch 127/1000 
	 loss: 48.0391, MinusLogProbMetric: 48.0391, val_loss: 48.8120, val_MinusLogProbMetric: 48.8120

Epoch 127: val_loss did not improve from 48.70426
196/196 - 39s - loss: 48.0391 - MinusLogProbMetric: 48.0391 - val_loss: 48.8120 - val_MinusLogProbMetric: 48.8120 - lr: 1.2346e-05 - 39s/epoch - 199ms/step
Epoch 128/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:01:40.592 
Epoch 128/1000 
	 loss: 47.8606, MinusLogProbMetric: 47.8606, val_loss: 48.7127, val_MinusLogProbMetric: 48.7127

Epoch 128: val_loss did not improve from 48.70426
196/196 - 36s - loss: 47.8606 - MinusLogProbMetric: 47.8606 - val_loss: 48.7127 - val_MinusLogProbMetric: 48.7127 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 129/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:02:17.127 
Epoch 129/1000 
	 loss: 48.1170, MinusLogProbMetric: 48.1170, val_loss: 48.7786, val_MinusLogProbMetric: 48.7786

Epoch 129: val_loss did not improve from 48.70426
196/196 - 37s - loss: 48.1170 - MinusLogProbMetric: 48.1170 - val_loss: 48.7786 - val_MinusLogProbMetric: 48.7786 - lr: 1.2346e-05 - 37s/epoch - 186ms/step
Epoch 130/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:02:58.276 
Epoch 130/1000 
	 loss: 48.0050, MinusLogProbMetric: 48.0050, val_loss: 48.9071, val_MinusLogProbMetric: 48.9072

Epoch 130: val_loss did not improve from 48.70426
196/196 - 41s - loss: 48.0050 - MinusLogProbMetric: 48.0050 - val_loss: 48.9071 - val_MinusLogProbMetric: 48.9072 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 131/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:03:37.591 
Epoch 131/1000 
	 loss: 47.8902, MinusLogProbMetric: 47.8902, val_loss: 48.5753, val_MinusLogProbMetric: 48.5753

Epoch 131: val_loss improved from 48.70426 to 48.57527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 40s - loss: 47.8902 - MinusLogProbMetric: 47.8902 - val_loss: 48.5753 - val_MinusLogProbMetric: 48.5753 - lr: 1.2346e-05 - 40s/epoch - 204ms/step
Epoch 132/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:04:15.509 
Epoch 132/1000 
	 loss: 47.8601, MinusLogProbMetric: 47.8601, val_loss: 49.3136, val_MinusLogProbMetric: 49.3136

Epoch 132: val_loss did not improve from 48.57527
196/196 - 37s - loss: 47.8601 - MinusLogProbMetric: 47.8601 - val_loss: 49.3136 - val_MinusLogProbMetric: 49.3136 - lr: 1.2346e-05 - 37s/epoch - 190ms/step
Epoch 133/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:04:51.941 
Epoch 133/1000 
	 loss: 47.6553, MinusLogProbMetric: 47.6553, val_loss: 48.4093, val_MinusLogProbMetric: 48.4093

Epoch 133: val_loss improved from 48.57527 to 48.40931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 47.6553 - MinusLogProbMetric: 47.6553 - val_loss: 48.4093 - val_MinusLogProbMetric: 48.4093 - lr: 1.2346e-05 - 37s/epoch - 189ms/step
Epoch 134/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:05:31.776 
Epoch 134/1000 
	 loss: 47.6806, MinusLogProbMetric: 47.6806, val_loss: 49.4619, val_MinusLogProbMetric: 49.4619

Epoch 134: val_loss did not improve from 48.40931
196/196 - 39s - loss: 47.6806 - MinusLogProbMetric: 47.6806 - val_loss: 49.4619 - val_MinusLogProbMetric: 49.4619 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 135/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:06:12.790 
Epoch 135/1000 
	 loss: 47.9178, MinusLogProbMetric: 47.9178, val_loss: 48.4930, val_MinusLogProbMetric: 48.4931

Epoch 135: val_loss did not improve from 48.40931
196/196 - 41s - loss: 47.9178 - MinusLogProbMetric: 47.9178 - val_loss: 48.4930 - val_MinusLogProbMetric: 48.4931 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 136/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:06:48.514 
Epoch 136/1000 
	 loss: 47.7290, MinusLogProbMetric: 47.7290, val_loss: 48.7422, val_MinusLogProbMetric: 48.7422

Epoch 136: val_loss did not improve from 48.40931
196/196 - 36s - loss: 47.7290 - MinusLogProbMetric: 47.7290 - val_loss: 48.7422 - val_MinusLogProbMetric: 48.7422 - lr: 1.2346e-05 - 36s/epoch - 182ms/step
Epoch 137/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:07:25.626 
Epoch 137/1000 
	 loss: 47.8577, MinusLogProbMetric: 47.8577, val_loss: 48.7119, val_MinusLogProbMetric: 48.7120

Epoch 137: val_loss did not improve from 48.40931
196/196 - 37s - loss: 47.8577 - MinusLogProbMetric: 47.8577 - val_loss: 48.7119 - val_MinusLogProbMetric: 48.7120 - lr: 1.2346e-05 - 37s/epoch - 189ms/step
Epoch 138/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:08:06.330 
Epoch 138/1000 
	 loss: 47.5779, MinusLogProbMetric: 47.5779, val_loss: 48.9368, val_MinusLogProbMetric: 48.9368

Epoch 138: val_loss did not improve from 48.40931
196/196 - 41s - loss: 47.5779 - MinusLogProbMetric: 47.5779 - val_loss: 48.9368 - val_MinusLogProbMetric: 48.9368 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 139/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:08:44.365 
Epoch 139/1000 
	 loss: 47.6564, MinusLogProbMetric: 47.6564, val_loss: 48.6770, val_MinusLogProbMetric: 48.6770

Epoch 139: val_loss did not improve from 48.40931
196/196 - 38s - loss: 47.6564 - MinusLogProbMetric: 47.6564 - val_loss: 48.6770 - val_MinusLogProbMetric: 48.6770 - lr: 1.2346e-05 - 38s/epoch - 194ms/step
Epoch 140/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:09:25.715 
Epoch 140/1000 
	 loss: 47.6799, MinusLogProbMetric: 47.6799, val_loss: 48.3632, val_MinusLogProbMetric: 48.3632

Epoch 140: val_loss improved from 48.40931 to 48.36318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 47.6799 - MinusLogProbMetric: 47.6799 - val_loss: 48.3632 - val_MinusLogProbMetric: 48.3632 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 141/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:10:02.622 
Epoch 141/1000 
	 loss: 47.6279, MinusLogProbMetric: 47.6279, val_loss: 48.2631, val_MinusLogProbMetric: 48.2631

Epoch 141: val_loss improved from 48.36318 to 48.26309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 47.6279 - MinusLogProbMetric: 47.6279 - val_loss: 48.2631 - val_MinusLogProbMetric: 48.2631 - lr: 1.2346e-05 - 37s/epoch - 187ms/step
Epoch 142/1000
2023-10-27 08:10:43.001 
Epoch 142/1000 
	 loss: 47.5643, MinusLogProbMetric: 47.5643, val_loss: 49.9038, val_MinusLogProbMetric: 49.9038

Epoch 142: val_loss did not improve from 48.26309
196/196 - 40s - loss: 47.5643 - MinusLogProbMetric: 47.5643 - val_loss: 49.9038 - val_MinusLogProbMetric: 49.9038 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 143/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:11:23.691 
Epoch 143/1000 
	 loss: 47.8432, MinusLogProbMetric: 47.8432, val_loss: 48.4144, val_MinusLogProbMetric: 48.4144

Epoch 143: val_loss did not improve from 48.26309
196/196 - 41s - loss: 47.8432 - MinusLogProbMetric: 47.8432 - val_loss: 48.4144 - val_MinusLogProbMetric: 48.4144 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 144/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:12:00.584 
Epoch 144/1000 
	 loss: 48.0486, MinusLogProbMetric: 48.0486, val_loss: 49.4047, val_MinusLogProbMetric: 49.4047

Epoch 144: val_loss did not improve from 48.26309
196/196 - 37s - loss: 48.0486 - MinusLogProbMetric: 48.0486 - val_loss: 49.4047 - val_MinusLogProbMetric: 49.4047 - lr: 1.2346e-05 - 37s/epoch - 188ms/step
Epoch 145/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:12:38.511 
Epoch 145/1000 
	 loss: 47.4362, MinusLogProbMetric: 47.4362, val_loss: 49.8540, val_MinusLogProbMetric: 49.8540

Epoch 145: val_loss did not improve from 48.26309
196/196 - 38s - loss: 47.4362 - MinusLogProbMetric: 47.4362 - val_loss: 49.8540 - val_MinusLogProbMetric: 49.8540 - lr: 1.2346e-05 - 38s/epoch - 193ms/step
Epoch 146/1000
2023-10-27 08:13:20.368 
Epoch 146/1000 
	 loss: 47.4922, MinusLogProbMetric: 47.4922, val_loss: 49.9806, val_MinusLogProbMetric: 49.9806

Epoch 146: val_loss did not improve from 48.26309
196/196 - 42s - loss: 47.4922 - MinusLogProbMetric: 47.4922 - val_loss: 49.9806 - val_MinusLogProbMetric: 49.9806 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 147/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:14:00.681 
Epoch 147/1000 
	 loss: 47.6363, MinusLogProbMetric: 47.6363, val_loss: 48.1545, val_MinusLogProbMetric: 48.1545

Epoch 147: val_loss improved from 48.26309 to 48.15453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 41s - loss: 47.6363 - MinusLogProbMetric: 47.6363 - val_loss: 48.1545 - val_MinusLogProbMetric: 48.1545 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 148/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:14:38.653 
Epoch 148/1000 
	 loss: 47.3006, MinusLogProbMetric: 47.3006, val_loss: 48.4131, val_MinusLogProbMetric: 48.4131

Epoch 148: val_loss did not improve from 48.15453
196/196 - 37s - loss: 47.3006 - MinusLogProbMetric: 47.3006 - val_loss: 48.4131 - val_MinusLogProbMetric: 48.4131 - lr: 1.2346e-05 - 37s/epoch - 189ms/step
Epoch 149/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:15:14.487 
Epoch 149/1000 
	 loss: 47.6355, MinusLogProbMetric: 47.6355, val_loss: 48.4421, val_MinusLogProbMetric: 48.4421

Epoch 149: val_loss did not improve from 48.15453
196/196 - 36s - loss: 47.6355 - MinusLogProbMetric: 47.6355 - val_loss: 48.4421 - val_MinusLogProbMetric: 48.4421 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 150/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:15:52.548 
Epoch 150/1000 
	 loss: 47.3822, MinusLogProbMetric: 47.3822, val_loss: 48.2027, val_MinusLogProbMetric: 48.2027

Epoch 150: val_loss did not improve from 48.15453
196/196 - 38s - loss: 47.3822 - MinusLogProbMetric: 47.3822 - val_loss: 48.2027 - val_MinusLogProbMetric: 48.2027 - lr: 1.2346e-05 - 38s/epoch - 194ms/step
Epoch 151/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:16:34.385 
Epoch 151/1000 
	 loss: 48.0375, MinusLogProbMetric: 48.0375, val_loss: 48.6638, val_MinusLogProbMetric: 48.6638

Epoch 151: val_loss did not improve from 48.15453
196/196 - 42s - loss: 48.0375 - MinusLogProbMetric: 48.0375 - val_loss: 48.6638 - val_MinusLogProbMetric: 48.6638 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 152/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:17:10.830 
Epoch 152/1000 
	 loss: 47.5987, MinusLogProbMetric: 47.5987, val_loss: 48.5958, val_MinusLogProbMetric: 48.5958

Epoch 152: val_loss did not improve from 48.15453
196/196 - 36s - loss: 47.5987 - MinusLogProbMetric: 47.5987 - val_loss: 48.5958 - val_MinusLogProbMetric: 48.5958 - lr: 1.2346e-05 - 36s/epoch - 186ms/step
Epoch 153/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:17:50.827 
Epoch 153/1000 
	 loss: 47.3888, MinusLogProbMetric: 47.3888, val_loss: 48.1128, val_MinusLogProbMetric: 48.1128

Epoch 153: val_loss improved from 48.15453 to 48.11281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 41s - loss: 47.3888 - MinusLogProbMetric: 47.3888 - val_loss: 48.1128 - val_MinusLogProbMetric: 48.1128 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 154/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:18:29.481 
Epoch 154/1000 
	 loss: 47.5221, MinusLogProbMetric: 47.5221, val_loss: 48.0522, val_MinusLogProbMetric: 48.0522

Epoch 154: val_loss improved from 48.11281 to 48.05217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 47.5221 - MinusLogProbMetric: 47.5221 - val_loss: 48.0522 - val_MinusLogProbMetric: 48.0522 - lr: 1.2346e-05 - 39s/epoch - 198ms/step
Epoch 155/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:19:11.808 
Epoch 155/1000 
	 loss: 47.2258, MinusLogProbMetric: 47.2258, val_loss: 48.0249, val_MinusLogProbMetric: 48.0249

Epoch 155: val_loss improved from 48.05217 to 48.02491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 47.2258 - MinusLogProbMetric: 47.2258 - val_loss: 48.0249 - val_MinusLogProbMetric: 48.0249 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 156/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:19:52.871 
Epoch 156/1000 
	 loss: 47.1969, MinusLogProbMetric: 47.1969, val_loss: 48.5653, val_MinusLogProbMetric: 48.5654

Epoch 156: val_loss did not improve from 48.02491
196/196 - 40s - loss: 47.1969 - MinusLogProbMetric: 47.1969 - val_loss: 48.5653 - val_MinusLogProbMetric: 48.5654 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 157/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:20:31.193 
Epoch 157/1000 
	 loss: 47.4195, MinusLogProbMetric: 47.4195, val_loss: 48.2441, val_MinusLogProbMetric: 48.2441

Epoch 157: val_loss did not improve from 48.02491
196/196 - 38s - loss: 47.4195 - MinusLogProbMetric: 47.4195 - val_loss: 48.2441 - val_MinusLogProbMetric: 48.2441 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 158/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:21:10.127 
Epoch 158/1000 
	 loss: 47.1598, MinusLogProbMetric: 47.1598, val_loss: 48.4552, val_MinusLogProbMetric: 48.4552

Epoch 158: val_loss did not improve from 48.02491
196/196 - 39s - loss: 47.1598 - MinusLogProbMetric: 47.1598 - val_loss: 48.4552 - val_MinusLogProbMetric: 48.4552 - lr: 1.2346e-05 - 39s/epoch - 199ms/step
Epoch 159/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:21:51.730 
Epoch 159/1000 
	 loss: 47.2151, MinusLogProbMetric: 47.2151, val_loss: 48.2347, val_MinusLogProbMetric: 48.2347

Epoch 159: val_loss did not improve from 48.02491
196/196 - 42s - loss: 47.2151 - MinusLogProbMetric: 47.2151 - val_loss: 48.2347 - val_MinusLogProbMetric: 48.2347 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 160/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:22:32.967 
Epoch 160/1000 
	 loss: 47.1940, MinusLogProbMetric: 47.1940, val_loss: 48.2402, val_MinusLogProbMetric: 48.2402

Epoch 160: val_loss did not improve from 48.02491
196/196 - 41s - loss: 47.1940 - MinusLogProbMetric: 47.1940 - val_loss: 48.2402 - val_MinusLogProbMetric: 48.2402 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 161/1000
2023-10-27 08:23:12.844 
Epoch 161/1000 
	 loss: 47.2878, MinusLogProbMetric: 47.2878, val_loss: 54.1114, val_MinusLogProbMetric: 54.1114

Epoch 161: val_loss did not improve from 48.02491
196/196 - 40s - loss: 47.2878 - MinusLogProbMetric: 47.2878 - val_loss: 54.1114 - val_MinusLogProbMetric: 54.1114 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 162/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:23:49.862 
Epoch 162/1000 
	 loss: 47.7444, MinusLogProbMetric: 47.7444, val_loss: 48.1436, val_MinusLogProbMetric: 48.1436

Epoch 162: val_loss did not improve from 48.02491
196/196 - 37s - loss: 47.7444 - MinusLogProbMetric: 47.7444 - val_loss: 48.1436 - val_MinusLogProbMetric: 48.1436 - lr: 1.2346e-05 - 37s/epoch - 189ms/step
Epoch 163/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:24:31.576 
Epoch 163/1000 
	 loss: 47.0681, MinusLogProbMetric: 47.0681, val_loss: 47.8691, val_MinusLogProbMetric: 47.8692

Epoch 163: val_loss improved from 48.02491 to 47.86915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 47.0681 - MinusLogProbMetric: 47.0681 - val_loss: 47.8691 - val_MinusLogProbMetric: 47.8692 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 164/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:25:14.524 
Epoch 164/1000 
	 loss: 46.8553, MinusLogProbMetric: 46.8553, val_loss: 47.9210, val_MinusLogProbMetric: 47.9210

Epoch 164: val_loss did not improve from 47.86915
196/196 - 42s - loss: 46.8553 - MinusLogProbMetric: 46.8553 - val_loss: 47.9210 - val_MinusLogProbMetric: 47.9210 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 165/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:25:54.940 
Epoch 165/1000 
	 loss: 47.0065, MinusLogProbMetric: 47.0065, val_loss: 49.0948, val_MinusLogProbMetric: 49.0948

Epoch 165: val_loss did not improve from 47.86915
196/196 - 40s - loss: 47.0065 - MinusLogProbMetric: 47.0065 - val_loss: 49.0948 - val_MinusLogProbMetric: 49.0948 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 166/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:26:33.537 
Epoch 166/1000 
	 loss: 47.6514, MinusLogProbMetric: 47.6514, val_loss: 47.9293, val_MinusLogProbMetric: 47.9293

Epoch 166: val_loss did not improve from 47.86915
196/196 - 39s - loss: 47.6514 - MinusLogProbMetric: 47.6514 - val_loss: 47.9293 - val_MinusLogProbMetric: 47.9293 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 167/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:27:14.327 
Epoch 167/1000 
	 loss: 47.1960, MinusLogProbMetric: 47.1960, val_loss: 48.0957, val_MinusLogProbMetric: 48.0957

Epoch 167: val_loss did not improve from 47.86915
196/196 - 41s - loss: 47.1960 - MinusLogProbMetric: 47.1960 - val_loss: 48.0957 - val_MinusLogProbMetric: 48.0957 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 168/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:27:57.289 
Epoch 168/1000 
	 loss: 47.3210, MinusLogProbMetric: 47.3210, val_loss: 48.6290, val_MinusLogProbMetric: 48.6290

Epoch 168: val_loss did not improve from 47.86915
196/196 - 43s - loss: 47.3210 - MinusLogProbMetric: 47.3210 - val_loss: 48.6290 - val_MinusLogProbMetric: 48.6290 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 169/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:28:39.356 
Epoch 169/1000 
	 loss: 47.0912, MinusLogProbMetric: 47.0912, val_loss: 47.6227, val_MinusLogProbMetric: 47.6227

Epoch 169: val_loss improved from 47.86915 to 47.62265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 47.0912 - MinusLogProbMetric: 47.0912 - val_loss: 47.6227 - val_MinusLogProbMetric: 47.6227 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 170/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:29:17.654 
Epoch 170/1000 
	 loss: 47.1411, MinusLogProbMetric: 47.1411, val_loss: 48.6500, val_MinusLogProbMetric: 48.6500

Epoch 170: val_loss did not improve from 47.62265
196/196 - 38s - loss: 47.1411 - MinusLogProbMetric: 47.1411 - val_loss: 48.6500 - val_MinusLogProbMetric: 48.6500 - lr: 1.2346e-05 - 38s/epoch - 192ms/step
Epoch 171/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:29:59.119 
Epoch 171/1000 
	 loss: 46.9154, MinusLogProbMetric: 46.9154, val_loss: 47.9242, val_MinusLogProbMetric: 47.9242

Epoch 171: val_loss did not improve from 47.62265
196/196 - 41s - loss: 46.9154 - MinusLogProbMetric: 46.9154 - val_loss: 47.9242 - val_MinusLogProbMetric: 47.9242 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 172/1000
2023-10-27 08:30:39.782 
Epoch 172/1000 
	 loss: 47.2705, MinusLogProbMetric: 47.2705, val_loss: 47.9026, val_MinusLogProbMetric: 47.9026

Epoch 172: val_loss did not improve from 47.62265
196/196 - 41s - loss: 47.2705 - MinusLogProbMetric: 47.2705 - val_loss: 47.9026 - val_MinusLogProbMetric: 47.9026 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 173/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:31:22.531 
Epoch 173/1000 
	 loss: 46.8446, MinusLogProbMetric: 46.8446, val_loss: 47.8384, val_MinusLogProbMetric: 47.8384

Epoch 173: val_loss did not improve from 47.62265
196/196 - 43s - loss: 46.8446 - MinusLogProbMetric: 46.8446 - val_loss: 47.8384 - val_MinusLogProbMetric: 47.8384 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 174/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:31:59.286 
Epoch 174/1000 
	 loss: 47.0399, MinusLogProbMetric: 47.0399, val_loss: 50.1335, val_MinusLogProbMetric: 50.1335

Epoch 174: val_loss did not improve from 47.62265
196/196 - 37s - loss: 47.0399 - MinusLogProbMetric: 47.0399 - val_loss: 50.1335 - val_MinusLogProbMetric: 50.1335 - lr: 1.2346e-05 - 37s/epoch - 188ms/step
Epoch 175/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:32:35.774 
Epoch 175/1000 
	 loss: 46.9831, MinusLogProbMetric: 46.9831, val_loss: 47.6497, val_MinusLogProbMetric: 47.6497

Epoch 175: val_loss did not improve from 47.62265
196/196 - 36s - loss: 46.9831 - MinusLogProbMetric: 46.9831 - val_loss: 47.6497 - val_MinusLogProbMetric: 47.6497 - lr: 1.2346e-05 - 36s/epoch - 186ms/step
Epoch 176/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:33:14.640 
Epoch 176/1000 
	 loss: 46.8260, MinusLogProbMetric: 46.8260, val_loss: 47.8757, val_MinusLogProbMetric: 47.8758

Epoch 176: val_loss did not improve from 47.62265
196/196 - 39s - loss: 46.8260 - MinusLogProbMetric: 46.8260 - val_loss: 47.8757 - val_MinusLogProbMetric: 47.8758 - lr: 1.2346e-05 - 39s/epoch - 198ms/step
Epoch 177/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:33:53.818 
Epoch 177/1000 
	 loss: 46.7488, MinusLogProbMetric: 46.7488, val_loss: 48.1177, val_MinusLogProbMetric: 48.1178

Epoch 177: val_loss did not improve from 47.62265
196/196 - 39s - loss: 46.7488 - MinusLogProbMetric: 46.7488 - val_loss: 48.1177 - val_MinusLogProbMetric: 48.1178 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 178/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:34:30.359 
Epoch 178/1000 
	 loss: 46.9385, MinusLogProbMetric: 46.9385, val_loss: 47.8842, val_MinusLogProbMetric: 47.8842

Epoch 178: val_loss did not improve from 47.62265
196/196 - 37s - loss: 46.9385 - MinusLogProbMetric: 46.9385 - val_loss: 47.8842 - val_MinusLogProbMetric: 47.8842 - lr: 1.2346e-05 - 37s/epoch - 186ms/step
Epoch 179/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:35:09.260 
Epoch 179/1000 
	 loss: 46.8791, MinusLogProbMetric: 46.8791, val_loss: 47.5717, val_MinusLogProbMetric: 47.5717

Epoch 179: val_loss improved from 47.62265 to 47.57171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 40s - loss: 46.8791 - MinusLogProbMetric: 46.8791 - val_loss: 47.5717 - val_MinusLogProbMetric: 47.5717 - lr: 1.2346e-05 - 40s/epoch - 202ms/step
Epoch 180/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:35:50.339 
Epoch 180/1000 
	 loss: 46.7826, MinusLogProbMetric: 46.7826, val_loss: 47.5324, val_MinusLogProbMetric: 47.5324

Epoch 180: val_loss improved from 47.57171 to 47.53242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 41s - loss: 46.7826 - MinusLogProbMetric: 46.7826 - val_loss: 47.5324 - val_MinusLogProbMetric: 47.5324 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 181/1000
2023-10-27 08:36:33.357 
Epoch 181/1000 
	 loss: 46.9891, MinusLogProbMetric: 46.9891, val_loss: 48.6255, val_MinusLogProbMetric: 48.6255

Epoch 181: val_loss did not improve from 47.53242
196/196 - 42s - loss: 46.9891 - MinusLogProbMetric: 46.9891 - val_loss: 48.6255 - val_MinusLogProbMetric: 48.6255 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 182/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:37:13.729 
Epoch 182/1000 
	 loss: 46.7749, MinusLogProbMetric: 46.7749, val_loss: 47.6150, val_MinusLogProbMetric: 47.6150

Epoch 182: val_loss did not improve from 47.53242
196/196 - 40s - loss: 46.7749 - MinusLogProbMetric: 46.7749 - val_loss: 47.6150 - val_MinusLogProbMetric: 47.6150 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 183/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:37:53.659 
Epoch 183/1000 
	 loss: 46.8014, MinusLogProbMetric: 46.8014, val_loss: 47.8031, val_MinusLogProbMetric: 47.8031

Epoch 183: val_loss did not improve from 47.53242
196/196 - 40s - loss: 46.8014 - MinusLogProbMetric: 46.8014 - val_loss: 47.8031 - val_MinusLogProbMetric: 47.8031 - lr: 1.2346e-05 - 40s/epoch - 204ms/step
Epoch 184/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:38:33.575 
Epoch 184/1000 
	 loss: 46.8881, MinusLogProbMetric: 46.8881, val_loss: 47.6388, val_MinusLogProbMetric: 47.6388

Epoch 184: val_loss did not improve from 47.53242
196/196 - 40s - loss: 46.8881 - MinusLogProbMetric: 46.8881 - val_loss: 47.6388 - val_MinusLogProbMetric: 47.6388 - lr: 1.2346e-05 - 40s/epoch - 204ms/step
Epoch 185/1000
2023-10-27 08:39:16.561 
Epoch 185/1000 
	 loss: 46.7820, MinusLogProbMetric: 46.7820, val_loss: 47.6696, val_MinusLogProbMetric: 47.6696

Epoch 185: val_loss did not improve from 47.53242
196/196 - 43s - loss: 46.7820 - MinusLogProbMetric: 46.7820 - val_loss: 47.6696 - val_MinusLogProbMetric: 47.6696 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 186/1000
2023-10-27 08:39:58.056 
Epoch 186/1000 
	 loss: 46.9971, MinusLogProbMetric: 46.9971, val_loss: 48.3102, val_MinusLogProbMetric: 48.3102

Epoch 186: val_loss did not improve from 47.53242
196/196 - 41s - loss: 46.9971 - MinusLogProbMetric: 46.9971 - val_loss: 48.3102 - val_MinusLogProbMetric: 48.3102 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 187/1000
2023-10-27 08:40:36.792 
Epoch 187/1000 
	 loss: 46.8169, MinusLogProbMetric: 46.8169, val_loss: 47.8654, val_MinusLogProbMetric: 47.8654

Epoch 187: val_loss did not improve from 47.53242
196/196 - 39s - loss: 46.8169 - MinusLogProbMetric: 46.8169 - val_loss: 47.8654 - val_MinusLogProbMetric: 47.8654 - lr: 1.2346e-05 - 39s/epoch - 198ms/step
Epoch 188/1000
2023-10-27 08:41:14.376 
Epoch 188/1000 
	 loss: 46.6256, MinusLogProbMetric: 46.6256, val_loss: 47.4748, val_MinusLogProbMetric: 47.4748

Epoch 188: val_loss improved from 47.53242 to 47.47483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 38s - loss: 46.6256 - MinusLogProbMetric: 46.6256 - val_loss: 47.4748 - val_MinusLogProbMetric: 47.4748 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 189/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-27 08:41:56.608 
Epoch 189/1000 
	 loss: 46.4665, MinusLogProbMetric: 46.4665, val_loss: 47.4980, val_MinusLogProbMetric: 47.4980

Epoch 189: val_loss did not improve from 47.47483
196/196 - 42s - loss: 46.4665 - MinusLogProbMetric: 46.4665 - val_loss: 47.4980 - val_MinusLogProbMetric: 47.4980 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 190/1000
2023-10-27 08:42:35.786 
Epoch 190/1000 
	 loss: 46.5970, MinusLogProbMetric: 46.5970, val_loss: 47.5453, val_MinusLogProbMetric: 47.5453

Epoch 190: val_loss did not improve from 47.47483
196/196 - 39s - loss: 46.5970 - MinusLogProbMetric: 46.5970 - val_loss: 47.5453 - val_MinusLogProbMetric: 47.5453 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 191/1000
2023-10-27 08:43:14.427 
Epoch 191/1000 
	 loss: 46.4355, MinusLogProbMetric: 46.4355, val_loss: 47.8201, val_MinusLogProbMetric: 47.8201

Epoch 191: val_loss did not improve from 47.47483
196/196 - 39s - loss: 46.4355 - MinusLogProbMetric: 46.4355 - val_loss: 47.8201 - val_MinusLogProbMetric: 47.8201 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 192/1000
2023-10-27 08:43:52.400 
Epoch 192/1000 
	 loss: 46.8220, MinusLogProbMetric: 46.8220, val_loss: 48.4900, val_MinusLogProbMetric: 48.4900

Epoch 192: val_loss did not improve from 47.47483
196/196 - 38s - loss: 46.8220 - MinusLogProbMetric: 46.8220 - val_loss: 48.4900 - val_MinusLogProbMetric: 48.4900 - lr: 1.2346e-05 - 38s/epoch - 194ms/step
Epoch 193/1000
2023-10-27 08:44:32.918 
Epoch 193/1000 
	 loss: 47.0990, MinusLogProbMetric: 47.0990, val_loss: 50.3945, val_MinusLogProbMetric: 50.3945

Epoch 193: val_loss did not improve from 47.47483
196/196 - 41s - loss: 47.0990 - MinusLogProbMetric: 47.0990 - val_loss: 50.3945 - val_MinusLogProbMetric: 50.3945 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 194/1000
2023-10-27 08:45:13.220 
Epoch 194/1000 
	 loss: 46.8435, MinusLogProbMetric: 46.8435, val_loss: 48.8881, val_MinusLogProbMetric: 48.8881

Epoch 194: val_loss did not improve from 47.47483
196/196 - 40s - loss: 46.8435 - MinusLogProbMetric: 46.8435 - val_loss: 48.8881 - val_MinusLogProbMetric: 48.8881 - lr: 1.2346e-05 - 40s/epoch - 206ms/step
Epoch 195/1000
2023-10-27 08:45:51.982 
Epoch 195/1000 
	 loss: 46.5247, MinusLogProbMetric: 46.5247, val_loss: 47.3403, val_MinusLogProbMetric: 47.3403

Epoch 195: val_loss improved from 47.47483 to 47.34032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 46.5247 - MinusLogProbMetric: 46.5247 - val_loss: 47.3403 - val_MinusLogProbMetric: 47.3403 - lr: 1.2346e-05 - 39s/epoch - 201ms/step
Epoch 196/1000
2023-10-27 08:46:30.583 
Epoch 196/1000 
	 loss: 46.4025, MinusLogProbMetric: 46.4025, val_loss: 47.9293, val_MinusLogProbMetric: 47.9293

Epoch 196: val_loss did not improve from 47.34032
196/196 - 38s - loss: 46.4025 - MinusLogProbMetric: 46.4025 - val_loss: 47.9293 - val_MinusLogProbMetric: 47.9293 - lr: 1.2346e-05 - 38s/epoch - 193ms/step
Epoch 197/1000
2023-10-27 08:47:10.435 
Epoch 197/1000 
	 loss: 46.5399, MinusLogProbMetric: 46.5399, val_loss: 47.3759, val_MinusLogProbMetric: 47.3759

Epoch 197: val_loss did not improve from 47.34032
196/196 - 40s - loss: 46.5399 - MinusLogProbMetric: 46.5399 - val_loss: 47.3759 - val_MinusLogProbMetric: 47.3759 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 198/1000
2023-10-27 08:47:51.805 
Epoch 198/1000 
	 loss: 46.4911, MinusLogProbMetric: 46.4911, val_loss: 48.3757, val_MinusLogProbMetric: 48.3757

Epoch 198: val_loss did not improve from 47.34032
196/196 - 41s - loss: 46.4911 - MinusLogProbMetric: 46.4911 - val_loss: 48.3757 - val_MinusLogProbMetric: 48.3757 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 199/1000
2023-10-27 08:48:32.330 
Epoch 199/1000 
	 loss: 46.8949, MinusLogProbMetric: 46.8949, val_loss: 47.6764, val_MinusLogProbMetric: 47.6764

Epoch 199: val_loss did not improve from 47.34032
196/196 - 41s - loss: 46.8949 - MinusLogProbMetric: 46.8949 - val_loss: 47.6764 - val_MinusLogProbMetric: 47.6764 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 200/1000
2023-10-27 08:49:09.695 
Epoch 200/1000 
	 loss: 46.3369, MinusLogProbMetric: 46.3369, val_loss: 48.2701, val_MinusLogProbMetric: 48.2701

Epoch 200: val_loss did not improve from 47.34032
196/196 - 37s - loss: 46.3369 - MinusLogProbMetric: 46.3369 - val_loss: 48.2701 - val_MinusLogProbMetric: 48.2701 - lr: 1.2346e-05 - 37s/epoch - 191ms/step
Epoch 201/1000
2023-10-27 08:49:48.336 
Epoch 201/1000 
	 loss: 46.7920, MinusLogProbMetric: 46.7920, val_loss: 47.4079, val_MinusLogProbMetric: 47.4079

Epoch 201: val_loss did not improve from 47.34032
196/196 - 39s - loss: 46.7920 - MinusLogProbMetric: 46.7920 - val_loss: 47.4079 - val_MinusLogProbMetric: 47.4079 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 202/1000
2023-10-27 08:50:28.463 
Epoch 202/1000 
	 loss: 46.7384, MinusLogProbMetric: 46.7384, val_loss: 47.8335, val_MinusLogProbMetric: 47.8335

Epoch 202: val_loss did not improve from 47.34032
196/196 - 40s - loss: 46.7384 - MinusLogProbMetric: 46.7384 - val_loss: 47.8335 - val_MinusLogProbMetric: 47.8335 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 203/1000
2023-10-27 08:51:09.748 
Epoch 203/1000 
	 loss: 46.4863, MinusLogProbMetric: 46.4863, val_loss: 48.0425, val_MinusLogProbMetric: 48.0425

Epoch 203: val_loss did not improve from 47.34032
196/196 - 41s - loss: 46.4863 - MinusLogProbMetric: 46.4863 - val_loss: 48.0425 - val_MinusLogProbMetric: 48.0425 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 204/1000
2023-10-27 08:51:51.757 
Epoch 204/1000 
	 loss: 46.2343, MinusLogProbMetric: 46.2343, val_loss: 47.4145, val_MinusLogProbMetric: 47.4145

Epoch 204: val_loss did not improve from 47.34032
196/196 - 42s - loss: 46.2343 - MinusLogProbMetric: 46.2343 - val_loss: 47.4145 - val_MinusLogProbMetric: 47.4145 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 205/1000
2023-10-27 08:52:34.849 
Epoch 205/1000 
	 loss: 46.3784, MinusLogProbMetric: 46.3784, val_loss: 47.1042, val_MinusLogProbMetric: 47.1042

Epoch 205: val_loss improved from 47.34032 to 47.10415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 46.3784 - MinusLogProbMetric: 46.3784 - val_loss: 47.1042 - val_MinusLogProbMetric: 47.1042 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 206/1000
2023-10-27 08:53:18.580 
Epoch 206/1000 
	 loss: 47.0889, MinusLogProbMetric: 47.0889, val_loss: 48.2944, val_MinusLogProbMetric: 48.2944

Epoch 206: val_loss did not improve from 47.10415
196/196 - 43s - loss: 47.0889 - MinusLogProbMetric: 47.0889 - val_loss: 48.2944 - val_MinusLogProbMetric: 48.2944 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 207/1000
2023-10-27 08:54:01.655 
Epoch 207/1000 
	 loss: 46.3937, MinusLogProbMetric: 46.3937, val_loss: 47.8561, val_MinusLogProbMetric: 47.8561

Epoch 207: val_loss did not improve from 47.10415
196/196 - 43s - loss: 46.3937 - MinusLogProbMetric: 46.3937 - val_loss: 47.8561 - val_MinusLogProbMetric: 47.8561 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 208/1000
2023-10-27 08:54:44.085 
Epoch 208/1000 
	 loss: 46.2859, MinusLogProbMetric: 46.2859, val_loss: 47.5791, val_MinusLogProbMetric: 47.5791

Epoch 208: val_loss did not improve from 47.10415
196/196 - 42s - loss: 46.2859 - MinusLogProbMetric: 46.2859 - val_loss: 47.5791 - val_MinusLogProbMetric: 47.5791 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 209/1000
2023-10-27 08:55:25.225 
Epoch 209/1000 
	 loss: 46.4072, MinusLogProbMetric: 46.4072, val_loss: 49.5286, val_MinusLogProbMetric: 49.5286

Epoch 209: val_loss did not improve from 47.10415
196/196 - 41s - loss: 46.4072 - MinusLogProbMetric: 46.4072 - val_loss: 49.5286 - val_MinusLogProbMetric: 49.5286 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 210/1000
2023-10-27 08:56:07.776 
Epoch 210/1000 
	 loss: 46.1911, MinusLogProbMetric: 46.1911, val_loss: 47.1964, val_MinusLogProbMetric: 47.1964

Epoch 210: val_loss did not improve from 47.10415
196/196 - 43s - loss: 46.1911 - MinusLogProbMetric: 46.1911 - val_loss: 47.1964 - val_MinusLogProbMetric: 47.1964 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 211/1000
2023-10-27 08:56:48.003 
Epoch 211/1000 
	 loss: 47.2686, MinusLogProbMetric: 47.2686, val_loss: 47.3718, val_MinusLogProbMetric: 47.3718

Epoch 211: val_loss did not improve from 47.10415
196/196 - 40s - loss: 47.2686 - MinusLogProbMetric: 47.2686 - val_loss: 47.3718 - val_MinusLogProbMetric: 47.3718 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 212/1000
2023-10-27 08:57:30.717 
Epoch 212/1000 
	 loss: 46.4638, MinusLogProbMetric: 46.4638, val_loss: 47.4739, val_MinusLogProbMetric: 47.4739

Epoch 212: val_loss did not improve from 47.10415
196/196 - 43s - loss: 46.4638 - MinusLogProbMetric: 46.4638 - val_loss: 47.4739 - val_MinusLogProbMetric: 47.4739 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 213/1000
2023-10-27 08:58:11.925 
Epoch 213/1000 
	 loss: 46.3819, MinusLogProbMetric: 46.3819, val_loss: 48.7321, val_MinusLogProbMetric: 48.7321

Epoch 213: val_loss did not improve from 47.10415
196/196 - 41s - loss: 46.3819 - MinusLogProbMetric: 46.3819 - val_loss: 48.7321 - val_MinusLogProbMetric: 48.7321 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 214/1000
2023-10-27 08:58:54.138 
Epoch 214/1000 
	 loss: 46.4422, MinusLogProbMetric: 46.4422, val_loss: 47.0204, val_MinusLogProbMetric: 47.0204

Epoch 214: val_loss improved from 47.10415 to 47.02036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 46.4422 - MinusLogProbMetric: 46.4422 - val_loss: 47.0204 - val_MinusLogProbMetric: 47.0204 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 215/1000
2023-10-27 08:59:35.965 
Epoch 215/1000 
	 loss: 46.1265, MinusLogProbMetric: 46.1265, val_loss: 47.2176, val_MinusLogProbMetric: 47.2176

Epoch 215: val_loss did not improve from 47.02036
196/196 - 41s - loss: 46.1265 - MinusLogProbMetric: 46.1265 - val_loss: 47.2176 - val_MinusLogProbMetric: 47.2176 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 216/1000
2023-10-27 09:00:18.732 
Epoch 216/1000 
	 loss: 46.5879, MinusLogProbMetric: 46.5879, val_loss: 51.3897, val_MinusLogProbMetric: 51.3897

Epoch 216: val_loss did not improve from 47.02036
196/196 - 43s - loss: 46.5879 - MinusLogProbMetric: 46.5879 - val_loss: 51.3897 - val_MinusLogProbMetric: 51.3897 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 217/1000
2023-10-27 09:01:01.323 
Epoch 217/1000 
	 loss: 46.6380, MinusLogProbMetric: 46.6380, val_loss: 47.7916, val_MinusLogProbMetric: 47.7916

Epoch 217: val_loss did not improve from 47.02036
196/196 - 43s - loss: 46.6380 - MinusLogProbMetric: 46.6380 - val_loss: 47.7916 - val_MinusLogProbMetric: 47.7916 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 218/1000
2023-10-27 09:01:42.274 
Epoch 218/1000 
	 loss: 46.4669, MinusLogProbMetric: 46.4669, val_loss: 47.1570, val_MinusLogProbMetric: 47.1570

Epoch 218: val_loss did not improve from 47.02036
196/196 - 41s - loss: 46.4669 - MinusLogProbMetric: 46.4669 - val_loss: 47.1570 - val_MinusLogProbMetric: 47.1570 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 219/1000
2023-10-27 09:02:23.831 
Epoch 219/1000 
	 loss: 46.0411, MinusLogProbMetric: 46.0411, val_loss: 47.3113, val_MinusLogProbMetric: 47.3113

Epoch 219: val_loss did not improve from 47.02036
196/196 - 42s - loss: 46.0411 - MinusLogProbMetric: 46.0411 - val_loss: 47.3113 - val_MinusLogProbMetric: 47.3113 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 220/1000
2023-10-27 09:03:05.773 
Epoch 220/1000 
	 loss: 46.4828, MinusLogProbMetric: 46.4828, val_loss: 47.5523, val_MinusLogProbMetric: 47.5523

Epoch 220: val_loss did not improve from 47.02036
196/196 - 42s - loss: 46.4828 - MinusLogProbMetric: 46.4828 - val_loss: 47.5523 - val_MinusLogProbMetric: 47.5523 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 221/1000
2023-10-27 09:03:46.369 
Epoch 221/1000 
	 loss: 46.0559, MinusLogProbMetric: 46.0559, val_loss: 47.5290, val_MinusLogProbMetric: 47.5290

Epoch 221: val_loss did not improve from 47.02036
196/196 - 41s - loss: 46.0559 - MinusLogProbMetric: 46.0559 - val_loss: 47.5290 - val_MinusLogProbMetric: 47.5290 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 222/1000
2023-10-27 09:04:28.790 
Epoch 222/1000 
	 loss: 46.2569, MinusLogProbMetric: 46.2569, val_loss: 46.9945, val_MinusLogProbMetric: 46.9945

Epoch 222: val_loss improved from 47.02036 to 46.99450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 46.2569 - MinusLogProbMetric: 46.2569 - val_loss: 46.9945 - val_MinusLogProbMetric: 46.9945 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 223/1000
2023-10-27 09:05:11.140 
Epoch 223/1000 
	 loss: 46.4792, MinusLogProbMetric: 46.4792, val_loss: 47.1795, val_MinusLogProbMetric: 47.1795

Epoch 223: val_loss did not improve from 46.99450
196/196 - 41s - loss: 46.4792 - MinusLogProbMetric: 46.4792 - val_loss: 47.1795 - val_MinusLogProbMetric: 47.1795 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 224/1000
2023-10-27 09:05:53.017 
Epoch 224/1000 
	 loss: 46.0507, MinusLogProbMetric: 46.0507, val_loss: 47.0392, val_MinusLogProbMetric: 47.0392

Epoch 224: val_loss did not improve from 46.99450
196/196 - 42s - loss: 46.0507 - MinusLogProbMetric: 46.0507 - val_loss: 47.0392 - val_MinusLogProbMetric: 47.0392 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 225/1000
2023-10-27 09:06:35.116 
Epoch 225/1000 
	 loss: 46.1625, MinusLogProbMetric: 46.1625, val_loss: 47.4332, val_MinusLogProbMetric: 47.4332

Epoch 225: val_loss did not improve from 46.99450
196/196 - 42s - loss: 46.1625 - MinusLogProbMetric: 46.1625 - val_loss: 47.4332 - val_MinusLogProbMetric: 47.4332 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 226/1000
2023-10-27 09:07:17.354 
Epoch 226/1000 
	 loss: 45.9130, MinusLogProbMetric: 45.9130, val_loss: 47.1821, val_MinusLogProbMetric: 47.1821

Epoch 226: val_loss did not improve from 46.99450
196/196 - 42s - loss: 45.9130 - MinusLogProbMetric: 45.9130 - val_loss: 47.1821 - val_MinusLogProbMetric: 47.1821 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 227/1000
2023-10-27 09:08:00.313 
Epoch 227/1000 
	 loss: 46.3005, MinusLogProbMetric: 46.3005, val_loss: 48.1344, val_MinusLogProbMetric: 48.1344

Epoch 227: val_loss did not improve from 46.99450
196/196 - 43s - loss: 46.3005 - MinusLogProbMetric: 46.3005 - val_loss: 48.1344 - val_MinusLogProbMetric: 48.1344 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 228/1000
2023-10-27 09:08:42.273 
Epoch 228/1000 
	 loss: 46.2595, MinusLogProbMetric: 46.2595, val_loss: 46.9122, val_MinusLogProbMetric: 46.9122

Epoch 228: val_loss improved from 46.99450 to 46.91218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 46.2595 - MinusLogProbMetric: 46.2595 - val_loss: 46.9122 - val_MinusLogProbMetric: 46.9122 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 229/1000
2023-10-27 09:09:24.299 
Epoch 229/1000 
	 loss: 46.0994, MinusLogProbMetric: 46.0994, val_loss: 47.2139, val_MinusLogProbMetric: 47.2139

Epoch 229: val_loss did not improve from 46.91218
196/196 - 41s - loss: 46.0994 - MinusLogProbMetric: 46.0994 - val_loss: 47.2139 - val_MinusLogProbMetric: 47.2139 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 230/1000
2023-10-27 09:10:06.581 
Epoch 230/1000 
	 loss: 45.9581, MinusLogProbMetric: 45.9581, val_loss: 49.7698, val_MinusLogProbMetric: 49.7698

Epoch 230: val_loss did not improve from 46.91218
196/196 - 42s - loss: 45.9581 - MinusLogProbMetric: 45.9581 - val_loss: 49.7698 - val_MinusLogProbMetric: 49.7698 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 231/1000
2023-10-27 09:10:49.403 
Epoch 231/1000 
	 loss: 46.3623, MinusLogProbMetric: 46.3623, val_loss: 47.0142, val_MinusLogProbMetric: 47.0142

Epoch 231: val_loss did not improve from 46.91218
196/196 - 43s - loss: 46.3623 - MinusLogProbMetric: 46.3623 - val_loss: 47.0142 - val_MinusLogProbMetric: 47.0142 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 232/1000
2023-10-27 09:11:32.233 
Epoch 232/1000 
	 loss: 46.1531, MinusLogProbMetric: 46.1531, val_loss: 49.8111, val_MinusLogProbMetric: 49.8111

Epoch 232: val_loss did not improve from 46.91218
196/196 - 43s - loss: 46.1531 - MinusLogProbMetric: 46.1531 - val_loss: 49.8111 - val_MinusLogProbMetric: 49.8111 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 233/1000
2023-10-27 09:12:15.139 
Epoch 233/1000 
	 loss: 46.4583, MinusLogProbMetric: 46.4583, val_loss: 47.4965, val_MinusLogProbMetric: 47.4965

Epoch 233: val_loss did not improve from 46.91218
196/196 - 43s - loss: 46.4583 - MinusLogProbMetric: 46.4583 - val_loss: 47.4965 - val_MinusLogProbMetric: 47.4965 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 234/1000
2023-10-27 09:12:57.992 
Epoch 234/1000 
	 loss: 45.9561, MinusLogProbMetric: 45.9561, val_loss: 46.9763, val_MinusLogProbMetric: 46.9763

Epoch 234: val_loss did not improve from 46.91218
196/196 - 43s - loss: 45.9561 - MinusLogProbMetric: 45.9561 - val_loss: 46.9763 - val_MinusLogProbMetric: 46.9763 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 235/1000
2023-10-27 09:13:40.539 
Epoch 235/1000 
	 loss: 45.8510, MinusLogProbMetric: 45.8510, val_loss: 46.8280, val_MinusLogProbMetric: 46.8280

Epoch 235: val_loss improved from 46.91218 to 46.82797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 45.8510 - MinusLogProbMetric: 45.8510 - val_loss: 46.8280 - val_MinusLogProbMetric: 46.8280 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 236/1000
2023-10-27 09:14:23.999 
Epoch 236/1000 
	 loss: 45.9508, MinusLogProbMetric: 45.9508, val_loss: 46.9298, val_MinusLogProbMetric: 46.9298

Epoch 236: val_loss did not improve from 46.82797
196/196 - 43s - loss: 45.9508 - MinusLogProbMetric: 45.9508 - val_loss: 46.9298 - val_MinusLogProbMetric: 46.9298 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 237/1000
2023-10-27 09:15:06.087 
Epoch 237/1000 
	 loss: 45.8713, MinusLogProbMetric: 45.8713, val_loss: 48.0664, val_MinusLogProbMetric: 48.0664

Epoch 237: val_loss did not improve from 46.82797
196/196 - 42s - loss: 45.8713 - MinusLogProbMetric: 45.8713 - val_loss: 48.0664 - val_MinusLogProbMetric: 48.0664 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 238/1000
2023-10-27 09:15:49.205 
Epoch 238/1000 
	 loss: 46.0766, MinusLogProbMetric: 46.0766, val_loss: 47.2744, val_MinusLogProbMetric: 47.2744

Epoch 238: val_loss did not improve from 46.82797
196/196 - 43s - loss: 46.0766 - MinusLogProbMetric: 46.0766 - val_loss: 47.2744 - val_MinusLogProbMetric: 47.2744 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 239/1000
2023-10-27 09:16:31.559 
Epoch 239/1000 
	 loss: 45.8163, MinusLogProbMetric: 45.8163, val_loss: 47.1211, val_MinusLogProbMetric: 47.1211

Epoch 239: val_loss did not improve from 46.82797
196/196 - 42s - loss: 45.8163 - MinusLogProbMetric: 45.8163 - val_loss: 47.1211 - val_MinusLogProbMetric: 47.1211 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 240/1000
2023-10-27 09:17:13.512 
Epoch 240/1000 
	 loss: 46.0143, MinusLogProbMetric: 46.0143, val_loss: 48.8856, val_MinusLogProbMetric: 48.8856

Epoch 240: val_loss did not improve from 46.82797
196/196 - 42s - loss: 46.0143 - MinusLogProbMetric: 46.0143 - val_loss: 48.8856 - val_MinusLogProbMetric: 48.8856 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 241/1000
2023-10-27 09:17:55.596 
Epoch 241/1000 
	 loss: 46.6110, MinusLogProbMetric: 46.6110, val_loss: 46.8417, val_MinusLogProbMetric: 46.8417

Epoch 241: val_loss did not improve from 46.82797
196/196 - 42s - loss: 46.6110 - MinusLogProbMetric: 46.6110 - val_loss: 46.8417 - val_MinusLogProbMetric: 46.8417 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 242/1000
2023-10-27 09:18:36.880 
Epoch 242/1000 
	 loss: 45.7544, MinusLogProbMetric: 45.7544, val_loss: 47.3585, val_MinusLogProbMetric: 47.3585

Epoch 242: val_loss did not improve from 46.82797
196/196 - 41s - loss: 45.7544 - MinusLogProbMetric: 45.7544 - val_loss: 47.3585 - val_MinusLogProbMetric: 47.3585 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 243/1000
2023-10-27 09:19:19.352 
Epoch 243/1000 
	 loss: 45.8265, MinusLogProbMetric: 45.8265, val_loss: 46.6085, val_MinusLogProbMetric: 46.6085

Epoch 243: val_loss improved from 46.82797 to 46.60845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 45.8265 - MinusLogProbMetric: 45.8265 - val_loss: 46.6085 - val_MinusLogProbMetric: 46.6085 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 244/1000
2023-10-27 09:20:02.161 
Epoch 244/1000 
	 loss: 46.0413, MinusLogProbMetric: 46.0413, val_loss: 46.7522, val_MinusLogProbMetric: 46.7522

Epoch 244: val_loss did not improve from 46.60845
196/196 - 42s - loss: 46.0413 - MinusLogProbMetric: 46.0413 - val_loss: 46.7522 - val_MinusLogProbMetric: 46.7522 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 245/1000
2023-10-27 09:20:44.926 
Epoch 245/1000 
	 loss: 47.0619, MinusLogProbMetric: 47.0619, val_loss: 47.0088, val_MinusLogProbMetric: 47.0088

Epoch 245: val_loss did not improve from 46.60845
196/196 - 43s - loss: 47.0619 - MinusLogProbMetric: 47.0619 - val_loss: 47.0088 - val_MinusLogProbMetric: 47.0088 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 246/1000
2023-10-27 09:21:27.026 
Epoch 246/1000 
	 loss: 45.8303, MinusLogProbMetric: 45.8303, val_loss: 47.0394, val_MinusLogProbMetric: 47.0394

Epoch 246: val_loss did not improve from 46.60845
196/196 - 42s - loss: 45.8303 - MinusLogProbMetric: 45.8303 - val_loss: 47.0394 - val_MinusLogProbMetric: 47.0394 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 247/1000
2023-10-27 09:22:10.218 
Epoch 247/1000 
	 loss: 45.7525, MinusLogProbMetric: 45.7525, val_loss: 46.7004, val_MinusLogProbMetric: 46.7004

Epoch 247: val_loss did not improve from 46.60845
196/196 - 43s - loss: 45.7525 - MinusLogProbMetric: 45.7525 - val_loss: 46.7004 - val_MinusLogProbMetric: 46.7004 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 248/1000
2023-10-27 09:22:52.030 
Epoch 248/1000 
	 loss: 46.0548, MinusLogProbMetric: 46.0548, val_loss: 48.6254, val_MinusLogProbMetric: 48.6254

Epoch 248: val_loss did not improve from 46.60845
196/196 - 42s - loss: 46.0548 - MinusLogProbMetric: 46.0548 - val_loss: 48.6254 - val_MinusLogProbMetric: 48.6254 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 249/1000
2023-10-27 09:23:35.271 
Epoch 249/1000 
	 loss: 45.8834, MinusLogProbMetric: 45.8834, val_loss: 46.8174, val_MinusLogProbMetric: 46.8174

Epoch 249: val_loss did not improve from 46.60845
196/196 - 43s - loss: 45.8834 - MinusLogProbMetric: 45.8834 - val_loss: 46.8174 - val_MinusLogProbMetric: 46.8174 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 250/1000
2023-10-27 09:24:17.493 
Epoch 250/1000 
	 loss: 45.5948, MinusLogProbMetric: 45.5948, val_loss: 54.3007, val_MinusLogProbMetric: 54.3007

Epoch 250: val_loss did not improve from 46.60845
196/196 - 42s - loss: 45.5948 - MinusLogProbMetric: 45.5948 - val_loss: 54.3007 - val_MinusLogProbMetric: 54.3007 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 251/1000
2023-10-27 09:24:59.958 
Epoch 251/1000 
	 loss: 46.0071, MinusLogProbMetric: 46.0071, val_loss: 47.0445, val_MinusLogProbMetric: 47.0445

Epoch 251: val_loss did not improve from 46.60845
196/196 - 42s - loss: 46.0071 - MinusLogProbMetric: 46.0071 - val_loss: 47.0445 - val_MinusLogProbMetric: 47.0445 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 252/1000
2023-10-27 09:25:42.945 
Epoch 252/1000 
	 loss: 45.8600, MinusLogProbMetric: 45.8600, val_loss: 46.9571, val_MinusLogProbMetric: 46.9571

Epoch 252: val_loss did not improve from 46.60845
196/196 - 43s - loss: 45.8600 - MinusLogProbMetric: 45.8600 - val_loss: 46.9571 - val_MinusLogProbMetric: 46.9571 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 253/1000
2023-10-27 09:26:24.790 
Epoch 253/1000 
	 loss: 45.7724, MinusLogProbMetric: 45.7724, val_loss: 47.4979, val_MinusLogProbMetric: 47.4979

Epoch 253: val_loss did not improve from 46.60845
196/196 - 42s - loss: 45.7724 - MinusLogProbMetric: 45.7724 - val_loss: 47.4979 - val_MinusLogProbMetric: 47.4979 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 254/1000
2023-10-27 09:27:05.806 
Epoch 254/1000 
	 loss: 45.7987, MinusLogProbMetric: 45.7987, val_loss: 47.3131, val_MinusLogProbMetric: 47.3131

Epoch 254: val_loss did not improve from 46.60845
196/196 - 41s - loss: 45.7987 - MinusLogProbMetric: 45.7987 - val_loss: 47.3131 - val_MinusLogProbMetric: 47.3131 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 255/1000
2023-10-27 09:27:47.354 
Epoch 255/1000 
	 loss: 45.5829, MinusLogProbMetric: 45.5829, val_loss: 46.6932, val_MinusLogProbMetric: 46.6932

Epoch 255: val_loss did not improve from 46.60845
196/196 - 42s - loss: 45.5829 - MinusLogProbMetric: 45.5829 - val_loss: 46.6932 - val_MinusLogProbMetric: 46.6932 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 256/1000
2023-10-27 09:28:29.306 
Epoch 256/1000 
	 loss: 45.6293, MinusLogProbMetric: 45.6293, val_loss: 47.0517, val_MinusLogProbMetric: 47.0517

Epoch 256: val_loss did not improve from 46.60845
196/196 - 42s - loss: 45.6293 - MinusLogProbMetric: 45.6293 - val_loss: 47.0517 - val_MinusLogProbMetric: 47.0517 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 257/1000
2023-10-27 09:29:10.010 
Epoch 257/1000 
	 loss: 45.9177, MinusLogProbMetric: 45.9177, val_loss: 46.4661, val_MinusLogProbMetric: 46.4661

Epoch 257: val_loss improved from 46.60845 to 46.46614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 45.9177 - MinusLogProbMetric: 45.9177 - val_loss: 46.4661 - val_MinusLogProbMetric: 46.4661 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 258/1000
2023-10-27 09:29:53.321 
Epoch 258/1000 
	 loss: 46.1194, MinusLogProbMetric: 46.1194, val_loss: 46.7188, val_MinusLogProbMetric: 46.7188

Epoch 258: val_loss did not improve from 46.46614
196/196 - 43s - loss: 46.1194 - MinusLogProbMetric: 46.1194 - val_loss: 46.7188 - val_MinusLogProbMetric: 46.7188 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 259/1000
2023-10-27 09:30:35.795 
Epoch 259/1000 
	 loss: 45.4543, MinusLogProbMetric: 45.4543, val_loss: 46.7182, val_MinusLogProbMetric: 46.7182

Epoch 259: val_loss did not improve from 46.46614
196/196 - 42s - loss: 45.4543 - MinusLogProbMetric: 45.4543 - val_loss: 46.7182 - val_MinusLogProbMetric: 46.7182 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 260/1000
2023-10-27 09:31:18.226 
Epoch 260/1000 
	 loss: 45.6477, MinusLogProbMetric: 45.6477, val_loss: 46.8572, val_MinusLogProbMetric: 46.8572

Epoch 260: val_loss did not improve from 46.46614
196/196 - 42s - loss: 45.6477 - MinusLogProbMetric: 45.6477 - val_loss: 46.8572 - val_MinusLogProbMetric: 46.8572 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 261/1000
2023-10-27 09:32:00.794 
Epoch 261/1000 
	 loss: 45.6087, MinusLogProbMetric: 45.6087, val_loss: 47.3028, val_MinusLogProbMetric: 47.3028

Epoch 261: val_loss did not improve from 46.46614
196/196 - 43s - loss: 45.6087 - MinusLogProbMetric: 45.6087 - val_loss: 47.3028 - val_MinusLogProbMetric: 47.3028 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 262/1000
2023-10-27 09:32:42.256 
Epoch 262/1000 
	 loss: 45.7962, MinusLogProbMetric: 45.7962, val_loss: 46.5469, val_MinusLogProbMetric: 46.5469

Epoch 262: val_loss did not improve from 46.46614
196/196 - 41s - loss: 45.7962 - MinusLogProbMetric: 45.7962 - val_loss: 46.5469 - val_MinusLogProbMetric: 46.5469 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 263/1000
2023-10-27 09:33:23.384 
Epoch 263/1000 
	 loss: 45.4058, MinusLogProbMetric: 45.4058, val_loss: 46.9620, val_MinusLogProbMetric: 46.9620

Epoch 263: val_loss did not improve from 46.46614
196/196 - 41s - loss: 45.4058 - MinusLogProbMetric: 45.4058 - val_loss: 46.9620 - val_MinusLogProbMetric: 46.9620 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 264/1000
2023-10-27 09:34:04.694 
Epoch 264/1000 
	 loss: 46.2757, MinusLogProbMetric: 46.2757, val_loss: 46.8131, val_MinusLogProbMetric: 46.8131

Epoch 264: val_loss did not improve from 46.46614
196/196 - 41s - loss: 46.2757 - MinusLogProbMetric: 46.2757 - val_loss: 46.8131 - val_MinusLogProbMetric: 46.8131 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 265/1000
2023-10-27 09:34:47.044 
Epoch 265/1000 
	 loss: 46.3813, MinusLogProbMetric: 46.3813, val_loss: 46.8833, val_MinusLogProbMetric: 46.8833

Epoch 265: val_loss did not improve from 46.46614
196/196 - 42s - loss: 46.3813 - MinusLogProbMetric: 46.3813 - val_loss: 46.8833 - val_MinusLogProbMetric: 46.8833 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 266/1000
2023-10-27 09:35:29.111 
Epoch 266/1000 
	 loss: 45.6019, MinusLogProbMetric: 45.6019, val_loss: 46.8215, val_MinusLogProbMetric: 46.8215

Epoch 266: val_loss did not improve from 46.46614
196/196 - 42s - loss: 45.6019 - MinusLogProbMetric: 45.6019 - val_loss: 46.8215 - val_MinusLogProbMetric: 46.8215 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 267/1000
2023-10-27 09:36:11.741 
Epoch 267/1000 
	 loss: 45.6098, MinusLogProbMetric: 45.6098, val_loss: 46.7352, val_MinusLogProbMetric: 46.7352

Epoch 267: val_loss did not improve from 46.46614
196/196 - 43s - loss: 45.6098 - MinusLogProbMetric: 45.6098 - val_loss: 46.7352 - val_MinusLogProbMetric: 46.7352 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 268/1000
2023-10-27 09:36:54.190 
Epoch 268/1000 
	 loss: 45.4356, MinusLogProbMetric: 45.4356, val_loss: 47.7293, val_MinusLogProbMetric: 47.7293

Epoch 268: val_loss did not improve from 46.46614
196/196 - 42s - loss: 45.4356 - MinusLogProbMetric: 45.4356 - val_loss: 47.7293 - val_MinusLogProbMetric: 47.7293 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 269/1000
2023-10-27 09:37:36.040 
Epoch 269/1000 
	 loss: 46.2098, MinusLogProbMetric: 46.2098, val_loss: 46.4707, val_MinusLogProbMetric: 46.4707

Epoch 269: val_loss did not improve from 46.46614
196/196 - 42s - loss: 46.2098 - MinusLogProbMetric: 46.2098 - val_loss: 46.4707 - val_MinusLogProbMetric: 46.4707 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 270/1000
2023-10-27 09:38:16.229 
Epoch 270/1000 
	 loss: 45.5554, MinusLogProbMetric: 45.5554, val_loss: 46.7922, val_MinusLogProbMetric: 46.7922

Epoch 270: val_loss did not improve from 46.46614
196/196 - 40s - loss: 45.5554 - MinusLogProbMetric: 45.5554 - val_loss: 46.7922 - val_MinusLogProbMetric: 46.7922 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 271/1000
2023-10-27 09:38:57.970 
Epoch 271/1000 
	 loss: 45.4981, MinusLogProbMetric: 45.4981, val_loss: 46.7507, val_MinusLogProbMetric: 46.7507

Epoch 271: val_loss did not improve from 46.46614
196/196 - 42s - loss: 45.4981 - MinusLogProbMetric: 45.4981 - val_loss: 46.7507 - val_MinusLogProbMetric: 46.7507 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 272/1000
2023-10-27 09:39:39.306 
Epoch 272/1000 
	 loss: 45.4429, MinusLogProbMetric: 45.4429, val_loss: 46.6029, val_MinusLogProbMetric: 46.6029

Epoch 272: val_loss did not improve from 46.46614
196/196 - 41s - loss: 45.4429 - MinusLogProbMetric: 45.4429 - val_loss: 46.6029 - val_MinusLogProbMetric: 46.6029 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 273/1000
2023-10-27 09:40:21.218 
Epoch 273/1000 
	 loss: 46.0172, MinusLogProbMetric: 46.0172, val_loss: 46.3448, val_MinusLogProbMetric: 46.3448

Epoch 273: val_loss improved from 46.46614 to 46.34478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 46.0172 - MinusLogProbMetric: 46.0172 - val_loss: 46.3448 - val_MinusLogProbMetric: 46.3448 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 274/1000
2023-10-27 09:41:04.136 
Epoch 274/1000 
	 loss: 45.5708, MinusLogProbMetric: 45.5708, val_loss: 46.5017, val_MinusLogProbMetric: 46.5017

Epoch 274: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.5708 - MinusLogProbMetric: 45.5708 - val_loss: 46.5017 - val_MinusLogProbMetric: 46.5017 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 275/1000
2023-10-27 09:41:46.894 
Epoch 275/1000 
	 loss: 45.4651, MinusLogProbMetric: 45.4651, val_loss: 47.1936, val_MinusLogProbMetric: 47.1936

Epoch 275: val_loss did not improve from 46.34478
196/196 - 43s - loss: 45.4651 - MinusLogProbMetric: 45.4651 - val_loss: 47.1936 - val_MinusLogProbMetric: 47.1936 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 276/1000
2023-10-27 09:42:29.489 
Epoch 276/1000 
	 loss: 45.8276, MinusLogProbMetric: 45.8276, val_loss: 47.2432, val_MinusLogProbMetric: 47.2432

Epoch 276: val_loss did not improve from 46.34478
196/196 - 43s - loss: 45.8276 - MinusLogProbMetric: 45.8276 - val_loss: 47.2432 - val_MinusLogProbMetric: 47.2432 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 277/1000
2023-10-27 09:43:11.098 
Epoch 277/1000 
	 loss: 45.6275, MinusLogProbMetric: 45.6275, val_loss: 47.0718, val_MinusLogProbMetric: 47.0718

Epoch 277: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.6275 - MinusLogProbMetric: 45.6275 - val_loss: 47.0718 - val_MinusLogProbMetric: 47.0718 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 278/1000
2023-10-27 09:43:53.395 
Epoch 278/1000 
	 loss: 45.7901, MinusLogProbMetric: 45.7901, val_loss: 47.0567, val_MinusLogProbMetric: 47.0567

Epoch 278: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.7901 - MinusLogProbMetric: 45.7901 - val_loss: 47.0567 - val_MinusLogProbMetric: 47.0567 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 279/1000
2023-10-27 09:44:36.053 
Epoch 279/1000 
	 loss: 45.6537, MinusLogProbMetric: 45.6537, val_loss: 46.6176, val_MinusLogProbMetric: 46.6176

Epoch 279: val_loss did not improve from 46.34478
196/196 - 43s - loss: 45.6537 - MinusLogProbMetric: 45.6537 - val_loss: 46.6176 - val_MinusLogProbMetric: 46.6176 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 280/1000
2023-10-27 09:45:17.857 
Epoch 280/1000 
	 loss: 45.4569, MinusLogProbMetric: 45.4569, val_loss: 46.4274, val_MinusLogProbMetric: 46.4274

Epoch 280: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.4569 - MinusLogProbMetric: 45.4569 - val_loss: 46.4274 - val_MinusLogProbMetric: 46.4274 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 281/1000
2023-10-27 09:45:59.795 
Epoch 281/1000 
	 loss: 45.4904, MinusLogProbMetric: 45.4904, val_loss: 46.4433, val_MinusLogProbMetric: 46.4433

Epoch 281: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.4904 - MinusLogProbMetric: 45.4904 - val_loss: 46.4433 - val_MinusLogProbMetric: 46.4433 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 282/1000
2023-10-27 09:46:41.674 
Epoch 282/1000 
	 loss: 45.5098, MinusLogProbMetric: 45.5098, val_loss: 46.8694, val_MinusLogProbMetric: 46.8694

Epoch 282: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.5098 - MinusLogProbMetric: 45.5098 - val_loss: 46.8694 - val_MinusLogProbMetric: 46.8694 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 283/1000
2023-10-27 09:47:21.720 
Epoch 283/1000 
	 loss: 45.7735, MinusLogProbMetric: 45.7735, val_loss: 47.1117, val_MinusLogProbMetric: 47.1117

Epoch 283: val_loss did not improve from 46.34478
196/196 - 40s - loss: 45.7735 - MinusLogProbMetric: 45.7735 - val_loss: 47.1117 - val_MinusLogProbMetric: 47.1117 - lr: 1.2346e-05 - 40s/epoch - 204ms/step
Epoch 284/1000
2023-10-27 09:48:03.934 
Epoch 284/1000 
	 loss: 45.2323, MinusLogProbMetric: 45.2323, val_loss: 46.6433, val_MinusLogProbMetric: 46.6433

Epoch 284: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.2323 - MinusLogProbMetric: 45.2323 - val_loss: 46.6433 - val_MinusLogProbMetric: 46.6433 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 285/1000
2023-10-27 09:48:45.492 
Epoch 285/1000 
	 loss: 45.9086, MinusLogProbMetric: 45.9086, val_loss: 46.8599, val_MinusLogProbMetric: 46.8599

Epoch 285: val_loss did not improve from 46.34478
196/196 - 42s - loss: 45.9086 - MinusLogProbMetric: 45.9086 - val_loss: 46.8599 - val_MinusLogProbMetric: 46.8599 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 286/1000
2023-10-27 09:49:27.178 
Epoch 286/1000 
	 loss: 45.4503, MinusLogProbMetric: 45.4503, val_loss: 46.2750, val_MinusLogProbMetric: 46.2750

Epoch 286: val_loss improved from 46.34478 to 46.27496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 45.4503 - MinusLogProbMetric: 45.4503 - val_loss: 46.2750 - val_MinusLogProbMetric: 46.2750 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 287/1000
2023-10-27 09:50:10.602 
Epoch 287/1000 
	 loss: 45.3961, MinusLogProbMetric: 45.3961, val_loss: 48.9065, val_MinusLogProbMetric: 48.9065

Epoch 287: val_loss did not improve from 46.27496
196/196 - 43s - loss: 45.3961 - MinusLogProbMetric: 45.3961 - val_loss: 48.9065 - val_MinusLogProbMetric: 48.9065 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 288/1000
2023-10-27 09:50:52.544 
Epoch 288/1000 
	 loss: 45.8820, MinusLogProbMetric: 45.8820, val_loss: 46.4568, val_MinusLogProbMetric: 46.4568

Epoch 288: val_loss did not improve from 46.27496
196/196 - 42s - loss: 45.8820 - MinusLogProbMetric: 45.8820 - val_loss: 46.4568 - val_MinusLogProbMetric: 46.4568 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 289/1000
2023-10-27 09:51:34.107 
Epoch 289/1000 
	 loss: 47.9730, MinusLogProbMetric: 47.9730, val_loss: 46.4402, val_MinusLogProbMetric: 46.4402

Epoch 289: val_loss did not improve from 46.27496
196/196 - 42s - loss: 47.9730 - MinusLogProbMetric: 47.9730 - val_loss: 46.4402 - val_MinusLogProbMetric: 46.4402 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 290/1000
2023-10-27 09:52:16.417 
Epoch 290/1000 
	 loss: 45.1645, MinusLogProbMetric: 45.1645, val_loss: 46.6312, val_MinusLogProbMetric: 46.6312

Epoch 290: val_loss did not improve from 46.27496
196/196 - 42s - loss: 45.1645 - MinusLogProbMetric: 45.1645 - val_loss: 46.6312 - val_MinusLogProbMetric: 46.6312 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 291/1000
2023-10-27 09:52:58.315 
Epoch 291/1000 
	 loss: 45.1686, MinusLogProbMetric: 45.1686, val_loss: 46.5109, val_MinusLogProbMetric: 46.5109

Epoch 291: val_loss did not improve from 46.27496
196/196 - 42s - loss: 45.1686 - MinusLogProbMetric: 45.1686 - val_loss: 46.5109 - val_MinusLogProbMetric: 46.5109 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 292/1000
2023-10-27 09:53:39.683 
Epoch 292/1000 
	 loss: 45.5306, MinusLogProbMetric: 45.5306, val_loss: 46.3139, val_MinusLogProbMetric: 46.3139

Epoch 292: val_loss did not improve from 46.27496
196/196 - 41s - loss: 45.5306 - MinusLogProbMetric: 45.5306 - val_loss: 46.3139 - val_MinusLogProbMetric: 46.3139 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 293/1000
2023-10-27 09:54:22.332 
Epoch 293/1000 
	 loss: 45.1464, MinusLogProbMetric: 45.1464, val_loss: 46.1768, val_MinusLogProbMetric: 46.1768

Epoch 293: val_loss improved from 46.27496 to 46.17680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 45.1464 - MinusLogProbMetric: 45.1464 - val_loss: 46.1768 - val_MinusLogProbMetric: 46.1768 - lr: 1.2346e-05 - 43s/epoch - 222ms/step
Epoch 294/1000
2023-10-27 09:55:05.212 
Epoch 294/1000 
	 loss: 46.1140, MinusLogProbMetric: 46.1140, val_loss: 46.3028, val_MinusLogProbMetric: 46.3028

Epoch 294: val_loss did not improve from 46.17680
196/196 - 42s - loss: 46.1140 - MinusLogProbMetric: 46.1140 - val_loss: 46.3028 - val_MinusLogProbMetric: 46.3028 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 295/1000
2023-10-27 09:55:46.742 
Epoch 295/1000 
	 loss: 45.1401, MinusLogProbMetric: 45.1401, val_loss: 46.8468, val_MinusLogProbMetric: 46.8468

Epoch 295: val_loss did not improve from 46.17680
196/196 - 42s - loss: 45.1401 - MinusLogProbMetric: 45.1401 - val_loss: 46.8468 - val_MinusLogProbMetric: 46.8468 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 296/1000
2023-10-27 09:56:29.059 
Epoch 296/1000 
	 loss: 45.2663, MinusLogProbMetric: 45.2663, val_loss: 46.3797, val_MinusLogProbMetric: 46.3797

Epoch 296: val_loss did not improve from 46.17680
196/196 - 42s - loss: 45.2663 - MinusLogProbMetric: 45.2663 - val_loss: 46.3797 - val_MinusLogProbMetric: 46.3797 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 297/1000
2023-10-27 09:57:11.684 
Epoch 297/1000 
	 loss: 45.3120, MinusLogProbMetric: 45.3120, val_loss: 47.5597, val_MinusLogProbMetric: 47.5597

Epoch 297: val_loss did not improve from 46.17680
196/196 - 43s - loss: 45.3120 - MinusLogProbMetric: 45.3120 - val_loss: 47.5597 - val_MinusLogProbMetric: 47.5597 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 298/1000
2023-10-27 09:57:53.999 
Epoch 298/1000 
	 loss: 45.2945, MinusLogProbMetric: 45.2945, val_loss: 46.5310, val_MinusLogProbMetric: 46.5310

Epoch 298: val_loss did not improve from 46.17680
196/196 - 42s - loss: 45.2945 - MinusLogProbMetric: 45.2945 - val_loss: 46.5310 - val_MinusLogProbMetric: 46.5310 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 299/1000
2023-10-27 09:58:35.355 
Epoch 299/1000 
	 loss: 45.2840, MinusLogProbMetric: 45.2840, val_loss: 46.8393, val_MinusLogProbMetric: 46.8393

Epoch 299: val_loss did not improve from 46.17680
196/196 - 41s - loss: 45.2840 - MinusLogProbMetric: 45.2840 - val_loss: 46.8393 - val_MinusLogProbMetric: 46.8393 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 300/1000
2023-10-27 09:59:18.105 
Epoch 300/1000 
	 loss: 45.3275, MinusLogProbMetric: 45.3275, val_loss: 46.3347, val_MinusLogProbMetric: 46.3347

Epoch 300: val_loss did not improve from 46.17680
196/196 - 43s - loss: 45.3275 - MinusLogProbMetric: 45.3275 - val_loss: 46.3347 - val_MinusLogProbMetric: 46.3347 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 301/1000
2023-10-27 09:59:59.142 
Epoch 301/1000 
	 loss: 45.3619, MinusLogProbMetric: 45.3619, val_loss: 46.5142, val_MinusLogProbMetric: 46.5142

Epoch 301: val_loss did not improve from 46.17680
196/196 - 41s - loss: 45.3619 - MinusLogProbMetric: 45.3619 - val_loss: 46.5142 - val_MinusLogProbMetric: 46.5142 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 302/1000
2023-10-27 10:00:42.099 
Epoch 302/1000 
	 loss: 45.0715, MinusLogProbMetric: 45.0715, val_loss: 46.6007, val_MinusLogProbMetric: 46.6007

Epoch 302: val_loss did not improve from 46.17680
196/196 - 43s - loss: 45.0715 - MinusLogProbMetric: 45.0715 - val_loss: 46.6007 - val_MinusLogProbMetric: 46.6007 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 303/1000
2023-10-27 10:01:24.362 
Epoch 303/1000 
	 loss: 45.2698, MinusLogProbMetric: 45.2698, val_loss: 46.6156, val_MinusLogProbMetric: 46.6156

Epoch 303: val_loss did not improve from 46.17680
196/196 - 42s - loss: 45.2698 - MinusLogProbMetric: 45.2698 - val_loss: 46.6156 - val_MinusLogProbMetric: 46.6156 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 304/1000
2023-10-27 10:02:07.054 
Epoch 304/1000 
	 loss: 45.2382, MinusLogProbMetric: 45.2382, val_loss: 46.3490, val_MinusLogProbMetric: 46.3490

Epoch 304: val_loss did not improve from 46.17680
196/196 - 43s - loss: 45.2382 - MinusLogProbMetric: 45.2382 - val_loss: 46.3490 - val_MinusLogProbMetric: 46.3490 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 305/1000
2023-10-27 10:02:49.378 
Epoch 305/1000 
	 loss: 45.1327, MinusLogProbMetric: 45.1327, val_loss: 45.9103, val_MinusLogProbMetric: 45.9103

Epoch 305: val_loss improved from 46.17680 to 45.91034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 45.1327 - MinusLogProbMetric: 45.1327 - val_loss: 45.9103 - val_MinusLogProbMetric: 45.9103 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 306/1000
2023-10-27 10:03:31.934 
Epoch 306/1000 
	 loss: 45.0593, MinusLogProbMetric: 45.0593, val_loss: 46.4899, val_MinusLogProbMetric: 46.4899

Epoch 306: val_loss did not improve from 45.91034
196/196 - 42s - loss: 45.0593 - MinusLogProbMetric: 45.0593 - val_loss: 46.4899 - val_MinusLogProbMetric: 46.4899 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 307/1000
2023-10-27 10:04:14.200 
Epoch 307/1000 
	 loss: 45.7601, MinusLogProbMetric: 45.7601, val_loss: 46.8876, val_MinusLogProbMetric: 46.8876

Epoch 307: val_loss did not improve from 45.91034
196/196 - 42s - loss: 45.7601 - MinusLogProbMetric: 45.7601 - val_loss: 46.8876 - val_MinusLogProbMetric: 46.8876 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 308/1000
2023-10-27 10:04:55.955 
Epoch 308/1000 
	 loss: 45.6345, MinusLogProbMetric: 45.6345, val_loss: 47.6960, val_MinusLogProbMetric: 47.6960

Epoch 308: val_loss did not improve from 45.91034
196/196 - 42s - loss: 45.6345 - MinusLogProbMetric: 45.6345 - val_loss: 47.6960 - val_MinusLogProbMetric: 47.6960 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 309/1000
2023-10-27 10:05:38.463 
Epoch 309/1000 
	 loss: 45.2386, MinusLogProbMetric: 45.2386, val_loss: 47.3170, val_MinusLogProbMetric: 47.3170

Epoch 309: val_loss did not improve from 45.91034
196/196 - 43s - loss: 45.2386 - MinusLogProbMetric: 45.2386 - val_loss: 47.3170 - val_MinusLogProbMetric: 47.3170 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 310/1000
2023-10-27 10:06:21.287 
Epoch 310/1000 
	 loss: 45.0326, MinusLogProbMetric: 45.0326, val_loss: 46.8034, val_MinusLogProbMetric: 46.8034

Epoch 310: val_loss did not improve from 45.91034
196/196 - 43s - loss: 45.0326 - MinusLogProbMetric: 45.0326 - val_loss: 46.8034 - val_MinusLogProbMetric: 46.8034 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 311/1000
2023-10-27 10:07:03.779 
Epoch 311/1000 
	 loss: 45.9341, MinusLogProbMetric: 45.9341, val_loss: 46.4033, val_MinusLogProbMetric: 46.4033

Epoch 311: val_loss did not improve from 45.91034
196/196 - 42s - loss: 45.9341 - MinusLogProbMetric: 45.9341 - val_loss: 46.4033 - val_MinusLogProbMetric: 46.4033 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 312/1000
2023-10-27 10:07:46.349 
Epoch 312/1000 
	 loss: 45.1269, MinusLogProbMetric: 45.1269, val_loss: 46.0067, val_MinusLogProbMetric: 46.0067

Epoch 312: val_loss did not improve from 45.91034
196/196 - 43s - loss: 45.1269 - MinusLogProbMetric: 45.1269 - val_loss: 46.0067 - val_MinusLogProbMetric: 46.0067 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 313/1000
2023-10-27 10:08:28.991 
Epoch 313/1000 
	 loss: 45.2745, MinusLogProbMetric: 45.2745, val_loss: 47.5219, val_MinusLogProbMetric: 47.5219

Epoch 313: val_loss did not improve from 45.91034
196/196 - 43s - loss: 45.2745 - MinusLogProbMetric: 45.2745 - val_loss: 47.5219 - val_MinusLogProbMetric: 47.5219 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 314/1000
2023-10-27 10:09:11.151 
Epoch 314/1000 
	 loss: 45.2163, MinusLogProbMetric: 45.2163, val_loss: 46.4347, val_MinusLogProbMetric: 46.4347

Epoch 314: val_loss did not improve from 45.91034
196/196 - 42s - loss: 45.2163 - MinusLogProbMetric: 45.2163 - val_loss: 46.4347 - val_MinusLogProbMetric: 46.4347 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 315/1000
2023-10-27 10:09:53.818 
Epoch 315/1000 
	 loss: 45.2464, MinusLogProbMetric: 45.2464, val_loss: 46.0400, val_MinusLogProbMetric: 46.0400

Epoch 315: val_loss did not improve from 45.91034
196/196 - 43s - loss: 45.2464 - MinusLogProbMetric: 45.2464 - val_loss: 46.0400 - val_MinusLogProbMetric: 46.0400 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 316/1000
2023-10-27 10:10:37.113 
Epoch 316/1000 
	 loss: 45.4037, MinusLogProbMetric: 45.4037, val_loss: 46.1793, val_MinusLogProbMetric: 46.1793

Epoch 316: val_loss did not improve from 45.91034
196/196 - 43s - loss: 45.4037 - MinusLogProbMetric: 45.4037 - val_loss: 46.1793 - val_MinusLogProbMetric: 46.1793 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 317/1000
2023-10-27 10:11:19.880 
Epoch 317/1000 
	 loss: 44.8559, MinusLogProbMetric: 44.8559, val_loss: 45.7803, val_MinusLogProbMetric: 45.7803

Epoch 317: val_loss improved from 45.91034 to 45.78033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 44.8559 - MinusLogProbMetric: 44.8559 - val_loss: 45.7803 - val_MinusLogProbMetric: 45.7803 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 318/1000
2023-10-27 10:12:03.200 
Epoch 318/1000 
	 loss: 45.4337, MinusLogProbMetric: 45.4337, val_loss: 46.2852, val_MinusLogProbMetric: 46.2852

Epoch 318: val_loss did not improve from 45.78033
196/196 - 42s - loss: 45.4337 - MinusLogProbMetric: 45.4337 - val_loss: 46.2852 - val_MinusLogProbMetric: 46.2852 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 319/1000
2023-10-27 10:12:45.294 
Epoch 319/1000 
	 loss: 45.0531, MinusLogProbMetric: 45.0531, val_loss: 46.0237, val_MinusLogProbMetric: 46.0237

Epoch 319: val_loss did not improve from 45.78033
196/196 - 42s - loss: 45.0531 - MinusLogProbMetric: 45.0531 - val_loss: 46.0237 - val_MinusLogProbMetric: 46.0237 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 320/1000
2023-10-27 10:13:27.563 
Epoch 320/1000 
	 loss: 45.1151, MinusLogProbMetric: 45.1151, val_loss: 48.3136, val_MinusLogProbMetric: 48.3136

Epoch 320: val_loss did not improve from 45.78033
196/196 - 42s - loss: 45.1151 - MinusLogProbMetric: 45.1151 - val_loss: 48.3136 - val_MinusLogProbMetric: 48.3136 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 321/1000
2023-10-27 10:14:10.678 
Epoch 321/1000 
	 loss: 45.4911, MinusLogProbMetric: 45.4911, val_loss: 52.1871, val_MinusLogProbMetric: 52.1871

Epoch 321: val_loss did not improve from 45.78033
196/196 - 43s - loss: 45.4911 - MinusLogProbMetric: 45.4911 - val_loss: 52.1871 - val_MinusLogProbMetric: 52.1871 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 322/1000
2023-10-27 10:14:54.581 
Epoch 322/1000 
	 loss: 45.4572, MinusLogProbMetric: 45.4572, val_loss: 46.9558, val_MinusLogProbMetric: 46.9558

Epoch 322: val_loss did not improve from 45.78033
196/196 - 44s - loss: 45.4572 - MinusLogProbMetric: 45.4572 - val_loss: 46.9558 - val_MinusLogProbMetric: 46.9558 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 323/1000
2023-10-27 10:15:38.685 
Epoch 323/1000 
	 loss: 45.2796, MinusLogProbMetric: 45.2796, val_loss: 50.2968, val_MinusLogProbMetric: 50.2968

Epoch 323: val_loss did not improve from 45.78033
196/196 - 44s - loss: 45.2796 - MinusLogProbMetric: 45.2796 - val_loss: 50.2968 - val_MinusLogProbMetric: 50.2968 - lr: 1.2346e-05 - 44s/epoch - 225ms/step
Epoch 324/1000
2023-10-27 10:16:22.249 
Epoch 324/1000 
	 loss: 46.0563, MinusLogProbMetric: 46.0563, val_loss: 46.2161, val_MinusLogProbMetric: 46.2161

Epoch 324: val_loss did not improve from 45.78033
196/196 - 44s - loss: 46.0563 - MinusLogProbMetric: 46.0563 - val_loss: 46.2161 - val_MinusLogProbMetric: 46.2161 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 325/1000
2023-10-27 10:17:06.206 
Epoch 325/1000 
	 loss: 44.9383, MinusLogProbMetric: 44.9383, val_loss: 45.9234, val_MinusLogProbMetric: 45.9234

Epoch 325: val_loss did not improve from 45.78033
196/196 - 44s - loss: 44.9383 - MinusLogProbMetric: 44.9383 - val_loss: 45.9234 - val_MinusLogProbMetric: 45.9234 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 326/1000
2023-10-27 10:17:49.962 
Epoch 326/1000 
	 loss: 45.0116, MinusLogProbMetric: 45.0116, val_loss: 47.1470, val_MinusLogProbMetric: 47.1470

Epoch 326: val_loss did not improve from 45.78033
196/196 - 44s - loss: 45.0116 - MinusLogProbMetric: 45.0116 - val_loss: 47.1470 - val_MinusLogProbMetric: 47.1470 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 327/1000
2023-10-27 10:18:33.396 
Epoch 327/1000 
	 loss: 45.0266, MinusLogProbMetric: 45.0266, val_loss: 46.6806, val_MinusLogProbMetric: 46.6806

Epoch 327: val_loss did not improve from 45.78033
196/196 - 43s - loss: 45.0266 - MinusLogProbMetric: 45.0266 - val_loss: 46.6806 - val_MinusLogProbMetric: 46.6806 - lr: 1.2346e-05 - 43s/epoch - 222ms/step
Epoch 328/1000
2023-10-27 10:19:17.292 
Epoch 328/1000 
	 loss: 46.0466, MinusLogProbMetric: 46.0466, val_loss: 46.1195, val_MinusLogProbMetric: 46.1195

Epoch 328: val_loss did not improve from 45.78033
196/196 - 44s - loss: 46.0466 - MinusLogProbMetric: 46.0466 - val_loss: 46.1195 - val_MinusLogProbMetric: 46.1195 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 329/1000
2023-10-27 10:20:00.991 
Epoch 329/1000 
	 loss: 44.8340, MinusLogProbMetric: 44.8340, val_loss: 46.5500, val_MinusLogProbMetric: 46.5500

Epoch 329: val_loss did not improve from 45.78033
196/196 - 44s - loss: 44.8340 - MinusLogProbMetric: 44.8340 - val_loss: 46.5500 - val_MinusLogProbMetric: 46.5500 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 330/1000
2023-10-27 10:20:44.984 
Epoch 330/1000 
	 loss: 44.9625, MinusLogProbMetric: 44.9625, val_loss: 46.2919, val_MinusLogProbMetric: 46.2919

Epoch 330: val_loss did not improve from 45.78033
196/196 - 44s - loss: 44.9625 - MinusLogProbMetric: 44.9625 - val_loss: 46.2919 - val_MinusLogProbMetric: 46.2919 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 331/1000
2023-10-27 10:21:29.518 
Epoch 331/1000 
	 loss: 45.5054, MinusLogProbMetric: 45.5054, val_loss: 46.1861, val_MinusLogProbMetric: 46.1861

Epoch 331: val_loss did not improve from 45.78033
196/196 - 45s - loss: 45.5054 - MinusLogProbMetric: 45.5054 - val_loss: 46.1861 - val_MinusLogProbMetric: 46.1861 - lr: 1.2346e-05 - 45s/epoch - 227ms/step
Epoch 332/1000
2023-10-27 10:22:13.423 
Epoch 332/1000 
	 loss: 44.8632, MinusLogProbMetric: 44.8632, val_loss: 46.6777, val_MinusLogProbMetric: 46.6777

Epoch 332: val_loss did not improve from 45.78033
196/196 - 44s - loss: 44.8632 - MinusLogProbMetric: 44.8632 - val_loss: 46.6777 - val_MinusLogProbMetric: 46.6777 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 333/1000
2023-10-27 10:22:57.232 
Epoch 333/1000 
	 loss: 44.7589, MinusLogProbMetric: 44.7589, val_loss: 46.3797, val_MinusLogProbMetric: 46.3797

Epoch 333: val_loss did not improve from 45.78033
196/196 - 44s - loss: 44.7589 - MinusLogProbMetric: 44.7589 - val_loss: 46.3797 - val_MinusLogProbMetric: 46.3797 - lr: 1.2346e-05 - 44s/epoch - 223ms/step
Epoch 334/1000
2023-10-27 10:23:41.138 
Epoch 334/1000 
	 loss: 45.2834, MinusLogProbMetric: 45.2834, val_loss: 46.4077, val_MinusLogProbMetric: 46.4077

Epoch 334: val_loss did not improve from 45.78033
196/196 - 44s - loss: 45.2834 - MinusLogProbMetric: 45.2834 - val_loss: 46.4077 - val_MinusLogProbMetric: 46.4077 - lr: 1.2346e-05 - 44s/epoch - 224ms/step
Epoch 335/1000
2023-10-27 10:24:24.439 
Epoch 335/1000 
	 loss: 44.8231, MinusLogProbMetric: 44.8231, val_loss: 45.9021, val_MinusLogProbMetric: 45.9021

Epoch 335: val_loss did not improve from 45.78033
196/196 - 43s - loss: 44.8231 - MinusLogProbMetric: 44.8231 - val_loss: 45.9021 - val_MinusLogProbMetric: 45.9021 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 336/1000
2023-10-27 10:25:07.247 
Epoch 336/1000 
	 loss: 44.7640, MinusLogProbMetric: 44.7640, val_loss: 46.3526, val_MinusLogProbMetric: 46.3526

Epoch 336: val_loss did not improve from 45.78033
196/196 - 43s - loss: 44.7640 - MinusLogProbMetric: 44.7640 - val_loss: 46.3526 - val_MinusLogProbMetric: 46.3526 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 337/1000
2023-10-27 10:25:49.058 
Epoch 337/1000 
	 loss: 45.1887, MinusLogProbMetric: 45.1887, val_loss: 46.0984, val_MinusLogProbMetric: 46.0984

Epoch 337: val_loss did not improve from 45.78033
196/196 - 42s - loss: 45.1887 - MinusLogProbMetric: 45.1887 - val_loss: 46.0984 - val_MinusLogProbMetric: 46.0984 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 338/1000
2023-10-27 10:26:31.777 
Epoch 338/1000 
	 loss: 44.8108, MinusLogProbMetric: 44.8108, val_loss: 46.4723, val_MinusLogProbMetric: 46.4723

Epoch 338: val_loss did not improve from 45.78033
196/196 - 43s - loss: 44.8108 - MinusLogProbMetric: 44.8108 - val_loss: 46.4723 - val_MinusLogProbMetric: 46.4723 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 339/1000
2023-10-27 10:27:14.234 
Epoch 339/1000 
	 loss: 45.2522, MinusLogProbMetric: 45.2522, val_loss: 45.6818, val_MinusLogProbMetric: 45.6818

Epoch 339: val_loss improved from 45.78033 to 45.68184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 45.2522 - MinusLogProbMetric: 45.2522 - val_loss: 45.6818 - val_MinusLogProbMetric: 45.6818 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 340/1000
2023-10-27 10:27:57.242 
Epoch 340/1000 
	 loss: 45.2224, MinusLogProbMetric: 45.2224, val_loss: 48.8697, val_MinusLogProbMetric: 48.8697

Epoch 340: val_loss did not improve from 45.68184
196/196 - 42s - loss: 45.2224 - MinusLogProbMetric: 45.2224 - val_loss: 48.8697 - val_MinusLogProbMetric: 48.8697 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 341/1000
2023-10-27 10:28:39.208 
Epoch 341/1000 
	 loss: 44.8190, MinusLogProbMetric: 44.8190, val_loss: 46.2232, val_MinusLogProbMetric: 46.2232

Epoch 341: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.8190 - MinusLogProbMetric: 44.8190 - val_loss: 46.2232 - val_MinusLogProbMetric: 46.2232 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 342/1000
2023-10-27 10:29:21.955 
Epoch 342/1000 
	 loss: 44.8350, MinusLogProbMetric: 44.8350, val_loss: 46.5392, val_MinusLogProbMetric: 46.5392

Epoch 342: val_loss did not improve from 45.68184
196/196 - 43s - loss: 44.8350 - MinusLogProbMetric: 44.8350 - val_loss: 46.5392 - val_MinusLogProbMetric: 46.5392 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 343/1000
2023-10-27 10:30:04.716 
Epoch 343/1000 
	 loss: 44.9635, MinusLogProbMetric: 44.9635, val_loss: 46.3272, val_MinusLogProbMetric: 46.3272

Epoch 343: val_loss did not improve from 45.68184
196/196 - 43s - loss: 44.9635 - MinusLogProbMetric: 44.9635 - val_loss: 46.3272 - val_MinusLogProbMetric: 46.3272 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 344/1000
2023-10-27 10:30:47.789 
Epoch 344/1000 
	 loss: 44.7682, MinusLogProbMetric: 44.7682, val_loss: 46.9347, val_MinusLogProbMetric: 46.9347

Epoch 344: val_loss did not improve from 45.68184
196/196 - 43s - loss: 44.7682 - MinusLogProbMetric: 44.7682 - val_loss: 46.9347 - val_MinusLogProbMetric: 46.9347 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 345/1000
2023-10-27 10:31:28.428 
Epoch 345/1000 
	 loss: 44.8500, MinusLogProbMetric: 44.8500, val_loss: 45.7814, val_MinusLogProbMetric: 45.7814

Epoch 345: val_loss did not improve from 45.68184
196/196 - 41s - loss: 44.8500 - MinusLogProbMetric: 44.8500 - val_loss: 45.7814 - val_MinusLogProbMetric: 45.7814 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 346/1000
2023-10-27 10:32:10.608 
Epoch 346/1000 
	 loss: 44.7007, MinusLogProbMetric: 44.7007, val_loss: 46.3094, val_MinusLogProbMetric: 46.3094

Epoch 346: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.7007 - MinusLogProbMetric: 44.7007 - val_loss: 46.3094 - val_MinusLogProbMetric: 46.3094 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 347/1000
2023-10-27 10:32:50.473 
Epoch 347/1000 
	 loss: 45.5516, MinusLogProbMetric: 45.5516, val_loss: 46.4172, val_MinusLogProbMetric: 46.4172

Epoch 347: val_loss did not improve from 45.68184
196/196 - 40s - loss: 45.5516 - MinusLogProbMetric: 45.5516 - val_loss: 46.4172 - val_MinusLogProbMetric: 46.4172 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 348/1000
2023-10-27 10:33:30.690 
Epoch 348/1000 
	 loss: 45.2204, MinusLogProbMetric: 45.2204, val_loss: 47.7072, val_MinusLogProbMetric: 47.7072

Epoch 348: val_loss did not improve from 45.68184
196/196 - 40s - loss: 45.2204 - MinusLogProbMetric: 45.2204 - val_loss: 47.7072 - val_MinusLogProbMetric: 47.7072 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 349/1000
2023-10-27 10:34:12.820 
Epoch 349/1000 
	 loss: 45.0923, MinusLogProbMetric: 45.0923, val_loss: 46.5822, val_MinusLogProbMetric: 46.5822

Epoch 349: val_loss did not improve from 45.68184
196/196 - 42s - loss: 45.0923 - MinusLogProbMetric: 45.0923 - val_loss: 46.5822 - val_MinusLogProbMetric: 46.5822 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 350/1000
2023-10-27 10:34:53.389 
Epoch 350/1000 
	 loss: 44.8115, MinusLogProbMetric: 44.8115, val_loss: 46.1780, val_MinusLogProbMetric: 46.1780

Epoch 350: val_loss did not improve from 45.68184
196/196 - 41s - loss: 44.8115 - MinusLogProbMetric: 44.8115 - val_loss: 46.1780 - val_MinusLogProbMetric: 46.1780 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 351/1000
2023-10-27 10:35:34.314 
Epoch 351/1000 
	 loss: 44.7528, MinusLogProbMetric: 44.7528, val_loss: 46.3612, val_MinusLogProbMetric: 46.3612

Epoch 351: val_loss did not improve from 45.68184
196/196 - 41s - loss: 44.7528 - MinusLogProbMetric: 44.7528 - val_loss: 46.3612 - val_MinusLogProbMetric: 46.3612 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 352/1000
2023-10-27 10:36:15.347 
Epoch 352/1000 
	 loss: 45.5302, MinusLogProbMetric: 45.5302, val_loss: 45.8745, val_MinusLogProbMetric: 45.8745

Epoch 352: val_loss did not improve from 45.68184
196/196 - 41s - loss: 45.5302 - MinusLogProbMetric: 45.5302 - val_loss: 45.8745 - val_MinusLogProbMetric: 45.8745 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 353/1000
2023-10-27 10:36:57.181 
Epoch 353/1000 
	 loss: 44.9327, MinusLogProbMetric: 44.9327, val_loss: 48.7013, val_MinusLogProbMetric: 48.7013

Epoch 353: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.9327 - MinusLogProbMetric: 44.9327 - val_loss: 48.7013 - val_MinusLogProbMetric: 48.7013 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 354/1000
2023-10-27 10:37:37.909 
Epoch 354/1000 
	 loss: 45.7934, MinusLogProbMetric: 45.7934, val_loss: 48.3254, val_MinusLogProbMetric: 48.3254

Epoch 354: val_loss did not improve from 45.68184
196/196 - 41s - loss: 45.7934 - MinusLogProbMetric: 45.7934 - val_loss: 48.3254 - val_MinusLogProbMetric: 48.3254 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 355/1000
2023-10-27 10:38:18.613 
Epoch 355/1000 
	 loss: 45.6323, MinusLogProbMetric: 45.6323, val_loss: 46.3238, val_MinusLogProbMetric: 46.3238

Epoch 355: val_loss did not improve from 45.68184
196/196 - 41s - loss: 45.6323 - MinusLogProbMetric: 45.6323 - val_loss: 46.3238 - val_MinusLogProbMetric: 46.3238 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 356/1000
2023-10-27 10:39:00.481 
Epoch 356/1000 
	 loss: 45.5221, MinusLogProbMetric: 45.5221, val_loss: 46.2727, val_MinusLogProbMetric: 46.2727

Epoch 356: val_loss did not improve from 45.68184
196/196 - 42s - loss: 45.5221 - MinusLogProbMetric: 45.5221 - val_loss: 46.2727 - val_MinusLogProbMetric: 46.2727 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 357/1000
2023-10-27 10:39:42.764 
Epoch 357/1000 
	 loss: 44.7180, MinusLogProbMetric: 44.7180, val_loss: 45.7849, val_MinusLogProbMetric: 45.7849

Epoch 357: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.7180 - MinusLogProbMetric: 44.7180 - val_loss: 45.7849 - val_MinusLogProbMetric: 45.7849 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 358/1000
2023-10-27 10:40:24.301 
Epoch 358/1000 
	 loss: 44.5739, MinusLogProbMetric: 44.5739, val_loss: 47.0848, val_MinusLogProbMetric: 47.0848

Epoch 358: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.5739 - MinusLogProbMetric: 44.5739 - val_loss: 47.0848 - val_MinusLogProbMetric: 47.0848 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 359/1000
2023-10-27 10:41:06.000 
Epoch 359/1000 
	 loss: 44.8079, MinusLogProbMetric: 44.8079, val_loss: 46.6805, val_MinusLogProbMetric: 46.6805

Epoch 359: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.8079 - MinusLogProbMetric: 44.8079 - val_loss: 46.6805 - val_MinusLogProbMetric: 46.6805 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 360/1000
2023-10-27 10:41:48.838 
Epoch 360/1000 
	 loss: 44.7260, MinusLogProbMetric: 44.7260, val_loss: 46.0482, val_MinusLogProbMetric: 46.0482

Epoch 360: val_loss did not improve from 45.68184
196/196 - 43s - loss: 44.7260 - MinusLogProbMetric: 44.7260 - val_loss: 46.0482 - val_MinusLogProbMetric: 46.0482 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 361/1000
2023-10-27 10:42:30.990 
Epoch 361/1000 
	 loss: 44.6890, MinusLogProbMetric: 44.6890, val_loss: 46.0700, val_MinusLogProbMetric: 46.0700

Epoch 361: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.6890 - MinusLogProbMetric: 44.6890 - val_loss: 46.0700 - val_MinusLogProbMetric: 46.0700 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 362/1000
2023-10-27 10:43:13.303 
Epoch 362/1000 
	 loss: 44.8184, MinusLogProbMetric: 44.8184, val_loss: 45.7107, val_MinusLogProbMetric: 45.7107

Epoch 362: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.8184 - MinusLogProbMetric: 44.8184 - val_loss: 45.7107 - val_MinusLogProbMetric: 45.7107 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 363/1000
2023-10-27 10:43:54.854 
Epoch 363/1000 
	 loss: 44.7696, MinusLogProbMetric: 44.7696, val_loss: 46.4915, val_MinusLogProbMetric: 46.4915

Epoch 363: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.7696 - MinusLogProbMetric: 44.7696 - val_loss: 46.4915 - val_MinusLogProbMetric: 46.4915 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 364/1000
2023-10-27 10:44:35.879 
Epoch 364/1000 
	 loss: 44.5043, MinusLogProbMetric: 44.5043, val_loss: 46.4207, val_MinusLogProbMetric: 46.4207

Epoch 364: val_loss did not improve from 45.68184
196/196 - 41s - loss: 44.5043 - MinusLogProbMetric: 44.5043 - val_loss: 46.4207 - val_MinusLogProbMetric: 46.4207 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 365/1000
2023-10-27 10:45:17.577 
Epoch 365/1000 
	 loss: 44.7344, MinusLogProbMetric: 44.7344, val_loss: 46.6189, val_MinusLogProbMetric: 46.6189

Epoch 365: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.7344 - MinusLogProbMetric: 44.7344 - val_loss: 46.6189 - val_MinusLogProbMetric: 46.6189 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 366/1000
2023-10-27 10:45:59.259 
Epoch 366/1000 
	 loss: 45.5621, MinusLogProbMetric: 45.5621, val_loss: 45.8832, val_MinusLogProbMetric: 45.8832

Epoch 366: val_loss did not improve from 45.68184
196/196 - 42s - loss: 45.5621 - MinusLogProbMetric: 45.5621 - val_loss: 45.8832 - val_MinusLogProbMetric: 45.8832 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 367/1000
2023-10-27 10:46:41.741 
Epoch 367/1000 
	 loss: 44.6631, MinusLogProbMetric: 44.6631, val_loss: 45.9569, val_MinusLogProbMetric: 45.9569

Epoch 367: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.6631 - MinusLogProbMetric: 44.6631 - val_loss: 45.9569 - val_MinusLogProbMetric: 45.9569 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 368/1000
2023-10-27 10:47:23.300 
Epoch 368/1000 
	 loss: 44.4582, MinusLogProbMetric: 44.4582, val_loss: 45.9502, val_MinusLogProbMetric: 45.9502

Epoch 368: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.4582 - MinusLogProbMetric: 44.4582 - val_loss: 45.9502 - val_MinusLogProbMetric: 45.9502 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 369/1000
2023-10-27 10:48:05.417 
Epoch 369/1000 
	 loss: 45.0035, MinusLogProbMetric: 45.0035, val_loss: 46.5951, val_MinusLogProbMetric: 46.5951

Epoch 369: val_loss did not improve from 45.68184
196/196 - 42s - loss: 45.0035 - MinusLogProbMetric: 45.0035 - val_loss: 46.5951 - val_MinusLogProbMetric: 46.5951 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 370/1000
2023-10-27 10:48:48.512 
Epoch 370/1000 
	 loss: 44.7237, MinusLogProbMetric: 44.7237, val_loss: 47.5933, val_MinusLogProbMetric: 47.5933

Epoch 370: val_loss did not improve from 45.68184
196/196 - 43s - loss: 44.7237 - MinusLogProbMetric: 44.7237 - val_loss: 47.5933 - val_MinusLogProbMetric: 47.5933 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 371/1000
2023-10-27 10:49:30.547 
Epoch 371/1000 
	 loss: 45.1703, MinusLogProbMetric: 45.1703, val_loss: 45.9453, val_MinusLogProbMetric: 45.9453

Epoch 371: val_loss did not improve from 45.68184
196/196 - 42s - loss: 45.1703 - MinusLogProbMetric: 45.1703 - val_loss: 45.9453 - val_MinusLogProbMetric: 45.9453 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 372/1000
2023-10-27 10:50:13.009 
Epoch 372/1000 
	 loss: 44.9485, MinusLogProbMetric: 44.9485, val_loss: 46.3234, val_MinusLogProbMetric: 46.3234

Epoch 372: val_loss did not improve from 45.68184
196/196 - 42s - loss: 44.9485 - MinusLogProbMetric: 44.9485 - val_loss: 46.3234 - val_MinusLogProbMetric: 46.3234 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 373/1000
2023-10-27 10:50:53.758 
Epoch 373/1000 
	 loss: 44.7649, MinusLogProbMetric: 44.7649, val_loss: 46.5226, val_MinusLogProbMetric: 46.5226

Epoch 373: val_loss did not improve from 45.68184
196/196 - 41s - loss: 44.7649 - MinusLogProbMetric: 44.7649 - val_loss: 46.5226 - val_MinusLogProbMetric: 46.5226 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 374/1000
2023-10-27 10:51:35.389 
Epoch 374/1000 
	 loss: 44.7143, MinusLogProbMetric: 44.7143, val_loss: 45.6589, val_MinusLogProbMetric: 45.6589

Epoch 374: val_loss improved from 45.68184 to 45.65894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 44.7143 - MinusLogProbMetric: 44.7143 - val_loss: 45.6589 - val_MinusLogProbMetric: 45.6589 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 375/1000
2023-10-27 10:52:18.914 
Epoch 375/1000 
	 loss: 44.8881, MinusLogProbMetric: 44.8881, val_loss: 46.0229, val_MinusLogProbMetric: 46.0229

Epoch 375: val_loss did not improve from 45.65894
196/196 - 43s - loss: 44.8881 - MinusLogProbMetric: 44.8881 - val_loss: 46.0229 - val_MinusLogProbMetric: 46.0229 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 376/1000
2023-10-27 10:53:01.584 
Epoch 376/1000 
	 loss: 44.7713, MinusLogProbMetric: 44.7713, val_loss: 45.9320, val_MinusLogProbMetric: 45.9320

Epoch 376: val_loss did not improve from 45.65894
196/196 - 43s - loss: 44.7713 - MinusLogProbMetric: 44.7713 - val_loss: 45.9320 - val_MinusLogProbMetric: 45.9320 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 377/1000
2023-10-27 10:53:42.967 
Epoch 377/1000 
	 loss: 44.5798, MinusLogProbMetric: 44.5798, val_loss: 46.3825, val_MinusLogProbMetric: 46.3825

Epoch 377: val_loss did not improve from 45.65894
196/196 - 41s - loss: 44.5798 - MinusLogProbMetric: 44.5798 - val_loss: 46.3825 - val_MinusLogProbMetric: 46.3825 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 378/1000
2023-10-27 10:54:23.833 
Epoch 378/1000 
	 loss: 45.0535, MinusLogProbMetric: 45.0535, val_loss: 46.0716, val_MinusLogProbMetric: 46.0716

Epoch 378: val_loss did not improve from 45.65894
196/196 - 41s - loss: 45.0535 - MinusLogProbMetric: 45.0535 - val_loss: 46.0716 - val_MinusLogProbMetric: 46.0716 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 379/1000
2023-10-27 10:55:05.035 
Epoch 379/1000 
	 loss: 44.9945, MinusLogProbMetric: 44.9945, val_loss: 45.8070, val_MinusLogProbMetric: 45.8070

Epoch 379: val_loss did not improve from 45.65894
196/196 - 41s - loss: 44.9945 - MinusLogProbMetric: 44.9945 - val_loss: 45.8070 - val_MinusLogProbMetric: 45.8070 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 380/1000
2023-10-27 10:55:46.791 
Epoch 380/1000 
	 loss: 44.9594, MinusLogProbMetric: 44.9594, val_loss: 46.3942, val_MinusLogProbMetric: 46.3942

Epoch 380: val_loss did not improve from 45.65894
196/196 - 42s - loss: 44.9594 - MinusLogProbMetric: 44.9594 - val_loss: 46.3942 - val_MinusLogProbMetric: 46.3942 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 381/1000
2023-10-27 10:56:28.190 
Epoch 381/1000 
	 loss: 44.5271, MinusLogProbMetric: 44.5271, val_loss: 45.6010, val_MinusLogProbMetric: 45.6010

Epoch 381: val_loss improved from 45.65894 to 45.60099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 44.5271 - MinusLogProbMetric: 44.5271 - val_loss: 45.6010 - val_MinusLogProbMetric: 45.6010 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 382/1000
2023-10-27 10:57:09.692 
Epoch 382/1000 
	 loss: 44.6876, MinusLogProbMetric: 44.6876, val_loss: 45.7965, val_MinusLogProbMetric: 45.7965

Epoch 382: val_loss did not improve from 45.60099
196/196 - 41s - loss: 44.6876 - MinusLogProbMetric: 44.6876 - val_loss: 45.7965 - val_MinusLogProbMetric: 45.7965 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 383/1000
2023-10-27 10:57:51.736 
Epoch 383/1000 
	 loss: 44.5661, MinusLogProbMetric: 44.5661, val_loss: 45.8700, val_MinusLogProbMetric: 45.8700

Epoch 383: val_loss did not improve from 45.60099
196/196 - 42s - loss: 44.5661 - MinusLogProbMetric: 44.5661 - val_loss: 45.8700 - val_MinusLogProbMetric: 45.8700 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 384/1000
2023-10-27 10:58:34.342 
Epoch 384/1000 
	 loss: 44.5364, MinusLogProbMetric: 44.5364, val_loss: 46.0790, val_MinusLogProbMetric: 46.0790

Epoch 384: val_loss did not improve from 45.60099
196/196 - 43s - loss: 44.5364 - MinusLogProbMetric: 44.5364 - val_loss: 46.0790 - val_MinusLogProbMetric: 46.0790 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 385/1000
2023-10-27 10:59:13.947 
Epoch 385/1000 
	 loss: 44.7284, MinusLogProbMetric: 44.7284, val_loss: 63.1450, val_MinusLogProbMetric: 63.1450

Epoch 385: val_loss did not improve from 45.60099
196/196 - 40s - loss: 44.7284 - MinusLogProbMetric: 44.7284 - val_loss: 63.1450 - val_MinusLogProbMetric: 63.1450 - lr: 1.2346e-05 - 40s/epoch - 202ms/step
Epoch 386/1000
2023-10-27 10:59:54.664 
Epoch 386/1000 
	 loss: 47.0443, MinusLogProbMetric: 47.0443, val_loss: 46.0965, val_MinusLogProbMetric: 46.0965

Epoch 386: val_loss did not improve from 45.60099
196/196 - 41s - loss: 47.0443 - MinusLogProbMetric: 47.0443 - val_loss: 46.0965 - val_MinusLogProbMetric: 46.0965 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 387/1000
2023-10-27 11:00:37.079 
Epoch 387/1000 
	 loss: 44.6634, MinusLogProbMetric: 44.6634, val_loss: 46.0574, val_MinusLogProbMetric: 46.0574

Epoch 387: val_loss did not improve from 45.60099
196/196 - 42s - loss: 44.6634 - MinusLogProbMetric: 44.6634 - val_loss: 46.0574 - val_MinusLogProbMetric: 46.0574 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 388/1000
2023-10-27 11:01:18.212 
Epoch 388/1000 
	 loss: 44.3971, MinusLogProbMetric: 44.3971, val_loss: 46.2459, val_MinusLogProbMetric: 46.2459

Epoch 388: val_loss did not improve from 45.60099
196/196 - 41s - loss: 44.3971 - MinusLogProbMetric: 44.3971 - val_loss: 46.2459 - val_MinusLogProbMetric: 46.2459 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 389/1000
2023-10-27 11:01:59.601 
Epoch 389/1000 
	 loss: 44.9894, MinusLogProbMetric: 44.9894, val_loss: 46.0644, val_MinusLogProbMetric: 46.0644

Epoch 389: val_loss did not improve from 45.60099
196/196 - 41s - loss: 44.9894 - MinusLogProbMetric: 44.9894 - val_loss: 46.0644 - val_MinusLogProbMetric: 46.0644 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 390/1000
2023-10-27 11:02:41.861 
Epoch 390/1000 
	 loss: 44.6619, MinusLogProbMetric: 44.6619, val_loss: 46.1334, val_MinusLogProbMetric: 46.1334

Epoch 390: val_loss did not improve from 45.60099
196/196 - 42s - loss: 44.6619 - MinusLogProbMetric: 44.6619 - val_loss: 46.1334 - val_MinusLogProbMetric: 46.1334 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 391/1000
2023-10-27 11:03:24.369 
Epoch 391/1000 
	 loss: 44.2670, MinusLogProbMetric: 44.2670, val_loss: 45.5525, val_MinusLogProbMetric: 45.5525

Epoch 391: val_loss improved from 45.60099 to 45.55254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 43s - loss: 44.2670 - MinusLogProbMetric: 44.2670 - val_loss: 45.5525 - val_MinusLogProbMetric: 45.5525 - lr: 1.2346e-05 - 43s/epoch - 221ms/step
Epoch 392/1000
2023-10-27 11:04:07.339 
Epoch 392/1000 
	 loss: 45.0033, MinusLogProbMetric: 45.0033, val_loss: 46.2597, val_MinusLogProbMetric: 46.2597

Epoch 392: val_loss did not improve from 45.55254
196/196 - 42s - loss: 45.0033 - MinusLogProbMetric: 45.0033 - val_loss: 46.2597 - val_MinusLogProbMetric: 46.2597 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 393/1000
2023-10-27 11:04:49.674 
Epoch 393/1000 
	 loss: 44.4449, MinusLogProbMetric: 44.4449, val_loss: 45.8369, val_MinusLogProbMetric: 45.8369

Epoch 393: val_loss did not improve from 45.55254
196/196 - 42s - loss: 44.4449 - MinusLogProbMetric: 44.4449 - val_loss: 45.8369 - val_MinusLogProbMetric: 45.8369 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 394/1000
2023-10-27 11:05:31.870 
Epoch 394/1000 
	 loss: 44.4656, MinusLogProbMetric: 44.4656, val_loss: 46.4063, val_MinusLogProbMetric: 46.4063

Epoch 394: val_loss did not improve from 45.55254
196/196 - 42s - loss: 44.4656 - MinusLogProbMetric: 44.4656 - val_loss: 46.4063 - val_MinusLogProbMetric: 46.4063 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 395/1000
2023-10-27 11:06:13.504 
Epoch 395/1000 
	 loss: 44.7695, MinusLogProbMetric: 44.7695, val_loss: 46.8887, val_MinusLogProbMetric: 46.8887

Epoch 395: val_loss did not improve from 45.55254
196/196 - 42s - loss: 44.7695 - MinusLogProbMetric: 44.7695 - val_loss: 46.8887 - val_MinusLogProbMetric: 46.8887 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 396/1000
2023-10-27 11:06:54.415 
Epoch 396/1000 
	 loss: 44.8356, MinusLogProbMetric: 44.8356, val_loss: 45.7607, val_MinusLogProbMetric: 45.7607

Epoch 396: val_loss did not improve from 45.55254
196/196 - 41s - loss: 44.8356 - MinusLogProbMetric: 44.8356 - val_loss: 45.7607 - val_MinusLogProbMetric: 45.7607 - lr: 1.2346e-05 - 41s/epoch - 209ms/step
Epoch 397/1000
2023-10-27 11:07:37.060 
Epoch 397/1000 
	 loss: 44.3859, MinusLogProbMetric: 44.3859, val_loss: 46.0270, val_MinusLogProbMetric: 46.0270

Epoch 397: val_loss did not improve from 45.55254
196/196 - 43s - loss: 44.3859 - MinusLogProbMetric: 44.3859 - val_loss: 46.0270 - val_MinusLogProbMetric: 46.0270 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 398/1000
2023-10-27 11:08:20.093 
Epoch 398/1000 
	 loss: 44.2781, MinusLogProbMetric: 44.2781, val_loss: 46.0936, val_MinusLogProbMetric: 46.0936

Epoch 398: val_loss did not improve from 45.55254
196/196 - 43s - loss: 44.2781 - MinusLogProbMetric: 44.2781 - val_loss: 46.0936 - val_MinusLogProbMetric: 46.0936 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 399/1000
2023-10-27 11:09:01.221 
Epoch 399/1000 
	 loss: 45.6340, MinusLogProbMetric: 45.6340, val_loss: 46.0036, val_MinusLogProbMetric: 46.0036

Epoch 399: val_loss did not improve from 45.55254
196/196 - 41s - loss: 45.6340 - MinusLogProbMetric: 45.6340 - val_loss: 46.0036 - val_MinusLogProbMetric: 46.0036 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 400/1000
2023-10-27 11:09:42.661 
Epoch 400/1000 
	 loss: 44.4058, MinusLogProbMetric: 44.4058, val_loss: 45.3810, val_MinusLogProbMetric: 45.3810

Epoch 400: val_loss improved from 45.55254 to 45.38102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 42s - loss: 44.4058 - MinusLogProbMetric: 44.4058 - val_loss: 45.3810 - val_MinusLogProbMetric: 45.3810 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 401/1000
2023-10-27 11:10:25.049 
Epoch 401/1000 
	 loss: 44.5762, MinusLogProbMetric: 44.5762, val_loss: 45.8639, val_MinusLogProbMetric: 45.8639

Epoch 401: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.5762 - MinusLogProbMetric: 44.5762 - val_loss: 45.8639 - val_MinusLogProbMetric: 45.8639 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 402/1000
2023-10-27 11:11:07.720 
Epoch 402/1000 
	 loss: 44.5746, MinusLogProbMetric: 44.5746, val_loss: 47.3654, val_MinusLogProbMetric: 47.3654

Epoch 402: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.5746 - MinusLogProbMetric: 44.5746 - val_loss: 47.3654 - val_MinusLogProbMetric: 47.3654 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 403/1000
2023-10-27 11:11:50.782 
Epoch 403/1000 
	 loss: 45.0360, MinusLogProbMetric: 45.0360, val_loss: 45.9882, val_MinusLogProbMetric: 45.9882

Epoch 403: val_loss did not improve from 45.38102
196/196 - 43s - loss: 45.0360 - MinusLogProbMetric: 45.0360 - val_loss: 45.9882 - val_MinusLogProbMetric: 45.9882 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 404/1000
2023-10-27 11:12:33.158 
Epoch 404/1000 
	 loss: 44.3751, MinusLogProbMetric: 44.3751, val_loss: 45.6718, val_MinusLogProbMetric: 45.6718

Epoch 404: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.3751 - MinusLogProbMetric: 44.3751 - val_loss: 45.6718 - val_MinusLogProbMetric: 45.6718 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 405/1000
2023-10-27 11:13:16.318 
Epoch 405/1000 
	 loss: 44.3980, MinusLogProbMetric: 44.3980, val_loss: 45.5510, val_MinusLogProbMetric: 45.5510

Epoch 405: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.3980 - MinusLogProbMetric: 44.3980 - val_loss: 45.5510 - val_MinusLogProbMetric: 45.5510 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 406/1000
2023-10-27 11:13:59.368 
Epoch 406/1000 
	 loss: 44.6872, MinusLogProbMetric: 44.6872, val_loss: 46.2597, val_MinusLogProbMetric: 46.2597

Epoch 406: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.6872 - MinusLogProbMetric: 44.6872 - val_loss: 46.2597 - val_MinusLogProbMetric: 46.2597 - lr: 1.2346e-05 - 43s/epoch - 220ms/step
Epoch 407/1000
2023-10-27 11:14:40.564 
Epoch 407/1000 
	 loss: 44.2772, MinusLogProbMetric: 44.2772, val_loss: 46.4281, val_MinusLogProbMetric: 46.4281

Epoch 407: val_loss did not improve from 45.38102
196/196 - 41s - loss: 44.2772 - MinusLogProbMetric: 44.2772 - val_loss: 46.4281 - val_MinusLogProbMetric: 46.4281 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 408/1000
2023-10-27 11:15:22.433 
Epoch 408/1000 
	 loss: 44.3916, MinusLogProbMetric: 44.3916, val_loss: 45.6811, val_MinusLogProbMetric: 45.6811

Epoch 408: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.3916 - MinusLogProbMetric: 44.3916 - val_loss: 45.6811 - val_MinusLogProbMetric: 45.6811 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 409/1000
2023-10-27 11:16:04.808 
Epoch 409/1000 
	 loss: 44.4475, MinusLogProbMetric: 44.4475, val_loss: 45.5384, val_MinusLogProbMetric: 45.5384

Epoch 409: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.4475 - MinusLogProbMetric: 44.4475 - val_loss: 45.5384 - val_MinusLogProbMetric: 45.5384 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 410/1000
2023-10-27 11:16:47.174 
Epoch 410/1000 
	 loss: 44.6289, MinusLogProbMetric: 44.6289, val_loss: 46.8809, val_MinusLogProbMetric: 46.8809

Epoch 410: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.6289 - MinusLogProbMetric: 44.6289 - val_loss: 46.8809 - val_MinusLogProbMetric: 46.8809 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 411/1000
2023-10-27 11:17:29.752 
Epoch 411/1000 
	 loss: 44.8383, MinusLogProbMetric: 44.8383, val_loss: 46.1874, val_MinusLogProbMetric: 46.1874

Epoch 411: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.8383 - MinusLogProbMetric: 44.8383 - val_loss: 46.1874 - val_MinusLogProbMetric: 46.1874 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 412/1000
2023-10-27 11:18:11.864 
Epoch 412/1000 
	 loss: 44.4541, MinusLogProbMetric: 44.4541, val_loss: 45.5690, val_MinusLogProbMetric: 45.5690

Epoch 412: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.4541 - MinusLogProbMetric: 44.4541 - val_loss: 45.5690 - val_MinusLogProbMetric: 45.5690 - lr: 1.2346e-05 - 42s/epoch - 215ms/step
Epoch 413/1000
2023-10-27 11:18:53.854 
Epoch 413/1000 
	 loss: 44.9183, MinusLogProbMetric: 44.9183, val_loss: 46.2228, val_MinusLogProbMetric: 46.2228

Epoch 413: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.9183 - MinusLogProbMetric: 44.9183 - val_loss: 46.2228 - val_MinusLogProbMetric: 46.2228 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 414/1000
2023-10-27 11:19:35.328 
Epoch 414/1000 
	 loss: 44.2931, MinusLogProbMetric: 44.2931, val_loss: 47.3220, val_MinusLogProbMetric: 47.3220

Epoch 414: val_loss did not improve from 45.38102
196/196 - 41s - loss: 44.2931 - MinusLogProbMetric: 44.2931 - val_loss: 47.3220 - val_MinusLogProbMetric: 47.3220 - lr: 1.2346e-05 - 41s/epoch - 212ms/step
Epoch 415/1000
2023-10-27 11:20:17.821 
Epoch 415/1000 
	 loss: 44.3226, MinusLogProbMetric: 44.3226, val_loss: 45.9295, val_MinusLogProbMetric: 45.9295

Epoch 415: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.3226 - MinusLogProbMetric: 44.3226 - val_loss: 45.9295 - val_MinusLogProbMetric: 45.9295 - lr: 1.2346e-05 - 42s/epoch - 217ms/step
Epoch 416/1000
2023-10-27 11:20:59.805 
Epoch 416/1000 
	 loss: 44.6321, MinusLogProbMetric: 44.6321, val_loss: 46.0090, val_MinusLogProbMetric: 46.0090

Epoch 416: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.6321 - MinusLogProbMetric: 44.6321 - val_loss: 46.0090 - val_MinusLogProbMetric: 46.0090 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 417/1000
2023-10-27 11:21:41.039 
Epoch 417/1000 
	 loss: 44.3751, MinusLogProbMetric: 44.3751, val_loss: 45.5248, val_MinusLogProbMetric: 45.5248

Epoch 417: val_loss did not improve from 45.38102
196/196 - 41s - loss: 44.3751 - MinusLogProbMetric: 44.3751 - val_loss: 45.5248 - val_MinusLogProbMetric: 45.5248 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 418/1000
2023-10-27 11:22:22.793 
Epoch 418/1000 
	 loss: 44.3949, MinusLogProbMetric: 44.3949, val_loss: 45.8129, val_MinusLogProbMetric: 45.8129

Epoch 418: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.3949 - MinusLogProbMetric: 44.3949 - val_loss: 45.8129 - val_MinusLogProbMetric: 45.8129 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 419/1000
2023-10-27 11:23:04.534 
Epoch 419/1000 
	 loss: 44.7302, MinusLogProbMetric: 44.7302, val_loss: 45.8777, val_MinusLogProbMetric: 45.8777

Epoch 419: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.7302 - MinusLogProbMetric: 44.7302 - val_loss: 45.8777 - val_MinusLogProbMetric: 45.8777 - lr: 1.2346e-05 - 42s/epoch - 213ms/step
Epoch 420/1000
2023-10-27 11:23:46.533 
Epoch 420/1000 
	 loss: 44.0843, MinusLogProbMetric: 44.0843, val_loss: 46.1063, val_MinusLogProbMetric: 46.1063

Epoch 420: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.0843 - MinusLogProbMetric: 44.0843 - val_loss: 46.1063 - val_MinusLogProbMetric: 46.1063 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 421/1000
2023-10-27 11:24:29.452 
Epoch 421/1000 
	 loss: 44.2691, MinusLogProbMetric: 44.2691, val_loss: 45.4915, val_MinusLogProbMetric: 45.4915

Epoch 421: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.2691 - MinusLogProbMetric: 44.2691 - val_loss: 45.4915 - val_MinusLogProbMetric: 45.4915 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 422/1000
2023-10-27 11:25:10.876 
Epoch 422/1000 
	 loss: 44.2306, MinusLogProbMetric: 44.2306, val_loss: 46.4305, val_MinusLogProbMetric: 46.4305

Epoch 422: val_loss did not improve from 45.38102
196/196 - 41s - loss: 44.2306 - MinusLogProbMetric: 44.2306 - val_loss: 46.4305 - val_MinusLogProbMetric: 46.4305 - lr: 1.2346e-05 - 41s/epoch - 211ms/step
Epoch 423/1000
2023-10-27 11:25:53.639 
Epoch 423/1000 
	 loss: 44.4630, MinusLogProbMetric: 44.4630, val_loss: 45.9993, val_MinusLogProbMetric: 45.9993

Epoch 423: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.4630 - MinusLogProbMetric: 44.4630 - val_loss: 45.9993 - val_MinusLogProbMetric: 45.9993 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 424/1000
2023-10-27 11:26:35.285 
Epoch 424/1000 
	 loss: 44.6599, MinusLogProbMetric: 44.6599, val_loss: 46.5781, val_MinusLogProbMetric: 46.5781

Epoch 424: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.6599 - MinusLogProbMetric: 44.6599 - val_loss: 46.5781 - val_MinusLogProbMetric: 46.5781 - lr: 1.2346e-05 - 42s/epoch - 212ms/step
Epoch 425/1000
2023-10-27 11:27:17.812 
Epoch 425/1000 
	 loss: 44.5157, MinusLogProbMetric: 44.5157, val_loss: 45.7521, val_MinusLogProbMetric: 45.7521

Epoch 425: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.5157 - MinusLogProbMetric: 44.5157 - val_loss: 45.7521 - val_MinusLogProbMetric: 45.7521 - lr: 1.2346e-05 - 43s/epoch - 217ms/step
Epoch 426/1000
2023-10-27 11:27:59.737 
Epoch 426/1000 
	 loss: 44.0714, MinusLogProbMetric: 44.0714, val_loss: 45.6485, val_MinusLogProbMetric: 45.6485

Epoch 426: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.0714 - MinusLogProbMetric: 44.0714 - val_loss: 45.6485 - val_MinusLogProbMetric: 45.6485 - lr: 1.2346e-05 - 42s/epoch - 214ms/step
Epoch 427/1000
2023-10-27 11:28:42.639 
Epoch 427/1000 
	 loss: 44.1886, MinusLogProbMetric: 44.1886, val_loss: 45.7969, val_MinusLogProbMetric: 45.7969

Epoch 427: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.1886 - MinusLogProbMetric: 44.1886 - val_loss: 45.7969 - val_MinusLogProbMetric: 45.7969 - lr: 1.2346e-05 - 43s/epoch - 219ms/step
Epoch 428/1000
2023-10-27 11:29:25.303 
Epoch 428/1000 
	 loss: 44.4282, MinusLogProbMetric: 44.4282, val_loss: 45.9779, val_MinusLogProbMetric: 45.9779

Epoch 428: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.4282 - MinusLogProbMetric: 44.4282 - val_loss: 45.9779 - val_MinusLogProbMetric: 45.9779 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 429/1000
2023-10-27 11:30:08.086 
Epoch 429/1000 
	 loss: 44.4812, MinusLogProbMetric: 44.4812, val_loss: 45.6963, val_MinusLogProbMetric: 45.6963

Epoch 429: val_loss did not improve from 45.38102
196/196 - 43s - loss: 44.4812 - MinusLogProbMetric: 44.4812 - val_loss: 45.6963 - val_MinusLogProbMetric: 45.6963 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 430/1000
2023-10-27 11:30:50.329 
Epoch 430/1000 
	 loss: 44.2301, MinusLogProbMetric: 44.2301, val_loss: 45.6875, val_MinusLogProbMetric: 45.6875

Epoch 430: val_loss did not improve from 45.38102
196/196 - 42s - loss: 44.2301 - MinusLogProbMetric: 44.2301 - val_loss: 45.6875 - val_MinusLogProbMetric: 45.6875 - lr: 1.2346e-05 - 42s/epoch - 216ms/step
Epoch 431/1000
2023-10-27 11:31:33.000 
Epoch 431/1000 
	 loss: 44.1117, MinusLogProbMetric: 44.1117, val_loss: 45.1747, val_MinusLogProbMetric: 45.1747

Epoch 431: val_loss improved from 45.38102 to 45.17472, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 44s - loss: 44.1117 - MinusLogProbMetric: 44.1117 - val_loss: 45.1747 - val_MinusLogProbMetric: 45.1747 - lr: 1.2346e-05 - 44s/epoch - 222ms/step
Epoch 432/1000
2023-10-27 11:32:16.654 
Epoch 432/1000 
	 loss: 44.9677, MinusLogProbMetric: 44.9677, val_loss: 46.3120, val_MinusLogProbMetric: 46.3120

Epoch 432: val_loss did not improve from 45.17472
196/196 - 43s - loss: 44.9677 - MinusLogProbMetric: 44.9677 - val_loss: 46.3120 - val_MinusLogProbMetric: 46.3120 - lr: 1.2346e-05 - 43s/epoch - 218ms/step
Epoch 433/1000
2023-10-27 11:32:56.876 
Epoch 433/1000 
	 loss: 44.2635, MinusLogProbMetric: 44.2635, val_loss: 46.7675, val_MinusLogProbMetric: 46.7675

Epoch 433: val_loss did not improve from 45.17472
196/196 - 40s - loss: 44.2635 - MinusLogProbMetric: 44.2635 - val_loss: 46.7675 - val_MinusLogProbMetric: 46.7675 - lr: 1.2346e-05 - 40s/epoch - 205ms/step
Epoch 434/1000
2023-10-27 11:33:33.256 
Epoch 434/1000 
	 loss: 44.8796, MinusLogProbMetric: 44.8796, val_loss: 49.5106, val_MinusLogProbMetric: 49.5106

Epoch 434: val_loss did not improve from 45.17472
196/196 - 36s - loss: 44.8796 - MinusLogProbMetric: 44.8796 - val_loss: 49.5106 - val_MinusLogProbMetric: 49.5106 - lr: 1.2346e-05 - 36s/epoch - 186ms/step
Epoch 435/1000
2023-10-27 11:34:09.613 
Epoch 435/1000 
	 loss: 44.2386, MinusLogProbMetric: 44.2386, val_loss: 46.2312, val_MinusLogProbMetric: 46.2312

Epoch 435: val_loss did not improve from 45.17472
196/196 - 36s - loss: 44.2386 - MinusLogProbMetric: 44.2386 - val_loss: 46.2312 - val_MinusLogProbMetric: 46.2312 - lr: 1.2346e-05 - 36s/epoch - 185ms/step
Epoch 436/1000
2023-10-27 11:34:45.573 
Epoch 436/1000 
	 loss: 44.8531, MinusLogProbMetric: 44.8531, val_loss: 45.4012, val_MinusLogProbMetric: 45.4012

Epoch 436: val_loss did not improve from 45.17472
196/196 - 36s - loss: 44.8531 - MinusLogProbMetric: 44.8531 - val_loss: 45.4012 - val_MinusLogProbMetric: 45.4012 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 437/1000
2023-10-27 11:35:24.047 
Epoch 437/1000 
	 loss: 44.0156, MinusLogProbMetric: 44.0156, val_loss: 45.9727, val_MinusLogProbMetric: 45.9727

Epoch 437: val_loss did not improve from 45.17472
196/196 - 38s - loss: 44.0156 - MinusLogProbMetric: 44.0156 - val_loss: 45.9727 - val_MinusLogProbMetric: 45.9727 - lr: 1.2346e-05 - 38s/epoch - 196ms/step
Epoch 438/1000
2023-10-27 11:36:02.026 
Epoch 438/1000 
	 loss: 44.3851, MinusLogProbMetric: 44.3851, val_loss: 45.5289, val_MinusLogProbMetric: 45.5289

Epoch 438: val_loss did not improve from 45.17472
196/196 - 38s - loss: 44.3851 - MinusLogProbMetric: 44.3851 - val_loss: 45.5289 - val_MinusLogProbMetric: 45.5289 - lr: 1.2346e-05 - 38s/epoch - 194ms/step
Epoch 439/1000
2023-10-27 11:36:38.001 
Epoch 439/1000 
	 loss: 44.1149, MinusLogProbMetric: 44.1149, val_loss: 45.7627, val_MinusLogProbMetric: 45.7627

Epoch 439: val_loss did not improve from 45.17472
196/196 - 36s - loss: 44.1149 - MinusLogProbMetric: 44.1149 - val_loss: 45.7627 - val_MinusLogProbMetric: 45.7627 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 440/1000
2023-10-27 11:37:14.077 
Epoch 440/1000 
	 loss: 44.3753, MinusLogProbMetric: 44.3753, val_loss: 45.3093, val_MinusLogProbMetric: 45.3093

Epoch 440: val_loss did not improve from 45.17472
196/196 - 36s - loss: 44.3753 - MinusLogProbMetric: 44.3753 - val_loss: 45.3093 - val_MinusLogProbMetric: 45.3093 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 441/1000
2023-10-27 11:37:51.935 
Epoch 441/1000 
	 loss: 44.0445, MinusLogProbMetric: 44.0445, val_loss: 45.1155, val_MinusLogProbMetric: 45.1155

Epoch 441: val_loss improved from 45.17472 to 45.11546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 44.0445 - MinusLogProbMetric: 44.0445 - val_loss: 45.1155 - val_MinusLogProbMetric: 45.1155 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 442/1000
2023-10-27 11:38:31.153 
Epoch 442/1000 
	 loss: 45.5453, MinusLogProbMetric: 45.5453, val_loss: 45.8403, val_MinusLogProbMetric: 45.8403

Epoch 442: val_loss did not improve from 45.11546
196/196 - 39s - loss: 45.5453 - MinusLogProbMetric: 45.5453 - val_loss: 45.8403 - val_MinusLogProbMetric: 45.8403 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 443/1000
2023-10-27 11:39:07.123 
Epoch 443/1000 
	 loss: 44.1196, MinusLogProbMetric: 44.1196, val_loss: 45.0467, val_MinusLogProbMetric: 45.0467

Epoch 443: val_loss improved from 45.11546 to 45.04672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 44.1196 - MinusLogProbMetric: 44.1196 - val_loss: 45.0467 - val_MinusLogProbMetric: 45.0467 - lr: 1.2346e-05 - 37s/epoch - 187ms/step
Epoch 444/1000
2023-10-27 11:39:43.888 
Epoch 444/1000 
	 loss: 44.0468, MinusLogProbMetric: 44.0468, val_loss: 47.1467, val_MinusLogProbMetric: 47.1467

Epoch 444: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0468 - MinusLogProbMetric: 44.0468 - val_loss: 47.1467 - val_MinusLogProbMetric: 47.1467 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 445/1000
2023-10-27 11:40:19.777 
Epoch 445/1000 
	 loss: 44.5507, MinusLogProbMetric: 44.5507, val_loss: 45.6891, val_MinusLogProbMetric: 45.6891

Epoch 445: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.5507 - MinusLogProbMetric: 44.5507 - val_loss: 45.6891 - val_MinusLogProbMetric: 45.6891 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 446/1000
2023-10-27 11:41:00.539 
Epoch 446/1000 
	 loss: 44.0385, MinusLogProbMetric: 44.0385, val_loss: 46.0951, val_MinusLogProbMetric: 46.0951

Epoch 446: val_loss did not improve from 45.04672
196/196 - 41s - loss: 44.0385 - MinusLogProbMetric: 44.0385 - val_loss: 46.0951 - val_MinusLogProbMetric: 46.0951 - lr: 1.2346e-05 - 41s/epoch - 208ms/step
Epoch 447/1000
2023-10-27 11:41:38.318 
Epoch 447/1000 
	 loss: 44.5104, MinusLogProbMetric: 44.5104, val_loss: 48.3932, val_MinusLogProbMetric: 48.3932

Epoch 447: val_loss did not improve from 45.04672
196/196 - 38s - loss: 44.5104 - MinusLogProbMetric: 44.5104 - val_loss: 48.3932 - val_MinusLogProbMetric: 48.3932 - lr: 1.2346e-05 - 38s/epoch - 193ms/step
Epoch 448/1000
2023-10-27 11:42:14.745 
Epoch 448/1000 
	 loss: 44.3793, MinusLogProbMetric: 44.3793, val_loss: 45.9224, val_MinusLogProbMetric: 45.9224

Epoch 448: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.3793 - MinusLogProbMetric: 44.3793 - val_loss: 45.9224 - val_MinusLogProbMetric: 45.9224 - lr: 1.2346e-05 - 36s/epoch - 186ms/step
Epoch 449/1000
2023-10-27 11:42:50.639 
Epoch 449/1000 
	 loss: 44.2139, MinusLogProbMetric: 44.2139, val_loss: 45.5222, val_MinusLogProbMetric: 45.5222

Epoch 449: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.2139 - MinusLogProbMetric: 44.2139 - val_loss: 45.5222 - val_MinusLogProbMetric: 45.5222 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 450/1000
2023-10-27 11:43:29.535 
Epoch 450/1000 
	 loss: 44.4215, MinusLogProbMetric: 44.4215, val_loss: 45.0838, val_MinusLogProbMetric: 45.0838

Epoch 450: val_loss did not improve from 45.04672
196/196 - 39s - loss: 44.4215 - MinusLogProbMetric: 44.4215 - val_loss: 45.0838 - val_MinusLogProbMetric: 45.0838 - lr: 1.2346e-05 - 39s/epoch - 198ms/step
Epoch 451/1000
2023-10-27 11:44:08.650 
Epoch 451/1000 
	 loss: 44.0304, MinusLogProbMetric: 44.0304, val_loss: 46.3454, val_MinusLogProbMetric: 46.3454

Epoch 451: val_loss did not improve from 45.04672
196/196 - 39s - loss: 44.0304 - MinusLogProbMetric: 44.0304 - val_loss: 46.3454 - val_MinusLogProbMetric: 46.3454 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 452/1000
2023-10-27 11:44:44.574 
Epoch 452/1000 
	 loss: 44.0840, MinusLogProbMetric: 44.0840, val_loss: 46.8337, val_MinusLogProbMetric: 46.8337

Epoch 452: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0840 - MinusLogProbMetric: 44.0840 - val_loss: 46.8337 - val_MinusLogProbMetric: 46.8337 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 453/1000
2023-10-27 11:45:20.645 
Epoch 453/1000 
	 loss: 44.9551, MinusLogProbMetric: 44.9551, val_loss: 45.3650, val_MinusLogProbMetric: 45.3650

Epoch 453: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.9551 - MinusLogProbMetric: 44.9551 - val_loss: 45.3650 - val_MinusLogProbMetric: 45.3650 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 454/1000
2023-10-27 11:45:56.529 
Epoch 454/1000 
	 loss: 44.4016, MinusLogProbMetric: 44.4016, val_loss: 45.4420, val_MinusLogProbMetric: 45.4420

Epoch 454: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.4016 - MinusLogProbMetric: 44.4016 - val_loss: 45.4420 - val_MinusLogProbMetric: 45.4420 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 455/1000
2023-10-27 11:46:37.121 
Epoch 455/1000 
	 loss: 44.0356, MinusLogProbMetric: 44.0356, val_loss: 45.9712, val_MinusLogProbMetric: 45.9712

Epoch 455: val_loss did not improve from 45.04672
196/196 - 41s - loss: 44.0356 - MinusLogProbMetric: 44.0356 - val_loss: 45.9712 - val_MinusLogProbMetric: 45.9712 - lr: 1.2346e-05 - 41s/epoch - 207ms/step
Epoch 456/1000
2023-10-27 11:47:14.319 
Epoch 456/1000 
	 loss: 44.6399, MinusLogProbMetric: 44.6399, val_loss: 45.8175, val_MinusLogProbMetric: 45.8175

Epoch 456: val_loss did not improve from 45.04672
196/196 - 37s - loss: 44.6399 - MinusLogProbMetric: 44.6399 - val_loss: 45.8175 - val_MinusLogProbMetric: 45.8175 - lr: 1.2346e-05 - 37s/epoch - 190ms/step
Epoch 457/1000
2023-10-27 11:47:50.293 
Epoch 457/1000 
	 loss: 44.4451, MinusLogProbMetric: 44.4451, val_loss: 45.7414, val_MinusLogProbMetric: 45.7414

Epoch 457: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.4451 - MinusLogProbMetric: 44.4451 - val_loss: 45.7414 - val_MinusLogProbMetric: 45.7414 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 458/1000
2023-10-27 11:48:25.149 
Epoch 458/1000 
	 loss: 44.6048, MinusLogProbMetric: 44.6048, val_loss: 45.2802, val_MinusLogProbMetric: 45.2802

Epoch 458: val_loss did not improve from 45.04672
196/196 - 35s - loss: 44.6048 - MinusLogProbMetric: 44.6048 - val_loss: 45.2802 - val_MinusLogProbMetric: 45.2802 - lr: 1.2346e-05 - 35s/epoch - 178ms/step
Epoch 459/1000
2023-10-27 11:49:02.332 
Epoch 459/1000 
	 loss: 43.9912, MinusLogProbMetric: 43.9912, val_loss: 45.1824, val_MinusLogProbMetric: 45.1824

Epoch 459: val_loss did not improve from 45.04672
196/196 - 37s - loss: 43.9912 - MinusLogProbMetric: 43.9912 - val_loss: 45.1824 - val_MinusLogProbMetric: 45.1824 - lr: 1.2346e-05 - 37s/epoch - 190ms/step
Epoch 460/1000
2023-10-27 11:49:40.594 
Epoch 460/1000 
	 loss: 44.7453, MinusLogProbMetric: 44.7453, val_loss: 46.5585, val_MinusLogProbMetric: 46.5585

Epoch 460: val_loss did not improve from 45.04672
196/196 - 38s - loss: 44.7453 - MinusLogProbMetric: 44.7453 - val_loss: 46.5585 - val_MinusLogProbMetric: 46.5585 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 461/1000
2023-10-27 11:50:16.632 
Epoch 461/1000 
	 loss: 44.3646, MinusLogProbMetric: 44.3646, val_loss: 45.5360, val_MinusLogProbMetric: 45.5360

Epoch 461: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.3646 - MinusLogProbMetric: 44.3646 - val_loss: 45.5360 - val_MinusLogProbMetric: 45.5360 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 462/1000
2023-10-27 11:50:52.445 
Epoch 462/1000 
	 loss: 44.0065, MinusLogProbMetric: 44.0065, val_loss: 45.5584, val_MinusLogProbMetric: 45.5584

Epoch 462: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0065 - MinusLogProbMetric: 44.0065 - val_loss: 45.5584 - val_MinusLogProbMetric: 45.5584 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 463/1000
2023-10-27 11:51:28.268 
Epoch 463/1000 
	 loss: 44.4951, MinusLogProbMetric: 44.4951, val_loss: 45.6888, val_MinusLogProbMetric: 45.6888

Epoch 463: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.4951 - MinusLogProbMetric: 44.4951 - val_loss: 45.6888 - val_MinusLogProbMetric: 45.6888 - lr: 1.2346e-05 - 36s/epoch - 183ms/step
Epoch 464/1000
2023-10-27 11:52:08.083 
Epoch 464/1000 
	 loss: 44.6447, MinusLogProbMetric: 44.6447, val_loss: 46.2288, val_MinusLogProbMetric: 46.2288

Epoch 464: val_loss did not improve from 45.04672
196/196 - 40s - loss: 44.6447 - MinusLogProbMetric: 44.6447 - val_loss: 46.2288 - val_MinusLogProbMetric: 46.2288 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 465/1000
2023-10-27 11:52:46.462 
Epoch 465/1000 
	 loss: 44.0229, MinusLogProbMetric: 44.0229, val_loss: 45.8869, val_MinusLogProbMetric: 45.8869

Epoch 465: val_loss did not improve from 45.04672
196/196 - 38s - loss: 44.0229 - MinusLogProbMetric: 44.0229 - val_loss: 45.8869 - val_MinusLogProbMetric: 45.8869 - lr: 1.2346e-05 - 38s/epoch - 196ms/step
Epoch 466/1000
2023-10-27 11:53:22.615 
Epoch 466/1000 
	 loss: 44.5844, MinusLogProbMetric: 44.5844, val_loss: 45.6289, val_MinusLogProbMetric: 45.6289

Epoch 466: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.5844 - MinusLogProbMetric: 44.5844 - val_loss: 45.6289 - val_MinusLogProbMetric: 45.6289 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 467/1000
2023-10-27 11:53:58.869 
Epoch 467/1000 
	 loss: 44.0404, MinusLogProbMetric: 44.0404, val_loss: 45.4416, val_MinusLogProbMetric: 45.4416

Epoch 467: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0404 - MinusLogProbMetric: 44.0404 - val_loss: 45.4416 - val_MinusLogProbMetric: 45.4416 - lr: 1.2346e-05 - 36s/epoch - 185ms/step
Epoch 468/1000
2023-10-27 11:54:37.026 
Epoch 468/1000 
	 loss: 43.9677, MinusLogProbMetric: 43.9677, val_loss: 45.6774, val_MinusLogProbMetric: 45.6774

Epoch 468: val_loss did not improve from 45.04672
196/196 - 38s - loss: 43.9677 - MinusLogProbMetric: 43.9677 - val_loss: 45.6774 - val_MinusLogProbMetric: 45.6774 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 469/1000
2023-10-27 11:55:15.144 
Epoch 469/1000 
	 loss: 44.5252, MinusLogProbMetric: 44.5252, val_loss: 49.0666, val_MinusLogProbMetric: 49.0666

Epoch 469: val_loss did not improve from 45.04672
196/196 - 38s - loss: 44.5252 - MinusLogProbMetric: 44.5252 - val_loss: 49.0666 - val_MinusLogProbMetric: 49.0666 - lr: 1.2346e-05 - 38s/epoch - 194ms/step
Epoch 470/1000
2023-10-27 11:55:51.304 
Epoch 470/1000 
	 loss: 44.1870, MinusLogProbMetric: 44.1870, val_loss: 45.3802, val_MinusLogProbMetric: 45.3802

Epoch 470: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.1870 - MinusLogProbMetric: 44.1870 - val_loss: 45.3802 - val_MinusLogProbMetric: 45.3802 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 471/1000
2023-10-27 11:56:27.457 
Epoch 471/1000 
	 loss: 44.2391, MinusLogProbMetric: 44.2391, val_loss: 45.3583, val_MinusLogProbMetric: 45.3583

Epoch 471: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.2391 - MinusLogProbMetric: 44.2391 - val_loss: 45.3583 - val_MinusLogProbMetric: 45.3583 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 472/1000
2023-10-27 11:57:03.899 
Epoch 472/1000 
	 loss: 44.0516, MinusLogProbMetric: 44.0516, val_loss: 45.4112, val_MinusLogProbMetric: 45.4112

Epoch 472: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0516 - MinusLogProbMetric: 44.0516 - val_loss: 45.4112 - val_MinusLogProbMetric: 45.4112 - lr: 1.2346e-05 - 36s/epoch - 186ms/step
Epoch 473/1000
2023-10-27 11:57:41.679 
Epoch 473/1000 
	 loss: 44.4877, MinusLogProbMetric: 44.4877, val_loss: 46.4902, val_MinusLogProbMetric: 46.4902

Epoch 473: val_loss did not improve from 45.04672
196/196 - 38s - loss: 44.4877 - MinusLogProbMetric: 44.4877 - val_loss: 46.4902 - val_MinusLogProbMetric: 46.4902 - lr: 1.2346e-05 - 38s/epoch - 193ms/step
Epoch 474/1000
2023-10-27 11:58:20.365 
Epoch 474/1000 
	 loss: 44.5706, MinusLogProbMetric: 44.5706, val_loss: 45.1749, val_MinusLogProbMetric: 45.1749

Epoch 474: val_loss did not improve from 45.04672
196/196 - 39s - loss: 44.5706 - MinusLogProbMetric: 44.5706 - val_loss: 45.1749 - val_MinusLogProbMetric: 45.1749 - lr: 1.2346e-05 - 39s/epoch - 197ms/step
Epoch 475/1000
2023-10-27 11:58:56.495 
Epoch 475/1000 
	 loss: 43.8137, MinusLogProbMetric: 43.8137, val_loss: 46.8437, val_MinusLogProbMetric: 46.8437

Epoch 475: val_loss did not improve from 45.04672
196/196 - 36s - loss: 43.8137 - MinusLogProbMetric: 43.8137 - val_loss: 46.8437 - val_MinusLogProbMetric: 46.8437 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 476/1000
2023-10-27 11:59:32.700 
Epoch 476/1000 
	 loss: 44.0518, MinusLogProbMetric: 44.0518, val_loss: 45.7879, val_MinusLogProbMetric: 45.7879

Epoch 476: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0518 - MinusLogProbMetric: 44.0518 - val_loss: 45.7879 - val_MinusLogProbMetric: 45.7879 - lr: 1.2346e-05 - 36s/epoch - 185ms/step
Epoch 477/1000
2023-10-27 12:00:10.913 
Epoch 477/1000 
	 loss: 43.9492, MinusLogProbMetric: 43.9492, val_loss: 45.4045, val_MinusLogProbMetric: 45.4045

Epoch 477: val_loss did not improve from 45.04672
196/196 - 38s - loss: 43.9492 - MinusLogProbMetric: 43.9492 - val_loss: 45.4045 - val_MinusLogProbMetric: 45.4045 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 478/1000
2023-10-27 12:00:49.131 
Epoch 478/1000 
	 loss: 44.2724, MinusLogProbMetric: 44.2724, val_loss: 45.9376, val_MinusLogProbMetric: 45.9376

Epoch 478: val_loss did not improve from 45.04672
196/196 - 38s - loss: 44.2724 - MinusLogProbMetric: 44.2724 - val_loss: 45.9376 - val_MinusLogProbMetric: 45.9376 - lr: 1.2346e-05 - 38s/epoch - 195ms/step
Epoch 479/1000
2023-10-27 12:01:25.486 
Epoch 479/1000 
	 loss: 44.6433, MinusLogProbMetric: 44.6433, val_loss: 45.1832, val_MinusLogProbMetric: 45.1832

Epoch 479: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.6433 - MinusLogProbMetric: 44.6433 - val_loss: 45.1832 - val_MinusLogProbMetric: 45.1832 - lr: 1.2346e-05 - 36s/epoch - 185ms/step
Epoch 480/1000
2023-10-27 12:02:01.627 
Epoch 480/1000 
	 loss: 43.9757, MinusLogProbMetric: 43.9757, val_loss: 45.7938, val_MinusLogProbMetric: 45.7938

Epoch 480: val_loss did not improve from 45.04672
196/196 - 36s - loss: 43.9757 - MinusLogProbMetric: 43.9757 - val_loss: 45.7938 - val_MinusLogProbMetric: 45.7938 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 481/1000
2023-10-27 12:02:38.113 
Epoch 481/1000 
	 loss: 43.8044, MinusLogProbMetric: 43.8044, val_loss: 45.7684, val_MinusLogProbMetric: 45.7684

Epoch 481: val_loss did not improve from 45.04672
196/196 - 36s - loss: 43.8044 - MinusLogProbMetric: 43.8044 - val_loss: 45.7684 - val_MinusLogProbMetric: 45.7684 - lr: 1.2346e-05 - 36s/epoch - 186ms/step
Epoch 482/1000
2023-10-27 12:03:17.691 
Epoch 482/1000 
	 loss: 43.9785, MinusLogProbMetric: 43.9785, val_loss: 45.5363, val_MinusLogProbMetric: 45.5363

Epoch 482: val_loss did not improve from 45.04672
196/196 - 40s - loss: 43.9785 - MinusLogProbMetric: 43.9785 - val_loss: 45.5363 - val_MinusLogProbMetric: 45.5363 - lr: 1.2346e-05 - 40s/epoch - 202ms/step
Epoch 483/1000
2023-10-27 12:03:56.939 
Epoch 483/1000 
	 loss: 44.3023, MinusLogProbMetric: 44.3023, val_loss: 45.5966, val_MinusLogProbMetric: 45.5966

Epoch 483: val_loss did not improve from 45.04672
196/196 - 39s - loss: 44.3023 - MinusLogProbMetric: 44.3023 - val_loss: 45.5966 - val_MinusLogProbMetric: 45.5966 - lr: 1.2346e-05 - 39s/epoch - 200ms/step
Epoch 484/1000
2023-10-27 12:04:32.496 
Epoch 484/1000 
	 loss: 44.2392, MinusLogProbMetric: 44.2392, val_loss: 45.2817, val_MinusLogProbMetric: 45.2817

Epoch 484: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.2392 - MinusLogProbMetric: 44.2392 - val_loss: 45.2817 - val_MinusLogProbMetric: 45.2817 - lr: 1.2346e-05 - 36s/epoch - 181ms/step
Epoch 485/1000
2023-10-27 12:05:08.582 
Epoch 485/1000 
	 loss: 44.0701, MinusLogProbMetric: 44.0701, val_loss: 45.3486, val_MinusLogProbMetric: 45.3486

Epoch 485: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0701 - MinusLogProbMetric: 44.0701 - val_loss: 45.3486 - val_MinusLogProbMetric: 45.3486 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 486/1000
2023-10-27 12:05:46.363 
Epoch 486/1000 
	 loss: 44.3721, MinusLogProbMetric: 44.3721, val_loss: 45.3646, val_MinusLogProbMetric: 45.3646

Epoch 486: val_loss did not improve from 45.04672
196/196 - 38s - loss: 44.3721 - MinusLogProbMetric: 44.3721 - val_loss: 45.3646 - val_MinusLogProbMetric: 45.3646 - lr: 1.2346e-05 - 38s/epoch - 193ms/step
Epoch 487/1000
2023-10-27 12:06:27.596 
Epoch 487/1000 
	 loss: 44.0331, MinusLogProbMetric: 44.0331, val_loss: 46.8657, val_MinusLogProbMetric: 46.8657

Epoch 487: val_loss did not improve from 45.04672
196/196 - 41s - loss: 44.0331 - MinusLogProbMetric: 44.0331 - val_loss: 46.8657 - val_MinusLogProbMetric: 46.8657 - lr: 1.2346e-05 - 41s/epoch - 210ms/step
Epoch 488/1000
2023-10-27 12:07:04.399 
Epoch 488/1000 
	 loss: 43.8367, MinusLogProbMetric: 43.8367, val_loss: 47.5639, val_MinusLogProbMetric: 47.5639

Epoch 488: val_loss did not improve from 45.04672
196/196 - 37s - loss: 43.8367 - MinusLogProbMetric: 43.8367 - val_loss: 47.5639 - val_MinusLogProbMetric: 47.5639 - lr: 1.2346e-05 - 37s/epoch - 188ms/step
Epoch 489/1000
2023-10-27 12:07:40.466 
Epoch 489/1000 
	 loss: 45.2403, MinusLogProbMetric: 45.2403, val_loss: 46.3811, val_MinusLogProbMetric: 46.3811

Epoch 489: val_loss did not improve from 45.04672
196/196 - 36s - loss: 45.2403 - MinusLogProbMetric: 45.2403 - val_loss: 46.3811 - val_MinusLogProbMetric: 46.3811 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 490/1000
2023-10-27 12:08:16.188 
Epoch 490/1000 
	 loss: 44.3434, MinusLogProbMetric: 44.3434, val_loss: 46.2225, val_MinusLogProbMetric: 46.2225

Epoch 490: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.3434 - MinusLogProbMetric: 44.3434 - val_loss: 46.2225 - val_MinusLogProbMetric: 46.2225 - lr: 1.2346e-05 - 36s/epoch - 182ms/step
Epoch 491/1000
2023-10-27 12:08:56.053 
Epoch 491/1000 
	 loss: 44.1704, MinusLogProbMetric: 44.1704, val_loss: 45.6099, val_MinusLogProbMetric: 45.6099

Epoch 491: val_loss did not improve from 45.04672
196/196 - 40s - loss: 44.1704 - MinusLogProbMetric: 44.1704 - val_loss: 45.6099 - val_MinusLogProbMetric: 45.6099 - lr: 1.2346e-05 - 40s/epoch - 203ms/step
Epoch 492/1000
2023-10-27 12:09:34.099 
Epoch 492/1000 
	 loss: 43.8800, MinusLogProbMetric: 43.8800, val_loss: 45.4407, val_MinusLogProbMetric: 45.4407

Epoch 492: val_loss did not improve from 45.04672
196/196 - 38s - loss: 43.8800 - MinusLogProbMetric: 43.8800 - val_loss: 45.4407 - val_MinusLogProbMetric: 45.4407 - lr: 1.2346e-05 - 38s/epoch - 194ms/step
Epoch 493/1000
2023-10-27 12:10:10.086 
Epoch 493/1000 
	 loss: 44.0753, MinusLogProbMetric: 44.0753, val_loss: 45.6674, val_MinusLogProbMetric: 45.6674

Epoch 493: val_loss did not improve from 45.04672
196/196 - 36s - loss: 44.0753 - MinusLogProbMetric: 44.0753 - val_loss: 45.6674 - val_MinusLogProbMetric: 45.6674 - lr: 1.2346e-05 - 36s/epoch - 184ms/step
Epoch 494/1000
2023-10-27 12:10:46.147 
Epoch 494/1000 
	 loss: 43.4780, MinusLogProbMetric: 43.4780, val_loss: 45.0723, val_MinusLogProbMetric: 45.0723

Epoch 494: val_loss did not improve from 45.04672
196/196 - 36s - loss: 43.4780 - MinusLogProbMetric: 43.4780 - val_loss: 45.0723 - val_MinusLogProbMetric: 45.0723 - lr: 6.1728e-06 - 36s/epoch - 184ms/step
Epoch 495/1000
2023-10-27 12:11:23.423 
Epoch 495/1000 
	 loss: 43.4304, MinusLogProbMetric: 43.4304, val_loss: 45.2390, val_MinusLogProbMetric: 45.2390

Epoch 495: val_loss did not improve from 45.04672
196/196 - 37s - loss: 43.4304 - MinusLogProbMetric: 43.4304 - val_loss: 45.2390 - val_MinusLogProbMetric: 45.2390 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 496/1000
2023-10-27 12:12:02.434 
Epoch 496/1000 
	 loss: 43.4120, MinusLogProbMetric: 43.4120, val_loss: 44.9570, val_MinusLogProbMetric: 44.9570

Epoch 496: val_loss improved from 45.04672 to 44.95698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 40s - loss: 43.4120 - MinusLogProbMetric: 43.4120 - val_loss: 44.9570 - val_MinusLogProbMetric: 44.9570 - lr: 6.1728e-06 - 40s/epoch - 202ms/step
Epoch 497/1000
2023-10-27 12:12:39.969 
Epoch 497/1000 
	 loss: 43.3912, MinusLogProbMetric: 43.3912, val_loss: 45.3467, val_MinusLogProbMetric: 45.3467

Epoch 497: val_loss did not improve from 44.95698
196/196 - 37s - loss: 43.3912 - MinusLogProbMetric: 43.3912 - val_loss: 45.3467 - val_MinusLogProbMetric: 45.3467 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 498/1000
2023-10-27 12:13:16.054 
Epoch 498/1000 
	 loss: 43.4450, MinusLogProbMetric: 43.4450, val_loss: 45.6550, val_MinusLogProbMetric: 45.6550

Epoch 498: val_loss did not improve from 44.95698
196/196 - 36s - loss: 43.4450 - MinusLogProbMetric: 43.4450 - val_loss: 45.6550 - val_MinusLogProbMetric: 45.6550 - lr: 6.1728e-06 - 36s/epoch - 184ms/step
Epoch 499/1000
2023-10-27 12:13:51.524 
Epoch 499/1000 
	 loss: 43.3710, MinusLogProbMetric: 43.3710, val_loss: 44.9601, val_MinusLogProbMetric: 44.9601

Epoch 499: val_loss did not improve from 44.95698
196/196 - 35s - loss: 43.3710 - MinusLogProbMetric: 43.3710 - val_loss: 44.9601 - val_MinusLogProbMetric: 44.9601 - lr: 6.1728e-06 - 35s/epoch - 181ms/step
Epoch 500/1000
2023-10-27 12:14:32.955 
Epoch 500/1000 
	 loss: 43.3685, MinusLogProbMetric: 43.3685, val_loss: 45.0016, val_MinusLogProbMetric: 45.0016

Epoch 500: val_loss did not improve from 44.95698
196/196 - 41s - loss: 43.3685 - MinusLogProbMetric: 43.3685 - val_loss: 45.0016 - val_MinusLogProbMetric: 45.0016 - lr: 6.1728e-06 - 41s/epoch - 211ms/step
Epoch 501/1000
2023-10-27 12:15:12.221 
Epoch 501/1000 
	 loss: 43.3911, MinusLogProbMetric: 43.3911, val_loss: 45.1201, val_MinusLogProbMetric: 45.1201

Epoch 501: val_loss did not improve from 44.95698
196/196 - 39s - loss: 43.3911 - MinusLogProbMetric: 43.3911 - val_loss: 45.1201 - val_MinusLogProbMetric: 45.1201 - lr: 6.1728e-06 - 39s/epoch - 200ms/step
Epoch 502/1000
2023-10-27 12:15:48.465 
Epoch 502/1000 
	 loss: 43.3858, MinusLogProbMetric: 43.3858, val_loss: 44.8235, val_MinusLogProbMetric: 44.8235

Epoch 502: val_loss improved from 44.95698 to 44.82352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 43.3858 - MinusLogProbMetric: 43.3858 - val_loss: 44.8235 - val_MinusLogProbMetric: 44.8235 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 503/1000
2023-10-27 12:16:24.896 
Epoch 503/1000 
	 loss: 43.4057, MinusLogProbMetric: 43.4057, val_loss: 45.0165, val_MinusLogProbMetric: 45.0165

Epoch 503: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.4057 - MinusLogProbMetric: 43.4057 - val_loss: 45.0165 - val_MinusLogProbMetric: 45.0165 - lr: 6.1728e-06 - 36s/epoch - 183ms/step
Epoch 504/1000
2023-10-27 12:17:02.223 
Epoch 504/1000 
	 loss: 43.4383, MinusLogProbMetric: 43.4383, val_loss: 45.0848, val_MinusLogProbMetric: 45.0848

Epoch 504: val_loss did not improve from 44.82352
196/196 - 37s - loss: 43.4383 - MinusLogProbMetric: 43.4383 - val_loss: 45.0848 - val_MinusLogProbMetric: 45.0848 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 505/1000
2023-10-27 12:17:40.834 
Epoch 505/1000 
	 loss: 43.3540, MinusLogProbMetric: 43.3540, val_loss: 45.3211, val_MinusLogProbMetric: 45.3211

Epoch 505: val_loss did not improve from 44.82352
196/196 - 39s - loss: 43.3540 - MinusLogProbMetric: 43.3540 - val_loss: 45.3211 - val_MinusLogProbMetric: 45.3211 - lr: 6.1728e-06 - 39s/epoch - 197ms/step
Epoch 506/1000
2023-10-27 12:18:17.014 
Epoch 506/1000 
	 loss: 43.3674, MinusLogProbMetric: 43.3674, val_loss: 44.9556, val_MinusLogProbMetric: 44.9556

Epoch 506: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.3674 - MinusLogProbMetric: 43.3674 - val_loss: 44.9556 - val_MinusLogProbMetric: 44.9556 - lr: 6.1728e-06 - 36s/epoch - 185ms/step
Epoch 507/1000
2023-10-27 12:18:52.719 
Epoch 507/1000 
	 loss: 43.3406, MinusLogProbMetric: 43.3406, val_loss: 45.1611, val_MinusLogProbMetric: 45.1611

Epoch 507: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.3406 - MinusLogProbMetric: 43.3406 - val_loss: 45.1611 - val_MinusLogProbMetric: 45.1611 - lr: 6.1728e-06 - 36s/epoch - 182ms/step
Epoch 508/1000
2023-10-27 12:19:28.312 
Epoch 508/1000 
	 loss: 43.3706, MinusLogProbMetric: 43.3706, val_loss: 45.5515, val_MinusLogProbMetric: 45.5515

Epoch 508: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.3706 - MinusLogProbMetric: 43.3706 - val_loss: 45.5515 - val_MinusLogProbMetric: 45.5515 - lr: 6.1728e-06 - 36s/epoch - 182ms/step
Epoch 509/1000
2023-10-27 12:20:06.134 
Epoch 509/1000 
	 loss: 43.3661, MinusLogProbMetric: 43.3661, val_loss: 44.9686, val_MinusLogProbMetric: 44.9686

Epoch 509: val_loss did not improve from 44.82352
196/196 - 38s - loss: 43.3661 - MinusLogProbMetric: 43.3661 - val_loss: 44.9686 - val_MinusLogProbMetric: 44.9686 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 510/1000
2023-10-27 12:20:46.086 
Epoch 510/1000 
	 loss: 43.3954, MinusLogProbMetric: 43.3954, val_loss: 45.4071, val_MinusLogProbMetric: 45.4071

Epoch 510: val_loss did not improve from 44.82352
196/196 - 40s - loss: 43.3954 - MinusLogProbMetric: 43.3954 - val_loss: 45.4071 - val_MinusLogProbMetric: 45.4071 - lr: 6.1728e-06 - 40s/epoch - 204ms/step
Epoch 511/1000
2023-10-27 12:21:22.247 
Epoch 511/1000 
	 loss: 43.3371, MinusLogProbMetric: 43.3371, val_loss: 45.0640, val_MinusLogProbMetric: 45.0640

Epoch 511: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.3371 - MinusLogProbMetric: 43.3371 - val_loss: 45.0640 - val_MinusLogProbMetric: 45.0640 - lr: 6.1728e-06 - 36s/epoch - 184ms/step
Epoch 512/1000
2023-10-27 12:21:58.084 
Epoch 512/1000 
	 loss: 43.3481, MinusLogProbMetric: 43.3481, val_loss: 45.0285, val_MinusLogProbMetric: 45.0285

Epoch 512: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.3481 - MinusLogProbMetric: 43.3481 - val_loss: 45.0285 - val_MinusLogProbMetric: 45.0285 - lr: 6.1728e-06 - 36s/epoch - 183ms/step
Epoch 513/1000
2023-10-27 12:22:34.024 
Epoch 513/1000 
	 loss: 43.4264, MinusLogProbMetric: 43.4264, val_loss: 45.3432, val_MinusLogProbMetric: 45.3432

Epoch 513: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.4264 - MinusLogProbMetric: 43.4264 - val_loss: 45.3432 - val_MinusLogProbMetric: 45.3432 - lr: 6.1728e-06 - 36s/epoch - 183ms/step
Epoch 514/1000
2023-10-27 12:23:14.680 
Epoch 514/1000 
	 loss: 43.3682, MinusLogProbMetric: 43.3682, val_loss: 44.9727, val_MinusLogProbMetric: 44.9727

Epoch 514: val_loss did not improve from 44.82352
196/196 - 41s - loss: 43.3682 - MinusLogProbMetric: 43.3682 - val_loss: 44.9727 - val_MinusLogProbMetric: 44.9727 - lr: 6.1728e-06 - 41s/epoch - 207ms/step
Epoch 515/1000
2023-10-27 12:23:50.869 
Epoch 515/1000 
	 loss: 43.3488, MinusLogProbMetric: 43.3488, val_loss: 44.9793, val_MinusLogProbMetric: 44.9793

Epoch 515: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.3488 - MinusLogProbMetric: 43.3488 - val_loss: 44.9793 - val_MinusLogProbMetric: 44.9793 - lr: 6.1728e-06 - 36s/epoch - 185ms/step
Epoch 516/1000
2023-10-27 12:24:26.642 
Epoch 516/1000 
	 loss: 43.4171, MinusLogProbMetric: 43.4171, val_loss: 44.9339, val_MinusLogProbMetric: 44.9339

Epoch 516: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.4171 - MinusLogProbMetric: 43.4171 - val_loss: 44.9339 - val_MinusLogProbMetric: 44.9339 - lr: 6.1728e-06 - 36s/epoch - 183ms/step
Epoch 517/1000
2023-10-27 12:25:03.062 
Epoch 517/1000 
	 loss: 43.3776, MinusLogProbMetric: 43.3776, val_loss: 45.2429, val_MinusLogProbMetric: 45.2429

Epoch 517: val_loss did not improve from 44.82352
196/196 - 36s - loss: 43.3776 - MinusLogProbMetric: 43.3776 - val_loss: 45.2429 - val_MinusLogProbMetric: 45.2429 - lr: 6.1728e-06 - 36s/epoch - 186ms/step
Epoch 518/1000
2023-10-27 12:25:40.077 
Epoch 518/1000 
	 loss: 43.3061, MinusLogProbMetric: 43.3061, val_loss: 45.1921, val_MinusLogProbMetric: 45.1921

Epoch 518: val_loss did not improve from 44.82352
196/196 - 37s - loss: 43.3061 - MinusLogProbMetric: 43.3061 - val_loss: 45.1921 - val_MinusLogProbMetric: 45.1921 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 519/1000
2023-10-27 12:26:20.002 
Epoch 519/1000 
	 loss: 43.2601, MinusLogProbMetric: 43.2601, val_loss: 44.7736, val_MinusLogProbMetric: 44.7736

Epoch 519: val_loss improved from 44.82352 to 44.77358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 41s - loss: 43.2601 - MinusLogProbMetric: 43.2601 - val_loss: 44.7736 - val_MinusLogProbMetric: 44.7736 - lr: 6.1728e-06 - 41s/epoch - 207ms/step
Epoch 520/1000
2023-10-27 12:26:56.432 
Epoch 520/1000 
	 loss: 43.3121, MinusLogProbMetric: 43.3121, val_loss: 45.2631, val_MinusLogProbMetric: 45.2631

Epoch 520: val_loss did not improve from 44.77358
196/196 - 36s - loss: 43.3121 - MinusLogProbMetric: 43.3121 - val_loss: 45.2631 - val_MinusLogProbMetric: 45.2631 - lr: 6.1728e-06 - 36s/epoch - 182ms/step
Epoch 521/1000
2023-10-27 12:27:32.496 
Epoch 521/1000 
	 loss: 43.3847, MinusLogProbMetric: 43.3847, val_loss: 45.1280, val_MinusLogProbMetric: 45.1280

Epoch 521: val_loss did not improve from 44.77358
196/196 - 36s - loss: 43.3847 - MinusLogProbMetric: 43.3847 - val_loss: 45.1280 - val_MinusLogProbMetric: 45.1280 - lr: 6.1728e-06 - 36s/epoch - 184ms/step
Epoch 522/1000
2023-10-27 12:28:08.262 
Epoch 522/1000 
	 loss: 43.2575, MinusLogProbMetric: 43.2575, val_loss: 44.7311, val_MinusLogProbMetric: 44.7311

Epoch 522: val_loss improved from 44.77358 to 44.73106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 43.2575 - MinusLogProbMetric: 43.2575 - val_loss: 44.7311 - val_MinusLogProbMetric: 44.7311 - lr: 6.1728e-06 - 37s/epoch - 186ms/step
Epoch 523/1000
2023-10-27 12:28:49.096 
Epoch 523/1000 
	 loss: 43.3324, MinusLogProbMetric: 43.3324, val_loss: 45.1656, val_MinusLogProbMetric: 45.1656

Epoch 523: val_loss did not improve from 44.73106
196/196 - 40s - loss: 43.3324 - MinusLogProbMetric: 43.3324 - val_loss: 45.1656 - val_MinusLogProbMetric: 45.1656 - lr: 6.1728e-06 - 40s/epoch - 204ms/step
Epoch 524/1000
2023-10-27 12:29:25.764 
Epoch 524/1000 
	 loss: 43.3168, MinusLogProbMetric: 43.3168, val_loss: 45.2676, val_MinusLogProbMetric: 45.2676

Epoch 524: val_loss did not improve from 44.73106
196/196 - 37s - loss: 43.3168 - MinusLogProbMetric: 43.3168 - val_loss: 45.2676 - val_MinusLogProbMetric: 45.2676 - lr: 6.1728e-06 - 37s/epoch - 187ms/step
Epoch 525/1000
2023-10-27 12:30:01.640 
Epoch 525/1000 
	 loss: 43.3795, MinusLogProbMetric: 43.3795, val_loss: 45.0696, val_MinusLogProbMetric: 45.0696

Epoch 525: val_loss did not improve from 44.73106
196/196 - 36s - loss: 43.3795 - MinusLogProbMetric: 43.3795 - val_loss: 45.0696 - val_MinusLogProbMetric: 45.0696 - lr: 6.1728e-06 - 36s/epoch - 183ms/step
Epoch 526/1000
2023-10-27 12:30:38.263 
Epoch 526/1000 
	 loss: 43.3486, MinusLogProbMetric: 43.3486, val_loss: 44.9853, val_MinusLogProbMetric: 44.9853

Epoch 526: val_loss did not improve from 44.73106
196/196 - 37s - loss: 43.3486 - MinusLogProbMetric: 43.3486 - val_loss: 44.9853 - val_MinusLogProbMetric: 44.9853 - lr: 6.1728e-06 - 37s/epoch - 187ms/step
Epoch 527/1000
2023-10-27 12:31:15.767 
Epoch 527/1000 
	 loss: 43.2651, MinusLogProbMetric: 43.2651, val_loss: 45.2244, val_MinusLogProbMetric: 45.2244

Epoch 527: val_loss did not improve from 44.73106
196/196 - 38s - loss: 43.2651 - MinusLogProbMetric: 43.2651 - val_loss: 45.2244 - val_MinusLogProbMetric: 45.2244 - lr: 6.1728e-06 - 38s/epoch - 191ms/step
Epoch 528/1000
2023-10-27 12:31:54.745 
Epoch 528/1000 
	 loss: 43.2801, MinusLogProbMetric: 43.2801, val_loss: 45.0348, val_MinusLogProbMetric: 45.0348

Epoch 528: val_loss did not improve from 44.73106
196/196 - 39s - loss: 43.2801 - MinusLogProbMetric: 43.2801 - val_loss: 45.0348 - val_MinusLogProbMetric: 45.0348 - lr: 6.1728e-06 - 39s/epoch - 199ms/step
Epoch 529/1000
2023-10-27 12:32:33.431 
Epoch 529/1000 
	 loss: 43.2663, MinusLogProbMetric: 43.2663, val_loss: 44.9991, val_MinusLogProbMetric: 44.9991

Epoch 529: val_loss did not improve from 44.73106
196/196 - 39s - loss: 43.2663 - MinusLogProbMetric: 43.2663 - val_loss: 44.9991 - val_MinusLogProbMetric: 44.9991 - lr: 6.1728e-06 - 39s/epoch - 197ms/step
Epoch 530/1000
2023-10-27 12:33:09.466 
Epoch 530/1000 
	 loss: 43.2318, MinusLogProbMetric: 43.2318, val_loss: 45.1350, val_MinusLogProbMetric: 45.1350

Epoch 530: val_loss did not improve from 44.73106
196/196 - 36s - loss: 43.2318 - MinusLogProbMetric: 43.2318 - val_loss: 45.1350 - val_MinusLogProbMetric: 45.1350 - lr: 6.1728e-06 - 36s/epoch - 184ms/step
Epoch 531/1000
2023-10-27 12:33:45.525 
Epoch 531/1000 
	 loss: 43.2660, MinusLogProbMetric: 43.2660, val_loss: 44.9524, val_MinusLogProbMetric: 44.9524

Epoch 531: val_loss did not improve from 44.73106
196/196 - 36s - loss: 43.2660 - MinusLogProbMetric: 43.2660 - val_loss: 44.9524 - val_MinusLogProbMetric: 44.9524 - lr: 6.1728e-06 - 36s/epoch - 184ms/step
Epoch 532/1000
2023-10-27 12:34:24.332 
Epoch 532/1000 
	 loss: 43.3126, MinusLogProbMetric: 43.3126, val_loss: 44.8589, val_MinusLogProbMetric: 44.8589

Epoch 532: val_loss did not improve from 44.73106
196/196 - 39s - loss: 43.3126 - MinusLogProbMetric: 43.3126 - val_loss: 44.8589 - val_MinusLogProbMetric: 44.8589 - lr: 6.1728e-06 - 39s/epoch - 198ms/step
Epoch 533/1000
2023-10-27 12:35:02.258 
Epoch 533/1000 
	 loss: 43.2557, MinusLogProbMetric: 43.2557, val_loss: 45.2495, val_MinusLogProbMetric: 45.2495

Epoch 533: val_loss did not improve from 44.73106
196/196 - 38s - loss: 43.2557 - MinusLogProbMetric: 43.2557 - val_loss: 45.2495 - val_MinusLogProbMetric: 45.2495 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 534/1000
2023-10-27 12:35:38.853 
Epoch 534/1000 
	 loss: 43.2774, MinusLogProbMetric: 43.2774, val_loss: 44.8117, val_MinusLogProbMetric: 44.8117

Epoch 534: val_loss did not improve from 44.73106
196/196 - 37s - loss: 43.2774 - MinusLogProbMetric: 43.2774 - val_loss: 44.8117 - val_MinusLogProbMetric: 44.8117 - lr: 6.1728e-06 - 37s/epoch - 187ms/step
Epoch 535/1000
2023-10-27 12:36:17.716 
Epoch 535/1000 
	 loss: 43.2889, MinusLogProbMetric: 43.2889, val_loss: 44.9405, val_MinusLogProbMetric: 44.9405

Epoch 535: val_loss did not improve from 44.73106
196/196 - 39s - loss: 43.2889 - MinusLogProbMetric: 43.2889 - val_loss: 44.9405 - val_MinusLogProbMetric: 44.9405 - lr: 6.1728e-06 - 39s/epoch - 198ms/step
Epoch 536/1000
2023-10-27 12:36:56.116 
Epoch 536/1000 
	 loss: 43.2700, MinusLogProbMetric: 43.2700, val_loss: 44.7948, val_MinusLogProbMetric: 44.7948

Epoch 536: val_loss did not improve from 44.73106
196/196 - 38s - loss: 43.2700 - MinusLogProbMetric: 43.2700 - val_loss: 44.7948 - val_MinusLogProbMetric: 44.7948 - lr: 6.1728e-06 - 38s/epoch - 196ms/step
Epoch 537/1000
2023-10-27 12:37:34.551 
Epoch 537/1000 
	 loss: 43.2551, MinusLogProbMetric: 43.2551, val_loss: 45.5968, val_MinusLogProbMetric: 45.5968

Epoch 537: val_loss did not improve from 44.73106
196/196 - 38s - loss: 43.2551 - MinusLogProbMetric: 43.2551 - val_loss: 45.5968 - val_MinusLogProbMetric: 45.5968 - lr: 6.1728e-06 - 38s/epoch - 196ms/step
Epoch 538/1000
2023-10-27 12:38:15.187 
Epoch 538/1000 
	 loss: 43.2547, MinusLogProbMetric: 43.2547, val_loss: 44.7697, val_MinusLogProbMetric: 44.7697

Epoch 538: val_loss did not improve from 44.73106
196/196 - 41s - loss: 43.2547 - MinusLogProbMetric: 43.2547 - val_loss: 44.7697 - val_MinusLogProbMetric: 44.7697 - lr: 6.1728e-06 - 41s/epoch - 207ms/step
Epoch 539/1000
2023-10-27 12:38:50.818 
Epoch 539/1000 
	 loss: 43.2939, MinusLogProbMetric: 43.2939, val_loss: 44.9576, val_MinusLogProbMetric: 44.9576

Epoch 539: val_loss did not improve from 44.73106
196/196 - 36s - loss: 43.2939 - MinusLogProbMetric: 43.2939 - val_loss: 44.9576 - val_MinusLogProbMetric: 44.9576 - lr: 6.1728e-06 - 36s/epoch - 182ms/step
Epoch 540/1000
2023-10-27 12:39:28.128 
Epoch 540/1000 
	 loss: 43.2101, MinusLogProbMetric: 43.2101, val_loss: 45.0927, val_MinusLogProbMetric: 45.0927

Epoch 540: val_loss did not improve from 44.73106
196/196 - 37s - loss: 43.2101 - MinusLogProbMetric: 43.2101 - val_loss: 45.0927 - val_MinusLogProbMetric: 45.0927 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 541/1000
2023-10-27 12:40:04.421 
Epoch 541/1000 
	 loss: 43.2169, MinusLogProbMetric: 43.2169, val_loss: 45.2476, val_MinusLogProbMetric: 45.2476

Epoch 541: val_loss did not improve from 44.73106
196/196 - 36s - loss: 43.2169 - MinusLogProbMetric: 43.2169 - val_loss: 45.2476 - val_MinusLogProbMetric: 45.2476 - lr: 6.1728e-06 - 36s/epoch - 185ms/step
Epoch 542/1000
2023-10-27 12:40:44.032 
Epoch 542/1000 
	 loss: 43.2530, MinusLogProbMetric: 43.2530, val_loss: 45.0162, val_MinusLogProbMetric: 45.0162

Epoch 542: val_loss did not improve from 44.73106
196/196 - 40s - loss: 43.2530 - MinusLogProbMetric: 43.2530 - val_loss: 45.0162 - val_MinusLogProbMetric: 45.0162 - lr: 6.1728e-06 - 40s/epoch - 202ms/step
Epoch 543/1000
2023-10-27 12:41:21.843 
Epoch 543/1000 
	 loss: 43.3393, MinusLogProbMetric: 43.3393, val_loss: 45.3858, val_MinusLogProbMetric: 45.3858

Epoch 543: val_loss did not improve from 44.73106
196/196 - 38s - loss: 43.3393 - MinusLogProbMetric: 43.3393 - val_loss: 45.3858 - val_MinusLogProbMetric: 45.3858 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 544/1000
2023-10-27 12:41:58.733 
Epoch 544/1000 
	 loss: 43.3069, MinusLogProbMetric: 43.3069, val_loss: 44.9626, val_MinusLogProbMetric: 44.9626

Epoch 544: val_loss did not improve from 44.73106
196/196 - 37s - loss: 43.3069 - MinusLogProbMetric: 43.3069 - val_loss: 44.9626 - val_MinusLogProbMetric: 44.9626 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 545/1000
2023-10-27 12:42:35.140 
Epoch 545/1000 
	 loss: 43.2214, MinusLogProbMetric: 43.2214, val_loss: 45.0010, val_MinusLogProbMetric: 45.0010

Epoch 545: val_loss did not improve from 44.73106
196/196 - 36s - loss: 43.2214 - MinusLogProbMetric: 43.2214 - val_loss: 45.0010 - val_MinusLogProbMetric: 45.0010 - lr: 6.1728e-06 - 36s/epoch - 186ms/step
Epoch 546/1000
2023-10-27 12:43:11.971 
Epoch 546/1000 
	 loss: 43.3649, MinusLogProbMetric: 43.3649, val_loss: 44.7926, val_MinusLogProbMetric: 44.7926

Epoch 546: val_loss did not improve from 44.73106
196/196 - 37s - loss: 43.3649 - MinusLogProbMetric: 43.3649 - val_loss: 44.7926 - val_MinusLogProbMetric: 44.7926 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 547/1000
2023-10-27 12:43:50.444 
Epoch 547/1000 
	 loss: 43.1897, MinusLogProbMetric: 43.1897, val_loss: 44.9553, val_MinusLogProbMetric: 44.9553

Epoch 547: val_loss did not improve from 44.73106
196/196 - 38s - loss: 43.1897 - MinusLogProbMetric: 43.1897 - val_loss: 44.9553 - val_MinusLogProbMetric: 44.9553 - lr: 6.1728e-06 - 38s/epoch - 196ms/step
Epoch 548/1000
2023-10-27 12:44:27.564 
Epoch 548/1000 
	 loss: 43.1554, MinusLogProbMetric: 43.1554, val_loss: 45.2673, val_MinusLogProbMetric: 45.2673

Epoch 548: val_loss did not improve from 44.73106
196/196 - 37s - loss: 43.1554 - MinusLogProbMetric: 43.1554 - val_loss: 45.2673 - val_MinusLogProbMetric: 45.2673 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 549/1000
2023-10-27 12:45:06.885 
Epoch 549/1000 
	 loss: 43.2869, MinusLogProbMetric: 43.2869, val_loss: 45.0833, val_MinusLogProbMetric: 45.0833

Epoch 549: val_loss did not improve from 44.73106
196/196 - 39s - loss: 43.2869 - MinusLogProbMetric: 43.2869 - val_loss: 45.0833 - val_MinusLogProbMetric: 45.0833 - lr: 6.1728e-06 - 39s/epoch - 201ms/step
Epoch 550/1000
2023-10-27 12:45:43.636 
Epoch 550/1000 
	 loss: 43.2076, MinusLogProbMetric: 43.2076, val_loss: 44.6864, val_MinusLogProbMetric: 44.6864

Epoch 550: val_loss improved from 44.73106 to 44.68642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 38s - loss: 43.2076 - MinusLogProbMetric: 43.2076 - val_loss: 44.6864 - val_MinusLogProbMetric: 44.6864 - lr: 6.1728e-06 - 38s/epoch - 192ms/step
Epoch 551/1000
2023-10-27 12:46:20.902 
Epoch 551/1000 
	 loss: 43.1990, MinusLogProbMetric: 43.1990, val_loss: 45.1290, val_MinusLogProbMetric: 45.1290

Epoch 551: val_loss did not improve from 44.68642
196/196 - 36s - loss: 43.1990 - MinusLogProbMetric: 43.1990 - val_loss: 45.1290 - val_MinusLogProbMetric: 45.1290 - lr: 6.1728e-06 - 36s/epoch - 186ms/step
Epoch 552/1000
2023-10-27 12:46:57.980 
Epoch 552/1000 
	 loss: 43.2186, MinusLogProbMetric: 43.2186, val_loss: 45.2780, val_MinusLogProbMetric: 45.2780

Epoch 552: val_loss did not improve from 44.68642
196/196 - 37s - loss: 43.2186 - MinusLogProbMetric: 43.2186 - val_loss: 45.2780 - val_MinusLogProbMetric: 45.2780 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 553/1000
2023-10-27 12:47:38.397 
Epoch 553/1000 
	 loss: 43.1954, MinusLogProbMetric: 43.1954, val_loss: 44.9183, val_MinusLogProbMetric: 44.9183

Epoch 553: val_loss did not improve from 44.68642
196/196 - 40s - loss: 43.1954 - MinusLogProbMetric: 43.1954 - val_loss: 44.9183 - val_MinusLogProbMetric: 44.9183 - lr: 6.1728e-06 - 40s/epoch - 207ms/step
Epoch 554/1000
2023-10-27 12:48:18.719 
Epoch 554/1000 
	 loss: 43.1939, MinusLogProbMetric: 43.1939, val_loss: 44.8091, val_MinusLogProbMetric: 44.8091

Epoch 554: val_loss did not improve from 44.68642
196/196 - 40s - loss: 43.1939 - MinusLogProbMetric: 43.1939 - val_loss: 44.8091 - val_MinusLogProbMetric: 44.8091 - lr: 6.1728e-06 - 40s/epoch - 205ms/step
Epoch 555/1000
2023-10-27 12:48:57.897 
Epoch 555/1000 
	 loss: 43.3327, MinusLogProbMetric: 43.3327, val_loss: 45.3907, val_MinusLogProbMetric: 45.3907

Epoch 555: val_loss did not improve from 44.68642
196/196 - 39s - loss: 43.3327 - MinusLogProbMetric: 43.3327 - val_loss: 45.3907 - val_MinusLogProbMetric: 45.3907 - lr: 6.1728e-06 - 39s/epoch - 200ms/step
Epoch 556/1000
2023-10-27 12:49:36.780 
Epoch 556/1000 
	 loss: 43.1984, MinusLogProbMetric: 43.1984, val_loss: 45.2198, val_MinusLogProbMetric: 45.2198

Epoch 556: val_loss did not improve from 44.68642
196/196 - 39s - loss: 43.1984 - MinusLogProbMetric: 43.1984 - val_loss: 45.2198 - val_MinusLogProbMetric: 45.2198 - lr: 6.1728e-06 - 39s/epoch - 198ms/step
Epoch 557/1000
2023-10-27 12:50:17.137 
Epoch 557/1000 
	 loss: 43.2956, MinusLogProbMetric: 43.2956, val_loss: 44.9337, val_MinusLogProbMetric: 44.9337

Epoch 557: val_loss did not improve from 44.68642
196/196 - 40s - loss: 43.2956 - MinusLogProbMetric: 43.2956 - val_loss: 44.9337 - val_MinusLogProbMetric: 44.9337 - lr: 6.1728e-06 - 40s/epoch - 206ms/step
Epoch 558/1000
2023-10-27 12:50:58.551 
Epoch 558/1000 
	 loss: 43.1480, MinusLogProbMetric: 43.1480, val_loss: 44.7429, val_MinusLogProbMetric: 44.7429

Epoch 558: val_loss did not improve from 44.68642
196/196 - 41s - loss: 43.1480 - MinusLogProbMetric: 43.1480 - val_loss: 44.7429 - val_MinusLogProbMetric: 44.7429 - lr: 6.1728e-06 - 41s/epoch - 211ms/step
Epoch 559/1000
2023-10-27 12:51:41.295 
Epoch 559/1000 
	 loss: 43.1777, MinusLogProbMetric: 43.1777, val_loss: 45.0323, val_MinusLogProbMetric: 45.0323

Epoch 559: val_loss did not improve from 44.68642
196/196 - 43s - loss: 43.1777 - MinusLogProbMetric: 43.1777 - val_loss: 45.0323 - val_MinusLogProbMetric: 45.0323 - lr: 6.1728e-06 - 43s/epoch - 218ms/step
Epoch 560/1000
2023-10-27 12:52:22.066 
Epoch 560/1000 
	 loss: 43.1500, MinusLogProbMetric: 43.1500, val_loss: 45.6793, val_MinusLogProbMetric: 45.6793

Epoch 560: val_loss did not improve from 44.68642
196/196 - 41s - loss: 43.1500 - MinusLogProbMetric: 43.1500 - val_loss: 45.6793 - val_MinusLogProbMetric: 45.6793 - lr: 6.1728e-06 - 41s/epoch - 208ms/step
Epoch 561/1000
2023-10-27 12:53:05.052 
Epoch 561/1000 
	 loss: 43.2183, MinusLogProbMetric: 43.2183, val_loss: 45.0350, val_MinusLogProbMetric: 45.0350

Epoch 561: val_loss did not improve from 44.68642
196/196 - 43s - loss: 43.2183 - MinusLogProbMetric: 43.2183 - val_loss: 45.0350 - val_MinusLogProbMetric: 45.0350 - lr: 6.1728e-06 - 43s/epoch - 219ms/step
Epoch 562/1000
2023-10-27 12:53:48.630 
Epoch 562/1000 
	 loss: 43.2555, MinusLogProbMetric: 43.2555, val_loss: 45.3044, val_MinusLogProbMetric: 45.3044

Epoch 562: val_loss did not improve from 44.68642
196/196 - 44s - loss: 43.2555 - MinusLogProbMetric: 43.2555 - val_loss: 45.3044 - val_MinusLogProbMetric: 45.3044 - lr: 6.1728e-06 - 44s/epoch - 222ms/step
Epoch 563/1000
2023-10-27 12:54:31.944 
Epoch 563/1000 
	 loss: 43.1497, MinusLogProbMetric: 43.1497, val_loss: 44.8422, val_MinusLogProbMetric: 44.8422

Epoch 563: val_loss did not improve from 44.68642
196/196 - 43s - loss: 43.1497 - MinusLogProbMetric: 43.1497 - val_loss: 44.8422 - val_MinusLogProbMetric: 44.8422 - lr: 6.1728e-06 - 43s/epoch - 221ms/step
Epoch 564/1000
2023-10-27 12:55:15.349 
Epoch 564/1000 
	 loss: 43.1729, MinusLogProbMetric: 43.1729, val_loss: 44.7370, val_MinusLogProbMetric: 44.7370

Epoch 564: val_loss did not improve from 44.68642
196/196 - 43s - loss: 43.1729 - MinusLogProbMetric: 43.1729 - val_loss: 44.7370 - val_MinusLogProbMetric: 44.7370 - lr: 6.1728e-06 - 43s/epoch - 221ms/step
Epoch 565/1000
2023-10-27 12:55:55.011 
Epoch 565/1000 
	 loss: 43.1772, MinusLogProbMetric: 43.1772, val_loss: 44.9739, val_MinusLogProbMetric: 44.9739

Epoch 565: val_loss did not improve from 44.68642
196/196 - 40s - loss: 43.1772 - MinusLogProbMetric: 43.1772 - val_loss: 44.9739 - val_MinusLogProbMetric: 44.9739 - lr: 6.1728e-06 - 40s/epoch - 202ms/step
Epoch 566/1000
2023-10-27 12:56:30.882 
Epoch 566/1000 
	 loss: 43.1489, MinusLogProbMetric: 43.1489, val_loss: 45.1120, val_MinusLogProbMetric: 45.1120

Epoch 566: val_loss did not improve from 44.68642
196/196 - 36s - loss: 43.1489 - MinusLogProbMetric: 43.1489 - val_loss: 45.1120 - val_MinusLogProbMetric: 45.1120 - lr: 6.1728e-06 - 36s/epoch - 183ms/step
Epoch 567/1000
2023-10-27 12:57:08.410 
Epoch 567/1000 
	 loss: 43.1162, MinusLogProbMetric: 43.1162, val_loss: 45.2191, val_MinusLogProbMetric: 45.2191

Epoch 567: val_loss did not improve from 44.68642
196/196 - 38s - loss: 43.1162 - MinusLogProbMetric: 43.1162 - val_loss: 45.2191 - val_MinusLogProbMetric: 45.2191 - lr: 6.1728e-06 - 38s/epoch - 191ms/step
Epoch 568/1000
2023-10-27 12:57:43.333 
Epoch 568/1000 
	 loss: 43.1973, MinusLogProbMetric: 43.1973, val_loss: 45.1567, val_MinusLogProbMetric: 45.1567

Epoch 568: val_loss did not improve from 44.68642
196/196 - 35s - loss: 43.1973 - MinusLogProbMetric: 43.1973 - val_loss: 45.1567 - val_MinusLogProbMetric: 45.1567 - lr: 6.1728e-06 - 35s/epoch - 178ms/step
Epoch 569/1000
2023-10-27 12:58:18.367 
Epoch 569/1000 
	 loss: 43.1609, MinusLogProbMetric: 43.1609, val_loss: 45.2114, val_MinusLogProbMetric: 45.2114

Epoch 569: val_loss did not improve from 44.68642
196/196 - 35s - loss: 43.1609 - MinusLogProbMetric: 43.1609 - val_loss: 45.2114 - val_MinusLogProbMetric: 45.2114 - lr: 6.1728e-06 - 35s/epoch - 179ms/step
Epoch 570/1000
2023-10-27 12:58:53.244 
Epoch 570/1000 
	 loss: 43.1815, MinusLogProbMetric: 43.1815, val_loss: 44.7632, val_MinusLogProbMetric: 44.7632

Epoch 570: val_loss did not improve from 44.68642
196/196 - 35s - loss: 43.1815 - MinusLogProbMetric: 43.1815 - val_loss: 44.7632 - val_MinusLogProbMetric: 44.7632 - lr: 6.1728e-06 - 35s/epoch - 178ms/step
Epoch 571/1000
2023-10-27 12:59:32.277 
Epoch 571/1000 
	 loss: 43.2261, MinusLogProbMetric: 43.2261, val_loss: 44.9148, val_MinusLogProbMetric: 44.9148

Epoch 571: val_loss did not improve from 44.68642
196/196 - 39s - loss: 43.2261 - MinusLogProbMetric: 43.2261 - val_loss: 44.9148 - val_MinusLogProbMetric: 44.9148 - lr: 6.1728e-06 - 39s/epoch - 199ms/step
Epoch 572/1000
2023-10-27 13:00:12.571 
Epoch 572/1000 
	 loss: 43.1056, MinusLogProbMetric: 43.1056, val_loss: 45.1434, val_MinusLogProbMetric: 45.1434

Epoch 572: val_loss did not improve from 44.68642
196/196 - 40s - loss: 43.1056 - MinusLogProbMetric: 43.1056 - val_loss: 45.1434 - val_MinusLogProbMetric: 45.1434 - lr: 6.1728e-06 - 40s/epoch - 206ms/step
Epoch 573/1000
2023-10-27 13:00:49.607 
Epoch 573/1000 
	 loss: 43.1873, MinusLogProbMetric: 43.1873, val_loss: 45.5103, val_MinusLogProbMetric: 45.5103

Epoch 573: val_loss did not improve from 44.68642
196/196 - 37s - loss: 43.1873 - MinusLogProbMetric: 43.1873 - val_loss: 45.5103 - val_MinusLogProbMetric: 45.5103 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 574/1000
2023-10-27 13:01:28.211 
Epoch 574/1000 
	 loss: 43.2147, MinusLogProbMetric: 43.2147, val_loss: 44.7503, val_MinusLogProbMetric: 44.7503

Epoch 574: val_loss did not improve from 44.68642
196/196 - 39s - loss: 43.2147 - MinusLogProbMetric: 43.2147 - val_loss: 44.7503 - val_MinusLogProbMetric: 44.7503 - lr: 6.1728e-06 - 39s/epoch - 197ms/step
Epoch 575/1000
2023-10-27 13:02:08.361 
Epoch 575/1000 
	 loss: 43.2398, MinusLogProbMetric: 43.2398, val_loss: 45.1944, val_MinusLogProbMetric: 45.1944

Epoch 575: val_loss did not improve from 44.68642
196/196 - 40s - loss: 43.2398 - MinusLogProbMetric: 43.2398 - val_loss: 45.1944 - val_MinusLogProbMetric: 45.1944 - lr: 6.1728e-06 - 40s/epoch - 205ms/step
Epoch 576/1000
2023-10-27 13:02:45.537 
Epoch 576/1000 
	 loss: 43.2193, MinusLogProbMetric: 43.2193, val_loss: 44.8163, val_MinusLogProbMetric: 44.8163

Epoch 576: val_loss did not improve from 44.68642
196/196 - 37s - loss: 43.2193 - MinusLogProbMetric: 43.2193 - val_loss: 44.8163 - val_MinusLogProbMetric: 44.8163 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 577/1000
2023-10-27 13:03:22.441 
Epoch 577/1000 
	 loss: 43.1241, MinusLogProbMetric: 43.1241, val_loss: 45.3951, val_MinusLogProbMetric: 45.3951

Epoch 577: val_loss did not improve from 44.68642
196/196 - 37s - loss: 43.1241 - MinusLogProbMetric: 43.1241 - val_loss: 45.3951 - val_MinusLogProbMetric: 45.3951 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 578/1000
2023-10-27 13:03:59.931 
Epoch 578/1000 
	 loss: 43.1483, MinusLogProbMetric: 43.1483, val_loss: 44.9388, val_MinusLogProbMetric: 44.9388

Epoch 578: val_loss did not improve from 44.68642
196/196 - 37s - loss: 43.1483 - MinusLogProbMetric: 43.1483 - val_loss: 44.9388 - val_MinusLogProbMetric: 44.9388 - lr: 6.1728e-06 - 37s/epoch - 191ms/step
Epoch 579/1000
2023-10-27 13:04:37.998 
Epoch 579/1000 
	 loss: 43.1897, MinusLogProbMetric: 43.1897, val_loss: 44.6325, val_MinusLogProbMetric: 44.6325

Epoch 579: val_loss improved from 44.68642 to 44.63249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 43.1897 - MinusLogProbMetric: 43.1897 - val_loss: 44.6325 - val_MinusLogProbMetric: 44.6325 - lr: 6.1728e-06 - 39s/epoch - 199ms/step
Epoch 580/1000
2023-10-27 13:05:15.884 
Epoch 580/1000 
	 loss: 43.1996, MinusLogProbMetric: 43.1996, val_loss: 45.0928, val_MinusLogProbMetric: 45.0928

Epoch 580: val_loss did not improve from 44.63249
196/196 - 37s - loss: 43.1996 - MinusLogProbMetric: 43.1996 - val_loss: 45.0928 - val_MinusLogProbMetric: 45.0928 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 581/1000
2023-10-27 13:05:53.230 
Epoch 581/1000 
	 loss: 43.2212, MinusLogProbMetric: 43.2212, val_loss: 44.6885, val_MinusLogProbMetric: 44.6885

Epoch 581: val_loss did not improve from 44.63249
196/196 - 37s - loss: 43.2212 - MinusLogProbMetric: 43.2212 - val_loss: 44.6885 - val_MinusLogProbMetric: 44.6885 - lr: 6.1728e-06 - 37s/epoch - 191ms/step
Epoch 582/1000
2023-10-27 13:06:30.834 
Epoch 582/1000 
	 loss: 43.4388, MinusLogProbMetric: 43.4388, val_loss: 45.5808, val_MinusLogProbMetric: 45.5808

Epoch 582: val_loss did not improve from 44.63249
196/196 - 38s - loss: 43.4388 - MinusLogProbMetric: 43.4388 - val_loss: 45.5808 - val_MinusLogProbMetric: 45.5808 - lr: 6.1728e-06 - 38s/epoch - 192ms/step
Epoch 583/1000
2023-10-27 13:07:08.330 
Epoch 583/1000 
	 loss: 43.2337, MinusLogProbMetric: 43.2337, val_loss: 44.9268, val_MinusLogProbMetric: 44.9268

Epoch 583: val_loss did not improve from 44.63249
196/196 - 37s - loss: 43.2337 - MinusLogProbMetric: 43.2337 - val_loss: 44.9268 - val_MinusLogProbMetric: 44.9268 - lr: 6.1728e-06 - 37s/epoch - 191ms/step
Epoch 584/1000
2023-10-27 13:07:45.359 
Epoch 584/1000 
	 loss: 43.1764, MinusLogProbMetric: 43.1764, val_loss: 45.0915, val_MinusLogProbMetric: 45.0915

Epoch 584: val_loss did not improve from 44.63249
196/196 - 37s - loss: 43.1764 - MinusLogProbMetric: 43.1764 - val_loss: 45.0915 - val_MinusLogProbMetric: 45.0915 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 585/1000
2023-10-27 13:08:23.191 
Epoch 585/1000 
	 loss: 43.1760, MinusLogProbMetric: 43.1760, val_loss: 44.6894, val_MinusLogProbMetric: 44.6894

Epoch 585: val_loss did not improve from 44.63249
196/196 - 38s - loss: 43.1760 - MinusLogProbMetric: 43.1760 - val_loss: 44.6894 - val_MinusLogProbMetric: 44.6894 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 586/1000
2023-10-27 13:09:00.914 
Epoch 586/1000 
	 loss: 43.1676, MinusLogProbMetric: 43.1676, val_loss: 44.9730, val_MinusLogProbMetric: 44.9730

Epoch 586: val_loss did not improve from 44.63249
196/196 - 38s - loss: 43.1676 - MinusLogProbMetric: 43.1676 - val_loss: 44.9730 - val_MinusLogProbMetric: 44.9730 - lr: 6.1728e-06 - 38s/epoch - 192ms/step
Epoch 587/1000
2023-10-27 13:09:38.270 
Epoch 587/1000 
	 loss: 43.2972, MinusLogProbMetric: 43.2972, val_loss: 44.8888, val_MinusLogProbMetric: 44.8888

Epoch 587: val_loss did not improve from 44.63249
196/196 - 37s - loss: 43.2972 - MinusLogProbMetric: 43.2972 - val_loss: 44.8888 - val_MinusLogProbMetric: 44.8888 - lr: 6.1728e-06 - 37s/epoch - 191ms/step
Epoch 588/1000
2023-10-27 13:10:15.455 
Epoch 588/1000 
	 loss: 43.1388, MinusLogProbMetric: 43.1388, val_loss: 44.7591, val_MinusLogProbMetric: 44.7591

Epoch 588: val_loss did not improve from 44.63249
196/196 - 37s - loss: 43.1388 - MinusLogProbMetric: 43.1388 - val_loss: 44.7591 - val_MinusLogProbMetric: 44.7591 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 589/1000
2023-10-27 13:10:53.527 
Epoch 589/1000 
	 loss: 43.1210, MinusLogProbMetric: 43.1210, val_loss: 44.5648, val_MinusLogProbMetric: 44.5648

Epoch 589: val_loss improved from 44.63249 to 44.56477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 39s - loss: 43.1210 - MinusLogProbMetric: 43.1210 - val_loss: 44.5648 - val_MinusLogProbMetric: 44.5648 - lr: 6.1728e-06 - 39s/epoch - 200ms/step
Epoch 590/1000
2023-10-27 13:11:32.345 
Epoch 590/1000 
	 loss: 43.1142, MinusLogProbMetric: 43.1142, val_loss: 45.1699, val_MinusLogProbMetric: 45.1699

Epoch 590: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.1142 - MinusLogProbMetric: 43.1142 - val_loss: 45.1699 - val_MinusLogProbMetric: 45.1699 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 591/1000
2023-10-27 13:12:09.961 
Epoch 591/1000 
	 loss: 43.1210, MinusLogProbMetric: 43.1210, val_loss: 44.7741, val_MinusLogProbMetric: 44.7741

Epoch 591: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.1210 - MinusLogProbMetric: 43.1210 - val_loss: 44.7741 - val_MinusLogProbMetric: 44.7741 - lr: 6.1728e-06 - 38s/epoch - 192ms/step
Epoch 592/1000
2023-10-27 13:12:47.952 
Epoch 592/1000 
	 loss: 43.1668, MinusLogProbMetric: 43.1668, val_loss: 45.2674, val_MinusLogProbMetric: 45.2674

Epoch 592: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.1668 - MinusLogProbMetric: 43.1668 - val_loss: 45.2674 - val_MinusLogProbMetric: 45.2674 - lr: 6.1728e-06 - 38s/epoch - 194ms/step
Epoch 593/1000
2023-10-27 13:13:25.366 
Epoch 593/1000 
	 loss: 43.1859, MinusLogProbMetric: 43.1859, val_loss: 45.2008, val_MinusLogProbMetric: 45.2008

Epoch 593: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1859 - MinusLogProbMetric: 43.1859 - val_loss: 45.2008 - val_MinusLogProbMetric: 45.2008 - lr: 6.1728e-06 - 37s/epoch - 191ms/step
Epoch 594/1000
2023-10-27 13:14:02.341 
Epoch 594/1000 
	 loss: 43.3403, MinusLogProbMetric: 43.3403, val_loss: 45.1170, val_MinusLogProbMetric: 45.1170

Epoch 594: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.3403 - MinusLogProbMetric: 43.3403 - val_loss: 45.1170 - val_MinusLogProbMetric: 45.1170 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 595/1000
2023-10-27 13:14:39.524 
Epoch 595/1000 
	 loss: 43.0868, MinusLogProbMetric: 43.0868, val_loss: 44.9558, val_MinusLogProbMetric: 44.9558

Epoch 595: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0868 - MinusLogProbMetric: 43.0868 - val_loss: 44.9558 - val_MinusLogProbMetric: 44.9558 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 596/1000
2023-10-27 13:15:16.919 
Epoch 596/1000 
	 loss: 43.1472, MinusLogProbMetric: 43.1472, val_loss: 44.8786, val_MinusLogProbMetric: 44.8786

Epoch 596: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1472 - MinusLogProbMetric: 43.1472 - val_loss: 44.8786 - val_MinusLogProbMetric: 44.8786 - lr: 6.1728e-06 - 37s/epoch - 191ms/step
Epoch 597/1000
2023-10-27 13:15:54.136 
Epoch 597/1000 
	 loss: 43.1027, MinusLogProbMetric: 43.1027, val_loss: 45.6084, val_MinusLogProbMetric: 45.6084

Epoch 597: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1027 - MinusLogProbMetric: 43.1027 - val_loss: 45.6084 - val_MinusLogProbMetric: 45.6084 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 598/1000
2023-10-27 13:16:31.662 
Epoch 598/1000 
	 loss: 43.1976, MinusLogProbMetric: 43.1976, val_loss: 44.7567, val_MinusLogProbMetric: 44.7567

Epoch 598: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.1976 - MinusLogProbMetric: 43.1976 - val_loss: 44.7567 - val_MinusLogProbMetric: 44.7567 - lr: 6.1728e-06 - 38s/epoch - 191ms/step
Epoch 599/1000
2023-10-27 13:17:09.268 
Epoch 599/1000 
	 loss: 43.1161, MinusLogProbMetric: 43.1161, val_loss: 44.9841, val_MinusLogProbMetric: 44.9841

Epoch 599: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.1161 - MinusLogProbMetric: 43.1161 - val_loss: 44.9841 - val_MinusLogProbMetric: 44.9841 - lr: 6.1728e-06 - 38s/epoch - 192ms/step
Epoch 600/1000
2023-10-27 13:17:46.170 
Epoch 600/1000 
	 loss: 43.0963, MinusLogProbMetric: 43.0963, val_loss: 45.2164, val_MinusLogProbMetric: 45.2164

Epoch 600: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0963 - MinusLogProbMetric: 43.0963 - val_loss: 45.2164 - val_MinusLogProbMetric: 45.2164 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 601/1000
2023-10-27 13:18:23.198 
Epoch 601/1000 
	 loss: 43.1170, MinusLogProbMetric: 43.1170, val_loss: 44.7328, val_MinusLogProbMetric: 44.7328

Epoch 601: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1170 - MinusLogProbMetric: 43.1170 - val_loss: 44.7328 - val_MinusLogProbMetric: 44.7328 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 602/1000
2023-10-27 13:19:00.353 
Epoch 602/1000 
	 loss: 43.0501, MinusLogProbMetric: 43.0501, val_loss: 45.0653, val_MinusLogProbMetric: 45.0653

Epoch 602: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0501 - MinusLogProbMetric: 43.0501 - val_loss: 45.0653 - val_MinusLogProbMetric: 45.0653 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 603/1000
2023-10-27 13:19:40.139 
Epoch 603/1000 
	 loss: 43.0927, MinusLogProbMetric: 43.0927, val_loss: 44.9594, val_MinusLogProbMetric: 44.9594

Epoch 603: val_loss did not improve from 44.56477
196/196 - 40s - loss: 43.0927 - MinusLogProbMetric: 43.0927 - val_loss: 44.9594 - val_MinusLogProbMetric: 44.9594 - lr: 6.1728e-06 - 40s/epoch - 203ms/step
Epoch 604/1000
2023-10-27 13:20:17.241 
Epoch 604/1000 
	 loss: 43.1180, MinusLogProbMetric: 43.1180, val_loss: 44.9977, val_MinusLogProbMetric: 44.9977

Epoch 604: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1180 - MinusLogProbMetric: 43.1180 - val_loss: 44.9977 - val_MinusLogProbMetric: 44.9977 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 605/1000
2023-10-27 13:20:54.393 
Epoch 605/1000 
	 loss: 43.1819, MinusLogProbMetric: 43.1819, val_loss: 45.7559, val_MinusLogProbMetric: 45.7559

Epoch 605: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1819 - MinusLogProbMetric: 43.1819 - val_loss: 45.7559 - val_MinusLogProbMetric: 45.7559 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 606/1000
2023-10-27 13:21:31.811 
Epoch 606/1000 
	 loss: 43.1207, MinusLogProbMetric: 43.1207, val_loss: 44.8172, val_MinusLogProbMetric: 44.8172

Epoch 606: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1207 - MinusLogProbMetric: 43.1207 - val_loss: 44.8172 - val_MinusLogProbMetric: 44.8172 - lr: 6.1728e-06 - 37s/epoch - 191ms/step
Epoch 607/1000
2023-10-27 13:22:09.814 
Epoch 607/1000 
	 loss: 43.0640, MinusLogProbMetric: 43.0640, val_loss: 45.0429, val_MinusLogProbMetric: 45.0429

Epoch 607: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.0640 - MinusLogProbMetric: 43.0640 - val_loss: 45.0429 - val_MinusLogProbMetric: 45.0429 - lr: 6.1728e-06 - 38s/epoch - 194ms/step
Epoch 608/1000
2023-10-27 13:22:48.113 
Epoch 608/1000 
	 loss: 43.1955, MinusLogProbMetric: 43.1955, val_loss: 44.6926, val_MinusLogProbMetric: 44.6926

Epoch 608: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.1955 - MinusLogProbMetric: 43.1955 - val_loss: 44.6926 - val_MinusLogProbMetric: 44.6926 - lr: 6.1728e-06 - 38s/epoch - 195ms/step
Epoch 609/1000
2023-10-27 13:23:25.883 
Epoch 609/1000 
	 loss: 43.0600, MinusLogProbMetric: 43.0600, val_loss: 45.2444, val_MinusLogProbMetric: 45.2444

Epoch 609: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.0600 - MinusLogProbMetric: 43.0600 - val_loss: 45.2444 - val_MinusLogProbMetric: 45.2444 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 610/1000
2023-10-27 13:24:03.626 
Epoch 610/1000 
	 loss: 42.9988, MinusLogProbMetric: 42.9988, val_loss: 44.8446, val_MinusLogProbMetric: 44.8446

Epoch 610: val_loss did not improve from 44.56477
196/196 - 38s - loss: 42.9988 - MinusLogProbMetric: 42.9988 - val_loss: 44.8446 - val_MinusLogProbMetric: 44.8446 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 611/1000
2023-10-27 13:24:41.449 
Epoch 611/1000 
	 loss: 43.0296, MinusLogProbMetric: 43.0296, val_loss: 44.8935, val_MinusLogProbMetric: 44.8935

Epoch 611: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.0296 - MinusLogProbMetric: 43.0296 - val_loss: 44.8935 - val_MinusLogProbMetric: 44.8935 - lr: 6.1728e-06 - 38s/epoch - 193ms/step
Epoch 612/1000
2023-10-27 13:25:18.675 
Epoch 612/1000 
	 loss: 43.1163, MinusLogProbMetric: 43.1163, val_loss: 44.6512, val_MinusLogProbMetric: 44.6512

Epoch 612: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1163 - MinusLogProbMetric: 43.1163 - val_loss: 44.6512 - val_MinusLogProbMetric: 44.6512 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 613/1000
2023-10-27 13:25:56.792 
Epoch 613/1000 
	 loss: 43.1447, MinusLogProbMetric: 43.1447, val_loss: 44.5670, val_MinusLogProbMetric: 44.5670

Epoch 613: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.1447 - MinusLogProbMetric: 43.1447 - val_loss: 44.5670 - val_MinusLogProbMetric: 44.5670 - lr: 6.1728e-06 - 38s/epoch - 194ms/step
Epoch 614/1000
2023-10-27 13:26:37.372 
Epoch 614/1000 
	 loss: 43.0148, MinusLogProbMetric: 43.0148, val_loss: 44.7018, val_MinusLogProbMetric: 44.7018

Epoch 614: val_loss did not improve from 44.56477
196/196 - 41s - loss: 43.0148 - MinusLogProbMetric: 43.0148 - val_loss: 44.7018 - val_MinusLogProbMetric: 44.7018 - lr: 6.1728e-06 - 41s/epoch - 207ms/step
Epoch 615/1000
2023-10-27 13:27:14.479 
Epoch 615/1000 
	 loss: 43.0430, MinusLogProbMetric: 43.0430, val_loss: 44.5868, val_MinusLogProbMetric: 44.5868

Epoch 615: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0430 - MinusLogProbMetric: 43.0430 - val_loss: 44.5868 - val_MinusLogProbMetric: 44.5868 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 616/1000
2023-10-27 13:27:51.442 
Epoch 616/1000 
	 loss: 43.0335, MinusLogProbMetric: 43.0335, val_loss: 44.8315, val_MinusLogProbMetric: 44.8315

Epoch 616: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0335 - MinusLogProbMetric: 43.0335 - val_loss: 44.8315 - val_MinusLogProbMetric: 44.8315 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 617/1000
2023-10-27 13:28:28.606 
Epoch 617/1000 
	 loss: 43.0358, MinusLogProbMetric: 43.0358, val_loss: 44.7318, val_MinusLogProbMetric: 44.7318

Epoch 617: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0358 - MinusLogProbMetric: 43.0358 - val_loss: 44.7318 - val_MinusLogProbMetric: 44.7318 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 618/1000
2023-10-27 13:29:06.910 
Epoch 618/1000 
	 loss: 43.0868, MinusLogProbMetric: 43.0868, val_loss: 44.9708, val_MinusLogProbMetric: 44.9708

Epoch 618: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.0868 - MinusLogProbMetric: 43.0868 - val_loss: 44.9708 - val_MinusLogProbMetric: 44.9708 - lr: 6.1728e-06 - 38s/epoch - 195ms/step
Epoch 619/1000
2023-10-27 13:29:48.555 
Epoch 619/1000 
	 loss: 43.0630, MinusLogProbMetric: 43.0630, val_loss: 44.8933, val_MinusLogProbMetric: 44.8933

Epoch 619: val_loss did not improve from 44.56477
196/196 - 42s - loss: 43.0630 - MinusLogProbMetric: 43.0630 - val_loss: 44.8933 - val_MinusLogProbMetric: 44.8933 - lr: 6.1728e-06 - 42s/epoch - 212ms/step
Epoch 620/1000
2023-10-27 13:30:25.460 
Epoch 620/1000 
	 loss: 43.2726, MinusLogProbMetric: 43.2726, val_loss: 44.7714, val_MinusLogProbMetric: 44.7714

Epoch 620: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.2726 - MinusLogProbMetric: 43.2726 - val_loss: 44.7714 - val_MinusLogProbMetric: 44.7714 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 621/1000
2023-10-27 13:31:02.248 
Epoch 621/1000 
	 loss: 43.1658, MinusLogProbMetric: 43.1658, val_loss: 45.1694, val_MinusLogProbMetric: 45.1694

Epoch 621: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1658 - MinusLogProbMetric: 43.1658 - val_loss: 45.1694 - val_MinusLogProbMetric: 45.1694 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 622/1000
2023-10-27 13:31:41.038 
Epoch 622/1000 
	 loss: 42.9974, MinusLogProbMetric: 42.9974, val_loss: 45.2837, val_MinusLogProbMetric: 45.2837

Epoch 622: val_loss did not improve from 44.56477
196/196 - 39s - loss: 42.9974 - MinusLogProbMetric: 42.9974 - val_loss: 45.2837 - val_MinusLogProbMetric: 45.2837 - lr: 6.1728e-06 - 39s/epoch - 198ms/step
Epoch 623/1000
2023-10-27 13:32:24.885 
Epoch 623/1000 
	 loss: 43.2017, MinusLogProbMetric: 43.2017, val_loss: 44.8149, val_MinusLogProbMetric: 44.8149

Epoch 623: val_loss did not improve from 44.56477
196/196 - 44s - loss: 43.2017 - MinusLogProbMetric: 43.2017 - val_loss: 44.8149 - val_MinusLogProbMetric: 44.8149 - lr: 6.1728e-06 - 44s/epoch - 224ms/step
Epoch 624/1000
2023-10-27 13:33:01.713 
Epoch 624/1000 
	 loss: 43.0116, MinusLogProbMetric: 43.0116, val_loss: 44.7477, val_MinusLogProbMetric: 44.7477

Epoch 624: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0116 - MinusLogProbMetric: 43.0116 - val_loss: 44.7477 - val_MinusLogProbMetric: 44.7477 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 625/1000
2023-10-27 13:33:35.300 
Epoch 625/1000 
	 loss: 43.1859, MinusLogProbMetric: 43.1859, val_loss: 45.4951, val_MinusLogProbMetric: 45.4951

Epoch 625: val_loss did not improve from 44.56477
196/196 - 34s - loss: 43.1859 - MinusLogProbMetric: 43.1859 - val_loss: 45.4951 - val_MinusLogProbMetric: 45.4951 - lr: 6.1728e-06 - 34s/epoch - 171ms/step
Epoch 626/1000
2023-10-27 13:34:09.081 
Epoch 626/1000 
	 loss: 43.1510, MinusLogProbMetric: 43.1510, val_loss: 44.8272, val_MinusLogProbMetric: 44.8272

Epoch 626: val_loss did not improve from 44.56477
196/196 - 34s - loss: 43.1510 - MinusLogProbMetric: 43.1510 - val_loss: 44.8272 - val_MinusLogProbMetric: 44.8272 - lr: 6.1728e-06 - 34s/epoch - 172ms/step
Epoch 627/1000
2023-10-27 13:34:47.105 
Epoch 627/1000 
	 loss: 43.0571, MinusLogProbMetric: 43.0571, val_loss: 46.0919, val_MinusLogProbMetric: 46.0919

Epoch 627: val_loss did not improve from 44.56477
196/196 - 38s - loss: 43.0571 - MinusLogProbMetric: 43.0571 - val_loss: 46.0919 - val_MinusLogProbMetric: 46.0919 - lr: 6.1728e-06 - 38s/epoch - 194ms/step
Epoch 628/1000
2023-10-27 13:35:27.882 
Epoch 628/1000 
	 loss: 43.1096, MinusLogProbMetric: 43.1096, val_loss: 45.5821, val_MinusLogProbMetric: 45.5821

Epoch 628: val_loss did not improve from 44.56477
196/196 - 41s - loss: 43.1096 - MinusLogProbMetric: 43.1096 - val_loss: 45.5821 - val_MinusLogProbMetric: 45.5821 - lr: 6.1728e-06 - 41s/epoch - 208ms/step
Epoch 629/1000
2023-10-27 13:36:04.954 
Epoch 629/1000 
	 loss: 43.0049, MinusLogProbMetric: 43.0049, val_loss: 45.0743, val_MinusLogProbMetric: 45.0743

Epoch 629: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0049 - MinusLogProbMetric: 43.0049 - val_loss: 45.0743 - val_MinusLogProbMetric: 45.0743 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 630/1000
2023-10-27 13:36:42.152 
Epoch 630/1000 
	 loss: 43.1410, MinusLogProbMetric: 43.1410, val_loss: 44.8124, val_MinusLogProbMetric: 44.8124

Epoch 630: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1410 - MinusLogProbMetric: 43.1410 - val_loss: 44.8124 - val_MinusLogProbMetric: 44.8124 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 631/1000
2023-10-27 13:37:18.845 
Epoch 631/1000 
	 loss: 43.0328, MinusLogProbMetric: 43.0328, val_loss: 44.9510, val_MinusLogProbMetric: 44.9510

Epoch 631: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0328 - MinusLogProbMetric: 43.0328 - val_loss: 44.9510 - val_MinusLogProbMetric: 44.9510 - lr: 6.1728e-06 - 37s/epoch - 187ms/step
Epoch 632/1000
2023-10-27 13:38:03.839 
Epoch 632/1000 
	 loss: 43.0095, MinusLogProbMetric: 43.0095, val_loss: 44.8970, val_MinusLogProbMetric: 44.8970

Epoch 632: val_loss did not improve from 44.56477
196/196 - 45s - loss: 43.0095 - MinusLogProbMetric: 43.0095 - val_loss: 44.8970 - val_MinusLogProbMetric: 44.8970 - lr: 6.1728e-06 - 45s/epoch - 230ms/step
Epoch 633/1000
2023-10-27 13:38:44.364 
Epoch 633/1000 
	 loss: 43.0926, MinusLogProbMetric: 43.0926, val_loss: 44.8210, val_MinusLogProbMetric: 44.8210

Epoch 633: val_loss did not improve from 44.56477
196/196 - 41s - loss: 43.0926 - MinusLogProbMetric: 43.0926 - val_loss: 44.8210 - val_MinusLogProbMetric: 44.8210 - lr: 6.1728e-06 - 41s/epoch - 207ms/step
Epoch 634/1000
2023-10-27 13:39:21.527 
Epoch 634/1000 
	 loss: 43.0429, MinusLogProbMetric: 43.0429, val_loss: 44.8373, val_MinusLogProbMetric: 44.8373

Epoch 634: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0429 - MinusLogProbMetric: 43.0429 - val_loss: 44.8373 - val_MinusLogProbMetric: 44.8373 - lr: 6.1728e-06 - 37s/epoch - 190ms/step
Epoch 635/1000
2023-10-27 13:39:57.982 
Epoch 635/1000 
	 loss: 43.0394, MinusLogProbMetric: 43.0394, val_loss: 45.0403, val_MinusLogProbMetric: 45.0403

Epoch 635: val_loss did not improve from 44.56477
196/196 - 36s - loss: 43.0394 - MinusLogProbMetric: 43.0394 - val_loss: 45.0403 - val_MinusLogProbMetric: 45.0403 - lr: 6.1728e-06 - 36s/epoch - 186ms/step
Epoch 636/1000
2023-10-27 13:40:38.273 
Epoch 636/1000 
	 loss: 43.0688, MinusLogProbMetric: 43.0688, val_loss: 45.3321, val_MinusLogProbMetric: 45.3321

Epoch 636: val_loss did not improve from 44.56477
196/196 - 40s - loss: 43.0688 - MinusLogProbMetric: 43.0688 - val_loss: 45.3321 - val_MinusLogProbMetric: 45.3321 - lr: 6.1728e-06 - 40s/epoch - 206ms/step
Epoch 637/1000
2023-10-27 13:41:22.211 
Epoch 637/1000 
	 loss: 43.0816, MinusLogProbMetric: 43.0816, val_loss: 44.6798, val_MinusLogProbMetric: 44.6798

Epoch 637: val_loss did not improve from 44.56477
196/196 - 44s - loss: 43.0816 - MinusLogProbMetric: 43.0816 - val_loss: 44.6798 - val_MinusLogProbMetric: 44.6798 - lr: 6.1728e-06 - 44s/epoch - 224ms/step
Epoch 638/1000
2023-10-27 13:41:59.234 
Epoch 638/1000 
	 loss: 43.0036, MinusLogProbMetric: 43.0036, val_loss: 45.4313, val_MinusLogProbMetric: 45.4313

Epoch 638: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.0036 - MinusLogProbMetric: 43.0036 - val_loss: 45.4313 - val_MinusLogProbMetric: 45.4313 - lr: 6.1728e-06 - 37s/epoch - 189ms/step
Epoch 639/1000
2023-10-27 13:42:36.011 
Epoch 639/1000 
	 loss: 43.1878, MinusLogProbMetric: 43.1878, val_loss: 44.9517, val_MinusLogProbMetric: 44.9517

Epoch 639: val_loss did not improve from 44.56477
196/196 - 37s - loss: 43.1878 - MinusLogProbMetric: 43.1878 - val_loss: 44.9517 - val_MinusLogProbMetric: 44.9517 - lr: 6.1728e-06 - 37s/epoch - 188ms/step
Epoch 640/1000
2023-10-27 13:43:12.337 
Epoch 640/1000 
	 loss: 42.7790, MinusLogProbMetric: 42.7790, val_loss: 44.7671, val_MinusLogProbMetric: 44.7671

Epoch 640: val_loss did not improve from 44.56477
196/196 - 36s - loss: 42.7790 - MinusLogProbMetric: 42.7790 - val_loss: 44.7671 - val_MinusLogProbMetric: 44.7671 - lr: 3.0864e-06 - 36s/epoch - 185ms/step
Epoch 641/1000
2023-10-27 13:43:54.895 
Epoch 641/1000 
	 loss: 42.7789, MinusLogProbMetric: 42.7789, val_loss: 45.0305, val_MinusLogProbMetric: 45.0305

Epoch 641: val_loss did not improve from 44.56477
196/196 - 43s - loss: 42.7789 - MinusLogProbMetric: 42.7789 - val_loss: 45.0305 - val_MinusLogProbMetric: 45.0305 - lr: 3.0864e-06 - 43s/epoch - 217ms/step
Epoch 642/1000
2023-10-27 13:44:35.941 
Epoch 642/1000 
	 loss: 42.7698, MinusLogProbMetric: 42.7698, val_loss: 44.6410, val_MinusLogProbMetric: 44.6410

Epoch 642: val_loss did not improve from 44.56477
196/196 - 41s - loss: 42.7698 - MinusLogProbMetric: 42.7698 - val_loss: 44.6410 - val_MinusLogProbMetric: 44.6410 - lr: 3.0864e-06 - 41s/epoch - 209ms/step
Epoch 643/1000
2023-10-27 13:45:13.457 
Epoch 643/1000 
	 loss: 42.7584, MinusLogProbMetric: 42.7584, val_loss: 45.0689, val_MinusLogProbMetric: 45.0689

Epoch 643: val_loss did not improve from 44.56477
196/196 - 38s - loss: 42.7584 - MinusLogProbMetric: 42.7584 - val_loss: 45.0689 - val_MinusLogProbMetric: 45.0689 - lr: 3.0864e-06 - 38s/epoch - 191ms/step
Epoch 644/1000
2023-10-27 13:45:53.946 
Epoch 644/1000 
	 loss: 42.7484, MinusLogProbMetric: 42.7484, val_loss: 44.5580, val_MinusLogProbMetric: 44.5580

Epoch 644: val_loss improved from 44.56477 to 44.55804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 41s - loss: 42.7484 - MinusLogProbMetric: 42.7484 - val_loss: 44.5580 - val_MinusLogProbMetric: 44.5580 - lr: 3.0864e-06 - 41s/epoch - 210ms/step
Epoch 645/1000
2023-10-27 13:46:31.755 
Epoch 645/1000 
	 loss: 42.7580, MinusLogProbMetric: 42.7580, val_loss: 44.7587, val_MinusLogProbMetric: 44.7587

Epoch 645: val_loss did not improve from 44.55804
196/196 - 37s - loss: 42.7580 - MinusLogProbMetric: 42.7580 - val_loss: 44.7587 - val_MinusLogProbMetric: 44.7587 - lr: 3.0864e-06 - 37s/epoch - 189ms/step
Epoch 646/1000
2023-10-27 13:47:16.068 
Epoch 646/1000 
	 loss: 42.7542, MinusLogProbMetric: 42.7542, val_loss: 44.6728, val_MinusLogProbMetric: 44.6728

Epoch 646: val_loss did not improve from 44.55804
196/196 - 44s - loss: 42.7542 - MinusLogProbMetric: 42.7542 - val_loss: 44.6728 - val_MinusLogProbMetric: 44.6728 - lr: 3.0864e-06 - 44s/epoch - 226ms/step
Epoch 647/1000
2023-10-27 13:47:53.006 
Epoch 647/1000 
	 loss: 42.7431, MinusLogProbMetric: 42.7431, val_loss: 44.7567, val_MinusLogProbMetric: 44.7567

Epoch 647: val_loss did not improve from 44.55804
196/196 - 37s - loss: 42.7431 - MinusLogProbMetric: 42.7431 - val_loss: 44.7567 - val_MinusLogProbMetric: 44.7567 - lr: 3.0864e-06 - 37s/epoch - 188ms/step
Epoch 648/1000
2023-10-27 13:48:37.068 
Epoch 648/1000 
	 loss: 42.7715, MinusLogProbMetric: 42.7715, val_loss: 44.7734, val_MinusLogProbMetric: 44.7734

Epoch 648: val_loss did not improve from 44.55804
196/196 - 44s - loss: 42.7715 - MinusLogProbMetric: 42.7715 - val_loss: 44.7734 - val_MinusLogProbMetric: 44.7734 - lr: 3.0864e-06 - 44s/epoch - 225ms/step
Epoch 649/1000
2023-10-27 13:49:13.117 
Epoch 649/1000 
	 loss: 42.7546, MinusLogProbMetric: 42.7546, val_loss: 44.8523, val_MinusLogProbMetric: 44.8523

Epoch 649: val_loss did not improve from 44.55804
196/196 - 36s - loss: 42.7546 - MinusLogProbMetric: 42.7546 - val_loss: 44.8523 - val_MinusLogProbMetric: 44.8523 - lr: 3.0864e-06 - 36s/epoch - 184ms/step
Epoch 650/1000
2023-10-27 13:49:55.774 
Epoch 650/1000 
	 loss: 42.7782, MinusLogProbMetric: 42.7782, val_loss: 44.8833, val_MinusLogProbMetric: 44.8833

Epoch 650: val_loss did not improve from 44.55804
196/196 - 43s - loss: 42.7782 - MinusLogProbMetric: 42.7782 - val_loss: 44.8833 - val_MinusLogProbMetric: 44.8833 - lr: 3.0864e-06 - 43s/epoch - 218ms/step
Epoch 651/1000
2023-10-27 13:50:32.489 
Epoch 651/1000 
	 loss: 42.7556, MinusLogProbMetric: 42.7556, val_loss: 44.6105, val_MinusLogProbMetric: 44.6105

Epoch 651: val_loss did not improve from 44.55804
196/196 - 37s - loss: 42.7556 - MinusLogProbMetric: 42.7556 - val_loss: 44.6105 - val_MinusLogProbMetric: 44.6105 - lr: 3.0864e-06 - 37s/epoch - 187ms/step
Epoch 652/1000
2023-10-27 13:51:12.964 
Epoch 652/1000 
	 loss: 42.7265, MinusLogProbMetric: 42.7265, val_loss: 44.8299, val_MinusLogProbMetric: 44.8299

Epoch 652: val_loss did not improve from 44.55804
196/196 - 40s - loss: 42.7265 - MinusLogProbMetric: 42.7265 - val_loss: 44.8299 - val_MinusLogProbMetric: 44.8299 - lr: 3.0864e-06 - 40s/epoch - 206ms/step
Epoch 653/1000
2023-10-27 13:51:52.778 
Epoch 653/1000 
	 loss: 42.7558, MinusLogProbMetric: 42.7558, val_loss: 44.6783, val_MinusLogProbMetric: 44.6783

Epoch 653: val_loss did not improve from 44.55804
196/196 - 40s - loss: 42.7558 - MinusLogProbMetric: 42.7558 - val_loss: 44.6783 - val_MinusLogProbMetric: 44.6783 - lr: 3.0864e-06 - 40s/epoch - 203ms/step
Epoch 654/1000
2023-10-27 13:52:33.089 
Epoch 654/1000 
	 loss: 42.7562, MinusLogProbMetric: 42.7562, val_loss: 44.7321, val_MinusLogProbMetric: 44.7321

Epoch 654: val_loss did not improve from 44.55804
196/196 - 40s - loss: 42.7562 - MinusLogProbMetric: 42.7562 - val_loss: 44.7321 - val_MinusLogProbMetric: 44.7321 - lr: 3.0864e-06 - 40s/epoch - 206ms/step
Epoch 655/1000
2023-10-27 13:53:12.839 
Epoch 655/1000 
	 loss: 42.7621, MinusLogProbMetric: 42.7621, val_loss: 44.5284, val_MinusLogProbMetric: 44.5284

Epoch 655: val_loss improved from 44.55804 to 44.52839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 40s - loss: 42.7621 - MinusLogProbMetric: 42.7621 - val_loss: 44.5284 - val_MinusLogProbMetric: 44.5284 - lr: 3.0864e-06 - 40s/epoch - 206ms/step
Epoch 656/1000
2023-10-27 13:53:55.184 
Epoch 656/1000 
	 loss: 42.7539, MinusLogProbMetric: 42.7539, val_loss: 44.8970, val_MinusLogProbMetric: 44.8970

Epoch 656: val_loss did not improve from 44.52839
196/196 - 42s - loss: 42.7539 - MinusLogProbMetric: 42.7539 - val_loss: 44.8970 - val_MinusLogProbMetric: 44.8970 - lr: 3.0864e-06 - 42s/epoch - 213ms/step
Epoch 657/1000
2023-10-27 13:54:34.418 
Epoch 657/1000 
	 loss: 42.7414, MinusLogProbMetric: 42.7414, val_loss: 44.6805, val_MinusLogProbMetric: 44.6805

Epoch 657: val_loss did not improve from 44.52839
196/196 - 39s - loss: 42.7414 - MinusLogProbMetric: 42.7414 - val_loss: 44.6805 - val_MinusLogProbMetric: 44.6805 - lr: 3.0864e-06 - 39s/epoch - 200ms/step
Epoch 658/1000
2023-10-27 13:55:14.186 
Epoch 658/1000 
	 loss: 42.7356, MinusLogProbMetric: 42.7356, val_loss: 44.5710, val_MinusLogProbMetric: 44.5710

Epoch 658: val_loss did not improve from 44.52839
196/196 - 40s - loss: 42.7356 - MinusLogProbMetric: 42.7356 - val_loss: 44.5710 - val_MinusLogProbMetric: 44.5710 - lr: 3.0864e-06 - 40s/epoch - 203ms/step
Epoch 659/1000
2023-10-27 13:55:53.130 
Epoch 659/1000 
	 loss: 42.7582, MinusLogProbMetric: 42.7582, val_loss: 44.7245, val_MinusLogProbMetric: 44.7245

Epoch 659: val_loss did not improve from 44.52839
196/196 - 39s - loss: 42.7582 - MinusLogProbMetric: 42.7582 - val_loss: 44.7245 - val_MinusLogProbMetric: 44.7245 - lr: 3.0864e-06 - 39s/epoch - 199ms/step
Epoch 660/1000
2023-10-27 13:56:30.969 
Epoch 660/1000 
	 loss: 42.7356, MinusLogProbMetric: 42.7356, val_loss: 44.6290, val_MinusLogProbMetric: 44.6290

Epoch 660: val_loss did not improve from 44.52839
196/196 - 38s - loss: 42.7356 - MinusLogProbMetric: 42.7356 - val_loss: 44.6290 - val_MinusLogProbMetric: 44.6290 - lr: 3.0864e-06 - 38s/epoch - 193ms/step
Epoch 661/1000
2023-10-27 13:57:10.051 
Epoch 661/1000 
	 loss: 42.7290, MinusLogProbMetric: 42.7290, val_loss: 44.5021, val_MinusLogProbMetric: 44.5021

Epoch 661: val_loss improved from 44.52839 to 44.50207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 40s - loss: 42.7290 - MinusLogProbMetric: 42.7290 - val_loss: 44.5021 - val_MinusLogProbMetric: 44.5021 - lr: 3.0864e-06 - 40s/epoch - 203ms/step
Epoch 662/1000
2023-10-27 13:57:48.463 
Epoch 662/1000 
	 loss: 42.7615, MinusLogProbMetric: 42.7615, val_loss: 44.9420, val_MinusLogProbMetric: 44.9420

Epoch 662: val_loss did not improve from 44.50207
196/196 - 38s - loss: 42.7615 - MinusLogProbMetric: 42.7615 - val_loss: 44.9420 - val_MinusLogProbMetric: 44.9420 - lr: 3.0864e-06 - 38s/epoch - 192ms/step
Epoch 663/1000
2023-10-27 13:58:28.850 
Epoch 663/1000 
	 loss: 42.7351, MinusLogProbMetric: 42.7351, val_loss: 44.5325, val_MinusLogProbMetric: 44.5325

Epoch 663: val_loss did not improve from 44.50207
196/196 - 40s - loss: 42.7351 - MinusLogProbMetric: 42.7351 - val_loss: 44.5325 - val_MinusLogProbMetric: 44.5325 - lr: 3.0864e-06 - 40s/epoch - 206ms/step
Epoch 664/1000
2023-10-27 13:59:06.357 
Epoch 664/1000 
	 loss: 42.7249, MinusLogProbMetric: 42.7249, val_loss: 44.7123, val_MinusLogProbMetric: 44.7123

Epoch 664: val_loss did not improve from 44.50207
196/196 - 38s - loss: 42.7249 - MinusLogProbMetric: 42.7249 - val_loss: 44.7123 - val_MinusLogProbMetric: 44.7123 - lr: 3.0864e-06 - 38s/epoch - 191ms/step
Epoch 665/1000
2023-10-27 13:59:48.065 
Epoch 665/1000 
	 loss: 42.7433, MinusLogProbMetric: 42.7433, val_loss: 44.8475, val_MinusLogProbMetric: 44.8475

Epoch 665: val_loss did not improve from 44.50207
196/196 - 42s - loss: 42.7433 - MinusLogProbMetric: 42.7433 - val_loss: 44.8475 - val_MinusLogProbMetric: 44.8475 - lr: 3.0864e-06 - 42s/epoch - 213ms/step
Epoch 666/1000
2023-10-27 14:00:24.920 
Epoch 666/1000 
	 loss: 42.7170, MinusLogProbMetric: 42.7170, val_loss: 44.5230, val_MinusLogProbMetric: 44.5230

Epoch 666: val_loss did not improve from 44.50207
196/196 - 37s - loss: 42.7170 - MinusLogProbMetric: 42.7170 - val_loss: 44.5230 - val_MinusLogProbMetric: 44.5230 - lr: 3.0864e-06 - 37s/epoch - 188ms/step
Epoch 667/1000
2023-10-27 14:01:07.688 
Epoch 667/1000 
	 loss: 42.7392, MinusLogProbMetric: 42.7392, val_loss: 44.5899, val_MinusLogProbMetric: 44.5899

Epoch 667: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.7392 - MinusLogProbMetric: 42.7392 - val_loss: 44.5899 - val_MinusLogProbMetric: 44.5899 - lr: 3.0864e-06 - 43s/epoch - 218ms/step
Epoch 668/1000
2023-10-27 14:01:47.399 
Epoch 668/1000 
	 loss: 42.7144, MinusLogProbMetric: 42.7144, val_loss: 44.6203, val_MinusLogProbMetric: 44.6203

Epoch 668: val_loss did not improve from 44.50207
196/196 - 40s - loss: 42.7144 - MinusLogProbMetric: 42.7144 - val_loss: 44.6203 - val_MinusLogProbMetric: 44.6203 - lr: 3.0864e-06 - 40s/epoch - 203ms/step
Epoch 669/1000
2023-10-27 14:02:26.566 
Epoch 669/1000 
	 loss: 42.7156, MinusLogProbMetric: 42.7156, val_loss: 44.8131, val_MinusLogProbMetric: 44.8131

Epoch 669: val_loss did not improve from 44.50207
196/196 - 39s - loss: 42.7156 - MinusLogProbMetric: 42.7156 - val_loss: 44.8131 - val_MinusLogProbMetric: 44.8131 - lr: 3.0864e-06 - 39s/epoch - 200ms/step
Epoch 670/1000
2023-10-27 14:03:11.933 
Epoch 670/1000 
	 loss: 42.7284, MinusLogProbMetric: 42.7284, val_loss: 44.8560, val_MinusLogProbMetric: 44.8560

Epoch 670: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7284 - MinusLogProbMetric: 42.7284 - val_loss: 44.8560 - val_MinusLogProbMetric: 44.8560 - lr: 3.0864e-06 - 45s/epoch - 231ms/step
Epoch 671/1000
2023-10-27 14:03:57.089 
Epoch 671/1000 
	 loss: 42.7379, MinusLogProbMetric: 42.7379, val_loss: 44.9526, val_MinusLogProbMetric: 44.9526

Epoch 671: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7379 - MinusLogProbMetric: 42.7379 - val_loss: 44.9526 - val_MinusLogProbMetric: 44.9526 - lr: 3.0864e-06 - 45s/epoch - 230ms/step
Epoch 672/1000
2023-10-27 14:04:41.705 
Epoch 672/1000 
	 loss: 42.7117, MinusLogProbMetric: 42.7117, val_loss: 45.0728, val_MinusLogProbMetric: 45.0728

Epoch 672: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7117 - MinusLogProbMetric: 42.7117 - val_loss: 45.0728 - val_MinusLogProbMetric: 45.0728 - lr: 3.0864e-06 - 45s/epoch - 228ms/step
Epoch 673/1000
2023-10-27 14:05:26.037 
Epoch 673/1000 
	 loss: 42.7291, MinusLogProbMetric: 42.7291, val_loss: 44.7855, val_MinusLogProbMetric: 44.7855

Epoch 673: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7291 - MinusLogProbMetric: 42.7291 - val_loss: 44.7855 - val_MinusLogProbMetric: 44.7855 - lr: 3.0864e-06 - 44s/epoch - 226ms/step
Epoch 674/1000
2023-10-27 14:06:10.509 
Epoch 674/1000 
	 loss: 42.6992, MinusLogProbMetric: 42.6992, val_loss: 44.6342, val_MinusLogProbMetric: 44.6342

Epoch 674: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6992 - MinusLogProbMetric: 42.6992 - val_loss: 44.6342 - val_MinusLogProbMetric: 44.6342 - lr: 3.0864e-06 - 44s/epoch - 227ms/step
Epoch 675/1000
2023-10-27 14:06:54.567 
Epoch 675/1000 
	 loss: 42.7358, MinusLogProbMetric: 42.7358, val_loss: 44.8900, val_MinusLogProbMetric: 44.8900

Epoch 675: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7358 - MinusLogProbMetric: 42.7358 - val_loss: 44.8900 - val_MinusLogProbMetric: 44.8900 - lr: 3.0864e-06 - 44s/epoch - 225ms/step
Epoch 676/1000
2023-10-27 14:07:39.340 
Epoch 676/1000 
	 loss: 42.7255, MinusLogProbMetric: 42.7255, val_loss: 44.7847, val_MinusLogProbMetric: 44.7847

Epoch 676: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7255 - MinusLogProbMetric: 42.7255 - val_loss: 44.7847 - val_MinusLogProbMetric: 44.7847 - lr: 3.0864e-06 - 45s/epoch - 228ms/step
Epoch 677/1000
2023-10-27 14:08:24.203 
Epoch 677/1000 
	 loss: 42.7160, MinusLogProbMetric: 42.7160, val_loss: 45.0185, val_MinusLogProbMetric: 45.0185

Epoch 677: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7160 - MinusLogProbMetric: 42.7160 - val_loss: 45.0185 - val_MinusLogProbMetric: 45.0185 - lr: 3.0864e-06 - 45s/epoch - 229ms/step
Epoch 678/1000
2023-10-27 14:09:07.841 
Epoch 678/1000 
	 loss: 42.7261, MinusLogProbMetric: 42.7261, val_loss: 44.9117, val_MinusLogProbMetric: 44.9117

Epoch 678: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7261 - MinusLogProbMetric: 42.7261 - val_loss: 44.9117 - val_MinusLogProbMetric: 44.9117 - lr: 3.0864e-06 - 44s/epoch - 223ms/step
Epoch 679/1000
2023-10-27 14:09:50.760 
Epoch 679/1000 
	 loss: 42.7046, MinusLogProbMetric: 42.7046, val_loss: 45.0803, val_MinusLogProbMetric: 45.0803

Epoch 679: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.7046 - MinusLogProbMetric: 42.7046 - val_loss: 45.0803 - val_MinusLogProbMetric: 45.0803 - lr: 3.0864e-06 - 43s/epoch - 219ms/step
Epoch 680/1000
2023-10-27 14:10:34.484 
Epoch 680/1000 
	 loss: 42.7154, MinusLogProbMetric: 42.7154, val_loss: 44.6658, val_MinusLogProbMetric: 44.6658

Epoch 680: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7154 - MinusLogProbMetric: 42.7154 - val_loss: 44.6658 - val_MinusLogProbMetric: 44.6658 - lr: 3.0864e-06 - 44s/epoch - 223ms/step
Epoch 681/1000
2023-10-27 14:11:19.281 
Epoch 681/1000 
	 loss: 42.7396, MinusLogProbMetric: 42.7396, val_loss: 44.6484, val_MinusLogProbMetric: 44.6484

Epoch 681: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7396 - MinusLogProbMetric: 42.7396 - val_loss: 44.6484 - val_MinusLogProbMetric: 44.6484 - lr: 3.0864e-06 - 45s/epoch - 229ms/step
Epoch 682/1000
2023-10-27 14:12:04.411 
Epoch 682/1000 
	 loss: 42.7325, MinusLogProbMetric: 42.7325, val_loss: 45.0249, val_MinusLogProbMetric: 45.0249

Epoch 682: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7325 - MinusLogProbMetric: 42.7325 - val_loss: 45.0249 - val_MinusLogProbMetric: 45.0249 - lr: 3.0864e-06 - 45s/epoch - 230ms/step
Epoch 683/1000
2023-10-27 14:12:48.526 
Epoch 683/1000 
	 loss: 42.6974, MinusLogProbMetric: 42.6974, val_loss: 44.7808, val_MinusLogProbMetric: 44.7808

Epoch 683: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6974 - MinusLogProbMetric: 42.6974 - val_loss: 44.7808 - val_MinusLogProbMetric: 44.7808 - lr: 3.0864e-06 - 44s/epoch - 225ms/step
Epoch 684/1000
2023-10-27 14:13:32.809 
Epoch 684/1000 
	 loss: 42.7092, MinusLogProbMetric: 42.7092, val_loss: 44.6880, val_MinusLogProbMetric: 44.6880

Epoch 684: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7092 - MinusLogProbMetric: 42.7092 - val_loss: 44.6880 - val_MinusLogProbMetric: 44.6880 - lr: 3.0864e-06 - 44s/epoch - 226ms/step
Epoch 685/1000
2023-10-27 14:14:16.927 
Epoch 685/1000 
	 loss: 42.7178, MinusLogProbMetric: 42.7178, val_loss: 44.7068, val_MinusLogProbMetric: 44.7068

Epoch 685: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7178 - MinusLogProbMetric: 42.7178 - val_loss: 44.7068 - val_MinusLogProbMetric: 44.7068 - lr: 3.0864e-06 - 44s/epoch - 225ms/step
Epoch 686/1000
2023-10-27 14:15:00.529 
Epoch 686/1000 
	 loss: 42.7086, MinusLogProbMetric: 42.7086, val_loss: 44.7237, val_MinusLogProbMetric: 44.7237

Epoch 686: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7086 - MinusLogProbMetric: 42.7086 - val_loss: 44.7237 - val_MinusLogProbMetric: 44.7237 - lr: 3.0864e-06 - 44s/epoch - 222ms/step
Epoch 687/1000
2023-10-27 14:15:44.434 
Epoch 687/1000 
	 loss: 42.7181, MinusLogProbMetric: 42.7181, val_loss: 45.0882, val_MinusLogProbMetric: 45.0882

Epoch 687: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7181 - MinusLogProbMetric: 42.7181 - val_loss: 45.0882 - val_MinusLogProbMetric: 45.0882 - lr: 3.0864e-06 - 44s/epoch - 224ms/step
Epoch 688/1000
2023-10-27 14:16:28.171 
Epoch 688/1000 
	 loss: 42.7064, MinusLogProbMetric: 42.7064, val_loss: 44.9528, val_MinusLogProbMetric: 44.9528

Epoch 688: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7064 - MinusLogProbMetric: 42.7064 - val_loss: 44.9528 - val_MinusLogProbMetric: 44.9528 - lr: 3.0864e-06 - 44s/epoch - 223ms/step
Epoch 689/1000
2023-10-27 14:17:12.407 
Epoch 689/1000 
	 loss: 42.6937, MinusLogProbMetric: 42.6937, val_loss: 44.5865, val_MinusLogProbMetric: 44.5865

Epoch 689: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6937 - MinusLogProbMetric: 42.6937 - val_loss: 44.5865 - val_MinusLogProbMetric: 44.5865 - lr: 3.0864e-06 - 44s/epoch - 226ms/step
Epoch 690/1000
2023-10-27 14:17:56.337 
Epoch 690/1000 
	 loss: 42.7207, MinusLogProbMetric: 42.7207, val_loss: 44.9457, val_MinusLogProbMetric: 44.9457

Epoch 690: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7207 - MinusLogProbMetric: 42.7207 - val_loss: 44.9457 - val_MinusLogProbMetric: 44.9457 - lr: 3.0864e-06 - 44s/epoch - 224ms/step
Epoch 691/1000
2023-10-27 14:18:40.994 
Epoch 691/1000 
	 loss: 42.7111, MinusLogProbMetric: 42.7111, val_loss: 45.1358, val_MinusLogProbMetric: 45.1358

Epoch 691: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7111 - MinusLogProbMetric: 42.7111 - val_loss: 45.1358 - val_MinusLogProbMetric: 45.1358 - lr: 3.0864e-06 - 45s/epoch - 228ms/step
Epoch 692/1000
2023-10-27 14:19:25.069 
Epoch 692/1000 
	 loss: 42.7008, MinusLogProbMetric: 42.7008, val_loss: 44.9191, val_MinusLogProbMetric: 44.9191

Epoch 692: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7008 - MinusLogProbMetric: 42.7008 - val_loss: 44.9191 - val_MinusLogProbMetric: 44.9191 - lr: 3.0864e-06 - 44s/epoch - 225ms/step
Epoch 693/1000
2023-10-27 14:20:09.759 
Epoch 693/1000 
	 loss: 42.7327, MinusLogProbMetric: 42.7327, val_loss: 44.6746, val_MinusLogProbMetric: 44.6746

Epoch 693: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.7327 - MinusLogProbMetric: 42.7327 - val_loss: 44.6746 - val_MinusLogProbMetric: 44.6746 - lr: 3.0864e-06 - 45s/epoch - 228ms/step
Epoch 694/1000
2023-10-27 14:20:54.391 
Epoch 694/1000 
	 loss: 42.6949, MinusLogProbMetric: 42.6949, val_loss: 44.9530, val_MinusLogProbMetric: 44.9530

Epoch 694: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.6949 - MinusLogProbMetric: 42.6949 - val_loss: 44.9530 - val_MinusLogProbMetric: 44.9530 - lr: 3.0864e-06 - 45s/epoch - 228ms/step
Epoch 695/1000
2023-10-27 14:21:38.592 
Epoch 695/1000 
	 loss: 42.6878, MinusLogProbMetric: 42.6878, val_loss: 44.7442, val_MinusLogProbMetric: 44.7442

Epoch 695: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6878 - MinusLogProbMetric: 42.6878 - val_loss: 44.7442 - val_MinusLogProbMetric: 44.7442 - lr: 3.0864e-06 - 44s/epoch - 226ms/step
Epoch 696/1000
2023-10-27 14:22:23.283 
Epoch 696/1000 
	 loss: 42.6894, MinusLogProbMetric: 42.6894, val_loss: 44.7411, val_MinusLogProbMetric: 44.7411

Epoch 696: val_loss did not improve from 44.50207
196/196 - 45s - loss: 42.6894 - MinusLogProbMetric: 42.6894 - val_loss: 44.7411 - val_MinusLogProbMetric: 44.7411 - lr: 3.0864e-06 - 45s/epoch - 228ms/step
Epoch 697/1000
2023-10-27 14:23:06.814 
Epoch 697/1000 
	 loss: 42.6898, MinusLogProbMetric: 42.6898, val_loss: 44.9802, val_MinusLogProbMetric: 44.9802

Epoch 697: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6898 - MinusLogProbMetric: 42.6898 - val_loss: 44.9802 - val_MinusLogProbMetric: 44.9802 - lr: 3.0864e-06 - 44s/epoch - 222ms/step
Epoch 698/1000
2023-10-27 14:23:50.375 
Epoch 698/1000 
	 loss: 42.7194, MinusLogProbMetric: 42.7194, val_loss: 44.8414, val_MinusLogProbMetric: 44.8414

Epoch 698: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7194 - MinusLogProbMetric: 42.7194 - val_loss: 44.8414 - val_MinusLogProbMetric: 44.8414 - lr: 3.0864e-06 - 44s/epoch - 222ms/step
Epoch 699/1000
2023-10-27 14:24:34.102 
Epoch 699/1000 
	 loss: 42.7102, MinusLogProbMetric: 42.7102, val_loss: 44.8485, val_MinusLogProbMetric: 44.8485

Epoch 699: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7102 - MinusLogProbMetric: 42.7102 - val_loss: 44.8485 - val_MinusLogProbMetric: 44.8485 - lr: 3.0864e-06 - 44s/epoch - 223ms/step
Epoch 700/1000
2023-10-27 14:25:17.664 
Epoch 700/1000 
	 loss: 42.7162, MinusLogProbMetric: 42.7162, val_loss: 44.5410, val_MinusLogProbMetric: 44.5410

Epoch 700: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7162 - MinusLogProbMetric: 42.7162 - val_loss: 44.5410 - val_MinusLogProbMetric: 44.5410 - lr: 3.0864e-06 - 44s/epoch - 222ms/step
Epoch 701/1000
2023-10-27 14:26:01.800 
Epoch 701/1000 
	 loss: 42.7005, MinusLogProbMetric: 42.7005, val_loss: 45.1198, val_MinusLogProbMetric: 45.1198

Epoch 701: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7005 - MinusLogProbMetric: 42.7005 - val_loss: 45.1198 - val_MinusLogProbMetric: 45.1198 - lr: 3.0864e-06 - 44s/epoch - 225ms/step
Epoch 702/1000
2023-10-27 14:26:46.068 
Epoch 702/1000 
	 loss: 42.7610, MinusLogProbMetric: 42.7610, val_loss: 44.8963, val_MinusLogProbMetric: 44.8963

Epoch 702: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.7610 - MinusLogProbMetric: 42.7610 - val_loss: 44.8963 - val_MinusLogProbMetric: 44.8963 - lr: 3.0864e-06 - 44s/epoch - 226ms/step
Epoch 703/1000
2023-10-27 14:27:29.212 
Epoch 703/1000 
	 loss: 42.6773, MinusLogProbMetric: 42.6773, val_loss: 44.7855, val_MinusLogProbMetric: 44.7855

Epoch 703: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6773 - MinusLogProbMetric: 42.6773 - val_loss: 44.7855 - val_MinusLogProbMetric: 44.7855 - lr: 3.0864e-06 - 43s/epoch - 220ms/step
Epoch 704/1000
2023-10-27 14:28:13.104 
Epoch 704/1000 
	 loss: 42.6972, MinusLogProbMetric: 42.6972, val_loss: 44.7794, val_MinusLogProbMetric: 44.7794

Epoch 704: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6972 - MinusLogProbMetric: 42.6972 - val_loss: 44.7794 - val_MinusLogProbMetric: 44.7794 - lr: 3.0864e-06 - 44s/epoch - 224ms/step
Epoch 705/1000
2023-10-27 14:28:56.330 
Epoch 705/1000 
	 loss: 42.6791, MinusLogProbMetric: 42.6791, val_loss: 44.5566, val_MinusLogProbMetric: 44.5566

Epoch 705: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6791 - MinusLogProbMetric: 42.6791 - val_loss: 44.5566 - val_MinusLogProbMetric: 44.5566 - lr: 3.0864e-06 - 43s/epoch - 221ms/step
Epoch 706/1000
2023-10-27 14:29:39.951 
Epoch 706/1000 
	 loss: 42.6653, MinusLogProbMetric: 42.6653, val_loss: 44.9922, val_MinusLogProbMetric: 44.9922

Epoch 706: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6653 - MinusLogProbMetric: 42.6653 - val_loss: 44.9922 - val_MinusLogProbMetric: 44.9922 - lr: 3.0864e-06 - 44s/epoch - 223ms/step
Epoch 707/1000
2023-10-27 14:30:23.824 
Epoch 707/1000 
	 loss: 42.6927, MinusLogProbMetric: 42.6927, val_loss: 44.8453, val_MinusLogProbMetric: 44.8453

Epoch 707: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6927 - MinusLogProbMetric: 42.6927 - val_loss: 44.8453 - val_MinusLogProbMetric: 44.8453 - lr: 3.0864e-06 - 44s/epoch - 224ms/step
Epoch 708/1000
2023-10-27 14:31:06.024 
Epoch 708/1000 
	 loss: 42.6998, MinusLogProbMetric: 42.6998, val_loss: 44.7504, val_MinusLogProbMetric: 44.7504

Epoch 708: val_loss did not improve from 44.50207
196/196 - 42s - loss: 42.6998 - MinusLogProbMetric: 42.6998 - val_loss: 44.7504 - val_MinusLogProbMetric: 44.7504 - lr: 3.0864e-06 - 42s/epoch - 215ms/step
Epoch 709/1000
2023-10-27 14:31:49.716 
Epoch 709/1000 
	 loss: 42.6816, MinusLogProbMetric: 42.6816, val_loss: 44.7460, val_MinusLogProbMetric: 44.7460

Epoch 709: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.6816 - MinusLogProbMetric: 42.6816 - val_loss: 44.7460 - val_MinusLogProbMetric: 44.7460 - lr: 3.0864e-06 - 44s/epoch - 223ms/step
Epoch 710/1000
2023-10-27 14:32:32.498 
Epoch 710/1000 
	 loss: 42.6732, MinusLogProbMetric: 42.6732, val_loss: 44.7393, val_MinusLogProbMetric: 44.7393

Epoch 710: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6732 - MinusLogProbMetric: 42.6732 - val_loss: 44.7393 - val_MinusLogProbMetric: 44.7393 - lr: 3.0864e-06 - 43s/epoch - 218ms/step
Epoch 711/1000
2023-10-27 14:33:15.496 
Epoch 711/1000 
	 loss: 42.6889, MinusLogProbMetric: 42.6889, val_loss: 44.7752, val_MinusLogProbMetric: 44.7752

Epoch 711: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6889 - MinusLogProbMetric: 42.6889 - val_loss: 44.7752 - val_MinusLogProbMetric: 44.7752 - lr: 3.0864e-06 - 43s/epoch - 219ms/step
Epoch 712/1000
2023-10-27 14:33:58.459 
Epoch 712/1000 
	 loss: 42.5990, MinusLogProbMetric: 42.5990, val_loss: 44.9356, val_MinusLogProbMetric: 44.9356

Epoch 712: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5990 - MinusLogProbMetric: 42.5990 - val_loss: 44.9356 - val_MinusLogProbMetric: 44.9356 - lr: 1.5432e-06 - 43s/epoch - 219ms/step
Epoch 713/1000
2023-10-27 14:34:40.319 
Epoch 713/1000 
	 loss: 42.5920, MinusLogProbMetric: 42.5920, val_loss: 44.7910, val_MinusLogProbMetric: 44.7910

Epoch 713: val_loss did not improve from 44.50207
196/196 - 42s - loss: 42.5920 - MinusLogProbMetric: 42.5920 - val_loss: 44.7910 - val_MinusLogProbMetric: 44.7910 - lr: 1.5432e-06 - 42s/epoch - 214ms/step
Epoch 714/1000
2023-10-27 14:35:22.789 
Epoch 714/1000 
	 loss: 42.5986, MinusLogProbMetric: 42.5986, val_loss: 44.5561, val_MinusLogProbMetric: 44.5561

Epoch 714: val_loss did not improve from 44.50207
196/196 - 42s - loss: 42.5986 - MinusLogProbMetric: 42.5986 - val_loss: 44.5561 - val_MinusLogProbMetric: 44.5561 - lr: 1.5432e-06 - 42s/epoch - 217ms/step
Epoch 715/1000
2023-10-27 14:36:05.844 
Epoch 715/1000 
	 loss: 42.5974, MinusLogProbMetric: 42.5974, val_loss: 44.7833, val_MinusLogProbMetric: 44.7833

Epoch 715: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5974 - MinusLogProbMetric: 42.5974 - val_loss: 44.7833 - val_MinusLogProbMetric: 44.7833 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 716/1000
2023-10-27 14:36:49.431 
Epoch 716/1000 
	 loss: 42.5942, MinusLogProbMetric: 42.5942, val_loss: 44.7309, val_MinusLogProbMetric: 44.7309

Epoch 716: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.5942 - MinusLogProbMetric: 42.5942 - val_loss: 44.7309 - val_MinusLogProbMetric: 44.7309 - lr: 1.5432e-06 - 44s/epoch - 222ms/step
Epoch 717/1000
2023-10-27 14:37:32.114 
Epoch 717/1000 
	 loss: 42.6015, MinusLogProbMetric: 42.6015, val_loss: 44.6631, val_MinusLogProbMetric: 44.6631

Epoch 717: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6015 - MinusLogProbMetric: 42.6015 - val_loss: 44.6631 - val_MinusLogProbMetric: 44.6631 - lr: 1.5432e-06 - 43s/epoch - 218ms/step
Epoch 718/1000
2023-10-27 14:38:08.525 
Epoch 718/1000 
	 loss: 42.6099, MinusLogProbMetric: 42.6099, val_loss: 44.8652, val_MinusLogProbMetric: 44.8652

Epoch 718: val_loss did not improve from 44.50207
196/196 - 36s - loss: 42.6099 - MinusLogProbMetric: 42.6099 - val_loss: 44.8652 - val_MinusLogProbMetric: 44.8652 - lr: 1.5432e-06 - 36s/epoch - 186ms/step
Epoch 719/1000
2023-10-27 14:38:45.493 
Epoch 719/1000 
	 loss: 42.5963, MinusLogProbMetric: 42.5963, val_loss: 44.7616, val_MinusLogProbMetric: 44.7616

Epoch 719: val_loss did not improve from 44.50207
196/196 - 37s - loss: 42.5963 - MinusLogProbMetric: 42.5963 - val_loss: 44.7616 - val_MinusLogProbMetric: 44.7616 - lr: 1.5432e-06 - 37s/epoch - 189ms/step
Epoch 720/1000
2023-10-27 14:39:28.473 
Epoch 720/1000 
	 loss: 42.5950, MinusLogProbMetric: 42.5950, val_loss: 44.7471, val_MinusLogProbMetric: 44.7471

Epoch 720: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5950 - MinusLogProbMetric: 42.5950 - val_loss: 44.7471 - val_MinusLogProbMetric: 44.7471 - lr: 1.5432e-06 - 43s/epoch - 219ms/step
Epoch 721/1000
2023-10-27 14:40:12.282 
Epoch 721/1000 
	 loss: 42.5879, MinusLogProbMetric: 42.5879, val_loss: 44.5736, val_MinusLogProbMetric: 44.5736

Epoch 721: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.5879 - MinusLogProbMetric: 42.5879 - val_loss: 44.5736 - val_MinusLogProbMetric: 44.5736 - lr: 1.5432e-06 - 44s/epoch - 223ms/step
Epoch 722/1000
2023-10-27 14:40:55.135 
Epoch 722/1000 
	 loss: 42.5961, MinusLogProbMetric: 42.5961, val_loss: 44.7265, val_MinusLogProbMetric: 44.7265

Epoch 722: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5961 - MinusLogProbMetric: 42.5961 - val_loss: 44.7265 - val_MinusLogProbMetric: 44.7265 - lr: 1.5432e-06 - 43s/epoch - 219ms/step
Epoch 723/1000
2023-10-27 14:41:38.163 
Epoch 723/1000 
	 loss: 42.5865, MinusLogProbMetric: 42.5865, val_loss: 44.5822, val_MinusLogProbMetric: 44.5822

Epoch 723: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5865 - MinusLogProbMetric: 42.5865 - val_loss: 44.5822 - val_MinusLogProbMetric: 44.5822 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 724/1000
2023-10-27 14:42:20.925 
Epoch 724/1000 
	 loss: 42.5855, MinusLogProbMetric: 42.5855, val_loss: 44.7351, val_MinusLogProbMetric: 44.7351

Epoch 724: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5855 - MinusLogProbMetric: 42.5855 - val_loss: 44.7351 - val_MinusLogProbMetric: 44.7351 - lr: 1.5432e-06 - 43s/epoch - 218ms/step
Epoch 725/1000
2023-10-27 14:43:03.748 
Epoch 725/1000 
	 loss: 42.5873, MinusLogProbMetric: 42.5873, val_loss: 45.1012, val_MinusLogProbMetric: 45.1012

Epoch 725: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5873 - MinusLogProbMetric: 42.5873 - val_loss: 45.1012 - val_MinusLogProbMetric: 45.1012 - lr: 1.5432e-06 - 43s/epoch - 218ms/step
Epoch 726/1000
2023-10-27 14:43:46.369 
Epoch 726/1000 
	 loss: 42.5900, MinusLogProbMetric: 42.5900, val_loss: 44.6579, val_MinusLogProbMetric: 44.6579

Epoch 726: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5900 - MinusLogProbMetric: 42.5900 - val_loss: 44.6579 - val_MinusLogProbMetric: 44.6579 - lr: 1.5432e-06 - 43s/epoch - 217ms/step
Epoch 727/1000
2023-10-27 14:44:29.338 
Epoch 727/1000 
	 loss: 42.5881, MinusLogProbMetric: 42.5881, val_loss: 44.7694, val_MinusLogProbMetric: 44.7694

Epoch 727: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5881 - MinusLogProbMetric: 42.5881 - val_loss: 44.7694 - val_MinusLogProbMetric: 44.7694 - lr: 1.5432e-06 - 43s/epoch - 219ms/step
Epoch 728/1000
2023-10-27 14:45:12.517 
Epoch 728/1000 
	 loss: 42.5923, MinusLogProbMetric: 42.5923, val_loss: 44.7248, val_MinusLogProbMetric: 44.7248

Epoch 728: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5923 - MinusLogProbMetric: 42.5923 - val_loss: 44.7248 - val_MinusLogProbMetric: 44.7248 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 729/1000
2023-10-27 14:45:55.687 
Epoch 729/1000 
	 loss: 42.6075, MinusLogProbMetric: 42.6075, val_loss: 44.5133, val_MinusLogProbMetric: 44.5133

Epoch 729: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6075 - MinusLogProbMetric: 42.6075 - val_loss: 44.5133 - val_MinusLogProbMetric: 44.5133 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 730/1000
2023-10-27 14:46:38.824 
Epoch 730/1000 
	 loss: 42.6010, MinusLogProbMetric: 42.6010, val_loss: 44.6087, val_MinusLogProbMetric: 44.6087

Epoch 730: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6010 - MinusLogProbMetric: 42.6010 - val_loss: 44.6087 - val_MinusLogProbMetric: 44.6087 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 731/1000
2023-10-27 14:47:21.867 
Epoch 731/1000 
	 loss: 42.5769, MinusLogProbMetric: 42.5769, val_loss: 44.7624, val_MinusLogProbMetric: 44.7624

Epoch 731: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5769 - MinusLogProbMetric: 42.5769 - val_loss: 44.7624 - val_MinusLogProbMetric: 44.7624 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 732/1000
2023-10-27 14:48:05.069 
Epoch 732/1000 
	 loss: 42.5871, MinusLogProbMetric: 42.5871, val_loss: 44.9219, val_MinusLogProbMetric: 44.9219

Epoch 732: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5871 - MinusLogProbMetric: 42.5871 - val_loss: 44.9219 - val_MinusLogProbMetric: 44.9219 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 733/1000
2023-10-27 14:48:48.568 
Epoch 733/1000 
	 loss: 42.5820, MinusLogProbMetric: 42.5820, val_loss: 44.6712, val_MinusLogProbMetric: 44.6712

Epoch 733: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5820 - MinusLogProbMetric: 42.5820 - val_loss: 44.6712 - val_MinusLogProbMetric: 44.6712 - lr: 1.5432e-06 - 43s/epoch - 222ms/step
Epoch 734/1000
2023-10-27 14:49:31.803 
Epoch 734/1000 
	 loss: 42.5841, MinusLogProbMetric: 42.5841, val_loss: 44.6018, val_MinusLogProbMetric: 44.6018

Epoch 734: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5841 - MinusLogProbMetric: 42.5841 - val_loss: 44.6018 - val_MinusLogProbMetric: 44.6018 - lr: 1.5432e-06 - 43s/epoch - 221ms/step
Epoch 735/1000
2023-10-27 14:50:14.377 
Epoch 735/1000 
	 loss: 42.6054, MinusLogProbMetric: 42.6054, val_loss: 44.7143, val_MinusLogProbMetric: 44.7143

Epoch 735: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.6054 - MinusLogProbMetric: 42.6054 - val_loss: 44.7143 - val_MinusLogProbMetric: 44.7143 - lr: 1.5432e-06 - 43s/epoch - 217ms/step
Epoch 736/1000
2023-10-27 14:50:57.386 
Epoch 736/1000 
	 loss: 42.5890, MinusLogProbMetric: 42.5890, val_loss: 45.0322, val_MinusLogProbMetric: 45.0322

Epoch 736: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5890 - MinusLogProbMetric: 42.5890 - val_loss: 45.0322 - val_MinusLogProbMetric: 45.0322 - lr: 1.5432e-06 - 43s/epoch - 219ms/step
Epoch 737/1000
2023-10-27 14:51:39.939 
Epoch 737/1000 
	 loss: 42.5811, MinusLogProbMetric: 42.5811, val_loss: 44.7484, val_MinusLogProbMetric: 44.7484

Epoch 737: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5811 - MinusLogProbMetric: 42.5811 - val_loss: 44.7484 - val_MinusLogProbMetric: 44.7484 - lr: 1.5432e-06 - 43s/epoch - 217ms/step
Epoch 738/1000
2023-10-27 14:52:22.713 
Epoch 738/1000 
	 loss: 42.5900, MinusLogProbMetric: 42.5900, val_loss: 44.7937, val_MinusLogProbMetric: 44.7937

Epoch 738: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5900 - MinusLogProbMetric: 42.5900 - val_loss: 44.7937 - val_MinusLogProbMetric: 44.7937 - lr: 1.5432e-06 - 43s/epoch - 218ms/step
Epoch 739/1000
2023-10-27 14:53:06.344 
Epoch 739/1000 
	 loss: 42.5856, MinusLogProbMetric: 42.5856, val_loss: 44.6687, val_MinusLogProbMetric: 44.6687

Epoch 739: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.5856 - MinusLogProbMetric: 42.5856 - val_loss: 44.6687 - val_MinusLogProbMetric: 44.6687 - lr: 1.5432e-06 - 44s/epoch - 223ms/step
Epoch 740/1000
2023-10-27 14:53:49.566 
Epoch 740/1000 
	 loss: 42.5783, MinusLogProbMetric: 42.5783, val_loss: 44.7302, val_MinusLogProbMetric: 44.7302

Epoch 740: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5783 - MinusLogProbMetric: 42.5783 - val_loss: 44.7302 - val_MinusLogProbMetric: 44.7302 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 741/1000
2023-10-27 14:54:32.502 
Epoch 741/1000 
	 loss: 42.5858, MinusLogProbMetric: 42.5858, val_loss: 44.6912, val_MinusLogProbMetric: 44.6912

Epoch 741: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5858 - MinusLogProbMetric: 42.5858 - val_loss: 44.6912 - val_MinusLogProbMetric: 44.6912 - lr: 1.5432e-06 - 43s/epoch - 219ms/step
Epoch 742/1000
2023-10-27 14:55:16.087 
Epoch 742/1000 
	 loss: 42.5828, MinusLogProbMetric: 42.5828, val_loss: 44.6094, val_MinusLogProbMetric: 44.6094

Epoch 742: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.5828 - MinusLogProbMetric: 42.5828 - val_loss: 44.6094 - val_MinusLogProbMetric: 44.6094 - lr: 1.5432e-06 - 44s/epoch - 222ms/step
Epoch 743/1000
2023-10-27 14:55:59.608 
Epoch 743/1000 
	 loss: 42.5763, MinusLogProbMetric: 42.5763, val_loss: 44.7388, val_MinusLogProbMetric: 44.7388

Epoch 743: val_loss did not improve from 44.50207
196/196 - 44s - loss: 42.5763 - MinusLogProbMetric: 42.5763 - val_loss: 44.7388 - val_MinusLogProbMetric: 44.7388 - lr: 1.5432e-06 - 44s/epoch - 222ms/step
Epoch 744/1000
2023-10-27 14:56:42.709 
Epoch 744/1000 
	 loss: 42.5831, MinusLogProbMetric: 42.5831, val_loss: 44.7103, val_MinusLogProbMetric: 44.7103

Epoch 744: val_loss did not improve from 44.50207
196/196 - 43s - loss: 42.5831 - MinusLogProbMetric: 42.5831 - val_loss: 44.7103 - val_MinusLogProbMetric: 44.7103 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 745/1000
2023-10-27 14:57:19.254 
Epoch 745/1000 
	 loss: 42.5839, MinusLogProbMetric: 42.5839, val_loss: 44.5888, val_MinusLogProbMetric: 44.5888

Epoch 745: val_loss did not improve from 44.50207
196/196 - 37s - loss: 42.5839 - MinusLogProbMetric: 42.5839 - val_loss: 44.5888 - val_MinusLogProbMetric: 44.5888 - lr: 1.5432e-06 - 37s/epoch - 186ms/step
Epoch 746/1000
2023-10-27 14:57:56.060 
Epoch 746/1000 
	 loss: 42.5825, MinusLogProbMetric: 42.5825, val_loss: 44.4915, val_MinusLogProbMetric: 44.4915

Epoch 746: val_loss improved from 44.50207 to 44.49153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_404/weights/best_weights.h5
196/196 - 37s - loss: 42.5825 - MinusLogProbMetric: 42.5825 - val_loss: 44.4915 - val_MinusLogProbMetric: 44.4915 - lr: 1.5432e-06 - 37s/epoch - 191ms/step
Epoch 747/1000
2023-10-27 14:58:37.874 
Epoch 747/1000 
	 loss: 42.5835, MinusLogProbMetric: 42.5835, val_loss: 44.7627, val_MinusLogProbMetric: 44.7627

Epoch 747: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5835 - MinusLogProbMetric: 42.5835 - val_loss: 44.7627 - val_MinusLogProbMetric: 44.7627 - lr: 1.5432e-06 - 41s/epoch - 210ms/step
Epoch 748/1000
2023-10-27 14:59:20.613 
Epoch 748/1000 
	 loss: 42.5796, MinusLogProbMetric: 42.5796, val_loss: 44.7054, val_MinusLogProbMetric: 44.7054

Epoch 748: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5796 - MinusLogProbMetric: 42.5796 - val_loss: 44.7054 - val_MinusLogProbMetric: 44.7054 - lr: 1.5432e-06 - 43s/epoch - 218ms/step
Epoch 749/1000
2023-10-27 14:59:59.068 
Epoch 749/1000 
	 loss: 42.5746, MinusLogProbMetric: 42.5746, val_loss: 44.9028, val_MinusLogProbMetric: 44.9028

Epoch 749: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5746 - MinusLogProbMetric: 42.5746 - val_loss: 44.9028 - val_MinusLogProbMetric: 44.9028 - lr: 1.5432e-06 - 38s/epoch - 196ms/step
Epoch 750/1000
2023-10-27 15:00:38.721 
Epoch 750/1000 
	 loss: 42.5706, MinusLogProbMetric: 42.5706, val_loss: 44.8118, val_MinusLogProbMetric: 44.8118

Epoch 750: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5706 - MinusLogProbMetric: 42.5706 - val_loss: 44.8118 - val_MinusLogProbMetric: 44.8118 - lr: 1.5432e-06 - 40s/epoch - 202ms/step
Epoch 751/1000
2023-10-27 15:01:18.407 
Epoch 751/1000 
	 loss: 42.5771, MinusLogProbMetric: 42.5771, val_loss: 44.8295, val_MinusLogProbMetric: 44.8295

Epoch 751: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5771 - MinusLogProbMetric: 42.5771 - val_loss: 44.8295 - val_MinusLogProbMetric: 44.8295 - lr: 1.5432e-06 - 40s/epoch - 202ms/step
Epoch 752/1000
2023-10-27 15:02:00.860 
Epoch 752/1000 
	 loss: 42.5717, MinusLogProbMetric: 42.5717, val_loss: 44.6919, val_MinusLogProbMetric: 44.6919

Epoch 752: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5717 - MinusLogProbMetric: 42.5717 - val_loss: 44.6919 - val_MinusLogProbMetric: 44.6919 - lr: 1.5432e-06 - 42s/epoch - 217ms/step
Epoch 753/1000
2023-10-27 15:02:40.358 
Epoch 753/1000 
	 loss: 42.5739, MinusLogProbMetric: 42.5739, val_loss: 44.8972, val_MinusLogProbMetric: 44.8972

Epoch 753: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5739 - MinusLogProbMetric: 42.5739 - val_loss: 44.8972 - val_MinusLogProbMetric: 44.8972 - lr: 1.5432e-06 - 39s/epoch - 202ms/step
Epoch 754/1000
2023-10-27 15:03:21.378 
Epoch 754/1000 
	 loss: 42.5827, MinusLogProbMetric: 42.5827, val_loss: 44.8228, val_MinusLogProbMetric: 44.8228

Epoch 754: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5827 - MinusLogProbMetric: 42.5827 - val_loss: 44.8228 - val_MinusLogProbMetric: 44.8228 - lr: 1.5432e-06 - 41s/epoch - 209ms/step
Epoch 755/1000
2023-10-27 15:04:00.322 
Epoch 755/1000 
	 loss: 42.5692, MinusLogProbMetric: 42.5692, val_loss: 44.7739, val_MinusLogProbMetric: 44.7739

Epoch 755: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5692 - MinusLogProbMetric: 42.5692 - val_loss: 44.7739 - val_MinusLogProbMetric: 44.7739 - lr: 1.5432e-06 - 39s/epoch - 199ms/step
Epoch 756/1000
2023-10-27 15:04:43.343 
Epoch 756/1000 
	 loss: 42.5673, MinusLogProbMetric: 42.5673, val_loss: 44.7477, val_MinusLogProbMetric: 44.7477

Epoch 756: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5673 - MinusLogProbMetric: 42.5673 - val_loss: 44.7477 - val_MinusLogProbMetric: 44.7477 - lr: 1.5432e-06 - 43s/epoch - 219ms/step
Epoch 757/1000
2023-10-27 15:05:25.252 
Epoch 757/1000 
	 loss: 42.5760, MinusLogProbMetric: 42.5760, val_loss: 44.6957, val_MinusLogProbMetric: 44.6957

Epoch 757: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5760 - MinusLogProbMetric: 42.5760 - val_loss: 44.6957 - val_MinusLogProbMetric: 44.6957 - lr: 1.5432e-06 - 42s/epoch - 214ms/step
Epoch 758/1000
2023-10-27 15:06:04.311 
Epoch 758/1000 
	 loss: 42.5688, MinusLogProbMetric: 42.5688, val_loss: 44.6822, val_MinusLogProbMetric: 44.6822

Epoch 758: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5688 - MinusLogProbMetric: 42.5688 - val_loss: 44.6822 - val_MinusLogProbMetric: 44.6822 - lr: 1.5432e-06 - 39s/epoch - 199ms/step
Epoch 759/1000
2023-10-27 15:06:43.641 
Epoch 759/1000 
	 loss: 42.5676, MinusLogProbMetric: 42.5676, val_loss: 44.7297, val_MinusLogProbMetric: 44.7297

Epoch 759: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5676 - MinusLogProbMetric: 42.5676 - val_loss: 44.7297 - val_MinusLogProbMetric: 44.7297 - lr: 1.5432e-06 - 39s/epoch - 201ms/step
Epoch 760/1000
2023-10-27 15:07:25.535 
Epoch 760/1000 
	 loss: 42.5702, MinusLogProbMetric: 42.5702, val_loss: 44.5869, val_MinusLogProbMetric: 44.5869

Epoch 760: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5702 - MinusLogProbMetric: 42.5702 - val_loss: 44.5869 - val_MinusLogProbMetric: 44.5869 - lr: 1.5432e-06 - 42s/epoch - 214ms/step
Epoch 761/1000
2023-10-27 15:08:06.101 
Epoch 761/1000 
	 loss: 42.5696, MinusLogProbMetric: 42.5696, val_loss: 44.9643, val_MinusLogProbMetric: 44.9643

Epoch 761: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5696 - MinusLogProbMetric: 42.5696 - val_loss: 44.9643 - val_MinusLogProbMetric: 44.9643 - lr: 1.5432e-06 - 41s/epoch - 207ms/step
Epoch 762/1000
2023-10-27 15:08:43.359 
Epoch 762/1000 
	 loss: 42.5658, MinusLogProbMetric: 42.5658, val_loss: 44.7513, val_MinusLogProbMetric: 44.7513

Epoch 762: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5658 - MinusLogProbMetric: 42.5658 - val_loss: 44.7513 - val_MinusLogProbMetric: 44.7513 - lr: 1.5432e-06 - 37s/epoch - 190ms/step
Epoch 763/1000
2023-10-27 15:09:21.685 
Epoch 763/1000 
	 loss: 42.5665, MinusLogProbMetric: 42.5665, val_loss: 44.6022, val_MinusLogProbMetric: 44.6022

Epoch 763: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5665 - MinusLogProbMetric: 42.5665 - val_loss: 44.6022 - val_MinusLogProbMetric: 44.6022 - lr: 1.5432e-06 - 38s/epoch - 196ms/step
Epoch 764/1000
2023-10-27 15:10:04.097 
Epoch 764/1000 
	 loss: 42.5673, MinusLogProbMetric: 42.5673, val_loss: 44.6555, val_MinusLogProbMetric: 44.6555

Epoch 764: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5673 - MinusLogProbMetric: 42.5673 - val_loss: 44.6555 - val_MinusLogProbMetric: 44.6555 - lr: 1.5432e-06 - 42s/epoch - 216ms/step
Epoch 765/1000
2023-10-27 15:10:46.338 
Epoch 765/1000 
	 loss: 42.5701, MinusLogProbMetric: 42.5701, val_loss: 44.7745, val_MinusLogProbMetric: 44.7745

Epoch 765: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5701 - MinusLogProbMetric: 42.5701 - val_loss: 44.7745 - val_MinusLogProbMetric: 44.7745 - lr: 1.5432e-06 - 42s/epoch - 215ms/step
Epoch 766/1000
2023-10-27 15:11:26.221 
Epoch 766/1000 
	 loss: 42.5649, MinusLogProbMetric: 42.5649, val_loss: 44.5125, val_MinusLogProbMetric: 44.5125

Epoch 766: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5649 - MinusLogProbMetric: 42.5649 - val_loss: 44.5125 - val_MinusLogProbMetric: 44.5125 - lr: 1.5432e-06 - 40s/epoch - 203ms/step
Epoch 767/1000
2023-10-27 15:12:08.109 
Epoch 767/1000 
	 loss: 42.5671, MinusLogProbMetric: 42.5671, val_loss: 44.6997, val_MinusLogProbMetric: 44.6997

Epoch 767: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5671 - MinusLogProbMetric: 42.5671 - val_loss: 44.6997 - val_MinusLogProbMetric: 44.6997 - lr: 1.5432e-06 - 42s/epoch - 214ms/step
Epoch 768/1000
2023-10-27 15:12:49.421 
Epoch 768/1000 
	 loss: 42.5618, MinusLogProbMetric: 42.5618, val_loss: 44.6377, val_MinusLogProbMetric: 44.6377

Epoch 768: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5618 - MinusLogProbMetric: 42.5618 - val_loss: 44.6377 - val_MinusLogProbMetric: 44.6377 - lr: 1.5432e-06 - 41s/epoch - 211ms/step
Epoch 769/1000
2023-10-27 15:13:30.531 
Epoch 769/1000 
	 loss: 42.5706, MinusLogProbMetric: 42.5706, val_loss: 44.7433, val_MinusLogProbMetric: 44.7433

Epoch 769: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5706 - MinusLogProbMetric: 42.5706 - val_loss: 44.7433 - val_MinusLogProbMetric: 44.7433 - lr: 1.5432e-06 - 41s/epoch - 210ms/step
Epoch 770/1000
2023-10-27 15:14:10.594 
Epoch 770/1000 
	 loss: 42.5606, MinusLogProbMetric: 42.5606, val_loss: 44.6593, val_MinusLogProbMetric: 44.6593

Epoch 770: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5606 - MinusLogProbMetric: 42.5606 - val_loss: 44.6593 - val_MinusLogProbMetric: 44.6593 - lr: 1.5432e-06 - 40s/epoch - 204ms/step
Epoch 771/1000
2023-10-27 15:14:51.579 
Epoch 771/1000 
	 loss: 42.5658, MinusLogProbMetric: 42.5658, val_loss: 44.7808, val_MinusLogProbMetric: 44.7808

Epoch 771: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5658 - MinusLogProbMetric: 42.5658 - val_loss: 44.7808 - val_MinusLogProbMetric: 44.7808 - lr: 1.5432e-06 - 41s/epoch - 209ms/step
Epoch 772/1000
2023-10-27 15:15:30.351 
Epoch 772/1000 
	 loss: 42.5708, MinusLogProbMetric: 42.5708, val_loss: 44.8112, val_MinusLogProbMetric: 44.8112

Epoch 772: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5708 - MinusLogProbMetric: 42.5708 - val_loss: 44.8112 - val_MinusLogProbMetric: 44.8112 - lr: 1.5432e-06 - 39s/epoch - 198ms/step
Epoch 773/1000
2023-10-27 15:16:12.923 
Epoch 773/1000 
	 loss: 42.5651, MinusLogProbMetric: 42.5651, val_loss: 44.8335, val_MinusLogProbMetric: 44.8335

Epoch 773: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5651 - MinusLogProbMetric: 42.5651 - val_loss: 44.8335 - val_MinusLogProbMetric: 44.8335 - lr: 1.5432e-06 - 43s/epoch - 217ms/step
Epoch 774/1000
2023-10-27 15:16:52.237 
Epoch 774/1000 
	 loss: 42.5686, MinusLogProbMetric: 42.5686, val_loss: 44.7994, val_MinusLogProbMetric: 44.7994

Epoch 774: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5686 - MinusLogProbMetric: 42.5686 - val_loss: 44.7994 - val_MinusLogProbMetric: 44.7994 - lr: 1.5432e-06 - 39s/epoch - 201ms/step
Epoch 775/1000
2023-10-27 15:17:32.069 
Epoch 775/1000 
	 loss: 42.5551, MinusLogProbMetric: 42.5551, val_loss: 44.6224, val_MinusLogProbMetric: 44.6224

Epoch 775: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5551 - MinusLogProbMetric: 42.5551 - val_loss: 44.6224 - val_MinusLogProbMetric: 44.6224 - lr: 1.5432e-06 - 40s/epoch - 203ms/step
Epoch 776/1000
2023-10-27 15:18:09.717 
Epoch 776/1000 
	 loss: 42.5529, MinusLogProbMetric: 42.5529, val_loss: 44.8298, val_MinusLogProbMetric: 44.8298

Epoch 776: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5529 - MinusLogProbMetric: 42.5529 - val_loss: 44.8298 - val_MinusLogProbMetric: 44.8298 - lr: 1.5432e-06 - 38s/epoch - 192ms/step
Epoch 777/1000
2023-10-27 15:18:52.745 
Epoch 777/1000 
	 loss: 42.5617, MinusLogProbMetric: 42.5617, val_loss: 44.6059, val_MinusLogProbMetric: 44.6059

Epoch 777: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5617 - MinusLogProbMetric: 42.5617 - val_loss: 44.6059 - val_MinusLogProbMetric: 44.6059 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 778/1000
2023-10-27 15:19:32.036 
Epoch 778/1000 
	 loss: 42.5596, MinusLogProbMetric: 42.5596, val_loss: 44.7686, val_MinusLogProbMetric: 44.7686

Epoch 778: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5596 - MinusLogProbMetric: 42.5596 - val_loss: 44.7686 - val_MinusLogProbMetric: 44.7686 - lr: 1.5432e-06 - 39s/epoch - 200ms/step
Epoch 779/1000
2023-10-27 15:20:09.304 
Epoch 779/1000 
	 loss: 42.5523, MinusLogProbMetric: 42.5523, val_loss: 44.6711, val_MinusLogProbMetric: 44.6711

Epoch 779: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5523 - MinusLogProbMetric: 42.5523 - val_loss: 44.6711 - val_MinusLogProbMetric: 44.6711 - lr: 1.5432e-06 - 37s/epoch - 190ms/step
Epoch 780/1000
2023-10-27 15:20:51.416 
Epoch 780/1000 
	 loss: 42.5726, MinusLogProbMetric: 42.5726, val_loss: 44.6505, val_MinusLogProbMetric: 44.6505

Epoch 780: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5726 - MinusLogProbMetric: 42.5726 - val_loss: 44.6505 - val_MinusLogProbMetric: 44.6505 - lr: 1.5432e-06 - 42s/epoch - 215ms/step
Epoch 781/1000
2023-10-27 15:21:33.788 
Epoch 781/1000 
	 loss: 42.5504, MinusLogProbMetric: 42.5504, val_loss: 44.7665, val_MinusLogProbMetric: 44.7665

Epoch 781: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5504 - MinusLogProbMetric: 42.5504 - val_loss: 44.7665 - val_MinusLogProbMetric: 44.7665 - lr: 1.5432e-06 - 42s/epoch - 216ms/step
Epoch 782/1000
2023-10-27 15:22:11.549 
Epoch 782/1000 
	 loss: 42.5576, MinusLogProbMetric: 42.5576, val_loss: 44.6786, val_MinusLogProbMetric: 44.6786

Epoch 782: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5576 - MinusLogProbMetric: 42.5576 - val_loss: 44.6786 - val_MinusLogProbMetric: 44.6786 - lr: 1.5432e-06 - 38s/epoch - 193ms/step
Epoch 783/1000
2023-10-27 15:22:52.002 
Epoch 783/1000 
	 loss: 42.5541, MinusLogProbMetric: 42.5541, val_loss: 44.6510, val_MinusLogProbMetric: 44.6510

Epoch 783: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5541 - MinusLogProbMetric: 42.5541 - val_loss: 44.6510 - val_MinusLogProbMetric: 44.6510 - lr: 1.5432e-06 - 40s/epoch - 206ms/step
Epoch 784/1000
2023-10-27 15:23:29.278 
Epoch 784/1000 
	 loss: 42.5677, MinusLogProbMetric: 42.5677, val_loss: 44.6885, val_MinusLogProbMetric: 44.6885

Epoch 784: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5677 - MinusLogProbMetric: 42.5677 - val_loss: 44.6885 - val_MinusLogProbMetric: 44.6885 - lr: 1.5432e-06 - 37s/epoch - 190ms/step
Epoch 785/1000
2023-10-27 15:24:11.909 
Epoch 785/1000 
	 loss: 42.5643, MinusLogProbMetric: 42.5643, val_loss: 44.7830, val_MinusLogProbMetric: 44.7830

Epoch 785: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5643 - MinusLogProbMetric: 42.5643 - val_loss: 44.7830 - val_MinusLogProbMetric: 44.7830 - lr: 1.5432e-06 - 43s/epoch - 217ms/step
Epoch 786/1000
2023-10-27 15:24:51.777 
Epoch 786/1000 
	 loss: 42.5462, MinusLogProbMetric: 42.5462, val_loss: 44.6071, val_MinusLogProbMetric: 44.6071

Epoch 786: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5462 - MinusLogProbMetric: 42.5462 - val_loss: 44.6071 - val_MinusLogProbMetric: 44.6071 - lr: 1.5432e-06 - 40s/epoch - 203ms/step
Epoch 787/1000
2023-10-27 15:25:28.702 
Epoch 787/1000 
	 loss: 42.5511, MinusLogProbMetric: 42.5511, val_loss: 44.7349, val_MinusLogProbMetric: 44.7349

Epoch 787: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5511 - MinusLogProbMetric: 42.5511 - val_loss: 44.7349 - val_MinusLogProbMetric: 44.7349 - lr: 1.5432e-06 - 37s/epoch - 188ms/step
Epoch 788/1000
2023-10-27 15:26:09.041 
Epoch 788/1000 
	 loss: 42.5601, MinusLogProbMetric: 42.5601, val_loss: 44.6877, val_MinusLogProbMetric: 44.6877

Epoch 788: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5601 - MinusLogProbMetric: 42.5601 - val_loss: 44.6877 - val_MinusLogProbMetric: 44.6877 - lr: 1.5432e-06 - 40s/epoch - 206ms/step
Epoch 789/1000
2023-10-27 15:26:52.210 
Epoch 789/1000 
	 loss: 42.5634, MinusLogProbMetric: 42.5634, val_loss: 44.7164, val_MinusLogProbMetric: 44.7164

Epoch 789: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5634 - MinusLogProbMetric: 42.5634 - val_loss: 44.7164 - val_MinusLogProbMetric: 44.7164 - lr: 1.5432e-06 - 43s/epoch - 220ms/step
Epoch 790/1000
2023-10-27 15:27:33.173 
Epoch 790/1000 
	 loss: 42.5555, MinusLogProbMetric: 42.5555, val_loss: 44.7739, val_MinusLogProbMetric: 44.7739

Epoch 790: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5555 - MinusLogProbMetric: 42.5555 - val_loss: 44.7739 - val_MinusLogProbMetric: 44.7739 - lr: 1.5432e-06 - 41s/epoch - 209ms/step
Epoch 791/1000
2023-10-27 15:28:09.177 
Epoch 791/1000 
	 loss: 42.5483, MinusLogProbMetric: 42.5483, val_loss: 44.9118, val_MinusLogProbMetric: 44.9118

Epoch 791: val_loss did not improve from 44.49153
196/196 - 36s - loss: 42.5483 - MinusLogProbMetric: 42.5483 - val_loss: 44.9118 - val_MinusLogProbMetric: 44.9118 - lr: 1.5432e-06 - 36s/epoch - 184ms/step
Epoch 792/1000
2023-10-27 15:28:48.139 
Epoch 792/1000 
	 loss: 42.5393, MinusLogProbMetric: 42.5393, val_loss: 44.7101, val_MinusLogProbMetric: 44.7101

Epoch 792: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5393 - MinusLogProbMetric: 42.5393 - val_loss: 44.7101 - val_MinusLogProbMetric: 44.7101 - lr: 1.5432e-06 - 39s/epoch - 199ms/step
Epoch 793/1000
2023-10-27 15:29:31.511 
Epoch 793/1000 
	 loss: 42.5528, MinusLogProbMetric: 42.5528, val_loss: 44.6497, val_MinusLogProbMetric: 44.6497

Epoch 793: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5528 - MinusLogProbMetric: 42.5528 - val_loss: 44.6497 - val_MinusLogProbMetric: 44.6497 - lr: 1.5432e-06 - 43s/epoch - 221ms/step
Epoch 794/1000
2023-10-27 15:30:10.559 
Epoch 794/1000 
	 loss: 42.5482, MinusLogProbMetric: 42.5482, val_loss: 44.6740, val_MinusLogProbMetric: 44.6740

Epoch 794: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5482 - MinusLogProbMetric: 42.5482 - val_loss: 44.6740 - val_MinusLogProbMetric: 44.6740 - lr: 1.5432e-06 - 39s/epoch - 199ms/step
Epoch 795/1000
2023-10-27 15:30:47.961 
Epoch 795/1000 
	 loss: 42.5554, MinusLogProbMetric: 42.5554, val_loss: 44.7881, val_MinusLogProbMetric: 44.7881

Epoch 795: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5554 - MinusLogProbMetric: 42.5554 - val_loss: 44.7881 - val_MinusLogProbMetric: 44.7881 - lr: 1.5432e-06 - 37s/epoch - 191ms/step
Epoch 796/1000
2023-10-27 15:31:28.937 
Epoch 796/1000 
	 loss: 42.5538, MinusLogProbMetric: 42.5538, val_loss: 44.7594, val_MinusLogProbMetric: 44.7594

Epoch 796: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5538 - MinusLogProbMetric: 42.5538 - val_loss: 44.7594 - val_MinusLogProbMetric: 44.7594 - lr: 1.5432e-06 - 41s/epoch - 209ms/step
Epoch 797/1000
2023-10-27 15:32:09.810 
Epoch 797/1000 
	 loss: 42.5230, MinusLogProbMetric: 42.5230, val_loss: 44.8891, val_MinusLogProbMetric: 44.8891

Epoch 797: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5230 - MinusLogProbMetric: 42.5230 - val_loss: 44.8891 - val_MinusLogProbMetric: 44.8891 - lr: 1.0000e-06 - 41s/epoch - 209ms/step
Epoch 798/1000
2023-10-27 15:32:51.389 
Epoch 798/1000 
	 loss: 42.5182, MinusLogProbMetric: 42.5182, val_loss: 44.7511, val_MinusLogProbMetric: 44.7511

Epoch 798: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5182 - MinusLogProbMetric: 42.5182 - val_loss: 44.7511 - val_MinusLogProbMetric: 44.7511 - lr: 1.0000e-06 - 42s/epoch - 212ms/step
Epoch 799/1000
2023-10-27 15:33:29.477 
Epoch 799/1000 
	 loss: 42.5209, MinusLogProbMetric: 42.5209, val_loss: 44.7380, val_MinusLogProbMetric: 44.7380

Epoch 799: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5209 - MinusLogProbMetric: 42.5209 - val_loss: 44.7380 - val_MinusLogProbMetric: 44.7380 - lr: 1.0000e-06 - 38s/epoch - 194ms/step
Epoch 800/1000
2023-10-27 15:34:09.316 
Epoch 800/1000 
	 loss: 42.5170, MinusLogProbMetric: 42.5170, val_loss: 44.6217, val_MinusLogProbMetric: 44.6217

Epoch 800: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5170 - MinusLogProbMetric: 42.5170 - val_loss: 44.6217 - val_MinusLogProbMetric: 44.6217 - lr: 1.0000e-06 - 40s/epoch - 203ms/step
Epoch 801/1000
2023-10-27 15:34:50.074 
Epoch 801/1000 
	 loss: 42.5233, MinusLogProbMetric: 42.5233, val_loss: 44.5990, val_MinusLogProbMetric: 44.5990

Epoch 801: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5233 - MinusLogProbMetric: 42.5233 - val_loss: 44.5990 - val_MinusLogProbMetric: 44.5990 - lr: 1.0000e-06 - 41s/epoch - 208ms/step
Epoch 802/1000
2023-10-27 15:35:33.523 
Epoch 802/1000 
	 loss: 42.5219, MinusLogProbMetric: 42.5219, val_loss: 44.8757, val_MinusLogProbMetric: 44.8757

Epoch 802: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5219 - MinusLogProbMetric: 42.5219 - val_loss: 44.8757 - val_MinusLogProbMetric: 44.8757 - lr: 1.0000e-06 - 43s/epoch - 222ms/step
Epoch 803/1000
2023-10-27 15:36:14.215 
Epoch 803/1000 
	 loss: 42.5179, MinusLogProbMetric: 42.5179, val_loss: 44.6920, val_MinusLogProbMetric: 44.6920

Epoch 803: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5179 - MinusLogProbMetric: 42.5179 - val_loss: 44.6920 - val_MinusLogProbMetric: 44.6920 - lr: 1.0000e-06 - 41s/epoch - 208ms/step
Epoch 804/1000
2023-10-27 15:36:52.706 
Epoch 804/1000 
	 loss: 42.5162, MinusLogProbMetric: 42.5162, val_loss: 44.5948, val_MinusLogProbMetric: 44.5948

Epoch 804: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5162 - MinusLogProbMetric: 42.5162 - val_loss: 44.5948 - val_MinusLogProbMetric: 44.5948 - lr: 1.0000e-06 - 38s/epoch - 196ms/step
Epoch 805/1000
2023-10-27 15:37:32.276 
Epoch 805/1000 
	 loss: 42.5168, MinusLogProbMetric: 42.5168, val_loss: 44.6536, val_MinusLogProbMetric: 44.6536

Epoch 805: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5168 - MinusLogProbMetric: 42.5168 - val_loss: 44.6536 - val_MinusLogProbMetric: 44.6536 - lr: 1.0000e-06 - 40s/epoch - 202ms/step
Epoch 806/1000
2023-10-27 15:38:13.995 
Epoch 806/1000 
	 loss: 42.5212, MinusLogProbMetric: 42.5212, val_loss: 44.7101, val_MinusLogProbMetric: 44.7101

Epoch 806: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5212 - MinusLogProbMetric: 42.5212 - val_loss: 44.7101 - val_MinusLogProbMetric: 44.7101 - lr: 1.0000e-06 - 42s/epoch - 213ms/step
Epoch 807/1000
2023-10-27 15:38:57.428 
Epoch 807/1000 
	 loss: 42.5158, MinusLogProbMetric: 42.5158, val_loss: 44.7108, val_MinusLogProbMetric: 44.7108

Epoch 807: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5158 - MinusLogProbMetric: 42.5158 - val_loss: 44.7108 - val_MinusLogProbMetric: 44.7108 - lr: 1.0000e-06 - 43s/epoch - 222ms/step
Epoch 808/1000
2023-10-27 15:39:38.688 
Epoch 808/1000 
	 loss: 42.5207, MinusLogProbMetric: 42.5207, val_loss: 44.6948, val_MinusLogProbMetric: 44.6948

Epoch 808: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5207 - MinusLogProbMetric: 42.5207 - val_loss: 44.6948 - val_MinusLogProbMetric: 44.6948 - lr: 1.0000e-06 - 41s/epoch - 210ms/step
Epoch 809/1000
2023-10-27 15:40:18.099 
Epoch 809/1000 
	 loss: 42.5204, MinusLogProbMetric: 42.5204, val_loss: 44.6680, val_MinusLogProbMetric: 44.6680

Epoch 809: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5204 - MinusLogProbMetric: 42.5204 - val_loss: 44.6680 - val_MinusLogProbMetric: 44.6680 - lr: 1.0000e-06 - 39s/epoch - 201ms/step
Epoch 810/1000
2023-10-27 15:40:58.011 
Epoch 810/1000 
	 loss: 42.5144, MinusLogProbMetric: 42.5144, val_loss: 44.6724, val_MinusLogProbMetric: 44.6724

Epoch 810: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5144 - MinusLogProbMetric: 42.5144 - val_loss: 44.6724 - val_MinusLogProbMetric: 44.6724 - lr: 1.0000e-06 - 40s/epoch - 204ms/step
Epoch 811/1000
2023-10-27 15:41:40.993 
Epoch 811/1000 
	 loss: 42.5135, MinusLogProbMetric: 42.5135, val_loss: 44.7125, val_MinusLogProbMetric: 44.7125

Epoch 811: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5135 - MinusLogProbMetric: 42.5135 - val_loss: 44.7125 - val_MinusLogProbMetric: 44.7125 - lr: 1.0000e-06 - 43s/epoch - 219ms/step
Epoch 812/1000
2023-10-27 15:42:20.301 
Epoch 812/1000 
	 loss: 42.5203, MinusLogProbMetric: 42.5203, val_loss: 44.7780, val_MinusLogProbMetric: 44.7780

Epoch 812: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5203 - MinusLogProbMetric: 42.5203 - val_loss: 44.7780 - val_MinusLogProbMetric: 44.7780 - lr: 1.0000e-06 - 39s/epoch - 201ms/step
Epoch 813/1000
2023-10-27 15:43:00.228 
Epoch 813/1000 
	 loss: 42.5165, MinusLogProbMetric: 42.5165, val_loss: 44.6514, val_MinusLogProbMetric: 44.6514

Epoch 813: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5165 - MinusLogProbMetric: 42.5165 - val_loss: 44.6514 - val_MinusLogProbMetric: 44.6514 - lr: 1.0000e-06 - 40s/epoch - 204ms/step
Epoch 814/1000
2023-10-27 15:43:41.902 
Epoch 814/1000 
	 loss: 42.5143, MinusLogProbMetric: 42.5143, val_loss: 44.7008, val_MinusLogProbMetric: 44.7008

Epoch 814: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5143 - MinusLogProbMetric: 42.5143 - val_loss: 44.7008 - val_MinusLogProbMetric: 44.7008 - lr: 1.0000e-06 - 42s/epoch - 213ms/step
Epoch 815/1000
2023-10-27 15:44:22.704 
Epoch 815/1000 
	 loss: 42.5177, MinusLogProbMetric: 42.5177, val_loss: 44.6806, val_MinusLogProbMetric: 44.6806

Epoch 815: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5177 - MinusLogProbMetric: 42.5177 - val_loss: 44.6806 - val_MinusLogProbMetric: 44.6806 - lr: 1.0000e-06 - 41s/epoch - 208ms/step
Epoch 816/1000
2023-10-27 15:45:05.019 
Epoch 816/1000 
	 loss: 42.5124, MinusLogProbMetric: 42.5124, val_loss: 44.6212, val_MinusLogProbMetric: 44.6212

Epoch 816: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5124 - MinusLogProbMetric: 42.5124 - val_loss: 44.6212 - val_MinusLogProbMetric: 44.6212 - lr: 1.0000e-06 - 42s/epoch - 216ms/step
Epoch 817/1000
2023-10-27 15:45:47.135 
Epoch 817/1000 
	 loss: 42.5092, MinusLogProbMetric: 42.5092, val_loss: 44.7223, val_MinusLogProbMetric: 44.7223

Epoch 817: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5092 - MinusLogProbMetric: 42.5092 - val_loss: 44.7223 - val_MinusLogProbMetric: 44.7223 - lr: 1.0000e-06 - 42s/epoch - 215ms/step
Epoch 818/1000
2023-10-27 15:46:29.118 
Epoch 818/1000 
	 loss: 42.5101, MinusLogProbMetric: 42.5101, val_loss: 44.7624, val_MinusLogProbMetric: 44.7624

Epoch 818: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5101 - MinusLogProbMetric: 42.5101 - val_loss: 44.7624 - val_MinusLogProbMetric: 44.7624 - lr: 1.0000e-06 - 42s/epoch - 214ms/step
Epoch 819/1000
2023-10-27 15:47:07.539 
Epoch 819/1000 
	 loss: 42.5131, MinusLogProbMetric: 42.5131, val_loss: 44.8852, val_MinusLogProbMetric: 44.8852

Epoch 819: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5131 - MinusLogProbMetric: 42.5131 - val_loss: 44.8852 - val_MinusLogProbMetric: 44.8852 - lr: 1.0000e-06 - 38s/epoch - 196ms/step
Epoch 820/1000
2023-10-27 15:47:50.877 
Epoch 820/1000 
	 loss: 42.5087, MinusLogProbMetric: 42.5087, val_loss: 44.5568, val_MinusLogProbMetric: 44.5568

Epoch 820: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5087 - MinusLogProbMetric: 42.5087 - val_loss: 44.5568 - val_MinusLogProbMetric: 44.5568 - lr: 1.0000e-06 - 43s/epoch - 221ms/step
Epoch 821/1000
2023-10-27 15:48:32.505 
Epoch 821/1000 
	 loss: 42.5099, MinusLogProbMetric: 42.5099, val_loss: 44.7651, val_MinusLogProbMetric: 44.7651

Epoch 821: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5099 - MinusLogProbMetric: 42.5099 - val_loss: 44.7651 - val_MinusLogProbMetric: 44.7651 - lr: 1.0000e-06 - 42s/epoch - 212ms/step
Epoch 822/1000
2023-10-27 15:49:14.910 
Epoch 822/1000 
	 loss: 42.5043, MinusLogProbMetric: 42.5043, val_loss: 44.8186, val_MinusLogProbMetric: 44.8186

Epoch 822: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5043 - MinusLogProbMetric: 42.5043 - val_loss: 44.8186 - val_MinusLogProbMetric: 44.8186 - lr: 1.0000e-06 - 42s/epoch - 216ms/step
Epoch 823/1000
2023-10-27 15:49:56.956 
Epoch 823/1000 
	 loss: 42.5077, MinusLogProbMetric: 42.5077, val_loss: 44.7671, val_MinusLogProbMetric: 44.7671

Epoch 823: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5077 - MinusLogProbMetric: 42.5077 - val_loss: 44.7671 - val_MinusLogProbMetric: 44.7671 - lr: 1.0000e-06 - 42s/epoch - 214ms/step
Epoch 824/1000
2023-10-27 15:50:36.610 
Epoch 824/1000 
	 loss: 42.5107, MinusLogProbMetric: 42.5107, val_loss: 44.7378, val_MinusLogProbMetric: 44.7378

Epoch 824: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5107 - MinusLogProbMetric: 42.5107 - val_loss: 44.7378 - val_MinusLogProbMetric: 44.7378 - lr: 1.0000e-06 - 40s/epoch - 202ms/step
Epoch 825/1000
2023-10-27 15:51:19.688 
Epoch 825/1000 
	 loss: 42.5098, MinusLogProbMetric: 42.5098, val_loss: 44.7383, val_MinusLogProbMetric: 44.7383

Epoch 825: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5098 - MinusLogProbMetric: 42.5098 - val_loss: 44.7383 - val_MinusLogProbMetric: 44.7383 - lr: 1.0000e-06 - 43s/epoch - 220ms/step
Epoch 826/1000
2023-10-27 15:52:01.508 
Epoch 826/1000 
	 loss: 42.5068, MinusLogProbMetric: 42.5068, val_loss: 44.5621, val_MinusLogProbMetric: 44.5621

Epoch 826: val_loss did not improve from 44.49153
196/196 - 42s - loss: 42.5068 - MinusLogProbMetric: 42.5068 - val_loss: 44.5621 - val_MinusLogProbMetric: 44.5621 - lr: 1.0000e-06 - 42s/epoch - 213ms/step
Epoch 827/1000
2023-10-27 15:52:39.914 
Epoch 827/1000 
	 loss: 42.5130, MinusLogProbMetric: 42.5130, val_loss: 44.8661, val_MinusLogProbMetric: 44.8661

Epoch 827: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5130 - MinusLogProbMetric: 42.5130 - val_loss: 44.8661 - val_MinusLogProbMetric: 44.8661 - lr: 1.0000e-06 - 38s/epoch - 196ms/step
Epoch 828/1000
2023-10-27 15:53:17.526 
Epoch 828/1000 
	 loss: 42.5129, MinusLogProbMetric: 42.5129, val_loss: 44.6863, val_MinusLogProbMetric: 44.6863

Epoch 828: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5129 - MinusLogProbMetric: 42.5129 - val_loss: 44.6863 - val_MinusLogProbMetric: 44.6863 - lr: 1.0000e-06 - 38s/epoch - 192ms/step
Epoch 829/1000
2023-10-27 15:54:00.855 
Epoch 829/1000 
	 loss: 42.5100, MinusLogProbMetric: 42.5100, val_loss: 44.8520, val_MinusLogProbMetric: 44.8520

Epoch 829: val_loss did not improve from 44.49153
196/196 - 43s - loss: 42.5100 - MinusLogProbMetric: 42.5100 - val_loss: 44.8520 - val_MinusLogProbMetric: 44.8520 - lr: 1.0000e-06 - 43s/epoch - 221ms/step
Epoch 830/1000
2023-10-27 15:54:38.992 
Epoch 830/1000 
	 loss: 42.5083, MinusLogProbMetric: 42.5083, val_loss: 44.7250, val_MinusLogProbMetric: 44.7250

Epoch 830: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5083 - MinusLogProbMetric: 42.5083 - val_loss: 44.7250 - val_MinusLogProbMetric: 44.7250 - lr: 1.0000e-06 - 38s/epoch - 195ms/step
Epoch 831/1000
2023-10-27 15:55:17.726 
Epoch 831/1000 
	 loss: 42.5084, MinusLogProbMetric: 42.5084, val_loss: 44.6402, val_MinusLogProbMetric: 44.6402

Epoch 831: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5084 - MinusLogProbMetric: 42.5084 - val_loss: 44.6402 - val_MinusLogProbMetric: 44.6402 - lr: 1.0000e-06 - 39s/epoch - 198ms/step
Epoch 832/1000
2023-10-27 15:55:51.626 
Epoch 832/1000 
	 loss: 42.5061, MinusLogProbMetric: 42.5061, val_loss: 44.6247, val_MinusLogProbMetric: 44.6247

Epoch 832: val_loss did not improve from 44.49153
196/196 - 34s - loss: 42.5061 - MinusLogProbMetric: 42.5061 - val_loss: 44.6247 - val_MinusLogProbMetric: 44.6247 - lr: 1.0000e-06 - 34s/epoch - 173ms/step
Epoch 833/1000
2023-10-27 15:56:31.108 
Epoch 833/1000 
	 loss: 42.5043, MinusLogProbMetric: 42.5043, val_loss: 44.7779, val_MinusLogProbMetric: 44.7779

Epoch 833: val_loss did not improve from 44.49153
196/196 - 39s - loss: 42.5043 - MinusLogProbMetric: 42.5043 - val_loss: 44.7779 - val_MinusLogProbMetric: 44.7779 - lr: 1.0000e-06 - 39s/epoch - 201ms/step
Epoch 834/1000
2023-10-27 15:57:12.288 
Epoch 834/1000 
	 loss: 42.5057, MinusLogProbMetric: 42.5057, val_loss: 44.5970, val_MinusLogProbMetric: 44.5970

Epoch 834: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5057 - MinusLogProbMetric: 42.5057 - val_loss: 44.5970 - val_MinusLogProbMetric: 44.5970 - lr: 1.0000e-06 - 41s/epoch - 210ms/step
Epoch 835/1000
2023-10-27 15:57:50.032 
Epoch 835/1000 
	 loss: 42.5085, MinusLogProbMetric: 42.5085, val_loss: 44.7803, val_MinusLogProbMetric: 44.7803

Epoch 835: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5085 - MinusLogProbMetric: 42.5085 - val_loss: 44.7803 - val_MinusLogProbMetric: 44.7803 - lr: 1.0000e-06 - 38s/epoch - 193ms/step
Epoch 836/1000
2023-10-27 15:58:24.301 
Epoch 836/1000 
	 loss: 42.5100, MinusLogProbMetric: 42.5100, val_loss: 44.6679, val_MinusLogProbMetric: 44.6679

Epoch 836: val_loss did not improve from 44.49153
196/196 - 34s - loss: 42.5100 - MinusLogProbMetric: 42.5100 - val_loss: 44.6679 - val_MinusLogProbMetric: 44.6679 - lr: 1.0000e-06 - 34s/epoch - 175ms/step
Epoch 837/1000
2023-10-27 15:59:04.024 
Epoch 837/1000 
	 loss: 42.5051, MinusLogProbMetric: 42.5051, val_loss: 44.6820, val_MinusLogProbMetric: 44.6820

Epoch 837: val_loss did not improve from 44.49153
196/196 - 40s - loss: 42.5051 - MinusLogProbMetric: 42.5051 - val_loss: 44.6820 - val_MinusLogProbMetric: 44.6820 - lr: 1.0000e-06 - 40s/epoch - 203ms/step
Epoch 838/1000
2023-10-27 15:59:41.439 
Epoch 838/1000 
	 loss: 42.5057, MinusLogProbMetric: 42.5057, val_loss: 44.6865, val_MinusLogProbMetric: 44.6865

Epoch 838: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5057 - MinusLogProbMetric: 42.5057 - val_loss: 44.6865 - val_MinusLogProbMetric: 44.6865 - lr: 1.0000e-06 - 37s/epoch - 191ms/step
Epoch 839/1000
2023-10-27 16:00:16.396 
Epoch 839/1000 
	 loss: 42.5017, MinusLogProbMetric: 42.5017, val_loss: 44.5859, val_MinusLogProbMetric: 44.5859

Epoch 839: val_loss did not improve from 44.49153
196/196 - 35s - loss: 42.5017 - MinusLogProbMetric: 42.5017 - val_loss: 44.5859 - val_MinusLogProbMetric: 44.5859 - lr: 1.0000e-06 - 35s/epoch - 178ms/step
Epoch 840/1000
2023-10-27 16:00:53.770 
Epoch 840/1000 
	 loss: 42.5053, MinusLogProbMetric: 42.5053, val_loss: 44.8096, val_MinusLogProbMetric: 44.8096

Epoch 840: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5053 - MinusLogProbMetric: 42.5053 - val_loss: 44.8096 - val_MinusLogProbMetric: 44.8096 - lr: 1.0000e-06 - 37s/epoch - 191ms/step
Epoch 841/1000
2023-10-27 16:01:30.311 
Epoch 841/1000 
	 loss: 42.5052, MinusLogProbMetric: 42.5052, val_loss: 44.7207, val_MinusLogProbMetric: 44.7207

Epoch 841: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5052 - MinusLogProbMetric: 42.5052 - val_loss: 44.7207 - val_MinusLogProbMetric: 44.7207 - lr: 1.0000e-06 - 37s/epoch - 186ms/step
Epoch 842/1000
2023-10-27 16:02:07.890 
Epoch 842/1000 
	 loss: 42.5026, MinusLogProbMetric: 42.5026, val_loss: 44.7028, val_MinusLogProbMetric: 44.7028

Epoch 842: val_loss did not improve from 44.49153
196/196 - 38s - loss: 42.5026 - MinusLogProbMetric: 42.5026 - val_loss: 44.7028 - val_MinusLogProbMetric: 44.7028 - lr: 1.0000e-06 - 38s/epoch - 192ms/step
Epoch 843/1000
2023-10-27 16:02:44.291 
Epoch 843/1000 
	 loss: 42.5044, MinusLogProbMetric: 42.5044, val_loss: 44.7651, val_MinusLogProbMetric: 44.7651

Epoch 843: val_loss did not improve from 44.49153
196/196 - 36s - loss: 42.5044 - MinusLogProbMetric: 42.5044 - val_loss: 44.7651 - val_MinusLogProbMetric: 44.7651 - lr: 1.0000e-06 - 36s/epoch - 186ms/step
Epoch 844/1000
2023-10-27 16:03:25.256 
Epoch 844/1000 
	 loss: 42.5047, MinusLogProbMetric: 42.5047, val_loss: 44.7142, val_MinusLogProbMetric: 44.7142

Epoch 844: val_loss did not improve from 44.49153
196/196 - 41s - loss: 42.5047 - MinusLogProbMetric: 42.5047 - val_loss: 44.7142 - val_MinusLogProbMetric: 44.7142 - lr: 1.0000e-06 - 41s/epoch - 209ms/step
Epoch 845/1000
2023-10-27 16:04:02.041 
Epoch 845/1000 
	 loss: 42.5065, MinusLogProbMetric: 42.5065, val_loss: 44.7443, val_MinusLogProbMetric: 44.7443

Epoch 845: val_loss did not improve from 44.49153
196/196 - 37s - loss: 42.5065 - MinusLogProbMetric: 42.5065 - val_loss: 44.7443 - val_MinusLogProbMetric: 44.7443 - lr: 1.0000e-06 - 37s/epoch - 188ms/step
Epoch 846/1000
2023-10-27 16:04:39.497 
Epoch 846/1000 
	 loss: 42.5027, MinusLogProbMetric: 42.5027, val_loss: 44.7218, val_MinusLogProbMetric: 44.7218

Epoch 846: val_loss did not improve from 44.49153
Restoring model weights from the end of the best epoch: 746.
196/196 - 38s - loss: 42.5027 - MinusLogProbMetric: 42.5027 - val_loss: 44.7218 - val_MinusLogProbMetric: 44.7218 - lr: 1.0000e-06 - 38s/epoch - 194ms/step
Epoch 846: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 0.
Model trained in 34513.31 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.93 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.16 s.
===========
Run 404/720 done in 41368.14 s.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_408/ already exists.
Skipping it.
===========
Run 408/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_409/ already exists.
Skipping it.
===========
Run 409/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_414/ already exists.
Skipping it.
===========
Run 414/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_415/ already exists.
Skipping it.
===========
Run 415/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_416/ already exists.
Skipping it.
===========
Run 416/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_417/ already exists.
Skipping it.
===========
Run 417/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_418/ already exists.
Skipping it.
===========
Run 418/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_419/ already exists.
Skipping it.
===========
Run 419/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_420/ already exists.
Skipping it.
===========
Run 420/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_421/ already exists.
Skipping it.
===========
Run 421/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_422/ already exists.
Skipping it.
===========
Run 422/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_423/ already exists.
Skipping it.
===========
Run 423/720 already exists. Skipping it.
===========

===========
Generating train data for run 424.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_444"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_445 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7ff51d53c8b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff13139b790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff13139b790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff133bbc880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff5a8486650>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff5a84873a0>, <keras.callbacks.ModelCheckpoint object at 0x7ff5a8485690>, <keras.callbacks.EarlyStopping object at 0x7ff5a8486050>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff5a8487b80>, <keras.callbacks.TerminateOnNaN object at 0x7ff5a8485f90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:04:49.681988
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:07:39.756 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 170s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 170s/epoch - 867ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 424.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_455"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_456 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7ff4d5d2df30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff450c573d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff450c573d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff75014f490>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff14a7d9720>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff14a7db100>, <keras.callbacks.ModelCheckpoint object at 0x7ff14a7db5b0>, <keras.callbacks.EarlyStopping object at 0x7ff14a7db400>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff14a7d9f90>, <keras.callbacks.TerminateOnNaN object at 0x7ff14a7db580>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:07:48.701216
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:10:30.862 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 162s/epoch - 826ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 424.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_466"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_467 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7ff407f9ceb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3252acfa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3252acfa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff40de0ff10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff3cff2aad0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff3cff2b040>, <keras.callbacks.ModelCheckpoint object at 0x7ff3cff2b100>, <keras.callbacks.EarlyStopping object at 0x7ff3cff2b370>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff3cff2b3a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff3cff2afe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:10:42.402177
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:13:39.351 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 177s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 177s/epoch - 901ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 424.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_477"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_478 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7ff0f876f280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1b61effa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1b61effa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff30d6bec20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff1489107f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff148910d60>, <keras.callbacks.ModelCheckpoint object at 0x7ff148910e20>, <keras.callbacks.EarlyStopping object at 0x7ff148911090>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff1489110c0>, <keras.callbacks.TerminateOnNaN object at 0x7ff148910d00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:13:49.986609
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:16:45.889 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 176s/epoch - 897ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 424.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_488"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_489 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7ff14a595450>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff1b777d030>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff1b777d030>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff122545db0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4523181f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff45231a530>, <keras.callbacks.ModelCheckpoint object at 0x7ff45231ab00>, <keras.callbacks.EarlyStopping object at 0x7ff45231bca0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4523183d0>, <keras.callbacks.TerminateOnNaN object at 0x7ff4523186d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:16:58.123114
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:19:43.223 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 165s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 165s/epoch - 841ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 424.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_499"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_500 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7ff42f862380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff405345540>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff405345540>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff486652830>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff405069b70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff40506a0e0>, <keras.callbacks.ModelCheckpoint object at 0x7ff40506a1a0>, <keras.callbacks.EarlyStopping object at 0x7ff40506a410>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff40506a440>, <keras.callbacks.TerminateOnNaN object at 0x7ff40506a080>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:19:52.448197
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:22:44.500 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 172s/epoch - 878ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 424.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_510"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_511 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7ffb9734bbb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff4523ee260>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff4523ee260>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff3cd9cd570>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff123b867d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff123b86d40>, <keras.callbacks.ModelCheckpoint object at 0x7ff123b86e00>, <keras.callbacks.EarlyStopping object at 0x7ff123b87070>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff123b870a0>, <keras.callbacks.TerminateOnNaN object at 0x7ff123b86ce0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:22:57.500081
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:25:56.424 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 179s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 179s/epoch - 912ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 424.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_521"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_522 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7ff1b5d74040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff5e81d1000>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff5e81d1000>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff51c956320>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff6d15afcd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff6d15af7c0>, <keras.callbacks.ModelCheckpoint object at 0x7ff6d15af7f0>, <keras.callbacks.EarlyStopping object at 0x7ff6d15afca0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff6d15ac0d0>, <keras.callbacks.TerminateOnNaN object at 0x7ff6d15ad240>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:26:06.063167
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:28:44.221 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 158s/epoch - 806ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 424.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_532"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_533 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7ff3c79c7a90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff3c7e44610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff3c7e44610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff09fb765c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff3c79fe6b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff3c79fec20>, <keras.callbacks.ModelCheckpoint object at 0x7ff3c79fece0>, <keras.callbacks.EarlyStopping object at 0x7ff3c79fef50>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff3c79fef80>, <keras.callbacks.TerminateOnNaN object at 0x7ff3c79febc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:28:52.445849
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:31:51.202 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11757.0029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 179s - loss: nan - MinusLogProbMetric: 11757.0029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 179s/epoch - 911ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 424.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_424/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_424
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_543"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_544 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7ff487952770>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff750159300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff750159300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff1184a1c60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff4879bceb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_424/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff4879bd420>, <keras.callbacks.ModelCheckpoint object at 0x7ff4879bd4e0>, <keras.callbacks.EarlyStopping object at 0x7ff4879bd750>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff4879bd780>, <keras.callbacks.TerminateOnNaN object at 0x7ff4879bd3c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_424/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 424/720 with hyperparameters:
timestamp = 2023-10-27 16:31:59.555827
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
LLVM ERROR: Unable to allocate section memory!
