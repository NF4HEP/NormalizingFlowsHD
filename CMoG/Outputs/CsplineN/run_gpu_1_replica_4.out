2023-09-23 15:06:33.207359: Importing os...
2023-09-23 15:06:33.207431: Importing sys...
2023-09-23 15:06:33.207449: Importing and initializing argparse...
Visible devices: [1]
2023-09-23 15:06:33.228045: Importing timer from timeit...
2023-09-23 15:06:33.229041: Setting env variables for tf import (only device [1] will be available)...
2023-09-23 15:06:33.229116: Importing numpy...
2023-09-23 15:06:33.387951: Importing pandas...
2023-09-23 15:06:33.523748: Importing shutil...
2023-09-23 15:06:33.523769: Importing subprocess...
2023-09-23 15:06:33.523775: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-23 15:06:35.223008: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-23 15:06:35.662565: Importing textwrap...
2023-09-23 15:06:35.662592: Importing timeit...
2023-09-23 15:06:35.662601: Importing traceback...
2023-09-23 15:06:35.662607: Importing typing...
2023-09-23 15:06:35.662616: Setting tf configs...
2023-09-23 15:06:39.206507: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-23 15:06:43.767924: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

===========
Generating train data for run 295.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1074400   
 r)                                                              
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7effe86184f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7effc0211540>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7effc0211540>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7effc0592fe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7effc0109510>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7effc0109a80>, <keras.callbacks.ModelCheckpoint object at 0x7effc0109bd0>, <keras.callbacks.EarlyStopping object at 0x7effc0109de0>, <keras.callbacks.ReduceLROnPlateau object at 0x7effc0109e10>, <keras.callbacks.TerminateOnNaN object at 0x7effc0109b40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_295/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-23 15:06:53.487630
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 85: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-23 15:10:16.186 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1571.9880, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 202s - loss: nan - MinusLogProbMetric: 1571.9880 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 202s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 295.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f03966b9ed0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f03a762f8e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f03a762f8e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7efc44212c80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f038e0cc970>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f038e0ccee0>, <keras.callbacks.ModelCheckpoint object at 0x7f038e0ccfa0>, <keras.callbacks.EarlyStopping object at 0x7f038e0cd210>, <keras.callbacks.ReduceLROnPlateau object at 0x7f038e0cd240>, <keras.callbacks.TerminateOnNaN object at 0x7f038e0cce80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_295/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-23 15:10:26.916637
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 121: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-23 15:14:07.544 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1704.8855, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 220s - loss: nan - MinusLogProbMetric: 1704.8855 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 220s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0001111111111111111.
===========
Generating train data for run 295.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7efdec2e34c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7efcd434b520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7efcd434b520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7efec839ed10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7efe444c7790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7efe444c7d00>, <keras.callbacks.ModelCheckpoint object at 0x7efe444c7dc0>, <keras.callbacks.EarlyStopping object at 0x7efe444c7fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7efe444c7cd0>, <keras.callbacks.TerminateOnNaN object at 0x7efe444c7f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_295/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-23 15:14:19.773783
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-23 15:18:27.275 
Epoch 1/1000 
	 loss: 2054.9185, MinusLogProbMetric: 2054.9185, val_loss: 1273.7853, val_MinusLogProbMetric: 1273.7853

Epoch 1: val_loss improved from inf to 1273.78528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 248s - loss: 2054.9185 - MinusLogProbMetric: 2054.9185 - val_loss: 1273.7853 - val_MinusLogProbMetric: 1273.7853 - lr: 1.1111e-04 - 248s/epoch - 1s/step
Epoch 2/1000
2023-09-23 15:19:43.099 
Epoch 2/1000 
	 loss: 770.0853, MinusLogProbMetric: 770.0853, val_loss: 532.4106, val_MinusLogProbMetric: 532.4106

Epoch 2: val_loss improved from 1273.78528 to 532.41064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 770.0853 - MinusLogProbMetric: 770.0853 - val_loss: 532.4106 - val_MinusLogProbMetric: 532.4106 - lr: 1.1111e-04 - 76s/epoch - 385ms/step
Epoch 3/1000
2023-09-23 15:20:58.736 
Epoch 3/1000 
	 loss: 429.5351, MinusLogProbMetric: 429.5351, val_loss: 405.6116, val_MinusLogProbMetric: 405.6116

Epoch 3: val_loss improved from 532.41064 to 405.61160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 429.5351 - MinusLogProbMetric: 429.5351 - val_loss: 405.6116 - val_MinusLogProbMetric: 405.6116 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 4/1000
2023-09-23 15:22:15.256 
Epoch 4/1000 
	 loss: 457.1241, MinusLogProbMetric: 457.1241, val_loss: 400.7206, val_MinusLogProbMetric: 400.7206

Epoch 4: val_loss improved from 405.61160 to 400.72061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 457.1241 - MinusLogProbMetric: 457.1241 - val_loss: 400.7206 - val_MinusLogProbMetric: 400.7206 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 5/1000
2023-09-23 15:23:31.058 
Epoch 5/1000 
	 loss: 385.3171, MinusLogProbMetric: 385.3171, val_loss: 345.1157, val_MinusLogProbMetric: 345.1157

Epoch 5: val_loss improved from 400.72061 to 345.11572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 385.3171 - MinusLogProbMetric: 385.3171 - val_loss: 345.1157 - val_MinusLogProbMetric: 345.1157 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 6/1000
2023-09-23 15:24:46.462 
Epoch 6/1000 
	 loss: 309.8152, MinusLogProbMetric: 309.8152, val_loss: 295.8203, val_MinusLogProbMetric: 295.8203

Epoch 6: val_loss improved from 345.11572 to 295.82031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 309.8152 - MinusLogProbMetric: 309.8152 - val_loss: 295.8203 - val_MinusLogProbMetric: 295.8203 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 7/1000
2023-09-23 15:26:02.606 
Epoch 7/1000 
	 loss: 313.1367, MinusLogProbMetric: 313.1367, val_loss: 289.2138, val_MinusLogProbMetric: 289.2138

Epoch 7: val_loss improved from 295.82031 to 289.21384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 313.1367 - MinusLogProbMetric: 313.1367 - val_loss: 289.2138 - val_MinusLogProbMetric: 289.2138 - lr: 1.1111e-04 - 76s/epoch - 388ms/step
Epoch 8/1000
2023-09-23 15:27:16.873 
Epoch 8/1000 
	 loss: 284.1653, MinusLogProbMetric: 284.1653, val_loss: 283.6356, val_MinusLogProbMetric: 283.6356

Epoch 8: val_loss improved from 289.21384 to 283.63559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 284.1653 - MinusLogProbMetric: 284.1653 - val_loss: 283.6356 - val_MinusLogProbMetric: 283.6356 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 130: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-23 15:28:09.706 
Epoch 9/1000 
	 loss: nan, MinusLogProbMetric: 270.9073, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 9: val_loss did not improve from 283.63559
196/196 - 52s - loss: nan - MinusLogProbMetric: 270.9073 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 52s/epoch - 263ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 3.703703703703703e-05.
===========
Generating train data for run 295.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7effb8115cc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7efcf9911bd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7efcf9911bd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7efe4441d5d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7efcf99dbd90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7efe0c164340>, <keras.callbacks.ModelCheckpoint object at 0x7efe0c164400>, <keras.callbacks.EarlyStopping object at 0x7efe0c164670>, <keras.callbacks.ReduceLROnPlateau object at 0x7efe0c1646a0>, <keras.callbacks.TerminateOnNaN object at 0x7efe0c1642e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-23 15:28:20.846037
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-23 15:32:19.304 
Epoch 1/1000 
	 loss: 295.3996, MinusLogProbMetric: 295.3996, val_loss: 260.4390, val_MinusLogProbMetric: 260.4390

Epoch 1: val_loss improved from inf to 260.43900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 240s - loss: 295.3996 - MinusLogProbMetric: 295.3996 - val_loss: 260.4390 - val_MinusLogProbMetric: 260.4390 - lr: 3.7037e-05 - 240s/epoch - 1s/step
Epoch 2/1000
2023-09-23 15:33:38.244 
Epoch 2/1000 
	 loss: 208.0427, MinusLogProbMetric: 208.0427, val_loss: 175.4140, val_MinusLogProbMetric: 175.4140

Epoch 2: val_loss improved from 260.43900 to 175.41402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 208.0427 - MinusLogProbMetric: 208.0427 - val_loss: 175.4140 - val_MinusLogProbMetric: 175.4140 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 3/1000
2023-09-23 15:34:57.103 
Epoch 3/1000 
	 loss: 161.9935, MinusLogProbMetric: 161.9935, val_loss: 151.5064, val_MinusLogProbMetric: 151.5064

Epoch 3: val_loss improved from 175.41402 to 151.50642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 161.9935 - MinusLogProbMetric: 161.9935 - val_loss: 151.5064 - val_MinusLogProbMetric: 151.5064 - lr: 3.7037e-05 - 79s/epoch - 401ms/step
Epoch 4/1000
2023-09-23 15:36:14.979 
Epoch 4/1000 
	 loss: 141.1520, MinusLogProbMetric: 141.1520, val_loss: 132.5034, val_MinusLogProbMetric: 132.5034

Epoch 4: val_loss improved from 151.50642 to 132.50343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 141.1520 - MinusLogProbMetric: 141.1520 - val_loss: 132.5034 - val_MinusLogProbMetric: 132.5034 - lr: 3.7037e-05 - 78s/epoch - 397ms/step
Epoch 5/1000
2023-09-23 15:37:32.926 
Epoch 5/1000 
	 loss: 159.1594, MinusLogProbMetric: 159.1594, val_loss: 195.0335, val_MinusLogProbMetric: 195.0335

Epoch 5: val_loss did not improve from 132.50343
196/196 - 77s - loss: 159.1594 - MinusLogProbMetric: 159.1594 - val_loss: 195.0335 - val_MinusLogProbMetric: 195.0335 - lr: 3.7037e-05 - 77s/epoch - 392ms/step
Epoch 6/1000
2023-09-23 15:38:53.196 
Epoch 6/1000 
	 loss: 171.5111, MinusLogProbMetric: 171.5111, val_loss: 167.1393, val_MinusLogProbMetric: 167.1393

Epoch 6: val_loss did not improve from 132.50343
196/196 - 80s - loss: 171.5111 - MinusLogProbMetric: 171.5111 - val_loss: 167.1393 - val_MinusLogProbMetric: 167.1393 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 7/1000
2023-09-23 15:40:11.030 
Epoch 7/1000 
	 loss: 190.3511, MinusLogProbMetric: 190.3511, val_loss: 188.2272, val_MinusLogProbMetric: 188.2272

Epoch 7: val_loss did not improve from 132.50343
196/196 - 78s - loss: 190.3511 - MinusLogProbMetric: 190.3511 - val_loss: 188.2272 - val_MinusLogProbMetric: 188.2272 - lr: 3.7037e-05 - 78s/epoch - 397ms/step
Epoch 8/1000
2023-09-23 15:41:28.013 
Epoch 8/1000 
	 loss: 170.7006, MinusLogProbMetric: 170.7006, val_loss: 147.4214, val_MinusLogProbMetric: 147.4214

Epoch 8: val_loss did not improve from 132.50343
196/196 - 77s - loss: 170.7006 - MinusLogProbMetric: 170.7006 - val_loss: 147.4214 - val_MinusLogProbMetric: 147.4214 - lr: 3.7037e-05 - 77s/epoch - 393ms/step
Epoch 9/1000
2023-09-23 15:42:44.595 
Epoch 9/1000 
	 loss: 139.3222, MinusLogProbMetric: 139.3222, val_loss: 128.7852, val_MinusLogProbMetric: 128.7852

Epoch 9: val_loss improved from 132.50343 to 128.78517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 139.3222 - MinusLogProbMetric: 139.3222 - val_loss: 128.7852 - val_MinusLogProbMetric: 128.7852 - lr: 3.7037e-05 - 78s/epoch - 398ms/step
Epoch 10/1000
2023-09-23 15:44:03.201 
Epoch 10/1000 
	 loss: 126.4221, MinusLogProbMetric: 126.4221, val_loss: 116.6625, val_MinusLogProbMetric: 116.6625

Epoch 10: val_loss improved from 128.78517 to 116.66251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 126.4221 - MinusLogProbMetric: 126.4221 - val_loss: 116.6625 - val_MinusLogProbMetric: 116.6625 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 11/1000
2023-09-23 15:45:20.742 
Epoch 11/1000 
	 loss: 110.5802, MinusLogProbMetric: 110.5802, val_loss: 106.5984, val_MinusLogProbMetric: 106.5984

Epoch 11: val_loss improved from 116.66251 to 106.59837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 110.5802 - MinusLogProbMetric: 110.5802 - val_loss: 106.5984 - val_MinusLogProbMetric: 106.5984 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 12/1000
2023-09-23 15:46:38.936 
Epoch 12/1000 
	 loss: 102.1551, MinusLogProbMetric: 102.1551, val_loss: 99.6338, val_MinusLogProbMetric: 99.6338

Epoch 12: val_loss improved from 106.59837 to 99.63379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 102.1551 - MinusLogProbMetric: 102.1551 - val_loss: 99.6338 - val_MinusLogProbMetric: 99.6338 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 13/1000
2023-09-23 15:47:58.009 
Epoch 13/1000 
	 loss: 121.2459, MinusLogProbMetric: 121.2459, val_loss: 148.4416, val_MinusLogProbMetric: 148.4416

Epoch 13: val_loss did not improve from 99.63379
196/196 - 78s - loss: 121.2459 - MinusLogProbMetric: 121.2459 - val_loss: 148.4416 - val_MinusLogProbMetric: 148.4416 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 14/1000
2023-09-23 15:49:14.574 
Epoch 14/1000 
	 loss: 126.9799, MinusLogProbMetric: 126.9799, val_loss: 114.4879, val_MinusLogProbMetric: 114.4879

Epoch 14: val_loss did not improve from 99.63379
196/196 - 77s - loss: 126.9799 - MinusLogProbMetric: 126.9799 - val_loss: 114.4879 - val_MinusLogProbMetric: 114.4879 - lr: 3.7037e-05 - 77s/epoch - 391ms/step
Epoch 15/1000
2023-09-23 15:50:32.362 
Epoch 15/1000 
	 loss: 112.3483, MinusLogProbMetric: 112.3483, val_loss: 104.0127, val_MinusLogProbMetric: 104.0127

Epoch 15: val_loss did not improve from 99.63379
196/196 - 78s - loss: 112.3483 - MinusLogProbMetric: 112.3483 - val_loss: 104.0127 - val_MinusLogProbMetric: 104.0127 - lr: 3.7037e-05 - 78s/epoch - 397ms/step
Epoch 16/1000
2023-09-23 15:51:49.926 
Epoch 16/1000 
	 loss: 98.4088, MinusLogProbMetric: 98.4088, val_loss: 93.3125, val_MinusLogProbMetric: 93.3125

Epoch 16: val_loss improved from 99.63379 to 93.31252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 98.4088 - MinusLogProbMetric: 98.4088 - val_loss: 93.3125 - val_MinusLogProbMetric: 93.3125 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 17/1000
2023-09-23 15:53:08.106 
Epoch 17/1000 
	 loss: 90.1162, MinusLogProbMetric: 90.1162, val_loss: 87.0177, val_MinusLogProbMetric: 87.0177

Epoch 17: val_loss improved from 93.31252 to 87.01773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 90.1162 - MinusLogProbMetric: 90.1162 - val_loss: 87.0177 - val_MinusLogProbMetric: 87.0177 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 18/1000
2023-09-23 15:54:27.166 
Epoch 18/1000 
	 loss: 84.6917, MinusLogProbMetric: 84.6917, val_loss: 82.5500, val_MinusLogProbMetric: 82.5500

Epoch 18: val_loss improved from 87.01773 to 82.55002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 84.6917 - MinusLogProbMetric: 84.6917 - val_loss: 82.5500 - val_MinusLogProbMetric: 82.5500 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 19/1000
2023-09-23 15:55:46.925 
Epoch 19/1000 
	 loss: 80.5515, MinusLogProbMetric: 80.5515, val_loss: 78.6534, val_MinusLogProbMetric: 78.6534

Epoch 19: val_loss improved from 82.55002 to 78.65338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 80.5515 - MinusLogProbMetric: 80.5515 - val_loss: 78.6534 - val_MinusLogProbMetric: 78.6534 - lr: 3.7037e-05 - 80s/epoch - 406ms/step
Epoch 20/1000
2023-09-23 15:57:06.615 
Epoch 20/1000 
	 loss: 77.1285, MinusLogProbMetric: 77.1285, val_loss: 76.8535, val_MinusLogProbMetric: 76.8535

Epoch 20: val_loss improved from 78.65338 to 76.85345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 77.1285 - MinusLogProbMetric: 77.1285 - val_loss: 76.8535 - val_MinusLogProbMetric: 76.8535 - lr: 3.7037e-05 - 80s/epoch - 407ms/step
Epoch 21/1000
2023-09-23 15:58:27.766 
Epoch 21/1000 
	 loss: 74.5347, MinusLogProbMetric: 74.5347, val_loss: 72.7526, val_MinusLogProbMetric: 72.7526

Epoch 21: val_loss improved from 76.85345 to 72.75263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 81s - loss: 74.5347 - MinusLogProbMetric: 74.5347 - val_loss: 72.7526 - val_MinusLogProbMetric: 72.7526 - lr: 3.7037e-05 - 81s/epoch - 414ms/step
Epoch 22/1000
2023-09-23 15:59:48.055 
Epoch 22/1000 
	 loss: 71.4307, MinusLogProbMetric: 71.4307, val_loss: 70.3110, val_MinusLogProbMetric: 70.3110

Epoch 22: val_loss improved from 72.75263 to 70.31104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 71.4307 - MinusLogProbMetric: 71.4307 - val_loss: 70.3110 - val_MinusLogProbMetric: 70.3110 - lr: 3.7037e-05 - 80s/epoch - 409ms/step
Epoch 23/1000
2023-09-23 16:01:07.728 
Epoch 23/1000 
	 loss: 68.9988, MinusLogProbMetric: 68.9988, val_loss: 68.2210, val_MinusLogProbMetric: 68.2210

Epoch 23: val_loss improved from 70.31104 to 68.22097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 68.9988 - MinusLogProbMetric: 68.9988 - val_loss: 68.2210 - val_MinusLogProbMetric: 68.2210 - lr: 3.7037e-05 - 80s/epoch - 407ms/step
Epoch 24/1000
2023-09-23 16:02:26.198 
Epoch 24/1000 
	 loss: 67.0524, MinusLogProbMetric: 67.0524, val_loss: 66.2989, val_MinusLogProbMetric: 66.2989

Epoch 24: val_loss improved from 68.22097 to 66.29887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 67.0524 - MinusLogProbMetric: 67.0524 - val_loss: 66.2989 - val_MinusLogProbMetric: 66.2989 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 25/1000
2023-09-23 16:03:43.802 
Epoch 25/1000 
	 loss: 65.0241, MinusLogProbMetric: 65.0241, val_loss: 64.2242, val_MinusLogProbMetric: 64.2242

Epoch 25: val_loss improved from 66.29887 to 64.22418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 65.0241 - MinusLogProbMetric: 65.0241 - val_loss: 64.2242 - val_MinusLogProbMetric: 64.2242 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 26/1000
2023-09-23 16:05:02.049 
Epoch 26/1000 
	 loss: 63.1548, MinusLogProbMetric: 63.1548, val_loss: 62.0735, val_MinusLogProbMetric: 62.0735

Epoch 26: val_loss improved from 64.22418 to 62.07351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 63.1548 - MinusLogProbMetric: 63.1548 - val_loss: 62.0735 - val_MinusLogProbMetric: 62.0735 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 27/1000
2023-09-23 16:06:20.888 
Epoch 27/1000 
	 loss: 61.4672, MinusLogProbMetric: 61.4672, val_loss: 61.3072, val_MinusLogProbMetric: 61.3072

Epoch 27: val_loss improved from 62.07351 to 61.30722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 61.4672 - MinusLogProbMetric: 61.4672 - val_loss: 61.3072 - val_MinusLogProbMetric: 61.3072 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 28/1000
2023-09-23 16:07:39.280 
Epoch 28/1000 
	 loss: 62.2421, MinusLogProbMetric: 62.2421, val_loss: 60.7524, val_MinusLogProbMetric: 60.7524

Epoch 28: val_loss improved from 61.30722 to 60.75241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 62.2421 - MinusLogProbMetric: 62.2421 - val_loss: 60.7524 - val_MinusLogProbMetric: 60.7524 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 29/1000
2023-09-23 16:08:56.919 
Epoch 29/1000 
	 loss: 59.4870, MinusLogProbMetric: 59.4870, val_loss: 58.4765, val_MinusLogProbMetric: 58.4765

Epoch 29: val_loss improved from 60.75241 to 58.47655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 59.4870 - MinusLogProbMetric: 59.4870 - val_loss: 58.4765 - val_MinusLogProbMetric: 58.4765 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 30/1000
2023-09-23 16:10:15.133 
Epoch 30/1000 
	 loss: 58.1049, MinusLogProbMetric: 58.1049, val_loss: 57.3813, val_MinusLogProbMetric: 57.3813

Epoch 30: val_loss improved from 58.47655 to 57.38126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 58.1049 - MinusLogProbMetric: 58.1049 - val_loss: 57.3813 - val_MinusLogProbMetric: 57.3813 - lr: 3.7037e-05 - 78s/epoch - 398ms/step
Epoch 31/1000
2023-09-23 16:11:33.769 
Epoch 31/1000 
	 loss: 57.2087, MinusLogProbMetric: 57.2087, val_loss: 56.8373, val_MinusLogProbMetric: 56.8373

Epoch 31: val_loss improved from 57.38126 to 56.83726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 57.2087 - MinusLogProbMetric: 57.2087 - val_loss: 56.8373 - val_MinusLogProbMetric: 56.8373 - lr: 3.7037e-05 - 79s/epoch - 401ms/step
Epoch 32/1000
2023-09-23 16:12:51.937 
Epoch 32/1000 
	 loss: 61.1964, MinusLogProbMetric: 61.1964, val_loss: 57.4485, val_MinusLogProbMetric: 57.4485

Epoch 32: val_loss did not improve from 56.83726
196/196 - 77s - loss: 61.1964 - MinusLogProbMetric: 61.1964 - val_loss: 57.4485 - val_MinusLogProbMetric: 57.4485 - lr: 3.7037e-05 - 77s/epoch - 393ms/step
Epoch 33/1000
2023-09-23 16:14:08.822 
Epoch 33/1000 
	 loss: 56.2246, MinusLogProbMetric: 56.2246, val_loss: 55.6873, val_MinusLogProbMetric: 55.6873

Epoch 33: val_loss improved from 56.83726 to 55.68726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 56.2246 - MinusLogProbMetric: 56.2246 - val_loss: 55.6873 - val_MinusLogProbMetric: 55.6873 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 34/1000
2023-09-23 16:15:26.930 
Epoch 34/1000 
	 loss: 56.8892, MinusLogProbMetric: 56.8892, val_loss: 55.2440, val_MinusLogProbMetric: 55.2440

Epoch 34: val_loss improved from 55.68726 to 55.24402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 56.8892 - MinusLogProbMetric: 56.8892 - val_loss: 55.2440 - val_MinusLogProbMetric: 55.2440 - lr: 3.7037e-05 - 78s/epoch - 398ms/step
Epoch 35/1000
2023-09-23 16:16:44.755 
Epoch 35/1000 
	 loss: 54.5968, MinusLogProbMetric: 54.5968, val_loss: 53.7503, val_MinusLogProbMetric: 53.7503

Epoch 35: val_loss improved from 55.24402 to 53.75031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 54.5968 - MinusLogProbMetric: 54.5968 - val_loss: 53.7503 - val_MinusLogProbMetric: 53.7503 - lr: 3.7037e-05 - 78s/epoch - 397ms/step
Epoch 36/1000
2023-09-23 16:18:03.580 
Epoch 36/1000 
	 loss: 53.1904, MinusLogProbMetric: 53.1904, val_loss: 52.6705, val_MinusLogProbMetric: 52.6705

Epoch 36: val_loss improved from 53.75031 to 52.67048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 53.1904 - MinusLogProbMetric: 53.1904 - val_loss: 52.6705 - val_MinusLogProbMetric: 52.6705 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 37/1000
2023-09-23 16:19:21.982 
Epoch 37/1000 
	 loss: 52.4446, MinusLogProbMetric: 52.4446, val_loss: 52.5019, val_MinusLogProbMetric: 52.5019

Epoch 37: val_loss improved from 52.67048 to 52.50194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 52.4446 - MinusLogProbMetric: 52.4446 - val_loss: 52.5019 - val_MinusLogProbMetric: 52.5019 - lr: 3.7037e-05 - 79s/epoch - 401ms/step
Epoch 38/1000
2023-09-23 16:20:39.398 
Epoch 38/1000 
	 loss: 51.7695, MinusLogProbMetric: 51.7695, val_loss: 51.6863, val_MinusLogProbMetric: 51.6863

Epoch 38: val_loss improved from 52.50194 to 51.68626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 51.7695 - MinusLogProbMetric: 51.7695 - val_loss: 51.6863 - val_MinusLogProbMetric: 51.6863 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 39/1000
2023-09-23 16:21:57.971 
Epoch 39/1000 
	 loss: 50.5644, MinusLogProbMetric: 50.5644, val_loss: 50.5285, val_MinusLogProbMetric: 50.5285

Epoch 39: val_loss improved from 51.68626 to 50.52855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 50.5644 - MinusLogProbMetric: 50.5644 - val_loss: 50.5285 - val_MinusLogProbMetric: 50.5285 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 40/1000
2023-09-23 16:23:16.379 
Epoch 40/1000 
	 loss: 49.9812, MinusLogProbMetric: 49.9812, val_loss: 49.6216, val_MinusLogProbMetric: 49.6216

Epoch 40: val_loss improved from 50.52855 to 49.62160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 49.9812 - MinusLogProbMetric: 49.9812 - val_loss: 49.6216 - val_MinusLogProbMetric: 49.6216 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 41/1000
2023-09-23 16:24:35.343 
Epoch 41/1000 
	 loss: 49.0409, MinusLogProbMetric: 49.0409, val_loss: 48.4442, val_MinusLogProbMetric: 48.4442

Epoch 41: val_loss improved from 49.62160 to 48.44422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 49.0409 - MinusLogProbMetric: 49.0409 - val_loss: 48.4442 - val_MinusLogProbMetric: 48.4442 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 42/1000
2023-09-23 16:25:54.244 
Epoch 42/1000 
	 loss: 48.3224, MinusLogProbMetric: 48.3224, val_loss: 49.4658, val_MinusLogProbMetric: 49.4658

Epoch 42: val_loss did not improve from 48.44422
196/196 - 78s - loss: 48.3224 - MinusLogProbMetric: 48.3224 - val_loss: 49.4658 - val_MinusLogProbMetric: 49.4658 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 43/1000
2023-09-23 16:27:11.826 
Epoch 43/1000 
	 loss: 47.6105, MinusLogProbMetric: 47.6105, val_loss: 47.3308, val_MinusLogProbMetric: 47.3308

Epoch 43: val_loss improved from 48.44422 to 47.33076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 47.6105 - MinusLogProbMetric: 47.6105 - val_loss: 47.3308 - val_MinusLogProbMetric: 47.3308 - lr: 3.7037e-05 - 79s/epoch - 402ms/step
Epoch 44/1000
2023-09-23 16:28:30.948 
Epoch 44/1000 
	 loss: 46.9285, MinusLogProbMetric: 46.9285, val_loss: 46.9250, val_MinusLogProbMetric: 46.9250

Epoch 44: val_loss improved from 47.33076 to 46.92498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 46.9285 - MinusLogProbMetric: 46.9285 - val_loss: 46.9250 - val_MinusLogProbMetric: 46.9250 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 45/1000
2023-09-23 16:29:49.791 
Epoch 45/1000 
	 loss: 46.2454, MinusLogProbMetric: 46.2454, val_loss: 46.0062, val_MinusLogProbMetric: 46.0062

Epoch 45: val_loss improved from 46.92498 to 46.00622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 46.2454 - MinusLogProbMetric: 46.2454 - val_loss: 46.0062 - val_MinusLogProbMetric: 46.0062 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 46/1000
2023-09-23 16:31:07.955 
Epoch 46/1000 
	 loss: 45.7655, MinusLogProbMetric: 45.7655, val_loss: 45.7375, val_MinusLogProbMetric: 45.7375

Epoch 46: val_loss improved from 46.00622 to 45.73748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 45.7655 - MinusLogProbMetric: 45.7655 - val_loss: 45.7375 - val_MinusLogProbMetric: 45.7375 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 47/1000
2023-09-23 16:32:26.672 
Epoch 47/1000 
	 loss: 45.1709, MinusLogProbMetric: 45.1709, val_loss: 45.1082, val_MinusLogProbMetric: 45.1082

Epoch 47: val_loss improved from 45.73748 to 45.10819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 45.1709 - MinusLogProbMetric: 45.1709 - val_loss: 45.1082 - val_MinusLogProbMetric: 45.1082 - lr: 3.7037e-05 - 79s/epoch - 401ms/step
Epoch 48/1000
2023-09-23 16:33:46.568 
Epoch 48/1000 
	 loss: 44.7026, MinusLogProbMetric: 44.7026, val_loss: 44.4122, val_MinusLogProbMetric: 44.4122

Epoch 48: val_loss improved from 45.10819 to 44.41216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 80s - loss: 44.7026 - MinusLogProbMetric: 44.7026 - val_loss: 44.4122 - val_MinusLogProbMetric: 44.4122 - lr: 3.7037e-05 - 80s/epoch - 407ms/step
Epoch 49/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 121: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-23 16:34:37.664 
Epoch 49/1000 
	 loss: nan, MinusLogProbMetric: 44.2425, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 49: val_loss did not improve from 44.41216
196/196 - 50s - loss: nan - MinusLogProbMetric: 44.2425 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 50s/epoch - 255ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.2345679012345677e-05.
===========
Generating train data for run 295.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_295/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_295
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4479074  7.1622143  6.6614265 ...  3.6775346  2.6042771  7.6607866]
 [ 1.558796   3.7643342  7.765504  ...  7.3985186  2.2847533  2.1737251]
 [ 2.023865   3.5498223  9.736043  ...  7.378399   2.9893079  1.7681804]
 ...
 [ 3.157689   3.8140697  9.128405  ...  7.3280826  3.3431854  1.6145309]
 [ 4.9035525  5.63367    1.3061054 ...  1.7336241  7.263643   1.3624198]
 [ 4.873677   6.0271535 -0.4844559 ...  1.4305246  6.6029577  1.4994687]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7efe0c27dc00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7efeac6565c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7efeac6565c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7efcd700a140>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7efcd7647c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7efe703481f0>, <keras.callbacks.ModelCheckpoint object at 0x7efe703482b0>, <keras.callbacks.EarlyStopping object at 0x7efe70348520>, <keras.callbacks.ReduceLROnPlateau object at 0x7efe70348550>, <keras.callbacks.TerminateOnNaN object at 0x7efe70348190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.3570018 ,  5.6703625 ,  0.48762453, ...,  0.76730525,
         6.4537516 ,  1.4289263 ],
       [ 4.8933635 ,  4.954244  , -0.7742728 , ...,  1.033045  ,
         7.5179    ,  1.3446053 ],
       [ 2.6192126 ,  3.6659784 ,  9.3581505 , ...,  6.4096394 ,
         2.8983781 ,  1.8163822 ],
       ...,
       [ 4.4286733 ,  5.582822  , -0.08033401, ...,  1.0755457 ,
         6.482341  ,  1.3259909 ],
       [ 5.2667904 ,  5.78094   ,  0.52473783, ...,  1.3584392 ,
         6.6371064 ,  1.4527634 ],
       [ 1.3711529 ,  3.985786  ,  8.434673  , ...,  6.650886  ,
         2.8764064 ,  2.071397  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 295/720 with hyperparameters:
timestamp = 2023-09-23 16:34:50.137286
ndims = 32
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.4479074  7.1622143  6.6614265  5.4466543  4.679343   6.5534453
  4.5869303  8.297877   9.431402   3.191049   9.163281   4.112924
  5.872972  10.008474   0.4502598  1.3920732 -0.6747763  8.487896
  9.224454   7.913118   8.449968   7.6997313  5.9028535  7.439646
  2.3174791  6.975212   1.6619607  9.750696   4.816901   3.6775346
  2.6042771  7.6607866]
Epoch 1/1000
2023-09-23 16:38:53.497 
Epoch 1/1000 
	 loss: 45.0426, MinusLogProbMetric: 45.0426, val_loss: 43.0456, val_MinusLogProbMetric: 43.0456

Epoch 1: val_loss improved from inf to 43.04565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 244s - loss: 45.0426 - MinusLogProbMetric: 45.0426 - val_loss: 43.0456 - val_MinusLogProbMetric: 43.0456 - lr: 1.2346e-05 - 244s/epoch - 1s/step
Epoch 2/1000
2023-09-23 16:40:11.405 
Epoch 2/1000 
	 loss: 42.4495, MinusLogProbMetric: 42.4495, val_loss: 41.9412, val_MinusLogProbMetric: 41.9412

Epoch 2: val_loss improved from 43.04565 to 41.94116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 42.4495 - MinusLogProbMetric: 42.4495 - val_loss: 41.9412 - val_MinusLogProbMetric: 41.9412 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 3/1000
2023-09-23 16:41:29.521 
Epoch 3/1000 
	 loss: 41.2207, MinusLogProbMetric: 41.2207, val_loss: 40.4064, val_MinusLogProbMetric: 40.4064

Epoch 3: val_loss improved from 41.94116 to 40.40636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 41.2207 - MinusLogProbMetric: 41.2207 - val_loss: 40.4064 - val_MinusLogProbMetric: 40.4064 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 4/1000
2023-09-23 16:42:47.558 
Epoch 4/1000 
	 loss: 41.0465, MinusLogProbMetric: 41.0465, val_loss: 40.1413, val_MinusLogProbMetric: 40.1413

Epoch 4: val_loss improved from 40.40636 to 40.14135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 41.0465 - MinusLogProbMetric: 41.0465 - val_loss: 40.1413 - val_MinusLogProbMetric: 40.1413 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 5/1000
2023-09-23 16:44:05.469 
Epoch 5/1000 
	 loss: 70.8812, MinusLogProbMetric: 70.8812, val_loss: 139.2097, val_MinusLogProbMetric: 139.2097

Epoch 5: val_loss did not improve from 40.14135
196/196 - 77s - loss: 70.8812 - MinusLogProbMetric: 70.8812 - val_loss: 139.2097 - val_MinusLogProbMetric: 139.2097 - lr: 1.2346e-05 - 77s/epoch - 390ms/step
Epoch 6/1000
2023-09-23 16:45:22.469 
Epoch 6/1000 
	 loss: 126.2276, MinusLogProbMetric: 126.2276, val_loss: 90.4879, val_MinusLogProbMetric: 90.4879

Epoch 6: val_loss did not improve from 40.14135
196/196 - 77s - loss: 126.2276 - MinusLogProbMetric: 126.2276 - val_loss: 90.4879 - val_MinusLogProbMetric: 90.4879 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 7/1000
2023-09-23 16:46:39.473 
Epoch 7/1000 
	 loss: 60.4945, MinusLogProbMetric: 60.4945, val_loss: 41.9451, val_MinusLogProbMetric: 41.9451

Epoch 7: val_loss did not improve from 40.14135
196/196 - 77s - loss: 60.4945 - MinusLogProbMetric: 60.4945 - val_loss: 41.9451 - val_MinusLogProbMetric: 41.9451 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 8/1000
2023-09-23 16:47:56.590 
Epoch 8/1000 
	 loss: 39.5685, MinusLogProbMetric: 39.5685, val_loss: 39.4864, val_MinusLogProbMetric: 39.4864

Epoch 8: val_loss improved from 40.14135 to 39.48644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 39.5685 - MinusLogProbMetric: 39.5685 - val_loss: 39.4864 - val_MinusLogProbMetric: 39.4864 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 9/1000
2023-09-23 16:49:14.867 
Epoch 9/1000 
	 loss: 37.6738, MinusLogProbMetric: 37.6738, val_loss: 36.9844, val_MinusLogProbMetric: 36.9844

Epoch 9: val_loss improved from 39.48644 to 36.98437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 37.6738 - MinusLogProbMetric: 37.6738 - val_loss: 36.9844 - val_MinusLogProbMetric: 36.9844 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 10/1000
2023-09-23 16:50:33.902 
Epoch 10/1000 
	 loss: 36.5862, MinusLogProbMetric: 36.5862, val_loss: 36.2602, val_MinusLogProbMetric: 36.2602

Epoch 10: val_loss improved from 36.98437 to 36.26019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 36.5862 - MinusLogProbMetric: 36.5862 - val_loss: 36.2602 - val_MinusLogProbMetric: 36.2602 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 11/1000
2023-09-23 16:51:51.445 
Epoch 11/1000 
	 loss: 35.7865, MinusLogProbMetric: 35.7865, val_loss: 35.9972, val_MinusLogProbMetric: 35.9972

Epoch 11: val_loss improved from 36.26019 to 35.99715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 35.7865 - MinusLogProbMetric: 35.7865 - val_loss: 35.9972 - val_MinusLogProbMetric: 35.9972 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 12/1000
2023-09-23 16:53:09.345 
Epoch 12/1000 
	 loss: 35.3765, MinusLogProbMetric: 35.3765, val_loss: 35.4045, val_MinusLogProbMetric: 35.4045

Epoch 12: val_loss improved from 35.99715 to 35.40450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 35.3765 - MinusLogProbMetric: 35.3765 - val_loss: 35.4045 - val_MinusLogProbMetric: 35.4045 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 13/1000
2023-09-23 16:54:27.488 
Epoch 13/1000 
	 loss: 34.7094, MinusLogProbMetric: 34.7094, val_loss: 34.5807, val_MinusLogProbMetric: 34.5807

Epoch 13: val_loss improved from 35.40450 to 34.58071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 34.7094 - MinusLogProbMetric: 34.7094 - val_loss: 34.5807 - val_MinusLogProbMetric: 34.5807 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 14/1000
2023-09-23 16:55:45.704 
Epoch 14/1000 
	 loss: 34.0899, MinusLogProbMetric: 34.0899, val_loss: 34.2545, val_MinusLogProbMetric: 34.2545

Epoch 14: val_loss improved from 34.58071 to 34.25447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 34.0899 - MinusLogProbMetric: 34.0899 - val_loss: 34.2545 - val_MinusLogProbMetric: 34.2545 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 15/1000
2023-09-23 16:57:03.931 
Epoch 15/1000 
	 loss: 34.2114, MinusLogProbMetric: 34.2114, val_loss: 34.0934, val_MinusLogProbMetric: 34.0934

Epoch 15: val_loss improved from 34.25447 to 34.09341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 34.2114 - MinusLogProbMetric: 34.2114 - val_loss: 34.0934 - val_MinusLogProbMetric: 34.0934 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 16/1000
2023-09-23 16:58:22.257 
Epoch 16/1000 
	 loss: 33.5147, MinusLogProbMetric: 33.5147, val_loss: 33.3162, val_MinusLogProbMetric: 33.3162

Epoch 16: val_loss improved from 34.09341 to 33.31616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 33.5147 - MinusLogProbMetric: 33.5147 - val_loss: 33.3162 - val_MinusLogProbMetric: 33.3162 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 17/1000
2023-09-23 16:59:40.404 
Epoch 17/1000 
	 loss: 33.0363, MinusLogProbMetric: 33.0363, val_loss: 32.8508, val_MinusLogProbMetric: 32.8508

Epoch 17: val_loss improved from 33.31616 to 32.85084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 33.0363 - MinusLogProbMetric: 33.0363 - val_loss: 32.8508 - val_MinusLogProbMetric: 32.8508 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 18/1000
2023-09-23 17:00:58.711 
Epoch 18/1000 
	 loss: 32.6706, MinusLogProbMetric: 32.6706, val_loss: 32.7089, val_MinusLogProbMetric: 32.7089

Epoch 18: val_loss improved from 32.85084 to 32.70891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 32.6706 - MinusLogProbMetric: 32.6706 - val_loss: 32.7089 - val_MinusLogProbMetric: 32.7089 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 19/1000
2023-09-23 17:02:16.308 
Epoch 19/1000 
	 loss: 32.4069, MinusLogProbMetric: 32.4069, val_loss: 32.3969, val_MinusLogProbMetric: 32.3969

Epoch 19: val_loss improved from 32.70891 to 32.39695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 32.4069 - MinusLogProbMetric: 32.4069 - val_loss: 32.3969 - val_MinusLogProbMetric: 32.3969 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 20/1000
2023-09-23 17:03:34.707 
Epoch 20/1000 
	 loss: 32.9933, MinusLogProbMetric: 32.9933, val_loss: 32.2660, val_MinusLogProbMetric: 32.2660

Epoch 20: val_loss improved from 32.39695 to 32.26596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 32.9933 - MinusLogProbMetric: 32.9933 - val_loss: 32.2660 - val_MinusLogProbMetric: 32.2660 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 21/1000
2023-09-23 17:04:53.352 
Epoch 21/1000 
	 loss: 31.8036, MinusLogProbMetric: 31.8036, val_loss: 31.8751, val_MinusLogProbMetric: 31.8751

Epoch 21: val_loss improved from 32.26596 to 31.87508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 31.8036 - MinusLogProbMetric: 31.8036 - val_loss: 31.8751 - val_MinusLogProbMetric: 31.8751 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 22/1000
2023-09-23 17:06:11.410 
Epoch 22/1000 
	 loss: 31.5199, MinusLogProbMetric: 31.5199, val_loss: 31.2930, val_MinusLogProbMetric: 31.2930

Epoch 22: val_loss improved from 31.87508 to 31.29301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 31.5199 - MinusLogProbMetric: 31.5199 - val_loss: 31.2930 - val_MinusLogProbMetric: 31.2930 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 23/1000
2023-09-23 17:07:28.575 
Epoch 23/1000 
	 loss: 31.1626, MinusLogProbMetric: 31.1626, val_loss: 31.0608, val_MinusLogProbMetric: 31.0608

Epoch 23: val_loss improved from 31.29301 to 31.06077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 31.1626 - MinusLogProbMetric: 31.1626 - val_loss: 31.0608 - val_MinusLogProbMetric: 31.0608 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 24/1000
2023-09-23 17:08:46.612 
Epoch 24/1000 
	 loss: 30.8630, MinusLogProbMetric: 30.8630, val_loss: 31.2558, val_MinusLogProbMetric: 31.2558

Epoch 24: val_loss did not improve from 31.06077
196/196 - 77s - loss: 30.8630 - MinusLogProbMetric: 30.8630 - val_loss: 31.2558 - val_MinusLogProbMetric: 31.2558 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 25/1000
2023-09-23 17:10:02.978 
Epoch 25/1000 
	 loss: 32.9899, MinusLogProbMetric: 32.9899, val_loss: 44.0858, val_MinusLogProbMetric: 44.0858

Epoch 25: val_loss did not improve from 31.06077
196/196 - 76s - loss: 32.9899 - MinusLogProbMetric: 32.9899 - val_loss: 44.0858 - val_MinusLogProbMetric: 44.0858 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 26/1000
2023-09-23 17:11:20.577 
Epoch 26/1000 
	 loss: 34.5226, MinusLogProbMetric: 34.5226, val_loss: 32.3448, val_MinusLogProbMetric: 32.3448

Epoch 26: val_loss did not improve from 31.06077
196/196 - 78s - loss: 34.5226 - MinusLogProbMetric: 34.5226 - val_loss: 32.3448 - val_MinusLogProbMetric: 32.3448 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 27/1000
2023-09-23 17:12:36.724 
Epoch 27/1000 
	 loss: 31.6681, MinusLogProbMetric: 31.6681, val_loss: 31.2607, val_MinusLogProbMetric: 31.2607

Epoch 27: val_loss did not improve from 31.06077
196/196 - 76s - loss: 31.6681 - MinusLogProbMetric: 31.6681 - val_loss: 31.2607 - val_MinusLogProbMetric: 31.2607 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 28/1000
2023-09-23 17:13:54.208 
Epoch 28/1000 
	 loss: 30.9910, MinusLogProbMetric: 30.9910, val_loss: 31.0753, val_MinusLogProbMetric: 31.0753

Epoch 28: val_loss did not improve from 31.06077
196/196 - 77s - loss: 30.9910 - MinusLogProbMetric: 30.9910 - val_loss: 31.0753 - val_MinusLogProbMetric: 31.0753 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 29/1000
2023-09-23 17:15:11.945 
Epoch 29/1000 
	 loss: 30.5178, MinusLogProbMetric: 30.5178, val_loss: 30.6844, val_MinusLogProbMetric: 30.6844

Epoch 29: val_loss improved from 31.06077 to 30.68443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 79s - loss: 30.5178 - MinusLogProbMetric: 30.5178 - val_loss: 30.6844 - val_MinusLogProbMetric: 30.6844 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 30/1000
2023-09-23 17:16:30.050 
Epoch 30/1000 
	 loss: 59.4767, MinusLogProbMetric: 59.4767, val_loss: 51.8186, val_MinusLogProbMetric: 51.8186

Epoch 30: val_loss did not improve from 30.68443
196/196 - 77s - loss: 59.4767 - MinusLogProbMetric: 59.4767 - val_loss: 51.8186 - val_MinusLogProbMetric: 51.8186 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 31/1000
2023-09-23 17:17:46.573 
Epoch 31/1000 
	 loss: 47.1684, MinusLogProbMetric: 47.1684, val_loss: 44.5233, val_MinusLogProbMetric: 44.5233

Epoch 31: val_loss did not improve from 30.68443
196/196 - 77s - loss: 47.1684 - MinusLogProbMetric: 47.1684 - val_loss: 44.5233 - val_MinusLogProbMetric: 44.5233 - lr: 1.2346e-05 - 77s/epoch - 390ms/step
Epoch 32/1000
2023-09-23 17:19:04.040 
Epoch 32/1000 
	 loss: 57.7608, MinusLogProbMetric: 57.7608, val_loss: 48.6983, val_MinusLogProbMetric: 48.6983

Epoch 32: val_loss did not improve from 30.68443
196/196 - 77s - loss: 57.7608 - MinusLogProbMetric: 57.7608 - val_loss: 48.6983 - val_MinusLogProbMetric: 48.6983 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 33/1000
2023-09-23 17:20:21.442 
Epoch 33/1000 
	 loss: 44.5055, MinusLogProbMetric: 44.5055, val_loss: 41.9638, val_MinusLogProbMetric: 41.9638

Epoch 33: val_loss did not improve from 30.68443
196/196 - 77s - loss: 44.5055 - MinusLogProbMetric: 44.5055 - val_loss: 41.9638 - val_MinusLogProbMetric: 41.9638 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 34/1000
2023-09-23 17:21:37.722 
Epoch 34/1000 
	 loss: 40.6011, MinusLogProbMetric: 40.6011, val_loss: 39.5772, val_MinusLogProbMetric: 39.5772

Epoch 34: val_loss did not improve from 30.68443
196/196 - 76s - loss: 40.6011 - MinusLogProbMetric: 40.6011 - val_loss: 39.5772 - val_MinusLogProbMetric: 39.5772 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 35/1000
2023-09-23 17:22:53.954 
Epoch 35/1000 
	 loss: 39.0579, MinusLogProbMetric: 39.0579, val_loss: 38.1704, val_MinusLogProbMetric: 38.1704

Epoch 35: val_loss did not improve from 30.68443
196/196 - 76s - loss: 39.0579 - MinusLogProbMetric: 39.0579 - val_loss: 38.1704 - val_MinusLogProbMetric: 38.1704 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 36/1000
2023-09-23 17:24:10.806 
Epoch 36/1000 
	 loss: 37.5008, MinusLogProbMetric: 37.5008, val_loss: 36.8184, val_MinusLogProbMetric: 36.8184

Epoch 36: val_loss did not improve from 30.68443
196/196 - 77s - loss: 37.5008 - MinusLogProbMetric: 37.5008 - val_loss: 36.8184 - val_MinusLogProbMetric: 36.8184 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 37/1000
2023-09-23 17:25:27.629 
Epoch 37/1000 
	 loss: 36.3893, MinusLogProbMetric: 36.3893, val_loss: 36.0126, val_MinusLogProbMetric: 36.0126

Epoch 37: val_loss did not improve from 30.68443
196/196 - 77s - loss: 36.3893 - MinusLogProbMetric: 36.3893 - val_loss: 36.0126 - val_MinusLogProbMetric: 36.0126 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 38/1000
2023-09-23 17:26:45.436 
Epoch 38/1000 
	 loss: 35.9700, MinusLogProbMetric: 35.9700, val_loss: 35.2161, val_MinusLogProbMetric: 35.2161

Epoch 38: val_loss did not improve from 30.68443
196/196 - 78s - loss: 35.9700 - MinusLogProbMetric: 35.9700 - val_loss: 35.2161 - val_MinusLogProbMetric: 35.2161 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 39/1000
2023-09-23 17:28:03.906 
Epoch 39/1000 
	 loss: 35.6803, MinusLogProbMetric: 35.6803, val_loss: 35.7862, val_MinusLogProbMetric: 35.7862

Epoch 39: val_loss did not improve from 30.68443
196/196 - 78s - loss: 35.6803 - MinusLogProbMetric: 35.6803 - val_loss: 35.7862 - val_MinusLogProbMetric: 35.7862 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 40/1000
2023-09-23 17:29:21.439 
Epoch 40/1000 
	 loss: 34.8082, MinusLogProbMetric: 34.8082, val_loss: 34.0040, val_MinusLogProbMetric: 34.0040

Epoch 40: val_loss did not improve from 30.68443
196/196 - 78s - loss: 34.8082 - MinusLogProbMetric: 34.8082 - val_loss: 34.0040 - val_MinusLogProbMetric: 34.0040 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 41/1000
2023-09-23 17:30:38.948 
Epoch 41/1000 
	 loss: 33.7350, MinusLogProbMetric: 33.7350, val_loss: 38.0178, val_MinusLogProbMetric: 38.0178

Epoch 41: val_loss did not improve from 30.68443
196/196 - 78s - loss: 33.7350 - MinusLogProbMetric: 33.7350 - val_loss: 38.0178 - val_MinusLogProbMetric: 38.0178 - lr: 1.2346e-05 - 78s/epoch - 395ms/step
Epoch 42/1000
2023-09-23 17:31:57.091 
Epoch 42/1000 
	 loss: 33.6518, MinusLogProbMetric: 33.6518, val_loss: 33.0112, val_MinusLogProbMetric: 33.0112

Epoch 42: val_loss did not improve from 30.68443
196/196 - 78s - loss: 33.6518 - MinusLogProbMetric: 33.6518 - val_loss: 33.0112 - val_MinusLogProbMetric: 33.0112 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 43/1000
2023-09-23 17:33:14.944 
Epoch 43/1000 
	 loss: 32.7348, MinusLogProbMetric: 32.7348, val_loss: 33.0247, val_MinusLogProbMetric: 33.0247

Epoch 43: val_loss did not improve from 30.68443
196/196 - 78s - loss: 32.7348 - MinusLogProbMetric: 32.7348 - val_loss: 33.0247 - val_MinusLogProbMetric: 33.0247 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 44/1000
2023-09-23 17:34:32.922 
Epoch 44/1000 
	 loss: 32.3766, MinusLogProbMetric: 32.3766, val_loss: 32.4126, val_MinusLogProbMetric: 32.4126

Epoch 44: val_loss did not improve from 30.68443
196/196 - 78s - loss: 32.3766 - MinusLogProbMetric: 32.3766 - val_loss: 32.4126 - val_MinusLogProbMetric: 32.4126 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 45/1000
2023-09-23 17:35:50.038 
Epoch 45/1000 
	 loss: 31.9378, MinusLogProbMetric: 31.9378, val_loss: 31.6816, val_MinusLogProbMetric: 31.6816

Epoch 45: val_loss did not improve from 30.68443
196/196 - 77s - loss: 31.9378 - MinusLogProbMetric: 31.9378 - val_loss: 31.6816 - val_MinusLogProbMetric: 31.6816 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 46/1000
2023-09-23 17:37:07.544 
Epoch 46/1000 
	 loss: 31.5157, MinusLogProbMetric: 31.5157, val_loss: 31.4155, val_MinusLogProbMetric: 31.4155

Epoch 46: val_loss did not improve from 30.68443
196/196 - 78s - loss: 31.5157 - MinusLogProbMetric: 31.5157 - val_loss: 31.4155 - val_MinusLogProbMetric: 31.4155 - lr: 1.2346e-05 - 78s/epoch - 395ms/step
Epoch 47/1000
2023-09-23 17:38:23.876 
Epoch 47/1000 
	 loss: 31.1908, MinusLogProbMetric: 31.1908, val_loss: 31.3206, val_MinusLogProbMetric: 31.3206

Epoch 47: val_loss did not improve from 30.68443
196/196 - 76s - loss: 31.1908 - MinusLogProbMetric: 31.1908 - val_loss: 31.3206 - val_MinusLogProbMetric: 31.3206 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 48/1000
2023-09-23 17:39:40.436 
Epoch 48/1000 
	 loss: 30.8945, MinusLogProbMetric: 30.8945, val_loss: 30.9113, val_MinusLogProbMetric: 30.9113

Epoch 48: val_loss did not improve from 30.68443
196/196 - 77s - loss: 30.8945 - MinusLogProbMetric: 30.8945 - val_loss: 30.9113 - val_MinusLogProbMetric: 30.9113 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 49/1000
2023-09-23 17:40:57.013 
Epoch 49/1000 
	 loss: 30.6227, MinusLogProbMetric: 30.6227, val_loss: 30.5281, val_MinusLogProbMetric: 30.5281

Epoch 49: val_loss improved from 30.68443 to 30.52813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 30.6227 - MinusLogProbMetric: 30.6227 - val_loss: 30.5281 - val_MinusLogProbMetric: 30.5281 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 50/1000
2023-09-23 17:42:14.912 
Epoch 50/1000 
	 loss: 30.5011, MinusLogProbMetric: 30.5011, val_loss: 30.4637, val_MinusLogProbMetric: 30.4637

Epoch 50: val_loss improved from 30.52813 to 30.46365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 30.5011 - MinusLogProbMetric: 30.5011 - val_loss: 30.4637 - val_MinusLogProbMetric: 30.4637 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 51/1000
2023-09-23 17:43:32.106 
Epoch 51/1000 
	 loss: 30.1616, MinusLogProbMetric: 30.1616, val_loss: 30.1629, val_MinusLogProbMetric: 30.1629

Epoch 51: val_loss improved from 30.46365 to 30.16291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 30.1616 - MinusLogProbMetric: 30.1616 - val_loss: 30.1629 - val_MinusLogProbMetric: 30.1629 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 52/1000
2023-09-23 17:44:49.766 
Epoch 52/1000 
	 loss: 29.9187, MinusLogProbMetric: 29.9187, val_loss: 29.8741, val_MinusLogProbMetric: 29.8741

Epoch 52: val_loss improved from 30.16291 to 29.87411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 29.9187 - MinusLogProbMetric: 29.9187 - val_loss: 29.8741 - val_MinusLogProbMetric: 29.8741 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 53/1000
2023-09-23 17:46:07.628 
Epoch 53/1000 
	 loss: 29.7070, MinusLogProbMetric: 29.7070, val_loss: 29.7550, val_MinusLogProbMetric: 29.7550

Epoch 53: val_loss improved from 29.87411 to 29.75502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 29.7070 - MinusLogProbMetric: 29.7070 - val_loss: 29.7550 - val_MinusLogProbMetric: 29.7550 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 54/1000
2023-09-23 17:47:25.252 
Epoch 54/1000 
	 loss: 29.4609, MinusLogProbMetric: 29.4609, val_loss: 29.5272, val_MinusLogProbMetric: 29.5272

Epoch 54: val_loss improved from 29.75502 to 29.52719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 29.4609 - MinusLogProbMetric: 29.4609 - val_loss: 29.5272 - val_MinusLogProbMetric: 29.5272 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 55/1000
2023-09-23 17:48:41.120 
Epoch 55/1000 
	 loss: 29.2989, MinusLogProbMetric: 29.2989, val_loss: 29.3176, val_MinusLogProbMetric: 29.3176

Epoch 55: val_loss improved from 29.52719 to 29.31756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 29.2989 - MinusLogProbMetric: 29.2989 - val_loss: 29.3176 - val_MinusLogProbMetric: 29.3176 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 56/1000
2023-09-23 17:49:58.575 
Epoch 56/1000 
	 loss: 29.1494, MinusLogProbMetric: 29.1494, val_loss: 29.1698, val_MinusLogProbMetric: 29.1698

Epoch 56: val_loss improved from 29.31756 to 29.16982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 29.1494 - MinusLogProbMetric: 29.1494 - val_loss: 29.1698 - val_MinusLogProbMetric: 29.1698 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 57/1000
2023-09-23 17:51:14.229 
Epoch 57/1000 
	 loss: 28.9931, MinusLogProbMetric: 28.9931, val_loss: 29.0103, val_MinusLogProbMetric: 29.0103

Epoch 57: val_loss improved from 29.16982 to 29.01025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 28.9931 - MinusLogProbMetric: 28.9931 - val_loss: 29.0103 - val_MinusLogProbMetric: 29.0103 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 58/1000
2023-09-23 17:52:30.453 
Epoch 58/1000 
	 loss: 28.7970, MinusLogProbMetric: 28.7970, val_loss: 28.8464, val_MinusLogProbMetric: 28.8464

Epoch 58: val_loss improved from 29.01025 to 28.84644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 28.7970 - MinusLogProbMetric: 28.7970 - val_loss: 28.8464 - val_MinusLogProbMetric: 28.8464 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 59/1000
2023-09-23 17:53:47.713 
Epoch 59/1000 
	 loss: 28.6041, MinusLogProbMetric: 28.6041, val_loss: 28.6458, val_MinusLogProbMetric: 28.6458

Epoch 59: val_loss improved from 28.84644 to 28.64582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 28.6041 - MinusLogProbMetric: 28.6041 - val_loss: 28.6458 - val_MinusLogProbMetric: 28.6458 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 60/1000
2023-09-23 17:55:04.724 
Epoch 60/1000 
	 loss: 28.4705, MinusLogProbMetric: 28.4705, val_loss: 28.5140, val_MinusLogProbMetric: 28.5140

Epoch 60: val_loss improved from 28.64582 to 28.51398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 28.4705 - MinusLogProbMetric: 28.4705 - val_loss: 28.5140 - val_MinusLogProbMetric: 28.5140 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 61/1000
2023-09-23 17:56:21.873 
Epoch 61/1000 
	 loss: 28.3228, MinusLogProbMetric: 28.3228, val_loss: 28.3151, val_MinusLogProbMetric: 28.3151

Epoch 61: val_loss improved from 28.51398 to 28.31511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 28.3228 - MinusLogProbMetric: 28.3228 - val_loss: 28.3151 - val_MinusLogProbMetric: 28.3151 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 62/1000
2023-09-23 17:57:39.063 
Epoch 62/1000 
	 loss: 28.1778, MinusLogProbMetric: 28.1778, val_loss: 28.2185, val_MinusLogProbMetric: 28.2185

Epoch 62: val_loss improved from 28.31511 to 28.21855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 28.1778 - MinusLogProbMetric: 28.1778 - val_loss: 28.2185 - val_MinusLogProbMetric: 28.2185 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 63/1000
2023-09-23 17:58:56.253 
Epoch 63/1000 
	 loss: 28.5998, MinusLogProbMetric: 28.5998, val_loss: 28.5796, val_MinusLogProbMetric: 28.5796

Epoch 63: val_loss did not improve from 28.21855
196/196 - 76s - loss: 28.5998 - MinusLogProbMetric: 28.5998 - val_loss: 28.5796 - val_MinusLogProbMetric: 28.5796 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 64/1000
2023-09-23 18:00:12.620 
Epoch 64/1000 
	 loss: 28.0278, MinusLogProbMetric: 28.0278, val_loss: 28.0928, val_MinusLogProbMetric: 28.0928

Epoch 64: val_loss improved from 28.21855 to 28.09279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 78s - loss: 28.0278 - MinusLogProbMetric: 28.0278 - val_loss: 28.0928 - val_MinusLogProbMetric: 28.0928 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 65/1000
2023-09-23 18:01:29.402 
Epoch 65/1000 
	 loss: 27.9487, MinusLogProbMetric: 27.9487, val_loss: 27.9895, val_MinusLogProbMetric: 27.9895

Epoch 65: val_loss improved from 28.09279 to 27.98950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 27.9487 - MinusLogProbMetric: 27.9487 - val_loss: 27.9895 - val_MinusLogProbMetric: 27.9895 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 66/1000
2023-09-23 18:02:45.998 
Epoch 66/1000 
	 loss: 27.7029, MinusLogProbMetric: 27.7029, val_loss: 27.7530, val_MinusLogProbMetric: 27.7530

Epoch 66: val_loss improved from 27.98950 to 27.75298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 27.7029 - MinusLogProbMetric: 27.7029 - val_loss: 27.7530 - val_MinusLogProbMetric: 27.7530 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 67/1000
2023-09-23 18:04:03.399 
Epoch 67/1000 
	 loss: 50.5084, MinusLogProbMetric: 50.5084, val_loss: 218.7910, val_MinusLogProbMetric: 218.7910

Epoch 67: val_loss did not improve from 27.75298
196/196 - 76s - loss: 50.5084 - MinusLogProbMetric: 50.5084 - val_loss: 218.7910 - val_MinusLogProbMetric: 218.7910 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 68/1000
2023-09-23 18:05:18.759 
Epoch 68/1000 
	 loss: 89.0751, MinusLogProbMetric: 89.0751, val_loss: 65.4447, val_MinusLogProbMetric: 65.4447

Epoch 68: val_loss did not improve from 27.75298
196/196 - 75s - loss: 89.0751 - MinusLogProbMetric: 89.0751 - val_loss: 65.4447 - val_MinusLogProbMetric: 65.4447 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 69/1000
2023-09-23 18:06:34.467 
Epoch 69/1000 
	 loss: 62.2434, MinusLogProbMetric: 62.2434, val_loss: 55.9188, val_MinusLogProbMetric: 55.9188

Epoch 69: val_loss did not improve from 27.75298
196/196 - 76s - loss: 62.2434 - MinusLogProbMetric: 62.2434 - val_loss: 55.9188 - val_MinusLogProbMetric: 55.9188 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 70/1000
2023-09-23 18:07:49.308 
Epoch 70/1000 
	 loss: 53.5362, MinusLogProbMetric: 53.5362, val_loss: 50.7275, val_MinusLogProbMetric: 50.7275

Epoch 70: val_loss did not improve from 27.75298
196/196 - 75s - loss: 53.5362 - MinusLogProbMetric: 53.5362 - val_loss: 50.7275 - val_MinusLogProbMetric: 50.7275 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 71/1000
2023-09-23 18:09:04.796 
Epoch 71/1000 
	 loss: 50.3756, MinusLogProbMetric: 50.3756, val_loss: 48.0649, val_MinusLogProbMetric: 48.0649

Epoch 71: val_loss did not improve from 27.75298
196/196 - 75s - loss: 50.3756 - MinusLogProbMetric: 50.3756 - val_loss: 48.0649 - val_MinusLogProbMetric: 48.0649 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 72/1000
2023-09-23 18:10:20.511 
Epoch 72/1000 
	 loss: 46.5439, MinusLogProbMetric: 46.5439, val_loss: 45.5226, val_MinusLogProbMetric: 45.5226

Epoch 72: val_loss did not improve from 27.75298
196/196 - 76s - loss: 46.5439 - MinusLogProbMetric: 46.5439 - val_loss: 45.5226 - val_MinusLogProbMetric: 45.5226 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 73/1000
2023-09-23 18:11:35.618 
Epoch 73/1000 
	 loss: 44.5600, MinusLogProbMetric: 44.5600, val_loss: 43.7929, val_MinusLogProbMetric: 43.7929

Epoch 73: val_loss did not improve from 27.75298
196/196 - 75s - loss: 44.5600 - MinusLogProbMetric: 44.5600 - val_loss: 43.7929 - val_MinusLogProbMetric: 43.7929 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 74/1000
2023-09-23 18:12:51.413 
Epoch 74/1000 
	 loss: 45.2888, MinusLogProbMetric: 45.2888, val_loss: 81.6976, val_MinusLogProbMetric: 81.6976

Epoch 74: val_loss did not improve from 27.75298
196/196 - 76s - loss: 45.2888 - MinusLogProbMetric: 45.2888 - val_loss: 81.6976 - val_MinusLogProbMetric: 81.6976 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 75/1000
2023-09-23 18:14:06.597 
Epoch 75/1000 
	 loss: 49.7340, MinusLogProbMetric: 49.7340, val_loss: 44.7814, val_MinusLogProbMetric: 44.7814

Epoch 75: val_loss did not improve from 27.75298
196/196 - 75s - loss: 49.7340 - MinusLogProbMetric: 49.7340 - val_loss: 44.7814 - val_MinusLogProbMetric: 44.7814 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 76/1000
2023-09-23 18:15:21.937 
Epoch 76/1000 
	 loss: 42.9249, MinusLogProbMetric: 42.9249, val_loss: 41.4688, val_MinusLogProbMetric: 41.4688

Epoch 76: val_loss did not improve from 27.75298
196/196 - 75s - loss: 42.9249 - MinusLogProbMetric: 42.9249 - val_loss: 41.4688 - val_MinusLogProbMetric: 41.4688 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 77/1000
2023-09-23 18:16:37.380 
Epoch 77/1000 
	 loss: 41.0233, MinusLogProbMetric: 41.0233, val_loss: 40.4463, val_MinusLogProbMetric: 40.4463

Epoch 77: val_loss did not improve from 27.75298
196/196 - 75s - loss: 41.0233 - MinusLogProbMetric: 41.0233 - val_loss: 40.4463 - val_MinusLogProbMetric: 40.4463 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 78/1000
2023-09-23 18:17:53.117 
Epoch 78/1000 
	 loss: 39.9077, MinusLogProbMetric: 39.9077, val_loss: 39.4650, val_MinusLogProbMetric: 39.4650

Epoch 78: val_loss did not improve from 27.75298
196/196 - 76s - loss: 39.9077 - MinusLogProbMetric: 39.9077 - val_loss: 39.4650 - val_MinusLogProbMetric: 39.4650 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 79/1000
2023-09-23 18:19:08.722 
Epoch 79/1000 
	 loss: 38.8880, MinusLogProbMetric: 38.8880, val_loss: 38.3911, val_MinusLogProbMetric: 38.3911

Epoch 79: val_loss did not improve from 27.75298
196/196 - 76s - loss: 38.8880 - MinusLogProbMetric: 38.8880 - val_loss: 38.3911 - val_MinusLogProbMetric: 38.3911 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 80/1000
2023-09-23 18:20:24.340 
Epoch 80/1000 
	 loss: 38.1870, MinusLogProbMetric: 38.1870, val_loss: 39.7965, val_MinusLogProbMetric: 39.7965

Epoch 80: val_loss did not improve from 27.75298
196/196 - 76s - loss: 38.1870 - MinusLogProbMetric: 38.1870 - val_loss: 39.7965 - val_MinusLogProbMetric: 39.7965 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 81/1000
2023-09-23 18:21:40.033 
Epoch 81/1000 
	 loss: 44.0470, MinusLogProbMetric: 44.0470, val_loss: 39.5132, val_MinusLogProbMetric: 39.5132

Epoch 81: val_loss did not improve from 27.75298
196/196 - 76s - loss: 44.0470 - MinusLogProbMetric: 44.0470 - val_loss: 39.5132 - val_MinusLogProbMetric: 39.5132 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 82/1000
2023-09-23 18:22:55.770 
Epoch 82/1000 
	 loss: 37.7877, MinusLogProbMetric: 37.7877, val_loss: 37.0253, val_MinusLogProbMetric: 37.0253

Epoch 82: val_loss did not improve from 27.75298
196/196 - 76s - loss: 37.7877 - MinusLogProbMetric: 37.7877 - val_loss: 37.0253 - val_MinusLogProbMetric: 37.0253 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 83/1000
2023-09-23 18:24:12.135 
Epoch 83/1000 
	 loss: 36.7837, MinusLogProbMetric: 36.7837, val_loss: 36.4879, val_MinusLogProbMetric: 36.4879

Epoch 83: val_loss did not improve from 27.75298
196/196 - 76s - loss: 36.7837 - MinusLogProbMetric: 36.7837 - val_loss: 36.4879 - val_MinusLogProbMetric: 36.4879 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 84/1000
2023-09-23 18:25:27.065 
Epoch 84/1000 
	 loss: 35.9177, MinusLogProbMetric: 35.9177, val_loss: 36.3686, val_MinusLogProbMetric: 36.3686

Epoch 84: val_loss did not improve from 27.75298
196/196 - 75s - loss: 35.9177 - MinusLogProbMetric: 35.9177 - val_loss: 36.3686 - val_MinusLogProbMetric: 36.3686 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 85/1000
2023-09-23 18:26:42.535 
Epoch 85/1000 
	 loss: 35.7078, MinusLogProbMetric: 35.7078, val_loss: 35.1586, val_MinusLogProbMetric: 35.1586

Epoch 85: val_loss did not improve from 27.75298
196/196 - 75s - loss: 35.7078 - MinusLogProbMetric: 35.7078 - val_loss: 35.1586 - val_MinusLogProbMetric: 35.1586 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 86/1000
2023-09-23 18:27:58.491 
Epoch 86/1000 
	 loss: 36.8978, MinusLogProbMetric: 36.8978, val_loss: 35.9368, val_MinusLogProbMetric: 35.9368

Epoch 86: val_loss did not improve from 27.75298
196/196 - 76s - loss: 36.8978 - MinusLogProbMetric: 36.8978 - val_loss: 35.9368 - val_MinusLogProbMetric: 35.9368 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 87/1000
2023-09-23 18:29:14.755 
Epoch 87/1000 
	 loss: 35.3143, MinusLogProbMetric: 35.3143, val_loss: 34.7193, val_MinusLogProbMetric: 34.7193

Epoch 87: val_loss did not improve from 27.75298
196/196 - 76s - loss: 35.3143 - MinusLogProbMetric: 35.3143 - val_loss: 34.7193 - val_MinusLogProbMetric: 34.7193 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 88/1000
2023-09-23 18:30:29.629 
Epoch 88/1000 
	 loss: 34.5078, MinusLogProbMetric: 34.5078, val_loss: 34.3357, val_MinusLogProbMetric: 34.3357

Epoch 88: val_loss did not improve from 27.75298
196/196 - 75s - loss: 34.5078 - MinusLogProbMetric: 34.5078 - val_loss: 34.3357 - val_MinusLogProbMetric: 34.3357 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 89/1000
2023-09-23 18:31:44.959 
Epoch 89/1000 
	 loss: 33.6685, MinusLogProbMetric: 33.6685, val_loss: 33.3843, val_MinusLogProbMetric: 33.3843

Epoch 89: val_loss did not improve from 27.75298
196/196 - 75s - loss: 33.6685 - MinusLogProbMetric: 33.6685 - val_loss: 33.3843 - val_MinusLogProbMetric: 33.3843 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 90/1000
2023-09-23 18:32:59.986 
Epoch 90/1000 
	 loss: 34.1605, MinusLogProbMetric: 34.1605, val_loss: 34.3905, val_MinusLogProbMetric: 34.3905

Epoch 90: val_loss did not improve from 27.75298
196/196 - 75s - loss: 34.1605 - MinusLogProbMetric: 34.1605 - val_loss: 34.3905 - val_MinusLogProbMetric: 34.3905 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 91/1000
2023-09-23 18:34:14.883 
Epoch 91/1000 
	 loss: 46.3015, MinusLogProbMetric: 46.3015, val_loss: 40.0674, val_MinusLogProbMetric: 40.0674

Epoch 91: val_loss did not improve from 27.75298
196/196 - 75s - loss: 46.3015 - MinusLogProbMetric: 46.3015 - val_loss: 40.0674 - val_MinusLogProbMetric: 40.0674 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 92/1000
2023-09-23 18:35:30.696 
Epoch 92/1000 
	 loss: 36.8271, MinusLogProbMetric: 36.8271, val_loss: 34.9014, val_MinusLogProbMetric: 34.9014

Epoch 92: val_loss did not improve from 27.75298
196/196 - 76s - loss: 36.8271 - MinusLogProbMetric: 36.8271 - val_loss: 34.9014 - val_MinusLogProbMetric: 34.9014 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 93/1000
2023-09-23 18:36:45.902 
Epoch 93/1000 
	 loss: 34.8731, MinusLogProbMetric: 34.8731, val_loss: 33.2946, val_MinusLogProbMetric: 33.2946

Epoch 93: val_loss did not improve from 27.75298
196/196 - 75s - loss: 34.8731 - MinusLogProbMetric: 34.8731 - val_loss: 33.2946 - val_MinusLogProbMetric: 33.2946 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 94/1000
2023-09-23 18:38:02.663 
Epoch 94/1000 
	 loss: 34.0566, MinusLogProbMetric: 34.0566, val_loss: 41.7882, val_MinusLogProbMetric: 41.7882

Epoch 94: val_loss did not improve from 27.75298
196/196 - 77s - loss: 34.0566 - MinusLogProbMetric: 34.0566 - val_loss: 41.7882 - val_MinusLogProbMetric: 41.7882 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 95/1000
2023-09-23 18:39:19.155 
Epoch 95/1000 
	 loss: 35.8514, MinusLogProbMetric: 35.8514, val_loss: 32.6484, val_MinusLogProbMetric: 32.6484

Epoch 95: val_loss did not improve from 27.75298
196/196 - 76s - loss: 35.8514 - MinusLogProbMetric: 35.8514 - val_loss: 32.6484 - val_MinusLogProbMetric: 32.6484 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 96/1000
2023-09-23 18:40:34.262 
Epoch 96/1000 
	 loss: 31.3811, MinusLogProbMetric: 31.3811, val_loss: 31.0182, val_MinusLogProbMetric: 31.0182

Epoch 96: val_loss did not improve from 27.75298
196/196 - 75s - loss: 31.3811 - MinusLogProbMetric: 31.3811 - val_loss: 31.0182 - val_MinusLogProbMetric: 31.0182 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 97/1000
2023-09-23 18:41:50.125 
Epoch 97/1000 
	 loss: 32.2844, MinusLogProbMetric: 32.2844, val_loss: 30.7693, val_MinusLogProbMetric: 30.7693

Epoch 97: val_loss did not improve from 27.75298
196/196 - 76s - loss: 32.2844 - MinusLogProbMetric: 32.2844 - val_loss: 30.7693 - val_MinusLogProbMetric: 30.7693 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 98/1000
2023-09-23 18:43:05.611 
Epoch 98/1000 
	 loss: 30.4772, MinusLogProbMetric: 30.4772, val_loss: 30.3868, val_MinusLogProbMetric: 30.3868

Epoch 98: val_loss did not improve from 27.75298
196/196 - 75s - loss: 30.4772 - MinusLogProbMetric: 30.4772 - val_loss: 30.3868 - val_MinusLogProbMetric: 30.3868 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 99/1000
2023-09-23 18:44:20.951 
Epoch 99/1000 
	 loss: 30.2580, MinusLogProbMetric: 30.2580, val_loss: 30.2105, val_MinusLogProbMetric: 30.2105

Epoch 99: val_loss did not improve from 27.75298
196/196 - 75s - loss: 30.2580 - MinusLogProbMetric: 30.2580 - val_loss: 30.2105 - val_MinusLogProbMetric: 30.2105 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 100/1000
2023-09-23 18:45:36.892 
Epoch 100/1000 
	 loss: 33.4250, MinusLogProbMetric: 33.4250, val_loss: 31.3382, val_MinusLogProbMetric: 31.3382

Epoch 100: val_loss did not improve from 27.75298
196/196 - 76s - loss: 33.4250 - MinusLogProbMetric: 33.4250 - val_loss: 31.3382 - val_MinusLogProbMetric: 31.3382 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 101/1000
2023-09-23 18:46:53.015 
Epoch 101/1000 
	 loss: 30.2279, MinusLogProbMetric: 30.2279, val_loss: 29.9162, val_MinusLogProbMetric: 29.9162

Epoch 101: val_loss did not improve from 27.75298
196/196 - 76s - loss: 30.2279 - MinusLogProbMetric: 30.2279 - val_loss: 29.9162 - val_MinusLogProbMetric: 29.9162 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 102/1000
2023-09-23 18:48:08.114 
Epoch 102/1000 
	 loss: 29.6469, MinusLogProbMetric: 29.6469, val_loss: 29.6131, val_MinusLogProbMetric: 29.6131

Epoch 102: val_loss did not improve from 27.75298
196/196 - 75s - loss: 29.6469 - MinusLogProbMetric: 29.6469 - val_loss: 29.6131 - val_MinusLogProbMetric: 29.6131 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 103/1000
2023-09-23 18:49:23.352 
Epoch 103/1000 
	 loss: 29.4050, MinusLogProbMetric: 29.4050, val_loss: 29.4742, val_MinusLogProbMetric: 29.4742

Epoch 103: val_loss did not improve from 27.75298
196/196 - 75s - loss: 29.4050 - MinusLogProbMetric: 29.4050 - val_loss: 29.4742 - val_MinusLogProbMetric: 29.4742 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 104/1000
2023-09-23 18:50:38.766 
Epoch 104/1000 
	 loss: 29.2102, MinusLogProbMetric: 29.2102, val_loss: 29.1918, val_MinusLogProbMetric: 29.1918

Epoch 104: val_loss did not improve from 27.75298
196/196 - 75s - loss: 29.2102 - MinusLogProbMetric: 29.2102 - val_loss: 29.1918 - val_MinusLogProbMetric: 29.1918 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 105/1000
2023-09-23 18:51:54.251 
Epoch 105/1000 
	 loss: 29.0087, MinusLogProbMetric: 29.0087, val_loss: 29.0461, val_MinusLogProbMetric: 29.0461

Epoch 105: val_loss did not improve from 27.75298
196/196 - 75s - loss: 29.0087 - MinusLogProbMetric: 29.0087 - val_loss: 29.0461 - val_MinusLogProbMetric: 29.0461 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 106/1000
2023-09-23 18:53:09.218 
Epoch 106/1000 
	 loss: 28.8439, MinusLogProbMetric: 28.8439, val_loss: 28.8812, val_MinusLogProbMetric: 28.8812

Epoch 106: val_loss did not improve from 27.75298
196/196 - 75s - loss: 28.8439 - MinusLogProbMetric: 28.8439 - val_loss: 28.8812 - val_MinusLogProbMetric: 28.8812 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 107/1000
2023-09-23 18:54:24.604 
Epoch 107/1000 
	 loss: 28.6865, MinusLogProbMetric: 28.6865, val_loss: 28.7211, val_MinusLogProbMetric: 28.7211

Epoch 107: val_loss did not improve from 27.75298
196/196 - 75s - loss: 28.6865 - MinusLogProbMetric: 28.6865 - val_loss: 28.7211 - val_MinusLogProbMetric: 28.7211 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 108/1000
2023-09-23 18:55:39.791 
Epoch 108/1000 
	 loss: 28.5474, MinusLogProbMetric: 28.5474, val_loss: 28.5703, val_MinusLogProbMetric: 28.5703

Epoch 108: val_loss did not improve from 27.75298
196/196 - 75s - loss: 28.5474 - MinusLogProbMetric: 28.5474 - val_loss: 28.5703 - val_MinusLogProbMetric: 28.5703 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 109/1000
2023-09-23 18:56:54.901 
Epoch 109/1000 
	 loss: 28.4270, MinusLogProbMetric: 28.4270, val_loss: 28.4595, val_MinusLogProbMetric: 28.4595

Epoch 109: val_loss did not improve from 27.75298
196/196 - 75s - loss: 28.4270 - MinusLogProbMetric: 28.4270 - val_loss: 28.4595 - val_MinusLogProbMetric: 28.4595 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 110/1000
2023-09-23 18:58:10.758 
Epoch 110/1000 
	 loss: 28.2708, MinusLogProbMetric: 28.2708, val_loss: 28.3435, val_MinusLogProbMetric: 28.3435

Epoch 110: val_loss did not improve from 27.75298
196/196 - 76s - loss: 28.2708 - MinusLogProbMetric: 28.2708 - val_loss: 28.3435 - val_MinusLogProbMetric: 28.3435 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 111/1000
2023-09-23 18:59:25.966 
Epoch 111/1000 
	 loss: 28.8703, MinusLogProbMetric: 28.8703, val_loss: 28.2839, val_MinusLogProbMetric: 28.2839

Epoch 111: val_loss did not improve from 27.75298
196/196 - 75s - loss: 28.8703 - MinusLogProbMetric: 28.8703 - val_loss: 28.2839 - val_MinusLogProbMetric: 28.2839 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 112/1000
2023-09-23 19:00:41.004 
Epoch 112/1000 
	 loss: 28.1123, MinusLogProbMetric: 28.1123, val_loss: 28.1983, val_MinusLogProbMetric: 28.1983

Epoch 112: val_loss did not improve from 27.75298
196/196 - 75s - loss: 28.1123 - MinusLogProbMetric: 28.1123 - val_loss: 28.1983 - val_MinusLogProbMetric: 28.1983 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 113/1000
2023-09-23 19:01:57.023 
Epoch 113/1000 
	 loss: 27.9479, MinusLogProbMetric: 27.9479, val_loss: 27.9753, val_MinusLogProbMetric: 27.9753

Epoch 113: val_loss did not improve from 27.75298
196/196 - 76s - loss: 27.9479 - MinusLogProbMetric: 27.9479 - val_loss: 27.9753 - val_MinusLogProbMetric: 27.9753 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 114/1000
2023-09-23 19:03:11.660 
Epoch 114/1000 
	 loss: 27.8894, MinusLogProbMetric: 27.8894, val_loss: 27.8604, val_MinusLogProbMetric: 27.8604

Epoch 114: val_loss did not improve from 27.75298
196/196 - 75s - loss: 27.8894 - MinusLogProbMetric: 27.8894 - val_loss: 27.8604 - val_MinusLogProbMetric: 27.8604 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 115/1000
2023-09-23 19:04:24.056 
Epoch 115/1000 
	 loss: 27.7192, MinusLogProbMetric: 27.7192, val_loss: 27.7702, val_MinusLogProbMetric: 27.7702

Epoch 115: val_loss did not improve from 27.75298
196/196 - 72s - loss: 27.7192 - MinusLogProbMetric: 27.7192 - val_loss: 27.7702 - val_MinusLogProbMetric: 27.7702 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 116/1000
2023-09-23 19:05:39.303 
Epoch 116/1000 
	 loss: 27.6119, MinusLogProbMetric: 27.6119, val_loss: 27.6521, val_MinusLogProbMetric: 27.6521

Epoch 116: val_loss improved from 27.75298 to 27.65207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 27.6119 - MinusLogProbMetric: 27.6119 - val_loss: 27.6521 - val_MinusLogProbMetric: 27.6521 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 117/1000
2023-09-23 19:06:54.995 
Epoch 117/1000 
	 loss: 27.6585, MinusLogProbMetric: 27.6585, val_loss: 31.7098, val_MinusLogProbMetric: 31.7098

Epoch 117: val_loss did not improve from 27.65207
196/196 - 74s - loss: 27.6585 - MinusLogProbMetric: 27.6585 - val_loss: 31.7098 - val_MinusLogProbMetric: 31.7098 - lr: 1.2346e-05 - 74s/epoch - 380ms/step
Epoch 118/1000
2023-09-23 19:08:09.251 
Epoch 118/1000 
	 loss: 28.1031, MinusLogProbMetric: 28.1031, val_loss: 36.7119, val_MinusLogProbMetric: 36.7119

Epoch 118: val_loss did not improve from 27.65207
196/196 - 74s - loss: 28.1031 - MinusLogProbMetric: 28.1031 - val_loss: 36.7119 - val_MinusLogProbMetric: 36.7119 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 119/1000
2023-09-23 19:09:23.999 
Epoch 119/1000 
	 loss: 28.2090, MinusLogProbMetric: 28.2090, val_loss: 27.3860, val_MinusLogProbMetric: 27.3860

Epoch 119: val_loss improved from 27.65207 to 27.38596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 28.2090 - MinusLogProbMetric: 28.2090 - val_loss: 27.3860 - val_MinusLogProbMetric: 27.3860 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 120/1000
2023-09-23 19:10:39.881 
Epoch 120/1000 
	 loss: 27.7618, MinusLogProbMetric: 27.7618, val_loss: 27.8442, val_MinusLogProbMetric: 27.8442

Epoch 120: val_loss did not improve from 27.38596
196/196 - 75s - loss: 27.7618 - MinusLogProbMetric: 27.7618 - val_loss: 27.8442 - val_MinusLogProbMetric: 27.8442 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 121/1000
2023-09-23 19:11:54.266 
Epoch 121/1000 
	 loss: 27.3159, MinusLogProbMetric: 27.3159, val_loss: 27.2936, val_MinusLogProbMetric: 27.2936

Epoch 121: val_loss improved from 27.38596 to 27.29361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 27.3159 - MinusLogProbMetric: 27.3159 - val_loss: 27.2936 - val_MinusLogProbMetric: 27.2936 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 122/1000
2023-09-23 19:13:11.083 
Epoch 122/1000 
	 loss: 27.2538, MinusLogProbMetric: 27.2538, val_loss: 27.1353, val_MinusLogProbMetric: 27.1353

Epoch 122: val_loss improved from 27.29361 to 27.13528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 27.2538 - MinusLogProbMetric: 27.2538 - val_loss: 27.1353 - val_MinusLogProbMetric: 27.1353 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 123/1000
2023-09-23 19:14:27.626 
Epoch 123/1000 
	 loss: 27.3130, MinusLogProbMetric: 27.3130, val_loss: 27.0671, val_MinusLogProbMetric: 27.0671

Epoch 123: val_loss improved from 27.13528 to 27.06713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 27.3130 - MinusLogProbMetric: 27.3130 - val_loss: 27.0671 - val_MinusLogProbMetric: 27.0671 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 124/1000
2023-09-23 19:15:43.606 
Epoch 124/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 26.9725, val_MinusLogProbMetric: 26.9725

Epoch 124: val_loss improved from 27.06713 to 26.97251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 26.9725 - val_MinusLogProbMetric: 26.9725 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 125/1000
2023-09-23 19:17:00.078 
Epoch 125/1000 
	 loss: 27.4533, MinusLogProbMetric: 27.4533, val_loss: 26.9413, val_MinusLogProbMetric: 26.9413

Epoch 125: val_loss improved from 26.97251 to 26.94133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 27.4533 - MinusLogProbMetric: 27.4533 - val_loss: 26.9413 - val_MinusLogProbMetric: 26.9413 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 126/1000
2023-09-23 19:18:15.947 
Epoch 126/1000 
	 loss: 26.7399, MinusLogProbMetric: 26.7399, val_loss: 26.8131, val_MinusLogProbMetric: 26.8131

Epoch 126: val_loss improved from 26.94133 to 26.81311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 26.7399 - MinusLogProbMetric: 26.7399 - val_loss: 26.8131 - val_MinusLogProbMetric: 26.8131 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 127/1000
2023-09-23 19:19:31.928 
Epoch 127/1000 
	 loss: 26.6533, MinusLogProbMetric: 26.6533, val_loss: 26.7335, val_MinusLogProbMetric: 26.7335

Epoch 127: val_loss improved from 26.81311 to 26.73354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 26.6533 - MinusLogProbMetric: 26.6533 - val_loss: 26.7335 - val_MinusLogProbMetric: 26.7335 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 128/1000
2023-09-23 19:20:47.923 
Epoch 128/1000 
	 loss: 26.5602, MinusLogProbMetric: 26.5602, val_loss: 26.6240, val_MinusLogProbMetric: 26.6240

Epoch 128: val_loss improved from 26.73354 to 26.62401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 26.5602 - MinusLogProbMetric: 26.5602 - val_loss: 26.6240 - val_MinusLogProbMetric: 26.6240 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 129/1000
2023-09-23 19:22:03.862 
Epoch 129/1000 
	 loss: 26.4738, MinusLogProbMetric: 26.4738, val_loss: 26.5311, val_MinusLogProbMetric: 26.5311

Epoch 129: val_loss improved from 26.62401 to 26.53105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 26.4738 - MinusLogProbMetric: 26.4738 - val_loss: 26.5311 - val_MinusLogProbMetric: 26.5311 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 130/1000
2023-09-23 19:23:18.986 
Epoch 130/1000 
	 loss: 26.3915, MinusLogProbMetric: 26.3915, val_loss: 26.4754, val_MinusLogProbMetric: 26.4754

Epoch 130: val_loss improved from 26.53105 to 26.47539, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 26.3915 - MinusLogProbMetric: 26.3915 - val_loss: 26.4754 - val_MinusLogProbMetric: 26.4754 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 131/1000
2023-09-23 19:24:34.248 
Epoch 131/1000 
	 loss: 26.3854, MinusLogProbMetric: 26.3854, val_loss: 26.3937, val_MinusLogProbMetric: 26.3937

Epoch 131: val_loss improved from 26.47539 to 26.39370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 26.3854 - MinusLogProbMetric: 26.3854 - val_loss: 26.3937 - val_MinusLogProbMetric: 26.3937 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 132/1000
2023-09-23 19:25:49.544 
Epoch 132/1000 
	 loss: 26.4005, MinusLogProbMetric: 26.4005, val_loss: 26.3525, val_MinusLogProbMetric: 26.3525

Epoch 132: val_loss improved from 26.39370 to 26.35252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 26.4005 - MinusLogProbMetric: 26.4005 - val_loss: 26.3525 - val_MinusLogProbMetric: 26.3525 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 133/1000
2023-09-23 19:27:06.178 
Epoch 133/1000 
	 loss: 26.1904, MinusLogProbMetric: 26.1904, val_loss: 26.2781, val_MinusLogProbMetric: 26.2781

Epoch 133: val_loss improved from 26.35252 to 26.27811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 77s - loss: 26.1904 - MinusLogProbMetric: 26.1904 - val_loss: 26.2781 - val_MinusLogProbMetric: 26.2781 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 134/1000
2023-09-23 19:28:21.970 
Epoch 134/1000 
	 loss: 26.1168, MinusLogProbMetric: 26.1168, val_loss: 26.1863, val_MinusLogProbMetric: 26.1863

Epoch 134: val_loss improved from 26.27811 to 26.18628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 26.1168 - MinusLogProbMetric: 26.1168 - val_loss: 26.1863 - val_MinusLogProbMetric: 26.1863 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 135/1000
2023-09-23 19:29:37.853 
Epoch 135/1000 
	 loss: 26.0480, MinusLogProbMetric: 26.0480, val_loss: 26.1352, val_MinusLogProbMetric: 26.1352

Epoch 135: val_loss improved from 26.18628 to 26.13519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 26.0480 - MinusLogProbMetric: 26.0480 - val_loss: 26.1352 - val_MinusLogProbMetric: 26.1352 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 136/1000
2023-09-23 19:30:53.043 
Epoch 136/1000 
	 loss: 25.9860, MinusLogProbMetric: 25.9860, val_loss: 26.0500, val_MinusLogProbMetric: 26.0500

Epoch 136: val_loss improved from 26.13519 to 26.05004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.9860 - MinusLogProbMetric: 25.9860 - val_loss: 26.0500 - val_MinusLogProbMetric: 26.0500 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 137/1000
2023-09-23 19:32:09.617 
Epoch 137/1000 
	 loss: 25.9468, MinusLogProbMetric: 25.9468, val_loss: 26.0602, val_MinusLogProbMetric: 26.0602

Epoch 137: val_loss did not improve from 26.05004
196/196 - 75s - loss: 25.9468 - MinusLogProbMetric: 25.9468 - val_loss: 26.0602 - val_MinusLogProbMetric: 26.0602 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 138/1000
2023-09-23 19:33:23.776 
Epoch 138/1000 
	 loss: 25.8531, MinusLogProbMetric: 25.8531, val_loss: 25.9478, val_MinusLogProbMetric: 25.9478

Epoch 138: val_loss improved from 26.05004 to 25.94785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.8531 - MinusLogProbMetric: 25.8531 - val_loss: 25.9478 - val_MinusLogProbMetric: 25.9478 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 139/1000
2023-09-23 19:34:39.340 
Epoch 139/1000 
	 loss: 25.7843, MinusLogProbMetric: 25.7843, val_loss: 25.8756, val_MinusLogProbMetric: 25.8756

Epoch 139: val_loss improved from 25.94785 to 25.87560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 25.7843 - MinusLogProbMetric: 25.7843 - val_loss: 25.8756 - val_MinusLogProbMetric: 25.8756 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 140/1000
2023-09-23 19:35:55.323 
Epoch 140/1000 
	 loss: 25.7602, MinusLogProbMetric: 25.7602, val_loss: 25.8473, val_MinusLogProbMetric: 25.8473

Epoch 140: val_loss improved from 25.87560 to 25.84728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 25.7602 - MinusLogProbMetric: 25.7602 - val_loss: 25.8473 - val_MinusLogProbMetric: 25.8473 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 141/1000
2023-09-23 19:37:10.968 
Epoch 141/1000 
	 loss: 25.7377, MinusLogProbMetric: 25.7377, val_loss: 25.7627, val_MinusLogProbMetric: 25.7627

Epoch 141: val_loss improved from 25.84728 to 25.76271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 25.7377 - MinusLogProbMetric: 25.7377 - val_loss: 25.7627 - val_MinusLogProbMetric: 25.7627 - lr: 1.2346e-05 - 76s/epoch - 385ms/step
Epoch 142/1000
2023-09-23 19:38:26.577 
Epoch 142/1000 
	 loss: 25.6096, MinusLogProbMetric: 25.6096, val_loss: 25.6901, val_MinusLogProbMetric: 25.6901

Epoch 142: val_loss improved from 25.76271 to 25.69008, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 25.6096 - MinusLogProbMetric: 25.6096 - val_loss: 25.6901 - val_MinusLogProbMetric: 25.6901 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 143/1000
2023-09-23 19:39:41.683 
Epoch 143/1000 
	 loss: 25.5527, MinusLogProbMetric: 25.5527, val_loss: 25.6603, val_MinusLogProbMetric: 25.6603

Epoch 143: val_loss improved from 25.69008 to 25.66032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.5527 - MinusLogProbMetric: 25.5527 - val_loss: 25.6603 - val_MinusLogProbMetric: 25.6603 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 144/1000
2023-09-23 19:40:56.790 
Epoch 144/1000 
	 loss: 25.5134, MinusLogProbMetric: 25.5134, val_loss: 25.5965, val_MinusLogProbMetric: 25.5965

Epoch 144: val_loss improved from 25.66032 to 25.59653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.5134 - MinusLogProbMetric: 25.5134 - val_loss: 25.5965 - val_MinusLogProbMetric: 25.5965 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 145/1000
2023-09-23 19:42:12.223 
Epoch 145/1000 
	 loss: 25.4555, MinusLogProbMetric: 25.4555, val_loss: 25.5176, val_MinusLogProbMetric: 25.5176

Epoch 145: val_loss improved from 25.59653 to 25.51755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.4555 - MinusLogProbMetric: 25.4555 - val_loss: 25.5176 - val_MinusLogProbMetric: 25.5176 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 146/1000
2023-09-23 19:43:27.451 
Epoch 146/1000 
	 loss: 25.3929, MinusLogProbMetric: 25.3929, val_loss: 25.4913, val_MinusLogProbMetric: 25.4913

Epoch 146: val_loss improved from 25.51755 to 25.49126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.3929 - MinusLogProbMetric: 25.3929 - val_loss: 25.4913 - val_MinusLogProbMetric: 25.4913 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 147/1000
2023-09-23 19:44:42.495 
Epoch 147/1000 
	 loss: 25.3277, MinusLogProbMetric: 25.3277, val_loss: 25.4418, val_MinusLogProbMetric: 25.4418

Epoch 147: val_loss improved from 25.49126 to 25.44184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.3277 - MinusLogProbMetric: 25.3277 - val_loss: 25.4418 - val_MinusLogProbMetric: 25.4418 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 148/1000
2023-09-23 19:45:58.251 
Epoch 148/1000 
	 loss: 25.3334, MinusLogProbMetric: 25.3334, val_loss: 25.4200, val_MinusLogProbMetric: 25.4200

Epoch 148: val_loss improved from 25.44184 to 25.41999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.3334 - MinusLogProbMetric: 25.3334 - val_loss: 25.4200 - val_MinusLogProbMetric: 25.4200 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 149/1000
2023-09-23 19:47:13.950 
Epoch 149/1000 
	 loss: 25.2415, MinusLogProbMetric: 25.2415, val_loss: 25.3776, val_MinusLogProbMetric: 25.3776

Epoch 149: val_loss improved from 25.41999 to 25.37755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 25.2415 - MinusLogProbMetric: 25.2415 - val_loss: 25.3776 - val_MinusLogProbMetric: 25.3776 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 150/1000
2023-09-23 19:48:29.513 
Epoch 150/1000 
	 loss: 25.1917, MinusLogProbMetric: 25.1917, val_loss: 25.2664, val_MinusLogProbMetric: 25.2664

Epoch 150: val_loss improved from 25.37755 to 25.26638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.1917 - MinusLogProbMetric: 25.1917 - val_loss: 25.2664 - val_MinusLogProbMetric: 25.2664 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 151/1000
2023-09-23 19:49:44.675 
Epoch 151/1000 
	 loss: 25.1319, MinusLogProbMetric: 25.1319, val_loss: 25.2362, val_MinusLogProbMetric: 25.2362

Epoch 151: val_loss improved from 25.26638 to 25.23622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 25.1319 - MinusLogProbMetric: 25.1319 - val_loss: 25.2362 - val_MinusLogProbMetric: 25.2362 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 152/1000
2023-09-23 19:51:00.424 
Epoch 152/1000 
	 loss: 25.1322, MinusLogProbMetric: 25.1322, val_loss: 25.2212, val_MinusLogProbMetric: 25.2212

Epoch 152: val_loss improved from 25.23622 to 25.22123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 25.1322 - MinusLogProbMetric: 25.1322 - val_loss: 25.2212 - val_MinusLogProbMetric: 25.2212 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 153/1000
2023-09-23 19:52:16.579 
Epoch 153/1000 
	 loss: 25.1496, MinusLogProbMetric: 25.1496, val_loss: 25.1347, val_MinusLogProbMetric: 25.1347

Epoch 153: val_loss improved from 25.22123 to 25.13467, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 76s - loss: 25.1496 - MinusLogProbMetric: 25.1496 - val_loss: 25.1347 - val_MinusLogProbMetric: 25.1347 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 154/1000
2023-09-23 19:53:31.342 
Epoch 154/1000 
	 loss: 24.9828, MinusLogProbMetric: 24.9828, val_loss: 25.0691, val_MinusLogProbMetric: 25.0691

Epoch 154: val_loss improved from 25.13467 to 25.06911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 24.9828 - MinusLogProbMetric: 24.9828 - val_loss: 25.0691 - val_MinusLogProbMetric: 25.0691 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 155/1000
2023-09-23 19:54:46.248 
Epoch 155/1000 
	 loss: 24.9286, MinusLogProbMetric: 24.9286, val_loss: 25.0630, val_MinusLogProbMetric: 25.0630

Epoch 155: val_loss improved from 25.06911 to 25.06305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 24.9286 - MinusLogProbMetric: 24.9286 - val_loss: 25.0630 - val_MinusLogProbMetric: 25.0630 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 156/1000
2023-09-23 19:56:01.814 
Epoch 156/1000 
	 loss: 24.8920, MinusLogProbMetric: 24.8920, val_loss: 25.0282, val_MinusLogProbMetric: 25.0282

Epoch 156: val_loss improved from 25.06305 to 25.02816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 24.8920 - MinusLogProbMetric: 24.8920 - val_loss: 25.0282 - val_MinusLogProbMetric: 25.0282 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 157/1000
2023-09-23 19:57:16.991 
Epoch 157/1000 
	 loss: 25.4729, MinusLogProbMetric: 25.4729, val_loss: 25.1952, val_MinusLogProbMetric: 25.1952

Epoch 157: val_loss did not improve from 25.02816
196/196 - 74s - loss: 25.4729 - MinusLogProbMetric: 25.4729 - val_loss: 25.1952 - val_MinusLogProbMetric: 25.1952 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 158/1000
2023-09-23 19:58:30.691 
Epoch 158/1000 
	 loss: 24.8249, MinusLogProbMetric: 24.8249, val_loss: 24.9329, val_MinusLogProbMetric: 24.9329

Epoch 158: val_loss improved from 25.02816 to 24.93286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 75s - loss: 24.8249 - MinusLogProbMetric: 24.8249 - val_loss: 24.9329 - val_MinusLogProbMetric: 24.9329 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 159/1000
2023-09-23 19:59:44.903 
Epoch 159/1000 
	 loss: 24.7567, MinusLogProbMetric: 24.7567, val_loss: 24.8586, val_MinusLogProbMetric: 24.8586

Epoch 159: val_loss improved from 24.93286 to 24.85860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 24.7567 - MinusLogProbMetric: 24.7567 - val_loss: 24.8586 - val_MinusLogProbMetric: 24.8586 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 160/1000
2023-09-23 20:00:58.615 
Epoch 160/1000 
	 loss: 24.7093, MinusLogProbMetric: 24.7093, val_loss: 24.7996, val_MinusLogProbMetric: 24.7996

Epoch 160: val_loss improved from 24.85860 to 24.79958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 24.7093 - MinusLogProbMetric: 24.7093 - val_loss: 24.7996 - val_MinusLogProbMetric: 24.7996 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 161/1000
2023-09-23 20:02:12.799 
Epoch 161/1000 
	 loss: 24.6642, MinusLogProbMetric: 24.6642, val_loss: 24.7865, val_MinusLogProbMetric: 24.7865

Epoch 161: val_loss improved from 24.79958 to 24.78645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 24.6642 - MinusLogProbMetric: 24.6642 - val_loss: 24.7865 - val_MinusLogProbMetric: 24.7865 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 162/1000
2023-09-23 20:03:26.910 
Epoch 162/1000 
	 loss: 24.7292, MinusLogProbMetric: 24.7292, val_loss: 24.7320, val_MinusLogProbMetric: 24.7320

Epoch 162: val_loss improved from 24.78645 to 24.73198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 24.7292 - MinusLogProbMetric: 24.7292 - val_loss: 24.7320 - val_MinusLogProbMetric: 24.7320 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 163/1000
2023-09-23 20:04:40.473 
Epoch 163/1000 
	 loss: 24.5959, MinusLogProbMetric: 24.5959, val_loss: 24.6950, val_MinusLogProbMetric: 24.6950

Epoch 163: val_loss improved from 24.73198 to 24.69498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 24.5959 - MinusLogProbMetric: 24.5959 - val_loss: 24.6950 - val_MinusLogProbMetric: 24.6950 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 164/1000
2023-09-23 20:05:54.680 
Epoch 164/1000 
	 loss: 24.5659, MinusLogProbMetric: 24.5659, val_loss: 24.6898, val_MinusLogProbMetric: 24.6898

Epoch 164: val_loss improved from 24.69498 to 24.68978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 24.5659 - MinusLogProbMetric: 24.5659 - val_loss: 24.6898 - val_MinusLogProbMetric: 24.6898 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 165/1000
2023-09-23 20:07:08.451 
Epoch 165/1000 
	 loss: 24.5333, MinusLogProbMetric: 24.5333, val_loss: 24.6595, val_MinusLogProbMetric: 24.6595

Epoch 165: val_loss improved from 24.68978 to 24.65952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_295/weights/best_weights.h5
196/196 - 74s - loss: 24.5333 - MinusLogProbMetric: 24.5333 - val_loss: 24.6595 - val_MinusLogProbMetric: 24.6595 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 166/1000
2023-09-23 20:08:22.204 
Epoch 166/1000 
	 loss: 37.8218, MinusLogProbMetric: 37.8218, val_loss: 35.1106, val_MinusLogProbMetric: 35.1106

Epoch 166: val_loss did not improve from 24.65952
196/196 - 73s - loss: 37.8218 - MinusLogProbMetric: 37.8218 - val_loss: 35.1106 - val_MinusLogProbMetric: 35.1106 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 167/1000
2023-09-23 20:09:34.880 
Epoch 167/1000 
	 loss: 35.8085, MinusLogProbMetric: 35.8085, val_loss: 31.8027, val_MinusLogProbMetric: 31.8027

Epoch 167: val_loss did not improve from 24.65952
196/196 - 73s - loss: 35.8085 - MinusLogProbMetric: 35.8085 - val_loss: 31.8027 - val_MinusLogProbMetric: 31.8027 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 168/1000
2023-09-23 20:10:47.000 
Epoch 168/1000 
	 loss: 30.1516, MinusLogProbMetric: 30.1516, val_loss: 29.1576, val_MinusLogProbMetric: 29.1576

Epoch 168: val_loss did not improve from 24.65952
196/196 - 72s - loss: 30.1516 - MinusLogProbMetric: 30.1516 - val_loss: 29.1576 - val_MinusLogProbMetric: 29.1576 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 169/1000
2023-09-23 20:11:59.656 
Epoch 169/1000 
	 loss: 28.4742, MinusLogProbMetric: 28.4742, val_loss: 28.1522, val_MinusLogProbMetric: 28.1522

Epoch 169: val_loss did not improve from 24.65952
196/196 - 73s - loss: 28.4742 - MinusLogProbMetric: 28.4742 - val_loss: 28.1522 - val_MinusLogProbMetric: 28.1522 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 170/1000
2023-09-23 20:13:12.051 
Epoch 170/1000 
	 loss: 27.7797, MinusLogProbMetric: 27.7797, val_loss: 47.0462, val_MinusLogProbMetric: 47.0462

Epoch 170: val_loss did not improve from 24.65952
196/196 - 72s - loss: 27.7797 - MinusLogProbMetric: 27.7797 - val_loss: 47.0462 - val_MinusLogProbMetric: 47.0462 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 171/1000
2023-09-23 20:14:24.978 
Epoch 171/1000 
	 loss: 29.9970, MinusLogProbMetric: 29.9970, val_loss: 28.3302, val_MinusLogProbMetric: 28.3302

Epoch 171: val_loss did not improve from 24.65952
196/196 - 73s - loss: 29.9970 - MinusLogProbMetric: 29.9970 - val_loss: 28.3302 - val_MinusLogProbMetric: 28.3302 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 172/1000
2023-09-23 20:15:37.694 
Epoch 172/1000 
	 loss: 27.2487, MinusLogProbMetric: 27.2487, val_loss: 26.8167, val_MinusLogProbMetric: 26.8167

Epoch 172: val_loss did not improve from 24.65952
196/196 - 73s - loss: 27.2487 - MinusLogProbMetric: 27.2487 - val_loss: 26.8167 - val_MinusLogProbMetric: 26.8167 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 173/1000
2023-09-23 20:16:50.423 
Epoch 173/1000 
	 loss: 26.5410, MinusLogProbMetric: 26.5410, val_loss: 26.4556, val_MinusLogProbMetric: 26.4556

Epoch 173: val_loss did not improve from 24.65952
196/196 - 73s - loss: 26.5410 - MinusLogProbMetric: 26.5410 - val_loss: 26.4556 - val_MinusLogProbMetric: 26.4556 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 174/1000
2023-09-23 20:18:04.075 
Epoch 174/1000 
	 loss: 27.4379, MinusLogProbMetric: 27.4379, val_loss: 27.7822, val_MinusLogProbMetric: 27.7822

Epoch 174: val_loss did not improve from 24.65952
196/196 - 74s - loss: 27.4379 - MinusLogProbMetric: 27.4379 - val_loss: 27.7822 - val_MinusLogProbMetric: 27.7822 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 175/1000
2023-09-23 20:19:17.028 
Epoch 175/1000 
	 loss: 26.7023, MinusLogProbMetric: 26.7023, val_loss: 26.0665, val_MinusLogProbMetric: 26.0665

Epoch 175: val_loss did not improve from 24.65952
196/196 - 73s - loss: 26.7023 - MinusLogProbMetric: 26.7023 - val_loss: 26.0665 - val_MinusLogProbMetric: 26.0665 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 176/1000
2023-09-23 20:20:29.450 
Epoch 176/1000 
	 loss: 26.0050, MinusLogProbMetric: 26.0050, val_loss: 30.2327, val_MinusLogProbMetric: 30.2327

Epoch 176: val_loss did not improve from 24.65952
196/196 - 72s - loss: 26.0050 - MinusLogProbMetric: 26.0050 - val_loss: 30.2327 - val_MinusLogProbMetric: 30.2327 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 177/1000
2023-09-23 20:21:41.829 
Epoch 177/1000 
	 loss: 26.8816, MinusLogProbMetric: 26.8816, val_loss: 25.7817, val_MinusLogProbMetric: 25.7817

Epoch 177: val_loss did not improve from 24.65952
196/196 - 72s - loss: 26.8816 - MinusLogProbMetric: 26.8816 - val_loss: 25.7817 - val_MinusLogProbMetric: 25.7817 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 178/1000
2023-09-23 20:22:54.740 
Epoch 178/1000 
	 loss: 25.6433, MinusLogProbMetric: 25.6433, val_loss: 25.6593, val_MinusLogProbMetric: 25.6593

Epoch 178: val_loss did not improve from 24.65952
196/196 - 73s - loss: 25.6433 - MinusLogProbMetric: 25.6433 - val_loss: 25.6593 - val_MinusLogProbMetric: 25.6593 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 179/1000
2023-09-23 20:24:07.305 
Epoch 179/1000 
	 loss: 25.4974, MinusLogProbMetric: 25.4974, val_loss: 25.4896, val_MinusLogProbMetric: 25.4896

Epoch 179: val_loss did not improve from 24.65952
196/196 - 73s - loss: 25.4974 - MinusLogProbMetric: 25.4974 - val_loss: 25.4896 - val_MinusLogProbMetric: 25.4896 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 180/1000
2023-09-23 20:25:19.942 
Epoch 180/1000 
	 loss: 25.3815, MinusLogProbMetric: 25.3815, val_loss: 25.3836, val_MinusLogProbMetric: 25.3836

Epoch 180: val_loss did not improve from 24.65952
196/196 - 73s - loss: 25.3815 - MinusLogProbMetric: 25.3815 - val_loss: 25.3836 - val_MinusLogProbMetric: 25.3836 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 181/1000
2023-09-23 20:26:32.589 
Epoch 181/1000 
	 loss: 25.2599, MinusLogProbMetric: 25.2599, val_loss: 25.2941, val_MinusLogProbMetric: 25.2941

Epoch 181: val_loss did not improve from 24.65952
196/196 - 73s - loss: 25.2599 - MinusLogProbMetric: 25.2599 - val_loss: 25.2941 - val_MinusLogProbMetric: 25.2941 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 182/1000
2023-09-23 20:27:44.794 
Epoch 182/1000 
	 loss: 66.9964, MinusLogProbMetric: 66.9964, val_loss: 85.8850, val_MinusLogProbMetric: 85.8850

Epoch 182: val_loss did not improve from 24.65952
196/196 - 72s - loss: 66.9964 - MinusLogProbMetric: 66.9964 - val_loss: 85.8850 - val_MinusLogProbMetric: 85.8850 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 183/1000
2023-09-23 20:28:57.194 
Epoch 183/1000 
	 loss: 64.9115, MinusLogProbMetric: 64.9115, val_loss: 54.9979, val_MinusLogProbMetric: 54.9979

Epoch 183: val_loss did not improve from 24.65952
196/196 - 72s - loss: 64.9115 - MinusLogProbMetric: 64.9115 - val_loss: 54.9979 - val_MinusLogProbMetric: 54.9979 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 184/1000
2023-09-23 20:30:09.883 
Epoch 184/1000 
	 loss: 50.0133, MinusLogProbMetric: 50.0133, val_loss: 46.6015, val_MinusLogProbMetric: 46.6015

Epoch 184: val_loss did not improve from 24.65952
196/196 - 73s - loss: 50.0133 - MinusLogProbMetric: 50.0133 - val_loss: 46.6015 - val_MinusLogProbMetric: 46.6015 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 185/1000
2023-09-23 20:31:23.782 
Epoch 185/1000 
	 loss: 44.4672, MinusLogProbMetric: 44.4672, val_loss: 42.9190, val_MinusLogProbMetric: 42.9190

Epoch 185: val_loss did not improve from 24.65952
196/196 - 74s - loss: 44.4672 - MinusLogProbMetric: 44.4672 - val_loss: 42.9190 - val_MinusLogProbMetric: 42.9190 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 186/1000
2023-09-23 20:32:37.265 
Epoch 186/1000 
	 loss: 41.5791, MinusLogProbMetric: 41.5791, val_loss: 40.6144, val_MinusLogProbMetric: 40.6144

Epoch 186: val_loss did not improve from 24.65952
196/196 - 73s - loss: 41.5791 - MinusLogProbMetric: 41.5791 - val_loss: 40.6144 - val_MinusLogProbMetric: 40.6144 - lr: 1.2346e-05 - 73s/epoch - 375ms/step
Epoch 187/1000
2023-09-23 20:33:50.430 
Epoch 187/1000 
	 loss: 39.6462, MinusLogProbMetric: 39.6462, val_loss: 38.8714, val_MinusLogProbMetric: 38.8714

Epoch 187: val_loss did not improve from 24.65952
196/196 - 73s - loss: 39.6462 - MinusLogProbMetric: 39.6462 - val_loss: 38.8714 - val_MinusLogProbMetric: 38.8714 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 188/1000
2023-09-23 20:35:03.068 
Epoch 188/1000 
	 loss: 38.0264, MinusLogProbMetric: 38.0264, val_loss: 37.4732, val_MinusLogProbMetric: 37.4732

Epoch 188: val_loss did not improve from 24.65952
196/196 - 73s - loss: 38.0264 - MinusLogProbMetric: 38.0264 - val_loss: 37.4732 - val_MinusLogProbMetric: 37.4732 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 189/1000
2023-09-23 20:36:16.220 
Epoch 189/1000 
	 loss: 36.8415, MinusLogProbMetric: 36.8415, val_loss: 36.4530, val_MinusLogProbMetric: 36.4530

Epoch 189: val_loss did not improve from 24.65952
196/196 - 73s - loss: 36.8415 - MinusLogProbMetric: 36.8415 - val_loss: 36.4530 - val_MinusLogProbMetric: 36.4530 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 190/1000
2023-09-23 20:37:29.707 
Epoch 190/1000 
	 loss: 35.9491, MinusLogProbMetric: 35.9491, val_loss: 35.6165, val_MinusLogProbMetric: 35.6165

Epoch 190: val_loss did not improve from 24.65952
196/196 - 73s - loss: 35.9491 - MinusLogProbMetric: 35.9491 - val_loss: 35.6165 - val_MinusLogProbMetric: 35.6165 - lr: 1.2346e-05 - 73s/epoch - 375ms/step
Epoch 191/1000
2023-09-23 20:38:42.743 
Epoch 191/1000 
	 loss: 35.1728, MinusLogProbMetric: 35.1728, val_loss: 34.8538, val_MinusLogProbMetric: 34.8538

Epoch 191: val_loss did not improve from 24.65952
196/196 - 73s - loss: 35.1728 - MinusLogProbMetric: 35.1728 - val_loss: 34.8538 - val_MinusLogProbMetric: 34.8538 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 192/1000
2023-09-23 20:39:55.389 
Epoch 192/1000 
	 loss: 34.4866, MinusLogProbMetric: 34.4866, val_loss: 34.1613, val_MinusLogProbMetric: 34.1613

Epoch 192: val_loss did not improve from 24.65952
196/196 - 73s - loss: 34.4866 - MinusLogProbMetric: 34.4866 - val_loss: 34.1613 - val_MinusLogProbMetric: 34.1613 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 193/1000
2023-09-23 20:41:08.260 
Epoch 193/1000 
	 loss: 40.3929, MinusLogProbMetric: 40.3929, val_loss: 35.3402, val_MinusLogProbMetric: 35.3402

Epoch 193: val_loss did not improve from 24.65952
196/196 - 73s - loss: 40.3929 - MinusLogProbMetric: 40.3929 - val_loss: 35.3402 - val_MinusLogProbMetric: 35.3402 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 194/1000
2023-09-23 20:42:20.913 
Epoch 194/1000 
	 loss: 34.4741, MinusLogProbMetric: 34.4741, val_loss: 33.6864, val_MinusLogProbMetric: 33.6864

Epoch 194: val_loss did not improve from 24.65952
196/196 - 73s - loss: 34.4741 - MinusLogProbMetric: 34.4741 - val_loss: 33.6864 - val_MinusLogProbMetric: 33.6864 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 195/1000
2023-09-23 20:43:33.319 
Epoch 195/1000 
	 loss: 33.4878, MinusLogProbMetric: 33.4878, val_loss: 33.6648, val_MinusLogProbMetric: 33.6648

Epoch 195: val_loss did not improve from 24.65952
196/196 - 72s - loss: 33.4878 - MinusLogProbMetric: 33.4878 - val_loss: 33.6648 - val_MinusLogProbMetric: 33.6648 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 196/1000
2023-09-23 20:44:46.068 
Epoch 196/1000 
	 loss: 32.6571, MinusLogProbMetric: 32.6571, val_loss: 32.4832, val_MinusLogProbMetric: 32.4832

Epoch 196: val_loss did not improve from 24.65952
196/196 - 73s - loss: 32.6571 - MinusLogProbMetric: 32.6571 - val_loss: 32.4832 - val_MinusLogProbMetric: 32.4832 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 197/1000
2023-09-23 20:45:58.217 
Epoch 197/1000 
	 loss: 32.2294, MinusLogProbMetric: 32.2294, val_loss: 32.1571, val_MinusLogProbMetric: 32.1571

Epoch 197: val_loss did not improve from 24.65952
196/196 - 72s - loss: 32.2294 - MinusLogProbMetric: 32.2294 - val_loss: 32.1571 - val_MinusLogProbMetric: 32.1571 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 198/1000
2023-09-23 20:47:10.622 
Epoch 198/1000 
	 loss: 31.8596, MinusLogProbMetric: 31.8596, val_loss: 31.7317, val_MinusLogProbMetric: 31.7317

Epoch 198: val_loss did not improve from 24.65952
196/196 - 72s - loss: 31.8596 - MinusLogProbMetric: 31.8596 - val_loss: 31.7317 - val_MinusLogProbMetric: 31.7317 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 199/1000
2023-09-23 20:48:23.128 
Epoch 199/1000 
	 loss: 50.0782, MinusLogProbMetric: 50.0782, val_loss: 59.5717, val_MinusLogProbMetric: 59.5717

Epoch 199: val_loss did not improve from 24.65952
196/196 - 73s - loss: 50.0782 - MinusLogProbMetric: 50.0782 - val_loss: 59.5717 - val_MinusLogProbMetric: 59.5717 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 200/1000
2023-09-23 20:49:36.496 
Epoch 200/1000 
	 loss: 52.5756, MinusLogProbMetric: 52.5756, val_loss: 48.7073, val_MinusLogProbMetric: 48.7073

Epoch 200: val_loss did not improve from 24.65952
196/196 - 73s - loss: 52.5756 - MinusLogProbMetric: 52.5756 - val_loss: 48.7073 - val_MinusLogProbMetric: 48.7073 - lr: 1.2346e-05 - 73s/epoch - 374ms/step
Epoch 201/1000
2023-09-23 20:50:49.119 
Epoch 201/1000 
	 loss: 46.1142, MinusLogProbMetric: 46.1142, val_loss: 43.7949, val_MinusLogProbMetric: 43.7949

Epoch 201: val_loss did not improve from 24.65952
196/196 - 73s - loss: 46.1142 - MinusLogProbMetric: 46.1142 - val_loss: 43.7949 - val_MinusLogProbMetric: 43.7949 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 202/1000
2023-09-23 20:52:01.364 
Epoch 202/1000 
	 loss: 42.7185, MinusLogProbMetric: 42.7185, val_loss: 41.8263, val_MinusLogProbMetric: 41.8263

Epoch 202: val_loss did not improve from 24.65952
196/196 - 72s - loss: 42.7185 - MinusLogProbMetric: 42.7185 - val_loss: 41.8263 - val_MinusLogProbMetric: 41.8263 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 203/1000
2023-09-23 20:53:14.573 
Epoch 203/1000 
	 loss: 41.0896, MinusLogProbMetric: 41.0896, val_loss: 40.5200, val_MinusLogProbMetric: 40.5200

Epoch 203: val_loss did not improve from 24.65952
196/196 - 73s - loss: 41.0896 - MinusLogProbMetric: 41.0896 - val_loss: 40.5200 - val_MinusLogProbMetric: 40.5200 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 204/1000
2023-09-23 20:54:27.042 
Epoch 204/1000 
	 loss: 39.8450, MinusLogProbMetric: 39.8450, val_loss: 39.3477, val_MinusLogProbMetric: 39.3477

Epoch 204: val_loss did not improve from 24.65952
196/196 - 72s - loss: 39.8450 - MinusLogProbMetric: 39.8450 - val_loss: 39.3477 - val_MinusLogProbMetric: 39.3477 - lr: 1.2346e-05 - 72s/epoch - 370ms/step
Epoch 205/1000
2023-09-23 20:55:39.671 
Epoch 205/1000 
	 loss: 38.8484, MinusLogProbMetric: 38.8484, val_loss: 38.5131, val_MinusLogProbMetric: 38.5131

Epoch 205: val_loss did not improve from 24.65952
196/196 - 73s - loss: 38.8484 - MinusLogProbMetric: 38.8484 - val_loss: 38.5131 - val_MinusLogProbMetric: 38.5131 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 206/1000
2023-09-23 20:56:52.584 
Epoch 206/1000 
	 loss: 38.0574, MinusLogProbMetric: 38.0574, val_loss: 37.7967, val_MinusLogProbMetric: 37.7967

Epoch 206: val_loss did not improve from 24.65952
196/196 - 73s - loss: 38.0574 - MinusLogProbMetric: 38.0574 - val_loss: 37.7967 - val_MinusLogProbMetric: 37.7967 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 207/1000
2023-09-23 20:58:05.022 
Epoch 207/1000 
	 loss: 37.4519, MinusLogProbMetric: 37.4519, val_loss: 37.2276, val_MinusLogProbMetric: 37.2276

Epoch 207: val_loss did not improve from 24.65952
196/196 - 72s - loss: 37.4519 - MinusLogProbMetric: 37.4519 - val_loss: 37.2276 - val_MinusLogProbMetric: 37.2276 - lr: 1.2346e-05 - 72s/epoch - 370ms/step
Epoch 208/1000
2023-09-23 20:59:17.305 
Epoch 208/1000 
	 loss: 36.8996, MinusLogProbMetric: 36.8996, val_loss: 36.6958, val_MinusLogProbMetric: 36.6958

Epoch 208: val_loss did not improve from 24.65952
196/196 - 72s - loss: 36.8996 - MinusLogProbMetric: 36.8996 - val_loss: 36.6958 - val_MinusLogProbMetric: 36.6958 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 209/1000
2023-09-23 21:00:29.726 
Epoch 209/1000 
	 loss: 36.3617, MinusLogProbMetric: 36.3617, val_loss: 36.1894, val_MinusLogProbMetric: 36.1894

Epoch 209: val_loss did not improve from 24.65952
196/196 - 72s - loss: 36.3617 - MinusLogProbMetric: 36.3617 - val_loss: 36.1894 - val_MinusLogProbMetric: 36.1894 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 210/1000
2023-09-23 21:01:42.367 
Epoch 210/1000 
	 loss: 35.8975, MinusLogProbMetric: 35.8975, val_loss: 35.7712, val_MinusLogProbMetric: 35.7712

Epoch 210: val_loss did not improve from 24.65952
196/196 - 73s - loss: 35.8975 - MinusLogProbMetric: 35.8975 - val_loss: 35.7712 - val_MinusLogProbMetric: 35.7712 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 211/1000
2023-09-23 21:02:56.098 
Epoch 211/1000 
	 loss: 35.4972, MinusLogProbMetric: 35.4972, val_loss: 35.4363, val_MinusLogProbMetric: 35.4363

Epoch 211: val_loss did not improve from 24.65952
196/196 - 74s - loss: 35.4972 - MinusLogProbMetric: 35.4972 - val_loss: 35.4363 - val_MinusLogProbMetric: 35.4363 - lr: 1.2346e-05 - 74s/epoch - 376ms/step
Epoch 212/1000
2023-09-23 21:04:08.767 
Epoch 212/1000 
	 loss: 35.1302, MinusLogProbMetric: 35.1302, val_loss: 35.0512, val_MinusLogProbMetric: 35.0512

Epoch 212: val_loss did not improve from 24.65952
196/196 - 73s - loss: 35.1302 - MinusLogProbMetric: 35.1302 - val_loss: 35.0512 - val_MinusLogProbMetric: 35.0512 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 213/1000
2023-09-23 21:05:21.189 
Epoch 213/1000 
	 loss: 34.7904, MinusLogProbMetric: 34.7904, val_loss: 34.7484, val_MinusLogProbMetric: 34.7484

Epoch 213: val_loss did not improve from 24.65952
196/196 - 72s - loss: 34.7904 - MinusLogProbMetric: 34.7904 - val_loss: 34.7484 - val_MinusLogProbMetric: 34.7484 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 214/1000
2023-09-23 21:06:33.304 
Epoch 214/1000 
	 loss: 34.4880, MinusLogProbMetric: 34.4880, val_loss: 34.3716, val_MinusLogProbMetric: 34.3716

Epoch 214: val_loss did not improve from 24.65952
196/196 - 72s - loss: 34.4880 - MinusLogProbMetric: 34.4880 - val_loss: 34.3716 - val_MinusLogProbMetric: 34.3716 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 215/1000
2023-09-23 21:07:46.127 
Epoch 215/1000 
	 loss: 34.1889, MinusLogProbMetric: 34.1889, val_loss: 34.1559, val_MinusLogProbMetric: 34.1559

Epoch 215: val_loss did not improve from 24.65952
196/196 - 73s - loss: 34.1889 - MinusLogProbMetric: 34.1889 - val_loss: 34.1559 - val_MinusLogProbMetric: 34.1559 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 216/1000
2023-09-23 21:08:58.799 
Epoch 216/1000 
	 loss: 33.9924, MinusLogProbMetric: 33.9924, val_loss: 34.0315, val_MinusLogProbMetric: 34.0315

Epoch 216: val_loss did not improve from 24.65952
196/196 - 73s - loss: 33.9924 - MinusLogProbMetric: 33.9924 - val_loss: 34.0315 - val_MinusLogProbMetric: 34.0315 - lr: 6.1728e-06 - 73s/epoch - 371ms/step
Epoch 217/1000
2023-09-23 21:10:11.484 
Epoch 217/1000 
	 loss: 33.9203, MinusLogProbMetric: 33.9203, val_loss: 34.4661, val_MinusLogProbMetric: 34.4661

Epoch 217: val_loss did not improve from 24.65952
196/196 - 73s - loss: 33.9203 - MinusLogProbMetric: 33.9203 - val_loss: 34.4661 - val_MinusLogProbMetric: 34.4661 - lr: 6.1728e-06 - 73s/epoch - 371ms/step
Epoch 218/1000
2023-09-23 21:11:23.712 
Epoch 218/1000 
	 loss: 34.5132, MinusLogProbMetric: 34.5132, val_loss: 34.3083, val_MinusLogProbMetric: 34.3083

Epoch 218: val_loss did not improve from 24.65952
196/196 - 72s - loss: 34.5132 - MinusLogProbMetric: 34.5132 - val_loss: 34.3083 - val_MinusLogProbMetric: 34.3083 - lr: 6.1728e-06 - 72s/epoch - 368ms/step
Epoch 219/1000
2023-09-23 21:12:36.241 
Epoch 219/1000 
	 loss: 34.0578, MinusLogProbMetric: 34.0578, val_loss: 33.7182, val_MinusLogProbMetric: 33.7182

Epoch 219: val_loss did not improve from 24.65952
196/196 - 73s - loss: 34.0578 - MinusLogProbMetric: 34.0578 - val_loss: 33.7182 - val_MinusLogProbMetric: 33.7182 - lr: 6.1728e-06 - 73s/epoch - 370ms/step
Epoch 220/1000
2023-09-23 21:13:47.507 
Epoch 220/1000 
	 loss: 33.2225, MinusLogProbMetric: 33.2225, val_loss: 33.0268, val_MinusLogProbMetric: 33.0268

Epoch 220: val_loss did not improve from 24.65952
196/196 - 71s - loss: 33.2225 - MinusLogProbMetric: 33.2225 - val_loss: 33.0268 - val_MinusLogProbMetric: 33.0268 - lr: 6.1728e-06 - 71s/epoch - 364ms/step
Epoch 221/1000
2023-09-23 21:14:58.711 
Epoch 221/1000 
	 loss: 32.7170, MinusLogProbMetric: 32.7170, val_loss: 32.6707, val_MinusLogProbMetric: 32.6707

Epoch 221: val_loss did not improve from 24.65952
196/196 - 71s - loss: 32.7170 - MinusLogProbMetric: 32.7170 - val_loss: 32.6707 - val_MinusLogProbMetric: 32.6707 - lr: 6.1728e-06 - 71s/epoch - 363ms/step
Epoch 222/1000
2023-09-23 21:16:11.326 
Epoch 222/1000 
	 loss: 32.4773, MinusLogProbMetric: 32.4773, val_loss: 32.4966, val_MinusLogProbMetric: 32.4966

Epoch 222: val_loss did not improve from 24.65952
196/196 - 73s - loss: 32.4773 - MinusLogProbMetric: 32.4773 - val_loss: 32.4966 - val_MinusLogProbMetric: 32.4966 - lr: 6.1728e-06 - 73s/epoch - 370ms/step
Epoch 223/1000
2023-09-23 21:17:23.141 
Epoch 223/1000 
	 loss: 32.2361, MinusLogProbMetric: 32.2361, val_loss: 32.2379, val_MinusLogProbMetric: 32.2379

Epoch 223: val_loss did not improve from 24.65952
196/196 - 72s - loss: 32.2361 - MinusLogProbMetric: 32.2361 - val_loss: 32.2379 - val_MinusLogProbMetric: 32.2379 - lr: 6.1728e-06 - 72s/epoch - 366ms/step
Epoch 224/1000
2023-09-23 21:18:34.741 
Epoch 224/1000 
	 loss: 32.0473, MinusLogProbMetric: 32.0473, val_loss: 32.0655, val_MinusLogProbMetric: 32.0655

Epoch 224: val_loss did not improve from 24.65952
196/196 - 72s - loss: 32.0473 - MinusLogProbMetric: 32.0473 - val_loss: 32.0655 - val_MinusLogProbMetric: 32.0655 - lr: 6.1728e-06 - 72s/epoch - 365ms/step
Epoch 225/1000
2023-09-23 21:19:45.654 
Epoch 225/1000 
	 loss: 31.8943, MinusLogProbMetric: 31.8943, val_loss: 31.9336, val_MinusLogProbMetric: 31.9336

Epoch 225: val_loss did not improve from 24.65952
196/196 - 71s - loss: 31.8943 - MinusLogProbMetric: 31.8943 - val_loss: 31.9336 - val_MinusLogProbMetric: 31.9336 - lr: 6.1728e-06 - 71s/epoch - 362ms/step
Epoch 226/1000
2023-09-23 21:20:56.878 
Epoch 226/1000 
	 loss: 31.7438, MinusLogProbMetric: 31.7438, val_loss: 31.8027, val_MinusLogProbMetric: 31.8027

Epoch 226: val_loss did not improve from 24.65952
196/196 - 71s - loss: 31.7438 - MinusLogProbMetric: 31.7438 - val_loss: 31.8027 - val_MinusLogProbMetric: 31.8027 - lr: 6.1728e-06 - 71s/epoch - 363ms/step
Epoch 227/1000
2023-09-23 21:22:07.500 
Epoch 227/1000 
	 loss: 31.6055, MinusLogProbMetric: 31.6055, val_loss: 31.6579, val_MinusLogProbMetric: 31.6579

Epoch 227: val_loss did not improve from 24.65952
196/196 - 71s - loss: 31.6055 - MinusLogProbMetric: 31.6055 - val_loss: 31.6579 - val_MinusLogProbMetric: 31.6579 - lr: 6.1728e-06 - 71s/epoch - 360ms/step
Epoch 228/1000
2023-09-23 21:23:19.825 
Epoch 228/1000 
	 loss: 31.4758, MinusLogProbMetric: 31.4758, val_loss: 31.5473, val_MinusLogProbMetric: 31.5473

Epoch 228: val_loss did not improve from 24.65952
196/196 - 72s - loss: 31.4758 - MinusLogProbMetric: 31.4758 - val_loss: 31.5473 - val_MinusLogProbMetric: 31.5473 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 229/1000
2023-09-23 21:24:32.026 
Epoch 229/1000 
	 loss: 31.3552, MinusLogProbMetric: 31.3552, val_loss: 31.3898, val_MinusLogProbMetric: 31.3898

Epoch 229: val_loss did not improve from 24.65952
196/196 - 72s - loss: 31.3552 - MinusLogProbMetric: 31.3552 - val_loss: 31.3898 - val_MinusLogProbMetric: 31.3898 - lr: 6.1728e-06 - 72s/epoch - 368ms/step
Epoch 230/1000
2023-09-23 21:25:45.856 
Epoch 230/1000 
	 loss: 31.2296, MinusLogProbMetric: 31.2296, val_loss: 31.2842, val_MinusLogProbMetric: 31.2842

Epoch 230: val_loss did not improve from 24.65952
196/196 - 74s - loss: 31.2296 - MinusLogProbMetric: 31.2296 - val_loss: 31.2842 - val_MinusLogProbMetric: 31.2842 - lr: 6.1728e-06 - 74s/epoch - 377ms/step
Epoch 231/1000
2023-09-23 21:26:59.549 
Epoch 231/1000 
	 loss: 31.1169, MinusLogProbMetric: 31.1169, val_loss: 31.1719, val_MinusLogProbMetric: 31.1719

Epoch 231: val_loss did not improve from 24.65952
196/196 - 74s - loss: 31.1169 - MinusLogProbMetric: 31.1169 - val_loss: 31.1719 - val_MinusLogProbMetric: 31.1719 - lr: 6.1728e-06 - 74s/epoch - 376ms/step
Epoch 232/1000
2023-09-23 21:28:12.516 
Epoch 232/1000 
	 loss: 31.0067, MinusLogProbMetric: 31.0067, val_loss: 31.0441, val_MinusLogProbMetric: 31.0441

Epoch 232: val_loss did not improve from 24.65952
196/196 - 73s - loss: 31.0067 - MinusLogProbMetric: 31.0067 - val_loss: 31.0441 - val_MinusLogProbMetric: 31.0441 - lr: 6.1728e-06 - 73s/epoch - 372ms/step
Epoch 233/1000
2023-09-23 21:29:25.804 
Epoch 233/1000 
	 loss: 30.8974, MinusLogProbMetric: 30.8974, val_loss: 30.9630, val_MinusLogProbMetric: 30.9630

Epoch 233: val_loss did not improve from 24.65952
196/196 - 73s - loss: 30.8974 - MinusLogProbMetric: 30.8974 - val_loss: 30.9630 - val_MinusLogProbMetric: 30.9630 - lr: 6.1728e-06 - 73s/epoch - 374ms/step
Epoch 234/1000
2023-09-23 21:30:39.614 
Epoch 234/1000 
	 loss: 30.8105, MinusLogProbMetric: 30.8105, val_loss: 30.8797, val_MinusLogProbMetric: 30.8797

Epoch 234: val_loss did not improve from 24.65952
196/196 - 74s - loss: 30.8105 - MinusLogProbMetric: 30.8105 - val_loss: 30.8797 - val_MinusLogProbMetric: 30.8797 - lr: 6.1728e-06 - 74s/epoch - 377ms/step
Epoch 235/1000
2023-09-23 21:31:52.041 
Epoch 235/1000 
	 loss: 30.7012, MinusLogProbMetric: 30.7012, val_loss: 30.7604, val_MinusLogProbMetric: 30.7604

Epoch 235: val_loss did not improve from 24.65952
196/196 - 72s - loss: 30.7012 - MinusLogProbMetric: 30.7012 - val_loss: 30.7604 - val_MinusLogProbMetric: 30.7604 - lr: 6.1728e-06 - 72s/epoch - 370ms/step
Epoch 236/1000
2023-09-23 21:33:02.871 
Epoch 236/1000 
	 loss: 30.6017, MinusLogProbMetric: 30.6017, val_loss: 30.6594, val_MinusLogProbMetric: 30.6594

Epoch 236: val_loss did not improve from 24.65952
196/196 - 71s - loss: 30.6017 - MinusLogProbMetric: 30.6017 - val_loss: 30.6594 - val_MinusLogProbMetric: 30.6594 - lr: 6.1728e-06 - 71s/epoch - 361ms/step
Epoch 237/1000
2023-09-23 21:34:13.980 
Epoch 237/1000 
	 loss: 30.5026, MinusLogProbMetric: 30.5026, val_loss: 30.5628, val_MinusLogProbMetric: 30.5628

Epoch 237: val_loss did not improve from 24.65952
196/196 - 71s - loss: 30.5026 - MinusLogProbMetric: 30.5026 - val_loss: 30.5628 - val_MinusLogProbMetric: 30.5628 - lr: 6.1728e-06 - 71s/epoch - 363ms/step
Epoch 238/1000
2023-09-23 21:35:25.382 
Epoch 238/1000 
	 loss: 30.4108, MinusLogProbMetric: 30.4108, val_loss: 30.4757, val_MinusLogProbMetric: 30.4757

Epoch 238: val_loss did not improve from 24.65952
196/196 - 71s - loss: 30.4108 - MinusLogProbMetric: 30.4108 - val_loss: 30.4757 - val_MinusLogProbMetric: 30.4757 - lr: 6.1728e-06 - 71s/epoch - 364ms/step
Epoch 239/1000
2023-09-23 21:36:37.614 
Epoch 239/1000 
	 loss: 30.3214, MinusLogProbMetric: 30.3214, val_loss: 30.3737, val_MinusLogProbMetric: 30.3737

Epoch 239: val_loss did not improve from 24.65952
196/196 - 72s - loss: 30.3214 - MinusLogProbMetric: 30.3214 - val_loss: 30.3737 - val_MinusLogProbMetric: 30.3737 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 240/1000
2023-09-23 21:37:49.214 
Epoch 240/1000 
	 loss: 30.2257, MinusLogProbMetric: 30.2257, val_loss: 30.2997, val_MinusLogProbMetric: 30.2997

Epoch 240: val_loss did not improve from 24.65952
196/196 - 72s - loss: 30.2257 - MinusLogProbMetric: 30.2257 - val_loss: 30.2997 - val_MinusLogProbMetric: 30.2997 - lr: 6.1728e-06 - 72s/epoch - 365ms/step
Epoch 241/1000
2023-09-23 21:39:01.778 
Epoch 241/1000 
	 loss: 30.1391, MinusLogProbMetric: 30.1391, val_loss: 30.2155, val_MinusLogProbMetric: 30.2155

Epoch 241: val_loss did not improve from 24.65952
196/196 - 73s - loss: 30.1391 - MinusLogProbMetric: 30.1391 - val_loss: 30.2155 - val_MinusLogProbMetric: 30.2155 - lr: 6.1728e-06 - 73s/epoch - 370ms/step
Epoch 242/1000
2023-09-23 21:40:13.490 
Epoch 242/1000 
	 loss: 30.0578, MinusLogProbMetric: 30.0578, val_loss: 30.1533, val_MinusLogProbMetric: 30.1533

Epoch 242: val_loss did not improve from 24.65952
196/196 - 72s - loss: 30.0578 - MinusLogProbMetric: 30.0578 - val_loss: 30.1533 - val_MinusLogProbMetric: 30.1533 - lr: 6.1728e-06 - 72s/epoch - 366ms/step
Epoch 243/1000
2023-09-23 21:41:24.653 
Epoch 243/1000 
	 loss: 29.9991, MinusLogProbMetric: 29.9991, val_loss: 30.0655, val_MinusLogProbMetric: 30.0655

Epoch 243: val_loss did not improve from 24.65952
196/196 - 71s - loss: 29.9991 - MinusLogProbMetric: 29.9991 - val_loss: 30.0655 - val_MinusLogProbMetric: 30.0655 - lr: 6.1728e-06 - 71s/epoch - 363ms/step
Epoch 244/1000
2023-09-23 21:42:36.886 
Epoch 244/1000 
	 loss: 29.9202, MinusLogProbMetric: 29.9202, val_loss: 29.9996, val_MinusLogProbMetric: 29.9996

Epoch 244: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.9202 - MinusLogProbMetric: 29.9202 - val_loss: 29.9996 - val_MinusLogProbMetric: 29.9996 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 245/1000
2023-09-23 21:43:48.883 
Epoch 245/1000 
	 loss: 29.8448, MinusLogProbMetric: 29.8448, val_loss: 29.9141, val_MinusLogProbMetric: 29.9141

Epoch 245: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.8448 - MinusLogProbMetric: 29.8448 - val_loss: 29.9141 - val_MinusLogProbMetric: 29.9141 - lr: 6.1728e-06 - 72s/epoch - 367ms/step
Epoch 246/1000
2023-09-23 21:45:01.239 
Epoch 246/1000 
	 loss: 29.7712, MinusLogProbMetric: 29.7712, val_loss: 29.8492, val_MinusLogProbMetric: 29.8492

Epoch 246: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.7712 - MinusLogProbMetric: 29.7712 - val_loss: 29.8492 - val_MinusLogProbMetric: 29.8492 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 247/1000
2023-09-23 21:46:14.234 
Epoch 247/1000 
	 loss: 29.6831, MinusLogProbMetric: 29.6831, val_loss: 29.7576, val_MinusLogProbMetric: 29.7576

Epoch 247: val_loss did not improve from 24.65952
196/196 - 73s - loss: 29.6831 - MinusLogProbMetric: 29.6831 - val_loss: 29.7576 - val_MinusLogProbMetric: 29.7576 - lr: 6.1728e-06 - 73s/epoch - 372ms/step
Epoch 248/1000
2023-09-23 21:47:25.995 
Epoch 248/1000 
	 loss: 29.6145, MinusLogProbMetric: 29.6145, val_loss: 29.6982, val_MinusLogProbMetric: 29.6982

Epoch 248: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.6145 - MinusLogProbMetric: 29.6145 - val_loss: 29.6982 - val_MinusLogProbMetric: 29.6982 - lr: 6.1728e-06 - 72s/epoch - 366ms/step
Epoch 249/1000
2023-09-23 21:48:37.888 
Epoch 249/1000 
	 loss: 29.5498, MinusLogProbMetric: 29.5498, val_loss: 29.6362, val_MinusLogProbMetric: 29.6362

Epoch 249: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.5498 - MinusLogProbMetric: 29.5498 - val_loss: 29.6362 - val_MinusLogProbMetric: 29.6362 - lr: 6.1728e-06 - 72s/epoch - 367ms/step
Epoch 250/1000
2023-09-23 21:49:49.883 
Epoch 250/1000 
	 loss: 29.4754, MinusLogProbMetric: 29.4754, val_loss: 29.5353, val_MinusLogProbMetric: 29.5353

Epoch 250: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.4754 - MinusLogProbMetric: 29.4754 - val_loss: 29.5353 - val_MinusLogProbMetric: 29.5353 - lr: 6.1728e-06 - 72s/epoch - 367ms/step
Epoch 251/1000
2023-09-23 21:51:01.592 
Epoch 251/1000 
	 loss: 29.3926, MinusLogProbMetric: 29.3926, val_loss: 29.4435, val_MinusLogProbMetric: 29.4435

Epoch 251: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.3926 - MinusLogProbMetric: 29.3926 - val_loss: 29.4435 - val_MinusLogProbMetric: 29.4435 - lr: 6.1728e-06 - 72s/epoch - 366ms/step
Epoch 252/1000
2023-09-23 21:52:13.523 
Epoch 252/1000 
	 loss: 29.2934, MinusLogProbMetric: 29.2934, val_loss: 29.3818, val_MinusLogProbMetric: 29.3818

Epoch 252: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.2934 - MinusLogProbMetric: 29.2934 - val_loss: 29.3818 - val_MinusLogProbMetric: 29.3818 - lr: 6.1728e-06 - 72s/epoch - 367ms/step
Epoch 253/1000
2023-09-23 21:53:25.248 
Epoch 253/1000 
	 loss: 29.2248, MinusLogProbMetric: 29.2248, val_loss: 29.3191, val_MinusLogProbMetric: 29.3191

Epoch 253: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.2248 - MinusLogProbMetric: 29.2248 - val_loss: 29.3191 - val_MinusLogProbMetric: 29.3191 - lr: 6.1728e-06 - 72s/epoch - 366ms/step
Epoch 254/1000
2023-09-23 21:54:36.614 
Epoch 254/1000 
	 loss: 29.1549, MinusLogProbMetric: 29.1549, val_loss: 29.2381, val_MinusLogProbMetric: 29.2381

Epoch 254: val_loss did not improve from 24.65952
196/196 - 71s - loss: 29.1549 - MinusLogProbMetric: 29.1549 - val_loss: 29.2381 - val_MinusLogProbMetric: 29.2381 - lr: 6.1728e-06 - 71s/epoch - 364ms/step
Epoch 255/1000
2023-09-23 21:55:47.676 
Epoch 255/1000 
	 loss: 29.0906, MinusLogProbMetric: 29.0906, val_loss: 29.1676, val_MinusLogProbMetric: 29.1676

Epoch 255: val_loss did not improve from 24.65952
196/196 - 71s - loss: 29.0906 - MinusLogProbMetric: 29.0906 - val_loss: 29.1676 - val_MinusLogProbMetric: 29.1676 - lr: 6.1728e-06 - 71s/epoch - 363ms/step
Epoch 256/1000
2023-09-23 21:57:00.072 
Epoch 256/1000 
	 loss: 29.0239, MinusLogProbMetric: 29.0239, val_loss: 29.1016, val_MinusLogProbMetric: 29.1016

Epoch 256: val_loss did not improve from 24.65952
196/196 - 72s - loss: 29.0239 - MinusLogProbMetric: 29.0239 - val_loss: 29.1016 - val_MinusLogProbMetric: 29.1016 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 257/1000
2023-09-23 21:58:12.445 
Epoch 257/1000 
	 loss: 28.9652, MinusLogProbMetric: 28.9652, val_loss: 29.0384, val_MinusLogProbMetric: 29.0384

Epoch 257: val_loss did not improve from 24.65952
196/196 - 72s - loss: 28.9652 - MinusLogProbMetric: 28.9652 - val_loss: 29.0384 - val_MinusLogProbMetric: 29.0384 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 258/1000
2023-09-23 21:59:23.416 
Epoch 258/1000 
	 loss: 28.9093, MinusLogProbMetric: 28.9093, val_loss: 29.0079, val_MinusLogProbMetric: 29.0079

Epoch 258: val_loss did not improve from 24.65952
196/196 - 71s - loss: 28.9093 - MinusLogProbMetric: 28.9093 - val_loss: 29.0079 - val_MinusLogProbMetric: 29.0079 - lr: 6.1728e-06 - 71s/epoch - 362ms/step
Epoch 259/1000
2023-09-23 22:00:36.021 
Epoch 259/1000 
	 loss: 28.8543, MinusLogProbMetric: 28.8543, val_loss: 28.9408, val_MinusLogProbMetric: 28.9408

Epoch 259: val_loss did not improve from 24.65952
196/196 - 73s - loss: 28.8543 - MinusLogProbMetric: 28.8543 - val_loss: 28.9408 - val_MinusLogProbMetric: 28.9408 - lr: 6.1728e-06 - 73s/epoch - 370ms/step
Epoch 260/1000
2023-09-23 22:01:46.789 
Epoch 260/1000 
	 loss: 28.7908, MinusLogProbMetric: 28.7908, val_loss: 28.8672, val_MinusLogProbMetric: 28.8672

Epoch 260: val_loss did not improve from 24.65952
196/196 - 71s - loss: 28.7908 - MinusLogProbMetric: 28.7908 - val_loss: 28.8672 - val_MinusLogProbMetric: 28.8672 - lr: 6.1728e-06 - 71s/epoch - 361ms/step
Epoch 261/1000
2023-09-23 22:02:57.880 
Epoch 261/1000 
	 loss: 28.7306, MinusLogProbMetric: 28.7306, val_loss: 28.8075, val_MinusLogProbMetric: 28.8075

Epoch 261: val_loss did not improve from 24.65952
196/196 - 71s - loss: 28.7306 - MinusLogProbMetric: 28.7306 - val_loss: 28.8075 - val_MinusLogProbMetric: 28.8075 - lr: 6.1728e-06 - 71s/epoch - 363ms/step
Epoch 262/1000
2023-09-23 22:04:10.192 
Epoch 262/1000 
	 loss: 28.6683, MinusLogProbMetric: 28.6683, val_loss: 28.7514, val_MinusLogProbMetric: 28.7514

Epoch 262: val_loss did not improve from 24.65952
196/196 - 72s - loss: 28.6683 - MinusLogProbMetric: 28.6683 - val_loss: 28.7514 - val_MinusLogProbMetric: 28.7514 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 263/1000
2023-09-23 22:05:21.811 
Epoch 263/1000 
	 loss: 28.6126, MinusLogProbMetric: 28.6126, val_loss: 28.6931, val_MinusLogProbMetric: 28.6931

Epoch 263: val_loss did not improve from 24.65952
196/196 - 72s - loss: 28.6126 - MinusLogProbMetric: 28.6126 - val_loss: 28.6931 - val_MinusLogProbMetric: 28.6931 - lr: 6.1728e-06 - 72s/epoch - 365ms/step
Epoch 264/1000
2023-09-23 22:06:34.309 
Epoch 264/1000 
	 loss: 28.5576, MinusLogProbMetric: 28.5576, val_loss: 28.6645, val_MinusLogProbMetric: 28.6645

Epoch 264: val_loss did not improve from 24.65952
196/196 - 72s - loss: 28.5576 - MinusLogProbMetric: 28.5576 - val_loss: 28.6645 - val_MinusLogProbMetric: 28.6645 - lr: 6.1728e-06 - 72s/epoch - 370ms/step
Epoch 265/1000
2023-09-23 22:07:46.179 
Epoch 265/1000 
	 loss: 28.4992, MinusLogProbMetric: 28.4992, val_loss: 28.5836, val_MinusLogProbMetric: 28.5836

Epoch 265: val_loss did not improve from 24.65952
Restoring model weights from the end of the best epoch: 165.
196/196 - 73s - loss: 28.4992 - MinusLogProbMetric: 28.4992 - val_loss: 28.5836 - val_MinusLogProbMetric: 28.5836 - lr: 6.1728e-06 - 73s/epoch - 370ms/step
Epoch 265: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 25.71379978799814 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 61.61961033600164 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
