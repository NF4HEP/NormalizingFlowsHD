2023-10-25 16:05:19.931027: Importing os...
2023-10-25 16:05:19.931124: Importing sys...
2023-10-25 16:05:19.931158: Importing and initializing argparse...
Visible devices: [3]
2023-10-25 16:05:19.949707: Importing timer from timeit...
2023-10-25 16:05:19.950406: Setting env variables for tf import (only device [3] will be available)...
2023-10-25 16:05:19.950457: Importing numpy...
2023-10-25 16:05:20.115012: Importing pandas...
2023-10-25 16:05:20.336040: Importing shutil...
2023-10-25 16:05:20.336073: Importing subprocess...
2023-10-25 16:05:20.336081: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-25 16:05:23.121210: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-25 16:05:23.607713: Importing textwrap...
2023-10-25 16:05:23.607755: Importing timeit...
2023-10-25 16:05:23.607768: Importing traceback...
2023-10-25 16:05:23.607776: Importing typing...
2023-10-25 16:05:23.607789: Setting tf configs...
2023-10-25 16:05:23.834474: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-25 16:05:25.242865: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

===========
Generating train data for run 357.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1321920   
 r)                                                              
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f10d4686c20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f10bc4fff10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f10bc4fff10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f10bc4abfa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f10bc4d7d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f10bc20c2b0>, <keras.callbacks.ModelCheckpoint object at 0x7f10bc20c400>, <keras.callbacks.EarlyStopping object at 0x7f10bc20c610>, <keras.callbacks.ReduceLROnPlateau object at 0x7f10bc20c640>, <keras.callbacks.TerminateOnNaN object at 0x7f10bc20c370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:05:33.059458
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:07:44.867 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 132s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 132s/epoch - 672ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 357.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f149497ff40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1494da92a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1494da92a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0e1461d360>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f149412a9b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f149412af20>, <keras.callbacks.ModelCheckpoint object at 0x7f149412afe0>, <keras.callbacks.EarlyStopping object at 0x7f149412b250>, <keras.callbacks.ReduceLROnPlateau object at 0x7f149412b280>, <keras.callbacks.TerminateOnNaN object at 0x7f149412aec0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:07:53.018220
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:09:57.733 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 124s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 124s/epoch - 635ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 357.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f0eb42d6290>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0f3c4d0a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0f3c4d0a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1000751810>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1493870f40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f14938714b0>, <keras.callbacks.ModelCheckpoint object at 0x7f1493871570>, <keras.callbacks.EarlyStopping object at 0x7f14938717e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1493871810>, <keras.callbacks.TerminateOnNaN object at 0x7f1493871450>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:10:06.185668
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:12:13.150 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 127s/epoch - 647ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 357.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f14609c8d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0eb42830a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0eb42830a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0fa4266710>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f146a551f90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f146a552500>, <keras.callbacks.ModelCheckpoint object at 0x7f146a5525c0>, <keras.callbacks.EarlyStopping object at 0x7f146a552830>, <keras.callbacks.ReduceLROnPlateau object at 0x7f146a552860>, <keras.callbacks.TerminateOnNaN object at 0x7f146a5524a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:12:21.310730
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:14:26.329 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 125s/epoch - 637ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 357.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f1493490a30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1493d80eb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1493d80eb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f146201fd00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1461e5a020>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1461e5a590>, <keras.callbacks.ModelCheckpoint object at 0x7f1461e5a650>, <keras.callbacks.EarlyStopping object at 0x7f1461e5a8c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1461e5a8f0>, <keras.callbacks.TerminateOnNaN object at 0x7f1461e5a530>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:14:34.145381
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0ecc15add0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:16:36.720 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 122s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 122s/epoch - 625ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 357.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f0f3c751a20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0f5c765390>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0f5c765390>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1493621c30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0e144378e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0e14437190>, <keras.callbacks.ModelCheckpoint object at 0x7f0e14437070>, <keras.callbacks.EarlyStopping object at 0x7f0e14436bc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0e14436c80>, <keras.callbacks.TerminateOnNaN object at 0x7f0e14437220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:16:44.949050
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0e24141bd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:18:53.901 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 129s/epoch - 658ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 357.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f0f2824fbe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0fa4598f70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0fa4598f70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0edc2d41c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0edc710580>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0edc7117b0>, <keras.callbacks.ModelCheckpoint object at 0x7f0edc7115a0>, <keras.callbacks.EarlyStopping object at 0x7f0edc711e40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0edc711c90>, <keras.callbacks.TerminateOnNaN object at 0x7f0edc710ee0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:19:01.922609
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:21:08.968 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 127s/epoch - 648ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 357.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f0db1177940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0db06d7a00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0db06d7a00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0db02e60e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0db2d7e380>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0db2d7e8f0>, <keras.callbacks.ModelCheckpoint object at 0x7f0db2d7e9b0>, <keras.callbacks.EarlyStopping object at 0x7f0db2d7ec20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0db2d7ec50>, <keras.callbacks.TerminateOnNaN object at 0x7f0db2d7e890>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:21:18.699114
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:23:29.490 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 131s/epoch - 666ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 357.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f1460dcaf20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0de579b790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0de579b790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f10a42393f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1460dafbb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1460daf0d0>, <keras.callbacks.ModelCheckpoint object at 0x7f1460daea40>, <keras.callbacks.EarlyStopping object at 0x7f1460dae6e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1460dae860>, <keras.callbacks.TerminateOnNaN object at 0x7f1460daed10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:23:36.536592
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:25:48.312 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 132s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 132s/epoch - 672ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 357.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f0de48ab670>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f146195e980>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f146195e980>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f14619ec430>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f14618546a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1461854c10>, <keras.callbacks.ModelCheckpoint object at 0x7f1461854cd0>, <keras.callbacks.EarlyStopping object at 0x7f1461854f40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1461854f70>, <keras.callbacks.TerminateOnNaN object at 0x7f1461854bb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:25:59.717021
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:28:11.968 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 132s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 132s/epoch - 673ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 357.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_357/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_357
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f0ee47cb310>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1493f766b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1493f766b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0ee47cbe80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0db0db91b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_357/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0db0db9720>, <keras.callbacks.ModelCheckpoint object at 0x7f0db0db97e0>, <keras.callbacks.EarlyStopping object at 0x7f0db0db9a50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0db0db9a80>, <keras.callbacks.TerminateOnNaN object at 0x7f0db0db96c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_357/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 357/720 with hyperparameters:
timestamp = 2023-10-25 16:28:21.473935
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:30:31.346 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7470.7720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 7470.7720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 130s/epoch - 662ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 357/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

===========
Generating train data for run 362.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_362/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_362/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_362/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_362
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f0d8696bf10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d85bdb820>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d85bdb820>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d87700850>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d86720ac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d86721030>, <keras.callbacks.ModelCheckpoint object at 0x7f0d867210f0>, <keras.callbacks.EarlyStopping object at 0x7f0d86721360>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d86721390>, <keras.callbacks.TerminateOnNaN object at 0x7f0d86720fd0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_362/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 362/720 with hyperparameters:
timestamp = 2023-10-25 16:30:36.329254
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 16:32:09.596 
Epoch 1/1000 
	 loss: 348.5058, MinusLogProbMetric: 348.5058, val_loss: 109.2056, val_MinusLogProbMetric: 109.2056

Epoch 1: val_loss improved from inf to 109.20562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 94s - loss: 348.5058 - MinusLogProbMetric: 348.5058 - val_loss: 109.2056 - val_MinusLogProbMetric: 109.2056 - lr: 0.0010 - 94s/epoch - 478ms/step
Epoch 2/1000
2023-10-25 16:32:44.339 
Epoch 2/1000 
	 loss: 84.0582, MinusLogProbMetric: 84.0582, val_loss: 80.0152, val_MinusLogProbMetric: 80.0152

Epoch 2: val_loss improved from 109.20562 to 80.01524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 84.0582 - MinusLogProbMetric: 84.0582 - val_loss: 80.0152 - val_MinusLogProbMetric: 80.0152 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 3/1000
2023-10-25 16:33:16.660 
Epoch 3/1000 
	 loss: 63.8181, MinusLogProbMetric: 63.8181, val_loss: 57.3058, val_MinusLogProbMetric: 57.3058

Epoch 3: val_loss improved from 80.01524 to 57.30580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 32s - loss: 63.8181 - MinusLogProbMetric: 63.8181 - val_loss: 57.3058 - val_MinusLogProbMetric: 57.3058 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 4/1000
2023-10-25 16:33:47.403 
Epoch 4/1000 
	 loss: 54.2115, MinusLogProbMetric: 54.2115, val_loss: 51.6419, val_MinusLogProbMetric: 51.6419

Epoch 4: val_loss improved from 57.30580 to 51.64186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 31s - loss: 54.2115 - MinusLogProbMetric: 54.2115 - val_loss: 51.6419 - val_MinusLogProbMetric: 51.6419 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 5/1000
2023-10-25 16:34:17.699 
Epoch 5/1000 
	 loss: 49.1206, MinusLogProbMetric: 49.1206, val_loss: 46.1374, val_MinusLogProbMetric: 46.1374

Epoch 5: val_loss improved from 51.64186 to 46.13744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 30s - loss: 49.1206 - MinusLogProbMetric: 49.1206 - val_loss: 46.1374 - val_MinusLogProbMetric: 46.1374 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 6/1000
2023-10-25 16:34:48.005 
Epoch 6/1000 
	 loss: 46.1776, MinusLogProbMetric: 46.1776, val_loss: 47.0928, val_MinusLogProbMetric: 47.0928

Epoch 6: val_loss did not improve from 46.13744
196/196 - 30s - loss: 46.1776 - MinusLogProbMetric: 46.1776 - val_loss: 47.0928 - val_MinusLogProbMetric: 47.0928 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 7/1000
2023-10-25 16:35:18.334 
Epoch 7/1000 
	 loss: 75.5577, MinusLogProbMetric: 75.5577, val_loss: 53.6793, val_MinusLogProbMetric: 53.6793

Epoch 7: val_loss did not improve from 46.13744
196/196 - 30s - loss: 75.5577 - MinusLogProbMetric: 75.5577 - val_loss: 53.6793 - val_MinusLogProbMetric: 53.6793 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 8/1000
2023-10-25 16:35:48.433 
Epoch 8/1000 
	 loss: 47.9846, MinusLogProbMetric: 47.9846, val_loss: 47.5912, val_MinusLogProbMetric: 47.5912

Epoch 8: val_loss did not improve from 46.13744
196/196 - 30s - loss: 47.9846 - MinusLogProbMetric: 47.9846 - val_loss: 47.5912 - val_MinusLogProbMetric: 47.5912 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 9/1000
2023-10-25 16:36:18.363 
Epoch 9/1000 
	 loss: 43.0210, MinusLogProbMetric: 43.0210, val_loss: 41.0629, val_MinusLogProbMetric: 41.0629

Epoch 9: val_loss improved from 46.13744 to 41.06293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 30s - loss: 43.0210 - MinusLogProbMetric: 43.0210 - val_loss: 41.0629 - val_MinusLogProbMetric: 41.0629 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 10/1000
2023-10-25 16:36:49.159 
Epoch 10/1000 
	 loss: 41.0738, MinusLogProbMetric: 41.0738, val_loss: 39.1753, val_MinusLogProbMetric: 39.1753

Epoch 10: val_loss improved from 41.06293 to 39.17532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 31s - loss: 41.0738 - MinusLogProbMetric: 41.0738 - val_loss: 39.1753 - val_MinusLogProbMetric: 39.1753 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 11/1000
2023-10-25 16:37:20.657 
Epoch 11/1000 
	 loss: 39.5485, MinusLogProbMetric: 39.5485, val_loss: 40.4037, val_MinusLogProbMetric: 40.4037

Epoch 11: val_loss did not improve from 39.17532
196/196 - 31s - loss: 39.5485 - MinusLogProbMetric: 39.5485 - val_loss: 40.4037 - val_MinusLogProbMetric: 40.4037 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 12/1000
2023-10-25 16:37:50.703 
Epoch 12/1000 
	 loss: 38.8705, MinusLogProbMetric: 38.8705, val_loss: 37.7819, val_MinusLogProbMetric: 37.7819

Epoch 12: val_loss improved from 39.17532 to 37.78187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 30s - loss: 38.8705 - MinusLogProbMetric: 38.8705 - val_loss: 37.7819 - val_MinusLogProbMetric: 37.7819 - lr: 0.0010 - 30s/epoch - 156ms/step
Epoch 13/1000
2023-10-25 16:38:21.298 
Epoch 13/1000 
	 loss: 38.3891, MinusLogProbMetric: 38.3891, val_loss: 37.8325, val_MinusLogProbMetric: 37.8325

Epoch 13: val_loss did not improve from 37.78187
196/196 - 30s - loss: 38.3891 - MinusLogProbMetric: 38.3891 - val_loss: 37.8325 - val_MinusLogProbMetric: 37.8325 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 14/1000
2023-10-25 16:38:51.534 
Epoch 14/1000 
	 loss: 36.9441, MinusLogProbMetric: 36.9441, val_loss: 38.0718, val_MinusLogProbMetric: 38.0718

Epoch 14: val_loss did not improve from 37.78187
196/196 - 30s - loss: 36.9441 - MinusLogProbMetric: 36.9441 - val_loss: 38.0718 - val_MinusLogProbMetric: 38.0718 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 15/1000
2023-10-25 16:39:21.614 
Epoch 15/1000 
	 loss: 37.0490, MinusLogProbMetric: 37.0490, val_loss: 38.1831, val_MinusLogProbMetric: 38.1831

Epoch 15: val_loss did not improve from 37.78187
196/196 - 30s - loss: 37.0490 - MinusLogProbMetric: 37.0490 - val_loss: 38.1831 - val_MinusLogProbMetric: 38.1831 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 16/1000
2023-10-25 16:39:52.082 
Epoch 16/1000 
	 loss: 35.9274, MinusLogProbMetric: 35.9274, val_loss: 38.6103, val_MinusLogProbMetric: 38.6103

Epoch 16: val_loss did not improve from 37.78187
196/196 - 30s - loss: 35.9274 - MinusLogProbMetric: 35.9274 - val_loss: 38.6103 - val_MinusLogProbMetric: 38.6103 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 17/1000
2023-10-25 16:40:24.026 
Epoch 17/1000 
	 loss: 35.4487, MinusLogProbMetric: 35.4487, val_loss: 35.9494, val_MinusLogProbMetric: 35.9494

Epoch 17: val_loss improved from 37.78187 to 35.94938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 33s - loss: 35.4487 - MinusLogProbMetric: 35.4487 - val_loss: 35.9494 - val_MinusLogProbMetric: 35.9494 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 18/1000
2023-10-25 16:40:58.403 
Epoch 18/1000 
	 loss: 35.6484, MinusLogProbMetric: 35.6484, val_loss: 36.4642, val_MinusLogProbMetric: 36.4642

Epoch 18: val_loss did not improve from 35.94938
196/196 - 34s - loss: 35.6484 - MinusLogProbMetric: 35.6484 - val_loss: 36.4642 - val_MinusLogProbMetric: 36.4642 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 19/1000
2023-10-25 16:41:30.247 
Epoch 19/1000 
	 loss: 35.3632, MinusLogProbMetric: 35.3632, val_loss: 33.9739, val_MinusLogProbMetric: 33.9739

Epoch 19: val_loss improved from 35.94938 to 33.97389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 32s - loss: 35.3632 - MinusLogProbMetric: 35.3632 - val_loss: 33.9739 - val_MinusLogProbMetric: 33.9739 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 20/1000
2023-10-25 16:42:00.724 
Epoch 20/1000 
	 loss: 35.3032, MinusLogProbMetric: 35.3032, val_loss: 33.7008, val_MinusLogProbMetric: 33.7008

Epoch 20: val_loss improved from 33.97389 to 33.70080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 30s - loss: 35.3032 - MinusLogProbMetric: 35.3032 - val_loss: 33.7008 - val_MinusLogProbMetric: 33.7008 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 21/1000
2023-10-25 16:42:31.220 
Epoch 21/1000 
	 loss: 34.7437, MinusLogProbMetric: 34.7437, val_loss: 34.0555, val_MinusLogProbMetric: 34.0555

Epoch 21: val_loss did not improve from 33.70080
196/196 - 30s - loss: 34.7437 - MinusLogProbMetric: 34.7437 - val_loss: 34.0555 - val_MinusLogProbMetric: 34.0555 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 22/1000
2023-10-25 16:43:01.513 
Epoch 22/1000 
	 loss: 34.8510, MinusLogProbMetric: 34.8510, val_loss: 33.4832, val_MinusLogProbMetric: 33.4832

Epoch 22: val_loss improved from 33.70080 to 33.48317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 31s - loss: 34.8510 - MinusLogProbMetric: 34.8510 - val_loss: 33.4832 - val_MinusLogProbMetric: 33.4832 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 23/1000
2023-10-25 16:43:35.214 
Epoch 23/1000 
	 loss: 34.0737, MinusLogProbMetric: 34.0737, val_loss: 35.3552, val_MinusLogProbMetric: 35.3552

Epoch 23: val_loss did not improve from 33.48317
196/196 - 33s - loss: 34.0737 - MinusLogProbMetric: 34.0737 - val_loss: 35.3552 - val_MinusLogProbMetric: 35.3552 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 24/1000
2023-10-25 16:44:09.216 
Epoch 24/1000 
	 loss: 33.7847, MinusLogProbMetric: 33.7847, val_loss: 33.5932, val_MinusLogProbMetric: 33.5932

Epoch 24: val_loss did not improve from 33.48317
196/196 - 34s - loss: 33.7847 - MinusLogProbMetric: 33.7847 - val_loss: 33.5932 - val_MinusLogProbMetric: 33.5932 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 25/1000
2023-10-25 16:44:44.018 
Epoch 25/1000 
	 loss: 33.9999, MinusLogProbMetric: 33.9999, val_loss: 34.4334, val_MinusLogProbMetric: 34.4334

Epoch 25: val_loss did not improve from 33.48317
196/196 - 35s - loss: 33.9999 - MinusLogProbMetric: 33.9999 - val_loss: 34.4334 - val_MinusLogProbMetric: 34.4334 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 26/1000
2023-10-25 16:45:18.184 
Epoch 26/1000 
	 loss: 33.5671, MinusLogProbMetric: 33.5671, val_loss: 33.1556, val_MinusLogProbMetric: 33.1556

Epoch 26: val_loss improved from 33.48317 to 33.15558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 33.5671 - MinusLogProbMetric: 33.5671 - val_loss: 33.1556 - val_MinusLogProbMetric: 33.1556 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 27/1000
2023-10-25 16:45:53.295 
Epoch 27/1000 
	 loss: 33.4311, MinusLogProbMetric: 33.4311, val_loss: 33.8577, val_MinusLogProbMetric: 33.8577

Epoch 27: val_loss did not improve from 33.15558
196/196 - 35s - loss: 33.4311 - MinusLogProbMetric: 33.4311 - val_loss: 33.8577 - val_MinusLogProbMetric: 33.8577 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 28/1000
2023-10-25 16:46:28.006 
Epoch 28/1000 
	 loss: 33.2958, MinusLogProbMetric: 33.2958, val_loss: 34.5743, val_MinusLogProbMetric: 34.5743

Epoch 28: val_loss did not improve from 33.15558
196/196 - 35s - loss: 33.2958 - MinusLogProbMetric: 33.2958 - val_loss: 34.5743 - val_MinusLogProbMetric: 34.5743 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 29/1000
2023-10-25 16:47:02.724 
Epoch 29/1000 
	 loss: 33.3195, MinusLogProbMetric: 33.3195, val_loss: 33.4827, val_MinusLogProbMetric: 33.4827

Epoch 29: val_loss did not improve from 33.15558
196/196 - 35s - loss: 33.3195 - MinusLogProbMetric: 33.3195 - val_loss: 33.4827 - val_MinusLogProbMetric: 33.4827 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 30/1000
2023-10-25 16:47:37.586 
Epoch 30/1000 
	 loss: 32.9289, MinusLogProbMetric: 32.9289, val_loss: 32.7759, val_MinusLogProbMetric: 32.7759

Epoch 30: val_loss improved from 33.15558 to 32.77591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 32.9289 - MinusLogProbMetric: 32.9289 - val_loss: 32.7759 - val_MinusLogProbMetric: 32.7759 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 31/1000
2023-10-25 16:48:13.305 
Epoch 31/1000 
	 loss: 32.8513, MinusLogProbMetric: 32.8513, val_loss: 35.3405, val_MinusLogProbMetric: 35.3405

Epoch 31: val_loss did not improve from 32.77591
196/196 - 35s - loss: 32.8513 - MinusLogProbMetric: 32.8513 - val_loss: 35.3405 - val_MinusLogProbMetric: 35.3405 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 32/1000
2023-10-25 16:48:48.234 
Epoch 32/1000 
	 loss: 32.8620, MinusLogProbMetric: 32.8620, val_loss: 34.7238, val_MinusLogProbMetric: 34.7238

Epoch 32: val_loss did not improve from 32.77591
196/196 - 35s - loss: 32.8620 - MinusLogProbMetric: 32.8620 - val_loss: 34.7238 - val_MinusLogProbMetric: 34.7238 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 33/1000
2023-10-25 16:49:23.437 
Epoch 33/1000 
	 loss: 32.7620, MinusLogProbMetric: 32.7620, val_loss: 33.2477, val_MinusLogProbMetric: 33.2477

Epoch 33: val_loss did not improve from 32.77591
196/196 - 35s - loss: 32.7620 - MinusLogProbMetric: 32.7620 - val_loss: 33.2477 - val_MinusLogProbMetric: 33.2477 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 34/1000
2023-10-25 16:49:58.356 
Epoch 34/1000 
	 loss: 32.6466, MinusLogProbMetric: 32.6466, val_loss: 32.6724, val_MinusLogProbMetric: 32.6724

Epoch 34: val_loss improved from 32.77591 to 32.67236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 32.6466 - MinusLogProbMetric: 32.6466 - val_loss: 32.6724 - val_MinusLogProbMetric: 32.6724 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 35/1000
2023-10-25 16:50:32.207 
Epoch 35/1000 
	 loss: 32.8295, MinusLogProbMetric: 32.8295, val_loss: 34.0288, val_MinusLogProbMetric: 34.0288

Epoch 35: val_loss did not improve from 32.67236
196/196 - 33s - loss: 32.8295 - MinusLogProbMetric: 32.8295 - val_loss: 34.0288 - val_MinusLogProbMetric: 34.0288 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 36/1000
2023-10-25 16:51:05.883 
Epoch 36/1000 
	 loss: 32.5161, MinusLogProbMetric: 32.5161, val_loss: 32.2996, val_MinusLogProbMetric: 32.2996

Epoch 36: val_loss improved from 32.67236 to 32.29964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 34s - loss: 32.5161 - MinusLogProbMetric: 32.5161 - val_loss: 32.2996 - val_MinusLogProbMetric: 32.2996 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 37/1000
2023-10-25 16:51:38.990 
Epoch 37/1000 
	 loss: 32.2416, MinusLogProbMetric: 32.2416, val_loss: 33.5074, val_MinusLogProbMetric: 33.5074

Epoch 37: val_loss did not improve from 32.29964
196/196 - 33s - loss: 32.2416 - MinusLogProbMetric: 32.2416 - val_loss: 33.5074 - val_MinusLogProbMetric: 33.5074 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 38/1000
2023-10-25 16:52:10.641 
Epoch 38/1000 
	 loss: 32.2775, MinusLogProbMetric: 32.2775, val_loss: 33.0697, val_MinusLogProbMetric: 33.0697

Epoch 38: val_loss did not improve from 32.29964
196/196 - 32s - loss: 32.2775 - MinusLogProbMetric: 32.2775 - val_loss: 33.0697 - val_MinusLogProbMetric: 33.0697 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 39/1000
2023-10-25 16:52:43.554 
Epoch 39/1000 
	 loss: 32.5302, MinusLogProbMetric: 32.5302, val_loss: 31.9843, val_MinusLogProbMetric: 31.9843

Epoch 39: val_loss improved from 32.29964 to 31.98426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 33s - loss: 32.5302 - MinusLogProbMetric: 32.5302 - val_loss: 31.9843 - val_MinusLogProbMetric: 31.9843 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 40/1000
2023-10-25 16:53:19.098 
Epoch 40/1000 
	 loss: 31.9708, MinusLogProbMetric: 31.9708, val_loss: 33.1528, val_MinusLogProbMetric: 33.1528

Epoch 40: val_loss did not improve from 31.98426
196/196 - 35s - loss: 31.9708 - MinusLogProbMetric: 31.9708 - val_loss: 33.1528 - val_MinusLogProbMetric: 33.1528 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 41/1000
2023-10-25 16:53:54.134 
Epoch 41/1000 
	 loss: 32.2130, MinusLogProbMetric: 32.2130, val_loss: 32.2506, val_MinusLogProbMetric: 32.2506

Epoch 41: val_loss did not improve from 31.98426
196/196 - 35s - loss: 32.2130 - MinusLogProbMetric: 32.2130 - val_loss: 32.2506 - val_MinusLogProbMetric: 32.2506 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 42/1000
2023-10-25 16:54:29.139 
Epoch 42/1000 
	 loss: 31.9035, MinusLogProbMetric: 31.9035, val_loss: 31.5646, val_MinusLogProbMetric: 31.5646

Epoch 42: val_loss improved from 31.98426 to 31.56458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 31.9035 - MinusLogProbMetric: 31.9035 - val_loss: 31.5646 - val_MinusLogProbMetric: 31.5646 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 43/1000
2023-10-25 16:55:04.743 
Epoch 43/1000 
	 loss: 32.0843, MinusLogProbMetric: 32.0843, val_loss: 32.1266, val_MinusLogProbMetric: 32.1266

Epoch 43: val_loss did not improve from 31.56458
196/196 - 35s - loss: 32.0843 - MinusLogProbMetric: 32.0843 - val_loss: 32.1266 - val_MinusLogProbMetric: 32.1266 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 44/1000
2023-10-25 16:55:39.978 
Epoch 44/1000 
	 loss: 32.0442, MinusLogProbMetric: 32.0442, val_loss: 32.8495, val_MinusLogProbMetric: 32.8495

Epoch 44: val_loss did not improve from 31.56458
196/196 - 35s - loss: 32.0442 - MinusLogProbMetric: 32.0442 - val_loss: 32.8495 - val_MinusLogProbMetric: 32.8495 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 45/1000
2023-10-25 16:56:12.733 
Epoch 45/1000 
	 loss: 31.9262, MinusLogProbMetric: 31.9262, val_loss: 31.8267, val_MinusLogProbMetric: 31.8267

Epoch 45: val_loss did not improve from 31.56458
196/196 - 33s - loss: 31.9262 - MinusLogProbMetric: 31.9262 - val_loss: 31.8267 - val_MinusLogProbMetric: 31.8267 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 46/1000
2023-10-25 16:56:45.603 
Epoch 46/1000 
	 loss: 32.1656, MinusLogProbMetric: 32.1656, val_loss: 32.2766, val_MinusLogProbMetric: 32.2766

Epoch 46: val_loss did not improve from 31.56458
196/196 - 33s - loss: 32.1656 - MinusLogProbMetric: 32.1656 - val_loss: 32.2766 - val_MinusLogProbMetric: 32.2766 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 47/1000
2023-10-25 16:57:19.804 
Epoch 47/1000 
	 loss: 31.5834, MinusLogProbMetric: 31.5834, val_loss: 33.0249, val_MinusLogProbMetric: 33.0249

Epoch 47: val_loss did not improve from 31.56458
196/196 - 34s - loss: 31.5834 - MinusLogProbMetric: 31.5834 - val_loss: 33.0249 - val_MinusLogProbMetric: 33.0249 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 48/1000
2023-10-25 16:57:52.784 
Epoch 48/1000 
	 loss: 31.5777, MinusLogProbMetric: 31.5777, val_loss: 31.9337, val_MinusLogProbMetric: 31.9337

Epoch 48: val_loss did not improve from 31.56458
196/196 - 33s - loss: 31.5777 - MinusLogProbMetric: 31.5777 - val_loss: 31.9337 - val_MinusLogProbMetric: 31.9337 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 49/1000
2023-10-25 16:58:28.111 
Epoch 49/1000 
	 loss: 31.8156, MinusLogProbMetric: 31.8156, val_loss: 31.8955, val_MinusLogProbMetric: 31.8955

Epoch 49: val_loss did not improve from 31.56458
196/196 - 35s - loss: 31.8156 - MinusLogProbMetric: 31.8156 - val_loss: 31.8955 - val_MinusLogProbMetric: 31.8955 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 50/1000
2023-10-25 16:59:02.836 
Epoch 50/1000 
	 loss: 31.5315, MinusLogProbMetric: 31.5315, val_loss: 31.4182, val_MinusLogProbMetric: 31.4182

Epoch 50: val_loss improved from 31.56458 to 31.41816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 31.5315 - MinusLogProbMetric: 31.5315 - val_loss: 31.4182 - val_MinusLogProbMetric: 31.4182 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 51/1000
2023-10-25 16:59:38.362 
Epoch 51/1000 
	 loss: 31.6909, MinusLogProbMetric: 31.6909, val_loss: 31.4650, val_MinusLogProbMetric: 31.4650

Epoch 51: val_loss did not improve from 31.41816
196/196 - 35s - loss: 31.6909 - MinusLogProbMetric: 31.6909 - val_loss: 31.4650 - val_MinusLogProbMetric: 31.4650 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 52/1000
2023-10-25 17:00:13.442 
Epoch 52/1000 
	 loss: 31.7612, MinusLogProbMetric: 31.7612, val_loss: 31.3833, val_MinusLogProbMetric: 31.3833

Epoch 52: val_loss improved from 31.41816 to 31.38328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 31.7612 - MinusLogProbMetric: 31.7612 - val_loss: 31.3833 - val_MinusLogProbMetric: 31.3833 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 53/1000
2023-10-25 17:00:49.272 
Epoch 53/1000 
	 loss: 31.5687, MinusLogProbMetric: 31.5687, val_loss: 33.2694, val_MinusLogProbMetric: 33.2694

Epoch 53: val_loss did not improve from 31.38328
196/196 - 35s - loss: 31.5687 - MinusLogProbMetric: 31.5687 - val_loss: 33.2694 - val_MinusLogProbMetric: 33.2694 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 54/1000
2023-10-25 17:01:24.444 
Epoch 54/1000 
	 loss: 31.4067, MinusLogProbMetric: 31.4067, val_loss: 30.8936, val_MinusLogProbMetric: 30.8936

Epoch 54: val_loss improved from 31.38328 to 30.89357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 31.4067 - MinusLogProbMetric: 31.4067 - val_loss: 30.8936 - val_MinusLogProbMetric: 30.8936 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 55/1000
2023-10-25 17:02:00.489 
Epoch 55/1000 
	 loss: 31.4942, MinusLogProbMetric: 31.4942, val_loss: 31.5864, val_MinusLogProbMetric: 31.5864

Epoch 55: val_loss did not improve from 30.89357
196/196 - 36s - loss: 31.4942 - MinusLogProbMetric: 31.4942 - val_loss: 31.5864 - val_MinusLogProbMetric: 31.5864 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 56/1000
2023-10-25 17:02:36.015 
Epoch 56/1000 
	 loss: 31.2411, MinusLogProbMetric: 31.2411, val_loss: 33.5503, val_MinusLogProbMetric: 33.5503

Epoch 56: val_loss did not improve from 30.89357
196/196 - 36s - loss: 31.2411 - MinusLogProbMetric: 31.2411 - val_loss: 33.5503 - val_MinusLogProbMetric: 33.5503 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 57/1000
2023-10-25 17:03:11.598 
Epoch 57/1000 
	 loss: 31.5017, MinusLogProbMetric: 31.5017, val_loss: 31.4639, val_MinusLogProbMetric: 31.4639

Epoch 57: val_loss did not improve from 30.89357
196/196 - 36s - loss: 31.5017 - MinusLogProbMetric: 31.5017 - val_loss: 31.4639 - val_MinusLogProbMetric: 31.4639 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 58/1000
2023-10-25 17:03:45.111 
Epoch 58/1000 
	 loss: 31.2073, MinusLogProbMetric: 31.2073, val_loss: 31.3615, val_MinusLogProbMetric: 31.3615

Epoch 58: val_loss did not improve from 30.89357
196/196 - 34s - loss: 31.2073 - MinusLogProbMetric: 31.2073 - val_loss: 31.3615 - val_MinusLogProbMetric: 31.3615 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 59/1000
2023-10-25 17:04:19.828 
Epoch 59/1000 
	 loss: 31.2579, MinusLogProbMetric: 31.2579, val_loss: 31.0658, val_MinusLogProbMetric: 31.0658

Epoch 59: val_loss did not improve from 30.89357
196/196 - 35s - loss: 31.2579 - MinusLogProbMetric: 31.2579 - val_loss: 31.0658 - val_MinusLogProbMetric: 31.0658 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 60/1000
2023-10-25 17:04:53.257 
Epoch 60/1000 
	 loss: 31.2287, MinusLogProbMetric: 31.2287, val_loss: 31.5029, val_MinusLogProbMetric: 31.5029

Epoch 60: val_loss did not improve from 30.89357
196/196 - 33s - loss: 31.2287 - MinusLogProbMetric: 31.2287 - val_loss: 31.5029 - val_MinusLogProbMetric: 31.5029 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 61/1000
2023-10-25 17:05:28.611 
Epoch 61/1000 
	 loss: 31.1380, MinusLogProbMetric: 31.1380, val_loss: 31.7978, val_MinusLogProbMetric: 31.7978

Epoch 61: val_loss did not improve from 30.89357
196/196 - 35s - loss: 31.1380 - MinusLogProbMetric: 31.1380 - val_loss: 31.7978 - val_MinusLogProbMetric: 31.7978 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 62/1000
2023-10-25 17:06:03.899 
Epoch 62/1000 
	 loss: 31.0850, MinusLogProbMetric: 31.0850, val_loss: 31.3967, val_MinusLogProbMetric: 31.3967

Epoch 62: val_loss did not improve from 30.89357
196/196 - 35s - loss: 31.0850 - MinusLogProbMetric: 31.0850 - val_loss: 31.3967 - val_MinusLogProbMetric: 31.3967 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 63/1000
2023-10-25 17:06:38.970 
Epoch 63/1000 
	 loss: 31.0544, MinusLogProbMetric: 31.0544, val_loss: 31.3303, val_MinusLogProbMetric: 31.3303

Epoch 63: val_loss did not improve from 30.89357
196/196 - 35s - loss: 31.0544 - MinusLogProbMetric: 31.0544 - val_loss: 31.3303 - val_MinusLogProbMetric: 31.3303 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 64/1000
2023-10-25 17:07:13.684 
Epoch 64/1000 
	 loss: 31.0826, MinusLogProbMetric: 31.0826, val_loss: 31.1668, val_MinusLogProbMetric: 31.1668

Epoch 64: val_loss did not improve from 30.89357
196/196 - 35s - loss: 31.0826 - MinusLogProbMetric: 31.0826 - val_loss: 31.1668 - val_MinusLogProbMetric: 31.1668 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 65/1000
2023-10-25 17:07:48.974 
Epoch 65/1000 
	 loss: 30.9596, MinusLogProbMetric: 30.9596, val_loss: 31.1359, val_MinusLogProbMetric: 31.1359

Epoch 65: val_loss did not improve from 30.89357
196/196 - 35s - loss: 30.9596 - MinusLogProbMetric: 30.9596 - val_loss: 31.1359 - val_MinusLogProbMetric: 31.1359 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 66/1000
2023-10-25 17:08:24.502 
Epoch 66/1000 
	 loss: 31.1254, MinusLogProbMetric: 31.1254, val_loss: 31.3615, val_MinusLogProbMetric: 31.3615

Epoch 66: val_loss did not improve from 30.89357
196/196 - 36s - loss: 31.1254 - MinusLogProbMetric: 31.1254 - val_loss: 31.3615 - val_MinusLogProbMetric: 31.3615 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 67/1000
2023-10-25 17:08:58.459 
Epoch 67/1000 
	 loss: 30.8368, MinusLogProbMetric: 30.8368, val_loss: 31.1304, val_MinusLogProbMetric: 31.1304

Epoch 67: val_loss did not improve from 30.89357
196/196 - 34s - loss: 30.8368 - MinusLogProbMetric: 30.8368 - val_loss: 31.1304 - val_MinusLogProbMetric: 31.1304 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 68/1000
2023-10-25 17:09:32.928 
Epoch 68/1000 
	 loss: 30.9294, MinusLogProbMetric: 30.9294, val_loss: 31.5185, val_MinusLogProbMetric: 31.5185

Epoch 68: val_loss did not improve from 30.89357
196/196 - 34s - loss: 30.9294 - MinusLogProbMetric: 30.9294 - val_loss: 31.5185 - val_MinusLogProbMetric: 31.5185 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 69/1000
2023-10-25 17:10:07.916 
Epoch 69/1000 
	 loss: 31.2417, MinusLogProbMetric: 31.2417, val_loss: 31.7036, val_MinusLogProbMetric: 31.7036

Epoch 69: val_loss did not improve from 30.89357
196/196 - 35s - loss: 31.2417 - MinusLogProbMetric: 31.2417 - val_loss: 31.7036 - val_MinusLogProbMetric: 31.7036 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 70/1000
2023-10-25 17:10:40.198 
Epoch 70/1000 
	 loss: 30.8401, MinusLogProbMetric: 30.8401, val_loss: 30.3309, val_MinusLogProbMetric: 30.3309

Epoch 70: val_loss improved from 30.89357 to 30.33088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 33s - loss: 30.8401 - MinusLogProbMetric: 30.8401 - val_loss: 30.3309 - val_MinusLogProbMetric: 30.3309 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 71/1000
2023-10-25 17:11:12.807 
Epoch 71/1000 
	 loss: 30.8897, MinusLogProbMetric: 30.8897, val_loss: 30.6484, val_MinusLogProbMetric: 30.6484

Epoch 71: val_loss did not improve from 30.33088
196/196 - 32s - loss: 30.8897 - MinusLogProbMetric: 30.8897 - val_loss: 30.6484 - val_MinusLogProbMetric: 30.6484 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 72/1000
2023-10-25 17:11:47.675 
Epoch 72/1000 
	 loss: 30.9192, MinusLogProbMetric: 30.9192, val_loss: 30.4973, val_MinusLogProbMetric: 30.4973

Epoch 72: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.9192 - MinusLogProbMetric: 30.9192 - val_loss: 30.4973 - val_MinusLogProbMetric: 30.4973 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 73/1000
2023-10-25 17:12:22.796 
Epoch 73/1000 
	 loss: 30.7562, MinusLogProbMetric: 30.7562, val_loss: 30.7242, val_MinusLogProbMetric: 30.7242

Epoch 73: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.7562 - MinusLogProbMetric: 30.7562 - val_loss: 30.7242 - val_MinusLogProbMetric: 30.7242 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 74/1000
2023-10-25 17:12:57.875 
Epoch 74/1000 
	 loss: 30.8676, MinusLogProbMetric: 30.8676, val_loss: 30.5587, val_MinusLogProbMetric: 30.5587

Epoch 74: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.8676 - MinusLogProbMetric: 30.8676 - val_loss: 30.5587 - val_MinusLogProbMetric: 30.5587 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 75/1000
2023-10-25 17:13:32.836 
Epoch 75/1000 
	 loss: 30.7787, MinusLogProbMetric: 30.7787, val_loss: 31.4126, val_MinusLogProbMetric: 31.4126

Epoch 75: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.7787 - MinusLogProbMetric: 30.7787 - val_loss: 31.4126 - val_MinusLogProbMetric: 31.4126 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 76/1000
2023-10-25 17:14:07.903 
Epoch 76/1000 
	 loss: 30.7840, MinusLogProbMetric: 30.7840, val_loss: 31.0866, val_MinusLogProbMetric: 31.0866

Epoch 76: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.7840 - MinusLogProbMetric: 30.7840 - val_loss: 31.0866 - val_MinusLogProbMetric: 31.0866 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 77/1000
2023-10-25 17:14:43.065 
Epoch 77/1000 
	 loss: 30.6257, MinusLogProbMetric: 30.6257, val_loss: 30.6751, val_MinusLogProbMetric: 30.6751

Epoch 77: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.6257 - MinusLogProbMetric: 30.6257 - val_loss: 30.6751 - val_MinusLogProbMetric: 30.6751 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 78/1000
2023-10-25 17:15:18.365 
Epoch 78/1000 
	 loss: 30.7492, MinusLogProbMetric: 30.7492, val_loss: 30.6454, val_MinusLogProbMetric: 30.6454

Epoch 78: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.7492 - MinusLogProbMetric: 30.7492 - val_loss: 30.6454 - val_MinusLogProbMetric: 30.6454 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 79/1000
2023-10-25 17:15:53.543 
Epoch 79/1000 
	 loss: 30.5916, MinusLogProbMetric: 30.5916, val_loss: 31.6063, val_MinusLogProbMetric: 31.6063

Epoch 79: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.5916 - MinusLogProbMetric: 30.5916 - val_loss: 31.6063 - val_MinusLogProbMetric: 31.6063 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 80/1000
2023-10-25 17:16:28.720 
Epoch 80/1000 
	 loss: 30.8877, MinusLogProbMetric: 30.8877, val_loss: 30.3744, val_MinusLogProbMetric: 30.3744

Epoch 80: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.8877 - MinusLogProbMetric: 30.8877 - val_loss: 30.3744 - val_MinusLogProbMetric: 30.3744 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 81/1000
2023-10-25 17:17:03.899 
Epoch 81/1000 
	 loss: 30.5555, MinusLogProbMetric: 30.5555, val_loss: 30.9237, val_MinusLogProbMetric: 30.9237

Epoch 81: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.5555 - MinusLogProbMetric: 30.5555 - val_loss: 30.9237 - val_MinusLogProbMetric: 30.9237 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 82/1000
2023-10-25 17:17:39.065 
Epoch 82/1000 
	 loss: 30.7183, MinusLogProbMetric: 30.7183, val_loss: 31.4347, val_MinusLogProbMetric: 31.4347

Epoch 82: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.7183 - MinusLogProbMetric: 30.7183 - val_loss: 31.4347 - val_MinusLogProbMetric: 31.4347 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 83/1000
2023-10-25 17:18:14.279 
Epoch 83/1000 
	 loss: 30.5790, MinusLogProbMetric: 30.5790, val_loss: 32.8612, val_MinusLogProbMetric: 32.8612

Epoch 83: val_loss did not improve from 30.33088
196/196 - 35s - loss: 30.5790 - MinusLogProbMetric: 30.5790 - val_loss: 32.8612 - val_MinusLogProbMetric: 32.8612 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 84/1000
2023-10-25 17:18:49.315 
Epoch 84/1000 
	 loss: 30.5818, MinusLogProbMetric: 30.5818, val_loss: 30.2106, val_MinusLogProbMetric: 30.2106

Epoch 84: val_loss improved from 30.33088 to 30.21063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 30.5818 - MinusLogProbMetric: 30.5818 - val_loss: 30.2106 - val_MinusLogProbMetric: 30.2106 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 85/1000
2023-10-25 17:19:25.216 
Epoch 85/1000 
	 loss: 30.5987, MinusLogProbMetric: 30.5987, val_loss: 30.8805, val_MinusLogProbMetric: 30.8805

Epoch 85: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.5987 - MinusLogProbMetric: 30.5987 - val_loss: 30.8805 - val_MinusLogProbMetric: 30.8805 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 86/1000
2023-10-25 17:20:00.447 
Epoch 86/1000 
	 loss: 30.5682, MinusLogProbMetric: 30.5682, val_loss: 30.6682, val_MinusLogProbMetric: 30.6682

Epoch 86: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.5682 - MinusLogProbMetric: 30.5682 - val_loss: 30.6682 - val_MinusLogProbMetric: 30.6682 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 87/1000
2023-10-25 17:20:35.440 
Epoch 87/1000 
	 loss: 30.7615, MinusLogProbMetric: 30.7615, val_loss: 31.4256, val_MinusLogProbMetric: 31.4256

Epoch 87: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.7615 - MinusLogProbMetric: 30.7615 - val_loss: 31.4256 - val_MinusLogProbMetric: 31.4256 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 88/1000
2023-10-25 17:21:10.917 
Epoch 88/1000 
	 loss: 30.5047, MinusLogProbMetric: 30.5047, val_loss: 30.5359, val_MinusLogProbMetric: 30.5359

Epoch 88: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.5047 - MinusLogProbMetric: 30.5047 - val_loss: 30.5359 - val_MinusLogProbMetric: 30.5359 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 89/1000
2023-10-25 17:21:45.759 
Epoch 89/1000 
	 loss: 30.9634, MinusLogProbMetric: 30.9634, val_loss: 30.5183, val_MinusLogProbMetric: 30.5183

Epoch 89: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.9634 - MinusLogProbMetric: 30.9634 - val_loss: 30.5183 - val_MinusLogProbMetric: 30.5183 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 90/1000
2023-10-25 17:22:21.074 
Epoch 90/1000 
	 loss: 30.5425, MinusLogProbMetric: 30.5425, val_loss: 30.6887, val_MinusLogProbMetric: 30.6887

Epoch 90: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.5425 - MinusLogProbMetric: 30.5425 - val_loss: 30.6887 - val_MinusLogProbMetric: 30.6887 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 91/1000
2023-10-25 17:22:56.162 
Epoch 91/1000 
	 loss: 30.4198, MinusLogProbMetric: 30.4198, val_loss: 30.6746, val_MinusLogProbMetric: 30.6746

Epoch 91: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.4198 - MinusLogProbMetric: 30.4198 - val_loss: 30.6746 - val_MinusLogProbMetric: 30.6746 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 92/1000
2023-10-25 17:23:31.281 
Epoch 92/1000 
	 loss: 30.4294, MinusLogProbMetric: 30.4294, val_loss: 30.2394, val_MinusLogProbMetric: 30.2394

Epoch 92: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.4294 - MinusLogProbMetric: 30.4294 - val_loss: 30.2394 - val_MinusLogProbMetric: 30.2394 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 93/1000
2023-10-25 17:24:06.523 
Epoch 93/1000 
	 loss: 30.3898, MinusLogProbMetric: 30.3898, val_loss: 33.6015, val_MinusLogProbMetric: 33.6015

Epoch 93: val_loss did not improve from 30.21063
196/196 - 35s - loss: 30.3898 - MinusLogProbMetric: 30.3898 - val_loss: 33.6015 - val_MinusLogProbMetric: 33.6015 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 94/1000
2023-10-25 17:24:41.589 
Epoch 94/1000 
	 loss: 30.5128, MinusLogProbMetric: 30.5128, val_loss: 30.1439, val_MinusLogProbMetric: 30.1439

Epoch 94: val_loss improved from 30.21063 to 30.14390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 30.5128 - MinusLogProbMetric: 30.5128 - val_loss: 30.1439 - val_MinusLogProbMetric: 30.1439 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 95/1000
2023-10-25 17:25:17.004 
Epoch 95/1000 
	 loss: 30.3747, MinusLogProbMetric: 30.3747, val_loss: 30.6024, val_MinusLogProbMetric: 30.6024

Epoch 95: val_loss did not improve from 30.14390
196/196 - 35s - loss: 30.3747 - MinusLogProbMetric: 30.3747 - val_loss: 30.6024 - val_MinusLogProbMetric: 30.6024 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 96/1000
2023-10-25 17:25:52.069 
Epoch 96/1000 
	 loss: 30.4511, MinusLogProbMetric: 30.4511, val_loss: 30.5814, val_MinusLogProbMetric: 30.5814

Epoch 96: val_loss did not improve from 30.14390
196/196 - 35s - loss: 30.4511 - MinusLogProbMetric: 30.4511 - val_loss: 30.5814 - val_MinusLogProbMetric: 30.5814 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 97/1000
2023-10-25 17:26:27.211 
Epoch 97/1000 
	 loss: 30.5325, MinusLogProbMetric: 30.5325, val_loss: 30.6301, val_MinusLogProbMetric: 30.6301

Epoch 97: val_loss did not improve from 30.14390
196/196 - 35s - loss: 30.5325 - MinusLogProbMetric: 30.5325 - val_loss: 30.6301 - val_MinusLogProbMetric: 30.6301 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 98/1000
2023-10-25 17:27:02.350 
Epoch 98/1000 
	 loss: 30.2918, MinusLogProbMetric: 30.2918, val_loss: 29.8873, val_MinusLogProbMetric: 29.8873

Epoch 98: val_loss improved from 30.14390 to 29.88729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 30.2918 - MinusLogProbMetric: 30.2918 - val_loss: 29.8873 - val_MinusLogProbMetric: 29.8873 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 99/1000
2023-10-25 17:27:38.044 
Epoch 99/1000 
	 loss: 30.3513, MinusLogProbMetric: 30.3513, val_loss: 30.6210, val_MinusLogProbMetric: 30.6210

Epoch 99: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.3513 - MinusLogProbMetric: 30.3513 - val_loss: 30.6210 - val_MinusLogProbMetric: 30.6210 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 100/1000
2023-10-25 17:28:13.442 
Epoch 100/1000 
	 loss: 30.2402, MinusLogProbMetric: 30.2402, val_loss: 31.0072, val_MinusLogProbMetric: 31.0072

Epoch 100: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.2402 - MinusLogProbMetric: 30.2402 - val_loss: 31.0072 - val_MinusLogProbMetric: 31.0072 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 101/1000
2023-10-25 17:28:48.692 
Epoch 101/1000 
	 loss: 30.2707, MinusLogProbMetric: 30.2707, val_loss: 31.2002, val_MinusLogProbMetric: 31.2002

Epoch 101: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.2707 - MinusLogProbMetric: 30.2707 - val_loss: 31.2002 - val_MinusLogProbMetric: 31.2002 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 102/1000
2023-10-25 17:29:23.979 
Epoch 102/1000 
	 loss: 30.2018, MinusLogProbMetric: 30.2018, val_loss: 31.2551, val_MinusLogProbMetric: 31.2551

Epoch 102: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.2018 - MinusLogProbMetric: 30.2018 - val_loss: 31.2551 - val_MinusLogProbMetric: 31.2551 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 103/1000
2023-10-25 17:29:58.934 
Epoch 103/1000 
	 loss: 30.4900, MinusLogProbMetric: 30.4900, val_loss: 31.9021, val_MinusLogProbMetric: 31.9021

Epoch 103: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.4900 - MinusLogProbMetric: 30.4900 - val_loss: 31.9021 - val_MinusLogProbMetric: 31.9021 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 104/1000
2023-10-25 17:30:34.248 
Epoch 104/1000 
	 loss: 30.4270, MinusLogProbMetric: 30.4270, val_loss: 30.6629, val_MinusLogProbMetric: 30.6629

Epoch 104: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.4270 - MinusLogProbMetric: 30.4270 - val_loss: 30.6629 - val_MinusLogProbMetric: 30.6629 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 105/1000
2023-10-25 17:31:09.606 
Epoch 105/1000 
	 loss: 30.1555, MinusLogProbMetric: 30.1555, val_loss: 31.4526, val_MinusLogProbMetric: 31.4526

Epoch 105: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.1555 - MinusLogProbMetric: 30.1555 - val_loss: 31.4526 - val_MinusLogProbMetric: 31.4526 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 106/1000
2023-10-25 17:31:44.618 
Epoch 106/1000 
	 loss: 30.2278, MinusLogProbMetric: 30.2278, val_loss: 31.0599, val_MinusLogProbMetric: 31.0599

Epoch 106: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.2278 - MinusLogProbMetric: 30.2278 - val_loss: 31.0599 - val_MinusLogProbMetric: 31.0599 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 107/1000
2023-10-25 17:32:19.904 
Epoch 107/1000 
	 loss: 30.1705, MinusLogProbMetric: 30.1705, val_loss: 31.3605, val_MinusLogProbMetric: 31.3605

Epoch 107: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.1705 - MinusLogProbMetric: 30.1705 - val_loss: 31.3605 - val_MinusLogProbMetric: 31.3605 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 108/1000
2023-10-25 17:32:55.270 
Epoch 108/1000 
	 loss: 30.2414, MinusLogProbMetric: 30.2414, val_loss: 29.9488, val_MinusLogProbMetric: 29.9488

Epoch 108: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.2414 - MinusLogProbMetric: 30.2414 - val_loss: 29.9488 - val_MinusLogProbMetric: 29.9488 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 109/1000
2023-10-25 17:33:30.120 
Epoch 109/1000 
	 loss: 30.0957, MinusLogProbMetric: 30.0957, val_loss: 31.4762, val_MinusLogProbMetric: 31.4762

Epoch 109: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.0957 - MinusLogProbMetric: 30.0957 - val_loss: 31.4762 - val_MinusLogProbMetric: 31.4762 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 110/1000
2023-10-25 17:34:05.330 
Epoch 110/1000 
	 loss: 30.3964, MinusLogProbMetric: 30.3964, val_loss: 30.5388, val_MinusLogProbMetric: 30.5388

Epoch 110: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.3964 - MinusLogProbMetric: 30.3964 - val_loss: 30.5388 - val_MinusLogProbMetric: 30.5388 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 111/1000
2023-10-25 17:34:40.418 
Epoch 111/1000 
	 loss: 30.1249, MinusLogProbMetric: 30.1249, val_loss: 30.4244, val_MinusLogProbMetric: 30.4244

Epoch 111: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.1249 - MinusLogProbMetric: 30.1249 - val_loss: 30.4244 - val_MinusLogProbMetric: 30.4244 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 112/1000
2023-10-25 17:35:15.523 
Epoch 112/1000 
	 loss: 30.1274, MinusLogProbMetric: 30.1274, val_loss: 30.5076, val_MinusLogProbMetric: 30.5076

Epoch 112: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.1274 - MinusLogProbMetric: 30.1274 - val_loss: 30.5076 - val_MinusLogProbMetric: 30.5076 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 113/1000
2023-10-25 17:35:50.692 
Epoch 113/1000 
	 loss: 30.2596, MinusLogProbMetric: 30.2596, val_loss: 30.2934, val_MinusLogProbMetric: 30.2934

Epoch 113: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.2596 - MinusLogProbMetric: 30.2596 - val_loss: 30.2934 - val_MinusLogProbMetric: 30.2934 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 114/1000
2023-10-25 17:36:25.966 
Epoch 114/1000 
	 loss: 30.0526, MinusLogProbMetric: 30.0526, val_loss: 31.3609, val_MinusLogProbMetric: 31.3609

Epoch 114: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.0526 - MinusLogProbMetric: 30.0526 - val_loss: 31.3609 - val_MinusLogProbMetric: 31.3609 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 115/1000
2023-10-25 17:37:01.304 
Epoch 115/1000 
	 loss: 29.9776, MinusLogProbMetric: 29.9776, val_loss: 30.4034, val_MinusLogProbMetric: 30.4034

Epoch 115: val_loss did not improve from 29.88729
196/196 - 35s - loss: 29.9776 - MinusLogProbMetric: 29.9776 - val_loss: 30.4034 - val_MinusLogProbMetric: 30.4034 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 116/1000
2023-10-25 17:37:36.604 
Epoch 116/1000 
	 loss: 30.0386, MinusLogProbMetric: 30.0386, val_loss: 31.1506, val_MinusLogProbMetric: 31.1506

Epoch 116: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.0386 - MinusLogProbMetric: 30.0386 - val_loss: 31.1506 - val_MinusLogProbMetric: 31.1506 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 117/1000
2023-10-25 17:38:11.597 
Epoch 117/1000 
	 loss: 30.1483, MinusLogProbMetric: 30.1483, val_loss: 30.0246, val_MinusLogProbMetric: 30.0246

Epoch 117: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.1483 - MinusLogProbMetric: 30.1483 - val_loss: 30.0246 - val_MinusLogProbMetric: 30.0246 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 118/1000
2023-10-25 17:38:47.129 
Epoch 118/1000 
	 loss: 30.1027, MinusLogProbMetric: 30.1027, val_loss: 30.1059, val_MinusLogProbMetric: 30.1059

Epoch 118: val_loss did not improve from 29.88729
196/196 - 36s - loss: 30.1027 - MinusLogProbMetric: 30.1027 - val_loss: 30.1059 - val_MinusLogProbMetric: 30.1059 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 119/1000
2023-10-25 17:39:22.344 
Epoch 119/1000 
	 loss: 30.1106, MinusLogProbMetric: 30.1106, val_loss: 31.2846, val_MinusLogProbMetric: 31.2846

Epoch 119: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.1106 - MinusLogProbMetric: 30.1106 - val_loss: 31.2846 - val_MinusLogProbMetric: 31.2846 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 120/1000
2023-10-25 17:39:57.368 
Epoch 120/1000 
	 loss: 30.1190, MinusLogProbMetric: 30.1190, val_loss: 29.9075, val_MinusLogProbMetric: 29.9075

Epoch 120: val_loss did not improve from 29.88729
196/196 - 35s - loss: 30.1190 - MinusLogProbMetric: 30.1190 - val_loss: 29.9075 - val_MinusLogProbMetric: 29.9075 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 121/1000
2023-10-25 17:40:32.741 
Epoch 121/1000 
	 loss: 29.9542, MinusLogProbMetric: 29.9542, val_loss: 30.3656, val_MinusLogProbMetric: 30.3656

Epoch 121: val_loss did not improve from 29.88729
196/196 - 35s - loss: 29.9542 - MinusLogProbMetric: 29.9542 - val_loss: 30.3656 - val_MinusLogProbMetric: 30.3656 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 122/1000
2023-10-25 17:41:08.046 
Epoch 122/1000 
	 loss: 29.9997, MinusLogProbMetric: 29.9997, val_loss: 31.2944, val_MinusLogProbMetric: 31.2944

Epoch 122: val_loss did not improve from 29.88729
196/196 - 35s - loss: 29.9997 - MinusLogProbMetric: 29.9997 - val_loss: 31.2944 - val_MinusLogProbMetric: 31.2944 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 123/1000
2023-10-25 17:41:43.091 
Epoch 123/1000 
	 loss: 29.9246, MinusLogProbMetric: 29.9246, val_loss: 29.7950, val_MinusLogProbMetric: 29.7950

Epoch 123: val_loss improved from 29.88729 to 29.79495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 29.9246 - MinusLogProbMetric: 29.9246 - val_loss: 29.7950 - val_MinusLogProbMetric: 29.7950 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 124/1000
2023-10-25 17:42:18.484 
Epoch 124/1000 
	 loss: 29.9313, MinusLogProbMetric: 29.9313, val_loss: 30.2395, val_MinusLogProbMetric: 30.2395

Epoch 124: val_loss did not improve from 29.79495
196/196 - 35s - loss: 29.9313 - MinusLogProbMetric: 29.9313 - val_loss: 30.2395 - val_MinusLogProbMetric: 30.2395 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 125/1000
2023-10-25 17:42:53.468 
Epoch 125/1000 
	 loss: 29.9824, MinusLogProbMetric: 29.9824, val_loss: 30.0541, val_MinusLogProbMetric: 30.0541

Epoch 125: val_loss did not improve from 29.79495
196/196 - 35s - loss: 29.9824 - MinusLogProbMetric: 29.9824 - val_loss: 30.0541 - val_MinusLogProbMetric: 30.0541 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 126/1000
2023-10-25 17:43:28.450 
Epoch 126/1000 
	 loss: 29.8819, MinusLogProbMetric: 29.8819, val_loss: 30.0025, val_MinusLogProbMetric: 30.0025

Epoch 126: val_loss did not improve from 29.79495
196/196 - 35s - loss: 29.8819 - MinusLogProbMetric: 29.8819 - val_loss: 30.0025 - val_MinusLogProbMetric: 30.0025 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 127/1000
2023-10-25 17:44:03.916 
Epoch 127/1000 
	 loss: 29.9638, MinusLogProbMetric: 29.9638, val_loss: 29.8294, val_MinusLogProbMetric: 29.8294

Epoch 127: val_loss did not improve from 29.79495
196/196 - 35s - loss: 29.9638 - MinusLogProbMetric: 29.9638 - val_loss: 29.8294 - val_MinusLogProbMetric: 29.8294 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 128/1000
2023-10-25 17:44:39.366 
Epoch 128/1000 
	 loss: 29.8420, MinusLogProbMetric: 29.8420, val_loss: 29.7476, val_MinusLogProbMetric: 29.7476

Epoch 128: val_loss improved from 29.79495 to 29.74759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 29.8420 - MinusLogProbMetric: 29.8420 - val_loss: 29.7476 - val_MinusLogProbMetric: 29.7476 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 129/1000
2023-10-25 17:45:14.747 
Epoch 129/1000 
	 loss: 29.7727, MinusLogProbMetric: 29.7727, val_loss: 30.1183, val_MinusLogProbMetric: 30.1183

Epoch 129: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7727 - MinusLogProbMetric: 29.7727 - val_loss: 30.1183 - val_MinusLogProbMetric: 30.1183 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 130/1000
2023-10-25 17:45:49.778 
Epoch 130/1000 
	 loss: 29.8837, MinusLogProbMetric: 29.8837, val_loss: 29.9239, val_MinusLogProbMetric: 29.9239

Epoch 130: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.8837 - MinusLogProbMetric: 29.8837 - val_loss: 29.9239 - val_MinusLogProbMetric: 29.9239 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 131/1000
2023-10-25 17:46:25.088 
Epoch 131/1000 
	 loss: 29.7815, MinusLogProbMetric: 29.7815, val_loss: 30.1641, val_MinusLogProbMetric: 30.1641

Epoch 131: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7815 - MinusLogProbMetric: 29.7815 - val_loss: 30.1641 - val_MinusLogProbMetric: 30.1641 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 132/1000
2023-10-25 17:47:00.024 
Epoch 132/1000 
	 loss: 29.8408, MinusLogProbMetric: 29.8408, val_loss: 30.3933, val_MinusLogProbMetric: 30.3933

Epoch 132: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.8408 - MinusLogProbMetric: 29.8408 - val_loss: 30.3933 - val_MinusLogProbMetric: 30.3933 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 133/1000
2023-10-25 17:47:35.422 
Epoch 133/1000 
	 loss: 29.7834, MinusLogProbMetric: 29.7834, val_loss: 29.9309, val_MinusLogProbMetric: 29.9309

Epoch 133: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7834 - MinusLogProbMetric: 29.7834 - val_loss: 29.9309 - val_MinusLogProbMetric: 29.9309 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 134/1000
2023-10-25 17:48:10.724 
Epoch 134/1000 
	 loss: 29.7522, MinusLogProbMetric: 29.7522, val_loss: 30.0844, val_MinusLogProbMetric: 30.0844

Epoch 134: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7522 - MinusLogProbMetric: 29.7522 - val_loss: 30.0844 - val_MinusLogProbMetric: 30.0844 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 135/1000
2023-10-25 17:48:46.035 
Epoch 135/1000 
	 loss: 29.7725, MinusLogProbMetric: 29.7725, val_loss: 30.5041, val_MinusLogProbMetric: 30.5041

Epoch 135: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7725 - MinusLogProbMetric: 29.7725 - val_loss: 30.5041 - val_MinusLogProbMetric: 30.5041 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 136/1000
2023-10-25 17:49:21.305 
Epoch 136/1000 
	 loss: 29.7384, MinusLogProbMetric: 29.7384, val_loss: 29.9133, val_MinusLogProbMetric: 29.9133

Epoch 136: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7384 - MinusLogProbMetric: 29.7384 - val_loss: 29.9133 - val_MinusLogProbMetric: 29.9133 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 137/1000
2023-10-25 17:49:56.529 
Epoch 137/1000 
	 loss: 29.7548, MinusLogProbMetric: 29.7548, val_loss: 29.7779, val_MinusLogProbMetric: 29.7779

Epoch 137: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7548 - MinusLogProbMetric: 29.7548 - val_loss: 29.7779 - val_MinusLogProbMetric: 29.7779 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 138/1000
2023-10-25 17:50:31.752 
Epoch 138/1000 
	 loss: 29.7759, MinusLogProbMetric: 29.7759, val_loss: 30.4163, val_MinusLogProbMetric: 30.4163

Epoch 138: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7759 - MinusLogProbMetric: 29.7759 - val_loss: 30.4163 - val_MinusLogProbMetric: 30.4163 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 139/1000
2023-10-25 17:51:06.973 
Epoch 139/1000 
	 loss: 29.7917, MinusLogProbMetric: 29.7917, val_loss: 29.8504, val_MinusLogProbMetric: 29.8504

Epoch 139: val_loss did not improve from 29.74759
196/196 - 35s - loss: 29.7917 - MinusLogProbMetric: 29.7917 - val_loss: 29.8504 - val_MinusLogProbMetric: 29.8504 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 140/1000
2023-10-25 17:51:42.108 
Epoch 140/1000 
	 loss: 29.8424, MinusLogProbMetric: 29.8424, val_loss: 29.5192, val_MinusLogProbMetric: 29.5192

Epoch 140: val_loss improved from 29.74759 to 29.51917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 29.8424 - MinusLogProbMetric: 29.8424 - val_loss: 29.5192 - val_MinusLogProbMetric: 29.5192 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 141/1000
2023-10-25 17:52:17.698 
Epoch 141/1000 
	 loss: 29.6843, MinusLogProbMetric: 29.6843, val_loss: 30.3697, val_MinusLogProbMetric: 30.3697

Epoch 141: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.6843 - MinusLogProbMetric: 29.6843 - val_loss: 30.3697 - val_MinusLogProbMetric: 30.3697 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 142/1000
2023-10-25 17:52:52.679 
Epoch 142/1000 
	 loss: 29.8842, MinusLogProbMetric: 29.8842, val_loss: 29.6839, val_MinusLogProbMetric: 29.6839

Epoch 142: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.8842 - MinusLogProbMetric: 29.8842 - val_loss: 29.6839 - val_MinusLogProbMetric: 29.6839 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 143/1000
2023-10-25 17:53:27.685 
Epoch 143/1000 
	 loss: 29.7218, MinusLogProbMetric: 29.7218, val_loss: 29.8435, val_MinusLogProbMetric: 29.8435

Epoch 143: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.7218 - MinusLogProbMetric: 29.7218 - val_loss: 29.8435 - val_MinusLogProbMetric: 29.8435 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 144/1000
2023-10-25 17:54:02.615 
Epoch 144/1000 
	 loss: 29.6037, MinusLogProbMetric: 29.6037, val_loss: 29.8758, val_MinusLogProbMetric: 29.8758

Epoch 144: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.6037 - MinusLogProbMetric: 29.6037 - val_loss: 29.8758 - val_MinusLogProbMetric: 29.8758 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 145/1000
2023-10-25 17:54:37.643 
Epoch 145/1000 
	 loss: 29.7342, MinusLogProbMetric: 29.7342, val_loss: 29.6797, val_MinusLogProbMetric: 29.6797

Epoch 145: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.7342 - MinusLogProbMetric: 29.7342 - val_loss: 29.6797 - val_MinusLogProbMetric: 29.6797 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 146/1000
2023-10-25 17:55:12.489 
Epoch 146/1000 
	 loss: 29.6270, MinusLogProbMetric: 29.6270, val_loss: 30.0183, val_MinusLogProbMetric: 30.0183

Epoch 146: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.6270 - MinusLogProbMetric: 29.6270 - val_loss: 30.0183 - val_MinusLogProbMetric: 30.0183 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 147/1000
2023-10-25 17:55:43.159 
Epoch 147/1000 
	 loss: 29.5918, MinusLogProbMetric: 29.5918, val_loss: 30.0791, val_MinusLogProbMetric: 30.0791

Epoch 147: val_loss did not improve from 29.51917
196/196 - 31s - loss: 29.5918 - MinusLogProbMetric: 29.5918 - val_loss: 30.0791 - val_MinusLogProbMetric: 30.0791 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 148/1000
2023-10-25 17:56:13.407 
Epoch 148/1000 
	 loss: 29.7167, MinusLogProbMetric: 29.7167, val_loss: 30.4549, val_MinusLogProbMetric: 30.4549

Epoch 148: val_loss did not improve from 29.51917
196/196 - 30s - loss: 29.7167 - MinusLogProbMetric: 29.7167 - val_loss: 30.4549 - val_MinusLogProbMetric: 30.4549 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 149/1000
2023-10-25 17:56:46.551 
Epoch 149/1000 
	 loss: 29.5866, MinusLogProbMetric: 29.5866, val_loss: 30.1992, val_MinusLogProbMetric: 30.1992

Epoch 149: val_loss did not improve from 29.51917
196/196 - 33s - loss: 29.5866 - MinusLogProbMetric: 29.5866 - val_loss: 30.1992 - val_MinusLogProbMetric: 30.1992 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 150/1000
2023-10-25 17:57:21.085 
Epoch 150/1000 
	 loss: 29.6711, MinusLogProbMetric: 29.6711, val_loss: 29.9770, val_MinusLogProbMetric: 29.9770

Epoch 150: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.6711 - MinusLogProbMetric: 29.6711 - val_loss: 29.9770 - val_MinusLogProbMetric: 29.9770 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 151/1000
2023-10-25 17:57:55.382 
Epoch 151/1000 
	 loss: 29.5475, MinusLogProbMetric: 29.5475, val_loss: 30.6500, val_MinusLogProbMetric: 30.6500

Epoch 151: val_loss did not improve from 29.51917
196/196 - 34s - loss: 29.5475 - MinusLogProbMetric: 29.5475 - val_loss: 30.6500 - val_MinusLogProbMetric: 30.6500 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 152/1000
2023-10-25 17:58:27.444 
Epoch 152/1000 
	 loss: 29.6387, MinusLogProbMetric: 29.6387, val_loss: 30.0471, val_MinusLogProbMetric: 30.0471

Epoch 152: val_loss did not improve from 29.51917
196/196 - 32s - loss: 29.6387 - MinusLogProbMetric: 29.6387 - val_loss: 30.0471 - val_MinusLogProbMetric: 30.0471 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 153/1000
2023-10-25 17:59:01.588 
Epoch 153/1000 
	 loss: 29.8162, MinusLogProbMetric: 29.8162, val_loss: 30.4608, val_MinusLogProbMetric: 30.4608

Epoch 153: val_loss did not improve from 29.51917
196/196 - 34s - loss: 29.8162 - MinusLogProbMetric: 29.8162 - val_loss: 30.4608 - val_MinusLogProbMetric: 30.4608 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 154/1000
2023-10-25 17:59:36.351 
Epoch 154/1000 
	 loss: 29.6974, MinusLogProbMetric: 29.6974, val_loss: 30.3029, val_MinusLogProbMetric: 30.3029

Epoch 154: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.6974 - MinusLogProbMetric: 29.6974 - val_loss: 30.3029 - val_MinusLogProbMetric: 30.3029 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 155/1000
2023-10-25 18:00:09.372 
Epoch 155/1000 
	 loss: 29.7487, MinusLogProbMetric: 29.7487, val_loss: 29.8101, val_MinusLogProbMetric: 29.8101

Epoch 155: val_loss did not improve from 29.51917
196/196 - 33s - loss: 29.7487 - MinusLogProbMetric: 29.7487 - val_loss: 29.8101 - val_MinusLogProbMetric: 29.8101 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 156/1000
2023-10-25 18:00:44.084 
Epoch 156/1000 
	 loss: 29.5303, MinusLogProbMetric: 29.5303, val_loss: 30.5022, val_MinusLogProbMetric: 30.5022

Epoch 156: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.5303 - MinusLogProbMetric: 29.5303 - val_loss: 30.5022 - val_MinusLogProbMetric: 30.5022 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 157/1000
2023-10-25 18:01:17.890 
Epoch 157/1000 
	 loss: 29.5265, MinusLogProbMetric: 29.5265, val_loss: 31.4463, val_MinusLogProbMetric: 31.4463

Epoch 157: val_loss did not improve from 29.51917
196/196 - 34s - loss: 29.5265 - MinusLogProbMetric: 29.5265 - val_loss: 31.4463 - val_MinusLogProbMetric: 31.4463 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 158/1000
2023-10-25 18:01:49.129 
Epoch 158/1000 
	 loss: 29.5410, MinusLogProbMetric: 29.5410, val_loss: 29.5375, val_MinusLogProbMetric: 29.5375

Epoch 158: val_loss did not improve from 29.51917
196/196 - 31s - loss: 29.5410 - MinusLogProbMetric: 29.5410 - val_loss: 29.5375 - val_MinusLogProbMetric: 29.5375 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 159/1000
2023-10-25 18:02:19.692 
Epoch 159/1000 
	 loss: 29.5101, MinusLogProbMetric: 29.5101, val_loss: 29.5371, val_MinusLogProbMetric: 29.5371

Epoch 159: val_loss did not improve from 29.51917
196/196 - 31s - loss: 29.5101 - MinusLogProbMetric: 29.5101 - val_loss: 29.5371 - val_MinusLogProbMetric: 29.5371 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 160/1000
2023-10-25 18:02:53.037 
Epoch 160/1000 
	 loss: 29.7871, MinusLogProbMetric: 29.7871, val_loss: 30.1130, val_MinusLogProbMetric: 30.1130

Epoch 160: val_loss did not improve from 29.51917
196/196 - 33s - loss: 29.7871 - MinusLogProbMetric: 29.7871 - val_loss: 30.1130 - val_MinusLogProbMetric: 30.1130 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 161/1000
2023-10-25 18:03:27.936 
Epoch 161/1000 
	 loss: 29.5826, MinusLogProbMetric: 29.5826, val_loss: 31.4097, val_MinusLogProbMetric: 31.4097

Epoch 161: val_loss did not improve from 29.51917
196/196 - 35s - loss: 29.5826 - MinusLogProbMetric: 29.5826 - val_loss: 31.4097 - val_MinusLogProbMetric: 31.4097 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 162/1000
2023-10-25 18:03:59.435 
Epoch 162/1000 
	 loss: 29.6340, MinusLogProbMetric: 29.6340, val_loss: 29.7416, val_MinusLogProbMetric: 29.7416

Epoch 162: val_loss did not improve from 29.51917
196/196 - 31s - loss: 29.6340 - MinusLogProbMetric: 29.6340 - val_loss: 29.7416 - val_MinusLogProbMetric: 29.7416 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 163/1000
2023-10-25 18:04:31.125 
Epoch 163/1000 
	 loss: 29.6968, MinusLogProbMetric: 29.6968, val_loss: 30.4209, val_MinusLogProbMetric: 30.4209

Epoch 163: val_loss did not improve from 29.51917
196/196 - 32s - loss: 29.6968 - MinusLogProbMetric: 29.6968 - val_loss: 30.4209 - val_MinusLogProbMetric: 30.4209 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 164/1000
2023-10-25 18:05:03.733 
Epoch 164/1000 
	 loss: 29.5298, MinusLogProbMetric: 29.5298, val_loss: 29.6179, val_MinusLogProbMetric: 29.6179

Epoch 164: val_loss did not improve from 29.51917
196/196 - 33s - loss: 29.5298 - MinusLogProbMetric: 29.5298 - val_loss: 29.6179 - val_MinusLogProbMetric: 29.6179 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 165/1000
2023-10-25 18:05:38.158 
Epoch 165/1000 
	 loss: 29.5034, MinusLogProbMetric: 29.5034, val_loss: 30.8217, val_MinusLogProbMetric: 30.8217

Epoch 165: val_loss did not improve from 29.51917
196/196 - 34s - loss: 29.5034 - MinusLogProbMetric: 29.5034 - val_loss: 30.8217 - val_MinusLogProbMetric: 30.8217 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 166/1000
2023-10-25 18:06:12.608 
Epoch 166/1000 
	 loss: 29.4676, MinusLogProbMetric: 29.4676, val_loss: 29.5113, val_MinusLogProbMetric: 29.5113

Epoch 166: val_loss improved from 29.51917 to 29.51129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 29.4676 - MinusLogProbMetric: 29.4676 - val_loss: 29.5113 - val_MinusLogProbMetric: 29.5113 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 167/1000
2023-10-25 18:06:47.117 
Epoch 167/1000 
	 loss: 29.5824, MinusLogProbMetric: 29.5824, val_loss: 29.8775, val_MinusLogProbMetric: 29.8775

Epoch 167: val_loss did not improve from 29.51129
196/196 - 34s - loss: 29.5824 - MinusLogProbMetric: 29.5824 - val_loss: 29.8775 - val_MinusLogProbMetric: 29.8775 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 168/1000
2023-10-25 18:07:21.609 
Epoch 168/1000 
	 loss: 29.4219, MinusLogProbMetric: 29.4219, val_loss: 30.3260, val_MinusLogProbMetric: 30.3260

Epoch 168: val_loss did not improve from 29.51129
196/196 - 34s - loss: 29.4219 - MinusLogProbMetric: 29.4219 - val_loss: 30.3260 - val_MinusLogProbMetric: 30.3260 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 169/1000
2023-10-25 18:07:55.079 
Epoch 169/1000 
	 loss: 29.4497, MinusLogProbMetric: 29.4497, val_loss: 29.9223, val_MinusLogProbMetric: 29.9223

Epoch 169: val_loss did not improve from 29.51129
196/196 - 33s - loss: 29.4497 - MinusLogProbMetric: 29.4497 - val_loss: 29.9223 - val_MinusLogProbMetric: 29.9223 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 170/1000
2023-10-25 18:08:28.775 
Epoch 170/1000 
	 loss: 29.3970, MinusLogProbMetric: 29.3970, val_loss: 29.4496, val_MinusLogProbMetric: 29.4496

Epoch 170: val_loss improved from 29.51129 to 29.44963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 34s - loss: 29.3970 - MinusLogProbMetric: 29.3970 - val_loss: 29.4496 - val_MinusLogProbMetric: 29.4496 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 171/1000
2023-10-25 18:09:03.430 
Epoch 171/1000 
	 loss: 29.6449, MinusLogProbMetric: 29.6449, val_loss: 30.3866, val_MinusLogProbMetric: 30.3866

Epoch 171: val_loss did not improve from 29.44963
196/196 - 34s - loss: 29.6449 - MinusLogProbMetric: 29.6449 - val_loss: 30.3866 - val_MinusLogProbMetric: 30.3866 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 172/1000
2023-10-25 18:09:38.333 
Epoch 172/1000 
	 loss: 29.4623, MinusLogProbMetric: 29.4623, val_loss: 29.5651, val_MinusLogProbMetric: 29.5651

Epoch 172: val_loss did not improve from 29.44963
196/196 - 35s - loss: 29.4623 - MinusLogProbMetric: 29.4623 - val_loss: 29.5651 - val_MinusLogProbMetric: 29.5651 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 173/1000
2023-10-25 18:10:12.872 
Epoch 173/1000 
	 loss: 29.3569, MinusLogProbMetric: 29.3569, val_loss: 29.9036, val_MinusLogProbMetric: 29.9036

Epoch 173: val_loss did not improve from 29.44963
196/196 - 35s - loss: 29.3569 - MinusLogProbMetric: 29.3569 - val_loss: 29.9036 - val_MinusLogProbMetric: 29.9036 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 174/1000
2023-10-25 18:10:45.106 
Epoch 174/1000 
	 loss: 29.4554, MinusLogProbMetric: 29.4554, val_loss: 29.4255, val_MinusLogProbMetric: 29.4255

Epoch 174: val_loss improved from 29.44963 to 29.42547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 33s - loss: 29.4554 - MinusLogProbMetric: 29.4554 - val_loss: 29.4255 - val_MinusLogProbMetric: 29.4255 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 175/1000
2023-10-25 18:11:16.160 
Epoch 175/1000 
	 loss: 29.3751, MinusLogProbMetric: 29.3751, val_loss: 29.7486, val_MinusLogProbMetric: 29.7486

Epoch 175: val_loss did not improve from 29.42547
196/196 - 30s - loss: 29.3751 - MinusLogProbMetric: 29.3751 - val_loss: 29.7486 - val_MinusLogProbMetric: 29.7486 - lr: 0.0010 - 30s/epoch - 156ms/step
Epoch 176/1000
2023-10-25 18:11:50.979 
Epoch 176/1000 
	 loss: 29.3610, MinusLogProbMetric: 29.3610, val_loss: 29.8485, val_MinusLogProbMetric: 29.8485

Epoch 176: val_loss did not improve from 29.42547
196/196 - 35s - loss: 29.3610 - MinusLogProbMetric: 29.3610 - val_loss: 29.8485 - val_MinusLogProbMetric: 29.8485 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 177/1000
2023-10-25 18:12:25.536 
Epoch 177/1000 
	 loss: 29.4507, MinusLogProbMetric: 29.4507, val_loss: 29.5794, val_MinusLogProbMetric: 29.5794

Epoch 177: val_loss did not improve from 29.42547
196/196 - 35s - loss: 29.4507 - MinusLogProbMetric: 29.4507 - val_loss: 29.5794 - val_MinusLogProbMetric: 29.5794 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 178/1000
2023-10-25 18:12:56.575 
Epoch 178/1000 
	 loss: 29.3791, MinusLogProbMetric: 29.3791, val_loss: 29.8887, val_MinusLogProbMetric: 29.8887

Epoch 178: val_loss did not improve from 29.42547
196/196 - 31s - loss: 29.3791 - MinusLogProbMetric: 29.3791 - val_loss: 29.8887 - val_MinusLogProbMetric: 29.8887 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 179/1000
2023-10-25 18:13:28.307 
Epoch 179/1000 
	 loss: 29.3634, MinusLogProbMetric: 29.3634, val_loss: 30.3208, val_MinusLogProbMetric: 30.3208

Epoch 179: val_loss did not improve from 29.42547
196/196 - 32s - loss: 29.3634 - MinusLogProbMetric: 29.3634 - val_loss: 30.3208 - val_MinusLogProbMetric: 30.3208 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 180/1000
2023-10-25 18:14:00.469 
Epoch 180/1000 
	 loss: 29.3829, MinusLogProbMetric: 29.3829, val_loss: 29.4813, val_MinusLogProbMetric: 29.4813

Epoch 180: val_loss did not improve from 29.42547
196/196 - 32s - loss: 29.3829 - MinusLogProbMetric: 29.3829 - val_loss: 29.4813 - val_MinusLogProbMetric: 29.4813 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 181/1000
2023-10-25 18:14:34.680 
Epoch 181/1000 
	 loss: 29.3626, MinusLogProbMetric: 29.3626, val_loss: 29.3711, val_MinusLogProbMetric: 29.3711

Epoch 181: val_loss improved from 29.42547 to 29.37109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 29.3626 - MinusLogProbMetric: 29.3626 - val_loss: 29.3711 - val_MinusLogProbMetric: 29.3711 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 182/1000
2023-10-25 18:15:09.872 
Epoch 182/1000 
	 loss: 29.3691, MinusLogProbMetric: 29.3691, val_loss: 30.0784, val_MinusLogProbMetric: 30.0784

Epoch 182: val_loss did not improve from 29.37109
196/196 - 34s - loss: 29.3691 - MinusLogProbMetric: 29.3691 - val_loss: 30.0784 - val_MinusLogProbMetric: 30.0784 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 183/1000
2023-10-25 18:15:43.126 
Epoch 183/1000 
	 loss: 29.5161, MinusLogProbMetric: 29.5161, val_loss: 29.4215, val_MinusLogProbMetric: 29.4215

Epoch 183: val_loss did not improve from 29.37109
196/196 - 33s - loss: 29.5161 - MinusLogProbMetric: 29.5161 - val_loss: 29.4215 - val_MinusLogProbMetric: 29.4215 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 184/1000
2023-10-25 18:16:15.158 
Epoch 184/1000 
	 loss: 29.2552, MinusLogProbMetric: 29.2552, val_loss: 31.3416, val_MinusLogProbMetric: 31.3416

Epoch 184: val_loss did not improve from 29.37109
196/196 - 32s - loss: 29.2552 - MinusLogProbMetric: 29.2552 - val_loss: 31.3416 - val_MinusLogProbMetric: 31.3416 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 185/1000
2023-10-25 18:16:48.391 
Epoch 185/1000 
	 loss: 29.3189, MinusLogProbMetric: 29.3189, val_loss: 30.6828, val_MinusLogProbMetric: 30.6828

Epoch 185: val_loss did not improve from 29.37109
196/196 - 33s - loss: 29.3189 - MinusLogProbMetric: 29.3189 - val_loss: 30.6828 - val_MinusLogProbMetric: 30.6828 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 186/1000
2023-10-25 18:17:21.979 
Epoch 186/1000 
	 loss: 29.3368, MinusLogProbMetric: 29.3368, val_loss: 32.0565, val_MinusLogProbMetric: 32.0565

Epoch 186: val_loss did not improve from 29.37109
196/196 - 34s - loss: 29.3368 - MinusLogProbMetric: 29.3368 - val_loss: 32.0565 - val_MinusLogProbMetric: 32.0565 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 187/1000
2023-10-25 18:17:57.165 
Epoch 187/1000 
	 loss: 29.2711, MinusLogProbMetric: 29.2711, val_loss: 30.6235, val_MinusLogProbMetric: 30.6235

Epoch 187: val_loss did not improve from 29.37109
196/196 - 35s - loss: 29.2711 - MinusLogProbMetric: 29.2711 - val_loss: 30.6235 - val_MinusLogProbMetric: 30.6235 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 188/1000
2023-10-25 18:18:30.355 
Epoch 188/1000 
	 loss: 29.3084, MinusLogProbMetric: 29.3084, val_loss: 29.3945, val_MinusLogProbMetric: 29.3945

Epoch 188: val_loss did not improve from 29.37109
196/196 - 33s - loss: 29.3084 - MinusLogProbMetric: 29.3084 - val_loss: 29.3945 - val_MinusLogProbMetric: 29.3945 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 189/1000
2023-10-25 18:19:03.331 
Epoch 189/1000 
	 loss: 29.4064, MinusLogProbMetric: 29.4064, val_loss: 32.0352, val_MinusLogProbMetric: 32.0352

Epoch 189: val_loss did not improve from 29.37109
196/196 - 33s - loss: 29.4064 - MinusLogProbMetric: 29.4064 - val_loss: 32.0352 - val_MinusLogProbMetric: 32.0352 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 190/1000
2023-10-25 18:19:38.155 
Epoch 190/1000 
	 loss: 29.3953, MinusLogProbMetric: 29.3953, val_loss: 30.6961, val_MinusLogProbMetric: 30.6961

Epoch 190: val_loss did not improve from 29.37109
196/196 - 35s - loss: 29.3953 - MinusLogProbMetric: 29.3953 - val_loss: 30.6961 - val_MinusLogProbMetric: 30.6961 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 191/1000
2023-10-25 18:20:10.623 
Epoch 191/1000 
	 loss: 29.3137, MinusLogProbMetric: 29.3137, val_loss: 29.5345, val_MinusLogProbMetric: 29.5345

Epoch 191: val_loss did not improve from 29.37109
196/196 - 32s - loss: 29.3137 - MinusLogProbMetric: 29.3137 - val_loss: 29.5345 - val_MinusLogProbMetric: 29.5345 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 192/1000
2023-10-25 18:20:45.715 
Epoch 192/1000 
	 loss: 29.2862, MinusLogProbMetric: 29.2862, val_loss: 29.7084, val_MinusLogProbMetric: 29.7084

Epoch 192: val_loss did not improve from 29.37109
196/196 - 35s - loss: 29.2862 - MinusLogProbMetric: 29.2862 - val_loss: 29.7084 - val_MinusLogProbMetric: 29.7084 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 193/1000
2023-10-25 18:21:19.311 
Epoch 193/1000 
	 loss: 29.3701, MinusLogProbMetric: 29.3701, val_loss: 31.2555, val_MinusLogProbMetric: 31.2555

Epoch 193: val_loss did not improve from 29.37109
196/196 - 34s - loss: 29.3701 - MinusLogProbMetric: 29.3701 - val_loss: 31.2555 - val_MinusLogProbMetric: 31.2555 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 194/1000
2023-10-25 18:21:53.258 
Epoch 194/1000 
	 loss: 29.2973, MinusLogProbMetric: 29.2973, val_loss: 29.6871, val_MinusLogProbMetric: 29.6871

Epoch 194: val_loss did not improve from 29.37109
196/196 - 34s - loss: 29.2973 - MinusLogProbMetric: 29.2973 - val_loss: 29.6871 - val_MinusLogProbMetric: 29.6871 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 195/1000
2023-10-25 18:22:26.357 
Epoch 195/1000 
	 loss: 29.3768, MinusLogProbMetric: 29.3768, val_loss: 30.3846, val_MinusLogProbMetric: 30.3846

Epoch 195: val_loss did not improve from 29.37109
196/196 - 33s - loss: 29.3768 - MinusLogProbMetric: 29.3768 - val_loss: 30.3846 - val_MinusLogProbMetric: 30.3846 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 196/1000
2023-10-25 18:22:56.811 
Epoch 196/1000 
	 loss: 29.2304, MinusLogProbMetric: 29.2304, val_loss: 29.9387, val_MinusLogProbMetric: 29.9387

Epoch 196: val_loss did not improve from 29.37109
196/196 - 30s - loss: 29.2304 - MinusLogProbMetric: 29.2304 - val_loss: 29.9387 - val_MinusLogProbMetric: 29.9387 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 197/1000
2023-10-25 18:23:31.212 
Epoch 197/1000 
	 loss: 29.1897, MinusLogProbMetric: 29.1897, val_loss: 30.6585, val_MinusLogProbMetric: 30.6585

Epoch 197: val_loss did not improve from 29.37109
196/196 - 34s - loss: 29.1897 - MinusLogProbMetric: 29.1897 - val_loss: 30.6585 - val_MinusLogProbMetric: 30.6585 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 198/1000
2023-10-25 18:24:06.178 
Epoch 198/1000 
	 loss: 29.3106, MinusLogProbMetric: 29.3106, val_loss: 29.4310, val_MinusLogProbMetric: 29.4310

Epoch 198: val_loss did not improve from 29.37109
196/196 - 35s - loss: 29.3106 - MinusLogProbMetric: 29.3106 - val_loss: 29.4310 - val_MinusLogProbMetric: 29.4310 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 199/1000
2023-10-25 18:24:40.808 
Epoch 199/1000 
	 loss: 29.1711, MinusLogProbMetric: 29.1711, val_loss: 31.2996, val_MinusLogProbMetric: 31.2996

Epoch 199: val_loss did not improve from 29.37109
196/196 - 35s - loss: 29.1711 - MinusLogProbMetric: 29.1711 - val_loss: 31.2996 - val_MinusLogProbMetric: 31.2996 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 200/1000
2023-10-25 18:25:12.409 
Epoch 200/1000 
	 loss: 29.1108, MinusLogProbMetric: 29.1108, val_loss: 29.2388, val_MinusLogProbMetric: 29.2388

Epoch 200: val_loss improved from 29.37109 to 29.23885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 32s - loss: 29.1108 - MinusLogProbMetric: 29.1108 - val_loss: 29.2388 - val_MinusLogProbMetric: 29.2388 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 201/1000
2023-10-25 18:25:44.948 
Epoch 201/1000 
	 loss: 29.3216, MinusLogProbMetric: 29.3216, val_loss: 29.5988, val_MinusLogProbMetric: 29.5988

Epoch 201: val_loss did not improve from 29.23885
196/196 - 32s - loss: 29.3216 - MinusLogProbMetric: 29.3216 - val_loss: 29.5988 - val_MinusLogProbMetric: 29.5988 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 202/1000
2023-10-25 18:26:19.153 
Epoch 202/1000 
	 loss: 29.1351, MinusLogProbMetric: 29.1351, val_loss: 29.2065, val_MinusLogProbMetric: 29.2065

Epoch 202: val_loss improved from 29.23885 to 29.20654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 29.1351 - MinusLogProbMetric: 29.1351 - val_loss: 29.2065 - val_MinusLogProbMetric: 29.2065 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 203/1000
2023-10-25 18:26:54.910 
Epoch 203/1000 
	 loss: 29.4069, MinusLogProbMetric: 29.4069, val_loss: 29.6995, val_MinusLogProbMetric: 29.6995

Epoch 203: val_loss did not improve from 29.20654
196/196 - 35s - loss: 29.4069 - MinusLogProbMetric: 29.4069 - val_loss: 29.6995 - val_MinusLogProbMetric: 29.6995 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 204/1000
2023-10-25 18:27:27.679 
Epoch 204/1000 
	 loss: 29.2147, MinusLogProbMetric: 29.2147, val_loss: 29.9466, val_MinusLogProbMetric: 29.9466

Epoch 204: val_loss did not improve from 29.20654
196/196 - 33s - loss: 29.2147 - MinusLogProbMetric: 29.2147 - val_loss: 29.9466 - val_MinusLogProbMetric: 29.9466 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 205/1000
2023-10-25 18:27:58.567 
Epoch 205/1000 
	 loss: 29.2540, MinusLogProbMetric: 29.2540, val_loss: 30.0394, val_MinusLogProbMetric: 30.0394

Epoch 205: val_loss did not improve from 29.20654
196/196 - 31s - loss: 29.2540 - MinusLogProbMetric: 29.2540 - val_loss: 30.0394 - val_MinusLogProbMetric: 30.0394 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 206/1000
2023-10-25 18:28:31.399 
Epoch 206/1000 
	 loss: 29.0654, MinusLogProbMetric: 29.0654, val_loss: 29.8398, val_MinusLogProbMetric: 29.8398

Epoch 206: val_loss did not improve from 29.20654
196/196 - 33s - loss: 29.0654 - MinusLogProbMetric: 29.0654 - val_loss: 29.8398 - val_MinusLogProbMetric: 29.8398 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 207/1000
2023-10-25 18:29:06.002 
Epoch 207/1000 
	 loss: 29.3999, MinusLogProbMetric: 29.3999, val_loss: 30.6845, val_MinusLogProbMetric: 30.6845

Epoch 207: val_loss did not improve from 29.20654
196/196 - 35s - loss: 29.3999 - MinusLogProbMetric: 29.3999 - val_loss: 30.6845 - val_MinusLogProbMetric: 30.6845 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 208/1000
2023-10-25 18:29:40.984 
Epoch 208/1000 
	 loss: 29.3286, MinusLogProbMetric: 29.3286, val_loss: 29.2361, val_MinusLogProbMetric: 29.2361

Epoch 208: val_loss did not improve from 29.20654
196/196 - 35s - loss: 29.3286 - MinusLogProbMetric: 29.3286 - val_loss: 29.2361 - val_MinusLogProbMetric: 29.2361 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 209/1000
2023-10-25 18:30:13.346 
Epoch 209/1000 
	 loss: 29.2898, MinusLogProbMetric: 29.2898, val_loss: 30.5582, val_MinusLogProbMetric: 30.5582

Epoch 209: val_loss did not improve from 29.20654
196/196 - 32s - loss: 29.2898 - MinusLogProbMetric: 29.2898 - val_loss: 30.5582 - val_MinusLogProbMetric: 30.5582 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 210/1000
2023-10-25 18:30:44.740 
Epoch 210/1000 
	 loss: 29.0957, MinusLogProbMetric: 29.0957, val_loss: 29.0877, val_MinusLogProbMetric: 29.0877

Epoch 210: val_loss improved from 29.20654 to 29.08773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 32s - loss: 29.0957 - MinusLogProbMetric: 29.0957 - val_loss: 29.0877 - val_MinusLogProbMetric: 29.0877 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 211/1000
2023-10-25 18:31:17.678 
Epoch 211/1000 
	 loss: 29.1724, MinusLogProbMetric: 29.1724, val_loss: 29.2056, val_MinusLogProbMetric: 29.2056

Epoch 211: val_loss did not improve from 29.08773
196/196 - 32s - loss: 29.1724 - MinusLogProbMetric: 29.1724 - val_loss: 29.2056 - val_MinusLogProbMetric: 29.2056 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 212/1000
2023-10-25 18:31:50.690 
Epoch 212/1000 
	 loss: 29.1232, MinusLogProbMetric: 29.1232, val_loss: 30.0587, val_MinusLogProbMetric: 30.0587

Epoch 212: val_loss did not improve from 29.08773
196/196 - 33s - loss: 29.1232 - MinusLogProbMetric: 29.1232 - val_loss: 30.0587 - val_MinusLogProbMetric: 30.0587 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 213/1000
2023-10-25 18:32:25.652 
Epoch 213/1000 
	 loss: 29.1962, MinusLogProbMetric: 29.1962, val_loss: 30.4656, val_MinusLogProbMetric: 30.4656

Epoch 213: val_loss did not improve from 29.08773
196/196 - 35s - loss: 29.1962 - MinusLogProbMetric: 29.1962 - val_loss: 30.4656 - val_MinusLogProbMetric: 30.4656 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 214/1000
2023-10-25 18:33:00.386 
Epoch 214/1000 
	 loss: 29.2654, MinusLogProbMetric: 29.2654, val_loss: 29.5905, val_MinusLogProbMetric: 29.5905

Epoch 214: val_loss did not improve from 29.08773
196/196 - 35s - loss: 29.2654 - MinusLogProbMetric: 29.2654 - val_loss: 29.5905 - val_MinusLogProbMetric: 29.5905 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 215/1000
2023-10-25 18:33:34.690 
Epoch 215/1000 
	 loss: 29.2682, MinusLogProbMetric: 29.2682, val_loss: 29.7236, val_MinusLogProbMetric: 29.7236

Epoch 215: val_loss did not improve from 29.08773
196/196 - 34s - loss: 29.2682 - MinusLogProbMetric: 29.2682 - val_loss: 29.7236 - val_MinusLogProbMetric: 29.7236 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 216/1000
2023-10-25 18:34:08.696 
Epoch 216/1000 
	 loss: 29.1874, MinusLogProbMetric: 29.1874, val_loss: 29.5323, val_MinusLogProbMetric: 29.5323

Epoch 216: val_loss did not improve from 29.08773
196/196 - 34s - loss: 29.1874 - MinusLogProbMetric: 29.1874 - val_loss: 29.5323 - val_MinusLogProbMetric: 29.5323 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 217/1000
2023-10-25 18:34:41.259 
Epoch 217/1000 
	 loss: 29.1220, MinusLogProbMetric: 29.1220, val_loss: 29.9124, val_MinusLogProbMetric: 29.9124

Epoch 217: val_loss did not improve from 29.08773
196/196 - 33s - loss: 29.1220 - MinusLogProbMetric: 29.1220 - val_loss: 29.9124 - val_MinusLogProbMetric: 29.9124 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 218/1000
2023-10-25 18:35:16.472 
Epoch 218/1000 
	 loss: 29.0638, MinusLogProbMetric: 29.0638, val_loss: 29.9156, val_MinusLogProbMetric: 29.9156

Epoch 218: val_loss did not improve from 29.08773
196/196 - 35s - loss: 29.0638 - MinusLogProbMetric: 29.0638 - val_loss: 29.9156 - val_MinusLogProbMetric: 29.9156 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 219/1000
2023-10-25 18:35:51.359 
Epoch 219/1000 
	 loss: 29.2356, MinusLogProbMetric: 29.2356, val_loss: 30.0459, val_MinusLogProbMetric: 30.0459

Epoch 219: val_loss did not improve from 29.08773
196/196 - 35s - loss: 29.2356 - MinusLogProbMetric: 29.2356 - val_loss: 30.0459 - val_MinusLogProbMetric: 30.0459 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 220/1000
2023-10-25 18:36:26.342 
Epoch 220/1000 
	 loss: 29.0609, MinusLogProbMetric: 29.0609, val_loss: 30.1791, val_MinusLogProbMetric: 30.1791

Epoch 220: val_loss did not improve from 29.08773
196/196 - 35s - loss: 29.0609 - MinusLogProbMetric: 29.0609 - val_loss: 30.1791 - val_MinusLogProbMetric: 30.1791 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 221/1000
2023-10-25 18:36:57.493 
Epoch 221/1000 
	 loss: 29.1539, MinusLogProbMetric: 29.1539, val_loss: 29.5966, val_MinusLogProbMetric: 29.5966

Epoch 221: val_loss did not improve from 29.08773
196/196 - 31s - loss: 29.1539 - MinusLogProbMetric: 29.1539 - val_loss: 29.5966 - val_MinusLogProbMetric: 29.5966 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 222/1000
2023-10-25 18:37:29.359 
Epoch 222/1000 
	 loss: 29.1873, MinusLogProbMetric: 29.1873, val_loss: 29.2335, val_MinusLogProbMetric: 29.2335

Epoch 222: val_loss did not improve from 29.08773
196/196 - 32s - loss: 29.1873 - MinusLogProbMetric: 29.1873 - val_loss: 29.2335 - val_MinusLogProbMetric: 29.2335 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 223/1000
2023-10-25 18:38:03.333 
Epoch 223/1000 
	 loss: 29.1379, MinusLogProbMetric: 29.1379, val_loss: 29.3835, val_MinusLogProbMetric: 29.3835

Epoch 223: val_loss did not improve from 29.08773
196/196 - 34s - loss: 29.1379 - MinusLogProbMetric: 29.1379 - val_loss: 29.3835 - val_MinusLogProbMetric: 29.3835 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 224/1000
2023-10-25 18:38:37.387 
Epoch 224/1000 
	 loss: 29.0848, MinusLogProbMetric: 29.0848, val_loss: 29.1940, val_MinusLogProbMetric: 29.1940

Epoch 224: val_loss did not improve from 29.08773
196/196 - 34s - loss: 29.0848 - MinusLogProbMetric: 29.0848 - val_loss: 29.1940 - val_MinusLogProbMetric: 29.1940 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 225/1000
2023-10-25 18:39:12.069 
Epoch 225/1000 
	 loss: 29.2711, MinusLogProbMetric: 29.2711, val_loss: 30.0194, val_MinusLogProbMetric: 30.0194

Epoch 225: val_loss did not improve from 29.08773
196/196 - 35s - loss: 29.2711 - MinusLogProbMetric: 29.2711 - val_loss: 30.0194 - val_MinusLogProbMetric: 30.0194 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 226/1000
2023-10-25 18:39:45.526 
Epoch 226/1000 
	 loss: 29.0178, MinusLogProbMetric: 29.0178, val_loss: 29.2014, val_MinusLogProbMetric: 29.2014

Epoch 226: val_loss did not improve from 29.08773
196/196 - 33s - loss: 29.0178 - MinusLogProbMetric: 29.0178 - val_loss: 29.2014 - val_MinusLogProbMetric: 29.2014 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 227/1000
2023-10-25 18:40:19.724 
Epoch 227/1000 
	 loss: 29.0813, MinusLogProbMetric: 29.0813, val_loss: 28.9212, val_MinusLogProbMetric: 28.9212

Epoch 227: val_loss improved from 29.08773 to 28.92119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 29.0813 - MinusLogProbMetric: 29.0813 - val_loss: 28.9212 - val_MinusLogProbMetric: 28.9212 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 228/1000
2023-10-25 18:40:54.530 
Epoch 228/1000 
	 loss: 29.0835, MinusLogProbMetric: 29.0835, val_loss: 29.9974, val_MinusLogProbMetric: 29.9974

Epoch 228: val_loss did not improve from 28.92119
196/196 - 34s - loss: 29.0835 - MinusLogProbMetric: 29.0835 - val_loss: 29.9974 - val_MinusLogProbMetric: 29.9974 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 229/1000
2023-10-25 18:41:30.154 
Epoch 229/1000 
	 loss: 29.1007, MinusLogProbMetric: 29.1007, val_loss: 31.2190, val_MinusLogProbMetric: 31.2190

Epoch 229: val_loss did not improve from 28.92119
196/196 - 36s - loss: 29.1007 - MinusLogProbMetric: 29.1007 - val_loss: 31.2190 - val_MinusLogProbMetric: 31.2190 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 230/1000
2023-10-25 18:42:04.375 
Epoch 230/1000 
	 loss: 28.9989, MinusLogProbMetric: 28.9989, val_loss: 29.4960, val_MinusLogProbMetric: 29.4960

Epoch 230: val_loss did not improve from 28.92119
196/196 - 34s - loss: 28.9989 - MinusLogProbMetric: 28.9989 - val_loss: 29.4960 - val_MinusLogProbMetric: 29.4960 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 231/1000
2023-10-25 18:42:35.443 
Epoch 231/1000 
	 loss: 29.0664, MinusLogProbMetric: 29.0664, val_loss: 29.4051, val_MinusLogProbMetric: 29.4051

Epoch 231: val_loss did not improve from 28.92119
196/196 - 31s - loss: 29.0664 - MinusLogProbMetric: 29.0664 - val_loss: 29.4051 - val_MinusLogProbMetric: 29.4051 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 232/1000
2023-10-25 18:43:09.752 
Epoch 232/1000 
	 loss: 29.0692, MinusLogProbMetric: 29.0692, val_loss: 29.4584, val_MinusLogProbMetric: 29.4584

Epoch 232: val_loss did not improve from 28.92119
196/196 - 34s - loss: 29.0692 - MinusLogProbMetric: 29.0692 - val_loss: 29.4584 - val_MinusLogProbMetric: 29.4584 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 233/1000
2023-10-25 18:43:42.668 
Epoch 233/1000 
	 loss: 29.0248, MinusLogProbMetric: 29.0248, val_loss: 29.6426, val_MinusLogProbMetric: 29.6426

Epoch 233: val_loss did not improve from 28.92119
196/196 - 33s - loss: 29.0248 - MinusLogProbMetric: 29.0248 - val_loss: 29.6426 - val_MinusLogProbMetric: 29.6426 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 234/1000
2023-10-25 18:44:15.619 
Epoch 234/1000 
	 loss: 29.1168, MinusLogProbMetric: 29.1168, val_loss: 29.9765, val_MinusLogProbMetric: 29.9765

Epoch 234: val_loss did not improve from 28.92119
196/196 - 33s - loss: 29.1168 - MinusLogProbMetric: 29.1168 - val_loss: 29.9765 - val_MinusLogProbMetric: 29.9765 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 235/1000
2023-10-25 18:44:50.416 
Epoch 235/1000 
	 loss: 29.1404, MinusLogProbMetric: 29.1404, val_loss: 29.3028, val_MinusLogProbMetric: 29.3028

Epoch 235: val_loss did not improve from 28.92119
196/196 - 35s - loss: 29.1404 - MinusLogProbMetric: 29.1404 - val_loss: 29.3028 - val_MinusLogProbMetric: 29.3028 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 236/1000
2023-10-25 18:45:24.637 
Epoch 236/1000 
	 loss: 28.9062, MinusLogProbMetric: 28.9062, val_loss: 30.6856, val_MinusLogProbMetric: 30.6856

Epoch 236: val_loss did not improve from 28.92119
196/196 - 34s - loss: 28.9062 - MinusLogProbMetric: 28.9062 - val_loss: 30.6856 - val_MinusLogProbMetric: 30.6856 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 237/1000
2023-10-25 18:45:57.113 
Epoch 237/1000 
	 loss: 29.4023, MinusLogProbMetric: 29.4023, val_loss: 29.9207, val_MinusLogProbMetric: 29.9207

Epoch 237: val_loss did not improve from 28.92119
196/196 - 32s - loss: 29.4023 - MinusLogProbMetric: 29.4023 - val_loss: 29.9207 - val_MinusLogProbMetric: 29.9207 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 238/1000
2023-10-25 18:46:29.840 
Epoch 238/1000 
	 loss: 29.0065, MinusLogProbMetric: 29.0065, val_loss: 29.8408, val_MinusLogProbMetric: 29.8408

Epoch 238: val_loss did not improve from 28.92119
196/196 - 33s - loss: 29.0065 - MinusLogProbMetric: 29.0065 - val_loss: 29.8408 - val_MinusLogProbMetric: 29.8408 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 239/1000
2023-10-25 18:46:59.401 
Epoch 239/1000 
	 loss: 28.9568, MinusLogProbMetric: 28.9568, val_loss: 29.0750, val_MinusLogProbMetric: 29.0750

Epoch 239: val_loss did not improve from 28.92119
196/196 - 30s - loss: 28.9568 - MinusLogProbMetric: 28.9568 - val_loss: 29.0750 - val_MinusLogProbMetric: 29.0750 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 240/1000
2023-10-25 18:47:28.972 
Epoch 240/1000 
	 loss: 29.0557, MinusLogProbMetric: 29.0557, val_loss: 29.3928, val_MinusLogProbMetric: 29.3928

Epoch 240: val_loss did not improve from 28.92119
196/196 - 30s - loss: 29.0557 - MinusLogProbMetric: 29.0557 - val_loss: 29.3928 - val_MinusLogProbMetric: 29.3928 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 241/1000
2023-10-25 18:48:00.173 
Epoch 241/1000 
	 loss: 29.1365, MinusLogProbMetric: 29.1365, val_loss: 29.3038, val_MinusLogProbMetric: 29.3038

Epoch 241: val_loss did not improve from 28.92119
196/196 - 31s - loss: 29.1365 - MinusLogProbMetric: 29.1365 - val_loss: 29.3038 - val_MinusLogProbMetric: 29.3038 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 242/1000
2023-10-25 18:48:33.268 
Epoch 242/1000 
	 loss: 29.0141, MinusLogProbMetric: 29.0141, val_loss: 29.8153, val_MinusLogProbMetric: 29.8153

Epoch 242: val_loss did not improve from 28.92119
196/196 - 33s - loss: 29.0141 - MinusLogProbMetric: 29.0141 - val_loss: 29.8153 - val_MinusLogProbMetric: 29.8153 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 243/1000
2023-10-25 18:49:08.083 
Epoch 243/1000 
	 loss: 29.0353, MinusLogProbMetric: 29.0353, val_loss: 30.7365, val_MinusLogProbMetric: 30.7365

Epoch 243: val_loss did not improve from 28.92119
196/196 - 35s - loss: 29.0353 - MinusLogProbMetric: 29.0353 - val_loss: 30.7365 - val_MinusLogProbMetric: 30.7365 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 244/1000
2023-10-25 18:49:39.047 
Epoch 244/1000 
	 loss: 29.0021, MinusLogProbMetric: 29.0021, val_loss: 29.6005, val_MinusLogProbMetric: 29.6005

Epoch 244: val_loss did not improve from 28.92119
196/196 - 31s - loss: 29.0021 - MinusLogProbMetric: 29.0021 - val_loss: 29.6005 - val_MinusLogProbMetric: 29.6005 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 245/1000
2023-10-25 18:50:08.671 
Epoch 245/1000 
	 loss: 29.0587, MinusLogProbMetric: 29.0587, val_loss: 29.1603, val_MinusLogProbMetric: 29.1603

Epoch 245: val_loss did not improve from 28.92119
196/196 - 30s - loss: 29.0587 - MinusLogProbMetric: 29.0587 - val_loss: 29.1603 - val_MinusLogProbMetric: 29.1603 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 246/1000
2023-10-25 18:50:40.435 
Epoch 246/1000 
	 loss: 28.9437, MinusLogProbMetric: 28.9437, val_loss: 29.4657, val_MinusLogProbMetric: 29.4657

Epoch 246: val_loss did not improve from 28.92119
196/196 - 32s - loss: 28.9437 - MinusLogProbMetric: 28.9437 - val_loss: 29.4657 - val_MinusLogProbMetric: 29.4657 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 247/1000
2023-10-25 18:51:11.046 
Epoch 247/1000 
	 loss: 29.0277, MinusLogProbMetric: 29.0277, val_loss: 29.4636, val_MinusLogProbMetric: 29.4636

Epoch 247: val_loss did not improve from 28.92119
196/196 - 31s - loss: 29.0277 - MinusLogProbMetric: 29.0277 - val_loss: 29.4636 - val_MinusLogProbMetric: 29.4636 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 248/1000
2023-10-25 18:51:43.030 
Epoch 248/1000 
	 loss: 29.0696, MinusLogProbMetric: 29.0696, val_loss: 29.9709, val_MinusLogProbMetric: 29.9709

Epoch 248: val_loss did not improve from 28.92119
196/196 - 32s - loss: 29.0696 - MinusLogProbMetric: 29.0696 - val_loss: 29.9709 - val_MinusLogProbMetric: 29.9709 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 249/1000
2023-10-25 18:52:14.477 
Epoch 249/1000 
	 loss: 28.9802, MinusLogProbMetric: 28.9802, val_loss: 29.8813, val_MinusLogProbMetric: 29.8813

Epoch 249: val_loss did not improve from 28.92119
196/196 - 31s - loss: 28.9802 - MinusLogProbMetric: 28.9802 - val_loss: 29.8813 - val_MinusLogProbMetric: 29.8813 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 250/1000
2023-10-25 18:52:48.337 
Epoch 250/1000 
	 loss: 29.0102, MinusLogProbMetric: 29.0102, val_loss: 29.1667, val_MinusLogProbMetric: 29.1667

Epoch 250: val_loss did not improve from 28.92119
196/196 - 34s - loss: 29.0102 - MinusLogProbMetric: 29.0102 - val_loss: 29.1667 - val_MinusLogProbMetric: 29.1667 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 251/1000
2023-10-25 18:53:22.321 
Epoch 251/1000 
	 loss: 28.9444, MinusLogProbMetric: 28.9444, val_loss: 29.4401, val_MinusLogProbMetric: 29.4401

Epoch 251: val_loss did not improve from 28.92119
196/196 - 34s - loss: 28.9444 - MinusLogProbMetric: 28.9444 - val_loss: 29.4401 - val_MinusLogProbMetric: 29.4401 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 252/1000
2023-10-25 18:53:50.631 
Epoch 252/1000 
	 loss: 28.9301, MinusLogProbMetric: 28.9301, val_loss: 29.4986, val_MinusLogProbMetric: 29.4986

Epoch 252: val_loss did not improve from 28.92119
196/196 - 28s - loss: 28.9301 - MinusLogProbMetric: 28.9301 - val_loss: 29.4986 - val_MinusLogProbMetric: 29.4986 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 253/1000
2023-10-25 18:54:18.945 
Epoch 253/1000 
	 loss: 29.0576, MinusLogProbMetric: 29.0576, val_loss: 29.3693, val_MinusLogProbMetric: 29.3693

Epoch 253: val_loss did not improve from 28.92119
196/196 - 28s - loss: 29.0576 - MinusLogProbMetric: 29.0576 - val_loss: 29.3693 - val_MinusLogProbMetric: 29.3693 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 254/1000
2023-10-25 18:54:47.855 
Epoch 254/1000 
	 loss: 28.9621, MinusLogProbMetric: 28.9621, val_loss: 29.1296, val_MinusLogProbMetric: 29.1296

Epoch 254: val_loss did not improve from 28.92119
196/196 - 29s - loss: 28.9621 - MinusLogProbMetric: 28.9621 - val_loss: 29.1296 - val_MinusLogProbMetric: 29.1296 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 255/1000
2023-10-25 18:55:19.026 
Epoch 255/1000 
	 loss: 28.9520, MinusLogProbMetric: 28.9520, val_loss: 31.5241, val_MinusLogProbMetric: 31.5241

Epoch 255: val_loss did not improve from 28.92119
196/196 - 31s - loss: 28.9520 - MinusLogProbMetric: 28.9520 - val_loss: 31.5241 - val_MinusLogProbMetric: 31.5241 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 256/1000
2023-10-25 18:55:50.113 
Epoch 256/1000 
	 loss: 29.1393, MinusLogProbMetric: 29.1393, val_loss: 29.6226, val_MinusLogProbMetric: 29.6226

Epoch 256: val_loss did not improve from 28.92119
196/196 - 31s - loss: 29.1393 - MinusLogProbMetric: 29.1393 - val_loss: 29.6226 - val_MinusLogProbMetric: 29.6226 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 257/1000
2023-10-25 18:56:22.857 
Epoch 257/1000 
	 loss: 29.0619, MinusLogProbMetric: 29.0619, val_loss: 29.6759, val_MinusLogProbMetric: 29.6759

Epoch 257: val_loss did not improve from 28.92119
196/196 - 33s - loss: 29.0619 - MinusLogProbMetric: 29.0619 - val_loss: 29.6759 - val_MinusLogProbMetric: 29.6759 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 258/1000
2023-10-25 18:56:54.684 
Epoch 258/1000 
	 loss: 28.9152, MinusLogProbMetric: 28.9152, val_loss: 29.1018, val_MinusLogProbMetric: 29.1018

Epoch 258: val_loss did not improve from 28.92119
196/196 - 32s - loss: 28.9152 - MinusLogProbMetric: 28.9152 - val_loss: 29.1018 - val_MinusLogProbMetric: 29.1018 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 259/1000
2023-10-25 18:57:27.261 
Epoch 259/1000 
	 loss: 28.9900, MinusLogProbMetric: 28.9900, val_loss: 29.6356, val_MinusLogProbMetric: 29.6356

Epoch 259: val_loss did not improve from 28.92119
196/196 - 33s - loss: 28.9900 - MinusLogProbMetric: 28.9900 - val_loss: 29.6356 - val_MinusLogProbMetric: 29.6356 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 260/1000
2023-10-25 18:57:58.894 
Epoch 260/1000 
	 loss: 28.9780, MinusLogProbMetric: 28.9780, val_loss: 29.3308, val_MinusLogProbMetric: 29.3308

Epoch 260: val_loss did not improve from 28.92119
196/196 - 32s - loss: 28.9780 - MinusLogProbMetric: 28.9780 - val_loss: 29.3308 - val_MinusLogProbMetric: 29.3308 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 261/1000
2023-10-25 18:58:29.141 
Epoch 261/1000 
	 loss: 28.9381, MinusLogProbMetric: 28.9381, val_loss: 29.7799, val_MinusLogProbMetric: 29.7799

Epoch 261: val_loss did not improve from 28.92119
196/196 - 30s - loss: 28.9381 - MinusLogProbMetric: 28.9381 - val_loss: 29.7799 - val_MinusLogProbMetric: 29.7799 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 262/1000
2023-10-25 18:58:59.248 
Epoch 262/1000 
	 loss: 28.8987, MinusLogProbMetric: 28.8987, val_loss: 29.5839, val_MinusLogProbMetric: 29.5839

Epoch 262: val_loss did not improve from 28.92119
196/196 - 30s - loss: 28.8987 - MinusLogProbMetric: 28.8987 - val_loss: 29.5839 - val_MinusLogProbMetric: 29.5839 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 263/1000
2023-10-25 18:59:30.925 
Epoch 263/1000 
	 loss: 28.8889, MinusLogProbMetric: 28.8889, val_loss: 29.0527, val_MinusLogProbMetric: 29.0527

Epoch 263: val_loss did not improve from 28.92119
196/196 - 32s - loss: 28.8889 - MinusLogProbMetric: 28.8889 - val_loss: 29.0527 - val_MinusLogProbMetric: 29.0527 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 264/1000
2023-10-25 19:00:04.103 
Epoch 264/1000 
	 loss: 29.0068, MinusLogProbMetric: 29.0068, val_loss: 29.1389, val_MinusLogProbMetric: 29.1389

Epoch 264: val_loss did not improve from 28.92119
196/196 - 33s - loss: 29.0068 - MinusLogProbMetric: 29.0068 - val_loss: 29.1389 - val_MinusLogProbMetric: 29.1389 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 265/1000
2023-10-25 19:00:37.721 
Epoch 265/1000 
	 loss: 28.8430, MinusLogProbMetric: 28.8430, val_loss: 29.0261, val_MinusLogProbMetric: 29.0261

Epoch 265: val_loss did not improve from 28.92119
196/196 - 34s - loss: 28.8430 - MinusLogProbMetric: 28.8430 - val_loss: 29.0261 - val_MinusLogProbMetric: 29.0261 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 266/1000
2023-10-25 19:01:08.068 
Epoch 266/1000 
	 loss: 28.9560, MinusLogProbMetric: 28.9560, val_loss: 28.8971, val_MinusLogProbMetric: 28.8971

Epoch 266: val_loss improved from 28.92119 to 28.89709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 31s - loss: 28.9560 - MinusLogProbMetric: 28.9560 - val_loss: 28.8971 - val_MinusLogProbMetric: 28.8971 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 267/1000
2023-10-25 19:01:39.005 
Epoch 267/1000 
	 loss: 28.9037, MinusLogProbMetric: 28.9037, val_loss: 29.2163, val_MinusLogProbMetric: 29.2163

Epoch 267: val_loss did not improve from 28.89709
196/196 - 31s - loss: 28.9037 - MinusLogProbMetric: 28.9037 - val_loss: 29.2163 - val_MinusLogProbMetric: 29.2163 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 268/1000
2023-10-25 19:02:08.270 
Epoch 268/1000 
	 loss: 28.9404, MinusLogProbMetric: 28.9404, val_loss: 29.3914, val_MinusLogProbMetric: 29.3914

Epoch 268: val_loss did not improve from 28.89709
196/196 - 29s - loss: 28.9404 - MinusLogProbMetric: 28.9404 - val_loss: 29.3914 - val_MinusLogProbMetric: 29.3914 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 269/1000
2023-10-25 19:02:40.211 
Epoch 269/1000 
	 loss: 28.8966, MinusLogProbMetric: 28.8966, val_loss: 29.4782, val_MinusLogProbMetric: 29.4782

Epoch 269: val_loss did not improve from 28.89709
196/196 - 32s - loss: 28.8966 - MinusLogProbMetric: 28.8966 - val_loss: 29.4782 - val_MinusLogProbMetric: 29.4782 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 270/1000
2023-10-25 19:03:14.640 
Epoch 270/1000 
	 loss: 28.9267, MinusLogProbMetric: 28.9267, val_loss: 29.0003, val_MinusLogProbMetric: 29.0003

Epoch 270: val_loss did not improve from 28.89709
196/196 - 34s - loss: 28.9267 - MinusLogProbMetric: 28.9267 - val_loss: 29.0003 - val_MinusLogProbMetric: 29.0003 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 271/1000
2023-10-25 19:03:47.221 
Epoch 271/1000 
	 loss: 28.8498, MinusLogProbMetric: 28.8498, val_loss: 29.1242, val_MinusLogProbMetric: 29.1242

Epoch 271: val_loss did not improve from 28.89709
196/196 - 33s - loss: 28.8498 - MinusLogProbMetric: 28.8498 - val_loss: 29.1242 - val_MinusLogProbMetric: 29.1242 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 272/1000
2023-10-25 19:04:17.719 
Epoch 272/1000 
	 loss: 29.0674, MinusLogProbMetric: 29.0674, val_loss: 29.0605, val_MinusLogProbMetric: 29.0605

Epoch 272: val_loss did not improve from 28.89709
196/196 - 30s - loss: 29.0674 - MinusLogProbMetric: 29.0674 - val_loss: 29.0605 - val_MinusLogProbMetric: 29.0605 - lr: 0.0010 - 30s/epoch - 156ms/step
Epoch 273/1000
2023-10-25 19:04:46.778 
Epoch 273/1000 
	 loss: 28.7876, MinusLogProbMetric: 28.7876, val_loss: 29.3578, val_MinusLogProbMetric: 29.3578

Epoch 273: val_loss did not improve from 28.89709
196/196 - 29s - loss: 28.7876 - MinusLogProbMetric: 28.7876 - val_loss: 29.3578 - val_MinusLogProbMetric: 29.3578 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 274/1000
2023-10-25 19:05:14.975 
Epoch 274/1000 
	 loss: 28.8684, MinusLogProbMetric: 28.8684, val_loss: 29.8189, val_MinusLogProbMetric: 29.8189

Epoch 274: val_loss did not improve from 28.89709
196/196 - 28s - loss: 28.8684 - MinusLogProbMetric: 28.8684 - val_loss: 29.8189 - val_MinusLogProbMetric: 29.8189 - lr: 0.0010 - 28s/epoch - 144ms/step
Epoch 275/1000
2023-10-25 19:05:45.371 
Epoch 275/1000 
	 loss: 28.9859, MinusLogProbMetric: 28.9859, val_loss: 29.1629, val_MinusLogProbMetric: 29.1629

Epoch 275: val_loss did not improve from 28.89709
196/196 - 30s - loss: 28.9859 - MinusLogProbMetric: 28.9859 - val_loss: 29.1629 - val_MinusLogProbMetric: 29.1629 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 276/1000
2023-10-25 19:06:18.663 
Epoch 276/1000 
	 loss: 28.9011, MinusLogProbMetric: 28.9011, val_loss: 30.7420, val_MinusLogProbMetric: 30.7420

Epoch 276: val_loss did not improve from 28.89709
196/196 - 33s - loss: 28.9011 - MinusLogProbMetric: 28.9011 - val_loss: 30.7420 - val_MinusLogProbMetric: 30.7420 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 277/1000
2023-10-25 19:06:52.582 
Epoch 277/1000 
	 loss: 28.8236, MinusLogProbMetric: 28.8236, val_loss: 31.3514, val_MinusLogProbMetric: 31.3514

Epoch 277: val_loss did not improve from 28.89709
196/196 - 34s - loss: 28.8236 - MinusLogProbMetric: 28.8236 - val_loss: 31.3514 - val_MinusLogProbMetric: 31.3514 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 278/1000
2023-10-25 19:07:21.924 
Epoch 278/1000 
	 loss: 28.9974, MinusLogProbMetric: 28.9974, val_loss: 29.3361, val_MinusLogProbMetric: 29.3361

Epoch 278: val_loss did not improve from 28.89709
196/196 - 29s - loss: 28.9974 - MinusLogProbMetric: 28.9974 - val_loss: 29.3361 - val_MinusLogProbMetric: 29.3361 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 279/1000
2023-10-25 19:07:54.311 
Epoch 279/1000 
	 loss: 28.8797, MinusLogProbMetric: 28.8797, val_loss: 29.1733, val_MinusLogProbMetric: 29.1733

Epoch 279: val_loss did not improve from 28.89709
196/196 - 32s - loss: 28.8797 - MinusLogProbMetric: 28.8797 - val_loss: 29.1733 - val_MinusLogProbMetric: 29.1733 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 280/1000
2023-10-25 19:08:28.502 
Epoch 280/1000 
	 loss: 29.1173, MinusLogProbMetric: 29.1173, val_loss: 29.0486, val_MinusLogProbMetric: 29.0486

Epoch 280: val_loss did not improve from 28.89709
196/196 - 34s - loss: 29.1173 - MinusLogProbMetric: 29.1173 - val_loss: 29.0486 - val_MinusLogProbMetric: 29.0486 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 281/1000
2023-10-25 19:09:00.154 
Epoch 281/1000 
	 loss: 28.7743, MinusLogProbMetric: 28.7743, val_loss: 29.0315, val_MinusLogProbMetric: 29.0315

Epoch 281: val_loss did not improve from 28.89709
196/196 - 32s - loss: 28.7743 - MinusLogProbMetric: 28.7743 - val_loss: 29.0315 - val_MinusLogProbMetric: 29.0315 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 282/1000
2023-10-25 19:09:30.748 
Epoch 282/1000 
	 loss: 28.9581, MinusLogProbMetric: 28.9581, val_loss: 29.2284, val_MinusLogProbMetric: 29.2284

Epoch 282: val_loss did not improve from 28.89709
196/196 - 31s - loss: 28.9581 - MinusLogProbMetric: 28.9581 - val_loss: 29.2284 - val_MinusLogProbMetric: 29.2284 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 283/1000
2023-10-25 19:10:02.677 
Epoch 283/1000 
	 loss: 28.8836, MinusLogProbMetric: 28.8836, val_loss: 29.8367, val_MinusLogProbMetric: 29.8367

Epoch 283: val_loss did not improve from 28.89709
196/196 - 32s - loss: 28.8836 - MinusLogProbMetric: 28.8836 - val_loss: 29.8367 - val_MinusLogProbMetric: 29.8367 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 284/1000
2023-10-25 19:10:34.327 
Epoch 284/1000 
	 loss: 28.7685, MinusLogProbMetric: 28.7685, val_loss: 29.2456, val_MinusLogProbMetric: 29.2456

Epoch 284: val_loss did not improve from 28.89709
196/196 - 32s - loss: 28.7685 - MinusLogProbMetric: 28.7685 - val_loss: 29.2456 - val_MinusLogProbMetric: 29.2456 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 285/1000
2023-10-25 19:11:04.836 
Epoch 285/1000 
	 loss: 28.7796, MinusLogProbMetric: 28.7796, val_loss: 29.8612, val_MinusLogProbMetric: 29.8612

Epoch 285: val_loss did not improve from 28.89709
196/196 - 31s - loss: 28.7796 - MinusLogProbMetric: 28.7796 - val_loss: 29.8612 - val_MinusLogProbMetric: 29.8612 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 286/1000
2023-10-25 19:11:35.842 
Epoch 286/1000 
	 loss: 28.8445, MinusLogProbMetric: 28.8445, val_loss: 29.3574, val_MinusLogProbMetric: 29.3574

Epoch 286: val_loss did not improve from 28.89709
196/196 - 31s - loss: 28.8445 - MinusLogProbMetric: 28.8445 - val_loss: 29.3574 - val_MinusLogProbMetric: 29.3574 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 287/1000
2023-10-25 19:12:06.288 
Epoch 287/1000 
	 loss: 28.9319, MinusLogProbMetric: 28.9319, val_loss: 29.2536, val_MinusLogProbMetric: 29.2536

Epoch 287: val_loss did not improve from 28.89709
196/196 - 30s - loss: 28.9319 - MinusLogProbMetric: 28.9319 - val_loss: 29.2536 - val_MinusLogProbMetric: 29.2536 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 288/1000
2023-10-25 19:12:34.363 
Epoch 288/1000 
	 loss: 28.7911, MinusLogProbMetric: 28.7911, val_loss: 29.3250, val_MinusLogProbMetric: 29.3250

Epoch 288: val_loss did not improve from 28.89709
196/196 - 28s - loss: 28.7911 - MinusLogProbMetric: 28.7911 - val_loss: 29.3250 - val_MinusLogProbMetric: 29.3250 - lr: 0.0010 - 28s/epoch - 143ms/step
Epoch 289/1000
2023-10-25 19:13:04.736 
Epoch 289/1000 
	 loss: 28.7379, MinusLogProbMetric: 28.7379, val_loss: 29.0816, val_MinusLogProbMetric: 29.0816

Epoch 289: val_loss did not improve from 28.89709
196/196 - 30s - loss: 28.7379 - MinusLogProbMetric: 28.7379 - val_loss: 29.0816 - val_MinusLogProbMetric: 29.0816 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 290/1000
2023-10-25 19:13:38.163 
Epoch 290/1000 
	 loss: 28.8362, MinusLogProbMetric: 28.8362, val_loss: 28.9973, val_MinusLogProbMetric: 28.9973

Epoch 290: val_loss did not improve from 28.89709
196/196 - 33s - loss: 28.8362 - MinusLogProbMetric: 28.8362 - val_loss: 28.9973 - val_MinusLogProbMetric: 28.9973 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 291/1000
2023-10-25 19:14:10.710 
Epoch 291/1000 
	 loss: 28.8088, MinusLogProbMetric: 28.8088, val_loss: 29.4900, val_MinusLogProbMetric: 29.4900

Epoch 291: val_loss did not improve from 28.89709
196/196 - 33s - loss: 28.8088 - MinusLogProbMetric: 28.8088 - val_loss: 29.4900 - val_MinusLogProbMetric: 29.4900 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 292/1000
2023-10-25 19:14:43.407 
Epoch 292/1000 
	 loss: 28.7394, MinusLogProbMetric: 28.7394, val_loss: 29.4366, val_MinusLogProbMetric: 29.4366

Epoch 292: val_loss did not improve from 28.89709
196/196 - 33s - loss: 28.7394 - MinusLogProbMetric: 28.7394 - val_loss: 29.4366 - val_MinusLogProbMetric: 29.4366 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 293/1000
2023-10-25 19:15:17.914 
Epoch 293/1000 
	 loss: 28.8698, MinusLogProbMetric: 28.8698, val_loss: 29.2956, val_MinusLogProbMetric: 29.2956

Epoch 293: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.8698 - MinusLogProbMetric: 28.8698 - val_loss: 29.2956 - val_MinusLogProbMetric: 29.2956 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 294/1000
2023-10-25 19:15:49.052 
Epoch 294/1000 
	 loss: 28.9160, MinusLogProbMetric: 28.9160, val_loss: 29.6075, val_MinusLogProbMetric: 29.6075

Epoch 294: val_loss did not improve from 28.89709
196/196 - 31s - loss: 28.9160 - MinusLogProbMetric: 28.9160 - val_loss: 29.6075 - val_MinusLogProbMetric: 29.6075 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 295/1000
2023-10-25 19:16:21.426 
Epoch 295/1000 
	 loss: 28.7154, MinusLogProbMetric: 28.7154, val_loss: 29.2083, val_MinusLogProbMetric: 29.2083

Epoch 295: val_loss did not improve from 28.89709
196/196 - 32s - loss: 28.7154 - MinusLogProbMetric: 28.7154 - val_loss: 29.2083 - val_MinusLogProbMetric: 29.2083 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 296/1000
2023-10-25 19:16:56.250 
Epoch 296/1000 
	 loss: 28.8070, MinusLogProbMetric: 28.8070, val_loss: 29.2456, val_MinusLogProbMetric: 29.2456

Epoch 296: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.8070 - MinusLogProbMetric: 28.8070 - val_loss: 29.2456 - val_MinusLogProbMetric: 29.2456 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 297/1000
2023-10-25 19:17:31.126 
Epoch 297/1000 
	 loss: 28.6495, MinusLogProbMetric: 28.6495, val_loss: 29.6310, val_MinusLogProbMetric: 29.6310

Epoch 297: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.6495 - MinusLogProbMetric: 28.6495 - val_loss: 29.6310 - val_MinusLogProbMetric: 29.6310 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 298/1000
2023-10-25 19:18:06.049 
Epoch 298/1000 
	 loss: 28.8120, MinusLogProbMetric: 28.8120, val_loss: 29.3768, val_MinusLogProbMetric: 29.3768

Epoch 298: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.8120 - MinusLogProbMetric: 28.8120 - val_loss: 29.3768 - val_MinusLogProbMetric: 29.3768 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 299/1000
2023-10-25 19:18:40.539 
Epoch 299/1000 
	 loss: 28.7673, MinusLogProbMetric: 28.7673, val_loss: 29.4017, val_MinusLogProbMetric: 29.4017

Epoch 299: val_loss did not improve from 28.89709
196/196 - 34s - loss: 28.7673 - MinusLogProbMetric: 28.7673 - val_loss: 29.4017 - val_MinusLogProbMetric: 29.4017 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 300/1000
2023-10-25 19:19:14.700 
Epoch 300/1000 
	 loss: 28.8736, MinusLogProbMetric: 28.8736, val_loss: 29.1485, val_MinusLogProbMetric: 29.1485

Epoch 300: val_loss did not improve from 28.89709
196/196 - 34s - loss: 28.8736 - MinusLogProbMetric: 28.8736 - val_loss: 29.1485 - val_MinusLogProbMetric: 29.1485 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 301/1000
2023-10-25 19:19:49.356 
Epoch 301/1000 
	 loss: 28.8215, MinusLogProbMetric: 28.8215, val_loss: 29.2577, val_MinusLogProbMetric: 29.2577

Epoch 301: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.8215 - MinusLogProbMetric: 28.8215 - val_loss: 29.2577 - val_MinusLogProbMetric: 29.2577 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 302/1000
2023-10-25 19:20:23.789 
Epoch 302/1000 
	 loss: 28.7502, MinusLogProbMetric: 28.7502, val_loss: 29.1474, val_MinusLogProbMetric: 29.1474

Epoch 302: val_loss did not improve from 28.89709
196/196 - 34s - loss: 28.7502 - MinusLogProbMetric: 28.7502 - val_loss: 29.1474 - val_MinusLogProbMetric: 29.1474 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 303/1000
2023-10-25 19:20:58.367 
Epoch 303/1000 
	 loss: 28.7659, MinusLogProbMetric: 28.7659, val_loss: 29.2510, val_MinusLogProbMetric: 29.2510

Epoch 303: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7659 - MinusLogProbMetric: 28.7659 - val_loss: 29.2510 - val_MinusLogProbMetric: 29.2510 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 304/1000
2023-10-25 19:21:33.028 
Epoch 304/1000 
	 loss: 28.7923, MinusLogProbMetric: 28.7923, val_loss: 29.6511, val_MinusLogProbMetric: 29.6511

Epoch 304: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7923 - MinusLogProbMetric: 28.7923 - val_loss: 29.6511 - val_MinusLogProbMetric: 29.6511 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 305/1000
2023-10-25 19:22:07.615 
Epoch 305/1000 
	 loss: 28.7505, MinusLogProbMetric: 28.7505, val_loss: 29.7548, val_MinusLogProbMetric: 29.7548

Epoch 305: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7505 - MinusLogProbMetric: 28.7505 - val_loss: 29.7548 - val_MinusLogProbMetric: 29.7548 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 306/1000
2023-10-25 19:22:42.591 
Epoch 306/1000 
	 loss: 28.7917, MinusLogProbMetric: 28.7917, val_loss: 30.5519, val_MinusLogProbMetric: 30.5519

Epoch 306: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7917 - MinusLogProbMetric: 28.7917 - val_loss: 30.5519 - val_MinusLogProbMetric: 30.5519 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 307/1000
2023-10-25 19:23:17.227 
Epoch 307/1000 
	 loss: 28.7120, MinusLogProbMetric: 28.7120, val_loss: 29.0186, val_MinusLogProbMetric: 29.0186

Epoch 307: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7120 - MinusLogProbMetric: 28.7120 - val_loss: 29.0186 - val_MinusLogProbMetric: 29.0186 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 308/1000
2023-10-25 19:23:51.921 
Epoch 308/1000 
	 loss: 28.7942, MinusLogProbMetric: 28.7942, val_loss: 29.6107, val_MinusLogProbMetric: 29.6107

Epoch 308: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7942 - MinusLogProbMetric: 28.7942 - val_loss: 29.6107 - val_MinusLogProbMetric: 29.6107 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 309/1000
2023-10-25 19:24:26.645 
Epoch 309/1000 
	 loss: 28.7372, MinusLogProbMetric: 28.7372, val_loss: 29.0886, val_MinusLogProbMetric: 29.0886

Epoch 309: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7372 - MinusLogProbMetric: 28.7372 - val_loss: 29.0886 - val_MinusLogProbMetric: 29.0886 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 310/1000
2023-10-25 19:25:01.613 
Epoch 310/1000 
	 loss: 28.6873, MinusLogProbMetric: 28.6873, val_loss: 29.2040, val_MinusLogProbMetric: 29.2040

Epoch 310: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.6873 - MinusLogProbMetric: 28.6873 - val_loss: 29.2040 - val_MinusLogProbMetric: 29.2040 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 311/1000
2023-10-25 19:25:36.379 
Epoch 311/1000 
	 loss: 28.7711, MinusLogProbMetric: 28.7711, val_loss: 29.5943, val_MinusLogProbMetric: 29.5943

Epoch 311: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7711 - MinusLogProbMetric: 28.7711 - val_loss: 29.5943 - val_MinusLogProbMetric: 29.5943 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 312/1000
2023-10-25 19:26:11.105 
Epoch 312/1000 
	 loss: 28.7382, MinusLogProbMetric: 28.7382, val_loss: 29.3678, val_MinusLogProbMetric: 29.3678

Epoch 312: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7382 - MinusLogProbMetric: 28.7382 - val_loss: 29.3678 - val_MinusLogProbMetric: 29.3678 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 313/1000
2023-10-25 19:26:45.642 
Epoch 313/1000 
	 loss: 28.9176, MinusLogProbMetric: 28.9176, val_loss: 29.0866, val_MinusLogProbMetric: 29.0866

Epoch 313: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.9176 - MinusLogProbMetric: 28.9176 - val_loss: 29.0866 - val_MinusLogProbMetric: 29.0866 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 314/1000
2023-10-25 19:27:20.384 
Epoch 314/1000 
	 loss: 28.8240, MinusLogProbMetric: 28.8240, val_loss: 29.9808, val_MinusLogProbMetric: 29.9808

Epoch 314: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.8240 - MinusLogProbMetric: 28.8240 - val_loss: 29.9808 - val_MinusLogProbMetric: 29.9808 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 315/1000
2023-10-25 19:27:55.149 
Epoch 315/1000 
	 loss: 28.8847, MinusLogProbMetric: 28.8847, val_loss: 29.2709, val_MinusLogProbMetric: 29.2709

Epoch 315: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.8847 - MinusLogProbMetric: 28.8847 - val_loss: 29.2709 - val_MinusLogProbMetric: 29.2709 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 316/1000
2023-10-25 19:28:29.864 
Epoch 316/1000 
	 loss: 28.7365, MinusLogProbMetric: 28.7365, val_loss: 31.5552, val_MinusLogProbMetric: 31.5552

Epoch 316: val_loss did not improve from 28.89709
196/196 - 35s - loss: 28.7365 - MinusLogProbMetric: 28.7365 - val_loss: 31.5552 - val_MinusLogProbMetric: 31.5552 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 317/1000
2023-10-25 19:29:04.902 
Epoch 317/1000 
	 loss: 28.0538, MinusLogProbMetric: 28.0538, val_loss: 28.7138, val_MinusLogProbMetric: 28.7138

Epoch 317: val_loss improved from 28.89709 to 28.71376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 28.0538 - MinusLogProbMetric: 28.0538 - val_loss: 28.7138 - val_MinusLogProbMetric: 28.7138 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 318/1000
2023-10-25 19:29:40.255 
Epoch 318/1000 
	 loss: 27.9983, MinusLogProbMetric: 27.9983, val_loss: 28.5997, val_MinusLogProbMetric: 28.5997

Epoch 318: val_loss improved from 28.71376 to 28.59975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.9983 - MinusLogProbMetric: 27.9983 - val_loss: 28.5997 - val_MinusLogProbMetric: 28.5997 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 319/1000
2023-10-25 19:30:15.975 
Epoch 319/1000 
	 loss: 28.0684, MinusLogProbMetric: 28.0684, val_loss: 28.4530, val_MinusLogProbMetric: 28.4530

Epoch 319: val_loss improved from 28.59975 to 28.45302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 28.0684 - MinusLogProbMetric: 28.0684 - val_loss: 28.4530 - val_MinusLogProbMetric: 28.4530 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 320/1000
2023-10-25 19:30:51.463 
Epoch 320/1000 
	 loss: 28.1002, MinusLogProbMetric: 28.1002, val_loss: 28.5496, val_MinusLogProbMetric: 28.5496

Epoch 320: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.1002 - MinusLogProbMetric: 28.1002 - val_loss: 28.5496 - val_MinusLogProbMetric: 28.5496 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 321/1000
2023-10-25 19:31:26.214 
Epoch 321/1000 
	 loss: 28.1428, MinusLogProbMetric: 28.1428, val_loss: 28.9529, val_MinusLogProbMetric: 28.9529

Epoch 321: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.1428 - MinusLogProbMetric: 28.1428 - val_loss: 28.9529 - val_MinusLogProbMetric: 28.9529 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 322/1000
2023-10-25 19:32:01.080 
Epoch 322/1000 
	 loss: 28.0561, MinusLogProbMetric: 28.0561, val_loss: 29.1983, val_MinusLogProbMetric: 29.1983

Epoch 322: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0561 - MinusLogProbMetric: 28.0561 - val_loss: 29.1983 - val_MinusLogProbMetric: 29.1983 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 323/1000
2023-10-25 19:32:36.270 
Epoch 323/1000 
	 loss: 27.9895, MinusLogProbMetric: 27.9895, val_loss: 28.6176, val_MinusLogProbMetric: 28.6176

Epoch 323: val_loss did not improve from 28.45302
196/196 - 35s - loss: 27.9895 - MinusLogProbMetric: 27.9895 - val_loss: 28.6176 - val_MinusLogProbMetric: 28.6176 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 324/1000
2023-10-25 19:33:11.263 
Epoch 324/1000 
	 loss: 28.0273, MinusLogProbMetric: 28.0273, val_loss: 28.9461, val_MinusLogProbMetric: 28.9461

Epoch 324: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0273 - MinusLogProbMetric: 28.0273 - val_loss: 28.9461 - val_MinusLogProbMetric: 28.9461 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 325/1000
2023-10-25 19:33:46.148 
Epoch 325/1000 
	 loss: 28.2333, MinusLogProbMetric: 28.2333, val_loss: 28.9961, val_MinusLogProbMetric: 28.9961

Epoch 325: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.2333 - MinusLogProbMetric: 28.2333 - val_loss: 28.9961 - val_MinusLogProbMetric: 28.9961 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 326/1000
2023-10-25 19:34:21.134 
Epoch 326/1000 
	 loss: 28.0246, MinusLogProbMetric: 28.0246, val_loss: 28.4974, val_MinusLogProbMetric: 28.4974

Epoch 326: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0246 - MinusLogProbMetric: 28.0246 - val_loss: 28.4974 - val_MinusLogProbMetric: 28.4974 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 327/1000
2023-10-25 19:34:55.993 
Epoch 327/1000 
	 loss: 27.9352, MinusLogProbMetric: 27.9352, val_loss: 29.3643, val_MinusLogProbMetric: 29.3643

Epoch 327: val_loss did not improve from 28.45302
196/196 - 35s - loss: 27.9352 - MinusLogProbMetric: 27.9352 - val_loss: 29.3643 - val_MinusLogProbMetric: 29.3643 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 328/1000
2023-10-25 19:35:31.098 
Epoch 328/1000 
	 loss: 28.0374, MinusLogProbMetric: 28.0374, val_loss: 28.8739, val_MinusLogProbMetric: 28.8739

Epoch 328: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0374 - MinusLogProbMetric: 28.0374 - val_loss: 28.8739 - val_MinusLogProbMetric: 28.8739 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 329/1000
2023-10-25 19:36:05.855 
Epoch 329/1000 
	 loss: 28.0002, MinusLogProbMetric: 28.0002, val_loss: 28.8364, val_MinusLogProbMetric: 28.8364

Epoch 329: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0002 - MinusLogProbMetric: 28.0002 - val_loss: 28.8364 - val_MinusLogProbMetric: 28.8364 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 330/1000
2023-10-25 19:36:40.498 
Epoch 330/1000 
	 loss: 28.3437, MinusLogProbMetric: 28.3437, val_loss: 29.1627, val_MinusLogProbMetric: 29.1627

Epoch 330: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.3437 - MinusLogProbMetric: 28.3437 - val_loss: 29.1627 - val_MinusLogProbMetric: 29.1627 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 331/1000
2023-10-25 19:37:15.077 
Epoch 331/1000 
	 loss: 28.0828, MinusLogProbMetric: 28.0828, val_loss: 28.9916, val_MinusLogProbMetric: 28.9916

Epoch 331: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0828 - MinusLogProbMetric: 28.0828 - val_loss: 28.9916 - val_MinusLogProbMetric: 28.9916 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 332/1000
2023-10-25 19:37:49.882 
Epoch 332/1000 
	 loss: 27.9936, MinusLogProbMetric: 27.9936, val_loss: 29.0450, val_MinusLogProbMetric: 29.0450

Epoch 332: val_loss did not improve from 28.45302
196/196 - 35s - loss: 27.9936 - MinusLogProbMetric: 27.9936 - val_loss: 29.0450 - val_MinusLogProbMetric: 29.0450 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 333/1000
2023-10-25 19:38:25.135 
Epoch 333/1000 
	 loss: 28.1594, MinusLogProbMetric: 28.1594, val_loss: 28.7433, val_MinusLogProbMetric: 28.7433

Epoch 333: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.1594 - MinusLogProbMetric: 28.1594 - val_loss: 28.7433 - val_MinusLogProbMetric: 28.7433 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 334/1000
2023-10-25 19:39:00.061 
Epoch 334/1000 
	 loss: 28.0442, MinusLogProbMetric: 28.0442, val_loss: 28.9002, val_MinusLogProbMetric: 28.9002

Epoch 334: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0442 - MinusLogProbMetric: 28.0442 - val_loss: 28.9002 - val_MinusLogProbMetric: 28.9002 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 335/1000
2023-10-25 19:39:35.209 
Epoch 335/1000 
	 loss: 27.9610, MinusLogProbMetric: 27.9610, val_loss: 28.8030, val_MinusLogProbMetric: 28.8030

Epoch 335: val_loss did not improve from 28.45302
196/196 - 35s - loss: 27.9610 - MinusLogProbMetric: 27.9610 - val_loss: 28.8030 - val_MinusLogProbMetric: 28.8030 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 336/1000
2023-10-25 19:40:09.792 
Epoch 336/1000 
	 loss: 28.0715, MinusLogProbMetric: 28.0715, val_loss: 29.2172, val_MinusLogProbMetric: 29.2172

Epoch 336: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0715 - MinusLogProbMetric: 28.0715 - val_loss: 29.2172 - val_MinusLogProbMetric: 29.2172 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 337/1000
2023-10-25 19:40:44.087 
Epoch 337/1000 
	 loss: 28.0438, MinusLogProbMetric: 28.0438, val_loss: 28.9206, val_MinusLogProbMetric: 28.9206

Epoch 337: val_loss did not improve from 28.45302
196/196 - 34s - loss: 28.0438 - MinusLogProbMetric: 28.0438 - val_loss: 28.9206 - val_MinusLogProbMetric: 28.9206 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 338/1000
2023-10-25 19:41:14.809 
Epoch 338/1000 
	 loss: 27.9981, MinusLogProbMetric: 27.9981, val_loss: 28.6822, val_MinusLogProbMetric: 28.6822

Epoch 338: val_loss did not improve from 28.45302
196/196 - 31s - loss: 27.9981 - MinusLogProbMetric: 27.9981 - val_loss: 28.6822 - val_MinusLogProbMetric: 28.6822 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 339/1000
2023-10-25 19:41:49.001 
Epoch 339/1000 
	 loss: 27.9866, MinusLogProbMetric: 27.9866, val_loss: 28.8644, val_MinusLogProbMetric: 28.8644

Epoch 339: val_loss did not improve from 28.45302
196/196 - 34s - loss: 27.9866 - MinusLogProbMetric: 27.9866 - val_loss: 28.8644 - val_MinusLogProbMetric: 28.8644 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 340/1000
2023-10-25 19:42:22.686 
Epoch 340/1000 
	 loss: 27.9911, MinusLogProbMetric: 27.9911, val_loss: 29.0657, val_MinusLogProbMetric: 29.0657

Epoch 340: val_loss did not improve from 28.45302
196/196 - 34s - loss: 27.9911 - MinusLogProbMetric: 27.9911 - val_loss: 29.0657 - val_MinusLogProbMetric: 29.0657 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 341/1000
2023-10-25 19:42:55.556 
Epoch 341/1000 
	 loss: 28.0896, MinusLogProbMetric: 28.0896, val_loss: 28.6542, val_MinusLogProbMetric: 28.6542

Epoch 341: val_loss did not improve from 28.45302
196/196 - 33s - loss: 28.0896 - MinusLogProbMetric: 28.0896 - val_loss: 28.6542 - val_MinusLogProbMetric: 28.6542 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 342/1000
2023-10-25 19:43:30.200 
Epoch 342/1000 
	 loss: 27.9639, MinusLogProbMetric: 27.9639, val_loss: 28.6467, val_MinusLogProbMetric: 28.6467

Epoch 342: val_loss did not improve from 28.45302
196/196 - 35s - loss: 27.9639 - MinusLogProbMetric: 27.9639 - val_loss: 28.6467 - val_MinusLogProbMetric: 28.6467 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 343/1000
2023-10-25 19:44:05.053 
Epoch 343/1000 
	 loss: 28.0329, MinusLogProbMetric: 28.0329, val_loss: 28.5781, val_MinusLogProbMetric: 28.5781

Epoch 343: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0329 - MinusLogProbMetric: 28.0329 - val_loss: 28.5781 - val_MinusLogProbMetric: 28.5781 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 344/1000
2023-10-25 19:44:40.235 
Epoch 344/1000 
	 loss: 28.0746, MinusLogProbMetric: 28.0746, val_loss: 30.0131, val_MinusLogProbMetric: 30.0131

Epoch 344: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0746 - MinusLogProbMetric: 28.0746 - val_loss: 30.0131 - val_MinusLogProbMetric: 30.0131 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 345/1000
2023-10-25 19:45:13.202 
Epoch 345/1000 
	 loss: 28.0162, MinusLogProbMetric: 28.0162, val_loss: 28.6480, val_MinusLogProbMetric: 28.6480

Epoch 345: val_loss did not improve from 28.45302
196/196 - 33s - loss: 28.0162 - MinusLogProbMetric: 28.0162 - val_loss: 28.6480 - val_MinusLogProbMetric: 28.6480 - lr: 5.0000e-04 - 33s/epoch - 168ms/step
Epoch 346/1000
2023-10-25 19:45:47.511 
Epoch 346/1000 
	 loss: 27.9890, MinusLogProbMetric: 27.9890, val_loss: 28.6905, val_MinusLogProbMetric: 28.6905

Epoch 346: val_loss did not improve from 28.45302
196/196 - 34s - loss: 27.9890 - MinusLogProbMetric: 27.9890 - val_loss: 28.6905 - val_MinusLogProbMetric: 28.6905 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 347/1000
2023-10-25 19:46:22.441 
Epoch 347/1000 
	 loss: 28.0034, MinusLogProbMetric: 28.0034, val_loss: 28.6747, val_MinusLogProbMetric: 28.6747

Epoch 347: val_loss did not improve from 28.45302
196/196 - 35s - loss: 28.0034 - MinusLogProbMetric: 28.0034 - val_loss: 28.6747 - val_MinusLogProbMetric: 28.6747 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 348/1000
2023-10-25 19:46:57.510 
Epoch 348/1000 
	 loss: 27.9089, MinusLogProbMetric: 27.9089, val_loss: 28.4459, val_MinusLogProbMetric: 28.4459

Epoch 348: val_loss improved from 28.45302 to 28.44588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 27.9089 - MinusLogProbMetric: 27.9089 - val_loss: 28.4459 - val_MinusLogProbMetric: 28.4459 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 349/1000
2023-10-25 19:47:33.080 
Epoch 349/1000 
	 loss: 27.9526, MinusLogProbMetric: 27.9526, val_loss: 28.6137, val_MinusLogProbMetric: 28.6137

Epoch 349: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9526 - MinusLogProbMetric: 27.9526 - val_loss: 28.6137 - val_MinusLogProbMetric: 28.6137 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 350/1000
2023-10-25 19:48:07.917 
Epoch 350/1000 
	 loss: 27.9728, MinusLogProbMetric: 27.9728, val_loss: 30.0033, val_MinusLogProbMetric: 30.0033

Epoch 350: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9728 - MinusLogProbMetric: 27.9728 - val_loss: 30.0033 - val_MinusLogProbMetric: 30.0033 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 351/1000
2023-10-25 19:48:42.741 
Epoch 351/1000 
	 loss: 28.1144, MinusLogProbMetric: 28.1144, val_loss: 28.6508, val_MinusLogProbMetric: 28.6508

Epoch 351: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.1144 - MinusLogProbMetric: 28.1144 - val_loss: 28.6508 - val_MinusLogProbMetric: 28.6508 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 352/1000
2023-10-25 19:49:17.877 
Epoch 352/1000 
	 loss: 27.9356, MinusLogProbMetric: 27.9356, val_loss: 28.4532, val_MinusLogProbMetric: 28.4532

Epoch 352: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9356 - MinusLogProbMetric: 27.9356 - val_loss: 28.4532 - val_MinusLogProbMetric: 28.4532 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 353/1000
2023-10-25 19:49:52.794 
Epoch 353/1000 
	 loss: 27.9945, MinusLogProbMetric: 27.9945, val_loss: 28.7461, val_MinusLogProbMetric: 28.7461

Epoch 353: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9945 - MinusLogProbMetric: 27.9945 - val_loss: 28.7461 - val_MinusLogProbMetric: 28.7461 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 354/1000
2023-10-25 19:50:27.742 
Epoch 354/1000 
	 loss: 28.0270, MinusLogProbMetric: 28.0270, val_loss: 28.5895, val_MinusLogProbMetric: 28.5895

Epoch 354: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0270 - MinusLogProbMetric: 28.0270 - val_loss: 28.5895 - val_MinusLogProbMetric: 28.5895 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 355/1000
2023-10-25 19:51:02.715 
Epoch 355/1000 
	 loss: 27.8932, MinusLogProbMetric: 27.8932, val_loss: 28.5010, val_MinusLogProbMetric: 28.5010

Epoch 355: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.8932 - MinusLogProbMetric: 27.8932 - val_loss: 28.5010 - val_MinusLogProbMetric: 28.5010 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 356/1000
2023-10-25 19:51:37.808 
Epoch 356/1000 
	 loss: 28.0485, MinusLogProbMetric: 28.0485, val_loss: 28.5223, val_MinusLogProbMetric: 28.5223

Epoch 356: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0485 - MinusLogProbMetric: 28.0485 - val_loss: 28.5223 - val_MinusLogProbMetric: 28.5223 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 357/1000
2023-10-25 19:52:12.568 
Epoch 357/1000 
	 loss: 27.9087, MinusLogProbMetric: 27.9087, val_loss: 28.5577, val_MinusLogProbMetric: 28.5577

Epoch 357: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9087 - MinusLogProbMetric: 27.9087 - val_loss: 28.5577 - val_MinusLogProbMetric: 28.5577 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 358/1000
2023-10-25 19:52:47.507 
Epoch 358/1000 
	 loss: 27.9244, MinusLogProbMetric: 27.9244, val_loss: 28.7558, val_MinusLogProbMetric: 28.7558

Epoch 358: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9244 - MinusLogProbMetric: 27.9244 - val_loss: 28.7558 - val_MinusLogProbMetric: 28.7558 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 359/1000
2023-10-25 19:53:22.624 
Epoch 359/1000 
	 loss: 27.9374, MinusLogProbMetric: 27.9374, val_loss: 28.5507, val_MinusLogProbMetric: 28.5507

Epoch 359: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9374 - MinusLogProbMetric: 27.9374 - val_loss: 28.5507 - val_MinusLogProbMetric: 28.5507 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 360/1000
2023-10-25 19:53:57.575 
Epoch 360/1000 
	 loss: 28.0924, MinusLogProbMetric: 28.0924, val_loss: 28.5480, val_MinusLogProbMetric: 28.5480

Epoch 360: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0924 - MinusLogProbMetric: 28.0924 - val_loss: 28.5480 - val_MinusLogProbMetric: 28.5480 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 361/1000
2023-10-25 19:54:32.668 
Epoch 361/1000 
	 loss: 27.8954, MinusLogProbMetric: 27.8954, val_loss: 28.6686, val_MinusLogProbMetric: 28.6686

Epoch 361: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.8954 - MinusLogProbMetric: 27.8954 - val_loss: 28.6686 - val_MinusLogProbMetric: 28.6686 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 362/1000
2023-10-25 19:55:07.853 
Epoch 362/1000 
	 loss: 27.9109, MinusLogProbMetric: 27.9109, val_loss: 28.5334, val_MinusLogProbMetric: 28.5334

Epoch 362: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9109 - MinusLogProbMetric: 27.9109 - val_loss: 28.5334 - val_MinusLogProbMetric: 28.5334 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 363/1000
2023-10-25 19:55:42.985 
Epoch 363/1000 
	 loss: 28.0058, MinusLogProbMetric: 28.0058, val_loss: 29.1939, val_MinusLogProbMetric: 29.1939

Epoch 363: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0058 - MinusLogProbMetric: 28.0058 - val_loss: 29.1939 - val_MinusLogProbMetric: 29.1939 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 364/1000
2023-10-25 19:56:18.347 
Epoch 364/1000 
	 loss: 27.9980, MinusLogProbMetric: 27.9980, val_loss: 29.5113, val_MinusLogProbMetric: 29.5113

Epoch 364: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9980 - MinusLogProbMetric: 27.9980 - val_loss: 29.5113 - val_MinusLogProbMetric: 29.5113 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 365/1000
2023-10-25 19:56:53.391 
Epoch 365/1000 
	 loss: 27.9634, MinusLogProbMetric: 27.9634, val_loss: 28.5461, val_MinusLogProbMetric: 28.5461

Epoch 365: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9634 - MinusLogProbMetric: 27.9634 - val_loss: 28.5461 - val_MinusLogProbMetric: 28.5461 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 366/1000
2023-10-25 19:57:28.706 
Epoch 366/1000 
	 loss: 27.9948, MinusLogProbMetric: 27.9948, val_loss: 28.6375, val_MinusLogProbMetric: 28.6375

Epoch 366: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9948 - MinusLogProbMetric: 27.9948 - val_loss: 28.6375 - val_MinusLogProbMetric: 28.6375 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 367/1000
2023-10-25 19:58:03.846 
Epoch 367/1000 
	 loss: 27.9207, MinusLogProbMetric: 27.9207, val_loss: 28.7366, val_MinusLogProbMetric: 28.7366

Epoch 367: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9207 - MinusLogProbMetric: 27.9207 - val_loss: 28.7366 - val_MinusLogProbMetric: 28.7366 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 368/1000
2023-10-25 19:58:38.915 
Epoch 368/1000 
	 loss: 27.9868, MinusLogProbMetric: 27.9868, val_loss: 28.5413, val_MinusLogProbMetric: 28.5413

Epoch 368: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9868 - MinusLogProbMetric: 27.9868 - val_loss: 28.5413 - val_MinusLogProbMetric: 28.5413 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 369/1000
2023-10-25 19:59:13.902 
Epoch 369/1000 
	 loss: 27.9624, MinusLogProbMetric: 27.9624, val_loss: 28.9980, val_MinusLogProbMetric: 28.9980

Epoch 369: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9624 - MinusLogProbMetric: 27.9624 - val_loss: 28.9980 - val_MinusLogProbMetric: 28.9980 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 370/1000
2023-10-25 19:59:49.118 
Epoch 370/1000 
	 loss: 27.9265, MinusLogProbMetric: 27.9265, val_loss: 29.9686, val_MinusLogProbMetric: 29.9686

Epoch 370: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9265 - MinusLogProbMetric: 27.9265 - val_loss: 29.9686 - val_MinusLogProbMetric: 29.9686 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 371/1000
2023-10-25 20:00:24.104 
Epoch 371/1000 
	 loss: 27.9148, MinusLogProbMetric: 27.9148, val_loss: 28.9185, val_MinusLogProbMetric: 28.9185

Epoch 371: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9148 - MinusLogProbMetric: 27.9148 - val_loss: 28.9185 - val_MinusLogProbMetric: 28.9185 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 372/1000
2023-10-25 20:00:59.013 
Epoch 372/1000 
	 loss: 28.1718, MinusLogProbMetric: 28.1718, val_loss: 28.5284, val_MinusLogProbMetric: 28.5284

Epoch 372: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.1718 - MinusLogProbMetric: 28.1718 - val_loss: 28.5284 - val_MinusLogProbMetric: 28.5284 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 373/1000
2023-10-25 20:01:34.151 
Epoch 373/1000 
	 loss: 27.9495, MinusLogProbMetric: 27.9495, val_loss: 28.6041, val_MinusLogProbMetric: 28.6041

Epoch 373: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9495 - MinusLogProbMetric: 27.9495 - val_loss: 28.6041 - val_MinusLogProbMetric: 28.6041 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 374/1000
2023-10-25 20:02:09.326 
Epoch 374/1000 
	 loss: 28.0169, MinusLogProbMetric: 28.0169, val_loss: 29.4866, val_MinusLogProbMetric: 29.4866

Epoch 374: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0169 - MinusLogProbMetric: 28.0169 - val_loss: 29.4866 - val_MinusLogProbMetric: 29.4866 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 375/1000
2023-10-25 20:02:44.329 
Epoch 375/1000 
	 loss: 27.9777, MinusLogProbMetric: 27.9777, val_loss: 29.3258, val_MinusLogProbMetric: 29.3258

Epoch 375: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9777 - MinusLogProbMetric: 27.9777 - val_loss: 29.3258 - val_MinusLogProbMetric: 29.3258 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 376/1000
2023-10-25 20:03:19.370 
Epoch 376/1000 
	 loss: 27.9983, MinusLogProbMetric: 27.9983, val_loss: 28.5576, val_MinusLogProbMetric: 28.5576

Epoch 376: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9983 - MinusLogProbMetric: 27.9983 - val_loss: 28.5576 - val_MinusLogProbMetric: 28.5576 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 377/1000
2023-10-25 20:03:54.534 
Epoch 377/1000 
	 loss: 27.8749, MinusLogProbMetric: 27.8749, val_loss: 28.7209, val_MinusLogProbMetric: 28.7209

Epoch 377: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.8749 - MinusLogProbMetric: 27.8749 - val_loss: 28.7209 - val_MinusLogProbMetric: 28.7209 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 378/1000
2023-10-25 20:04:29.534 
Epoch 378/1000 
	 loss: 27.9511, MinusLogProbMetric: 27.9511, val_loss: 28.5189, val_MinusLogProbMetric: 28.5189

Epoch 378: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9511 - MinusLogProbMetric: 27.9511 - val_loss: 28.5189 - val_MinusLogProbMetric: 28.5189 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 379/1000
2023-10-25 20:05:04.360 
Epoch 379/1000 
	 loss: 28.0406, MinusLogProbMetric: 28.0406, val_loss: 28.6264, val_MinusLogProbMetric: 28.6264

Epoch 379: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0406 - MinusLogProbMetric: 28.0406 - val_loss: 28.6264 - val_MinusLogProbMetric: 28.6264 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 380/1000
2023-10-25 20:05:39.234 
Epoch 380/1000 
	 loss: 27.8640, MinusLogProbMetric: 27.8640, val_loss: 28.8053, val_MinusLogProbMetric: 28.8053

Epoch 380: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.8640 - MinusLogProbMetric: 27.8640 - val_loss: 28.8053 - val_MinusLogProbMetric: 28.8053 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 381/1000
2023-10-25 20:06:14.234 
Epoch 381/1000 
	 loss: 27.9209, MinusLogProbMetric: 27.9209, val_loss: 28.4724, val_MinusLogProbMetric: 28.4724

Epoch 381: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9209 - MinusLogProbMetric: 27.9209 - val_loss: 28.4724 - val_MinusLogProbMetric: 28.4724 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 382/1000
2023-10-25 20:06:49.164 
Epoch 382/1000 
	 loss: 27.9126, MinusLogProbMetric: 27.9126, val_loss: 28.7217, val_MinusLogProbMetric: 28.7217

Epoch 382: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9126 - MinusLogProbMetric: 27.9126 - val_loss: 28.7217 - val_MinusLogProbMetric: 28.7217 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 383/1000
2023-10-25 20:07:24.252 
Epoch 383/1000 
	 loss: 28.0563, MinusLogProbMetric: 28.0563, val_loss: 28.5731, val_MinusLogProbMetric: 28.5731

Epoch 383: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0563 - MinusLogProbMetric: 28.0563 - val_loss: 28.5731 - val_MinusLogProbMetric: 28.5731 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 384/1000
2023-10-25 20:07:59.420 
Epoch 384/1000 
	 loss: 28.0236, MinusLogProbMetric: 28.0236, val_loss: 28.6103, val_MinusLogProbMetric: 28.6103

Epoch 384: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0236 - MinusLogProbMetric: 28.0236 - val_loss: 28.6103 - val_MinusLogProbMetric: 28.6103 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 385/1000
2023-10-25 20:08:34.187 
Epoch 385/1000 
	 loss: 27.9048, MinusLogProbMetric: 27.9048, val_loss: 28.5150, val_MinusLogProbMetric: 28.5150

Epoch 385: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9048 - MinusLogProbMetric: 27.9048 - val_loss: 28.5150 - val_MinusLogProbMetric: 28.5150 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 386/1000
2023-10-25 20:09:09.025 
Epoch 386/1000 
	 loss: 27.8691, MinusLogProbMetric: 27.8691, val_loss: 28.6648, val_MinusLogProbMetric: 28.6648

Epoch 386: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.8691 - MinusLogProbMetric: 27.8691 - val_loss: 28.6648 - val_MinusLogProbMetric: 28.6648 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 387/1000
2023-10-25 20:09:43.753 
Epoch 387/1000 
	 loss: 27.9763, MinusLogProbMetric: 27.9763, val_loss: 28.8877, val_MinusLogProbMetric: 28.8877

Epoch 387: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9763 - MinusLogProbMetric: 27.9763 - val_loss: 28.8877 - val_MinusLogProbMetric: 28.8877 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 388/1000
2023-10-25 20:10:18.509 
Epoch 388/1000 
	 loss: 27.9397, MinusLogProbMetric: 27.9397, val_loss: 29.1720, val_MinusLogProbMetric: 29.1720

Epoch 388: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9397 - MinusLogProbMetric: 27.9397 - val_loss: 29.1720 - val_MinusLogProbMetric: 29.1720 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 389/1000
2023-10-25 20:10:53.785 
Epoch 389/1000 
	 loss: 27.9407, MinusLogProbMetric: 27.9407, val_loss: 28.4634, val_MinusLogProbMetric: 28.4634

Epoch 389: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9407 - MinusLogProbMetric: 27.9407 - val_loss: 28.4634 - val_MinusLogProbMetric: 28.4634 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 390/1000
2023-10-25 20:11:28.309 
Epoch 390/1000 
	 loss: 27.8874, MinusLogProbMetric: 27.8874, val_loss: 28.8484, val_MinusLogProbMetric: 28.8484

Epoch 390: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.8874 - MinusLogProbMetric: 27.8874 - val_loss: 28.8484 - val_MinusLogProbMetric: 28.8484 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 391/1000
2023-10-25 20:12:03.276 
Epoch 391/1000 
	 loss: 28.0243, MinusLogProbMetric: 28.0243, val_loss: 28.8314, val_MinusLogProbMetric: 28.8314

Epoch 391: val_loss did not improve from 28.44588
196/196 - 35s - loss: 28.0243 - MinusLogProbMetric: 28.0243 - val_loss: 28.8314 - val_MinusLogProbMetric: 28.8314 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 392/1000
2023-10-25 20:12:37.708 
Epoch 392/1000 
	 loss: 27.8846, MinusLogProbMetric: 27.8846, val_loss: 28.5506, val_MinusLogProbMetric: 28.5506

Epoch 392: val_loss did not improve from 28.44588
196/196 - 34s - loss: 27.8846 - MinusLogProbMetric: 27.8846 - val_loss: 28.5506 - val_MinusLogProbMetric: 28.5506 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 393/1000
2023-10-25 20:13:12.205 
Epoch 393/1000 
	 loss: 27.9230, MinusLogProbMetric: 27.9230, val_loss: 28.5841, val_MinusLogProbMetric: 28.5841

Epoch 393: val_loss did not improve from 28.44588
196/196 - 34s - loss: 27.9230 - MinusLogProbMetric: 27.9230 - val_loss: 28.5841 - val_MinusLogProbMetric: 28.5841 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 394/1000
2023-10-25 20:13:44.353 
Epoch 394/1000 
	 loss: 28.0136, MinusLogProbMetric: 28.0136, val_loss: 29.5214, val_MinusLogProbMetric: 29.5214

Epoch 394: val_loss did not improve from 28.44588
196/196 - 32s - loss: 28.0136 - MinusLogProbMetric: 28.0136 - val_loss: 29.5214 - val_MinusLogProbMetric: 29.5214 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 395/1000
2023-10-25 20:14:17.058 
Epoch 395/1000 
	 loss: 27.9587, MinusLogProbMetric: 27.9587, val_loss: 28.8268, val_MinusLogProbMetric: 28.8268

Epoch 395: val_loss did not improve from 28.44588
196/196 - 33s - loss: 27.9587 - MinusLogProbMetric: 27.9587 - val_loss: 28.8268 - val_MinusLogProbMetric: 28.8268 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 396/1000
2023-10-25 20:14:50.322 
Epoch 396/1000 
	 loss: 27.9172, MinusLogProbMetric: 27.9172, val_loss: 28.6209, val_MinusLogProbMetric: 28.6209

Epoch 396: val_loss did not improve from 28.44588
196/196 - 33s - loss: 27.9172 - MinusLogProbMetric: 27.9172 - val_loss: 28.6209 - val_MinusLogProbMetric: 28.6209 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 397/1000
2023-10-25 20:15:24.845 
Epoch 397/1000 
	 loss: 27.9767, MinusLogProbMetric: 27.9767, val_loss: 29.0424, val_MinusLogProbMetric: 29.0424

Epoch 397: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9767 - MinusLogProbMetric: 27.9767 - val_loss: 29.0424 - val_MinusLogProbMetric: 29.0424 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 398/1000
2023-10-25 20:15:59.755 
Epoch 398/1000 
	 loss: 27.9101, MinusLogProbMetric: 27.9101, val_loss: 30.6091, val_MinusLogProbMetric: 30.6091

Epoch 398: val_loss did not improve from 28.44588
196/196 - 35s - loss: 27.9101 - MinusLogProbMetric: 27.9101 - val_loss: 30.6091 - val_MinusLogProbMetric: 30.6091 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 399/1000
2023-10-25 20:16:34.535 
Epoch 399/1000 
	 loss: 27.5948, MinusLogProbMetric: 27.5948, val_loss: 28.3988, val_MinusLogProbMetric: 28.3988

Epoch 399: val_loss improved from 28.44588 to 28.39884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.5948 - MinusLogProbMetric: 27.5948 - val_loss: 28.3988 - val_MinusLogProbMetric: 28.3988 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 400/1000
2023-10-25 20:17:09.772 
Epoch 400/1000 
	 loss: 27.5595, MinusLogProbMetric: 27.5595, val_loss: 28.6087, val_MinusLogProbMetric: 28.6087

Epoch 400: val_loss did not improve from 28.39884
196/196 - 35s - loss: 27.5595 - MinusLogProbMetric: 27.5595 - val_loss: 28.6087 - val_MinusLogProbMetric: 28.6087 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 401/1000
2023-10-25 20:17:44.354 
Epoch 401/1000 
	 loss: 27.5433, MinusLogProbMetric: 27.5433, val_loss: 28.3580, val_MinusLogProbMetric: 28.3580

Epoch 401: val_loss improved from 28.39884 to 28.35796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.5433 - MinusLogProbMetric: 27.5433 - val_loss: 28.3580 - val_MinusLogProbMetric: 28.3580 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 402/1000
2023-10-25 20:18:18.395 
Epoch 402/1000 
	 loss: 27.5222, MinusLogProbMetric: 27.5222, val_loss: 28.5011, val_MinusLogProbMetric: 28.5011

Epoch 402: val_loss did not improve from 28.35796
196/196 - 34s - loss: 27.5222 - MinusLogProbMetric: 27.5222 - val_loss: 28.5011 - val_MinusLogProbMetric: 28.5011 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 403/1000
2023-10-25 20:18:53.045 
Epoch 403/1000 
	 loss: 27.5504, MinusLogProbMetric: 27.5504, val_loss: 28.3306, val_MinusLogProbMetric: 28.3306

Epoch 403: val_loss improved from 28.35796 to 28.33058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.5504 - MinusLogProbMetric: 27.5504 - val_loss: 28.3306 - val_MinusLogProbMetric: 28.3306 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 404/1000
2023-10-25 20:19:28.119 
Epoch 404/1000 
	 loss: 27.5102, MinusLogProbMetric: 27.5102, val_loss: 29.2283, val_MinusLogProbMetric: 29.2283

Epoch 404: val_loss did not improve from 28.33058
196/196 - 35s - loss: 27.5102 - MinusLogProbMetric: 27.5102 - val_loss: 29.2283 - val_MinusLogProbMetric: 29.2283 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 405/1000
2023-10-25 20:20:02.824 
Epoch 405/1000 
	 loss: 27.5451, MinusLogProbMetric: 27.5451, val_loss: 28.3740, val_MinusLogProbMetric: 28.3740

Epoch 405: val_loss did not improve from 28.33058
196/196 - 35s - loss: 27.5451 - MinusLogProbMetric: 27.5451 - val_loss: 28.3740 - val_MinusLogProbMetric: 28.3740 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 406/1000
2023-10-25 20:20:37.360 
Epoch 406/1000 
	 loss: 27.4978, MinusLogProbMetric: 27.4978, val_loss: 28.2471, val_MinusLogProbMetric: 28.2471

Epoch 406: val_loss improved from 28.33058 to 28.24706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.4978 - MinusLogProbMetric: 27.4978 - val_loss: 28.2471 - val_MinusLogProbMetric: 28.2471 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 407/1000
2023-10-25 20:21:12.804 
Epoch 407/1000 
	 loss: 27.5965, MinusLogProbMetric: 27.5965, val_loss: 28.5005, val_MinusLogProbMetric: 28.5005

Epoch 407: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5965 - MinusLogProbMetric: 27.5965 - val_loss: 28.5005 - val_MinusLogProbMetric: 28.5005 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 408/1000
2023-10-25 20:21:47.739 
Epoch 408/1000 
	 loss: 27.5404, MinusLogProbMetric: 27.5404, val_loss: 28.6235, val_MinusLogProbMetric: 28.6235

Epoch 408: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5404 - MinusLogProbMetric: 27.5404 - val_loss: 28.6235 - val_MinusLogProbMetric: 28.6235 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 409/1000
2023-10-25 20:22:22.237 
Epoch 409/1000 
	 loss: 27.5200, MinusLogProbMetric: 27.5200, val_loss: 28.4540, val_MinusLogProbMetric: 28.4540

Epoch 409: val_loss did not improve from 28.24706
196/196 - 34s - loss: 27.5200 - MinusLogProbMetric: 27.5200 - val_loss: 28.4540 - val_MinusLogProbMetric: 28.4540 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 410/1000
2023-10-25 20:22:57.103 
Epoch 410/1000 
	 loss: 27.5708, MinusLogProbMetric: 27.5708, val_loss: 28.2880, val_MinusLogProbMetric: 28.2880

Epoch 410: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5708 - MinusLogProbMetric: 27.5708 - val_loss: 28.2880 - val_MinusLogProbMetric: 28.2880 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 411/1000
2023-10-25 20:23:31.947 
Epoch 411/1000 
	 loss: 27.5456, MinusLogProbMetric: 27.5456, val_loss: 28.2609, val_MinusLogProbMetric: 28.2609

Epoch 411: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5456 - MinusLogProbMetric: 27.5456 - val_loss: 28.2609 - val_MinusLogProbMetric: 28.2609 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 412/1000
2023-10-25 20:24:06.559 
Epoch 412/1000 
	 loss: 27.5712, MinusLogProbMetric: 27.5712, val_loss: 28.3237, val_MinusLogProbMetric: 28.3237

Epoch 412: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5712 - MinusLogProbMetric: 27.5712 - val_loss: 28.3237 - val_MinusLogProbMetric: 28.3237 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 413/1000
2023-10-25 20:24:41.286 
Epoch 413/1000 
	 loss: 27.5383, MinusLogProbMetric: 27.5383, val_loss: 28.2789, val_MinusLogProbMetric: 28.2789

Epoch 413: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5383 - MinusLogProbMetric: 27.5383 - val_loss: 28.2789 - val_MinusLogProbMetric: 28.2789 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 414/1000
2023-10-25 20:25:16.282 
Epoch 414/1000 
	 loss: 27.6198, MinusLogProbMetric: 27.6198, val_loss: 28.3030, val_MinusLogProbMetric: 28.3030

Epoch 414: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.6198 - MinusLogProbMetric: 27.6198 - val_loss: 28.3030 - val_MinusLogProbMetric: 28.3030 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 415/1000
2023-10-25 20:25:51.021 
Epoch 415/1000 
	 loss: 27.4696, MinusLogProbMetric: 27.4696, val_loss: 28.2905, val_MinusLogProbMetric: 28.2905

Epoch 415: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4696 - MinusLogProbMetric: 27.4696 - val_loss: 28.2905 - val_MinusLogProbMetric: 28.2905 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 416/1000
2023-10-25 20:26:26.066 
Epoch 416/1000 
	 loss: 27.5209, MinusLogProbMetric: 27.5209, val_loss: 28.3038, val_MinusLogProbMetric: 28.3038

Epoch 416: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5209 - MinusLogProbMetric: 27.5209 - val_loss: 28.3038 - val_MinusLogProbMetric: 28.3038 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 417/1000
2023-10-25 20:27:00.978 
Epoch 417/1000 
	 loss: 27.5591, MinusLogProbMetric: 27.5591, val_loss: 28.2915, val_MinusLogProbMetric: 28.2915

Epoch 417: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5591 - MinusLogProbMetric: 27.5591 - val_loss: 28.2915 - val_MinusLogProbMetric: 28.2915 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 418/1000
2023-10-25 20:27:35.918 
Epoch 418/1000 
	 loss: 27.4788, MinusLogProbMetric: 27.4788, val_loss: 28.3307, val_MinusLogProbMetric: 28.3307

Epoch 418: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4788 - MinusLogProbMetric: 27.4788 - val_loss: 28.3307 - val_MinusLogProbMetric: 28.3307 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 419/1000
2023-10-25 20:28:10.905 
Epoch 419/1000 
	 loss: 27.6048, MinusLogProbMetric: 27.6048, val_loss: 28.3969, val_MinusLogProbMetric: 28.3969

Epoch 419: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.6048 - MinusLogProbMetric: 27.6048 - val_loss: 28.3969 - val_MinusLogProbMetric: 28.3969 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 420/1000
2023-10-25 20:28:45.798 
Epoch 420/1000 
	 loss: 27.5767, MinusLogProbMetric: 27.5767, val_loss: 28.3824, val_MinusLogProbMetric: 28.3824

Epoch 420: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5767 - MinusLogProbMetric: 27.5767 - val_loss: 28.3824 - val_MinusLogProbMetric: 28.3824 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 421/1000
2023-10-25 20:29:20.852 
Epoch 421/1000 
	 loss: 27.5151, MinusLogProbMetric: 27.5151, val_loss: 28.3439, val_MinusLogProbMetric: 28.3439

Epoch 421: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5151 - MinusLogProbMetric: 27.5151 - val_loss: 28.3439 - val_MinusLogProbMetric: 28.3439 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 422/1000
2023-10-25 20:29:55.531 
Epoch 422/1000 
	 loss: 27.4997, MinusLogProbMetric: 27.4997, val_loss: 28.2782, val_MinusLogProbMetric: 28.2782

Epoch 422: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4997 - MinusLogProbMetric: 27.4997 - val_loss: 28.2782 - val_MinusLogProbMetric: 28.2782 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 423/1000
2023-10-25 20:30:30.601 
Epoch 423/1000 
	 loss: 27.6189, MinusLogProbMetric: 27.6189, val_loss: 28.2946, val_MinusLogProbMetric: 28.2946

Epoch 423: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.6189 - MinusLogProbMetric: 27.6189 - val_loss: 28.2946 - val_MinusLogProbMetric: 28.2946 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 424/1000
2023-10-25 20:31:05.570 
Epoch 424/1000 
	 loss: 27.4938, MinusLogProbMetric: 27.4938, val_loss: 28.2812, val_MinusLogProbMetric: 28.2812

Epoch 424: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4938 - MinusLogProbMetric: 27.4938 - val_loss: 28.2812 - val_MinusLogProbMetric: 28.2812 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 425/1000
2023-10-25 20:31:40.381 
Epoch 425/1000 
	 loss: 27.5157, MinusLogProbMetric: 27.5157, val_loss: 28.3522, val_MinusLogProbMetric: 28.3522

Epoch 425: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5157 - MinusLogProbMetric: 27.5157 - val_loss: 28.3522 - val_MinusLogProbMetric: 28.3522 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 426/1000
2023-10-25 20:32:15.530 
Epoch 426/1000 
	 loss: 27.5458, MinusLogProbMetric: 27.5458, val_loss: 28.3449, val_MinusLogProbMetric: 28.3449

Epoch 426: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5458 - MinusLogProbMetric: 27.5458 - val_loss: 28.3449 - val_MinusLogProbMetric: 28.3449 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 427/1000
2023-10-25 20:32:50.656 
Epoch 427/1000 
	 loss: 27.4987, MinusLogProbMetric: 27.4987, val_loss: 28.4312, val_MinusLogProbMetric: 28.4312

Epoch 427: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4987 - MinusLogProbMetric: 27.4987 - val_loss: 28.4312 - val_MinusLogProbMetric: 28.4312 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 428/1000
2023-10-25 20:33:25.422 
Epoch 428/1000 
	 loss: 27.5391, MinusLogProbMetric: 27.5391, val_loss: 28.2683, val_MinusLogProbMetric: 28.2683

Epoch 428: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5391 - MinusLogProbMetric: 27.5391 - val_loss: 28.2683 - val_MinusLogProbMetric: 28.2683 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 429/1000
2023-10-25 20:34:00.443 
Epoch 429/1000 
	 loss: 27.5383, MinusLogProbMetric: 27.5383, val_loss: 28.2805, val_MinusLogProbMetric: 28.2805

Epoch 429: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5383 - MinusLogProbMetric: 27.5383 - val_loss: 28.2805 - val_MinusLogProbMetric: 28.2805 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 430/1000
2023-10-25 20:34:35.050 
Epoch 430/1000 
	 loss: 27.5754, MinusLogProbMetric: 27.5754, val_loss: 28.3046, val_MinusLogProbMetric: 28.3046

Epoch 430: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5754 - MinusLogProbMetric: 27.5754 - val_loss: 28.3046 - val_MinusLogProbMetric: 28.3046 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 431/1000
2023-10-25 20:35:10.063 
Epoch 431/1000 
	 loss: 27.4860, MinusLogProbMetric: 27.4860, val_loss: 28.3230, val_MinusLogProbMetric: 28.3230

Epoch 431: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4860 - MinusLogProbMetric: 27.4860 - val_loss: 28.3230 - val_MinusLogProbMetric: 28.3230 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 432/1000
2023-10-25 20:35:45.019 
Epoch 432/1000 
	 loss: 27.5646, MinusLogProbMetric: 27.5646, val_loss: 28.4343, val_MinusLogProbMetric: 28.4343

Epoch 432: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5646 - MinusLogProbMetric: 27.5646 - val_loss: 28.4343 - val_MinusLogProbMetric: 28.4343 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 433/1000
2023-10-25 20:36:20.221 
Epoch 433/1000 
	 loss: 27.7616, MinusLogProbMetric: 27.7616, val_loss: 28.2729, val_MinusLogProbMetric: 28.2729

Epoch 433: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.7616 - MinusLogProbMetric: 27.7616 - val_loss: 28.2729 - val_MinusLogProbMetric: 28.2729 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 434/1000
2023-10-25 20:36:55.241 
Epoch 434/1000 
	 loss: 27.5062, MinusLogProbMetric: 27.5062, val_loss: 28.4892, val_MinusLogProbMetric: 28.4892

Epoch 434: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5062 - MinusLogProbMetric: 27.5062 - val_loss: 28.4892 - val_MinusLogProbMetric: 28.4892 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 435/1000
2023-10-25 20:37:30.445 
Epoch 435/1000 
	 loss: 27.5583, MinusLogProbMetric: 27.5583, val_loss: 28.3123, val_MinusLogProbMetric: 28.3123

Epoch 435: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5583 - MinusLogProbMetric: 27.5583 - val_loss: 28.3123 - val_MinusLogProbMetric: 28.3123 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 436/1000
2023-10-25 20:38:05.582 
Epoch 436/1000 
	 loss: 27.4660, MinusLogProbMetric: 27.4660, val_loss: 28.3239, val_MinusLogProbMetric: 28.3239

Epoch 436: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4660 - MinusLogProbMetric: 27.4660 - val_loss: 28.3239 - val_MinusLogProbMetric: 28.3239 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 437/1000
2023-10-25 20:38:40.752 
Epoch 437/1000 
	 loss: 27.5136, MinusLogProbMetric: 27.5136, val_loss: 28.4990, val_MinusLogProbMetric: 28.4990

Epoch 437: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5136 - MinusLogProbMetric: 27.5136 - val_loss: 28.4990 - val_MinusLogProbMetric: 28.4990 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 438/1000
2023-10-25 20:39:15.669 
Epoch 438/1000 
	 loss: 27.5187, MinusLogProbMetric: 27.5187, val_loss: 28.6007, val_MinusLogProbMetric: 28.6007

Epoch 438: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5187 - MinusLogProbMetric: 27.5187 - val_loss: 28.6007 - val_MinusLogProbMetric: 28.6007 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 439/1000
2023-10-25 20:39:50.642 
Epoch 439/1000 
	 loss: 27.5454, MinusLogProbMetric: 27.5454, val_loss: 28.4195, val_MinusLogProbMetric: 28.4195

Epoch 439: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5454 - MinusLogProbMetric: 27.5454 - val_loss: 28.4195 - val_MinusLogProbMetric: 28.4195 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 440/1000
2023-10-25 20:40:25.571 
Epoch 440/1000 
	 loss: 27.4942, MinusLogProbMetric: 27.4942, val_loss: 28.3700, val_MinusLogProbMetric: 28.3700

Epoch 440: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4942 - MinusLogProbMetric: 27.4942 - val_loss: 28.3700 - val_MinusLogProbMetric: 28.3700 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 441/1000
2023-10-25 20:41:00.594 
Epoch 441/1000 
	 loss: 27.5758, MinusLogProbMetric: 27.5758, val_loss: 28.4459, val_MinusLogProbMetric: 28.4459

Epoch 441: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5758 - MinusLogProbMetric: 27.5758 - val_loss: 28.4459 - val_MinusLogProbMetric: 28.4459 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 442/1000
2023-10-25 20:41:35.248 
Epoch 442/1000 
	 loss: 27.4918, MinusLogProbMetric: 27.4918, val_loss: 28.4446, val_MinusLogProbMetric: 28.4446

Epoch 442: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4918 - MinusLogProbMetric: 27.4918 - val_loss: 28.4446 - val_MinusLogProbMetric: 28.4446 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 443/1000
2023-10-25 20:42:07.119 
Epoch 443/1000 
	 loss: 27.5560, MinusLogProbMetric: 27.5560, val_loss: 28.3540, val_MinusLogProbMetric: 28.3540

Epoch 443: val_loss did not improve from 28.24706
196/196 - 32s - loss: 27.5560 - MinusLogProbMetric: 27.5560 - val_loss: 28.3540 - val_MinusLogProbMetric: 28.3540 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 444/1000
2023-10-25 20:42:41.689 
Epoch 444/1000 
	 loss: 27.4956, MinusLogProbMetric: 27.4956, val_loss: 28.4637, val_MinusLogProbMetric: 28.4637

Epoch 444: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4956 - MinusLogProbMetric: 27.4956 - val_loss: 28.4637 - val_MinusLogProbMetric: 28.4637 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 445/1000
2023-10-25 20:43:14.244 
Epoch 445/1000 
	 loss: 27.5439, MinusLogProbMetric: 27.5439, val_loss: 28.3344, val_MinusLogProbMetric: 28.3344

Epoch 445: val_loss did not improve from 28.24706
196/196 - 33s - loss: 27.5439 - MinusLogProbMetric: 27.5439 - val_loss: 28.3344 - val_MinusLogProbMetric: 28.3344 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 446/1000
2023-10-25 20:43:48.557 
Epoch 446/1000 
	 loss: 27.5007, MinusLogProbMetric: 27.5007, val_loss: 28.2750, val_MinusLogProbMetric: 28.2750

Epoch 446: val_loss did not improve from 28.24706
196/196 - 34s - loss: 27.5007 - MinusLogProbMetric: 27.5007 - val_loss: 28.2750 - val_MinusLogProbMetric: 28.2750 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 447/1000
2023-10-25 20:44:21.882 
Epoch 447/1000 
	 loss: 27.5567, MinusLogProbMetric: 27.5567, val_loss: 28.3647, val_MinusLogProbMetric: 28.3647

Epoch 447: val_loss did not improve from 28.24706
196/196 - 33s - loss: 27.5567 - MinusLogProbMetric: 27.5567 - val_loss: 28.3647 - val_MinusLogProbMetric: 28.3647 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 448/1000
2023-10-25 20:44:55.066 
Epoch 448/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 28.6434, val_MinusLogProbMetric: 28.6434

Epoch 448: val_loss did not improve from 28.24706
196/196 - 33s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 28.6434 - val_MinusLogProbMetric: 28.6434 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 449/1000
2023-10-25 20:45:29.718 
Epoch 449/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 28.5443, val_MinusLogProbMetric: 28.5443

Epoch 449: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 28.5443 - val_MinusLogProbMetric: 28.5443 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 450/1000
2023-10-25 20:46:04.344 
Epoch 450/1000 
	 loss: 27.5308, MinusLogProbMetric: 27.5308, val_loss: 28.6236, val_MinusLogProbMetric: 28.6236

Epoch 450: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5308 - MinusLogProbMetric: 27.5308 - val_loss: 28.6236 - val_MinusLogProbMetric: 28.6236 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 451/1000
2023-10-25 20:46:37.075 
Epoch 451/1000 
	 loss: 27.5179, MinusLogProbMetric: 27.5179, val_loss: 28.4843, val_MinusLogProbMetric: 28.4843

Epoch 451: val_loss did not improve from 28.24706
196/196 - 33s - loss: 27.5179 - MinusLogProbMetric: 27.5179 - val_loss: 28.4843 - val_MinusLogProbMetric: 28.4843 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 452/1000
2023-10-25 20:47:11.666 
Epoch 452/1000 
	 loss: 27.4741, MinusLogProbMetric: 27.4741, val_loss: 28.3242, val_MinusLogProbMetric: 28.3242

Epoch 452: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4741 - MinusLogProbMetric: 27.4741 - val_loss: 28.3242 - val_MinusLogProbMetric: 28.3242 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 453/1000
2023-10-25 20:47:46.580 
Epoch 453/1000 
	 loss: 27.5587, MinusLogProbMetric: 27.5587, val_loss: 28.3684, val_MinusLogProbMetric: 28.3684

Epoch 453: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5587 - MinusLogProbMetric: 27.5587 - val_loss: 28.3684 - val_MinusLogProbMetric: 28.3684 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 454/1000
2023-10-25 20:48:21.277 
Epoch 454/1000 
	 loss: 27.4953, MinusLogProbMetric: 27.4953, val_loss: 28.3457, val_MinusLogProbMetric: 28.3457

Epoch 454: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4953 - MinusLogProbMetric: 27.4953 - val_loss: 28.3457 - val_MinusLogProbMetric: 28.3457 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 455/1000
2023-10-25 20:48:56.283 
Epoch 455/1000 
	 loss: 27.5184, MinusLogProbMetric: 27.5184, val_loss: 28.3132, val_MinusLogProbMetric: 28.3132

Epoch 455: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.5184 - MinusLogProbMetric: 27.5184 - val_loss: 28.3132 - val_MinusLogProbMetric: 28.3132 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 456/1000
2023-10-25 20:49:31.443 
Epoch 456/1000 
	 loss: 27.4860, MinusLogProbMetric: 27.4860, val_loss: 28.2648, val_MinusLogProbMetric: 28.2648

Epoch 456: val_loss did not improve from 28.24706
196/196 - 35s - loss: 27.4860 - MinusLogProbMetric: 27.4860 - val_loss: 28.2648 - val_MinusLogProbMetric: 28.2648 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 457/1000
2023-10-25 20:50:06.398 
Epoch 457/1000 
	 loss: 27.3212, MinusLogProbMetric: 27.3212, val_loss: 28.2350, val_MinusLogProbMetric: 28.2350

Epoch 457: val_loss improved from 28.24706 to 28.23495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 27.3212 - MinusLogProbMetric: 27.3212 - val_loss: 28.2350 - val_MinusLogProbMetric: 28.2350 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 458/1000
2023-10-25 20:50:41.673 
Epoch 458/1000 
	 loss: 27.3208, MinusLogProbMetric: 27.3208, val_loss: 28.2692, val_MinusLogProbMetric: 28.2692

Epoch 458: val_loss did not improve from 28.23495
196/196 - 35s - loss: 27.3208 - MinusLogProbMetric: 27.3208 - val_loss: 28.2692 - val_MinusLogProbMetric: 28.2692 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 459/1000
2023-10-25 20:51:16.653 
Epoch 459/1000 
	 loss: 27.3181, MinusLogProbMetric: 27.3181, val_loss: 28.2050, val_MinusLogProbMetric: 28.2050

Epoch 459: val_loss improved from 28.23495 to 28.20498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 27.3181 - MinusLogProbMetric: 27.3181 - val_loss: 28.2050 - val_MinusLogProbMetric: 28.2050 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 460/1000
2023-10-25 20:51:52.212 
Epoch 460/1000 
	 loss: 27.3137, MinusLogProbMetric: 27.3137, val_loss: 28.3608, val_MinusLogProbMetric: 28.3608

Epoch 460: val_loss did not improve from 28.20498
196/196 - 35s - loss: 27.3137 - MinusLogProbMetric: 27.3137 - val_loss: 28.3608 - val_MinusLogProbMetric: 28.3608 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 461/1000
2023-10-25 20:52:27.050 
Epoch 461/1000 
	 loss: 27.3344, MinusLogProbMetric: 27.3344, val_loss: 28.2204, val_MinusLogProbMetric: 28.2204

Epoch 461: val_loss did not improve from 28.20498
196/196 - 35s - loss: 27.3344 - MinusLogProbMetric: 27.3344 - val_loss: 28.2204 - val_MinusLogProbMetric: 28.2204 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 462/1000
2023-10-25 20:53:01.980 
Epoch 462/1000 
	 loss: 27.3143, MinusLogProbMetric: 27.3143, val_loss: 28.2030, val_MinusLogProbMetric: 28.2030

Epoch 462: val_loss improved from 28.20498 to 28.20295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.3143 - MinusLogProbMetric: 27.3143 - val_loss: 28.2030 - val_MinusLogProbMetric: 28.2030 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 463/1000
2023-10-25 20:53:37.985 
Epoch 463/1000 
	 loss: 27.3270, MinusLogProbMetric: 27.3270, val_loss: 28.2132, val_MinusLogProbMetric: 28.2132

Epoch 463: val_loss did not improve from 28.20295
196/196 - 35s - loss: 27.3270 - MinusLogProbMetric: 27.3270 - val_loss: 28.2132 - val_MinusLogProbMetric: 28.2132 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 464/1000
2023-10-25 20:54:13.261 
Epoch 464/1000 
	 loss: 27.3236, MinusLogProbMetric: 27.3236, val_loss: 28.2163, val_MinusLogProbMetric: 28.2163

Epoch 464: val_loss did not improve from 28.20295
196/196 - 35s - loss: 27.3236 - MinusLogProbMetric: 27.3236 - val_loss: 28.2163 - val_MinusLogProbMetric: 28.2163 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 465/1000
2023-10-25 20:54:48.502 
Epoch 465/1000 
	 loss: 27.3236, MinusLogProbMetric: 27.3236, val_loss: 28.1919, val_MinusLogProbMetric: 28.1919

Epoch 465: val_loss improved from 28.20295 to 28.19193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 27.3236 - MinusLogProbMetric: 27.3236 - val_loss: 28.1919 - val_MinusLogProbMetric: 28.1919 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 466/1000
2023-10-25 20:55:23.726 
Epoch 466/1000 
	 loss: 27.3287, MinusLogProbMetric: 27.3287, val_loss: 28.2139, val_MinusLogProbMetric: 28.2139

Epoch 466: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3287 - MinusLogProbMetric: 27.3287 - val_loss: 28.2139 - val_MinusLogProbMetric: 28.2139 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 467/1000
2023-10-25 20:55:58.633 
Epoch 467/1000 
	 loss: 27.3199, MinusLogProbMetric: 27.3199, val_loss: 28.2109, val_MinusLogProbMetric: 28.2109

Epoch 467: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3199 - MinusLogProbMetric: 27.3199 - val_loss: 28.2109 - val_MinusLogProbMetric: 28.2109 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 468/1000
2023-10-25 20:56:33.402 
Epoch 468/1000 
	 loss: 27.3263, MinusLogProbMetric: 27.3263, val_loss: 28.2417, val_MinusLogProbMetric: 28.2417

Epoch 468: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3263 - MinusLogProbMetric: 27.3263 - val_loss: 28.2417 - val_MinusLogProbMetric: 28.2417 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 469/1000
2023-10-25 20:57:08.396 
Epoch 469/1000 
	 loss: 27.3579, MinusLogProbMetric: 27.3579, val_loss: 28.2641, val_MinusLogProbMetric: 28.2641

Epoch 469: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3579 - MinusLogProbMetric: 27.3579 - val_loss: 28.2641 - val_MinusLogProbMetric: 28.2641 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 470/1000
2023-10-25 20:57:43.824 
Epoch 470/1000 
	 loss: 27.3309, MinusLogProbMetric: 27.3309, val_loss: 28.2747, val_MinusLogProbMetric: 28.2747

Epoch 470: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3309 - MinusLogProbMetric: 27.3309 - val_loss: 28.2747 - val_MinusLogProbMetric: 28.2747 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 471/1000
2023-10-25 20:58:18.898 
Epoch 471/1000 
	 loss: 27.3107, MinusLogProbMetric: 27.3107, val_loss: 28.4588, val_MinusLogProbMetric: 28.4588

Epoch 471: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3107 - MinusLogProbMetric: 27.3107 - val_loss: 28.4588 - val_MinusLogProbMetric: 28.4588 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 472/1000
2023-10-25 20:58:53.810 
Epoch 472/1000 
	 loss: 27.3222, MinusLogProbMetric: 27.3222, val_loss: 28.1944, val_MinusLogProbMetric: 28.1944

Epoch 472: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3222 - MinusLogProbMetric: 27.3222 - val_loss: 28.1944 - val_MinusLogProbMetric: 28.1944 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 473/1000
2023-10-25 20:59:28.808 
Epoch 473/1000 
	 loss: 27.3311, MinusLogProbMetric: 27.3311, val_loss: 28.1950, val_MinusLogProbMetric: 28.1950

Epoch 473: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3311 - MinusLogProbMetric: 27.3311 - val_loss: 28.1950 - val_MinusLogProbMetric: 28.1950 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 474/1000
2023-10-25 21:00:03.778 
Epoch 474/1000 
	 loss: 27.3125, MinusLogProbMetric: 27.3125, val_loss: 28.2625, val_MinusLogProbMetric: 28.2625

Epoch 474: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3125 - MinusLogProbMetric: 27.3125 - val_loss: 28.2625 - val_MinusLogProbMetric: 28.2625 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 475/1000
2023-10-25 21:00:38.902 
Epoch 475/1000 
	 loss: 27.3408, MinusLogProbMetric: 27.3408, val_loss: 28.2098, val_MinusLogProbMetric: 28.2098

Epoch 475: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3408 - MinusLogProbMetric: 27.3408 - val_loss: 28.2098 - val_MinusLogProbMetric: 28.2098 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 476/1000
2023-10-25 21:01:13.911 
Epoch 476/1000 
	 loss: 27.3244, MinusLogProbMetric: 27.3244, val_loss: 28.2820, val_MinusLogProbMetric: 28.2820

Epoch 476: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3244 - MinusLogProbMetric: 27.3244 - val_loss: 28.2820 - val_MinusLogProbMetric: 28.2820 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 477/1000
2023-10-25 21:01:48.787 
Epoch 477/1000 
	 loss: 27.3406, MinusLogProbMetric: 27.3406, val_loss: 28.2253, val_MinusLogProbMetric: 28.2253

Epoch 477: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3406 - MinusLogProbMetric: 27.3406 - val_loss: 28.2253 - val_MinusLogProbMetric: 28.2253 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 478/1000
2023-10-25 21:02:23.975 
Epoch 478/1000 
	 loss: 27.3309, MinusLogProbMetric: 27.3309, val_loss: 28.2632, val_MinusLogProbMetric: 28.2632

Epoch 478: val_loss did not improve from 28.19193
196/196 - 35s - loss: 27.3309 - MinusLogProbMetric: 27.3309 - val_loss: 28.2632 - val_MinusLogProbMetric: 28.2632 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 479/1000
2023-10-25 21:02:57.525 
Epoch 479/1000 
	 loss: 27.3235, MinusLogProbMetric: 27.3235, val_loss: 28.1999, val_MinusLogProbMetric: 28.1999

Epoch 479: val_loss did not improve from 28.19193
196/196 - 34s - loss: 27.3235 - MinusLogProbMetric: 27.3235 - val_loss: 28.1999 - val_MinusLogProbMetric: 28.1999 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 480/1000
2023-10-25 21:03:31.239 
Epoch 480/1000 
	 loss: 27.3272, MinusLogProbMetric: 27.3272, val_loss: 28.2069, val_MinusLogProbMetric: 28.2069

Epoch 480: val_loss did not improve from 28.19193
196/196 - 34s - loss: 27.3272 - MinusLogProbMetric: 27.3272 - val_loss: 28.2069 - val_MinusLogProbMetric: 28.2069 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 481/1000
2023-10-25 21:04:05.499 
Epoch 481/1000 
	 loss: 27.3150, MinusLogProbMetric: 27.3150, val_loss: 28.2943, val_MinusLogProbMetric: 28.2943

Epoch 481: val_loss did not improve from 28.19193
196/196 - 34s - loss: 27.3150 - MinusLogProbMetric: 27.3150 - val_loss: 28.2943 - val_MinusLogProbMetric: 28.2943 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 482/1000
2023-10-25 21:04:40.202 
Epoch 482/1000 
	 loss: 27.3236, MinusLogProbMetric: 27.3236, val_loss: 28.1902, val_MinusLogProbMetric: 28.1902

Epoch 482: val_loss improved from 28.19193 to 28.19024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.3236 - MinusLogProbMetric: 27.3236 - val_loss: 28.1902 - val_MinusLogProbMetric: 28.1902 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 483/1000
2023-10-25 21:05:14.619 
Epoch 483/1000 
	 loss: 27.3275, MinusLogProbMetric: 27.3275, val_loss: 28.2975, val_MinusLogProbMetric: 28.2975

Epoch 483: val_loss did not improve from 28.19024
196/196 - 34s - loss: 27.3275 - MinusLogProbMetric: 27.3275 - val_loss: 28.2975 - val_MinusLogProbMetric: 28.2975 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 484/1000
2023-10-25 21:05:48.623 
Epoch 484/1000 
	 loss: 27.3101, MinusLogProbMetric: 27.3101, val_loss: 28.2104, val_MinusLogProbMetric: 28.2104

Epoch 484: val_loss did not improve from 28.19024
196/196 - 34s - loss: 27.3101 - MinusLogProbMetric: 27.3101 - val_loss: 28.2104 - val_MinusLogProbMetric: 28.2104 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 485/1000
2023-10-25 21:06:23.328 
Epoch 485/1000 
	 loss: 27.3331, MinusLogProbMetric: 27.3331, val_loss: 28.1857, val_MinusLogProbMetric: 28.1857

Epoch 485: val_loss improved from 28.19024 to 28.18567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.3331 - MinusLogProbMetric: 27.3331 - val_loss: 28.1857 - val_MinusLogProbMetric: 28.1857 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 486/1000
2023-10-25 21:06:58.688 
Epoch 486/1000 
	 loss: 27.3409, MinusLogProbMetric: 27.3409, val_loss: 28.2428, val_MinusLogProbMetric: 28.2428

Epoch 486: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3409 - MinusLogProbMetric: 27.3409 - val_loss: 28.2428 - val_MinusLogProbMetric: 28.2428 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 487/1000
2023-10-25 21:07:33.383 
Epoch 487/1000 
	 loss: 27.3272, MinusLogProbMetric: 27.3272, val_loss: 28.2487, val_MinusLogProbMetric: 28.2487

Epoch 487: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3272 - MinusLogProbMetric: 27.3272 - val_loss: 28.2487 - val_MinusLogProbMetric: 28.2487 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 488/1000
2023-10-25 21:08:08.149 
Epoch 488/1000 
	 loss: 27.3128, MinusLogProbMetric: 27.3128, val_loss: 28.3391, val_MinusLogProbMetric: 28.3391

Epoch 488: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3128 - MinusLogProbMetric: 27.3128 - val_loss: 28.3391 - val_MinusLogProbMetric: 28.3391 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 489/1000
2023-10-25 21:08:43.451 
Epoch 489/1000 
	 loss: 27.3224, MinusLogProbMetric: 27.3224, val_loss: 28.2166, val_MinusLogProbMetric: 28.2166

Epoch 489: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3224 - MinusLogProbMetric: 27.3224 - val_loss: 28.2166 - val_MinusLogProbMetric: 28.2166 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 490/1000
2023-10-25 21:09:18.451 
Epoch 490/1000 
	 loss: 27.3249, MinusLogProbMetric: 27.3249, val_loss: 28.2620, val_MinusLogProbMetric: 28.2620

Epoch 490: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3249 - MinusLogProbMetric: 27.3249 - val_loss: 28.2620 - val_MinusLogProbMetric: 28.2620 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 491/1000
2023-10-25 21:09:52.889 
Epoch 491/1000 
	 loss: 27.3147, MinusLogProbMetric: 27.3147, val_loss: 28.2197, val_MinusLogProbMetric: 28.2197

Epoch 491: val_loss did not improve from 28.18567
196/196 - 34s - loss: 27.3147 - MinusLogProbMetric: 27.3147 - val_loss: 28.2197 - val_MinusLogProbMetric: 28.2197 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 492/1000
2023-10-25 21:10:27.636 
Epoch 492/1000 
	 loss: 27.3046, MinusLogProbMetric: 27.3046, val_loss: 28.2079, val_MinusLogProbMetric: 28.2079

Epoch 492: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3046 - MinusLogProbMetric: 27.3046 - val_loss: 28.2079 - val_MinusLogProbMetric: 28.2079 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 493/1000
2023-10-25 21:11:02.416 
Epoch 493/1000 
	 loss: 27.3240, MinusLogProbMetric: 27.3240, val_loss: 28.1926, val_MinusLogProbMetric: 28.1926

Epoch 493: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3240 - MinusLogProbMetric: 27.3240 - val_loss: 28.1926 - val_MinusLogProbMetric: 28.1926 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 494/1000
2023-10-25 21:11:37.487 
Epoch 494/1000 
	 loss: 27.3043, MinusLogProbMetric: 27.3043, val_loss: 28.2212, val_MinusLogProbMetric: 28.2212

Epoch 494: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3043 - MinusLogProbMetric: 27.3043 - val_loss: 28.2212 - val_MinusLogProbMetric: 28.2212 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 495/1000
2023-10-25 21:12:12.131 
Epoch 495/1000 
	 loss: 27.3515, MinusLogProbMetric: 27.3515, val_loss: 28.1988, val_MinusLogProbMetric: 28.1988

Epoch 495: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3515 - MinusLogProbMetric: 27.3515 - val_loss: 28.1988 - val_MinusLogProbMetric: 28.1988 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 496/1000
2023-10-25 21:12:46.985 
Epoch 496/1000 
	 loss: 27.3212, MinusLogProbMetric: 27.3212, val_loss: 28.1923, val_MinusLogProbMetric: 28.1923

Epoch 496: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3212 - MinusLogProbMetric: 27.3212 - val_loss: 28.1923 - val_MinusLogProbMetric: 28.1923 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 497/1000
2023-10-25 21:13:21.947 
Epoch 497/1000 
	 loss: 27.3104, MinusLogProbMetric: 27.3104, val_loss: 28.2290, val_MinusLogProbMetric: 28.2290

Epoch 497: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3104 - MinusLogProbMetric: 27.3104 - val_loss: 28.2290 - val_MinusLogProbMetric: 28.2290 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 498/1000
2023-10-25 21:13:56.845 
Epoch 498/1000 
	 loss: 27.3428, MinusLogProbMetric: 27.3428, val_loss: 28.2388, val_MinusLogProbMetric: 28.2388

Epoch 498: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3428 - MinusLogProbMetric: 27.3428 - val_loss: 28.2388 - val_MinusLogProbMetric: 28.2388 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 499/1000
2023-10-25 21:14:32.052 
Epoch 499/1000 
	 loss: 27.2909, MinusLogProbMetric: 27.2909, val_loss: 28.2294, val_MinusLogProbMetric: 28.2294

Epoch 499: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2909 - MinusLogProbMetric: 27.2909 - val_loss: 28.2294 - val_MinusLogProbMetric: 28.2294 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 500/1000
2023-10-25 21:15:07.100 
Epoch 500/1000 
	 loss: 27.3278, MinusLogProbMetric: 27.3278, val_loss: 28.2307, val_MinusLogProbMetric: 28.2307

Epoch 500: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3278 - MinusLogProbMetric: 27.3278 - val_loss: 28.2307 - val_MinusLogProbMetric: 28.2307 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 501/1000
2023-10-25 21:15:42.110 
Epoch 501/1000 
	 loss: 27.2972, MinusLogProbMetric: 27.2972, val_loss: 28.2260, val_MinusLogProbMetric: 28.2260

Epoch 501: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2972 - MinusLogProbMetric: 27.2972 - val_loss: 28.2260 - val_MinusLogProbMetric: 28.2260 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 502/1000
2023-10-25 21:16:16.640 
Epoch 502/1000 
	 loss: 27.3259, MinusLogProbMetric: 27.3259, val_loss: 28.2297, val_MinusLogProbMetric: 28.2297

Epoch 502: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3259 - MinusLogProbMetric: 27.3259 - val_loss: 28.2297 - val_MinusLogProbMetric: 28.2297 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 503/1000
2023-10-25 21:16:51.226 
Epoch 503/1000 
	 loss: 27.2961, MinusLogProbMetric: 27.2961, val_loss: 28.4976, val_MinusLogProbMetric: 28.4976

Epoch 503: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2961 - MinusLogProbMetric: 27.2961 - val_loss: 28.4976 - val_MinusLogProbMetric: 28.4976 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 504/1000
2023-10-25 21:17:26.068 
Epoch 504/1000 
	 loss: 27.3181, MinusLogProbMetric: 27.3181, val_loss: 28.3268, val_MinusLogProbMetric: 28.3268

Epoch 504: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3181 - MinusLogProbMetric: 27.3181 - val_loss: 28.3268 - val_MinusLogProbMetric: 28.3268 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 505/1000
2023-10-25 21:18:00.720 
Epoch 505/1000 
	 loss: 27.3100, MinusLogProbMetric: 27.3100, val_loss: 28.3396, val_MinusLogProbMetric: 28.3396

Epoch 505: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3100 - MinusLogProbMetric: 27.3100 - val_loss: 28.3396 - val_MinusLogProbMetric: 28.3396 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 506/1000
2023-10-25 21:18:35.472 
Epoch 506/1000 
	 loss: 27.3092, MinusLogProbMetric: 27.3092, val_loss: 28.2279, val_MinusLogProbMetric: 28.2279

Epoch 506: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3092 - MinusLogProbMetric: 27.3092 - val_loss: 28.2279 - val_MinusLogProbMetric: 28.2279 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 507/1000
2023-10-25 21:19:09.954 
Epoch 507/1000 
	 loss: 27.3166, MinusLogProbMetric: 27.3166, val_loss: 28.2564, val_MinusLogProbMetric: 28.2564

Epoch 507: val_loss did not improve from 28.18567
196/196 - 34s - loss: 27.3166 - MinusLogProbMetric: 27.3166 - val_loss: 28.2564 - val_MinusLogProbMetric: 28.2564 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 508/1000
2023-10-25 21:19:44.485 
Epoch 508/1000 
	 loss: 27.3007, MinusLogProbMetric: 27.3007, val_loss: 28.2062, val_MinusLogProbMetric: 28.2062

Epoch 508: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3007 - MinusLogProbMetric: 27.3007 - val_loss: 28.2062 - val_MinusLogProbMetric: 28.2062 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 509/1000
2023-10-25 21:20:18.926 
Epoch 509/1000 
	 loss: 27.2968, MinusLogProbMetric: 27.2968, val_loss: 28.2261, val_MinusLogProbMetric: 28.2261

Epoch 509: val_loss did not improve from 28.18567
196/196 - 34s - loss: 27.2968 - MinusLogProbMetric: 27.2968 - val_loss: 28.2261 - val_MinusLogProbMetric: 28.2261 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 510/1000
2023-10-25 21:20:53.583 
Epoch 510/1000 
	 loss: 27.3026, MinusLogProbMetric: 27.3026, val_loss: 28.1889, val_MinusLogProbMetric: 28.1889

Epoch 510: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3026 - MinusLogProbMetric: 27.3026 - val_loss: 28.1889 - val_MinusLogProbMetric: 28.1889 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 511/1000
2023-10-25 21:21:28.652 
Epoch 511/1000 
	 loss: 27.3021, MinusLogProbMetric: 27.3021, val_loss: 28.4476, val_MinusLogProbMetric: 28.4476

Epoch 511: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3021 - MinusLogProbMetric: 27.3021 - val_loss: 28.4476 - val_MinusLogProbMetric: 28.4476 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 512/1000
2023-10-25 21:22:03.263 
Epoch 512/1000 
	 loss: 27.3221, MinusLogProbMetric: 27.3221, val_loss: 28.2146, val_MinusLogProbMetric: 28.2146

Epoch 512: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3221 - MinusLogProbMetric: 27.3221 - val_loss: 28.2146 - val_MinusLogProbMetric: 28.2146 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 513/1000
2023-10-25 21:22:38.219 
Epoch 513/1000 
	 loss: 27.3104, MinusLogProbMetric: 27.3104, val_loss: 28.1978, val_MinusLogProbMetric: 28.1978

Epoch 513: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3104 - MinusLogProbMetric: 27.3104 - val_loss: 28.1978 - val_MinusLogProbMetric: 28.1978 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 514/1000
2023-10-25 21:23:13.241 
Epoch 514/1000 
	 loss: 27.3078, MinusLogProbMetric: 27.3078, val_loss: 28.2462, val_MinusLogProbMetric: 28.2462

Epoch 514: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3078 - MinusLogProbMetric: 27.3078 - val_loss: 28.2462 - val_MinusLogProbMetric: 28.2462 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 515/1000
2023-10-25 21:23:47.866 
Epoch 515/1000 
	 loss: 27.3091, MinusLogProbMetric: 27.3091, val_loss: 28.2170, val_MinusLogProbMetric: 28.2170

Epoch 515: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3091 - MinusLogProbMetric: 27.3091 - val_loss: 28.2170 - val_MinusLogProbMetric: 28.2170 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 516/1000
2023-10-25 21:24:22.846 
Epoch 516/1000 
	 loss: 27.3146, MinusLogProbMetric: 27.3146, val_loss: 28.3031, val_MinusLogProbMetric: 28.3031

Epoch 516: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3146 - MinusLogProbMetric: 27.3146 - val_loss: 28.3031 - val_MinusLogProbMetric: 28.3031 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 517/1000
2023-10-25 21:24:57.662 
Epoch 517/1000 
	 loss: 27.3384, MinusLogProbMetric: 27.3384, val_loss: 28.1935, val_MinusLogProbMetric: 28.1935

Epoch 517: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3384 - MinusLogProbMetric: 27.3384 - val_loss: 28.1935 - val_MinusLogProbMetric: 28.1935 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 518/1000
2023-10-25 21:25:32.577 
Epoch 518/1000 
	 loss: 27.2939, MinusLogProbMetric: 27.2939, val_loss: 28.2605, val_MinusLogProbMetric: 28.2605

Epoch 518: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2939 - MinusLogProbMetric: 27.2939 - val_loss: 28.2605 - val_MinusLogProbMetric: 28.2605 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 519/1000
2023-10-25 21:26:07.421 
Epoch 519/1000 
	 loss: 27.2967, MinusLogProbMetric: 27.2967, val_loss: 28.3784, val_MinusLogProbMetric: 28.3784

Epoch 519: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2967 - MinusLogProbMetric: 27.2967 - val_loss: 28.3784 - val_MinusLogProbMetric: 28.3784 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 520/1000
2023-10-25 21:26:42.213 
Epoch 520/1000 
	 loss: 27.2945, MinusLogProbMetric: 27.2945, val_loss: 28.3117, val_MinusLogProbMetric: 28.3117

Epoch 520: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2945 - MinusLogProbMetric: 27.2945 - val_loss: 28.3117 - val_MinusLogProbMetric: 28.3117 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 521/1000
2023-10-25 21:27:16.794 
Epoch 521/1000 
	 loss: 27.2998, MinusLogProbMetric: 27.2998, val_loss: 28.3749, val_MinusLogProbMetric: 28.3749

Epoch 521: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2998 - MinusLogProbMetric: 27.2998 - val_loss: 28.3749 - val_MinusLogProbMetric: 28.3749 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 522/1000
2023-10-25 21:27:51.474 
Epoch 522/1000 
	 loss: 27.3181, MinusLogProbMetric: 27.3181, val_loss: 28.2735, val_MinusLogProbMetric: 28.2735

Epoch 522: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3181 - MinusLogProbMetric: 27.3181 - val_loss: 28.2735 - val_MinusLogProbMetric: 28.2735 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 523/1000
2023-10-25 21:28:26.156 
Epoch 523/1000 
	 loss: 27.2916, MinusLogProbMetric: 27.2916, val_loss: 28.2428, val_MinusLogProbMetric: 28.2428

Epoch 523: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2916 - MinusLogProbMetric: 27.2916 - val_loss: 28.2428 - val_MinusLogProbMetric: 28.2428 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 524/1000
2023-10-25 21:29:00.862 
Epoch 524/1000 
	 loss: 27.3076, MinusLogProbMetric: 27.3076, val_loss: 28.2090, val_MinusLogProbMetric: 28.2090

Epoch 524: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3076 - MinusLogProbMetric: 27.3076 - val_loss: 28.2090 - val_MinusLogProbMetric: 28.2090 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 525/1000
2023-10-25 21:29:35.523 
Epoch 525/1000 
	 loss: 27.2901, MinusLogProbMetric: 27.2901, val_loss: 28.2049, val_MinusLogProbMetric: 28.2049

Epoch 525: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2901 - MinusLogProbMetric: 27.2901 - val_loss: 28.2049 - val_MinusLogProbMetric: 28.2049 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 526/1000
2023-10-25 21:30:10.469 
Epoch 526/1000 
	 loss: 27.3082, MinusLogProbMetric: 27.3082, val_loss: 28.2086, val_MinusLogProbMetric: 28.2086

Epoch 526: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3082 - MinusLogProbMetric: 27.3082 - val_loss: 28.2086 - val_MinusLogProbMetric: 28.2086 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 527/1000
2023-10-25 21:30:45.083 
Epoch 527/1000 
	 loss: 27.3004, MinusLogProbMetric: 27.3004, val_loss: 28.3359, val_MinusLogProbMetric: 28.3359

Epoch 527: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3004 - MinusLogProbMetric: 27.3004 - val_loss: 28.3359 - val_MinusLogProbMetric: 28.3359 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 528/1000
2023-10-25 21:31:19.784 
Epoch 528/1000 
	 loss: 27.2954, MinusLogProbMetric: 27.2954, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 528: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2954 - MinusLogProbMetric: 27.2954 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 529/1000
2023-10-25 21:31:54.467 
Epoch 529/1000 
	 loss: 27.2974, MinusLogProbMetric: 27.2974, val_loss: 28.2143, val_MinusLogProbMetric: 28.2143

Epoch 529: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2974 - MinusLogProbMetric: 27.2974 - val_loss: 28.2143 - val_MinusLogProbMetric: 28.2143 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 530/1000
2023-10-25 21:32:29.508 
Epoch 530/1000 
	 loss: 27.3025, MinusLogProbMetric: 27.3025, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 530: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3025 - MinusLogProbMetric: 27.3025 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 531/1000
2023-10-25 21:33:04.588 
Epoch 531/1000 
	 loss: 27.2940, MinusLogProbMetric: 27.2940, val_loss: 28.2159, val_MinusLogProbMetric: 28.2159

Epoch 531: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2940 - MinusLogProbMetric: 27.2940 - val_loss: 28.2159 - val_MinusLogProbMetric: 28.2159 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 532/1000
2023-10-25 21:33:39.340 
Epoch 532/1000 
	 loss: 27.3093, MinusLogProbMetric: 27.3093, val_loss: 28.3850, val_MinusLogProbMetric: 28.3850

Epoch 532: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3093 - MinusLogProbMetric: 27.3093 - val_loss: 28.3850 - val_MinusLogProbMetric: 28.3850 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 533/1000
2023-10-25 21:34:14.285 
Epoch 533/1000 
	 loss: 27.3036, MinusLogProbMetric: 27.3036, val_loss: 28.3462, val_MinusLogProbMetric: 28.3462

Epoch 533: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.3036 - MinusLogProbMetric: 27.3036 - val_loss: 28.3462 - val_MinusLogProbMetric: 28.3462 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 534/1000
2023-10-25 21:34:49.263 
Epoch 534/1000 
	 loss: 27.2942, MinusLogProbMetric: 27.2942, val_loss: 28.3768, val_MinusLogProbMetric: 28.3768

Epoch 534: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2942 - MinusLogProbMetric: 27.2942 - val_loss: 28.3768 - val_MinusLogProbMetric: 28.3768 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 535/1000
2023-10-25 21:35:24.210 
Epoch 535/1000 
	 loss: 27.2957, MinusLogProbMetric: 27.2957, val_loss: 28.5216, val_MinusLogProbMetric: 28.5216

Epoch 535: val_loss did not improve from 28.18567
196/196 - 35s - loss: 27.2957 - MinusLogProbMetric: 27.2957 - val_loss: 28.5216 - val_MinusLogProbMetric: 28.5216 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 536/1000
2023-10-25 21:35:58.785 
Epoch 536/1000 
	 loss: 27.2379, MinusLogProbMetric: 27.2379, val_loss: 28.1845, val_MinusLogProbMetric: 28.1845

Epoch 536: val_loss improved from 28.18567 to 28.18453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.2379 - MinusLogProbMetric: 27.2379 - val_loss: 28.1845 - val_MinusLogProbMetric: 28.1845 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 537/1000
2023-10-25 21:36:34.328 
Epoch 537/1000 
	 loss: 27.2340, MinusLogProbMetric: 27.2340, val_loss: 28.1769, val_MinusLogProbMetric: 28.1769

Epoch 537: val_loss improved from 28.18453 to 28.17686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.2340 - MinusLogProbMetric: 27.2340 - val_loss: 28.1769 - val_MinusLogProbMetric: 28.1769 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 538/1000
2023-10-25 21:37:09.883 
Epoch 538/1000 
	 loss: 27.2323, MinusLogProbMetric: 27.2323, val_loss: 28.1865, val_MinusLogProbMetric: 28.1865

Epoch 538: val_loss did not improve from 28.17686
196/196 - 35s - loss: 27.2323 - MinusLogProbMetric: 27.2323 - val_loss: 28.1865 - val_MinusLogProbMetric: 28.1865 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 539/1000
2023-10-25 21:37:44.847 
Epoch 539/1000 
	 loss: 27.2312, MinusLogProbMetric: 27.2312, val_loss: 28.1739, val_MinusLogProbMetric: 28.1739

Epoch 539: val_loss improved from 28.17686 to 28.17385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 27.2312 - MinusLogProbMetric: 27.2312 - val_loss: 28.1739 - val_MinusLogProbMetric: 28.1739 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 540/1000
2023-10-25 21:38:20.071 
Epoch 540/1000 
	 loss: 27.2291, MinusLogProbMetric: 27.2291, val_loss: 28.1788, val_MinusLogProbMetric: 28.1788

Epoch 540: val_loss did not improve from 28.17385
196/196 - 35s - loss: 27.2291 - MinusLogProbMetric: 27.2291 - val_loss: 28.1788 - val_MinusLogProbMetric: 28.1788 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 541/1000
2023-10-25 21:38:54.998 
Epoch 541/1000 
	 loss: 27.2341, MinusLogProbMetric: 27.2341, val_loss: 28.1970, val_MinusLogProbMetric: 28.1970

Epoch 541: val_loss did not improve from 28.17385
196/196 - 35s - loss: 27.2341 - MinusLogProbMetric: 27.2341 - val_loss: 28.1970 - val_MinusLogProbMetric: 28.1970 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 542/1000
2023-10-25 21:39:29.717 
Epoch 542/1000 
	 loss: 27.2388, MinusLogProbMetric: 27.2388, val_loss: 28.1850, val_MinusLogProbMetric: 28.1850

Epoch 542: val_loss did not improve from 28.17385
196/196 - 35s - loss: 27.2388 - MinusLogProbMetric: 27.2388 - val_loss: 28.1850 - val_MinusLogProbMetric: 28.1850 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 543/1000
2023-10-25 21:40:04.657 
Epoch 543/1000 
	 loss: 27.2311, MinusLogProbMetric: 27.2311, val_loss: 28.2414, val_MinusLogProbMetric: 28.2414

Epoch 543: val_loss did not improve from 28.17385
196/196 - 35s - loss: 27.2311 - MinusLogProbMetric: 27.2311 - val_loss: 28.2414 - val_MinusLogProbMetric: 28.2414 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 544/1000
2023-10-25 21:40:39.802 
Epoch 544/1000 
	 loss: 27.2339, MinusLogProbMetric: 27.2339, val_loss: 28.2636, val_MinusLogProbMetric: 28.2636

Epoch 544: val_loss did not improve from 28.17385
196/196 - 35s - loss: 27.2339 - MinusLogProbMetric: 27.2339 - val_loss: 28.2636 - val_MinusLogProbMetric: 28.2636 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 545/1000
2023-10-25 21:41:14.560 
Epoch 545/1000 
	 loss: 27.2377, MinusLogProbMetric: 27.2377, val_loss: 28.2003, val_MinusLogProbMetric: 28.2003

Epoch 545: val_loss did not improve from 28.17385
196/196 - 35s - loss: 27.2377 - MinusLogProbMetric: 27.2377 - val_loss: 28.2003 - val_MinusLogProbMetric: 28.2003 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 546/1000
2023-10-25 21:41:49.502 
Epoch 546/1000 
	 loss: 27.2402, MinusLogProbMetric: 27.2402, val_loss: 28.1669, val_MinusLogProbMetric: 28.1669

Epoch 546: val_loss improved from 28.17385 to 28.16685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.2402 - MinusLogProbMetric: 27.2402 - val_loss: 28.1669 - val_MinusLogProbMetric: 28.1669 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 547/1000
2023-10-25 21:42:24.694 
Epoch 547/1000 
	 loss: 27.2458, MinusLogProbMetric: 27.2458, val_loss: 28.1793, val_MinusLogProbMetric: 28.1793

Epoch 547: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2458 - MinusLogProbMetric: 27.2458 - val_loss: 28.1793 - val_MinusLogProbMetric: 28.1793 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 548/1000
2023-10-25 21:42:59.414 
Epoch 548/1000 
	 loss: 27.2327, MinusLogProbMetric: 27.2327, val_loss: 28.1898, val_MinusLogProbMetric: 28.1898

Epoch 548: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2327 - MinusLogProbMetric: 27.2327 - val_loss: 28.1898 - val_MinusLogProbMetric: 28.1898 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 549/1000
2023-10-25 21:43:34.563 
Epoch 549/1000 
	 loss: 27.2318, MinusLogProbMetric: 27.2318, val_loss: 28.2362, val_MinusLogProbMetric: 28.2362

Epoch 549: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2318 - MinusLogProbMetric: 27.2318 - val_loss: 28.2362 - val_MinusLogProbMetric: 28.2362 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 550/1000
2023-10-25 21:44:09.520 
Epoch 550/1000 
	 loss: 27.2349, MinusLogProbMetric: 27.2349, val_loss: 28.2104, val_MinusLogProbMetric: 28.2104

Epoch 550: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2349 - MinusLogProbMetric: 27.2349 - val_loss: 28.2104 - val_MinusLogProbMetric: 28.2104 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 551/1000
2023-10-25 21:44:44.762 
Epoch 551/1000 
	 loss: 27.2328, MinusLogProbMetric: 27.2328, val_loss: 28.2255, val_MinusLogProbMetric: 28.2255

Epoch 551: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2328 - MinusLogProbMetric: 27.2328 - val_loss: 28.2255 - val_MinusLogProbMetric: 28.2255 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 552/1000
2023-10-25 21:45:19.775 
Epoch 552/1000 
	 loss: 27.2362, MinusLogProbMetric: 27.2362, val_loss: 28.1808, val_MinusLogProbMetric: 28.1808

Epoch 552: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2362 - MinusLogProbMetric: 27.2362 - val_loss: 28.1808 - val_MinusLogProbMetric: 28.1808 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 553/1000
2023-10-25 21:45:54.624 
Epoch 553/1000 
	 loss: 27.2290, MinusLogProbMetric: 27.2290, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 553: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2290 - MinusLogProbMetric: 27.2290 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 554/1000
2023-10-25 21:46:29.431 
Epoch 554/1000 
	 loss: 27.2280, MinusLogProbMetric: 27.2280, val_loss: 28.1830, val_MinusLogProbMetric: 28.1830

Epoch 554: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2280 - MinusLogProbMetric: 27.2280 - val_loss: 28.1830 - val_MinusLogProbMetric: 28.1830 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 555/1000
2023-10-25 21:47:04.403 
Epoch 555/1000 
	 loss: 27.2260, MinusLogProbMetric: 27.2260, val_loss: 28.1995, val_MinusLogProbMetric: 28.1995

Epoch 555: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2260 - MinusLogProbMetric: 27.2260 - val_loss: 28.1995 - val_MinusLogProbMetric: 28.1995 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 556/1000
2023-10-25 21:47:39.237 
Epoch 556/1000 
	 loss: 27.2292, MinusLogProbMetric: 27.2292, val_loss: 28.2127, val_MinusLogProbMetric: 28.2127

Epoch 556: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2292 - MinusLogProbMetric: 27.2292 - val_loss: 28.2127 - val_MinusLogProbMetric: 28.2127 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 557/1000
2023-10-25 21:48:14.341 
Epoch 557/1000 
	 loss: 27.2370, MinusLogProbMetric: 27.2370, val_loss: 28.2006, val_MinusLogProbMetric: 28.2006

Epoch 557: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2370 - MinusLogProbMetric: 27.2370 - val_loss: 28.2006 - val_MinusLogProbMetric: 28.2006 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 558/1000
2023-10-25 21:48:49.374 
Epoch 558/1000 
	 loss: 27.2307, MinusLogProbMetric: 27.2307, val_loss: 28.1799, val_MinusLogProbMetric: 28.1799

Epoch 558: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2307 - MinusLogProbMetric: 27.2307 - val_loss: 28.1799 - val_MinusLogProbMetric: 28.1799 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 559/1000
2023-10-25 21:49:24.590 
Epoch 559/1000 
	 loss: 27.2300, MinusLogProbMetric: 27.2300, val_loss: 28.2233, val_MinusLogProbMetric: 28.2233

Epoch 559: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2300 - MinusLogProbMetric: 27.2300 - val_loss: 28.2233 - val_MinusLogProbMetric: 28.2233 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 560/1000
2023-10-25 21:49:59.572 
Epoch 560/1000 
	 loss: 27.2313, MinusLogProbMetric: 27.2313, val_loss: 28.2217, val_MinusLogProbMetric: 28.2217

Epoch 560: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2313 - MinusLogProbMetric: 27.2313 - val_loss: 28.2217 - val_MinusLogProbMetric: 28.2217 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 561/1000
2023-10-25 21:50:34.472 
Epoch 561/1000 
	 loss: 27.2240, MinusLogProbMetric: 27.2240, val_loss: 28.1805, val_MinusLogProbMetric: 28.1805

Epoch 561: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2240 - MinusLogProbMetric: 27.2240 - val_loss: 28.1805 - val_MinusLogProbMetric: 28.1805 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 562/1000
2023-10-25 21:51:09.460 
Epoch 562/1000 
	 loss: 27.2333, MinusLogProbMetric: 27.2333, val_loss: 28.1954, val_MinusLogProbMetric: 28.1954

Epoch 562: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2333 - MinusLogProbMetric: 27.2333 - val_loss: 28.1954 - val_MinusLogProbMetric: 28.1954 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 563/1000
2023-10-25 21:51:44.363 
Epoch 563/1000 
	 loss: 27.2307, MinusLogProbMetric: 27.2307, val_loss: 28.1784, val_MinusLogProbMetric: 28.1784

Epoch 563: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2307 - MinusLogProbMetric: 27.2307 - val_loss: 28.1784 - val_MinusLogProbMetric: 28.1784 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 564/1000
2023-10-25 21:52:18.955 
Epoch 564/1000 
	 loss: 27.2299, MinusLogProbMetric: 27.2299, val_loss: 28.2252, val_MinusLogProbMetric: 28.2252

Epoch 564: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2299 - MinusLogProbMetric: 27.2299 - val_loss: 28.2252 - val_MinusLogProbMetric: 28.2252 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 565/1000
2023-10-25 21:52:53.580 
Epoch 565/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.2065, val_MinusLogProbMetric: 28.2065

Epoch 565: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.2065 - val_MinusLogProbMetric: 28.2065 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 566/1000
2023-10-25 21:53:28.490 
Epoch 566/1000 
	 loss: 27.2288, MinusLogProbMetric: 27.2288, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 566: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2288 - MinusLogProbMetric: 27.2288 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 567/1000
2023-10-25 21:54:03.292 
Epoch 567/1000 
	 loss: 27.2313, MinusLogProbMetric: 27.2313, val_loss: 28.1971, val_MinusLogProbMetric: 28.1971

Epoch 567: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2313 - MinusLogProbMetric: 27.2313 - val_loss: 28.1971 - val_MinusLogProbMetric: 28.1971 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 568/1000
2023-10-25 21:54:38.617 
Epoch 568/1000 
	 loss: 27.2241, MinusLogProbMetric: 27.2241, val_loss: 28.1989, val_MinusLogProbMetric: 28.1989

Epoch 568: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2241 - MinusLogProbMetric: 27.2241 - val_loss: 28.1989 - val_MinusLogProbMetric: 28.1989 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 569/1000
2023-10-25 21:55:13.737 
Epoch 569/1000 
	 loss: 27.2265, MinusLogProbMetric: 27.2265, val_loss: 28.2148, val_MinusLogProbMetric: 28.2148

Epoch 569: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2265 - MinusLogProbMetric: 27.2265 - val_loss: 28.2148 - val_MinusLogProbMetric: 28.2148 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 570/1000
2023-10-25 21:55:48.885 
Epoch 570/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.1821, val_MinusLogProbMetric: 28.1821

Epoch 570: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.1821 - val_MinusLogProbMetric: 28.1821 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 571/1000
2023-10-25 21:56:23.787 
Epoch 571/1000 
	 loss: 27.2311, MinusLogProbMetric: 27.2311, val_loss: 28.1727, val_MinusLogProbMetric: 28.1727

Epoch 571: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2311 - MinusLogProbMetric: 27.2311 - val_loss: 28.1727 - val_MinusLogProbMetric: 28.1727 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 572/1000
2023-10-25 21:56:58.938 
Epoch 572/1000 
	 loss: 27.2270, MinusLogProbMetric: 27.2270, val_loss: 28.1821, val_MinusLogProbMetric: 28.1821

Epoch 572: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2270 - MinusLogProbMetric: 27.2270 - val_loss: 28.1821 - val_MinusLogProbMetric: 28.1821 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 573/1000
2023-10-25 21:57:33.701 
Epoch 573/1000 
	 loss: 27.2346, MinusLogProbMetric: 27.2346, val_loss: 28.2154, val_MinusLogProbMetric: 28.2154

Epoch 573: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2346 - MinusLogProbMetric: 27.2346 - val_loss: 28.2154 - val_MinusLogProbMetric: 28.2154 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 574/1000
2023-10-25 21:58:08.331 
Epoch 574/1000 
	 loss: 27.2255, MinusLogProbMetric: 27.2255, val_loss: 28.1900, val_MinusLogProbMetric: 28.1900

Epoch 574: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2255 - MinusLogProbMetric: 27.2255 - val_loss: 28.1900 - val_MinusLogProbMetric: 28.1900 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 575/1000
2023-10-25 21:58:43.195 
Epoch 575/1000 
	 loss: 27.2279, MinusLogProbMetric: 27.2279, val_loss: 28.1759, val_MinusLogProbMetric: 28.1759

Epoch 575: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2279 - MinusLogProbMetric: 27.2279 - val_loss: 28.1759 - val_MinusLogProbMetric: 28.1759 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 576/1000
2023-10-25 21:59:17.959 
Epoch 576/1000 
	 loss: 27.2300, MinusLogProbMetric: 27.2300, val_loss: 28.1919, val_MinusLogProbMetric: 28.1919

Epoch 576: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2300 - MinusLogProbMetric: 27.2300 - val_loss: 28.1919 - val_MinusLogProbMetric: 28.1919 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 577/1000
2023-10-25 21:59:52.885 
Epoch 577/1000 
	 loss: 27.2260, MinusLogProbMetric: 27.2260, val_loss: 28.1910, val_MinusLogProbMetric: 28.1910

Epoch 577: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2260 - MinusLogProbMetric: 27.2260 - val_loss: 28.1910 - val_MinusLogProbMetric: 28.1910 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 578/1000
2023-10-25 22:00:27.720 
Epoch 578/1000 
	 loss: 27.2249, MinusLogProbMetric: 27.2249, val_loss: 28.2193, val_MinusLogProbMetric: 28.2193

Epoch 578: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2249 - MinusLogProbMetric: 27.2249 - val_loss: 28.2193 - val_MinusLogProbMetric: 28.2193 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 579/1000
2023-10-25 22:01:02.292 
Epoch 579/1000 
	 loss: 27.2257, MinusLogProbMetric: 27.2257, val_loss: 28.2017, val_MinusLogProbMetric: 28.2017

Epoch 579: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2257 - MinusLogProbMetric: 27.2257 - val_loss: 28.2017 - val_MinusLogProbMetric: 28.2017 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 580/1000
2023-10-25 22:01:36.966 
Epoch 580/1000 
	 loss: 27.2255, MinusLogProbMetric: 27.2255, val_loss: 28.1906, val_MinusLogProbMetric: 28.1906

Epoch 580: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2255 - MinusLogProbMetric: 27.2255 - val_loss: 28.1906 - val_MinusLogProbMetric: 28.1906 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 581/1000
2023-10-25 22:02:11.752 
Epoch 581/1000 
	 loss: 27.2319, MinusLogProbMetric: 27.2319, val_loss: 28.1822, val_MinusLogProbMetric: 28.1822

Epoch 581: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2319 - MinusLogProbMetric: 27.2319 - val_loss: 28.1822 - val_MinusLogProbMetric: 28.1822 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 582/1000
2023-10-25 22:02:46.822 
Epoch 582/1000 
	 loss: 27.2220, MinusLogProbMetric: 27.2220, val_loss: 28.2142, val_MinusLogProbMetric: 28.2142

Epoch 582: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2220 - MinusLogProbMetric: 27.2220 - val_loss: 28.2142 - val_MinusLogProbMetric: 28.2142 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 583/1000
2023-10-25 22:03:22.283 
Epoch 583/1000 
	 loss: 27.2323, MinusLogProbMetric: 27.2323, val_loss: 28.1835, val_MinusLogProbMetric: 28.1835

Epoch 583: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2323 - MinusLogProbMetric: 27.2323 - val_loss: 28.1835 - val_MinusLogProbMetric: 28.1835 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 584/1000
2023-10-25 22:03:57.572 
Epoch 584/1000 
	 loss: 27.2244, MinusLogProbMetric: 27.2244, val_loss: 28.1996, val_MinusLogProbMetric: 28.1996

Epoch 584: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2244 - MinusLogProbMetric: 27.2244 - val_loss: 28.1996 - val_MinusLogProbMetric: 28.1996 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 585/1000
2023-10-25 22:04:32.676 
Epoch 585/1000 
	 loss: 27.2255, MinusLogProbMetric: 27.2255, val_loss: 28.2104, val_MinusLogProbMetric: 28.2104

Epoch 585: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2255 - MinusLogProbMetric: 27.2255 - val_loss: 28.2104 - val_MinusLogProbMetric: 28.2104 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 586/1000
2023-10-25 22:05:07.838 
Epoch 586/1000 
	 loss: 27.2263, MinusLogProbMetric: 27.2263, val_loss: 28.2368, val_MinusLogProbMetric: 28.2368

Epoch 586: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2263 - MinusLogProbMetric: 27.2263 - val_loss: 28.2368 - val_MinusLogProbMetric: 28.2368 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 587/1000
2023-10-25 22:05:42.995 
Epoch 587/1000 
	 loss: 27.2245, MinusLogProbMetric: 27.2245, val_loss: 28.1941, val_MinusLogProbMetric: 28.1941

Epoch 587: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2245 - MinusLogProbMetric: 27.2245 - val_loss: 28.1941 - val_MinusLogProbMetric: 28.1941 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 588/1000
2023-10-25 22:06:17.895 
Epoch 588/1000 
	 loss: 27.2296, MinusLogProbMetric: 27.2296, val_loss: 28.1797, val_MinusLogProbMetric: 28.1797

Epoch 588: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2296 - MinusLogProbMetric: 27.2296 - val_loss: 28.1797 - val_MinusLogProbMetric: 28.1797 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 589/1000
2023-10-25 22:06:52.680 
Epoch 589/1000 
	 loss: 27.2224, MinusLogProbMetric: 27.2224, val_loss: 28.1908, val_MinusLogProbMetric: 28.1908

Epoch 589: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2224 - MinusLogProbMetric: 27.2224 - val_loss: 28.1908 - val_MinusLogProbMetric: 28.1908 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 590/1000
2023-10-25 22:07:27.510 
Epoch 590/1000 
	 loss: 27.2254, MinusLogProbMetric: 27.2254, val_loss: 28.1849, val_MinusLogProbMetric: 28.1849

Epoch 590: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2254 - MinusLogProbMetric: 27.2254 - val_loss: 28.1849 - val_MinusLogProbMetric: 28.1849 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 591/1000
2023-10-25 22:08:02.296 
Epoch 591/1000 
	 loss: 27.2315, MinusLogProbMetric: 27.2315, val_loss: 28.2002, val_MinusLogProbMetric: 28.2002

Epoch 591: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2315 - MinusLogProbMetric: 27.2315 - val_loss: 28.2002 - val_MinusLogProbMetric: 28.2002 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 592/1000
2023-10-25 22:08:36.943 
Epoch 592/1000 
	 loss: 27.2196, MinusLogProbMetric: 27.2196, val_loss: 28.1826, val_MinusLogProbMetric: 28.1826

Epoch 592: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2196 - MinusLogProbMetric: 27.2196 - val_loss: 28.1826 - val_MinusLogProbMetric: 28.1826 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 593/1000
2023-10-25 22:09:12.108 
Epoch 593/1000 
	 loss: 27.2252, MinusLogProbMetric: 27.2252, val_loss: 28.1868, val_MinusLogProbMetric: 28.1868

Epoch 593: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2252 - MinusLogProbMetric: 27.2252 - val_loss: 28.1868 - val_MinusLogProbMetric: 28.1868 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 594/1000
2023-10-25 22:09:47.106 
Epoch 594/1000 
	 loss: 27.2193, MinusLogProbMetric: 27.2193, val_loss: 28.1769, val_MinusLogProbMetric: 28.1769

Epoch 594: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2193 - MinusLogProbMetric: 27.2193 - val_loss: 28.1769 - val_MinusLogProbMetric: 28.1769 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 595/1000
2023-10-25 22:10:22.355 
Epoch 595/1000 
	 loss: 27.2190, MinusLogProbMetric: 27.2190, val_loss: 28.1785, val_MinusLogProbMetric: 28.1785

Epoch 595: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2190 - MinusLogProbMetric: 27.2190 - val_loss: 28.1785 - val_MinusLogProbMetric: 28.1785 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 596/1000
2023-10-25 22:10:57.566 
Epoch 596/1000 
	 loss: 27.2187, MinusLogProbMetric: 27.2187, val_loss: 28.2147, val_MinusLogProbMetric: 28.2147

Epoch 596: val_loss did not improve from 28.16685
196/196 - 35s - loss: 27.2187 - MinusLogProbMetric: 27.2187 - val_loss: 28.2147 - val_MinusLogProbMetric: 28.2147 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 597/1000
2023-10-25 22:11:32.897 
Epoch 597/1000 
	 loss: 27.2002, MinusLogProbMetric: 27.2002, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 597: val_loss improved from 28.16685 to 28.16413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 36s - loss: 27.2002 - MinusLogProbMetric: 27.2002 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 3.1250e-05 - 36s/epoch - 183ms/step
Epoch 598/1000
2023-10-25 22:12:08.674 
Epoch 598/1000 
	 loss: 27.1994, MinusLogProbMetric: 27.1994, val_loss: 28.1694, val_MinusLogProbMetric: 28.1694

Epoch 598: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1994 - MinusLogProbMetric: 27.1994 - val_loss: 28.1694 - val_MinusLogProbMetric: 28.1694 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 599/1000
2023-10-25 22:12:43.759 
Epoch 599/1000 
	 loss: 27.1970, MinusLogProbMetric: 27.1970, val_loss: 28.1756, val_MinusLogProbMetric: 28.1756

Epoch 599: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1970 - MinusLogProbMetric: 27.1970 - val_loss: 28.1756 - val_MinusLogProbMetric: 28.1756 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 600/1000
2023-10-25 22:13:18.745 
Epoch 600/1000 
	 loss: 27.1989, MinusLogProbMetric: 27.1989, val_loss: 28.1885, val_MinusLogProbMetric: 28.1885

Epoch 600: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1989 - MinusLogProbMetric: 27.1989 - val_loss: 28.1885 - val_MinusLogProbMetric: 28.1885 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 601/1000
2023-10-25 22:13:53.893 
Epoch 601/1000 
	 loss: 27.1989, MinusLogProbMetric: 27.1989, val_loss: 28.1854, val_MinusLogProbMetric: 28.1854

Epoch 601: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1989 - MinusLogProbMetric: 27.1989 - val_loss: 28.1854 - val_MinusLogProbMetric: 28.1854 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 602/1000
2023-10-25 22:14:29.067 
Epoch 602/1000 
	 loss: 27.1962, MinusLogProbMetric: 27.1962, val_loss: 28.1649, val_MinusLogProbMetric: 28.1649

Epoch 602: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1962 - MinusLogProbMetric: 27.1962 - val_loss: 28.1649 - val_MinusLogProbMetric: 28.1649 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 603/1000
2023-10-25 22:15:04.115 
Epoch 603/1000 
	 loss: 27.2030, MinusLogProbMetric: 27.2030, val_loss: 28.1729, val_MinusLogProbMetric: 28.1729

Epoch 603: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.2030 - MinusLogProbMetric: 27.2030 - val_loss: 28.1729 - val_MinusLogProbMetric: 28.1729 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 604/1000
2023-10-25 22:15:39.207 
Epoch 604/1000 
	 loss: 27.1967, MinusLogProbMetric: 27.1967, val_loss: 28.1770, val_MinusLogProbMetric: 28.1770

Epoch 604: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1967 - MinusLogProbMetric: 27.1967 - val_loss: 28.1770 - val_MinusLogProbMetric: 28.1770 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 605/1000
2023-10-25 22:16:14.263 
Epoch 605/1000 
	 loss: 27.1964, MinusLogProbMetric: 27.1964, val_loss: 28.1843, val_MinusLogProbMetric: 28.1843

Epoch 605: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1964 - MinusLogProbMetric: 27.1964 - val_loss: 28.1843 - val_MinusLogProbMetric: 28.1843 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 606/1000
2023-10-25 22:16:49.518 
Epoch 606/1000 
	 loss: 27.2010, MinusLogProbMetric: 27.2010, val_loss: 28.1820, val_MinusLogProbMetric: 28.1820

Epoch 606: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.2010 - MinusLogProbMetric: 27.2010 - val_loss: 28.1820 - val_MinusLogProbMetric: 28.1820 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 607/1000
2023-10-25 22:17:24.703 
Epoch 607/1000 
	 loss: 27.2001, MinusLogProbMetric: 27.2001, val_loss: 28.1798, val_MinusLogProbMetric: 28.1798

Epoch 607: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.2001 - MinusLogProbMetric: 27.2001 - val_loss: 28.1798 - val_MinusLogProbMetric: 28.1798 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 608/1000
2023-10-25 22:18:00.073 
Epoch 608/1000 
	 loss: 27.1980, MinusLogProbMetric: 27.1980, val_loss: 28.1683, val_MinusLogProbMetric: 28.1683

Epoch 608: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1980 - MinusLogProbMetric: 27.1980 - val_loss: 28.1683 - val_MinusLogProbMetric: 28.1683 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 609/1000
2023-10-25 22:18:35.183 
Epoch 609/1000 
	 loss: 27.1963, MinusLogProbMetric: 27.1963, val_loss: 28.1815, val_MinusLogProbMetric: 28.1815

Epoch 609: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1963 - MinusLogProbMetric: 27.1963 - val_loss: 28.1815 - val_MinusLogProbMetric: 28.1815 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 610/1000
2023-10-25 22:19:10.516 
Epoch 610/1000 
	 loss: 27.1990, MinusLogProbMetric: 27.1990, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 610: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1990 - MinusLogProbMetric: 27.1990 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 611/1000
2023-10-25 22:19:45.485 
Epoch 611/1000 
	 loss: 27.1991, MinusLogProbMetric: 27.1991, val_loss: 28.1715, val_MinusLogProbMetric: 28.1715

Epoch 611: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1991 - MinusLogProbMetric: 27.1991 - val_loss: 28.1715 - val_MinusLogProbMetric: 28.1715 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 612/1000
2023-10-25 22:20:20.365 
Epoch 612/1000 
	 loss: 27.1952, MinusLogProbMetric: 27.1952, val_loss: 28.1726, val_MinusLogProbMetric: 28.1726

Epoch 612: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1952 - MinusLogProbMetric: 27.1952 - val_loss: 28.1726 - val_MinusLogProbMetric: 28.1726 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 613/1000
2023-10-25 22:20:55.033 
Epoch 613/1000 
	 loss: 27.1956, MinusLogProbMetric: 27.1956, val_loss: 28.1718, val_MinusLogProbMetric: 28.1718

Epoch 613: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1956 - MinusLogProbMetric: 27.1956 - val_loss: 28.1718 - val_MinusLogProbMetric: 28.1718 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 614/1000
2023-10-25 22:21:29.965 
Epoch 614/1000 
	 loss: 27.1988, MinusLogProbMetric: 27.1988, val_loss: 28.1816, val_MinusLogProbMetric: 28.1816

Epoch 614: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1988 - MinusLogProbMetric: 27.1988 - val_loss: 28.1816 - val_MinusLogProbMetric: 28.1816 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 615/1000
2023-10-25 22:22:04.936 
Epoch 615/1000 
	 loss: 27.1972, MinusLogProbMetric: 27.1972, val_loss: 28.1718, val_MinusLogProbMetric: 28.1718

Epoch 615: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1972 - MinusLogProbMetric: 27.1972 - val_loss: 28.1718 - val_MinusLogProbMetric: 28.1718 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 616/1000
2023-10-25 22:22:39.907 
Epoch 616/1000 
	 loss: 27.1957, MinusLogProbMetric: 27.1957, val_loss: 28.1656, val_MinusLogProbMetric: 28.1656

Epoch 616: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1957 - MinusLogProbMetric: 27.1957 - val_loss: 28.1656 - val_MinusLogProbMetric: 28.1656 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 617/1000
2023-10-25 22:23:15.018 
Epoch 617/1000 
	 loss: 27.1968, MinusLogProbMetric: 27.1968, val_loss: 28.1714, val_MinusLogProbMetric: 28.1714

Epoch 617: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1968 - MinusLogProbMetric: 27.1968 - val_loss: 28.1714 - val_MinusLogProbMetric: 28.1714 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 618/1000
2023-10-25 22:23:50.156 
Epoch 618/1000 
	 loss: 27.1974, MinusLogProbMetric: 27.1974, val_loss: 28.1752, val_MinusLogProbMetric: 28.1752

Epoch 618: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1974 - MinusLogProbMetric: 27.1974 - val_loss: 28.1752 - val_MinusLogProbMetric: 28.1752 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 619/1000
2023-10-25 22:24:24.769 
Epoch 619/1000 
	 loss: 27.1954, MinusLogProbMetric: 27.1954, val_loss: 28.1849, val_MinusLogProbMetric: 28.1849

Epoch 619: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1954 - MinusLogProbMetric: 27.1954 - val_loss: 28.1849 - val_MinusLogProbMetric: 28.1849 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 620/1000
2023-10-25 22:24:59.484 
Epoch 620/1000 
	 loss: 27.1951, MinusLogProbMetric: 27.1951, val_loss: 28.1721, val_MinusLogProbMetric: 28.1721

Epoch 620: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1951 - MinusLogProbMetric: 27.1951 - val_loss: 28.1721 - val_MinusLogProbMetric: 28.1721 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 621/1000
2023-10-25 22:25:34.305 
Epoch 621/1000 
	 loss: 27.1968, MinusLogProbMetric: 27.1968, val_loss: 28.1752, val_MinusLogProbMetric: 28.1752

Epoch 621: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1968 - MinusLogProbMetric: 27.1968 - val_loss: 28.1752 - val_MinusLogProbMetric: 28.1752 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 622/1000
2023-10-25 22:26:09.184 
Epoch 622/1000 
	 loss: 27.1945, MinusLogProbMetric: 27.1945, val_loss: 28.1943, val_MinusLogProbMetric: 28.1943

Epoch 622: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1945 - MinusLogProbMetric: 27.1945 - val_loss: 28.1943 - val_MinusLogProbMetric: 28.1943 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 623/1000
2023-10-25 22:26:44.140 
Epoch 623/1000 
	 loss: 27.1977, MinusLogProbMetric: 27.1977, val_loss: 28.2143, val_MinusLogProbMetric: 28.2143

Epoch 623: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1977 - MinusLogProbMetric: 27.1977 - val_loss: 28.2143 - val_MinusLogProbMetric: 28.2143 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 624/1000
2023-10-25 22:27:18.958 
Epoch 624/1000 
	 loss: 27.1997, MinusLogProbMetric: 27.1997, val_loss: 28.1718, val_MinusLogProbMetric: 28.1718

Epoch 624: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1997 - MinusLogProbMetric: 27.1997 - val_loss: 28.1718 - val_MinusLogProbMetric: 28.1718 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 625/1000
2023-10-25 22:27:53.730 
Epoch 625/1000 
	 loss: 27.1955, MinusLogProbMetric: 27.1955, val_loss: 28.1764, val_MinusLogProbMetric: 28.1764

Epoch 625: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1955 - MinusLogProbMetric: 27.1955 - val_loss: 28.1764 - val_MinusLogProbMetric: 28.1764 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 626/1000
2023-10-25 22:28:28.721 
Epoch 626/1000 
	 loss: 27.1945, MinusLogProbMetric: 27.1945, val_loss: 28.1745, val_MinusLogProbMetric: 28.1745

Epoch 626: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1945 - MinusLogProbMetric: 27.1945 - val_loss: 28.1745 - val_MinusLogProbMetric: 28.1745 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 627/1000
2023-10-25 22:29:03.459 
Epoch 627/1000 
	 loss: 27.1974, MinusLogProbMetric: 27.1974, val_loss: 28.1737, val_MinusLogProbMetric: 28.1737

Epoch 627: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1974 - MinusLogProbMetric: 27.1974 - val_loss: 28.1737 - val_MinusLogProbMetric: 28.1737 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 628/1000
2023-10-25 22:29:33.028 
Epoch 628/1000 
	 loss: 27.1947, MinusLogProbMetric: 27.1947, val_loss: 28.1832, val_MinusLogProbMetric: 28.1832

Epoch 628: val_loss did not improve from 28.16413
196/196 - 30s - loss: 27.1947 - MinusLogProbMetric: 27.1947 - val_loss: 28.1832 - val_MinusLogProbMetric: 28.1832 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 629/1000
2023-10-25 22:30:03.334 
Epoch 629/1000 
	 loss: 27.1935, MinusLogProbMetric: 27.1935, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 629: val_loss did not improve from 28.16413
196/196 - 30s - loss: 27.1935 - MinusLogProbMetric: 27.1935 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 3.1250e-05 - 30s/epoch - 155ms/step
Epoch 630/1000
2023-10-25 22:30:38.361 
Epoch 630/1000 
	 loss: 27.1969, MinusLogProbMetric: 27.1969, val_loss: 28.1787, val_MinusLogProbMetric: 28.1787

Epoch 630: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1969 - MinusLogProbMetric: 27.1969 - val_loss: 28.1787 - val_MinusLogProbMetric: 28.1787 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 631/1000
2023-10-25 22:31:13.285 
Epoch 631/1000 
	 loss: 27.2003, MinusLogProbMetric: 27.2003, val_loss: 28.1709, val_MinusLogProbMetric: 28.1709

Epoch 631: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.2003 - MinusLogProbMetric: 27.2003 - val_loss: 28.1709 - val_MinusLogProbMetric: 28.1709 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 632/1000
2023-10-25 22:31:48.163 
Epoch 632/1000 
	 loss: 27.1948, MinusLogProbMetric: 27.1948, val_loss: 28.2306, val_MinusLogProbMetric: 28.2306

Epoch 632: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1948 - MinusLogProbMetric: 27.1948 - val_loss: 28.2306 - val_MinusLogProbMetric: 28.2306 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 633/1000
2023-10-25 22:32:22.847 
Epoch 633/1000 
	 loss: 27.1961, MinusLogProbMetric: 27.1961, val_loss: 28.2014, val_MinusLogProbMetric: 28.2014

Epoch 633: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1961 - MinusLogProbMetric: 27.1961 - val_loss: 28.2014 - val_MinusLogProbMetric: 28.2014 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 634/1000
2023-10-25 22:32:57.435 
Epoch 634/1000 
	 loss: 27.1969, MinusLogProbMetric: 27.1969, val_loss: 28.1672, val_MinusLogProbMetric: 28.1672

Epoch 634: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1969 - MinusLogProbMetric: 27.1969 - val_loss: 28.1672 - val_MinusLogProbMetric: 28.1672 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 635/1000
2023-10-25 22:33:32.208 
Epoch 635/1000 
	 loss: 27.1958, MinusLogProbMetric: 27.1958, val_loss: 28.1909, val_MinusLogProbMetric: 28.1909

Epoch 635: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1958 - MinusLogProbMetric: 27.1958 - val_loss: 28.1909 - val_MinusLogProbMetric: 28.1909 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 636/1000
2023-10-25 22:34:07.078 
Epoch 636/1000 
	 loss: 27.1933, MinusLogProbMetric: 27.1933, val_loss: 28.1792, val_MinusLogProbMetric: 28.1792

Epoch 636: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1933 - MinusLogProbMetric: 27.1933 - val_loss: 28.1792 - val_MinusLogProbMetric: 28.1792 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 637/1000
2023-10-25 22:34:41.865 
Epoch 637/1000 
	 loss: 27.1941, MinusLogProbMetric: 27.1941, val_loss: 28.1819, val_MinusLogProbMetric: 28.1819

Epoch 637: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1941 - MinusLogProbMetric: 27.1941 - val_loss: 28.1819 - val_MinusLogProbMetric: 28.1819 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 638/1000
2023-10-25 22:35:16.498 
Epoch 638/1000 
	 loss: 27.1928, MinusLogProbMetric: 27.1928, val_loss: 28.1730, val_MinusLogProbMetric: 28.1730

Epoch 638: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1928 - MinusLogProbMetric: 27.1928 - val_loss: 28.1730 - val_MinusLogProbMetric: 28.1730 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 639/1000
2023-10-25 22:35:51.092 
Epoch 639/1000 
	 loss: 27.1964, MinusLogProbMetric: 27.1964, val_loss: 28.1684, val_MinusLogProbMetric: 28.1684

Epoch 639: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1964 - MinusLogProbMetric: 27.1964 - val_loss: 28.1684 - val_MinusLogProbMetric: 28.1684 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 640/1000
2023-10-25 22:36:25.842 
Epoch 640/1000 
	 loss: 27.1976, MinusLogProbMetric: 27.1976, val_loss: 28.1680, val_MinusLogProbMetric: 28.1680

Epoch 640: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1976 - MinusLogProbMetric: 27.1976 - val_loss: 28.1680 - val_MinusLogProbMetric: 28.1680 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 641/1000
2023-10-25 22:37:00.313 
Epoch 641/1000 
	 loss: 27.1949, MinusLogProbMetric: 27.1949, val_loss: 28.1669, val_MinusLogProbMetric: 28.1669

Epoch 641: val_loss did not improve from 28.16413
196/196 - 34s - loss: 27.1949 - MinusLogProbMetric: 27.1949 - val_loss: 28.1669 - val_MinusLogProbMetric: 28.1669 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 642/1000
2023-10-25 22:37:35.173 
Epoch 642/1000 
	 loss: 27.1925, MinusLogProbMetric: 27.1925, val_loss: 28.1794, val_MinusLogProbMetric: 28.1794

Epoch 642: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1925 - MinusLogProbMetric: 27.1925 - val_loss: 28.1794 - val_MinusLogProbMetric: 28.1794 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 643/1000
2023-10-25 22:38:09.747 
Epoch 643/1000 
	 loss: 27.1948, MinusLogProbMetric: 27.1948, val_loss: 28.1680, val_MinusLogProbMetric: 28.1680

Epoch 643: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1948 - MinusLogProbMetric: 27.1948 - val_loss: 28.1680 - val_MinusLogProbMetric: 28.1680 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 644/1000
2023-10-25 22:38:44.445 
Epoch 644/1000 
	 loss: 27.1950, MinusLogProbMetric: 27.1950, val_loss: 28.1796, val_MinusLogProbMetric: 28.1796

Epoch 644: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1950 - MinusLogProbMetric: 27.1950 - val_loss: 28.1796 - val_MinusLogProbMetric: 28.1796 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 645/1000
2023-10-25 22:39:19.309 
Epoch 645/1000 
	 loss: 27.1957, MinusLogProbMetric: 27.1957, val_loss: 28.1755, val_MinusLogProbMetric: 28.1755

Epoch 645: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1957 - MinusLogProbMetric: 27.1957 - val_loss: 28.1755 - val_MinusLogProbMetric: 28.1755 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 646/1000
2023-10-25 22:39:54.033 
Epoch 646/1000 
	 loss: 27.1942, MinusLogProbMetric: 27.1942, val_loss: 28.1709, val_MinusLogProbMetric: 28.1709

Epoch 646: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1942 - MinusLogProbMetric: 27.1942 - val_loss: 28.1709 - val_MinusLogProbMetric: 28.1709 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 647/1000
2023-10-25 22:40:28.675 
Epoch 647/1000 
	 loss: 27.1946, MinusLogProbMetric: 27.1946, val_loss: 28.1718, val_MinusLogProbMetric: 28.1718

Epoch 647: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1946 - MinusLogProbMetric: 27.1946 - val_loss: 28.1718 - val_MinusLogProbMetric: 28.1718 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 648/1000
2023-10-25 22:41:03.584 
Epoch 648/1000 
	 loss: 27.1840, MinusLogProbMetric: 27.1840, val_loss: 28.1757, val_MinusLogProbMetric: 28.1757

Epoch 648: val_loss did not improve from 28.16413
196/196 - 35s - loss: 27.1840 - MinusLogProbMetric: 27.1840 - val_loss: 28.1757 - val_MinusLogProbMetric: 28.1757 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 649/1000
2023-10-25 22:41:38.521 
Epoch 649/1000 
	 loss: 27.1823, MinusLogProbMetric: 27.1823, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 649: val_loss improved from 28.16413 to 28.16316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.1823 - MinusLogProbMetric: 27.1823 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 650/1000
2023-10-25 22:42:13.990 
Epoch 650/1000 
	 loss: 27.1831, MinusLogProbMetric: 27.1831, val_loss: 28.1709, val_MinusLogProbMetric: 28.1709

Epoch 650: val_loss did not improve from 28.16316
196/196 - 35s - loss: 27.1831 - MinusLogProbMetric: 27.1831 - val_loss: 28.1709 - val_MinusLogProbMetric: 28.1709 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 651/1000
2023-10-25 22:42:48.706 
Epoch 651/1000 
	 loss: 27.1830, MinusLogProbMetric: 27.1830, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 651: val_loss improved from 28.16316 to 28.16294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.1830 - MinusLogProbMetric: 27.1830 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 652/1000
2023-10-25 22:43:23.863 
Epoch 652/1000 
	 loss: 27.1845, MinusLogProbMetric: 27.1845, val_loss: 28.1701, val_MinusLogProbMetric: 28.1701

Epoch 652: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1845 - MinusLogProbMetric: 27.1845 - val_loss: 28.1701 - val_MinusLogProbMetric: 28.1701 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 653/1000
2023-10-25 22:43:58.551 
Epoch 653/1000 
	 loss: 27.1840, MinusLogProbMetric: 27.1840, val_loss: 28.1662, val_MinusLogProbMetric: 28.1662

Epoch 653: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1840 - MinusLogProbMetric: 27.1840 - val_loss: 28.1662 - val_MinusLogProbMetric: 28.1662 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 654/1000
2023-10-25 22:44:33.266 
Epoch 654/1000 
	 loss: 27.1824, MinusLogProbMetric: 27.1824, val_loss: 28.1694, val_MinusLogProbMetric: 28.1694

Epoch 654: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1824 - MinusLogProbMetric: 27.1824 - val_loss: 28.1694 - val_MinusLogProbMetric: 28.1694 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 655/1000
2023-10-25 22:45:08.231 
Epoch 655/1000 
	 loss: 27.1825, MinusLogProbMetric: 27.1825, val_loss: 28.1711, val_MinusLogProbMetric: 28.1711

Epoch 655: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1825 - MinusLogProbMetric: 27.1825 - val_loss: 28.1711 - val_MinusLogProbMetric: 28.1711 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 656/1000
2023-10-25 22:45:42.946 
Epoch 656/1000 
	 loss: 27.1818, MinusLogProbMetric: 27.1818, val_loss: 28.1731, val_MinusLogProbMetric: 28.1731

Epoch 656: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1818 - MinusLogProbMetric: 27.1818 - val_loss: 28.1731 - val_MinusLogProbMetric: 28.1731 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 657/1000
2023-10-25 22:46:17.648 
Epoch 657/1000 
	 loss: 27.1839, MinusLogProbMetric: 27.1839, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 657: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1839 - MinusLogProbMetric: 27.1839 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 658/1000
2023-10-25 22:46:52.410 
Epoch 658/1000 
	 loss: 27.1812, MinusLogProbMetric: 27.1812, val_loss: 28.1681, val_MinusLogProbMetric: 28.1681

Epoch 658: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1812 - MinusLogProbMetric: 27.1812 - val_loss: 28.1681 - val_MinusLogProbMetric: 28.1681 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 659/1000
2023-10-25 22:47:27.071 
Epoch 659/1000 
	 loss: 27.1826, MinusLogProbMetric: 27.1826, val_loss: 28.1901, val_MinusLogProbMetric: 28.1901

Epoch 659: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1826 - MinusLogProbMetric: 27.1826 - val_loss: 28.1901 - val_MinusLogProbMetric: 28.1901 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 660/1000
2023-10-25 22:48:01.835 
Epoch 660/1000 
	 loss: 27.1843, MinusLogProbMetric: 27.1843, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 660: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1843 - MinusLogProbMetric: 27.1843 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 661/1000
2023-10-25 22:48:36.633 
Epoch 661/1000 
	 loss: 27.1841, MinusLogProbMetric: 27.1841, val_loss: 28.1818, val_MinusLogProbMetric: 28.1818

Epoch 661: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1841 - MinusLogProbMetric: 27.1841 - val_loss: 28.1818 - val_MinusLogProbMetric: 28.1818 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 662/1000
2023-10-25 22:49:11.502 
Epoch 662/1000 
	 loss: 27.1836, MinusLogProbMetric: 27.1836, val_loss: 28.1818, val_MinusLogProbMetric: 28.1818

Epoch 662: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1836 - MinusLogProbMetric: 27.1836 - val_loss: 28.1818 - val_MinusLogProbMetric: 28.1818 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 663/1000
2023-10-25 22:49:46.338 
Epoch 663/1000 
	 loss: 27.1833, MinusLogProbMetric: 27.1833, val_loss: 28.1796, val_MinusLogProbMetric: 28.1796

Epoch 663: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1833 - MinusLogProbMetric: 27.1833 - val_loss: 28.1796 - val_MinusLogProbMetric: 28.1796 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 664/1000
2023-10-25 22:50:21.150 
Epoch 664/1000 
	 loss: 27.1820, MinusLogProbMetric: 27.1820, val_loss: 28.1793, val_MinusLogProbMetric: 28.1793

Epoch 664: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1820 - MinusLogProbMetric: 27.1820 - val_loss: 28.1793 - val_MinusLogProbMetric: 28.1793 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 665/1000
2023-10-25 22:50:55.962 
Epoch 665/1000 
	 loss: 27.1817, MinusLogProbMetric: 27.1817, val_loss: 28.1784, val_MinusLogProbMetric: 28.1784

Epoch 665: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1817 - MinusLogProbMetric: 27.1817 - val_loss: 28.1784 - val_MinusLogProbMetric: 28.1784 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 666/1000
2023-10-25 22:51:30.649 
Epoch 666/1000 
	 loss: 27.1841, MinusLogProbMetric: 27.1841, val_loss: 28.1730, val_MinusLogProbMetric: 28.1730

Epoch 666: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1841 - MinusLogProbMetric: 27.1841 - val_loss: 28.1730 - val_MinusLogProbMetric: 28.1730 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 667/1000
2023-10-25 22:52:05.749 
Epoch 667/1000 
	 loss: 27.1835, MinusLogProbMetric: 27.1835, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 667: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1835 - MinusLogProbMetric: 27.1835 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 668/1000
2023-10-25 22:52:40.783 
Epoch 668/1000 
	 loss: 27.1811, MinusLogProbMetric: 27.1811, val_loss: 28.1637, val_MinusLogProbMetric: 28.1637

Epoch 668: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1811 - MinusLogProbMetric: 27.1811 - val_loss: 28.1637 - val_MinusLogProbMetric: 28.1637 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 669/1000
2023-10-25 22:53:15.662 
Epoch 669/1000 
	 loss: 27.1827, MinusLogProbMetric: 27.1827, val_loss: 28.1706, val_MinusLogProbMetric: 28.1706

Epoch 669: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1827 - MinusLogProbMetric: 27.1827 - val_loss: 28.1706 - val_MinusLogProbMetric: 28.1706 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 670/1000
2023-10-25 22:53:50.779 
Epoch 670/1000 
	 loss: 27.1824, MinusLogProbMetric: 27.1824, val_loss: 28.1734, val_MinusLogProbMetric: 28.1734

Epoch 670: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1824 - MinusLogProbMetric: 27.1824 - val_loss: 28.1734 - val_MinusLogProbMetric: 28.1734 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 671/1000
2023-10-25 22:54:25.653 
Epoch 671/1000 
	 loss: 27.1814, MinusLogProbMetric: 27.1814, val_loss: 28.1753, val_MinusLogProbMetric: 28.1753

Epoch 671: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1814 - MinusLogProbMetric: 27.1814 - val_loss: 28.1753 - val_MinusLogProbMetric: 28.1753 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 672/1000
2023-10-25 22:55:00.473 
Epoch 672/1000 
	 loss: 27.1835, MinusLogProbMetric: 27.1835, val_loss: 28.1857, val_MinusLogProbMetric: 28.1857

Epoch 672: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1835 - MinusLogProbMetric: 27.1835 - val_loss: 28.1857 - val_MinusLogProbMetric: 28.1857 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 673/1000
2023-10-25 22:55:35.328 
Epoch 673/1000 
	 loss: 27.1818, MinusLogProbMetric: 27.1818, val_loss: 28.1644, val_MinusLogProbMetric: 28.1644

Epoch 673: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1818 - MinusLogProbMetric: 27.1818 - val_loss: 28.1644 - val_MinusLogProbMetric: 28.1644 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 674/1000
2023-10-25 22:56:10.149 
Epoch 674/1000 
	 loss: 27.1819, MinusLogProbMetric: 27.1819, val_loss: 28.1634, val_MinusLogProbMetric: 28.1634

Epoch 674: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1819 - MinusLogProbMetric: 27.1819 - val_loss: 28.1634 - val_MinusLogProbMetric: 28.1634 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 675/1000
2023-10-25 22:56:44.904 
Epoch 675/1000 
	 loss: 27.1819, MinusLogProbMetric: 27.1819, val_loss: 28.1687, val_MinusLogProbMetric: 28.1687

Epoch 675: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1819 - MinusLogProbMetric: 27.1819 - val_loss: 28.1687 - val_MinusLogProbMetric: 28.1687 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 676/1000
2023-10-25 22:57:20.011 
Epoch 676/1000 
	 loss: 27.1833, MinusLogProbMetric: 27.1833, val_loss: 28.1661, val_MinusLogProbMetric: 28.1661

Epoch 676: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1833 - MinusLogProbMetric: 27.1833 - val_loss: 28.1661 - val_MinusLogProbMetric: 28.1661 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 677/1000
2023-10-25 22:57:54.763 
Epoch 677/1000 
	 loss: 27.1813, MinusLogProbMetric: 27.1813, val_loss: 28.1682, val_MinusLogProbMetric: 28.1682

Epoch 677: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1813 - MinusLogProbMetric: 27.1813 - val_loss: 28.1682 - val_MinusLogProbMetric: 28.1682 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 678/1000
2023-10-25 22:58:29.570 
Epoch 678/1000 
	 loss: 27.1826, MinusLogProbMetric: 27.1826, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 678: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1826 - MinusLogProbMetric: 27.1826 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 679/1000
2023-10-25 22:59:04.536 
Epoch 679/1000 
	 loss: 27.1824, MinusLogProbMetric: 27.1824, val_loss: 28.1673, val_MinusLogProbMetric: 28.1673

Epoch 679: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1824 - MinusLogProbMetric: 27.1824 - val_loss: 28.1673 - val_MinusLogProbMetric: 28.1673 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 680/1000
2023-10-25 22:59:39.379 
Epoch 680/1000 
	 loss: 27.1818, MinusLogProbMetric: 27.1818, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 680: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1818 - MinusLogProbMetric: 27.1818 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 681/1000
2023-10-25 23:00:14.382 
Epoch 681/1000 
	 loss: 27.1827, MinusLogProbMetric: 27.1827, val_loss: 28.1678, val_MinusLogProbMetric: 28.1678

Epoch 681: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1827 - MinusLogProbMetric: 27.1827 - val_loss: 28.1678 - val_MinusLogProbMetric: 28.1678 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 682/1000
2023-10-25 23:00:48.983 
Epoch 682/1000 
	 loss: 27.1835, MinusLogProbMetric: 27.1835, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 682: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1835 - MinusLogProbMetric: 27.1835 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 683/1000
2023-10-25 23:01:24.129 
Epoch 683/1000 
	 loss: 27.1834, MinusLogProbMetric: 27.1834, val_loss: 28.1729, val_MinusLogProbMetric: 28.1729

Epoch 683: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1834 - MinusLogProbMetric: 27.1834 - val_loss: 28.1729 - val_MinusLogProbMetric: 28.1729 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 684/1000
2023-10-25 23:01:58.801 
Epoch 684/1000 
	 loss: 27.1835, MinusLogProbMetric: 27.1835, val_loss: 28.1679, val_MinusLogProbMetric: 28.1679

Epoch 684: val_loss did not improve from 28.16294
196/196 - 35s - loss: 27.1835 - MinusLogProbMetric: 27.1835 - val_loss: 28.1679 - val_MinusLogProbMetric: 28.1679 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 685/1000
2023-10-25 23:02:33.510 
Epoch 685/1000 
	 loss: 27.1816, MinusLogProbMetric: 27.1816, val_loss: 28.1612, val_MinusLogProbMetric: 28.1612

Epoch 685: val_loss improved from 28.16294 to 28.16124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_362/weights/best_weights.h5
196/196 - 35s - loss: 27.1816 - MinusLogProbMetric: 27.1816 - val_loss: 28.1612 - val_MinusLogProbMetric: 28.1612 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 686/1000
2023-10-25 23:03:09.027 
Epoch 686/1000 
	 loss: 27.1808, MinusLogProbMetric: 27.1808, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 686: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1808 - MinusLogProbMetric: 27.1808 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 687/1000
2023-10-25 23:03:43.880 
Epoch 687/1000 
	 loss: 27.1820, MinusLogProbMetric: 27.1820, val_loss: 28.1684, val_MinusLogProbMetric: 28.1684

Epoch 687: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1820 - MinusLogProbMetric: 27.1820 - val_loss: 28.1684 - val_MinusLogProbMetric: 28.1684 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 688/1000
2023-10-25 23:04:19.053 
Epoch 688/1000 
	 loss: 27.1825, MinusLogProbMetric: 27.1825, val_loss: 28.1793, val_MinusLogProbMetric: 28.1793

Epoch 688: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1825 - MinusLogProbMetric: 27.1825 - val_loss: 28.1793 - val_MinusLogProbMetric: 28.1793 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 689/1000
2023-10-25 23:04:53.900 
Epoch 689/1000 
	 loss: 27.1832, MinusLogProbMetric: 27.1832, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 689: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1832 - MinusLogProbMetric: 27.1832 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 690/1000
2023-10-25 23:05:28.674 
Epoch 690/1000 
	 loss: 27.1819, MinusLogProbMetric: 27.1819, val_loss: 28.1679, val_MinusLogProbMetric: 28.1679

Epoch 690: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1819 - MinusLogProbMetric: 27.1819 - val_loss: 28.1679 - val_MinusLogProbMetric: 28.1679 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 691/1000
2023-10-25 23:06:03.520 
Epoch 691/1000 
	 loss: 27.1833, MinusLogProbMetric: 27.1833, val_loss: 28.1645, val_MinusLogProbMetric: 28.1645

Epoch 691: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1833 - MinusLogProbMetric: 27.1833 - val_loss: 28.1645 - val_MinusLogProbMetric: 28.1645 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 692/1000
2023-10-25 23:06:38.818 
Epoch 692/1000 
	 loss: 27.1823, MinusLogProbMetric: 27.1823, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 692: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1823 - MinusLogProbMetric: 27.1823 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 693/1000
2023-10-25 23:07:13.518 
Epoch 693/1000 
	 loss: 27.1805, MinusLogProbMetric: 27.1805, val_loss: 28.1662, val_MinusLogProbMetric: 28.1662

Epoch 693: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1805 - MinusLogProbMetric: 27.1805 - val_loss: 28.1662 - val_MinusLogProbMetric: 28.1662 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 694/1000
2023-10-25 23:07:48.229 
Epoch 694/1000 
	 loss: 27.1825, MinusLogProbMetric: 27.1825, val_loss: 28.1683, val_MinusLogProbMetric: 28.1683

Epoch 694: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1825 - MinusLogProbMetric: 27.1825 - val_loss: 28.1683 - val_MinusLogProbMetric: 28.1683 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 695/1000
2023-10-25 23:08:23.065 
Epoch 695/1000 
	 loss: 27.1817, MinusLogProbMetric: 27.1817, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 695: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1817 - MinusLogProbMetric: 27.1817 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 696/1000
2023-10-25 23:08:57.849 
Epoch 696/1000 
	 loss: 27.1836, MinusLogProbMetric: 27.1836, val_loss: 28.1696, val_MinusLogProbMetric: 28.1696

Epoch 696: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1836 - MinusLogProbMetric: 27.1836 - val_loss: 28.1696 - val_MinusLogProbMetric: 28.1696 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 697/1000
2023-10-25 23:09:32.794 
Epoch 697/1000 
	 loss: 27.1807, MinusLogProbMetric: 27.1807, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 697: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1807 - MinusLogProbMetric: 27.1807 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 698/1000
2023-10-25 23:10:07.599 
Epoch 698/1000 
	 loss: 27.1825, MinusLogProbMetric: 27.1825, val_loss: 28.1709, val_MinusLogProbMetric: 28.1709

Epoch 698: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1825 - MinusLogProbMetric: 27.1825 - val_loss: 28.1709 - val_MinusLogProbMetric: 28.1709 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 699/1000
2023-10-25 23:10:42.833 
Epoch 699/1000 
	 loss: 27.1820, MinusLogProbMetric: 27.1820, val_loss: 28.1732, val_MinusLogProbMetric: 28.1732

Epoch 699: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1820 - MinusLogProbMetric: 27.1820 - val_loss: 28.1732 - val_MinusLogProbMetric: 28.1732 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 700/1000
2023-10-25 23:11:17.981 
Epoch 700/1000 
	 loss: 27.1808, MinusLogProbMetric: 27.1808, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 700: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1808 - MinusLogProbMetric: 27.1808 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 701/1000
2023-10-25 23:11:53.008 
Epoch 701/1000 
	 loss: 27.1824, MinusLogProbMetric: 27.1824, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 701: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1824 - MinusLogProbMetric: 27.1824 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 702/1000
2023-10-25 23:12:27.924 
Epoch 702/1000 
	 loss: 27.1821, MinusLogProbMetric: 27.1821, val_loss: 28.1782, val_MinusLogProbMetric: 28.1782

Epoch 702: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1821 - MinusLogProbMetric: 27.1821 - val_loss: 28.1782 - val_MinusLogProbMetric: 28.1782 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 703/1000
2023-10-25 23:13:02.508 
Epoch 703/1000 
	 loss: 27.1820, MinusLogProbMetric: 27.1820, val_loss: 28.1655, val_MinusLogProbMetric: 28.1655

Epoch 703: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1820 - MinusLogProbMetric: 27.1820 - val_loss: 28.1655 - val_MinusLogProbMetric: 28.1655 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 704/1000
2023-10-25 23:13:37.391 
Epoch 704/1000 
	 loss: 27.1819, MinusLogProbMetric: 27.1819, val_loss: 28.1857, val_MinusLogProbMetric: 28.1857

Epoch 704: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1819 - MinusLogProbMetric: 27.1819 - val_loss: 28.1857 - val_MinusLogProbMetric: 28.1857 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 705/1000
2023-10-25 23:14:12.050 
Epoch 705/1000 
	 loss: 27.1822, MinusLogProbMetric: 27.1822, val_loss: 28.1661, val_MinusLogProbMetric: 28.1661

Epoch 705: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1822 - MinusLogProbMetric: 27.1822 - val_loss: 28.1661 - val_MinusLogProbMetric: 28.1661 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 706/1000
2023-10-25 23:14:47.049 
Epoch 706/1000 
	 loss: 27.1827, MinusLogProbMetric: 27.1827, val_loss: 28.1901, val_MinusLogProbMetric: 28.1901

Epoch 706: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1827 - MinusLogProbMetric: 27.1827 - val_loss: 28.1901 - val_MinusLogProbMetric: 28.1901 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 707/1000
2023-10-25 23:15:22.034 
Epoch 707/1000 
	 loss: 27.1818, MinusLogProbMetric: 27.1818, val_loss: 28.1661, val_MinusLogProbMetric: 28.1661

Epoch 707: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1818 - MinusLogProbMetric: 27.1818 - val_loss: 28.1661 - val_MinusLogProbMetric: 28.1661 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 708/1000
2023-10-25 23:15:56.841 
Epoch 708/1000 
	 loss: 27.1800, MinusLogProbMetric: 27.1800, val_loss: 28.1785, val_MinusLogProbMetric: 28.1785

Epoch 708: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1800 - MinusLogProbMetric: 27.1800 - val_loss: 28.1785 - val_MinusLogProbMetric: 28.1785 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 709/1000
2023-10-25 23:16:32.043 
Epoch 709/1000 
	 loss: 27.1799, MinusLogProbMetric: 27.1799, val_loss: 28.1702, val_MinusLogProbMetric: 28.1702

Epoch 709: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1799 - MinusLogProbMetric: 27.1799 - val_loss: 28.1702 - val_MinusLogProbMetric: 28.1702 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 710/1000
2023-10-25 23:17:06.948 
Epoch 710/1000 
	 loss: 27.1812, MinusLogProbMetric: 27.1812, val_loss: 28.1773, val_MinusLogProbMetric: 28.1773

Epoch 710: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1812 - MinusLogProbMetric: 27.1812 - val_loss: 28.1773 - val_MinusLogProbMetric: 28.1773 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 711/1000
2023-10-25 23:17:41.596 
Epoch 711/1000 
	 loss: 27.1815, MinusLogProbMetric: 27.1815, val_loss: 28.1676, val_MinusLogProbMetric: 28.1676

Epoch 711: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1815 - MinusLogProbMetric: 27.1815 - val_loss: 28.1676 - val_MinusLogProbMetric: 28.1676 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 712/1000
2023-10-25 23:18:16.450 
Epoch 712/1000 
	 loss: 27.1802, MinusLogProbMetric: 27.1802, val_loss: 28.1706, val_MinusLogProbMetric: 28.1706

Epoch 712: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1802 - MinusLogProbMetric: 27.1802 - val_loss: 28.1706 - val_MinusLogProbMetric: 28.1706 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 713/1000
2023-10-25 23:18:50.997 
Epoch 713/1000 
	 loss: 27.1815, MinusLogProbMetric: 27.1815, val_loss: 28.1849, val_MinusLogProbMetric: 28.1849

Epoch 713: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1815 - MinusLogProbMetric: 27.1815 - val_loss: 28.1849 - val_MinusLogProbMetric: 28.1849 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 714/1000
2023-10-25 23:19:26.118 
Epoch 714/1000 
	 loss: 27.1801, MinusLogProbMetric: 27.1801, val_loss: 28.1805, val_MinusLogProbMetric: 28.1805

Epoch 714: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1801 - MinusLogProbMetric: 27.1801 - val_loss: 28.1805 - val_MinusLogProbMetric: 28.1805 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 715/1000
2023-10-25 23:20:01.053 
Epoch 715/1000 
	 loss: 27.1800, MinusLogProbMetric: 27.1800, val_loss: 28.1680, val_MinusLogProbMetric: 28.1680

Epoch 715: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1800 - MinusLogProbMetric: 27.1800 - val_loss: 28.1680 - val_MinusLogProbMetric: 28.1680 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 716/1000
2023-10-25 23:20:35.843 
Epoch 716/1000 
	 loss: 27.1822, MinusLogProbMetric: 27.1822, val_loss: 28.1678, val_MinusLogProbMetric: 28.1678

Epoch 716: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1822 - MinusLogProbMetric: 27.1822 - val_loss: 28.1678 - val_MinusLogProbMetric: 28.1678 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 717/1000
2023-10-25 23:21:10.959 
Epoch 717/1000 
	 loss: 27.1819, MinusLogProbMetric: 27.1819, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 717: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1819 - MinusLogProbMetric: 27.1819 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 718/1000
2023-10-25 23:21:46.062 
Epoch 718/1000 
	 loss: 27.1817, MinusLogProbMetric: 27.1817, val_loss: 28.1741, val_MinusLogProbMetric: 28.1741

Epoch 718: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1817 - MinusLogProbMetric: 27.1817 - val_loss: 28.1741 - val_MinusLogProbMetric: 28.1741 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 719/1000
2023-10-25 23:22:21.046 
Epoch 719/1000 
	 loss: 27.1796, MinusLogProbMetric: 27.1796, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 719: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1796 - MinusLogProbMetric: 27.1796 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 720/1000
2023-10-25 23:22:55.899 
Epoch 720/1000 
	 loss: 27.1812, MinusLogProbMetric: 27.1812, val_loss: 28.1686, val_MinusLogProbMetric: 28.1686

Epoch 720: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1812 - MinusLogProbMetric: 27.1812 - val_loss: 28.1686 - val_MinusLogProbMetric: 28.1686 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 721/1000
2023-10-25 23:23:30.781 
Epoch 721/1000 
	 loss: 27.1806, MinusLogProbMetric: 27.1806, val_loss: 28.1689, val_MinusLogProbMetric: 28.1689

Epoch 721: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1806 - MinusLogProbMetric: 27.1806 - val_loss: 28.1689 - val_MinusLogProbMetric: 28.1689 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 722/1000
2023-10-25 23:24:05.737 
Epoch 722/1000 
	 loss: 27.1799, MinusLogProbMetric: 27.1799, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 722: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1799 - MinusLogProbMetric: 27.1799 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 723/1000
2023-10-25 23:24:40.797 
Epoch 723/1000 
	 loss: 27.1798, MinusLogProbMetric: 27.1798, val_loss: 28.1751, val_MinusLogProbMetric: 28.1751

Epoch 723: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1798 - MinusLogProbMetric: 27.1798 - val_loss: 28.1751 - val_MinusLogProbMetric: 28.1751 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 724/1000
2023-10-25 23:25:15.618 
Epoch 724/1000 
	 loss: 27.1809, MinusLogProbMetric: 27.1809, val_loss: 28.1703, val_MinusLogProbMetric: 28.1703

Epoch 724: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1809 - MinusLogProbMetric: 27.1809 - val_loss: 28.1703 - val_MinusLogProbMetric: 28.1703 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 725/1000
2023-10-25 23:25:50.635 
Epoch 725/1000 
	 loss: 27.1804, MinusLogProbMetric: 27.1804, val_loss: 28.1731, val_MinusLogProbMetric: 28.1731

Epoch 725: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1804 - MinusLogProbMetric: 27.1804 - val_loss: 28.1731 - val_MinusLogProbMetric: 28.1731 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 726/1000
2023-10-25 23:26:25.528 
Epoch 726/1000 
	 loss: 27.1796, MinusLogProbMetric: 27.1796, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 726: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1796 - MinusLogProbMetric: 27.1796 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 727/1000
2023-10-25 23:27:00.312 
Epoch 727/1000 
	 loss: 27.1809, MinusLogProbMetric: 27.1809, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 727: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1809 - MinusLogProbMetric: 27.1809 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 728/1000
2023-10-25 23:27:35.308 
Epoch 728/1000 
	 loss: 27.1782, MinusLogProbMetric: 27.1782, val_loss: 28.1671, val_MinusLogProbMetric: 28.1671

Epoch 728: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1782 - MinusLogProbMetric: 27.1782 - val_loss: 28.1671 - val_MinusLogProbMetric: 28.1671 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 729/1000
2023-10-25 23:28:10.194 
Epoch 729/1000 
	 loss: 27.1802, MinusLogProbMetric: 27.1802, val_loss: 28.1704, val_MinusLogProbMetric: 28.1704

Epoch 729: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1802 - MinusLogProbMetric: 27.1802 - val_loss: 28.1704 - val_MinusLogProbMetric: 28.1704 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 730/1000
2023-10-25 23:28:44.923 
Epoch 730/1000 
	 loss: 27.1812, MinusLogProbMetric: 27.1812, val_loss: 28.1687, val_MinusLogProbMetric: 28.1687

Epoch 730: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1812 - MinusLogProbMetric: 27.1812 - val_loss: 28.1687 - val_MinusLogProbMetric: 28.1687 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 731/1000
2023-10-25 23:29:19.881 
Epoch 731/1000 
	 loss: 27.1806, MinusLogProbMetric: 27.1806, val_loss: 28.1666, val_MinusLogProbMetric: 28.1666

Epoch 731: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1806 - MinusLogProbMetric: 27.1806 - val_loss: 28.1666 - val_MinusLogProbMetric: 28.1666 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 732/1000
2023-10-25 23:29:54.804 
Epoch 732/1000 
	 loss: 27.1782, MinusLogProbMetric: 27.1782, val_loss: 28.1657, val_MinusLogProbMetric: 28.1657

Epoch 732: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1782 - MinusLogProbMetric: 27.1782 - val_loss: 28.1657 - val_MinusLogProbMetric: 28.1657 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 733/1000
2023-10-25 23:30:29.852 
Epoch 733/1000 
	 loss: 27.1778, MinusLogProbMetric: 27.1778, val_loss: 28.1702, val_MinusLogProbMetric: 28.1702

Epoch 733: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1778 - MinusLogProbMetric: 27.1778 - val_loss: 28.1702 - val_MinusLogProbMetric: 28.1702 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 734/1000
2023-10-25 23:31:04.912 
Epoch 734/1000 
	 loss: 27.1815, MinusLogProbMetric: 27.1815, val_loss: 28.1769, val_MinusLogProbMetric: 28.1769

Epoch 734: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1815 - MinusLogProbMetric: 27.1815 - val_loss: 28.1769 - val_MinusLogProbMetric: 28.1769 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 735/1000
2023-10-25 23:31:39.618 
Epoch 735/1000 
	 loss: 27.1804, MinusLogProbMetric: 27.1804, val_loss: 28.1780, val_MinusLogProbMetric: 28.1780

Epoch 735: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1804 - MinusLogProbMetric: 27.1804 - val_loss: 28.1780 - val_MinusLogProbMetric: 28.1780 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 736/1000
2023-10-25 23:32:14.502 
Epoch 736/1000 
	 loss: 27.1743, MinusLogProbMetric: 27.1743, val_loss: 28.1681, val_MinusLogProbMetric: 28.1681

Epoch 736: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1743 - MinusLogProbMetric: 27.1743 - val_loss: 28.1681 - val_MinusLogProbMetric: 28.1681 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 737/1000
2023-10-25 23:32:49.402 
Epoch 737/1000 
	 loss: 27.1739, MinusLogProbMetric: 27.1739, val_loss: 28.1681, val_MinusLogProbMetric: 28.1681

Epoch 737: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1739 - MinusLogProbMetric: 27.1739 - val_loss: 28.1681 - val_MinusLogProbMetric: 28.1681 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 738/1000
2023-10-25 23:33:24.311 
Epoch 738/1000 
	 loss: 27.1744, MinusLogProbMetric: 27.1744, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 738: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1744 - MinusLogProbMetric: 27.1744 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 739/1000
2023-10-25 23:33:59.022 
Epoch 739/1000 
	 loss: 27.1736, MinusLogProbMetric: 27.1736, val_loss: 28.1639, val_MinusLogProbMetric: 28.1639

Epoch 739: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1736 - MinusLogProbMetric: 27.1736 - val_loss: 28.1639 - val_MinusLogProbMetric: 28.1639 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 740/1000
2023-10-25 23:34:34.053 
Epoch 740/1000 
	 loss: 27.1739, MinusLogProbMetric: 27.1739, val_loss: 28.1626, val_MinusLogProbMetric: 28.1626

Epoch 740: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1739 - MinusLogProbMetric: 27.1739 - val_loss: 28.1626 - val_MinusLogProbMetric: 28.1626 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 741/1000
2023-10-25 23:35:09.094 
Epoch 741/1000 
	 loss: 27.1739, MinusLogProbMetric: 27.1739, val_loss: 28.1669, val_MinusLogProbMetric: 28.1669

Epoch 741: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1739 - MinusLogProbMetric: 27.1739 - val_loss: 28.1669 - val_MinusLogProbMetric: 28.1669 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 742/1000
2023-10-25 23:35:44.006 
Epoch 742/1000 
	 loss: 27.1745, MinusLogProbMetric: 27.1745, val_loss: 28.1685, val_MinusLogProbMetric: 28.1685

Epoch 742: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1745 - MinusLogProbMetric: 27.1745 - val_loss: 28.1685 - val_MinusLogProbMetric: 28.1685 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 743/1000
2023-10-25 23:36:18.963 
Epoch 743/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.1734, val_MinusLogProbMetric: 28.1734

Epoch 743: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.1734 - val_MinusLogProbMetric: 28.1734 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 744/1000
2023-10-25 23:36:53.939 
Epoch 744/1000 
	 loss: 27.1736, MinusLogProbMetric: 27.1736, val_loss: 28.1659, val_MinusLogProbMetric: 28.1659

Epoch 744: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1736 - MinusLogProbMetric: 27.1736 - val_loss: 28.1659 - val_MinusLogProbMetric: 28.1659 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 745/1000
2023-10-25 23:37:28.690 
Epoch 745/1000 
	 loss: 27.1738, MinusLogProbMetric: 27.1738, val_loss: 28.1646, val_MinusLogProbMetric: 28.1646

Epoch 745: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1738 - MinusLogProbMetric: 27.1738 - val_loss: 28.1646 - val_MinusLogProbMetric: 28.1646 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 746/1000
2023-10-25 23:38:03.509 
Epoch 746/1000 
	 loss: 27.1742, MinusLogProbMetric: 27.1742, val_loss: 28.1657, val_MinusLogProbMetric: 28.1657

Epoch 746: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1742 - MinusLogProbMetric: 27.1742 - val_loss: 28.1657 - val_MinusLogProbMetric: 28.1657 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 747/1000
2023-10-25 23:38:38.768 
Epoch 747/1000 
	 loss: 27.1740, MinusLogProbMetric: 27.1740, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 747: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1740 - MinusLogProbMetric: 27.1740 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 7.8125e-06 - 35s/epoch - 180ms/step
Epoch 748/1000
2023-10-25 23:39:13.888 
Epoch 748/1000 
	 loss: 27.1740, MinusLogProbMetric: 27.1740, val_loss: 28.1650, val_MinusLogProbMetric: 28.1650

Epoch 748: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1740 - MinusLogProbMetric: 27.1740 - val_loss: 28.1650 - val_MinusLogProbMetric: 28.1650 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 749/1000
2023-10-25 23:39:49.024 
Epoch 749/1000 
	 loss: 27.1736, MinusLogProbMetric: 27.1736, val_loss: 28.1674, val_MinusLogProbMetric: 28.1674

Epoch 749: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1736 - MinusLogProbMetric: 27.1736 - val_loss: 28.1674 - val_MinusLogProbMetric: 28.1674 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 750/1000
2023-10-25 23:40:23.853 
Epoch 750/1000 
	 loss: 27.1738, MinusLogProbMetric: 27.1738, val_loss: 28.1663, val_MinusLogProbMetric: 28.1663

Epoch 750: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1738 - MinusLogProbMetric: 27.1738 - val_loss: 28.1663 - val_MinusLogProbMetric: 28.1663 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 751/1000
2023-10-25 23:40:58.652 
Epoch 751/1000 
	 loss: 27.1744, MinusLogProbMetric: 27.1744, val_loss: 28.1679, val_MinusLogProbMetric: 28.1679

Epoch 751: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1744 - MinusLogProbMetric: 27.1744 - val_loss: 28.1679 - val_MinusLogProbMetric: 28.1679 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 752/1000
2023-10-25 23:41:33.582 
Epoch 752/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 752: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 753/1000
2023-10-25 23:42:08.677 
Epoch 753/1000 
	 loss: 27.1726, MinusLogProbMetric: 27.1726, val_loss: 28.1693, val_MinusLogProbMetric: 28.1693

Epoch 753: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1726 - MinusLogProbMetric: 27.1726 - val_loss: 28.1693 - val_MinusLogProbMetric: 28.1693 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 754/1000
2023-10-25 23:42:43.761 
Epoch 754/1000 
	 loss: 27.1732, MinusLogProbMetric: 27.1732, val_loss: 28.1649, val_MinusLogProbMetric: 28.1649

Epoch 754: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1732 - MinusLogProbMetric: 27.1732 - val_loss: 28.1649 - val_MinusLogProbMetric: 28.1649 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 755/1000
2023-10-25 23:43:18.866 
Epoch 755/1000 
	 loss: 27.1738, MinusLogProbMetric: 27.1738, val_loss: 28.1655, val_MinusLogProbMetric: 28.1655

Epoch 755: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1738 - MinusLogProbMetric: 27.1738 - val_loss: 28.1655 - val_MinusLogProbMetric: 28.1655 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 756/1000
2023-10-25 23:43:53.770 
Epoch 756/1000 
	 loss: 27.1738, MinusLogProbMetric: 27.1738, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 756: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1738 - MinusLogProbMetric: 27.1738 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 757/1000
2023-10-25 23:44:28.838 
Epoch 757/1000 
	 loss: 27.1730, MinusLogProbMetric: 27.1730, val_loss: 28.1673, val_MinusLogProbMetric: 28.1673

Epoch 757: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1730 - MinusLogProbMetric: 27.1730 - val_loss: 28.1673 - val_MinusLogProbMetric: 28.1673 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 758/1000
2023-10-25 23:45:03.622 
Epoch 758/1000 
	 loss: 27.1735, MinusLogProbMetric: 27.1735, val_loss: 28.1676, val_MinusLogProbMetric: 28.1676

Epoch 758: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1735 - MinusLogProbMetric: 27.1735 - val_loss: 28.1676 - val_MinusLogProbMetric: 28.1676 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 759/1000
2023-10-25 23:45:38.575 
Epoch 759/1000 
	 loss: 27.1737, MinusLogProbMetric: 27.1737, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 759: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1737 - MinusLogProbMetric: 27.1737 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 760/1000
2023-10-25 23:46:13.372 
Epoch 760/1000 
	 loss: 27.1728, MinusLogProbMetric: 27.1728, val_loss: 28.1657, val_MinusLogProbMetric: 28.1657

Epoch 760: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1728 - MinusLogProbMetric: 27.1728 - val_loss: 28.1657 - val_MinusLogProbMetric: 28.1657 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 761/1000
2023-10-25 23:46:48.272 
Epoch 761/1000 
	 loss: 27.1736, MinusLogProbMetric: 27.1736, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 761: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1736 - MinusLogProbMetric: 27.1736 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 762/1000
2023-10-25 23:47:22.914 
Epoch 762/1000 
	 loss: 27.1726, MinusLogProbMetric: 27.1726, val_loss: 28.1706, val_MinusLogProbMetric: 28.1706

Epoch 762: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1726 - MinusLogProbMetric: 27.1726 - val_loss: 28.1706 - val_MinusLogProbMetric: 28.1706 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 763/1000
2023-10-25 23:47:57.061 
Epoch 763/1000 
	 loss: 27.1737, MinusLogProbMetric: 27.1737, val_loss: 28.1709, val_MinusLogProbMetric: 28.1709

Epoch 763: val_loss did not improve from 28.16124
196/196 - 34s - loss: 27.1737 - MinusLogProbMetric: 27.1737 - val_loss: 28.1709 - val_MinusLogProbMetric: 28.1709 - lr: 7.8125e-06 - 34s/epoch - 174ms/step
Epoch 764/1000
2023-10-25 23:48:31.835 
Epoch 764/1000 
	 loss: 27.1733, MinusLogProbMetric: 27.1733, val_loss: 28.1696, val_MinusLogProbMetric: 28.1696

Epoch 764: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1733 - MinusLogProbMetric: 27.1733 - val_loss: 28.1696 - val_MinusLogProbMetric: 28.1696 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 765/1000
2023-10-25 23:49:06.736 
Epoch 765/1000 
	 loss: 27.1737, MinusLogProbMetric: 27.1737, val_loss: 28.1634, val_MinusLogProbMetric: 28.1634

Epoch 765: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1737 - MinusLogProbMetric: 27.1737 - val_loss: 28.1634 - val_MinusLogProbMetric: 28.1634 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 766/1000
2023-10-25 23:49:41.598 
Epoch 766/1000 
	 loss: 27.1736, MinusLogProbMetric: 27.1736, val_loss: 28.1692, val_MinusLogProbMetric: 28.1692

Epoch 766: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1736 - MinusLogProbMetric: 27.1736 - val_loss: 28.1692 - val_MinusLogProbMetric: 28.1692 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 767/1000
2023-10-25 23:50:16.471 
Epoch 767/1000 
	 loss: 27.1738, MinusLogProbMetric: 27.1738, val_loss: 28.1664, val_MinusLogProbMetric: 28.1664

Epoch 767: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1738 - MinusLogProbMetric: 27.1738 - val_loss: 28.1664 - val_MinusLogProbMetric: 28.1664 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 768/1000
2023-10-25 23:50:51.404 
Epoch 768/1000 
	 loss: 27.1733, MinusLogProbMetric: 27.1733, val_loss: 28.1636, val_MinusLogProbMetric: 28.1636

Epoch 768: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1733 - MinusLogProbMetric: 27.1733 - val_loss: 28.1636 - val_MinusLogProbMetric: 28.1636 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 769/1000
2023-10-25 23:51:26.359 
Epoch 769/1000 
	 loss: 27.1731, MinusLogProbMetric: 27.1731, val_loss: 28.1723, val_MinusLogProbMetric: 28.1723

Epoch 769: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1731 - MinusLogProbMetric: 27.1731 - val_loss: 28.1723 - val_MinusLogProbMetric: 28.1723 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 770/1000
2023-10-25 23:52:01.123 
Epoch 770/1000 
	 loss: 27.1738, MinusLogProbMetric: 27.1738, val_loss: 28.1640, val_MinusLogProbMetric: 28.1640

Epoch 770: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1738 - MinusLogProbMetric: 27.1738 - val_loss: 28.1640 - val_MinusLogProbMetric: 28.1640 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 771/1000
2023-10-25 23:52:36.192 
Epoch 771/1000 
	 loss: 27.1732, MinusLogProbMetric: 27.1732, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 771: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1732 - MinusLogProbMetric: 27.1732 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 772/1000
2023-10-25 23:53:10.838 
Epoch 772/1000 
	 loss: 27.1737, MinusLogProbMetric: 27.1737, val_loss: 28.1679, val_MinusLogProbMetric: 28.1679

Epoch 772: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1737 - MinusLogProbMetric: 27.1737 - val_loss: 28.1679 - val_MinusLogProbMetric: 28.1679 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 773/1000
2023-10-25 23:53:45.602 
Epoch 773/1000 
	 loss: 27.1728, MinusLogProbMetric: 27.1728, val_loss: 28.1640, val_MinusLogProbMetric: 28.1640

Epoch 773: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1728 - MinusLogProbMetric: 27.1728 - val_loss: 28.1640 - val_MinusLogProbMetric: 28.1640 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 774/1000
2023-10-25 23:54:20.332 
Epoch 774/1000 
	 loss: 27.1733, MinusLogProbMetric: 27.1733, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 774: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1733 - MinusLogProbMetric: 27.1733 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 775/1000
2023-10-25 23:54:55.282 
Epoch 775/1000 
	 loss: 27.1726, MinusLogProbMetric: 27.1726, val_loss: 28.1651, val_MinusLogProbMetric: 28.1651

Epoch 775: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1726 - MinusLogProbMetric: 27.1726 - val_loss: 28.1651 - val_MinusLogProbMetric: 28.1651 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 776/1000
2023-10-25 23:55:30.183 
Epoch 776/1000 
	 loss: 27.1730, MinusLogProbMetric: 27.1730, val_loss: 28.1637, val_MinusLogProbMetric: 28.1637

Epoch 776: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1730 - MinusLogProbMetric: 27.1730 - val_loss: 28.1637 - val_MinusLogProbMetric: 28.1637 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 777/1000
2023-10-25 23:56:04.873 
Epoch 777/1000 
	 loss: 27.1727, MinusLogProbMetric: 27.1727, val_loss: 28.1696, val_MinusLogProbMetric: 28.1696

Epoch 777: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1727 - MinusLogProbMetric: 27.1727 - val_loss: 28.1696 - val_MinusLogProbMetric: 28.1696 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 778/1000
2023-10-25 23:56:39.513 
Epoch 778/1000 
	 loss: 27.1738, MinusLogProbMetric: 27.1738, val_loss: 28.1648, val_MinusLogProbMetric: 28.1648

Epoch 778: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1738 - MinusLogProbMetric: 27.1738 - val_loss: 28.1648 - val_MinusLogProbMetric: 28.1648 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 779/1000
2023-10-25 23:57:14.267 
Epoch 779/1000 
	 loss: 27.1727, MinusLogProbMetric: 27.1727, val_loss: 28.1689, val_MinusLogProbMetric: 28.1689

Epoch 779: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1727 - MinusLogProbMetric: 27.1727 - val_loss: 28.1689 - val_MinusLogProbMetric: 28.1689 - lr: 7.8125e-06 - 35s/epoch - 177ms/step
Epoch 780/1000
2023-10-25 23:57:49.191 
Epoch 780/1000 
	 loss: 27.1737, MinusLogProbMetric: 27.1737, val_loss: 28.1670, val_MinusLogProbMetric: 28.1670

Epoch 780: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1737 - MinusLogProbMetric: 27.1737 - val_loss: 28.1670 - val_MinusLogProbMetric: 28.1670 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 781/1000
2023-10-25 23:58:24.051 
Epoch 781/1000 
	 loss: 27.1724, MinusLogProbMetric: 27.1724, val_loss: 28.1630, val_MinusLogProbMetric: 28.1630

Epoch 781: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1724 - MinusLogProbMetric: 27.1724 - val_loss: 28.1630 - val_MinusLogProbMetric: 28.1630 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 782/1000
2023-10-25 23:58:59.062 
Epoch 782/1000 
	 loss: 27.1736, MinusLogProbMetric: 27.1736, val_loss: 28.1677, val_MinusLogProbMetric: 28.1677

Epoch 782: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1736 - MinusLogProbMetric: 27.1736 - val_loss: 28.1677 - val_MinusLogProbMetric: 28.1677 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 783/1000
2023-10-25 23:59:33.636 
Epoch 783/1000 
	 loss: 27.1725, MinusLogProbMetric: 27.1725, val_loss: 28.1647, val_MinusLogProbMetric: 28.1647

Epoch 783: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1725 - MinusLogProbMetric: 27.1725 - val_loss: 28.1647 - val_MinusLogProbMetric: 28.1647 - lr: 7.8125e-06 - 35s/epoch - 176ms/step
Epoch 784/1000
2023-10-26 00:00:08.540 
Epoch 784/1000 
	 loss: 27.1727, MinusLogProbMetric: 27.1727, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 784: val_loss did not improve from 28.16124
196/196 - 35s - loss: 27.1727 - MinusLogProbMetric: 27.1727 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 7.8125e-06 - 35s/epoch - 178ms/step
Epoch 785/1000
2023-10-26 00:00:43.222 
Epoch 785/1000 
	 loss: 27.1733, MinusLogProbMetric: 27.1733, val_loss: 28.1634, val_MinusLogProbMetric: 28.1634

Epoch 785: val_loss did not improve from 28.16124
Restoring model weights from the end of the best epoch: 685.
196/196 - 35s - loss: 27.1733 - MinusLogProbMetric: 27.1733 - val_loss: 28.1634 - val_MinusLogProbMetric: 28.1634 - lr: 7.8125e-06 - 35s/epoch - 179ms/step
Epoch 785: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 541.
Model trained in 27007.30 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.02 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.54 s.
===========
Run 362/720 done in 27013.41 s.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

===========
Generating train data for run 370.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_370/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_370/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_370/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_370
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f0db371c8b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0db31cf760>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0db31cf760>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0db3707a30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0db27c3a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0db27c3fa0>, <keras.callbacks.ModelCheckpoint object at 0x7f0db27c3f40>, <keras.callbacks.EarlyStopping object at 0x7f0db27c3f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0db2610130>, <keras.callbacks.TerminateOnNaN object at 0x7f0db2610310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_370/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 370/720 with hyperparameters:
timestamp = 2023-10-26 00:00:49.534921
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 00:02:24.508 
Epoch 1/1000 
	 loss: 884.1562, MinusLogProbMetric: 884.1562, val_loss: 237.2650, val_MinusLogProbMetric: 237.2650

Epoch 1: val_loss improved from inf to 237.26501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 95s - loss: 884.1562 - MinusLogProbMetric: 884.1562 - val_loss: 237.2650 - val_MinusLogProbMetric: 237.2650 - lr: 0.0010 - 95s/epoch - 486ms/step
Epoch 2/1000
2023-10-26 00:03:00.217 
Epoch 2/1000 
	 loss: 149.7342, MinusLogProbMetric: 149.7342, val_loss: 109.2663, val_MinusLogProbMetric: 109.2663

Epoch 2: val_loss improved from 237.26501 to 109.26628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 149.7342 - MinusLogProbMetric: 149.7342 - val_loss: 109.2663 - val_MinusLogProbMetric: 109.2663 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 3/1000
2023-10-26 00:03:35.850 
Epoch 3/1000 
	 loss: 122.7674, MinusLogProbMetric: 122.7674, val_loss: 110.9176, val_MinusLogProbMetric: 110.9176

Epoch 3: val_loss did not improve from 109.26628
196/196 - 35s - loss: 122.7674 - MinusLogProbMetric: 122.7674 - val_loss: 110.9176 - val_MinusLogProbMetric: 110.9176 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 00:04:10.897 
Epoch 4/1000 
	 loss: 237.2171, MinusLogProbMetric: 237.2171, val_loss: 289.8797, val_MinusLogProbMetric: 289.8797

Epoch 4: val_loss did not improve from 109.26628
196/196 - 35s - loss: 237.2171 - MinusLogProbMetric: 237.2171 - val_loss: 289.8797 - val_MinusLogProbMetric: 289.8797 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 14: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 00:04:15.884 
Epoch 5/1000 
	 loss: nan, MinusLogProbMetric: 298.1558, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 109.26628
196/196 - 5s - loss: nan - MinusLogProbMetric: 298.1558 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 5s/epoch - 25ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 370.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_370/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_370/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_370/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_370
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_138"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_139 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f0bf825cb80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0bf848b7c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0bf848b7c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bf821ba60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d27f91f00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d27f92470>, <keras.callbacks.ModelCheckpoint object at 0x7f0d27f92530>, <keras.callbacks.EarlyStopping object at 0x7f0d27f927a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d27f927d0>, <keras.callbacks.TerminateOnNaN object at 0x7f0d27f92410>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 370/720 with hyperparameters:
timestamp = 2023-10-26 00:04:21.610269
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 00:05:58.611 
Epoch 1/1000 
	 loss: 152.4655, MinusLogProbMetric: 152.4655, val_loss: 111.1007, val_MinusLogProbMetric: 111.1007

Epoch 1: val_loss improved from inf to 111.10067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 98s - loss: 152.4655 - MinusLogProbMetric: 152.4655 - val_loss: 111.1007 - val_MinusLogProbMetric: 111.1007 - lr: 3.3333e-04 - 98s/epoch - 498ms/step
Epoch 2/1000
2023-10-26 00:06:33.990 
Epoch 2/1000 
	 loss: 89.4895, MinusLogProbMetric: 89.4895, val_loss: 74.9989, val_MinusLogProbMetric: 74.9989

Epoch 2: val_loss improved from 111.10067 to 74.99893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 89.4895 - MinusLogProbMetric: 89.4895 - val_loss: 74.9989 - val_MinusLogProbMetric: 74.9989 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 3/1000
2023-10-26 00:07:09.260 
Epoch 3/1000 
	 loss: 67.1207, MinusLogProbMetric: 67.1207, val_loss: 63.0982, val_MinusLogProbMetric: 63.0982

Epoch 3: val_loss improved from 74.99893 to 63.09816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 67.1207 - MinusLogProbMetric: 67.1207 - val_loss: 63.0982 - val_MinusLogProbMetric: 63.0982 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 4/1000
2023-10-26 00:07:44.844 
Epoch 4/1000 
	 loss: 58.4995, MinusLogProbMetric: 58.4995, val_loss: 53.4367, val_MinusLogProbMetric: 53.4367

Epoch 4: val_loss improved from 63.09816 to 53.43669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 58.4995 - MinusLogProbMetric: 58.4995 - val_loss: 53.4367 - val_MinusLogProbMetric: 53.4367 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 5/1000
2023-10-26 00:08:20.154 
Epoch 5/1000 
	 loss: 53.6071, MinusLogProbMetric: 53.6071, val_loss: 50.8668, val_MinusLogProbMetric: 50.8668

Epoch 5: val_loss improved from 53.43669 to 50.86675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 53.6071 - MinusLogProbMetric: 53.6071 - val_loss: 50.8668 - val_MinusLogProbMetric: 50.8668 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 6/1000
2023-10-26 00:08:55.347 
Epoch 6/1000 
	 loss: 52.1295, MinusLogProbMetric: 52.1295, val_loss: 52.1742, val_MinusLogProbMetric: 52.1742

Epoch 6: val_loss did not improve from 50.86675
196/196 - 35s - loss: 52.1295 - MinusLogProbMetric: 52.1295 - val_loss: 52.1742 - val_MinusLogProbMetric: 52.1742 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 7/1000
2023-10-26 00:09:29.803 
Epoch 7/1000 
	 loss: 60.4075, MinusLogProbMetric: 60.4075, val_loss: 49.4055, val_MinusLogProbMetric: 49.4055

Epoch 7: val_loss improved from 50.86675 to 49.40545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 60.4075 - MinusLogProbMetric: 60.4075 - val_loss: 49.4055 - val_MinusLogProbMetric: 49.4055 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 8/1000
2023-10-26 00:10:05.064 
Epoch 8/1000 
	 loss: 46.8750, MinusLogProbMetric: 46.8750, val_loss: 44.1694, val_MinusLogProbMetric: 44.1694

Epoch 8: val_loss improved from 49.40545 to 44.16938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 46.8750 - MinusLogProbMetric: 46.8750 - val_loss: 44.1694 - val_MinusLogProbMetric: 44.1694 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 9/1000
2023-10-26 00:10:40.395 
Epoch 9/1000 
	 loss: 44.7492, MinusLogProbMetric: 44.7492, val_loss: 43.8634, val_MinusLogProbMetric: 43.8634

Epoch 9: val_loss improved from 44.16938 to 43.86344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 44.7492 - MinusLogProbMetric: 44.7492 - val_loss: 43.8634 - val_MinusLogProbMetric: 43.8634 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 10/1000
2023-10-26 00:11:15.588 
Epoch 10/1000 
	 loss: 43.1785, MinusLogProbMetric: 43.1785, val_loss: 42.4725, val_MinusLogProbMetric: 42.4725

Epoch 10: val_loss improved from 43.86344 to 42.47249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 43.1785 - MinusLogProbMetric: 43.1785 - val_loss: 42.4725 - val_MinusLogProbMetric: 42.4725 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 11/1000
2023-10-26 00:11:51.042 
Epoch 11/1000 
	 loss: 41.6228, MinusLogProbMetric: 41.6228, val_loss: 40.2380, val_MinusLogProbMetric: 40.2380

Epoch 11: val_loss improved from 42.47249 to 40.23796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 41.6228 - MinusLogProbMetric: 41.6228 - val_loss: 40.2380 - val_MinusLogProbMetric: 40.2380 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 12/1000
2023-10-26 00:12:26.377 
Epoch 12/1000 
	 loss: 40.1374, MinusLogProbMetric: 40.1374, val_loss: 40.9912, val_MinusLogProbMetric: 40.9912

Epoch 12: val_loss did not improve from 40.23796
196/196 - 35s - loss: 40.1374 - MinusLogProbMetric: 40.1374 - val_loss: 40.9912 - val_MinusLogProbMetric: 40.9912 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 13/1000
2023-10-26 00:13:01.091 
Epoch 13/1000 
	 loss: 39.4759, MinusLogProbMetric: 39.4759, val_loss: 38.5734, val_MinusLogProbMetric: 38.5734

Epoch 13: val_loss improved from 40.23796 to 38.57344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 39.4759 - MinusLogProbMetric: 39.4759 - val_loss: 38.5734 - val_MinusLogProbMetric: 38.5734 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 14/1000
2023-10-26 00:13:36.410 
Epoch 14/1000 
	 loss: 38.6926, MinusLogProbMetric: 38.6926, val_loss: 37.7750, val_MinusLogProbMetric: 37.7750

Epoch 14: val_loss improved from 38.57344 to 37.77499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 38.6926 - MinusLogProbMetric: 38.6926 - val_loss: 37.7750 - val_MinusLogProbMetric: 37.7750 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 15/1000
2023-10-26 00:14:11.356 
Epoch 15/1000 
	 loss: 38.1722, MinusLogProbMetric: 38.1722, val_loss: 38.2094, val_MinusLogProbMetric: 38.2094

Epoch 15: val_loss did not improve from 37.77499
196/196 - 34s - loss: 38.1722 - MinusLogProbMetric: 38.1722 - val_loss: 38.2094 - val_MinusLogProbMetric: 38.2094 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 16/1000
2023-10-26 00:14:45.950 
Epoch 16/1000 
	 loss: 37.8432, MinusLogProbMetric: 37.8432, val_loss: 36.9989, val_MinusLogProbMetric: 36.9989

Epoch 16: val_loss improved from 37.77499 to 36.99894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 37.8432 - MinusLogProbMetric: 37.8432 - val_loss: 36.9989 - val_MinusLogProbMetric: 36.9989 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 17/1000
2023-10-26 00:15:21.023 
Epoch 17/1000 
	 loss: 37.2522, MinusLogProbMetric: 37.2522, val_loss: 36.3106, val_MinusLogProbMetric: 36.3106

Epoch 17: val_loss improved from 36.99894 to 36.31056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 37.2522 - MinusLogProbMetric: 37.2522 - val_loss: 36.3106 - val_MinusLogProbMetric: 36.3106 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 18/1000
2023-10-26 00:15:55.955 
Epoch 18/1000 
	 loss: 36.8961, MinusLogProbMetric: 36.8961, val_loss: 39.0130, val_MinusLogProbMetric: 39.0130

Epoch 18: val_loss did not improve from 36.31056
196/196 - 34s - loss: 36.8961 - MinusLogProbMetric: 36.8961 - val_loss: 39.0130 - val_MinusLogProbMetric: 39.0130 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 19/1000
2023-10-26 00:16:30.712 
Epoch 19/1000 
	 loss: 36.5325, MinusLogProbMetric: 36.5325, val_loss: 35.3739, val_MinusLogProbMetric: 35.3739

Epoch 19: val_loss improved from 36.31056 to 35.37394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 36.5325 - MinusLogProbMetric: 36.5325 - val_loss: 35.3739 - val_MinusLogProbMetric: 35.3739 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 20/1000
2023-10-26 00:17:05.849 
Epoch 20/1000 
	 loss: 36.0972, MinusLogProbMetric: 36.0972, val_loss: 39.1805, val_MinusLogProbMetric: 39.1805

Epoch 20: val_loss did not improve from 35.37394
196/196 - 34s - loss: 36.0972 - MinusLogProbMetric: 36.0972 - val_loss: 39.1805 - val_MinusLogProbMetric: 39.1805 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 21/1000
2023-10-26 00:17:40.321 
Epoch 21/1000 
	 loss: 35.5571, MinusLogProbMetric: 35.5571, val_loss: 35.1263, val_MinusLogProbMetric: 35.1263

Epoch 21: val_loss improved from 35.37394 to 35.12625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 35.5571 - MinusLogProbMetric: 35.5571 - val_loss: 35.1263 - val_MinusLogProbMetric: 35.1263 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 22/1000
2023-10-26 00:18:15.238 
Epoch 22/1000 
	 loss: 35.8887, MinusLogProbMetric: 35.8887, val_loss: 34.9107, val_MinusLogProbMetric: 34.9107

Epoch 22: val_loss improved from 35.12625 to 34.91067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 35.8887 - MinusLogProbMetric: 35.8887 - val_loss: 34.9107 - val_MinusLogProbMetric: 34.9107 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 23/1000
2023-10-26 00:18:50.414 
Epoch 23/1000 
	 loss: 35.2947, MinusLogProbMetric: 35.2947, val_loss: 33.9296, val_MinusLogProbMetric: 33.9296

Epoch 23: val_loss improved from 34.91067 to 33.92956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 35.2947 - MinusLogProbMetric: 35.2947 - val_loss: 33.9296 - val_MinusLogProbMetric: 33.9296 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 24/1000
2023-10-26 00:19:25.746 
Epoch 24/1000 
	 loss: 34.9538, MinusLogProbMetric: 34.9538, val_loss: 35.6760, val_MinusLogProbMetric: 35.6760

Epoch 24: val_loss did not improve from 33.92956
196/196 - 35s - loss: 34.9538 - MinusLogProbMetric: 34.9538 - val_loss: 35.6760 - val_MinusLogProbMetric: 35.6760 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 25/1000
2023-10-26 00:20:00.315 
Epoch 25/1000 
	 loss: 35.0286, MinusLogProbMetric: 35.0286, val_loss: 35.5807, val_MinusLogProbMetric: 35.5807

Epoch 25: val_loss did not improve from 33.92956
196/196 - 35s - loss: 35.0286 - MinusLogProbMetric: 35.0286 - val_loss: 35.5807 - val_MinusLogProbMetric: 35.5807 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 26/1000
2023-10-26 00:20:34.870 
Epoch 26/1000 
	 loss: 34.8671, MinusLogProbMetric: 34.8671, val_loss: 34.9557, val_MinusLogProbMetric: 34.9557

Epoch 26: val_loss did not improve from 33.92956
196/196 - 35s - loss: 34.8671 - MinusLogProbMetric: 34.8671 - val_loss: 34.9557 - val_MinusLogProbMetric: 34.9557 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 27/1000
2023-10-26 00:21:09.548 
Epoch 27/1000 
	 loss: 34.2665, MinusLogProbMetric: 34.2665, val_loss: 33.6722, val_MinusLogProbMetric: 33.6722

Epoch 27: val_loss improved from 33.92956 to 33.67218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 34.2665 - MinusLogProbMetric: 34.2665 - val_loss: 33.6722 - val_MinusLogProbMetric: 33.6722 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 28/1000
2023-10-26 00:21:44.917 
Epoch 28/1000 
	 loss: 34.2407, MinusLogProbMetric: 34.2407, val_loss: 33.2162, val_MinusLogProbMetric: 33.2162

Epoch 28: val_loss improved from 33.67218 to 33.21622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 34.2407 - MinusLogProbMetric: 34.2407 - val_loss: 33.2162 - val_MinusLogProbMetric: 33.2162 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 29/1000
2023-10-26 00:22:19.859 
Epoch 29/1000 
	 loss: 34.1938, MinusLogProbMetric: 34.1938, val_loss: 33.9967, val_MinusLogProbMetric: 33.9967

Epoch 29: val_loss did not improve from 33.21622
196/196 - 34s - loss: 34.1938 - MinusLogProbMetric: 34.1938 - val_loss: 33.9967 - val_MinusLogProbMetric: 33.9967 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 30/1000
2023-10-26 00:22:54.557 
Epoch 30/1000 
	 loss: 34.0614, MinusLogProbMetric: 34.0614, val_loss: 33.3767, val_MinusLogProbMetric: 33.3767

Epoch 30: val_loss did not improve from 33.21622
196/196 - 35s - loss: 34.0614 - MinusLogProbMetric: 34.0614 - val_loss: 33.3767 - val_MinusLogProbMetric: 33.3767 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 31/1000
2023-10-26 00:23:29.123 
Epoch 31/1000 
	 loss: 33.6434, MinusLogProbMetric: 33.6434, val_loss: 34.7511, val_MinusLogProbMetric: 34.7511

Epoch 31: val_loss did not improve from 33.21622
196/196 - 35s - loss: 33.6434 - MinusLogProbMetric: 33.6434 - val_loss: 34.7511 - val_MinusLogProbMetric: 34.7511 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 32/1000
2023-10-26 00:24:03.848 
Epoch 32/1000 
	 loss: 34.0058, MinusLogProbMetric: 34.0058, val_loss: 32.8463, val_MinusLogProbMetric: 32.8463

Epoch 32: val_loss improved from 33.21622 to 32.84630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 34.0058 - MinusLogProbMetric: 34.0058 - val_loss: 32.8463 - val_MinusLogProbMetric: 32.8463 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 33/1000
2023-10-26 00:24:38.867 
Epoch 33/1000 
	 loss: 33.3577, MinusLogProbMetric: 33.3577, val_loss: 34.5017, val_MinusLogProbMetric: 34.5017

Epoch 33: val_loss did not improve from 32.84630
196/196 - 34s - loss: 33.3577 - MinusLogProbMetric: 33.3577 - val_loss: 34.5017 - val_MinusLogProbMetric: 34.5017 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 34/1000
2023-10-26 00:25:13.422 
Epoch 34/1000 
	 loss: 33.8019, MinusLogProbMetric: 33.8019, val_loss: 35.8328, val_MinusLogProbMetric: 35.8328

Epoch 34: val_loss did not improve from 32.84630
196/196 - 35s - loss: 33.8019 - MinusLogProbMetric: 33.8019 - val_loss: 35.8328 - val_MinusLogProbMetric: 35.8328 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 35/1000
2023-10-26 00:25:48.426 
Epoch 35/1000 
	 loss: 33.1765, MinusLogProbMetric: 33.1765, val_loss: 33.1515, val_MinusLogProbMetric: 33.1515

Epoch 35: val_loss did not improve from 32.84630
196/196 - 35s - loss: 33.1765 - MinusLogProbMetric: 33.1765 - val_loss: 33.1515 - val_MinusLogProbMetric: 33.1515 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 36/1000
2023-10-26 00:26:23.071 
Epoch 36/1000 
	 loss: 33.2334, MinusLogProbMetric: 33.2334, val_loss: 33.9656, val_MinusLogProbMetric: 33.9656

Epoch 36: val_loss did not improve from 32.84630
196/196 - 35s - loss: 33.2334 - MinusLogProbMetric: 33.2334 - val_loss: 33.9656 - val_MinusLogProbMetric: 33.9656 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 37/1000
2023-10-26 00:26:57.605 
Epoch 37/1000 
	 loss: 33.1850, MinusLogProbMetric: 33.1850, val_loss: 34.3353, val_MinusLogProbMetric: 34.3353

Epoch 37: val_loss did not improve from 32.84630
196/196 - 35s - loss: 33.1850 - MinusLogProbMetric: 33.1850 - val_loss: 34.3353 - val_MinusLogProbMetric: 34.3353 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 38/1000
2023-10-26 00:27:32.213 
Epoch 38/1000 
	 loss: 33.4345, MinusLogProbMetric: 33.4345, val_loss: 33.5588, val_MinusLogProbMetric: 33.5588

Epoch 38: val_loss did not improve from 32.84630
196/196 - 35s - loss: 33.4345 - MinusLogProbMetric: 33.4345 - val_loss: 33.5588 - val_MinusLogProbMetric: 33.5588 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 39/1000
2023-10-26 00:28:06.857 
Epoch 39/1000 
	 loss: 32.9273, MinusLogProbMetric: 32.9273, val_loss: 34.6685, val_MinusLogProbMetric: 34.6685

Epoch 39: val_loss did not improve from 32.84630
196/196 - 35s - loss: 32.9273 - MinusLogProbMetric: 32.9273 - val_loss: 34.6685 - val_MinusLogProbMetric: 34.6685 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 40/1000
2023-10-26 00:28:41.825 
Epoch 40/1000 
	 loss: 32.7839, MinusLogProbMetric: 32.7839, val_loss: 33.1274, val_MinusLogProbMetric: 33.1274

Epoch 40: val_loss did not improve from 32.84630
196/196 - 35s - loss: 32.7839 - MinusLogProbMetric: 32.7839 - val_loss: 33.1274 - val_MinusLogProbMetric: 33.1274 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 41/1000
2023-10-26 00:29:16.294 
Epoch 41/1000 
	 loss: 32.6513, MinusLogProbMetric: 32.6513, val_loss: 32.5454, val_MinusLogProbMetric: 32.5454

Epoch 41: val_loss improved from 32.84630 to 32.54543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 32.6513 - MinusLogProbMetric: 32.6513 - val_loss: 32.5454 - val_MinusLogProbMetric: 32.5454 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 42/1000
2023-10-26 00:29:51.297 
Epoch 42/1000 
	 loss: 32.6822, MinusLogProbMetric: 32.6822, val_loss: 32.2893, val_MinusLogProbMetric: 32.2893

Epoch 42: val_loss improved from 32.54543 to 32.28931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 32.6822 - MinusLogProbMetric: 32.6822 - val_loss: 32.2893 - val_MinusLogProbMetric: 32.2893 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 43/1000
2023-10-26 00:30:26.192 
Epoch 43/1000 
	 loss: 32.8825, MinusLogProbMetric: 32.8825, val_loss: 32.6764, val_MinusLogProbMetric: 32.6764

Epoch 43: val_loss did not improve from 32.28931
196/196 - 34s - loss: 32.8825 - MinusLogProbMetric: 32.8825 - val_loss: 32.6764 - val_MinusLogProbMetric: 32.6764 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 44/1000
2023-10-26 00:31:00.751 
Epoch 44/1000 
	 loss: 32.6199, MinusLogProbMetric: 32.6199, val_loss: 35.2336, val_MinusLogProbMetric: 35.2336

Epoch 44: val_loss did not improve from 32.28931
196/196 - 35s - loss: 32.6199 - MinusLogProbMetric: 32.6199 - val_loss: 35.2336 - val_MinusLogProbMetric: 35.2336 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 45/1000
2023-10-26 00:31:35.314 
Epoch 45/1000 
	 loss: 32.2758, MinusLogProbMetric: 32.2758, val_loss: 32.6923, val_MinusLogProbMetric: 32.6923

Epoch 45: val_loss did not improve from 32.28931
196/196 - 35s - loss: 32.2758 - MinusLogProbMetric: 32.2758 - val_loss: 32.6923 - val_MinusLogProbMetric: 32.6923 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 46/1000
2023-10-26 00:32:09.899 
Epoch 46/1000 
	 loss: 32.5317, MinusLogProbMetric: 32.5317, val_loss: 33.3942, val_MinusLogProbMetric: 33.3942

Epoch 46: val_loss did not improve from 32.28931
196/196 - 35s - loss: 32.5317 - MinusLogProbMetric: 32.5317 - val_loss: 33.3942 - val_MinusLogProbMetric: 33.3942 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 47/1000
2023-10-26 00:32:44.316 
Epoch 47/1000 
	 loss: 32.1223, MinusLogProbMetric: 32.1223, val_loss: 32.7551, val_MinusLogProbMetric: 32.7551

Epoch 47: val_loss did not improve from 32.28931
196/196 - 34s - loss: 32.1223 - MinusLogProbMetric: 32.1223 - val_loss: 32.7551 - val_MinusLogProbMetric: 32.7551 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 48/1000
2023-10-26 00:33:19.100 
Epoch 48/1000 
	 loss: 32.3158, MinusLogProbMetric: 32.3158, val_loss: 33.6760, val_MinusLogProbMetric: 33.6760

Epoch 48: val_loss did not improve from 32.28931
196/196 - 35s - loss: 32.3158 - MinusLogProbMetric: 32.3158 - val_loss: 33.6760 - val_MinusLogProbMetric: 33.6760 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 49/1000
2023-10-26 00:33:53.416 
Epoch 49/1000 
	 loss: 32.1671, MinusLogProbMetric: 32.1671, val_loss: 33.2273, val_MinusLogProbMetric: 33.2273

Epoch 49: val_loss did not improve from 32.28931
196/196 - 34s - loss: 32.1671 - MinusLogProbMetric: 32.1671 - val_loss: 33.2273 - val_MinusLogProbMetric: 33.2273 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 50/1000
2023-10-26 00:34:28.106 
Epoch 50/1000 
	 loss: 32.2673, MinusLogProbMetric: 32.2673, val_loss: 32.7709, val_MinusLogProbMetric: 32.7709

Epoch 50: val_loss did not improve from 32.28931
196/196 - 35s - loss: 32.2673 - MinusLogProbMetric: 32.2673 - val_loss: 32.7709 - val_MinusLogProbMetric: 32.7709 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 51/1000
2023-10-26 00:35:03.154 
Epoch 51/1000 
	 loss: 32.1360, MinusLogProbMetric: 32.1360, val_loss: 33.1422, val_MinusLogProbMetric: 33.1422

Epoch 51: val_loss did not improve from 32.28931
196/196 - 35s - loss: 32.1360 - MinusLogProbMetric: 32.1360 - val_loss: 33.1422 - val_MinusLogProbMetric: 33.1422 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 52/1000
2023-10-26 00:35:37.906 
Epoch 52/1000 
	 loss: 31.9288, MinusLogProbMetric: 31.9288, val_loss: 31.2560, val_MinusLogProbMetric: 31.2560

Epoch 52: val_loss improved from 32.28931 to 31.25601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 31.9288 - MinusLogProbMetric: 31.9288 - val_loss: 31.2560 - val_MinusLogProbMetric: 31.2560 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 53/1000
2023-10-26 00:36:13.217 
Epoch 53/1000 
	 loss: 31.7553, MinusLogProbMetric: 31.7553, val_loss: 32.2888, val_MinusLogProbMetric: 32.2888

Epoch 53: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.7553 - MinusLogProbMetric: 31.7553 - val_loss: 32.2888 - val_MinusLogProbMetric: 32.2888 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 54/1000
2023-10-26 00:36:48.061 
Epoch 54/1000 
	 loss: 31.8120, MinusLogProbMetric: 31.8120, val_loss: 32.9775, val_MinusLogProbMetric: 32.9775

Epoch 54: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.8120 - MinusLogProbMetric: 31.8120 - val_loss: 32.9775 - val_MinusLogProbMetric: 32.9775 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 55/1000
2023-10-26 00:37:22.732 
Epoch 55/1000 
	 loss: 31.7707, MinusLogProbMetric: 31.7707, val_loss: 33.1152, val_MinusLogProbMetric: 33.1152

Epoch 55: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.7707 - MinusLogProbMetric: 31.7707 - val_loss: 33.1152 - val_MinusLogProbMetric: 33.1152 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 56/1000
2023-10-26 00:37:57.406 
Epoch 56/1000 
	 loss: 31.8790, MinusLogProbMetric: 31.8790, val_loss: 31.7792, val_MinusLogProbMetric: 31.7792

Epoch 56: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.8790 - MinusLogProbMetric: 31.8790 - val_loss: 31.7792 - val_MinusLogProbMetric: 31.7792 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 57/1000
2023-10-26 00:38:31.434 
Epoch 57/1000 
	 loss: 31.8034, MinusLogProbMetric: 31.8034, val_loss: 32.4462, val_MinusLogProbMetric: 32.4462

Epoch 57: val_loss did not improve from 31.25601
196/196 - 34s - loss: 31.8034 - MinusLogProbMetric: 31.8034 - val_loss: 32.4462 - val_MinusLogProbMetric: 32.4462 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 58/1000
2023-10-26 00:39:06.225 
Epoch 58/1000 
	 loss: 31.5826, MinusLogProbMetric: 31.5826, val_loss: 32.8612, val_MinusLogProbMetric: 32.8612

Epoch 58: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.5826 - MinusLogProbMetric: 31.5826 - val_loss: 32.8612 - val_MinusLogProbMetric: 32.8612 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 59/1000
2023-10-26 00:39:40.891 
Epoch 59/1000 
	 loss: 31.5931, MinusLogProbMetric: 31.5931, val_loss: 32.0583, val_MinusLogProbMetric: 32.0583

Epoch 59: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.5931 - MinusLogProbMetric: 31.5931 - val_loss: 32.0583 - val_MinusLogProbMetric: 32.0583 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 60/1000
2023-10-26 00:40:15.446 
Epoch 60/1000 
	 loss: 31.5269, MinusLogProbMetric: 31.5269, val_loss: 32.0816, val_MinusLogProbMetric: 32.0816

Epoch 60: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.5269 - MinusLogProbMetric: 31.5269 - val_loss: 32.0816 - val_MinusLogProbMetric: 32.0816 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 61/1000
2023-10-26 00:40:50.350 
Epoch 61/1000 
	 loss: 31.5705, MinusLogProbMetric: 31.5705, val_loss: 32.0042, val_MinusLogProbMetric: 32.0042

Epoch 61: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.5705 - MinusLogProbMetric: 31.5705 - val_loss: 32.0042 - val_MinusLogProbMetric: 32.0042 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 62/1000
2023-10-26 00:41:24.936 
Epoch 62/1000 
	 loss: 31.4079, MinusLogProbMetric: 31.4079, val_loss: 31.7878, val_MinusLogProbMetric: 31.7878

Epoch 62: val_loss did not improve from 31.25601
196/196 - 35s - loss: 31.4079 - MinusLogProbMetric: 31.4079 - val_loss: 31.7878 - val_MinusLogProbMetric: 31.7878 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 63/1000
2023-10-26 00:41:59.374 
Epoch 63/1000 
	 loss: 31.4200, MinusLogProbMetric: 31.4200, val_loss: 31.2632, val_MinusLogProbMetric: 31.2632

Epoch 63: val_loss did not improve from 31.25601
196/196 - 34s - loss: 31.4200 - MinusLogProbMetric: 31.4200 - val_loss: 31.2632 - val_MinusLogProbMetric: 31.2632 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 64/1000
2023-10-26 00:42:34.362 
Epoch 64/1000 
	 loss: 31.4988, MinusLogProbMetric: 31.4988, val_loss: 30.9085, val_MinusLogProbMetric: 30.9085

Epoch 64: val_loss improved from 31.25601 to 30.90847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 31.4988 - MinusLogProbMetric: 31.4988 - val_loss: 30.9085 - val_MinusLogProbMetric: 30.9085 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 65/1000
2023-10-26 00:43:09.614 
Epoch 65/1000 
	 loss: 31.3262, MinusLogProbMetric: 31.3262, val_loss: 31.4545, val_MinusLogProbMetric: 31.4545

Epoch 65: val_loss did not improve from 30.90847
196/196 - 35s - loss: 31.3262 - MinusLogProbMetric: 31.3262 - val_loss: 31.4545 - val_MinusLogProbMetric: 31.4545 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 66/1000
2023-10-26 00:43:44.452 
Epoch 66/1000 
	 loss: 31.1216, MinusLogProbMetric: 31.1216, val_loss: 31.4946, val_MinusLogProbMetric: 31.4946

Epoch 66: val_loss did not improve from 30.90847
196/196 - 35s - loss: 31.1216 - MinusLogProbMetric: 31.1216 - val_loss: 31.4946 - val_MinusLogProbMetric: 31.4946 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 67/1000
2023-10-26 00:44:19.140 
Epoch 67/1000 
	 loss: 31.2404, MinusLogProbMetric: 31.2404, val_loss: 31.2980, val_MinusLogProbMetric: 31.2980

Epoch 67: val_loss did not improve from 30.90847
196/196 - 35s - loss: 31.2404 - MinusLogProbMetric: 31.2404 - val_loss: 31.2980 - val_MinusLogProbMetric: 31.2980 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 68/1000
2023-10-26 00:44:53.723 
Epoch 68/1000 
	 loss: 31.0304, MinusLogProbMetric: 31.0304, val_loss: 30.9011, val_MinusLogProbMetric: 30.9011

Epoch 68: val_loss improved from 30.90847 to 30.90111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 31.0304 - MinusLogProbMetric: 31.0304 - val_loss: 30.9011 - val_MinusLogProbMetric: 30.9011 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 69/1000
2023-10-26 00:45:28.909 
Epoch 69/1000 
	 loss: 30.9776, MinusLogProbMetric: 30.9776, val_loss: 31.0519, val_MinusLogProbMetric: 31.0519

Epoch 69: val_loss did not improve from 30.90111
196/196 - 35s - loss: 30.9776 - MinusLogProbMetric: 30.9776 - val_loss: 31.0519 - val_MinusLogProbMetric: 31.0519 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 70/1000
2023-10-26 00:46:03.213 
Epoch 70/1000 
	 loss: 31.3164, MinusLogProbMetric: 31.3164, val_loss: 31.8110, val_MinusLogProbMetric: 31.8110

Epoch 70: val_loss did not improve from 30.90111
196/196 - 34s - loss: 31.3164 - MinusLogProbMetric: 31.3164 - val_loss: 31.8110 - val_MinusLogProbMetric: 31.8110 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 71/1000
2023-10-26 00:46:37.939 
Epoch 71/1000 
	 loss: 31.2157, MinusLogProbMetric: 31.2157, val_loss: 31.4201, val_MinusLogProbMetric: 31.4201

Epoch 71: val_loss did not improve from 30.90111
196/196 - 35s - loss: 31.2157 - MinusLogProbMetric: 31.2157 - val_loss: 31.4201 - val_MinusLogProbMetric: 31.4201 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 72/1000
2023-10-26 00:47:12.718 
Epoch 72/1000 
	 loss: 30.7932, MinusLogProbMetric: 30.7932, val_loss: 30.9458, val_MinusLogProbMetric: 30.9458

Epoch 72: val_loss did not improve from 30.90111
196/196 - 35s - loss: 30.7932 - MinusLogProbMetric: 30.7932 - val_loss: 30.9458 - val_MinusLogProbMetric: 30.9458 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 73/1000
2023-10-26 00:47:47.348 
Epoch 73/1000 
	 loss: 30.8379, MinusLogProbMetric: 30.8379, val_loss: 31.1242, val_MinusLogProbMetric: 31.1242

Epoch 73: val_loss did not improve from 30.90111
196/196 - 35s - loss: 30.8379 - MinusLogProbMetric: 30.8379 - val_loss: 31.1242 - val_MinusLogProbMetric: 31.1242 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 74/1000
2023-10-26 00:48:22.137 
Epoch 74/1000 
	 loss: 31.3314, MinusLogProbMetric: 31.3314, val_loss: 31.1820, val_MinusLogProbMetric: 31.1820

Epoch 74: val_loss did not improve from 30.90111
196/196 - 35s - loss: 31.3314 - MinusLogProbMetric: 31.3314 - val_loss: 31.1820 - val_MinusLogProbMetric: 31.1820 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 75/1000
2023-10-26 00:48:56.681 
Epoch 75/1000 
	 loss: 30.9886, MinusLogProbMetric: 30.9886, val_loss: 31.5993, val_MinusLogProbMetric: 31.5993

Epoch 75: val_loss did not improve from 30.90111
196/196 - 35s - loss: 30.9886 - MinusLogProbMetric: 30.9886 - val_loss: 31.5993 - val_MinusLogProbMetric: 31.5993 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 76/1000
2023-10-26 00:49:31.379 
Epoch 76/1000 
	 loss: 31.0200, MinusLogProbMetric: 31.0200, val_loss: 30.5251, val_MinusLogProbMetric: 30.5251

Epoch 76: val_loss improved from 30.90111 to 30.52514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 31.0200 - MinusLogProbMetric: 31.0200 - val_loss: 30.5251 - val_MinusLogProbMetric: 30.5251 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 77/1000
2023-10-26 00:50:07.003 
Epoch 77/1000 
	 loss: 30.7962, MinusLogProbMetric: 30.7962, val_loss: 32.5447, val_MinusLogProbMetric: 32.5447

Epoch 77: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.7962 - MinusLogProbMetric: 30.7962 - val_loss: 32.5447 - val_MinusLogProbMetric: 32.5447 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 78/1000
2023-10-26 00:50:41.897 
Epoch 78/1000 
	 loss: 30.6957, MinusLogProbMetric: 30.6957, val_loss: 33.9404, val_MinusLogProbMetric: 33.9404

Epoch 78: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.6957 - MinusLogProbMetric: 30.6957 - val_loss: 33.9404 - val_MinusLogProbMetric: 33.9404 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 79/1000
2023-10-26 00:51:17.090 
Epoch 79/1000 
	 loss: 31.1307, MinusLogProbMetric: 31.1307, val_loss: 31.1379, val_MinusLogProbMetric: 31.1379

Epoch 79: val_loss did not improve from 30.52514
196/196 - 35s - loss: 31.1307 - MinusLogProbMetric: 31.1307 - val_loss: 31.1379 - val_MinusLogProbMetric: 31.1379 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 80/1000
2023-10-26 00:51:51.439 
Epoch 80/1000 
	 loss: 30.8201, MinusLogProbMetric: 30.8201, val_loss: 31.4081, val_MinusLogProbMetric: 31.4081

Epoch 80: val_loss did not improve from 30.52514
196/196 - 34s - loss: 30.8201 - MinusLogProbMetric: 30.8201 - val_loss: 31.4081 - val_MinusLogProbMetric: 31.4081 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 81/1000
2023-10-26 00:52:21.535 
Epoch 81/1000 
	 loss: 30.7810, MinusLogProbMetric: 30.7810, val_loss: 31.2290, val_MinusLogProbMetric: 31.2290

Epoch 81: val_loss did not improve from 30.52514
196/196 - 30s - loss: 30.7810 - MinusLogProbMetric: 30.7810 - val_loss: 31.2290 - val_MinusLogProbMetric: 31.2290 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 82/1000
2023-10-26 00:52:54.019 
Epoch 82/1000 
	 loss: 30.5534, MinusLogProbMetric: 30.5534, val_loss: 31.4978, val_MinusLogProbMetric: 31.4978

Epoch 82: val_loss did not improve from 30.52514
196/196 - 32s - loss: 30.5534 - MinusLogProbMetric: 30.5534 - val_loss: 31.4978 - val_MinusLogProbMetric: 31.4978 - lr: 3.3333e-04 - 32s/epoch - 166ms/step
Epoch 83/1000
2023-10-26 00:53:28.720 
Epoch 83/1000 
	 loss: 30.6353, MinusLogProbMetric: 30.6353, val_loss: 31.2194, val_MinusLogProbMetric: 31.2194

Epoch 83: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.6353 - MinusLogProbMetric: 30.6353 - val_loss: 31.2194 - val_MinusLogProbMetric: 31.2194 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 84/1000
2023-10-26 00:54:03.257 
Epoch 84/1000 
	 loss: 30.6202, MinusLogProbMetric: 30.6202, val_loss: 30.9729, val_MinusLogProbMetric: 30.9729

Epoch 84: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.6202 - MinusLogProbMetric: 30.6202 - val_loss: 30.9729 - val_MinusLogProbMetric: 30.9729 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 85/1000
2023-10-26 00:54:37.830 
Epoch 85/1000 
	 loss: 30.6026, MinusLogProbMetric: 30.6026, val_loss: 31.1889, val_MinusLogProbMetric: 31.1889

Epoch 85: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.6026 - MinusLogProbMetric: 30.6026 - val_loss: 31.1889 - val_MinusLogProbMetric: 31.1889 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 86/1000
2023-10-26 00:55:12.416 
Epoch 86/1000 
	 loss: 30.6483, MinusLogProbMetric: 30.6483, val_loss: 32.6548, val_MinusLogProbMetric: 32.6548

Epoch 86: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.6483 - MinusLogProbMetric: 30.6483 - val_loss: 32.6548 - val_MinusLogProbMetric: 32.6548 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 87/1000
2023-10-26 00:55:47.109 
Epoch 87/1000 
	 loss: 30.5233, MinusLogProbMetric: 30.5233, val_loss: 30.7775, val_MinusLogProbMetric: 30.7775

Epoch 87: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.5233 - MinusLogProbMetric: 30.5233 - val_loss: 30.7775 - val_MinusLogProbMetric: 30.7775 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 88/1000
2023-10-26 00:56:21.794 
Epoch 88/1000 
	 loss: 30.4816, MinusLogProbMetric: 30.4816, val_loss: 30.5348, val_MinusLogProbMetric: 30.5348

Epoch 88: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.4816 - MinusLogProbMetric: 30.4816 - val_loss: 30.5348 - val_MinusLogProbMetric: 30.5348 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 89/1000
2023-10-26 00:56:56.629 
Epoch 89/1000 
	 loss: 30.3044, MinusLogProbMetric: 30.3044, val_loss: 30.9304, val_MinusLogProbMetric: 30.9304

Epoch 89: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.3044 - MinusLogProbMetric: 30.3044 - val_loss: 30.9304 - val_MinusLogProbMetric: 30.9304 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 90/1000
2023-10-26 00:57:31.317 
Epoch 90/1000 
	 loss: 30.3778, MinusLogProbMetric: 30.3778, val_loss: 31.5505, val_MinusLogProbMetric: 31.5505

Epoch 90: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.3778 - MinusLogProbMetric: 30.3778 - val_loss: 31.5505 - val_MinusLogProbMetric: 31.5505 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 91/1000
2023-10-26 00:58:05.787 
Epoch 91/1000 
	 loss: 30.3534, MinusLogProbMetric: 30.3534, val_loss: 33.9822, val_MinusLogProbMetric: 33.9822

Epoch 91: val_loss did not improve from 30.52514
196/196 - 34s - loss: 30.3534 - MinusLogProbMetric: 30.3534 - val_loss: 33.9822 - val_MinusLogProbMetric: 33.9822 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 92/1000
2023-10-26 00:58:40.422 
Epoch 92/1000 
	 loss: 30.3840, MinusLogProbMetric: 30.3840, val_loss: 30.6306, val_MinusLogProbMetric: 30.6306

Epoch 92: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.3840 - MinusLogProbMetric: 30.3840 - val_loss: 30.6306 - val_MinusLogProbMetric: 30.6306 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 93/1000
2023-10-26 00:59:15.323 
Epoch 93/1000 
	 loss: 30.2831, MinusLogProbMetric: 30.2831, val_loss: 31.0249, val_MinusLogProbMetric: 31.0249

Epoch 93: val_loss did not improve from 30.52514
196/196 - 35s - loss: 30.2831 - MinusLogProbMetric: 30.2831 - val_loss: 31.0249 - val_MinusLogProbMetric: 31.0249 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 94/1000
2023-10-26 00:59:49.967 
Epoch 94/1000 
	 loss: 30.2554, MinusLogProbMetric: 30.2554, val_loss: 30.5019, val_MinusLogProbMetric: 30.5019

Epoch 94: val_loss improved from 30.52514 to 30.50192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 30.2554 - MinusLogProbMetric: 30.2554 - val_loss: 30.5019 - val_MinusLogProbMetric: 30.5019 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 95/1000
2023-10-26 01:00:25.056 
Epoch 95/1000 
	 loss: 30.2907, MinusLogProbMetric: 30.2907, val_loss: 31.8305, val_MinusLogProbMetric: 31.8305

Epoch 95: val_loss did not improve from 30.50192
196/196 - 34s - loss: 30.2907 - MinusLogProbMetric: 30.2907 - val_loss: 31.8305 - val_MinusLogProbMetric: 31.8305 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 96/1000
2023-10-26 01:01:00.067 
Epoch 96/1000 
	 loss: 30.1965, MinusLogProbMetric: 30.1965, val_loss: 29.9992, val_MinusLogProbMetric: 29.9992

Epoch 96: val_loss improved from 30.50192 to 29.99925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 30.1965 - MinusLogProbMetric: 30.1965 - val_loss: 29.9992 - val_MinusLogProbMetric: 29.9992 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 97/1000
2023-10-26 01:01:35.096 
Epoch 97/1000 
	 loss: 30.4157, MinusLogProbMetric: 30.4157, val_loss: 31.6287, val_MinusLogProbMetric: 31.6287

Epoch 97: val_loss did not improve from 29.99925
196/196 - 34s - loss: 30.4157 - MinusLogProbMetric: 30.4157 - val_loss: 31.6287 - val_MinusLogProbMetric: 31.6287 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 98/1000
2023-10-26 01:02:09.781 
Epoch 98/1000 
	 loss: 30.3195, MinusLogProbMetric: 30.3195, val_loss: 30.6946, val_MinusLogProbMetric: 30.6946

Epoch 98: val_loss did not improve from 29.99925
196/196 - 35s - loss: 30.3195 - MinusLogProbMetric: 30.3195 - val_loss: 30.6946 - val_MinusLogProbMetric: 30.6946 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 99/1000
2023-10-26 01:02:44.378 
Epoch 99/1000 
	 loss: 30.1144, MinusLogProbMetric: 30.1144, val_loss: 31.4042, val_MinusLogProbMetric: 31.4042

Epoch 99: val_loss did not improve from 29.99925
196/196 - 35s - loss: 30.1144 - MinusLogProbMetric: 30.1144 - val_loss: 31.4042 - val_MinusLogProbMetric: 31.4042 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 100/1000
2023-10-26 01:03:19.002 
Epoch 100/1000 
	 loss: 30.1009, MinusLogProbMetric: 30.1009, val_loss: 31.1116, val_MinusLogProbMetric: 31.1116

Epoch 100: val_loss did not improve from 29.99925
196/196 - 35s - loss: 30.1009 - MinusLogProbMetric: 30.1009 - val_loss: 31.1116 - val_MinusLogProbMetric: 31.1116 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 101/1000
2023-10-26 01:03:53.823 
Epoch 101/1000 
	 loss: 30.0970, MinusLogProbMetric: 30.0970, val_loss: 31.6099, val_MinusLogProbMetric: 31.6099

Epoch 101: val_loss did not improve from 29.99925
196/196 - 35s - loss: 30.0970 - MinusLogProbMetric: 30.0970 - val_loss: 31.6099 - val_MinusLogProbMetric: 31.6099 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 102/1000
2023-10-26 01:04:28.283 
Epoch 102/1000 
	 loss: 30.0301, MinusLogProbMetric: 30.0301, val_loss: 30.9147, val_MinusLogProbMetric: 30.9147

Epoch 102: val_loss did not improve from 29.99925
196/196 - 34s - loss: 30.0301 - MinusLogProbMetric: 30.0301 - val_loss: 30.9147 - val_MinusLogProbMetric: 30.9147 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 103/1000
2023-10-26 01:05:03.096 
Epoch 103/1000 
	 loss: 29.9957, MinusLogProbMetric: 29.9957, val_loss: 29.9600, val_MinusLogProbMetric: 29.9600

Epoch 103: val_loss improved from 29.99925 to 29.96000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 29.9957 - MinusLogProbMetric: 29.9957 - val_loss: 29.9600 - val_MinusLogProbMetric: 29.9600 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 104/1000
2023-10-26 01:05:38.626 
Epoch 104/1000 
	 loss: 30.1484, MinusLogProbMetric: 30.1484, val_loss: 30.9497, val_MinusLogProbMetric: 30.9497

Epoch 104: val_loss did not improve from 29.96000
196/196 - 35s - loss: 30.1484 - MinusLogProbMetric: 30.1484 - val_loss: 30.9497 - val_MinusLogProbMetric: 30.9497 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 105/1000
2023-10-26 01:06:13.843 
Epoch 105/1000 
	 loss: 29.9435, MinusLogProbMetric: 29.9435, val_loss: 30.3831, val_MinusLogProbMetric: 30.3831

Epoch 105: val_loss did not improve from 29.96000
196/196 - 35s - loss: 29.9435 - MinusLogProbMetric: 29.9435 - val_loss: 30.3831 - val_MinusLogProbMetric: 30.3831 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 106/1000
2023-10-26 01:06:48.660 
Epoch 106/1000 
	 loss: 30.1444, MinusLogProbMetric: 30.1444, val_loss: 30.5768, val_MinusLogProbMetric: 30.5768

Epoch 106: val_loss did not improve from 29.96000
196/196 - 35s - loss: 30.1444 - MinusLogProbMetric: 30.1444 - val_loss: 30.5768 - val_MinusLogProbMetric: 30.5768 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 107/1000
2023-10-26 01:07:23.301 
Epoch 107/1000 
	 loss: 30.1074, MinusLogProbMetric: 30.1074, val_loss: 30.0074, val_MinusLogProbMetric: 30.0074

Epoch 107: val_loss did not improve from 29.96000
196/196 - 35s - loss: 30.1074 - MinusLogProbMetric: 30.1074 - val_loss: 30.0074 - val_MinusLogProbMetric: 30.0074 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 108/1000
2023-10-26 01:07:58.003 
Epoch 108/1000 
	 loss: 30.0501, MinusLogProbMetric: 30.0501, val_loss: 29.8808, val_MinusLogProbMetric: 29.8808

Epoch 108: val_loss improved from 29.96000 to 29.88076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 30.0501 - MinusLogProbMetric: 30.0501 - val_loss: 29.8808 - val_MinusLogProbMetric: 29.8808 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 109/1000
2023-10-26 01:08:33.785 
Epoch 109/1000 
	 loss: 29.8679, MinusLogProbMetric: 29.8679, val_loss: 30.0868, val_MinusLogProbMetric: 30.0868

Epoch 109: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.8679 - MinusLogProbMetric: 29.8679 - val_loss: 30.0868 - val_MinusLogProbMetric: 30.0868 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 110/1000
2023-10-26 01:09:08.499 
Epoch 110/1000 
	 loss: 29.9319, MinusLogProbMetric: 29.9319, val_loss: 30.6151, val_MinusLogProbMetric: 30.6151

Epoch 110: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.9319 - MinusLogProbMetric: 29.9319 - val_loss: 30.6151 - val_MinusLogProbMetric: 30.6151 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 111/1000
2023-10-26 01:09:43.498 
Epoch 111/1000 
	 loss: 29.9404, MinusLogProbMetric: 29.9404, val_loss: 31.5651, val_MinusLogProbMetric: 31.5651

Epoch 111: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.9404 - MinusLogProbMetric: 29.9404 - val_loss: 31.5651 - val_MinusLogProbMetric: 31.5651 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 112/1000
2023-10-26 01:10:18.184 
Epoch 112/1000 
	 loss: 29.8473, MinusLogProbMetric: 29.8473, val_loss: 30.6450, val_MinusLogProbMetric: 30.6450

Epoch 112: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.8473 - MinusLogProbMetric: 29.8473 - val_loss: 30.6450 - val_MinusLogProbMetric: 30.6450 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 113/1000
2023-10-26 01:10:53.244 
Epoch 113/1000 
	 loss: 29.9661, MinusLogProbMetric: 29.9661, val_loss: 30.8522, val_MinusLogProbMetric: 30.8522

Epoch 113: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.9661 - MinusLogProbMetric: 29.9661 - val_loss: 30.8522 - val_MinusLogProbMetric: 30.8522 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 114/1000
2023-10-26 01:11:28.177 
Epoch 114/1000 
	 loss: 29.8488, MinusLogProbMetric: 29.8488, val_loss: 30.0178, val_MinusLogProbMetric: 30.0178

Epoch 114: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.8488 - MinusLogProbMetric: 29.8488 - val_loss: 30.0178 - val_MinusLogProbMetric: 30.0178 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 115/1000
2023-10-26 01:12:03.331 
Epoch 115/1000 
	 loss: 29.8661, MinusLogProbMetric: 29.8661, val_loss: 30.6969, val_MinusLogProbMetric: 30.6969

Epoch 115: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.8661 - MinusLogProbMetric: 29.8661 - val_loss: 30.6969 - val_MinusLogProbMetric: 30.6969 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 116/1000
2023-10-26 01:12:38.260 
Epoch 116/1000 
	 loss: 29.8713, MinusLogProbMetric: 29.8713, val_loss: 30.0358, val_MinusLogProbMetric: 30.0358

Epoch 116: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.8713 - MinusLogProbMetric: 29.8713 - val_loss: 30.0358 - val_MinusLogProbMetric: 30.0358 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 117/1000
2023-10-26 01:13:13.125 
Epoch 117/1000 
	 loss: 29.9191, MinusLogProbMetric: 29.9191, val_loss: 30.2305, val_MinusLogProbMetric: 30.2305

Epoch 117: val_loss did not improve from 29.88076
196/196 - 35s - loss: 29.9191 - MinusLogProbMetric: 29.9191 - val_loss: 30.2305 - val_MinusLogProbMetric: 30.2305 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 118/1000
2023-10-26 01:13:47.961 
Epoch 118/1000 
	 loss: 29.8353, MinusLogProbMetric: 29.8353, val_loss: 29.8436, val_MinusLogProbMetric: 29.8436

Epoch 118: val_loss improved from 29.88076 to 29.84363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 29.8353 - MinusLogProbMetric: 29.8353 - val_loss: 29.8436 - val_MinusLogProbMetric: 29.8436 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 119/1000
2023-10-26 01:14:23.830 
Epoch 119/1000 
	 loss: 29.8740, MinusLogProbMetric: 29.8740, val_loss: 29.9776, val_MinusLogProbMetric: 29.9776

Epoch 119: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.8740 - MinusLogProbMetric: 29.8740 - val_loss: 29.9776 - val_MinusLogProbMetric: 29.9776 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 120/1000
2023-10-26 01:14:58.814 
Epoch 120/1000 
	 loss: 29.7568, MinusLogProbMetric: 29.7568, val_loss: 30.2815, val_MinusLogProbMetric: 30.2815

Epoch 120: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.7568 - MinusLogProbMetric: 29.7568 - val_loss: 30.2815 - val_MinusLogProbMetric: 30.2815 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 121/1000
2023-10-26 01:15:33.865 
Epoch 121/1000 
	 loss: 29.6207, MinusLogProbMetric: 29.6207, val_loss: 30.9319, val_MinusLogProbMetric: 30.9319

Epoch 121: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.6207 - MinusLogProbMetric: 29.6207 - val_loss: 30.9319 - val_MinusLogProbMetric: 30.9319 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 122/1000
2023-10-26 01:16:08.270 
Epoch 122/1000 
	 loss: 29.8292, MinusLogProbMetric: 29.8292, val_loss: 30.7108, val_MinusLogProbMetric: 30.7108

Epoch 122: val_loss did not improve from 29.84363
196/196 - 34s - loss: 29.8292 - MinusLogProbMetric: 29.8292 - val_loss: 30.7108 - val_MinusLogProbMetric: 30.7108 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 123/1000
2023-10-26 01:16:42.926 
Epoch 123/1000 
	 loss: 29.5545, MinusLogProbMetric: 29.5545, val_loss: 30.6857, val_MinusLogProbMetric: 30.6857

Epoch 123: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.5545 - MinusLogProbMetric: 29.5545 - val_loss: 30.6857 - val_MinusLogProbMetric: 30.6857 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 124/1000
2023-10-26 01:17:17.722 
Epoch 124/1000 
	 loss: 29.8781, MinusLogProbMetric: 29.8781, val_loss: 30.5562, val_MinusLogProbMetric: 30.5562

Epoch 124: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.8781 - MinusLogProbMetric: 29.8781 - val_loss: 30.5562 - val_MinusLogProbMetric: 30.5562 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 125/1000
2023-10-26 01:17:52.726 
Epoch 125/1000 
	 loss: 29.6776, MinusLogProbMetric: 29.6776, val_loss: 29.9148, val_MinusLogProbMetric: 29.9148

Epoch 125: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.6776 - MinusLogProbMetric: 29.6776 - val_loss: 29.9148 - val_MinusLogProbMetric: 29.9148 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 126/1000
2023-10-26 01:18:27.205 
Epoch 126/1000 
	 loss: 29.5902, MinusLogProbMetric: 29.5902, val_loss: 30.1762, val_MinusLogProbMetric: 30.1762

Epoch 126: val_loss did not improve from 29.84363
196/196 - 34s - loss: 29.5902 - MinusLogProbMetric: 29.5902 - val_loss: 30.1762 - val_MinusLogProbMetric: 30.1762 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 127/1000
2023-10-26 01:19:02.033 
Epoch 127/1000 
	 loss: 29.7250, MinusLogProbMetric: 29.7250, val_loss: 29.9001, val_MinusLogProbMetric: 29.9001

Epoch 127: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.7250 - MinusLogProbMetric: 29.7250 - val_loss: 29.9001 - val_MinusLogProbMetric: 29.9001 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 128/1000
2023-10-26 01:19:36.902 
Epoch 128/1000 
	 loss: 29.5223, MinusLogProbMetric: 29.5223, val_loss: 31.7397, val_MinusLogProbMetric: 31.7397

Epoch 128: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.5223 - MinusLogProbMetric: 29.5223 - val_loss: 31.7397 - val_MinusLogProbMetric: 31.7397 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 129/1000
2023-10-26 01:20:11.671 
Epoch 129/1000 
	 loss: 29.7558, MinusLogProbMetric: 29.7558, val_loss: 31.2811, val_MinusLogProbMetric: 31.2811

Epoch 129: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.7558 - MinusLogProbMetric: 29.7558 - val_loss: 31.2811 - val_MinusLogProbMetric: 31.2811 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 130/1000
2023-10-26 01:20:46.591 
Epoch 130/1000 
	 loss: 29.8329, MinusLogProbMetric: 29.8329, val_loss: 30.9730, val_MinusLogProbMetric: 30.9730

Epoch 130: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.8329 - MinusLogProbMetric: 29.8329 - val_loss: 30.9730 - val_MinusLogProbMetric: 30.9730 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 131/1000
2023-10-26 01:21:21.315 
Epoch 131/1000 
	 loss: 29.6216, MinusLogProbMetric: 29.6216, val_loss: 29.8954, val_MinusLogProbMetric: 29.8954

Epoch 131: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.6216 - MinusLogProbMetric: 29.6216 - val_loss: 29.8954 - val_MinusLogProbMetric: 29.8954 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 132/1000
2023-10-26 01:21:56.224 
Epoch 132/1000 
	 loss: 29.6126, MinusLogProbMetric: 29.6126, val_loss: 29.8575, val_MinusLogProbMetric: 29.8575

Epoch 132: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.6126 - MinusLogProbMetric: 29.6126 - val_loss: 29.8575 - val_MinusLogProbMetric: 29.8575 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 133/1000
2023-10-26 01:22:31.037 
Epoch 133/1000 
	 loss: 29.4648, MinusLogProbMetric: 29.4648, val_loss: 30.2134, val_MinusLogProbMetric: 30.2134

Epoch 133: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.4648 - MinusLogProbMetric: 29.4648 - val_loss: 30.2134 - val_MinusLogProbMetric: 30.2134 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 134/1000
2023-10-26 01:23:05.822 
Epoch 134/1000 
	 loss: 29.4854, MinusLogProbMetric: 29.4854, val_loss: 30.3737, val_MinusLogProbMetric: 30.3737

Epoch 134: val_loss did not improve from 29.84363
196/196 - 35s - loss: 29.4854 - MinusLogProbMetric: 29.4854 - val_loss: 30.3737 - val_MinusLogProbMetric: 30.3737 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 135/1000
2023-10-26 01:23:40.487 
Epoch 135/1000 
	 loss: 29.5171, MinusLogProbMetric: 29.5171, val_loss: 29.7972, val_MinusLogProbMetric: 29.7972

Epoch 135: val_loss improved from 29.84363 to 29.79723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 29.5171 - MinusLogProbMetric: 29.5171 - val_loss: 29.7972 - val_MinusLogProbMetric: 29.7972 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 136/1000
2023-10-26 01:24:15.691 
Epoch 136/1000 
	 loss: 29.4283, MinusLogProbMetric: 29.4283, val_loss: 29.7263, val_MinusLogProbMetric: 29.7263

Epoch 136: val_loss improved from 29.79723 to 29.72626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 29.4283 - MinusLogProbMetric: 29.4283 - val_loss: 29.7263 - val_MinusLogProbMetric: 29.7263 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 137/1000
2023-10-26 01:24:51.299 
Epoch 137/1000 
	 loss: 29.5476, MinusLogProbMetric: 29.5476, val_loss: 29.7211, val_MinusLogProbMetric: 29.7211

Epoch 137: val_loss improved from 29.72626 to 29.72108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 29.5476 - MinusLogProbMetric: 29.5476 - val_loss: 29.7211 - val_MinusLogProbMetric: 29.7211 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 138/1000
2023-10-26 01:25:26.676 
Epoch 138/1000 
	 loss: 29.5996, MinusLogProbMetric: 29.5996, val_loss: 30.4342, val_MinusLogProbMetric: 30.4342

Epoch 138: val_loss did not improve from 29.72108
196/196 - 35s - loss: 29.5996 - MinusLogProbMetric: 29.5996 - val_loss: 30.4342 - val_MinusLogProbMetric: 30.4342 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 139/1000
2023-10-26 01:26:01.759 
Epoch 139/1000 
	 loss: 29.4352, MinusLogProbMetric: 29.4352, val_loss: 29.7387, val_MinusLogProbMetric: 29.7387

Epoch 139: val_loss did not improve from 29.72108
196/196 - 35s - loss: 29.4352 - MinusLogProbMetric: 29.4352 - val_loss: 29.7387 - val_MinusLogProbMetric: 29.7387 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 140/1000
2023-10-26 01:26:36.680 
Epoch 140/1000 
	 loss: 29.4606, MinusLogProbMetric: 29.4606, val_loss: 29.8360, val_MinusLogProbMetric: 29.8360

Epoch 140: val_loss did not improve from 29.72108
196/196 - 35s - loss: 29.4606 - MinusLogProbMetric: 29.4606 - val_loss: 29.8360 - val_MinusLogProbMetric: 29.8360 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 141/1000
2023-10-26 01:27:11.348 
Epoch 141/1000 
	 loss: 29.4988, MinusLogProbMetric: 29.4988, val_loss: 30.0692, val_MinusLogProbMetric: 30.0692

Epoch 141: val_loss did not improve from 29.72108
196/196 - 35s - loss: 29.4988 - MinusLogProbMetric: 29.4988 - val_loss: 30.0692 - val_MinusLogProbMetric: 30.0692 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 142/1000
2023-10-26 01:27:46.306 
Epoch 142/1000 
	 loss: 29.6410, MinusLogProbMetric: 29.6410, val_loss: 29.9553, val_MinusLogProbMetric: 29.9553

Epoch 142: val_loss did not improve from 29.72108
196/196 - 35s - loss: 29.6410 - MinusLogProbMetric: 29.6410 - val_loss: 29.9553 - val_MinusLogProbMetric: 29.9553 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 143/1000
2023-10-26 01:28:20.912 
Epoch 143/1000 
	 loss: 29.4562, MinusLogProbMetric: 29.4562, val_loss: 30.2703, val_MinusLogProbMetric: 30.2703

Epoch 143: val_loss did not improve from 29.72108
196/196 - 35s - loss: 29.4562 - MinusLogProbMetric: 29.4562 - val_loss: 30.2703 - val_MinusLogProbMetric: 30.2703 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 144/1000
2023-10-26 01:28:56.034 
Epoch 144/1000 
	 loss: 29.4966, MinusLogProbMetric: 29.4966, val_loss: 30.6663, val_MinusLogProbMetric: 30.6663

Epoch 144: val_loss did not improve from 29.72108
196/196 - 35s - loss: 29.4966 - MinusLogProbMetric: 29.4966 - val_loss: 30.6663 - val_MinusLogProbMetric: 30.6663 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 145/1000
2023-10-26 01:29:30.913 
Epoch 145/1000 
	 loss: 29.4747, MinusLogProbMetric: 29.4747, val_loss: 29.6461, val_MinusLogProbMetric: 29.6461

Epoch 145: val_loss improved from 29.72108 to 29.64611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 29.4747 - MinusLogProbMetric: 29.4747 - val_loss: 29.6461 - val_MinusLogProbMetric: 29.6461 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 146/1000
2023-10-26 01:30:06.667 
Epoch 146/1000 
	 loss: 29.3318, MinusLogProbMetric: 29.3318, val_loss: 29.6301, val_MinusLogProbMetric: 29.6301

Epoch 146: val_loss improved from 29.64611 to 29.63007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 29.3318 - MinusLogProbMetric: 29.3318 - val_loss: 29.6301 - val_MinusLogProbMetric: 29.6301 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 147/1000
2023-10-26 01:30:42.345 
Epoch 147/1000 
	 loss: 29.4355, MinusLogProbMetric: 29.4355, val_loss: 29.9041, val_MinusLogProbMetric: 29.9041

Epoch 147: val_loss did not improve from 29.63007
196/196 - 35s - loss: 29.4355 - MinusLogProbMetric: 29.4355 - val_loss: 29.9041 - val_MinusLogProbMetric: 29.9041 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 148/1000
2023-10-26 01:31:17.368 
Epoch 148/1000 
	 loss: 29.3055, MinusLogProbMetric: 29.3055, val_loss: 29.6820, val_MinusLogProbMetric: 29.6820

Epoch 148: val_loss did not improve from 29.63007
196/196 - 35s - loss: 29.3055 - MinusLogProbMetric: 29.3055 - val_loss: 29.6820 - val_MinusLogProbMetric: 29.6820 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 149/1000
2023-10-26 01:31:52.036 
Epoch 149/1000 
	 loss: 29.4997, MinusLogProbMetric: 29.4997, val_loss: 29.5240, val_MinusLogProbMetric: 29.5240

Epoch 149: val_loss improved from 29.63007 to 29.52403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 29.4997 - MinusLogProbMetric: 29.4997 - val_loss: 29.5240 - val_MinusLogProbMetric: 29.5240 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 150/1000
2023-10-26 01:32:27.631 
Epoch 150/1000 
	 loss: 29.2803, MinusLogProbMetric: 29.2803, val_loss: 30.4152, val_MinusLogProbMetric: 30.4152

Epoch 150: val_loss did not improve from 29.52403
196/196 - 35s - loss: 29.2803 - MinusLogProbMetric: 29.2803 - val_loss: 30.4152 - val_MinusLogProbMetric: 30.4152 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 151/1000
2023-10-26 01:33:03.165 
Epoch 151/1000 
	 loss: 29.3491, MinusLogProbMetric: 29.3491, val_loss: 29.4815, val_MinusLogProbMetric: 29.4815

Epoch 151: val_loss improved from 29.52403 to 29.48149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 29.3491 - MinusLogProbMetric: 29.3491 - val_loss: 29.4815 - val_MinusLogProbMetric: 29.4815 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 152/1000
2023-10-26 01:33:39.231 
Epoch 152/1000 
	 loss: 29.3197, MinusLogProbMetric: 29.3197, val_loss: 30.2337, val_MinusLogProbMetric: 30.2337

Epoch 152: val_loss did not improve from 29.48149
196/196 - 35s - loss: 29.3197 - MinusLogProbMetric: 29.3197 - val_loss: 30.2337 - val_MinusLogProbMetric: 30.2337 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 153/1000
2023-10-26 01:34:14.178 
Epoch 153/1000 
	 loss: 29.4197, MinusLogProbMetric: 29.4197, val_loss: 30.1824, val_MinusLogProbMetric: 30.1824

Epoch 153: val_loss did not improve from 29.48149
196/196 - 35s - loss: 29.4197 - MinusLogProbMetric: 29.4197 - val_loss: 30.1824 - val_MinusLogProbMetric: 30.1824 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 154/1000
2023-10-26 01:34:48.923 
Epoch 154/1000 
	 loss: 29.3574, MinusLogProbMetric: 29.3574, val_loss: 29.7447, val_MinusLogProbMetric: 29.7447

Epoch 154: val_loss did not improve from 29.48149
196/196 - 35s - loss: 29.3574 - MinusLogProbMetric: 29.3574 - val_loss: 29.7447 - val_MinusLogProbMetric: 29.7447 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 155/1000
2023-10-26 01:35:23.875 
Epoch 155/1000 
	 loss: 29.4166, MinusLogProbMetric: 29.4166, val_loss: 30.1745, val_MinusLogProbMetric: 30.1745

Epoch 155: val_loss did not improve from 29.48149
196/196 - 35s - loss: 29.4166 - MinusLogProbMetric: 29.4166 - val_loss: 30.1745 - val_MinusLogProbMetric: 30.1745 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 156/1000
2023-10-26 01:35:58.702 
Epoch 156/1000 
	 loss: 29.2599, MinusLogProbMetric: 29.2599, val_loss: 30.2201, val_MinusLogProbMetric: 30.2201

Epoch 156: val_loss did not improve from 29.48149
196/196 - 35s - loss: 29.2599 - MinusLogProbMetric: 29.2599 - val_loss: 30.2201 - val_MinusLogProbMetric: 30.2201 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 157/1000
2023-10-26 01:36:33.543 
Epoch 157/1000 
	 loss: 29.2986, MinusLogProbMetric: 29.2986, val_loss: 29.8727, val_MinusLogProbMetric: 29.8727

Epoch 157: val_loss did not improve from 29.48149
196/196 - 35s - loss: 29.2986 - MinusLogProbMetric: 29.2986 - val_loss: 29.8727 - val_MinusLogProbMetric: 29.8727 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 158/1000
2023-10-26 01:37:08.533 
Epoch 158/1000 
	 loss: 29.2903, MinusLogProbMetric: 29.2903, val_loss: 29.8906, val_MinusLogProbMetric: 29.8906

Epoch 158: val_loss did not improve from 29.48149
196/196 - 35s - loss: 29.2903 - MinusLogProbMetric: 29.2903 - val_loss: 29.8906 - val_MinusLogProbMetric: 29.8906 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 159/1000
2023-10-26 01:37:40.577 
Epoch 159/1000 
	 loss: 29.2437, MinusLogProbMetric: 29.2437, val_loss: 29.4304, val_MinusLogProbMetric: 29.4304

Epoch 159: val_loss improved from 29.48149 to 29.43042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 32s - loss: 29.2437 - MinusLogProbMetric: 29.2437 - val_loss: 29.4304 - val_MinusLogProbMetric: 29.4304 - lr: 3.3333e-04 - 32s/epoch - 166ms/step
Epoch 160/1000
2023-10-26 01:38:08.493 
Epoch 160/1000 
	 loss: 29.1587, MinusLogProbMetric: 29.1587, val_loss: 29.1748, val_MinusLogProbMetric: 29.1748

Epoch 160: val_loss improved from 29.43042 to 29.17481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 28s - loss: 29.1587 - MinusLogProbMetric: 29.1587 - val_loss: 29.1748 - val_MinusLogProbMetric: 29.1748 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 161/1000
2023-10-26 01:38:39.156 
Epoch 161/1000 
	 loss: 29.1990, MinusLogProbMetric: 29.1990, val_loss: 29.8333, val_MinusLogProbMetric: 29.8333

Epoch 161: val_loss did not improve from 29.17481
196/196 - 30s - loss: 29.1990 - MinusLogProbMetric: 29.1990 - val_loss: 29.8333 - val_MinusLogProbMetric: 29.8333 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 162/1000
2023-10-26 01:39:11.700 
Epoch 162/1000 
	 loss: 29.3015, MinusLogProbMetric: 29.3015, val_loss: 30.5718, val_MinusLogProbMetric: 30.5718

Epoch 162: val_loss did not improve from 29.17481
196/196 - 33s - loss: 29.3015 - MinusLogProbMetric: 29.3015 - val_loss: 30.5718 - val_MinusLogProbMetric: 30.5718 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 163/1000
2023-10-26 01:39:40.416 
Epoch 163/1000 
	 loss: 29.1751, MinusLogProbMetric: 29.1751, val_loss: 29.4599, val_MinusLogProbMetric: 29.4599

Epoch 163: val_loss did not improve from 29.17481
196/196 - 29s - loss: 29.1751 - MinusLogProbMetric: 29.1751 - val_loss: 29.4599 - val_MinusLogProbMetric: 29.4599 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 164/1000
2023-10-26 01:40:13.233 
Epoch 164/1000 
	 loss: 29.3147, MinusLogProbMetric: 29.3147, val_loss: 30.6925, val_MinusLogProbMetric: 30.6925

Epoch 164: val_loss did not improve from 29.17481
196/196 - 33s - loss: 29.3147 - MinusLogProbMetric: 29.3147 - val_loss: 30.6925 - val_MinusLogProbMetric: 30.6925 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 165/1000
2023-10-26 01:40:47.879 
Epoch 165/1000 
	 loss: 29.3635, MinusLogProbMetric: 29.3635, val_loss: 29.2708, val_MinusLogProbMetric: 29.2708

Epoch 165: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.3635 - MinusLogProbMetric: 29.3635 - val_loss: 29.2708 - val_MinusLogProbMetric: 29.2708 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 166/1000
2023-10-26 01:41:22.524 
Epoch 166/1000 
	 loss: 29.2221, MinusLogProbMetric: 29.2221, val_loss: 30.6985, val_MinusLogProbMetric: 30.6985

Epoch 166: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.2221 - MinusLogProbMetric: 29.2221 - val_loss: 30.6985 - val_MinusLogProbMetric: 30.6985 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 167/1000
2023-10-26 01:41:57.352 
Epoch 167/1000 
	 loss: 29.1640, MinusLogProbMetric: 29.1640, val_loss: 29.6075, val_MinusLogProbMetric: 29.6075

Epoch 167: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.1640 - MinusLogProbMetric: 29.1640 - val_loss: 29.6075 - val_MinusLogProbMetric: 29.6075 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 168/1000
2023-10-26 01:42:32.438 
Epoch 168/1000 
	 loss: 29.1931, MinusLogProbMetric: 29.1931, val_loss: 29.5556, val_MinusLogProbMetric: 29.5556

Epoch 168: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.1931 - MinusLogProbMetric: 29.1931 - val_loss: 29.5556 - val_MinusLogProbMetric: 29.5556 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 169/1000
2023-10-26 01:43:07.405 
Epoch 169/1000 
	 loss: 29.3200, MinusLogProbMetric: 29.3200, val_loss: 29.6400, val_MinusLogProbMetric: 29.6400

Epoch 169: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.3200 - MinusLogProbMetric: 29.3200 - val_loss: 29.6400 - val_MinusLogProbMetric: 29.6400 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 170/1000
2023-10-26 01:43:42.305 
Epoch 170/1000 
	 loss: 29.1078, MinusLogProbMetric: 29.1078, val_loss: 29.4966, val_MinusLogProbMetric: 29.4966

Epoch 170: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.1078 - MinusLogProbMetric: 29.1078 - val_loss: 29.4966 - val_MinusLogProbMetric: 29.4966 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 171/1000
2023-10-26 01:44:17.017 
Epoch 171/1000 
	 loss: 29.0954, MinusLogProbMetric: 29.0954, val_loss: 29.5597, val_MinusLogProbMetric: 29.5597

Epoch 171: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0954 - MinusLogProbMetric: 29.0954 - val_loss: 29.5597 - val_MinusLogProbMetric: 29.5597 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 172/1000
2023-10-26 01:44:51.721 
Epoch 172/1000 
	 loss: 29.0912, MinusLogProbMetric: 29.0912, val_loss: 29.5452, val_MinusLogProbMetric: 29.5452

Epoch 172: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0912 - MinusLogProbMetric: 29.0912 - val_loss: 29.5452 - val_MinusLogProbMetric: 29.5452 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 173/1000
2023-10-26 01:45:26.454 
Epoch 173/1000 
	 loss: 29.1173, MinusLogProbMetric: 29.1173, val_loss: 29.4946, val_MinusLogProbMetric: 29.4946

Epoch 173: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.1173 - MinusLogProbMetric: 29.1173 - val_loss: 29.4946 - val_MinusLogProbMetric: 29.4946 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 174/1000
2023-10-26 01:46:01.010 
Epoch 174/1000 
	 loss: 29.0485, MinusLogProbMetric: 29.0485, val_loss: 29.2801, val_MinusLogProbMetric: 29.2801

Epoch 174: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0485 - MinusLogProbMetric: 29.0485 - val_loss: 29.2801 - val_MinusLogProbMetric: 29.2801 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 175/1000
2023-10-26 01:46:35.697 
Epoch 175/1000 
	 loss: 29.0825, MinusLogProbMetric: 29.0825, val_loss: 29.6049, val_MinusLogProbMetric: 29.6049

Epoch 175: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0825 - MinusLogProbMetric: 29.0825 - val_loss: 29.6049 - val_MinusLogProbMetric: 29.6049 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 176/1000
2023-10-26 01:47:10.778 
Epoch 176/1000 
	 loss: 29.1612, MinusLogProbMetric: 29.1612, val_loss: 30.1539, val_MinusLogProbMetric: 30.1539

Epoch 176: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.1612 - MinusLogProbMetric: 29.1612 - val_loss: 30.1539 - val_MinusLogProbMetric: 30.1539 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 177/1000
2023-10-26 01:47:45.508 
Epoch 177/1000 
	 loss: 29.0397, MinusLogProbMetric: 29.0397, val_loss: 29.8184, val_MinusLogProbMetric: 29.8184

Epoch 177: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0397 - MinusLogProbMetric: 29.0397 - val_loss: 29.8184 - val_MinusLogProbMetric: 29.8184 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 178/1000
2023-10-26 01:48:20.471 
Epoch 178/1000 
	 loss: 29.1754, MinusLogProbMetric: 29.1754, val_loss: 29.6382, val_MinusLogProbMetric: 29.6382

Epoch 178: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.1754 - MinusLogProbMetric: 29.1754 - val_loss: 29.6382 - val_MinusLogProbMetric: 29.6382 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 179/1000
2023-10-26 01:48:55.141 
Epoch 179/1000 
	 loss: 29.0067, MinusLogProbMetric: 29.0067, val_loss: 29.5740, val_MinusLogProbMetric: 29.5740

Epoch 179: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0067 - MinusLogProbMetric: 29.0067 - val_loss: 29.5740 - val_MinusLogProbMetric: 29.5740 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 180/1000
2023-10-26 01:49:29.539 
Epoch 180/1000 
	 loss: 29.0255, MinusLogProbMetric: 29.0255, val_loss: 29.6739, val_MinusLogProbMetric: 29.6739

Epoch 180: val_loss did not improve from 29.17481
196/196 - 34s - loss: 29.0255 - MinusLogProbMetric: 29.0255 - val_loss: 29.6739 - val_MinusLogProbMetric: 29.6739 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 181/1000
2023-10-26 01:50:04.419 
Epoch 181/1000 
	 loss: 29.0667, MinusLogProbMetric: 29.0667, val_loss: 29.7642, val_MinusLogProbMetric: 29.7642

Epoch 181: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0667 - MinusLogProbMetric: 29.0667 - val_loss: 29.7642 - val_MinusLogProbMetric: 29.7642 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 182/1000
2023-10-26 01:50:39.222 
Epoch 182/1000 
	 loss: 29.0034, MinusLogProbMetric: 29.0034, val_loss: 29.7495, val_MinusLogProbMetric: 29.7495

Epoch 182: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0034 - MinusLogProbMetric: 29.0034 - val_loss: 29.7495 - val_MinusLogProbMetric: 29.7495 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 183/1000
2023-10-26 01:51:14.197 
Epoch 183/1000 
	 loss: 28.9999, MinusLogProbMetric: 28.9999, val_loss: 29.4800, val_MinusLogProbMetric: 29.4800

Epoch 183: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.9999 - MinusLogProbMetric: 28.9999 - val_loss: 29.4800 - val_MinusLogProbMetric: 29.4800 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 184/1000
2023-10-26 01:51:49.183 
Epoch 184/1000 
	 loss: 29.0201, MinusLogProbMetric: 29.0201, val_loss: 29.7360, val_MinusLogProbMetric: 29.7360

Epoch 184: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0201 - MinusLogProbMetric: 29.0201 - val_loss: 29.7360 - val_MinusLogProbMetric: 29.7360 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 185/1000
2023-10-26 01:52:24.079 
Epoch 185/1000 
	 loss: 29.1295, MinusLogProbMetric: 29.1295, val_loss: 29.3380, val_MinusLogProbMetric: 29.3380

Epoch 185: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.1295 - MinusLogProbMetric: 29.1295 - val_loss: 29.3380 - val_MinusLogProbMetric: 29.3380 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 186/1000
2023-10-26 01:52:58.860 
Epoch 186/1000 
	 loss: 28.9515, MinusLogProbMetric: 28.9515, val_loss: 30.2790, val_MinusLogProbMetric: 30.2790

Epoch 186: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.9515 - MinusLogProbMetric: 28.9515 - val_loss: 30.2790 - val_MinusLogProbMetric: 30.2790 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 187/1000
2023-10-26 01:53:33.534 
Epoch 187/1000 
	 loss: 29.0432, MinusLogProbMetric: 29.0432, val_loss: 29.7350, val_MinusLogProbMetric: 29.7350

Epoch 187: val_loss did not improve from 29.17481
196/196 - 35s - loss: 29.0432 - MinusLogProbMetric: 29.0432 - val_loss: 29.7350 - val_MinusLogProbMetric: 29.7350 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 188/1000
2023-10-26 01:54:07.970 
Epoch 188/1000 
	 loss: 28.9471, MinusLogProbMetric: 28.9471, val_loss: 29.3348, val_MinusLogProbMetric: 29.3348

Epoch 188: val_loss did not improve from 29.17481
196/196 - 34s - loss: 28.9471 - MinusLogProbMetric: 28.9471 - val_loss: 29.3348 - val_MinusLogProbMetric: 29.3348 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 189/1000
2023-10-26 01:54:42.951 
Epoch 189/1000 
	 loss: 28.9079, MinusLogProbMetric: 28.9079, val_loss: 30.1163, val_MinusLogProbMetric: 30.1163

Epoch 189: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.9079 - MinusLogProbMetric: 28.9079 - val_loss: 30.1163 - val_MinusLogProbMetric: 30.1163 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 190/1000
2023-10-26 01:55:17.653 
Epoch 190/1000 
	 loss: 28.9376, MinusLogProbMetric: 28.9376, val_loss: 29.5028, val_MinusLogProbMetric: 29.5028

Epoch 190: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.9376 - MinusLogProbMetric: 28.9376 - val_loss: 29.5028 - val_MinusLogProbMetric: 29.5028 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 191/1000
2023-10-26 01:55:52.385 
Epoch 191/1000 
	 loss: 28.9194, MinusLogProbMetric: 28.9194, val_loss: 29.8648, val_MinusLogProbMetric: 29.8648

Epoch 191: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.9194 - MinusLogProbMetric: 28.9194 - val_loss: 29.8648 - val_MinusLogProbMetric: 29.8648 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 192/1000
2023-10-26 01:56:27.053 
Epoch 192/1000 
	 loss: 28.9125, MinusLogProbMetric: 28.9125, val_loss: 29.7641, val_MinusLogProbMetric: 29.7641

Epoch 192: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.9125 - MinusLogProbMetric: 28.9125 - val_loss: 29.7641 - val_MinusLogProbMetric: 29.7641 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 193/1000
2023-10-26 01:57:01.861 
Epoch 193/1000 
	 loss: 28.8744, MinusLogProbMetric: 28.8744, val_loss: 30.4180, val_MinusLogProbMetric: 30.4180

Epoch 193: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.8744 - MinusLogProbMetric: 28.8744 - val_loss: 30.4180 - val_MinusLogProbMetric: 30.4180 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 194/1000
2023-10-26 01:57:36.863 
Epoch 194/1000 
	 loss: 28.9376, MinusLogProbMetric: 28.9376, val_loss: 29.5380, val_MinusLogProbMetric: 29.5380

Epoch 194: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.9376 - MinusLogProbMetric: 28.9376 - val_loss: 29.5380 - val_MinusLogProbMetric: 29.5380 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 195/1000
2023-10-26 01:58:11.757 
Epoch 195/1000 
	 loss: 28.8304, MinusLogProbMetric: 28.8304, val_loss: 30.2894, val_MinusLogProbMetric: 30.2894

Epoch 195: val_loss did not improve from 29.17481
196/196 - 35s - loss: 28.8304 - MinusLogProbMetric: 28.8304 - val_loss: 30.2894 - val_MinusLogProbMetric: 30.2894 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 196/1000
2023-10-26 01:58:46.512 
Epoch 196/1000 
	 loss: 28.9549, MinusLogProbMetric: 28.9549, val_loss: 29.1591, val_MinusLogProbMetric: 29.1591

Epoch 196: val_loss improved from 29.17481 to 29.15908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 28.9549 - MinusLogProbMetric: 28.9549 - val_loss: 29.1591 - val_MinusLogProbMetric: 29.1591 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 197/1000
2023-10-26 01:59:22.161 
Epoch 197/1000 
	 loss: 28.8804, MinusLogProbMetric: 28.8804, val_loss: 29.5395, val_MinusLogProbMetric: 29.5395

Epoch 197: val_loss did not improve from 29.15908
196/196 - 35s - loss: 28.8804 - MinusLogProbMetric: 28.8804 - val_loss: 29.5395 - val_MinusLogProbMetric: 29.5395 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 198/1000
2023-10-26 01:59:57.171 
Epoch 198/1000 
	 loss: 28.8355, MinusLogProbMetric: 28.8355, val_loss: 30.2878, val_MinusLogProbMetric: 30.2878

Epoch 198: val_loss did not improve from 29.15908
196/196 - 35s - loss: 28.8355 - MinusLogProbMetric: 28.8355 - val_loss: 30.2878 - val_MinusLogProbMetric: 30.2878 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 199/1000
2023-10-26 02:00:31.974 
Epoch 199/1000 
	 loss: 28.8033, MinusLogProbMetric: 28.8033, val_loss: 29.6178, val_MinusLogProbMetric: 29.6178

Epoch 199: val_loss did not improve from 29.15908
196/196 - 35s - loss: 28.8033 - MinusLogProbMetric: 28.8033 - val_loss: 29.6178 - val_MinusLogProbMetric: 29.6178 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 200/1000
2023-10-26 02:01:06.790 
Epoch 200/1000 
	 loss: 28.8373, MinusLogProbMetric: 28.8373, val_loss: 29.6936, val_MinusLogProbMetric: 29.6936

Epoch 200: val_loss did not improve from 29.15908
196/196 - 35s - loss: 28.8373 - MinusLogProbMetric: 28.8373 - val_loss: 29.6936 - val_MinusLogProbMetric: 29.6936 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 201/1000
2023-10-26 02:01:41.136 
Epoch 201/1000 
	 loss: 28.8417, MinusLogProbMetric: 28.8417, val_loss: 29.9518, val_MinusLogProbMetric: 29.9518

Epoch 201: val_loss did not improve from 29.15908
196/196 - 34s - loss: 28.8417 - MinusLogProbMetric: 28.8417 - val_loss: 29.9518 - val_MinusLogProbMetric: 29.9518 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 202/1000
2023-10-26 02:02:16.003 
Epoch 202/1000 
	 loss: 28.9637, MinusLogProbMetric: 28.9637, val_loss: 29.9730, val_MinusLogProbMetric: 29.9730

Epoch 202: val_loss did not improve from 29.15908
196/196 - 35s - loss: 28.9637 - MinusLogProbMetric: 28.9637 - val_loss: 29.9730 - val_MinusLogProbMetric: 29.9730 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 203/1000
2023-10-26 02:02:50.901 
Epoch 203/1000 
	 loss: 28.8342, MinusLogProbMetric: 28.8342, val_loss: 29.1041, val_MinusLogProbMetric: 29.1041

Epoch 203: val_loss improved from 29.15908 to 29.10413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 28.8342 - MinusLogProbMetric: 28.8342 - val_loss: 29.1041 - val_MinusLogProbMetric: 29.1041 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 204/1000
2023-10-26 02:03:26.656 
Epoch 204/1000 
	 loss: 28.7854, MinusLogProbMetric: 28.7854, val_loss: 29.8465, val_MinusLogProbMetric: 29.8465

Epoch 204: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.7854 - MinusLogProbMetric: 28.7854 - val_loss: 29.8465 - val_MinusLogProbMetric: 29.8465 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 205/1000
2023-10-26 02:04:01.503 
Epoch 205/1000 
	 loss: 28.8388, MinusLogProbMetric: 28.8388, val_loss: 29.5660, val_MinusLogProbMetric: 29.5660

Epoch 205: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.8388 - MinusLogProbMetric: 28.8388 - val_loss: 29.5660 - val_MinusLogProbMetric: 29.5660 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 206/1000
2023-10-26 02:04:36.376 
Epoch 206/1000 
	 loss: 28.7920, MinusLogProbMetric: 28.7920, val_loss: 31.7635, val_MinusLogProbMetric: 31.7635

Epoch 206: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.7920 - MinusLogProbMetric: 28.7920 - val_loss: 31.7635 - val_MinusLogProbMetric: 31.7635 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 207/1000
2023-10-26 02:05:11.375 
Epoch 207/1000 
	 loss: 28.8531, MinusLogProbMetric: 28.8531, val_loss: 29.6281, val_MinusLogProbMetric: 29.6281

Epoch 207: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.8531 - MinusLogProbMetric: 28.8531 - val_loss: 29.6281 - val_MinusLogProbMetric: 29.6281 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 208/1000
2023-10-26 02:05:46.135 
Epoch 208/1000 
	 loss: 28.7846, MinusLogProbMetric: 28.7846, val_loss: 29.2492, val_MinusLogProbMetric: 29.2492

Epoch 208: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.7846 - MinusLogProbMetric: 28.7846 - val_loss: 29.2492 - val_MinusLogProbMetric: 29.2492 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 209/1000
2023-10-26 02:06:21.045 
Epoch 209/1000 
	 loss: 28.7467, MinusLogProbMetric: 28.7467, val_loss: 29.1857, val_MinusLogProbMetric: 29.1857

Epoch 209: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.7467 - MinusLogProbMetric: 28.7467 - val_loss: 29.1857 - val_MinusLogProbMetric: 29.1857 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 210/1000
2023-10-26 02:06:55.737 
Epoch 210/1000 
	 loss: 28.8539, MinusLogProbMetric: 28.8539, val_loss: 29.6884, val_MinusLogProbMetric: 29.6884

Epoch 210: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.8539 - MinusLogProbMetric: 28.8539 - val_loss: 29.6884 - val_MinusLogProbMetric: 29.6884 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 211/1000
2023-10-26 02:07:30.361 
Epoch 211/1000 
	 loss: 28.7852, MinusLogProbMetric: 28.7852, val_loss: 29.9757, val_MinusLogProbMetric: 29.9757

Epoch 211: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.7852 - MinusLogProbMetric: 28.7852 - val_loss: 29.9757 - val_MinusLogProbMetric: 29.9757 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 212/1000
2023-10-26 02:08:05.363 
Epoch 212/1000 
	 loss: 28.9251, MinusLogProbMetric: 28.9251, val_loss: 30.0822, val_MinusLogProbMetric: 30.0822

Epoch 212: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.9251 - MinusLogProbMetric: 28.9251 - val_loss: 30.0822 - val_MinusLogProbMetric: 30.0822 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 213/1000
2023-10-26 02:08:40.063 
Epoch 213/1000 
	 loss: 28.6761, MinusLogProbMetric: 28.6761, val_loss: 29.7175, val_MinusLogProbMetric: 29.7175

Epoch 213: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.6761 - MinusLogProbMetric: 28.6761 - val_loss: 29.7175 - val_MinusLogProbMetric: 29.7175 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 214/1000
2023-10-26 02:09:14.635 
Epoch 214/1000 
	 loss: 28.7783, MinusLogProbMetric: 28.7783, val_loss: 30.4808, val_MinusLogProbMetric: 30.4808

Epoch 214: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.7783 - MinusLogProbMetric: 28.7783 - val_loss: 30.4808 - val_MinusLogProbMetric: 30.4808 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 215/1000
2023-10-26 02:09:49.212 
Epoch 215/1000 
	 loss: 28.6677, MinusLogProbMetric: 28.6677, val_loss: 29.4419, val_MinusLogProbMetric: 29.4419

Epoch 215: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.6677 - MinusLogProbMetric: 28.6677 - val_loss: 29.4419 - val_MinusLogProbMetric: 29.4419 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 216/1000
2023-10-26 02:10:23.965 
Epoch 216/1000 
	 loss: 28.8424, MinusLogProbMetric: 28.8424, val_loss: 29.2859, val_MinusLogProbMetric: 29.2859

Epoch 216: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.8424 - MinusLogProbMetric: 28.8424 - val_loss: 29.2859 - val_MinusLogProbMetric: 29.2859 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 217/1000
2023-10-26 02:10:58.833 
Epoch 217/1000 
	 loss: 28.7177, MinusLogProbMetric: 28.7177, val_loss: 29.1812, val_MinusLogProbMetric: 29.1812

Epoch 217: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.7177 - MinusLogProbMetric: 28.7177 - val_loss: 29.1812 - val_MinusLogProbMetric: 29.1812 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 218/1000
2023-10-26 02:11:33.314 
Epoch 218/1000 
	 loss: 28.7796, MinusLogProbMetric: 28.7796, val_loss: 29.2983, val_MinusLogProbMetric: 29.2983

Epoch 218: val_loss did not improve from 29.10413
196/196 - 34s - loss: 28.7796 - MinusLogProbMetric: 28.7796 - val_loss: 29.2983 - val_MinusLogProbMetric: 29.2983 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 219/1000
2023-10-26 02:12:07.969 
Epoch 219/1000 
	 loss: 28.6900, MinusLogProbMetric: 28.6900, val_loss: 29.1240, val_MinusLogProbMetric: 29.1240

Epoch 219: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.6900 - MinusLogProbMetric: 28.6900 - val_loss: 29.1240 - val_MinusLogProbMetric: 29.1240 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 220/1000
2023-10-26 02:12:42.873 
Epoch 220/1000 
	 loss: 28.6635, MinusLogProbMetric: 28.6635, val_loss: 30.3186, val_MinusLogProbMetric: 30.3186

Epoch 220: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.6635 - MinusLogProbMetric: 28.6635 - val_loss: 30.3186 - val_MinusLogProbMetric: 30.3186 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 221/1000
2023-10-26 02:13:17.894 
Epoch 221/1000 
	 loss: 28.5998, MinusLogProbMetric: 28.5998, val_loss: 29.3563, val_MinusLogProbMetric: 29.3563

Epoch 221: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.5998 - MinusLogProbMetric: 28.5998 - val_loss: 29.3563 - val_MinusLogProbMetric: 29.3563 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 222/1000
2023-10-26 02:13:52.387 
Epoch 222/1000 
	 loss: 28.6887, MinusLogProbMetric: 28.6887, val_loss: 30.2635, val_MinusLogProbMetric: 30.2635

Epoch 222: val_loss did not improve from 29.10413
196/196 - 34s - loss: 28.6887 - MinusLogProbMetric: 28.6887 - val_loss: 30.2635 - val_MinusLogProbMetric: 30.2635 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 223/1000
2023-10-26 02:14:26.743 
Epoch 223/1000 
	 loss: 28.7061, MinusLogProbMetric: 28.7061, val_loss: 29.1993, val_MinusLogProbMetric: 29.1993

Epoch 223: val_loss did not improve from 29.10413
196/196 - 34s - loss: 28.7061 - MinusLogProbMetric: 28.7061 - val_loss: 29.1993 - val_MinusLogProbMetric: 29.1993 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 224/1000
2023-10-26 02:15:01.200 
Epoch 224/1000 
	 loss: 28.6910, MinusLogProbMetric: 28.6910, val_loss: 30.0095, val_MinusLogProbMetric: 30.0095

Epoch 224: val_loss did not improve from 29.10413
196/196 - 34s - loss: 28.6910 - MinusLogProbMetric: 28.6910 - val_loss: 30.0095 - val_MinusLogProbMetric: 30.0095 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 225/1000
2023-10-26 02:15:35.868 
Epoch 225/1000 
	 loss: 28.6912, MinusLogProbMetric: 28.6912, val_loss: 31.0141, val_MinusLogProbMetric: 31.0141

Epoch 225: val_loss did not improve from 29.10413
196/196 - 35s - loss: 28.6912 - MinusLogProbMetric: 28.6912 - val_loss: 31.0141 - val_MinusLogProbMetric: 31.0141 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 226/1000
2023-10-26 02:16:10.559 
Epoch 226/1000 
	 loss: 28.7633, MinusLogProbMetric: 28.7633, val_loss: 29.0348, val_MinusLogProbMetric: 29.0348

Epoch 226: val_loss improved from 29.10413 to 29.03478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 28.7633 - MinusLogProbMetric: 28.7633 - val_loss: 29.0348 - val_MinusLogProbMetric: 29.0348 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 227/1000
2023-10-26 02:16:45.717 
Epoch 227/1000 
	 loss: 28.6741, MinusLogProbMetric: 28.6741, val_loss: 29.1570, val_MinusLogProbMetric: 29.1570

Epoch 227: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6741 - MinusLogProbMetric: 28.6741 - val_loss: 29.1570 - val_MinusLogProbMetric: 29.1570 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 228/1000
2023-10-26 02:17:20.535 
Epoch 228/1000 
	 loss: 28.6272, MinusLogProbMetric: 28.6272, val_loss: 29.2034, val_MinusLogProbMetric: 29.2034

Epoch 228: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6272 - MinusLogProbMetric: 28.6272 - val_loss: 29.2034 - val_MinusLogProbMetric: 29.2034 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 229/1000
2023-10-26 02:17:55.296 
Epoch 229/1000 
	 loss: 28.6683, MinusLogProbMetric: 28.6683, val_loss: 29.1025, val_MinusLogProbMetric: 29.1025

Epoch 229: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6683 - MinusLogProbMetric: 28.6683 - val_loss: 29.1025 - val_MinusLogProbMetric: 29.1025 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 230/1000
2023-10-26 02:18:30.252 
Epoch 230/1000 
	 loss: 28.7009, MinusLogProbMetric: 28.7009, val_loss: 29.9191, val_MinusLogProbMetric: 29.9191

Epoch 230: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.7009 - MinusLogProbMetric: 28.7009 - val_loss: 29.9191 - val_MinusLogProbMetric: 29.9191 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 231/1000
2023-10-26 02:19:04.800 
Epoch 231/1000 
	 loss: 28.7498, MinusLogProbMetric: 28.7498, val_loss: 29.7269, val_MinusLogProbMetric: 29.7269

Epoch 231: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.7498 - MinusLogProbMetric: 28.7498 - val_loss: 29.7269 - val_MinusLogProbMetric: 29.7269 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 232/1000
2023-10-26 02:19:39.902 
Epoch 232/1000 
	 loss: 28.5753, MinusLogProbMetric: 28.5753, val_loss: 30.1258, val_MinusLogProbMetric: 30.1258

Epoch 232: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5753 - MinusLogProbMetric: 28.5753 - val_loss: 30.1258 - val_MinusLogProbMetric: 30.1258 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 233/1000
2023-10-26 02:20:14.297 
Epoch 233/1000 
	 loss: 28.6436, MinusLogProbMetric: 28.6436, val_loss: 29.4639, val_MinusLogProbMetric: 29.4639

Epoch 233: val_loss did not improve from 29.03478
196/196 - 34s - loss: 28.6436 - MinusLogProbMetric: 28.6436 - val_loss: 29.4639 - val_MinusLogProbMetric: 29.4639 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 234/1000
2023-10-26 02:20:49.044 
Epoch 234/1000 
	 loss: 28.5318, MinusLogProbMetric: 28.5318, val_loss: 30.0217, val_MinusLogProbMetric: 30.0217

Epoch 234: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5318 - MinusLogProbMetric: 28.5318 - val_loss: 30.0217 - val_MinusLogProbMetric: 30.0217 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 235/1000
2023-10-26 02:21:23.637 
Epoch 235/1000 
	 loss: 28.6321, MinusLogProbMetric: 28.6321, val_loss: 29.9090, val_MinusLogProbMetric: 29.9090

Epoch 235: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6321 - MinusLogProbMetric: 28.6321 - val_loss: 29.9090 - val_MinusLogProbMetric: 29.9090 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 236/1000
2023-10-26 02:21:58.386 
Epoch 236/1000 
	 loss: 28.5880, MinusLogProbMetric: 28.5880, val_loss: 29.5588, val_MinusLogProbMetric: 29.5588

Epoch 236: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5880 - MinusLogProbMetric: 28.5880 - val_loss: 29.5588 - val_MinusLogProbMetric: 29.5588 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 237/1000
2023-10-26 02:22:33.214 
Epoch 237/1000 
	 loss: 28.6878, MinusLogProbMetric: 28.6878, val_loss: 29.9491, val_MinusLogProbMetric: 29.9491

Epoch 237: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6878 - MinusLogProbMetric: 28.6878 - val_loss: 29.9491 - val_MinusLogProbMetric: 29.9491 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 238/1000
2023-10-26 02:23:08.223 
Epoch 238/1000 
	 loss: 28.5087, MinusLogProbMetric: 28.5087, val_loss: 31.1088, val_MinusLogProbMetric: 31.1088

Epoch 238: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5087 - MinusLogProbMetric: 28.5087 - val_loss: 31.1088 - val_MinusLogProbMetric: 31.1088 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 239/1000
2023-10-26 02:23:43.507 
Epoch 239/1000 
	 loss: 28.6509, MinusLogProbMetric: 28.6509, val_loss: 29.8181, val_MinusLogProbMetric: 29.8181

Epoch 239: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6509 - MinusLogProbMetric: 28.6509 - val_loss: 29.8181 - val_MinusLogProbMetric: 29.8181 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 240/1000
2023-10-26 02:24:18.270 
Epoch 240/1000 
	 loss: 28.6702, MinusLogProbMetric: 28.6702, val_loss: 29.2906, val_MinusLogProbMetric: 29.2906

Epoch 240: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6702 - MinusLogProbMetric: 28.6702 - val_loss: 29.2906 - val_MinusLogProbMetric: 29.2906 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 241/1000
2023-10-26 02:24:52.763 
Epoch 241/1000 
	 loss: 28.5527, MinusLogProbMetric: 28.5527, val_loss: 29.6080, val_MinusLogProbMetric: 29.6080

Epoch 241: val_loss did not improve from 29.03478
196/196 - 34s - loss: 28.5527 - MinusLogProbMetric: 28.5527 - val_loss: 29.6080 - val_MinusLogProbMetric: 29.6080 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 242/1000
2023-10-26 02:25:27.332 
Epoch 242/1000 
	 loss: 28.6265, MinusLogProbMetric: 28.6265, val_loss: 29.1207, val_MinusLogProbMetric: 29.1207

Epoch 242: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6265 - MinusLogProbMetric: 28.6265 - val_loss: 29.1207 - val_MinusLogProbMetric: 29.1207 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 243/1000
2023-10-26 02:26:02.069 
Epoch 243/1000 
	 loss: 28.5543, MinusLogProbMetric: 28.5543, val_loss: 30.6802, val_MinusLogProbMetric: 30.6802

Epoch 243: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5543 - MinusLogProbMetric: 28.5543 - val_loss: 30.6802 - val_MinusLogProbMetric: 30.6802 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 244/1000
2023-10-26 02:26:37.137 
Epoch 244/1000 
	 loss: 28.6480, MinusLogProbMetric: 28.6480, val_loss: 29.3428, val_MinusLogProbMetric: 29.3428

Epoch 244: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6480 - MinusLogProbMetric: 28.6480 - val_loss: 29.3428 - val_MinusLogProbMetric: 29.3428 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 245/1000
2023-10-26 02:27:12.057 
Epoch 245/1000 
	 loss: 28.6062, MinusLogProbMetric: 28.6062, val_loss: 29.2589, val_MinusLogProbMetric: 29.2589

Epoch 245: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6062 - MinusLogProbMetric: 28.6062 - val_loss: 29.2589 - val_MinusLogProbMetric: 29.2589 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 246/1000
2023-10-26 02:27:46.845 
Epoch 246/1000 
	 loss: 28.5975, MinusLogProbMetric: 28.5975, val_loss: 29.5252, val_MinusLogProbMetric: 29.5252

Epoch 246: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5975 - MinusLogProbMetric: 28.5975 - val_loss: 29.5252 - val_MinusLogProbMetric: 29.5252 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 247/1000
2023-10-26 02:28:21.560 
Epoch 247/1000 
	 loss: 28.5707, MinusLogProbMetric: 28.5707, val_loss: 29.0930, val_MinusLogProbMetric: 29.0930

Epoch 247: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5707 - MinusLogProbMetric: 28.5707 - val_loss: 29.0930 - val_MinusLogProbMetric: 29.0930 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 248/1000
2023-10-26 02:28:56.505 
Epoch 248/1000 
	 loss: 28.5301, MinusLogProbMetric: 28.5301, val_loss: 29.3872, val_MinusLogProbMetric: 29.3872

Epoch 248: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5301 - MinusLogProbMetric: 28.5301 - val_loss: 29.3872 - val_MinusLogProbMetric: 29.3872 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 249/1000
2023-10-26 02:29:31.489 
Epoch 249/1000 
	 loss: 28.6000, MinusLogProbMetric: 28.6000, val_loss: 29.0866, val_MinusLogProbMetric: 29.0866

Epoch 249: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6000 - MinusLogProbMetric: 28.6000 - val_loss: 29.0866 - val_MinusLogProbMetric: 29.0866 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 250/1000
2023-10-26 02:30:05.865 
Epoch 250/1000 
	 loss: 28.5218, MinusLogProbMetric: 28.5218, val_loss: 29.9342, val_MinusLogProbMetric: 29.9342

Epoch 250: val_loss did not improve from 29.03478
196/196 - 34s - loss: 28.5218 - MinusLogProbMetric: 28.5218 - val_loss: 29.9342 - val_MinusLogProbMetric: 29.9342 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 251/1000
2023-10-26 02:30:40.380 
Epoch 251/1000 
	 loss: 28.4846, MinusLogProbMetric: 28.4846, val_loss: 29.9294, val_MinusLogProbMetric: 29.9294

Epoch 251: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.4846 - MinusLogProbMetric: 28.4846 - val_loss: 29.9294 - val_MinusLogProbMetric: 29.9294 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 252/1000
2023-10-26 02:31:15.403 
Epoch 252/1000 
	 loss: 28.4836, MinusLogProbMetric: 28.4836, val_loss: 30.4682, val_MinusLogProbMetric: 30.4682

Epoch 252: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.4836 - MinusLogProbMetric: 28.4836 - val_loss: 30.4682 - val_MinusLogProbMetric: 30.4682 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 253/1000
2023-10-26 02:31:50.570 
Epoch 253/1000 
	 loss: 28.5249, MinusLogProbMetric: 28.5249, val_loss: 29.2796, val_MinusLogProbMetric: 29.2796

Epoch 253: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5249 - MinusLogProbMetric: 28.5249 - val_loss: 29.2796 - val_MinusLogProbMetric: 29.2796 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 254/1000
2023-10-26 02:32:25.320 
Epoch 254/1000 
	 loss: 28.6463, MinusLogProbMetric: 28.6463, val_loss: 29.7694, val_MinusLogProbMetric: 29.7694

Epoch 254: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6463 - MinusLogProbMetric: 28.6463 - val_loss: 29.7694 - val_MinusLogProbMetric: 29.7694 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 255/1000
2023-10-26 02:33:00.236 
Epoch 255/1000 
	 loss: 28.4380, MinusLogProbMetric: 28.4380, val_loss: 29.5487, val_MinusLogProbMetric: 29.5487

Epoch 255: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.4380 - MinusLogProbMetric: 28.4380 - val_loss: 29.5487 - val_MinusLogProbMetric: 29.5487 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 256/1000
2023-10-26 02:33:35.105 
Epoch 256/1000 
	 loss: 28.5936, MinusLogProbMetric: 28.5936, val_loss: 29.5534, val_MinusLogProbMetric: 29.5534

Epoch 256: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.5936 - MinusLogProbMetric: 28.5936 - val_loss: 29.5534 - val_MinusLogProbMetric: 29.5534 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 257/1000
2023-10-26 02:34:09.919 
Epoch 257/1000 
	 loss: 28.4548, MinusLogProbMetric: 28.4548, val_loss: 29.3662, val_MinusLogProbMetric: 29.3662

Epoch 257: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.4548 - MinusLogProbMetric: 28.4548 - val_loss: 29.3662 - val_MinusLogProbMetric: 29.3662 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 258/1000
2023-10-26 02:34:44.982 
Epoch 258/1000 
	 loss: 28.6812, MinusLogProbMetric: 28.6812, val_loss: 29.0420, val_MinusLogProbMetric: 29.0420

Epoch 258: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.6812 - MinusLogProbMetric: 28.6812 - val_loss: 29.0420 - val_MinusLogProbMetric: 29.0420 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 259/1000
2023-10-26 02:35:19.835 
Epoch 259/1000 
	 loss: 28.3977, MinusLogProbMetric: 28.3977, val_loss: 29.3817, val_MinusLogProbMetric: 29.3817

Epoch 259: val_loss did not improve from 29.03478
196/196 - 35s - loss: 28.3977 - MinusLogProbMetric: 28.3977 - val_loss: 29.3817 - val_MinusLogProbMetric: 29.3817 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 260/1000
2023-10-26 02:35:54.780 
Epoch 260/1000 
	 loss: 28.4948, MinusLogProbMetric: 28.4948, val_loss: 28.9898, val_MinusLogProbMetric: 28.9898

Epoch 260: val_loss improved from 29.03478 to 28.98979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 28.4948 - MinusLogProbMetric: 28.4948 - val_loss: 28.9898 - val_MinusLogProbMetric: 28.9898 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 261/1000
2023-10-26 02:36:30.204 
Epoch 261/1000 
	 loss: 28.4876, MinusLogProbMetric: 28.4876, val_loss: 29.3672, val_MinusLogProbMetric: 29.3672

Epoch 261: val_loss did not improve from 28.98979
196/196 - 35s - loss: 28.4876 - MinusLogProbMetric: 28.4876 - val_loss: 29.3672 - val_MinusLogProbMetric: 29.3672 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 262/1000
2023-10-26 02:37:05.010 
Epoch 262/1000 
	 loss: 28.3921, MinusLogProbMetric: 28.3921, val_loss: 29.5613, val_MinusLogProbMetric: 29.5613

Epoch 262: val_loss did not improve from 28.98979
196/196 - 35s - loss: 28.3921 - MinusLogProbMetric: 28.3921 - val_loss: 29.5613 - val_MinusLogProbMetric: 29.5613 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 263/1000
2023-10-26 02:37:40.058 
Epoch 263/1000 
	 loss: 28.5048, MinusLogProbMetric: 28.5048, val_loss: 29.1491, val_MinusLogProbMetric: 29.1491

Epoch 263: val_loss did not improve from 28.98979
196/196 - 35s - loss: 28.5048 - MinusLogProbMetric: 28.5048 - val_loss: 29.1491 - val_MinusLogProbMetric: 29.1491 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 264/1000
2023-10-26 02:38:14.692 
Epoch 264/1000 
	 loss: 28.5461, MinusLogProbMetric: 28.5461, val_loss: 29.0266, val_MinusLogProbMetric: 29.0266

Epoch 264: val_loss did not improve from 28.98979
196/196 - 35s - loss: 28.5461 - MinusLogProbMetric: 28.5461 - val_loss: 29.0266 - val_MinusLogProbMetric: 29.0266 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 265/1000
2023-10-26 02:38:49.383 
Epoch 265/1000 
	 loss: 28.3783, MinusLogProbMetric: 28.3783, val_loss: 29.5665, val_MinusLogProbMetric: 29.5665

Epoch 265: val_loss did not improve from 28.98979
196/196 - 35s - loss: 28.3783 - MinusLogProbMetric: 28.3783 - val_loss: 29.5665 - val_MinusLogProbMetric: 29.5665 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 266/1000
2023-10-26 02:39:24.016 
Epoch 266/1000 
	 loss: 28.4212, MinusLogProbMetric: 28.4212, val_loss: 29.1268, val_MinusLogProbMetric: 29.1268

Epoch 266: val_loss did not improve from 28.98979
196/196 - 35s - loss: 28.4212 - MinusLogProbMetric: 28.4212 - val_loss: 29.1268 - val_MinusLogProbMetric: 29.1268 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 267/1000
2023-10-26 02:39:58.952 
Epoch 267/1000 
	 loss: 28.5220, MinusLogProbMetric: 28.5220, val_loss: 28.9261, val_MinusLogProbMetric: 28.9261

Epoch 267: val_loss improved from 28.98979 to 28.92610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 28.5220 - MinusLogProbMetric: 28.5220 - val_loss: 28.9261 - val_MinusLogProbMetric: 28.9261 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 268/1000
2023-10-26 02:40:34.382 
Epoch 268/1000 
	 loss: 28.4313, MinusLogProbMetric: 28.4313, val_loss: 30.7892, val_MinusLogProbMetric: 30.7892

Epoch 268: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.4313 - MinusLogProbMetric: 28.4313 - val_loss: 30.7892 - val_MinusLogProbMetric: 30.7892 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 269/1000
2023-10-26 02:41:09.259 
Epoch 269/1000 
	 loss: 28.4371, MinusLogProbMetric: 28.4371, val_loss: 29.2483, val_MinusLogProbMetric: 29.2483

Epoch 269: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.4371 - MinusLogProbMetric: 28.4371 - val_loss: 29.2483 - val_MinusLogProbMetric: 29.2483 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 270/1000
2023-10-26 02:41:43.909 
Epoch 270/1000 
	 loss: 28.4037, MinusLogProbMetric: 28.4037, val_loss: 29.2175, val_MinusLogProbMetric: 29.2175

Epoch 270: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.4037 - MinusLogProbMetric: 28.4037 - val_loss: 29.2175 - val_MinusLogProbMetric: 29.2175 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 271/1000
2023-10-26 02:42:18.793 
Epoch 271/1000 
	 loss: 28.4253, MinusLogProbMetric: 28.4253, val_loss: 29.4665, val_MinusLogProbMetric: 29.4665

Epoch 271: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.4253 - MinusLogProbMetric: 28.4253 - val_loss: 29.4665 - val_MinusLogProbMetric: 29.4665 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 272/1000
2023-10-26 02:42:53.317 
Epoch 272/1000 
	 loss: 28.3498, MinusLogProbMetric: 28.3498, val_loss: 29.1210, val_MinusLogProbMetric: 29.1210

Epoch 272: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3498 - MinusLogProbMetric: 28.3498 - val_loss: 29.1210 - val_MinusLogProbMetric: 29.1210 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 273/1000
2023-10-26 02:43:28.303 
Epoch 273/1000 
	 loss: 28.4153, MinusLogProbMetric: 28.4153, val_loss: 29.8066, val_MinusLogProbMetric: 29.8066

Epoch 273: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.4153 - MinusLogProbMetric: 28.4153 - val_loss: 29.8066 - val_MinusLogProbMetric: 29.8066 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 274/1000
2023-10-26 02:44:03.194 
Epoch 274/1000 
	 loss: 28.3854, MinusLogProbMetric: 28.3854, val_loss: 29.0922, val_MinusLogProbMetric: 29.0922

Epoch 274: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3854 - MinusLogProbMetric: 28.3854 - val_loss: 29.0922 - val_MinusLogProbMetric: 29.0922 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 275/1000
2023-10-26 02:44:37.807 
Epoch 275/1000 
	 loss: 28.6029, MinusLogProbMetric: 28.6029, val_loss: 29.0147, val_MinusLogProbMetric: 29.0147

Epoch 275: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.6029 - MinusLogProbMetric: 28.6029 - val_loss: 29.0147 - val_MinusLogProbMetric: 29.0147 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 276/1000
2023-10-26 02:45:12.394 
Epoch 276/1000 
	 loss: 28.4271, MinusLogProbMetric: 28.4271, val_loss: 29.9212, val_MinusLogProbMetric: 29.9212

Epoch 276: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.4271 - MinusLogProbMetric: 28.4271 - val_loss: 29.9212 - val_MinusLogProbMetric: 29.9212 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 277/1000
2023-10-26 02:45:47.159 
Epoch 277/1000 
	 loss: 28.3508, MinusLogProbMetric: 28.3508, val_loss: 29.7659, val_MinusLogProbMetric: 29.7659

Epoch 277: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3508 - MinusLogProbMetric: 28.3508 - val_loss: 29.7659 - val_MinusLogProbMetric: 29.7659 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 278/1000
2023-10-26 02:46:21.980 
Epoch 278/1000 
	 loss: 28.3778, MinusLogProbMetric: 28.3778, val_loss: 29.1497, val_MinusLogProbMetric: 29.1497

Epoch 278: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3778 - MinusLogProbMetric: 28.3778 - val_loss: 29.1497 - val_MinusLogProbMetric: 29.1497 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 279/1000
2023-10-26 02:46:56.861 
Epoch 279/1000 
	 loss: 28.3637, MinusLogProbMetric: 28.3637, val_loss: 29.1585, val_MinusLogProbMetric: 29.1585

Epoch 279: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3637 - MinusLogProbMetric: 28.3637 - val_loss: 29.1585 - val_MinusLogProbMetric: 29.1585 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 280/1000
2023-10-26 02:47:31.082 
Epoch 280/1000 
	 loss: 28.4604, MinusLogProbMetric: 28.4604, val_loss: 29.6558, val_MinusLogProbMetric: 29.6558

Epoch 280: val_loss did not improve from 28.92610
196/196 - 34s - loss: 28.4604 - MinusLogProbMetric: 28.4604 - val_loss: 29.6558 - val_MinusLogProbMetric: 29.6558 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 281/1000
2023-10-26 02:48:05.672 
Epoch 281/1000 
	 loss: 28.3541, MinusLogProbMetric: 28.3541, val_loss: 29.2473, val_MinusLogProbMetric: 29.2473

Epoch 281: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3541 - MinusLogProbMetric: 28.3541 - val_loss: 29.2473 - val_MinusLogProbMetric: 29.2473 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 282/1000
2023-10-26 02:48:40.307 
Epoch 282/1000 
	 loss: 28.3055, MinusLogProbMetric: 28.3055, val_loss: 29.3813, val_MinusLogProbMetric: 29.3813

Epoch 282: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3055 - MinusLogProbMetric: 28.3055 - val_loss: 29.3813 - val_MinusLogProbMetric: 29.3813 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 283/1000
2023-10-26 02:49:14.965 
Epoch 283/1000 
	 loss: 28.3727, MinusLogProbMetric: 28.3727, val_loss: 29.3826, val_MinusLogProbMetric: 29.3826

Epoch 283: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3727 - MinusLogProbMetric: 28.3727 - val_loss: 29.3826 - val_MinusLogProbMetric: 29.3826 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 284/1000
2023-10-26 02:49:49.844 
Epoch 284/1000 
	 loss: 28.3402, MinusLogProbMetric: 28.3402, val_loss: 29.5672, val_MinusLogProbMetric: 29.5672

Epoch 284: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3402 - MinusLogProbMetric: 28.3402 - val_loss: 29.5672 - val_MinusLogProbMetric: 29.5672 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 285/1000
2023-10-26 02:50:24.517 
Epoch 285/1000 
	 loss: 28.3911, MinusLogProbMetric: 28.3911, val_loss: 29.3250, val_MinusLogProbMetric: 29.3250

Epoch 285: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3911 - MinusLogProbMetric: 28.3911 - val_loss: 29.3250 - val_MinusLogProbMetric: 29.3250 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 286/1000
2023-10-26 02:50:59.195 
Epoch 286/1000 
	 loss: 28.3242, MinusLogProbMetric: 28.3242, val_loss: 29.3107, val_MinusLogProbMetric: 29.3107

Epoch 286: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3242 - MinusLogProbMetric: 28.3242 - val_loss: 29.3107 - val_MinusLogProbMetric: 29.3107 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 287/1000
2023-10-26 02:51:33.998 
Epoch 287/1000 
	 loss: 28.3059, MinusLogProbMetric: 28.3059, val_loss: 29.4316, val_MinusLogProbMetric: 29.4316

Epoch 287: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3059 - MinusLogProbMetric: 28.3059 - val_loss: 29.4316 - val_MinusLogProbMetric: 29.4316 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 288/1000
2023-10-26 02:52:08.533 
Epoch 288/1000 
	 loss: 28.3197, MinusLogProbMetric: 28.3197, val_loss: 29.4790, val_MinusLogProbMetric: 29.4790

Epoch 288: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.3197 - MinusLogProbMetric: 28.3197 - val_loss: 29.4790 - val_MinusLogProbMetric: 29.4790 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 289/1000
2023-10-26 02:52:43.060 
Epoch 289/1000 
	 loss: 28.2868, MinusLogProbMetric: 28.2868, val_loss: 29.2839, val_MinusLogProbMetric: 29.2839

Epoch 289: val_loss did not improve from 28.92610
196/196 - 35s - loss: 28.2868 - MinusLogProbMetric: 28.2868 - val_loss: 29.2839 - val_MinusLogProbMetric: 29.2839 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 290/1000
2023-10-26 02:53:17.640 
Epoch 290/1000 
	 loss: 28.3444, MinusLogProbMetric: 28.3444, val_loss: 28.8597, val_MinusLogProbMetric: 28.8597

Epoch 290: val_loss improved from 28.92610 to 28.85972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 28.3444 - MinusLogProbMetric: 28.3444 - val_loss: 28.8597 - val_MinusLogProbMetric: 28.8597 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 291/1000
2023-10-26 02:53:52.895 
Epoch 291/1000 
	 loss: 28.3023, MinusLogProbMetric: 28.3023, val_loss: 29.3190, val_MinusLogProbMetric: 29.3190

Epoch 291: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.3023 - MinusLogProbMetric: 28.3023 - val_loss: 29.3190 - val_MinusLogProbMetric: 29.3190 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 292/1000
2023-10-26 02:54:27.352 
Epoch 292/1000 
	 loss: 28.3332, MinusLogProbMetric: 28.3332, val_loss: 29.3774, val_MinusLogProbMetric: 29.3774

Epoch 292: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.3332 - MinusLogProbMetric: 28.3332 - val_loss: 29.3774 - val_MinusLogProbMetric: 29.3774 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 293/1000
2023-10-26 02:55:01.861 
Epoch 293/1000 
	 loss: 28.3341, MinusLogProbMetric: 28.3341, val_loss: 30.8355, val_MinusLogProbMetric: 30.8355

Epoch 293: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.3341 - MinusLogProbMetric: 28.3341 - val_loss: 30.8355 - val_MinusLogProbMetric: 30.8355 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 294/1000
2023-10-26 02:55:36.506 
Epoch 294/1000 
	 loss: 28.3942, MinusLogProbMetric: 28.3942, val_loss: 29.5613, val_MinusLogProbMetric: 29.5613

Epoch 294: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.3942 - MinusLogProbMetric: 28.3942 - val_loss: 29.5613 - val_MinusLogProbMetric: 29.5613 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 295/1000
2023-10-26 02:56:11.172 
Epoch 295/1000 
	 loss: 28.2394, MinusLogProbMetric: 28.2394, val_loss: 28.9453, val_MinusLogProbMetric: 28.9453

Epoch 295: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2394 - MinusLogProbMetric: 28.2394 - val_loss: 28.9453 - val_MinusLogProbMetric: 28.9453 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 296/1000
2023-10-26 02:56:45.694 
Epoch 296/1000 
	 loss: 28.3034, MinusLogProbMetric: 28.3034, val_loss: 29.0719, val_MinusLogProbMetric: 29.0719

Epoch 296: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.3034 - MinusLogProbMetric: 28.3034 - val_loss: 29.0719 - val_MinusLogProbMetric: 29.0719 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 297/1000
2023-10-26 02:57:20.378 
Epoch 297/1000 
	 loss: 28.2291, MinusLogProbMetric: 28.2291, val_loss: 29.8474, val_MinusLogProbMetric: 29.8474

Epoch 297: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2291 - MinusLogProbMetric: 28.2291 - val_loss: 29.8474 - val_MinusLogProbMetric: 29.8474 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 298/1000
2023-10-26 02:57:55.102 
Epoch 298/1000 
	 loss: 28.2516, MinusLogProbMetric: 28.2516, val_loss: 28.9124, val_MinusLogProbMetric: 28.9124

Epoch 298: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2516 - MinusLogProbMetric: 28.2516 - val_loss: 28.9124 - val_MinusLogProbMetric: 28.9124 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 299/1000
2023-10-26 02:58:29.789 
Epoch 299/1000 
	 loss: 28.4450, MinusLogProbMetric: 28.4450, val_loss: 30.3981, val_MinusLogProbMetric: 30.3981

Epoch 299: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.4450 - MinusLogProbMetric: 28.4450 - val_loss: 30.3981 - val_MinusLogProbMetric: 30.3981 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 300/1000
2023-10-26 02:59:04.334 
Epoch 300/1000 
	 loss: 28.4393, MinusLogProbMetric: 28.4393, val_loss: 29.8054, val_MinusLogProbMetric: 29.8054

Epoch 300: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.4393 - MinusLogProbMetric: 28.4393 - val_loss: 29.8054 - val_MinusLogProbMetric: 29.8054 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 301/1000
2023-10-26 02:59:38.977 
Epoch 301/1000 
	 loss: 28.3767, MinusLogProbMetric: 28.3767, val_loss: 29.1876, val_MinusLogProbMetric: 29.1876

Epoch 301: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.3767 - MinusLogProbMetric: 28.3767 - val_loss: 29.1876 - val_MinusLogProbMetric: 29.1876 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 302/1000
2023-10-26 03:00:13.715 
Epoch 302/1000 
	 loss: 28.2524, MinusLogProbMetric: 28.2524, val_loss: 29.3643, val_MinusLogProbMetric: 29.3643

Epoch 302: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2524 - MinusLogProbMetric: 28.2524 - val_loss: 29.3643 - val_MinusLogProbMetric: 29.3643 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 303/1000
2023-10-26 03:00:48.557 
Epoch 303/1000 
	 loss: 28.4261, MinusLogProbMetric: 28.4261, val_loss: 29.2241, val_MinusLogProbMetric: 29.2241

Epoch 303: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.4261 - MinusLogProbMetric: 28.4261 - val_loss: 29.2241 - val_MinusLogProbMetric: 29.2241 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 304/1000
2023-10-26 03:01:23.096 
Epoch 304/1000 
	 loss: 28.2365, MinusLogProbMetric: 28.2365, val_loss: 29.5840, val_MinusLogProbMetric: 29.5840

Epoch 304: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2365 - MinusLogProbMetric: 28.2365 - val_loss: 29.5840 - val_MinusLogProbMetric: 29.5840 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 305/1000
2023-10-26 03:01:57.990 
Epoch 305/1000 
	 loss: 28.2226, MinusLogProbMetric: 28.2226, val_loss: 29.4214, val_MinusLogProbMetric: 29.4214

Epoch 305: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2226 - MinusLogProbMetric: 28.2226 - val_loss: 29.4214 - val_MinusLogProbMetric: 29.4214 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 306/1000
2023-10-26 03:02:32.385 
Epoch 306/1000 
	 loss: 28.2664, MinusLogProbMetric: 28.2664, val_loss: 29.1427, val_MinusLogProbMetric: 29.1427

Epoch 306: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.2664 - MinusLogProbMetric: 28.2664 - val_loss: 29.1427 - val_MinusLogProbMetric: 29.1427 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 307/1000
2023-10-26 03:03:07.344 
Epoch 307/1000 
	 loss: 28.2660, MinusLogProbMetric: 28.2660, val_loss: 28.9699, val_MinusLogProbMetric: 28.9699

Epoch 307: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2660 - MinusLogProbMetric: 28.2660 - val_loss: 28.9699 - val_MinusLogProbMetric: 28.9699 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 308/1000
2023-10-26 03:03:41.723 
Epoch 308/1000 
	 loss: 28.2197, MinusLogProbMetric: 28.2197, val_loss: 29.2556, val_MinusLogProbMetric: 29.2556

Epoch 308: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.2197 - MinusLogProbMetric: 28.2197 - val_loss: 29.2556 - val_MinusLogProbMetric: 29.2556 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 309/1000
2023-10-26 03:04:16.076 
Epoch 309/1000 
	 loss: 28.2670, MinusLogProbMetric: 28.2670, val_loss: 29.0723, val_MinusLogProbMetric: 29.0723

Epoch 309: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.2670 - MinusLogProbMetric: 28.2670 - val_loss: 29.0723 - val_MinusLogProbMetric: 29.0723 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 310/1000
2023-10-26 03:04:50.838 
Epoch 310/1000 
	 loss: 28.2494, MinusLogProbMetric: 28.2494, val_loss: 29.3271, val_MinusLogProbMetric: 29.3271

Epoch 310: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2494 - MinusLogProbMetric: 28.2494 - val_loss: 29.3271 - val_MinusLogProbMetric: 29.3271 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 311/1000
2023-10-26 03:05:25.816 
Epoch 311/1000 
	 loss: 28.2245, MinusLogProbMetric: 28.2245, val_loss: 29.0622, val_MinusLogProbMetric: 29.0622

Epoch 311: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2245 - MinusLogProbMetric: 28.2245 - val_loss: 29.0622 - val_MinusLogProbMetric: 29.0622 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 312/1000
2023-10-26 03:06:00.300 
Epoch 312/1000 
	 loss: 28.2275, MinusLogProbMetric: 28.2275, val_loss: 29.8652, val_MinusLogProbMetric: 29.8652

Epoch 312: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.2275 - MinusLogProbMetric: 28.2275 - val_loss: 29.8652 - val_MinusLogProbMetric: 29.8652 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 313/1000
2023-10-26 03:06:34.805 
Epoch 313/1000 
	 loss: 28.2323, MinusLogProbMetric: 28.2323, val_loss: 29.2370, val_MinusLogProbMetric: 29.2370

Epoch 313: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2323 - MinusLogProbMetric: 28.2323 - val_loss: 29.2370 - val_MinusLogProbMetric: 29.2370 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 314/1000
2023-10-26 03:07:09.598 
Epoch 314/1000 
	 loss: 28.2394, MinusLogProbMetric: 28.2394, val_loss: 29.0232, val_MinusLogProbMetric: 29.0232

Epoch 314: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2394 - MinusLogProbMetric: 28.2394 - val_loss: 29.0232 - val_MinusLogProbMetric: 29.0232 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 315/1000
2023-10-26 03:07:44.264 
Epoch 315/1000 
	 loss: 28.2359, MinusLogProbMetric: 28.2359, val_loss: 29.6292, val_MinusLogProbMetric: 29.6292

Epoch 315: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.2359 - MinusLogProbMetric: 28.2359 - val_loss: 29.6292 - val_MinusLogProbMetric: 29.6292 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 316/1000
2023-10-26 03:08:19.130 
Epoch 316/1000 
	 loss: 28.3330, MinusLogProbMetric: 28.3330, val_loss: 28.9582, val_MinusLogProbMetric: 28.9582

Epoch 316: val_loss did not improve from 28.85972
196/196 - 35s - loss: 28.3330 - MinusLogProbMetric: 28.3330 - val_loss: 28.9582 - val_MinusLogProbMetric: 28.9582 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 317/1000
2023-10-26 03:08:53.247 
Epoch 317/1000 
	 loss: 28.1546, MinusLogProbMetric: 28.1546, val_loss: 29.2425, val_MinusLogProbMetric: 29.2425

Epoch 317: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.1546 - MinusLogProbMetric: 28.1546 - val_loss: 29.2425 - val_MinusLogProbMetric: 29.2425 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 318/1000
2023-10-26 03:09:27.688 
Epoch 318/1000 
	 loss: 28.3529, MinusLogProbMetric: 28.3529, val_loss: 29.1542, val_MinusLogProbMetric: 29.1542

Epoch 318: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.3529 - MinusLogProbMetric: 28.3529 - val_loss: 29.1542 - val_MinusLogProbMetric: 29.1542 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 319/1000
2023-10-26 03:10:01.208 
Epoch 319/1000 
	 loss: 28.1442, MinusLogProbMetric: 28.1442, val_loss: 29.2522, val_MinusLogProbMetric: 29.2522

Epoch 319: val_loss did not improve from 28.85972
196/196 - 34s - loss: 28.1442 - MinusLogProbMetric: 28.1442 - val_loss: 29.2522 - val_MinusLogProbMetric: 29.2522 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 320/1000
2023-10-26 03:10:32.805 
Epoch 320/1000 
	 loss: 28.2560, MinusLogProbMetric: 28.2560, val_loss: 29.2870, val_MinusLogProbMetric: 29.2870

Epoch 320: val_loss did not improve from 28.85972
196/196 - 32s - loss: 28.2560 - MinusLogProbMetric: 28.2560 - val_loss: 29.2870 - val_MinusLogProbMetric: 29.2870 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 321/1000
2023-10-26 03:11:05.771 
Epoch 321/1000 
	 loss: 28.1529, MinusLogProbMetric: 28.1529, val_loss: 29.4694, val_MinusLogProbMetric: 29.4694

Epoch 321: val_loss did not improve from 28.85972
196/196 - 33s - loss: 28.1529 - MinusLogProbMetric: 28.1529 - val_loss: 29.4694 - val_MinusLogProbMetric: 29.4694 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 322/1000
2023-10-26 03:11:39.592 
Epoch 322/1000 
	 loss: 28.2754, MinusLogProbMetric: 28.2754, val_loss: 28.8544, val_MinusLogProbMetric: 28.8544

Epoch 322: val_loss improved from 28.85972 to 28.85437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 34s - loss: 28.2754 - MinusLogProbMetric: 28.2754 - val_loss: 28.8544 - val_MinusLogProbMetric: 28.8544 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 323/1000
2023-10-26 03:12:14.639 
Epoch 323/1000 
	 loss: 28.2564, MinusLogProbMetric: 28.2564, val_loss: 29.1754, val_MinusLogProbMetric: 29.1754

Epoch 323: val_loss did not improve from 28.85437
196/196 - 34s - loss: 28.2564 - MinusLogProbMetric: 28.2564 - val_loss: 29.1754 - val_MinusLogProbMetric: 29.1754 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 324/1000
2023-10-26 03:12:49.250 
Epoch 324/1000 
	 loss: 28.2175, MinusLogProbMetric: 28.2175, val_loss: 29.6616, val_MinusLogProbMetric: 29.6616

Epoch 324: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.2175 - MinusLogProbMetric: 28.2175 - val_loss: 29.6616 - val_MinusLogProbMetric: 29.6616 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 325/1000
2023-10-26 03:13:23.985 
Epoch 325/1000 
	 loss: 28.2077, MinusLogProbMetric: 28.2077, val_loss: 29.0073, val_MinusLogProbMetric: 29.0073

Epoch 325: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.2077 - MinusLogProbMetric: 28.2077 - val_loss: 29.0073 - val_MinusLogProbMetric: 29.0073 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 326/1000
2023-10-26 03:13:58.503 
Epoch 326/1000 
	 loss: 28.2047, MinusLogProbMetric: 28.2047, val_loss: 29.6309, val_MinusLogProbMetric: 29.6309

Epoch 326: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.2047 - MinusLogProbMetric: 28.2047 - val_loss: 29.6309 - val_MinusLogProbMetric: 29.6309 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 327/1000
2023-10-26 03:14:33.018 
Epoch 327/1000 
	 loss: 28.2595, MinusLogProbMetric: 28.2595, val_loss: 29.4138, val_MinusLogProbMetric: 29.4138

Epoch 327: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.2595 - MinusLogProbMetric: 28.2595 - val_loss: 29.4138 - val_MinusLogProbMetric: 29.4138 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 328/1000
2023-10-26 03:15:07.703 
Epoch 328/1000 
	 loss: 28.1493, MinusLogProbMetric: 28.1493, val_loss: 29.0611, val_MinusLogProbMetric: 29.0611

Epoch 328: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1493 - MinusLogProbMetric: 28.1493 - val_loss: 29.0611 - val_MinusLogProbMetric: 29.0611 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 329/1000
2023-10-26 03:15:42.292 
Epoch 329/1000 
	 loss: 28.1143, MinusLogProbMetric: 28.1143, val_loss: 30.0827, val_MinusLogProbMetric: 30.0827

Epoch 329: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1143 - MinusLogProbMetric: 28.1143 - val_loss: 30.0827 - val_MinusLogProbMetric: 30.0827 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 330/1000
2023-10-26 03:16:16.975 
Epoch 330/1000 
	 loss: 28.2323, MinusLogProbMetric: 28.2323, val_loss: 29.5418, val_MinusLogProbMetric: 29.5418

Epoch 330: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.2323 - MinusLogProbMetric: 28.2323 - val_loss: 29.5418 - val_MinusLogProbMetric: 29.5418 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 331/1000
2023-10-26 03:16:51.492 
Epoch 331/1000 
	 loss: 28.1770, MinusLogProbMetric: 28.1770, val_loss: 29.0449, val_MinusLogProbMetric: 29.0449

Epoch 331: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1770 - MinusLogProbMetric: 28.1770 - val_loss: 29.0449 - val_MinusLogProbMetric: 29.0449 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 332/1000
2023-10-26 03:17:26.311 
Epoch 332/1000 
	 loss: 28.1380, MinusLogProbMetric: 28.1380, val_loss: 29.1534, val_MinusLogProbMetric: 29.1534

Epoch 332: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1380 - MinusLogProbMetric: 28.1380 - val_loss: 29.1534 - val_MinusLogProbMetric: 29.1534 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 333/1000
2023-10-26 03:18:00.992 
Epoch 333/1000 
	 loss: 28.2515, MinusLogProbMetric: 28.2515, val_loss: 29.0565, val_MinusLogProbMetric: 29.0565

Epoch 333: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.2515 - MinusLogProbMetric: 28.2515 - val_loss: 29.0565 - val_MinusLogProbMetric: 29.0565 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 334/1000
2023-10-26 03:18:35.779 
Epoch 334/1000 
	 loss: 28.1286, MinusLogProbMetric: 28.1286, val_loss: 29.1193, val_MinusLogProbMetric: 29.1193

Epoch 334: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1286 - MinusLogProbMetric: 28.1286 - val_loss: 29.1193 - val_MinusLogProbMetric: 29.1193 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 335/1000
2023-10-26 03:19:10.379 
Epoch 335/1000 
	 loss: 28.2231, MinusLogProbMetric: 28.2231, val_loss: 29.2439, val_MinusLogProbMetric: 29.2439

Epoch 335: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.2231 - MinusLogProbMetric: 28.2231 - val_loss: 29.2439 - val_MinusLogProbMetric: 29.2439 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 336/1000
2023-10-26 03:19:44.934 
Epoch 336/1000 
	 loss: 28.0874, MinusLogProbMetric: 28.0874, val_loss: 29.2902, val_MinusLogProbMetric: 29.2902

Epoch 336: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0874 - MinusLogProbMetric: 28.0874 - val_loss: 29.2902 - val_MinusLogProbMetric: 29.2902 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 337/1000
2023-10-26 03:20:19.389 
Epoch 337/1000 
	 loss: 28.1025, MinusLogProbMetric: 28.1025, val_loss: 29.2448, val_MinusLogProbMetric: 29.2448

Epoch 337: val_loss did not improve from 28.85437
196/196 - 34s - loss: 28.1025 - MinusLogProbMetric: 28.1025 - val_loss: 29.2448 - val_MinusLogProbMetric: 29.2448 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 338/1000
2023-10-26 03:20:54.064 
Epoch 338/1000 
	 loss: 28.1249, MinusLogProbMetric: 28.1249, val_loss: 28.9275, val_MinusLogProbMetric: 28.9275

Epoch 338: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1249 - MinusLogProbMetric: 28.1249 - val_loss: 28.9275 - val_MinusLogProbMetric: 28.9275 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 339/1000
2023-10-26 03:21:28.573 
Epoch 339/1000 
	 loss: 28.1331, MinusLogProbMetric: 28.1331, val_loss: 29.0510, val_MinusLogProbMetric: 29.0510

Epoch 339: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1331 - MinusLogProbMetric: 28.1331 - val_loss: 29.0510 - val_MinusLogProbMetric: 29.0510 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 340/1000
2023-10-26 03:22:03.367 
Epoch 340/1000 
	 loss: 28.1792, MinusLogProbMetric: 28.1792, val_loss: 28.9247, val_MinusLogProbMetric: 28.9247

Epoch 340: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1792 - MinusLogProbMetric: 28.1792 - val_loss: 28.9247 - val_MinusLogProbMetric: 28.9247 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 341/1000
2023-10-26 03:22:38.065 
Epoch 341/1000 
	 loss: 28.1000, MinusLogProbMetric: 28.1000, val_loss: 29.2054, val_MinusLogProbMetric: 29.2054

Epoch 341: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1000 - MinusLogProbMetric: 28.1000 - val_loss: 29.2054 - val_MinusLogProbMetric: 29.2054 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 342/1000
2023-10-26 03:23:12.875 
Epoch 342/1000 
	 loss: 28.3211, MinusLogProbMetric: 28.3211, val_loss: 28.9546, val_MinusLogProbMetric: 28.9546

Epoch 342: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.3211 - MinusLogProbMetric: 28.3211 - val_loss: 28.9546 - val_MinusLogProbMetric: 28.9546 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 343/1000
2023-10-26 03:23:47.768 
Epoch 343/1000 
	 loss: 28.1773, MinusLogProbMetric: 28.1773, val_loss: 29.4251, val_MinusLogProbMetric: 29.4251

Epoch 343: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1773 - MinusLogProbMetric: 28.1773 - val_loss: 29.4251 - val_MinusLogProbMetric: 29.4251 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 344/1000
2023-10-26 03:24:22.548 
Epoch 344/1000 
	 loss: 28.1425, MinusLogProbMetric: 28.1425, val_loss: 29.0062, val_MinusLogProbMetric: 29.0062

Epoch 344: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1425 - MinusLogProbMetric: 28.1425 - val_loss: 29.0062 - val_MinusLogProbMetric: 29.0062 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 345/1000
2023-10-26 03:24:57.167 
Epoch 345/1000 
	 loss: 28.1069, MinusLogProbMetric: 28.1069, val_loss: 29.2547, val_MinusLogProbMetric: 29.2547

Epoch 345: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1069 - MinusLogProbMetric: 28.1069 - val_loss: 29.2547 - val_MinusLogProbMetric: 29.2547 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 346/1000
2023-10-26 03:25:31.716 
Epoch 346/1000 
	 loss: 28.0826, MinusLogProbMetric: 28.0826, val_loss: 29.2249, val_MinusLogProbMetric: 29.2249

Epoch 346: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0826 - MinusLogProbMetric: 28.0826 - val_loss: 29.2249 - val_MinusLogProbMetric: 29.2249 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 347/1000
2023-10-26 03:26:06.367 
Epoch 347/1000 
	 loss: 28.1444, MinusLogProbMetric: 28.1444, val_loss: 29.1309, val_MinusLogProbMetric: 29.1309

Epoch 347: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1444 - MinusLogProbMetric: 28.1444 - val_loss: 29.1309 - val_MinusLogProbMetric: 29.1309 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 348/1000
2023-10-26 03:26:40.935 
Epoch 348/1000 
	 loss: 28.1461, MinusLogProbMetric: 28.1461, val_loss: 29.3375, val_MinusLogProbMetric: 29.3375

Epoch 348: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1461 - MinusLogProbMetric: 28.1461 - val_loss: 29.3375 - val_MinusLogProbMetric: 29.3375 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 349/1000
2023-10-26 03:27:15.482 
Epoch 349/1000 
	 loss: 28.1434, MinusLogProbMetric: 28.1434, val_loss: 29.2869, val_MinusLogProbMetric: 29.2869

Epoch 349: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1434 - MinusLogProbMetric: 28.1434 - val_loss: 29.2869 - val_MinusLogProbMetric: 29.2869 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 350/1000
2023-10-26 03:27:49.841 
Epoch 350/1000 
	 loss: 28.1384, MinusLogProbMetric: 28.1384, val_loss: 30.2420, val_MinusLogProbMetric: 30.2420

Epoch 350: val_loss did not improve from 28.85437
196/196 - 34s - loss: 28.1384 - MinusLogProbMetric: 28.1384 - val_loss: 30.2420 - val_MinusLogProbMetric: 30.2420 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 351/1000
2023-10-26 03:28:24.421 
Epoch 351/1000 
	 loss: 28.0970, MinusLogProbMetric: 28.0970, val_loss: 29.0958, val_MinusLogProbMetric: 29.0958

Epoch 351: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0970 - MinusLogProbMetric: 28.0970 - val_loss: 29.0958 - val_MinusLogProbMetric: 29.0958 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 352/1000
2023-10-26 03:28:59.075 
Epoch 352/1000 
	 loss: 28.0497, MinusLogProbMetric: 28.0497, val_loss: 29.6199, val_MinusLogProbMetric: 29.6199

Epoch 352: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0497 - MinusLogProbMetric: 28.0497 - val_loss: 29.6199 - val_MinusLogProbMetric: 29.6199 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 353/1000
2023-10-26 03:29:33.556 
Epoch 353/1000 
	 loss: 28.0859, MinusLogProbMetric: 28.0859, val_loss: 29.2245, val_MinusLogProbMetric: 29.2245

Epoch 353: val_loss did not improve from 28.85437
196/196 - 34s - loss: 28.0859 - MinusLogProbMetric: 28.0859 - val_loss: 29.2245 - val_MinusLogProbMetric: 29.2245 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 354/1000
2023-10-26 03:30:08.382 
Epoch 354/1000 
	 loss: 28.0921, MinusLogProbMetric: 28.0921, val_loss: 28.9646, val_MinusLogProbMetric: 28.9646

Epoch 354: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0921 - MinusLogProbMetric: 28.0921 - val_loss: 28.9646 - val_MinusLogProbMetric: 28.9646 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 355/1000
2023-10-26 03:30:42.981 
Epoch 355/1000 
	 loss: 28.1022, MinusLogProbMetric: 28.1022, val_loss: 29.1152, val_MinusLogProbMetric: 29.1152

Epoch 355: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1022 - MinusLogProbMetric: 28.1022 - val_loss: 29.1152 - val_MinusLogProbMetric: 29.1152 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 356/1000
2023-10-26 03:31:17.578 
Epoch 356/1000 
	 loss: 28.0821, MinusLogProbMetric: 28.0821, val_loss: 29.0047, val_MinusLogProbMetric: 29.0047

Epoch 356: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0821 - MinusLogProbMetric: 28.0821 - val_loss: 29.0047 - val_MinusLogProbMetric: 29.0047 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 357/1000
2023-10-26 03:31:52.362 
Epoch 357/1000 
	 loss: 28.0876, MinusLogProbMetric: 28.0876, val_loss: 29.2687, val_MinusLogProbMetric: 29.2687

Epoch 357: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0876 - MinusLogProbMetric: 28.0876 - val_loss: 29.2687 - val_MinusLogProbMetric: 29.2687 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 358/1000
2023-10-26 03:32:27.265 
Epoch 358/1000 
	 loss: 28.1327, MinusLogProbMetric: 28.1327, val_loss: 29.0432, val_MinusLogProbMetric: 29.0432

Epoch 358: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1327 - MinusLogProbMetric: 28.1327 - val_loss: 29.0432 - val_MinusLogProbMetric: 29.0432 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 359/1000
2023-10-26 03:33:02.110 
Epoch 359/1000 
	 loss: 28.0325, MinusLogProbMetric: 28.0325, val_loss: 29.2546, val_MinusLogProbMetric: 29.2546

Epoch 359: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0325 - MinusLogProbMetric: 28.0325 - val_loss: 29.2546 - val_MinusLogProbMetric: 29.2546 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 360/1000
2023-10-26 03:33:36.925 
Epoch 360/1000 
	 loss: 28.0788, MinusLogProbMetric: 28.0788, val_loss: 29.4043, val_MinusLogProbMetric: 29.4043

Epoch 360: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0788 - MinusLogProbMetric: 28.0788 - val_loss: 29.4043 - val_MinusLogProbMetric: 29.4043 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 361/1000
2023-10-26 03:34:11.861 
Epoch 361/1000 
	 loss: 28.1271, MinusLogProbMetric: 28.1271, val_loss: 29.2857, val_MinusLogProbMetric: 29.2857

Epoch 361: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.1271 - MinusLogProbMetric: 28.1271 - val_loss: 29.2857 - val_MinusLogProbMetric: 29.2857 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 362/1000
2023-10-26 03:34:46.679 
Epoch 362/1000 
	 loss: 28.0807, MinusLogProbMetric: 28.0807, val_loss: 29.0863, val_MinusLogProbMetric: 29.0863

Epoch 362: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0807 - MinusLogProbMetric: 28.0807 - val_loss: 29.0863 - val_MinusLogProbMetric: 29.0863 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 363/1000
2023-10-26 03:35:21.793 
Epoch 363/1000 
	 loss: 28.0427, MinusLogProbMetric: 28.0427, val_loss: 29.0986, val_MinusLogProbMetric: 29.0986

Epoch 363: val_loss did not improve from 28.85437
196/196 - 35s - loss: 28.0427 - MinusLogProbMetric: 28.0427 - val_loss: 29.0986 - val_MinusLogProbMetric: 29.0986 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 364/1000
2023-10-26 03:35:56.768 
Epoch 364/1000 
	 loss: 28.0660, MinusLogProbMetric: 28.0660, val_loss: 28.8444, val_MinusLogProbMetric: 28.8444

Epoch 364: val_loss improved from 28.85437 to 28.84435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 28.0660 - MinusLogProbMetric: 28.0660 - val_loss: 28.8444 - val_MinusLogProbMetric: 28.8444 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 365/1000
2023-10-26 03:36:31.971 
Epoch 365/1000 
	 loss: 28.0345, MinusLogProbMetric: 28.0345, val_loss: 29.0054, val_MinusLogProbMetric: 29.0054

Epoch 365: val_loss did not improve from 28.84435
196/196 - 34s - loss: 28.0345 - MinusLogProbMetric: 28.0345 - val_loss: 29.0054 - val_MinusLogProbMetric: 29.0054 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 366/1000
2023-10-26 03:37:06.306 
Epoch 366/1000 
	 loss: 28.0807, MinusLogProbMetric: 28.0807, val_loss: 29.2259, val_MinusLogProbMetric: 29.2259

Epoch 366: val_loss did not improve from 28.84435
196/196 - 34s - loss: 28.0807 - MinusLogProbMetric: 28.0807 - val_loss: 29.2259 - val_MinusLogProbMetric: 29.2259 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 367/1000
2023-10-26 03:37:40.802 
Epoch 367/1000 
	 loss: 28.0299, MinusLogProbMetric: 28.0299, val_loss: 29.1682, val_MinusLogProbMetric: 29.1682

Epoch 367: val_loss did not improve from 28.84435
196/196 - 34s - loss: 28.0299 - MinusLogProbMetric: 28.0299 - val_loss: 29.1682 - val_MinusLogProbMetric: 29.1682 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 368/1000
2023-10-26 03:38:15.358 
Epoch 368/1000 
	 loss: 28.0486, MinusLogProbMetric: 28.0486, val_loss: 29.9045, val_MinusLogProbMetric: 29.9045

Epoch 368: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0486 - MinusLogProbMetric: 28.0486 - val_loss: 29.9045 - val_MinusLogProbMetric: 29.9045 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 369/1000
2023-10-26 03:38:49.922 
Epoch 369/1000 
	 loss: 28.0666, MinusLogProbMetric: 28.0666, val_loss: 28.8944, val_MinusLogProbMetric: 28.8944

Epoch 369: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0666 - MinusLogProbMetric: 28.0666 - val_loss: 28.8944 - val_MinusLogProbMetric: 28.8944 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 370/1000
2023-10-26 03:39:24.600 
Epoch 370/1000 
	 loss: 28.0567, MinusLogProbMetric: 28.0567, val_loss: 29.0161, val_MinusLogProbMetric: 29.0161

Epoch 370: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0567 - MinusLogProbMetric: 28.0567 - val_loss: 29.0161 - val_MinusLogProbMetric: 29.0161 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 371/1000
2023-10-26 03:39:59.330 
Epoch 371/1000 
	 loss: 28.0548, MinusLogProbMetric: 28.0548, val_loss: 29.0371, val_MinusLogProbMetric: 29.0371

Epoch 371: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0548 - MinusLogProbMetric: 28.0548 - val_loss: 29.0371 - val_MinusLogProbMetric: 29.0371 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 372/1000
2023-10-26 03:40:33.813 
Epoch 372/1000 
	 loss: 27.9785, MinusLogProbMetric: 27.9785, val_loss: 29.1043, val_MinusLogProbMetric: 29.1043

Epoch 372: val_loss did not improve from 28.84435
196/196 - 34s - loss: 27.9785 - MinusLogProbMetric: 27.9785 - val_loss: 29.1043 - val_MinusLogProbMetric: 29.1043 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 373/1000
2023-10-26 03:41:08.430 
Epoch 373/1000 
	 loss: 27.9429, MinusLogProbMetric: 27.9429, val_loss: 29.2756, val_MinusLogProbMetric: 29.2756

Epoch 373: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9429 - MinusLogProbMetric: 27.9429 - val_loss: 29.2756 - val_MinusLogProbMetric: 29.2756 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 374/1000
2023-10-26 03:41:42.950 
Epoch 374/1000 
	 loss: 28.1325, MinusLogProbMetric: 28.1325, val_loss: 29.7632, val_MinusLogProbMetric: 29.7632

Epoch 374: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.1325 - MinusLogProbMetric: 28.1325 - val_loss: 29.7632 - val_MinusLogProbMetric: 29.7632 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 375/1000
2023-10-26 03:42:17.590 
Epoch 375/1000 
	 loss: 28.0396, MinusLogProbMetric: 28.0396, val_loss: 28.9714, val_MinusLogProbMetric: 28.9714

Epoch 375: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0396 - MinusLogProbMetric: 28.0396 - val_loss: 28.9714 - val_MinusLogProbMetric: 28.9714 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 376/1000
2023-10-26 03:42:52.666 
Epoch 376/1000 
	 loss: 28.0403, MinusLogProbMetric: 28.0403, val_loss: 29.2105, val_MinusLogProbMetric: 29.2105

Epoch 376: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0403 - MinusLogProbMetric: 28.0403 - val_loss: 29.2105 - val_MinusLogProbMetric: 29.2105 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 377/1000
2023-10-26 03:43:27.608 
Epoch 377/1000 
	 loss: 27.9911, MinusLogProbMetric: 27.9911, val_loss: 28.9491, val_MinusLogProbMetric: 28.9491

Epoch 377: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9911 - MinusLogProbMetric: 27.9911 - val_loss: 28.9491 - val_MinusLogProbMetric: 28.9491 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 378/1000
2023-10-26 03:44:02.062 
Epoch 378/1000 
	 loss: 27.9311, MinusLogProbMetric: 27.9311, val_loss: 28.9163, val_MinusLogProbMetric: 28.9163

Epoch 378: val_loss did not improve from 28.84435
196/196 - 34s - loss: 27.9311 - MinusLogProbMetric: 27.9311 - val_loss: 28.9163 - val_MinusLogProbMetric: 28.9163 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 379/1000
2023-10-26 03:44:36.664 
Epoch 379/1000 
	 loss: 27.9719, MinusLogProbMetric: 27.9719, val_loss: 29.4337, val_MinusLogProbMetric: 29.4337

Epoch 379: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9719 - MinusLogProbMetric: 27.9719 - val_loss: 29.4337 - val_MinusLogProbMetric: 29.4337 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 380/1000
2023-10-26 03:45:11.412 
Epoch 380/1000 
	 loss: 27.9971, MinusLogProbMetric: 27.9971, val_loss: 29.4898, val_MinusLogProbMetric: 29.4898

Epoch 380: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9971 - MinusLogProbMetric: 27.9971 - val_loss: 29.4898 - val_MinusLogProbMetric: 29.4898 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 381/1000
2023-10-26 03:45:46.011 
Epoch 381/1000 
	 loss: 28.0201, MinusLogProbMetric: 28.0201, val_loss: 29.0573, val_MinusLogProbMetric: 29.0573

Epoch 381: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0201 - MinusLogProbMetric: 28.0201 - val_loss: 29.0573 - val_MinusLogProbMetric: 29.0573 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 382/1000
2023-10-26 03:46:20.964 
Epoch 382/1000 
	 loss: 28.0164, MinusLogProbMetric: 28.0164, val_loss: 29.0911, val_MinusLogProbMetric: 29.0911

Epoch 382: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0164 - MinusLogProbMetric: 28.0164 - val_loss: 29.0911 - val_MinusLogProbMetric: 29.0911 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 383/1000
2023-10-26 03:46:55.690 
Epoch 383/1000 
	 loss: 27.9886, MinusLogProbMetric: 27.9886, val_loss: 29.0522, val_MinusLogProbMetric: 29.0522

Epoch 383: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9886 - MinusLogProbMetric: 27.9886 - val_loss: 29.0522 - val_MinusLogProbMetric: 29.0522 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 384/1000
2023-10-26 03:47:30.459 
Epoch 384/1000 
	 loss: 28.0389, MinusLogProbMetric: 28.0389, val_loss: 28.9812, val_MinusLogProbMetric: 28.9812

Epoch 384: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0389 - MinusLogProbMetric: 28.0389 - val_loss: 28.9812 - val_MinusLogProbMetric: 28.9812 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 385/1000
2023-10-26 03:48:05.260 
Epoch 385/1000 
	 loss: 28.0290, MinusLogProbMetric: 28.0290, val_loss: 29.1830, val_MinusLogProbMetric: 29.1830

Epoch 385: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0290 - MinusLogProbMetric: 28.0290 - val_loss: 29.1830 - val_MinusLogProbMetric: 29.1830 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 386/1000
2023-10-26 03:48:40.207 
Epoch 386/1000 
	 loss: 27.9771, MinusLogProbMetric: 27.9771, val_loss: 29.0797, val_MinusLogProbMetric: 29.0797

Epoch 386: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9771 - MinusLogProbMetric: 27.9771 - val_loss: 29.0797 - val_MinusLogProbMetric: 29.0797 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 387/1000
2023-10-26 03:49:14.823 
Epoch 387/1000 
	 loss: 27.9563, MinusLogProbMetric: 27.9563, val_loss: 29.3231, val_MinusLogProbMetric: 29.3231

Epoch 387: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9563 - MinusLogProbMetric: 27.9563 - val_loss: 29.3231 - val_MinusLogProbMetric: 29.3231 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 388/1000
2023-10-26 03:49:49.326 
Epoch 388/1000 
	 loss: 28.0632, MinusLogProbMetric: 28.0632, val_loss: 29.7188, val_MinusLogProbMetric: 29.7188

Epoch 388: val_loss did not improve from 28.84435
196/196 - 34s - loss: 28.0632 - MinusLogProbMetric: 28.0632 - val_loss: 29.7188 - val_MinusLogProbMetric: 29.7188 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 389/1000
2023-10-26 03:50:24.085 
Epoch 389/1000 
	 loss: 27.9686, MinusLogProbMetric: 27.9686, val_loss: 29.2025, val_MinusLogProbMetric: 29.2025

Epoch 389: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9686 - MinusLogProbMetric: 27.9686 - val_loss: 29.2025 - val_MinusLogProbMetric: 29.2025 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 390/1000
2023-10-26 03:50:59.194 
Epoch 390/1000 
	 loss: 27.9641, MinusLogProbMetric: 27.9641, val_loss: 29.0441, val_MinusLogProbMetric: 29.0441

Epoch 390: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9641 - MinusLogProbMetric: 27.9641 - val_loss: 29.0441 - val_MinusLogProbMetric: 29.0441 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 391/1000
2023-10-26 03:51:34.121 
Epoch 391/1000 
	 loss: 27.9721, MinusLogProbMetric: 27.9721, val_loss: 29.3188, val_MinusLogProbMetric: 29.3188

Epoch 391: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9721 - MinusLogProbMetric: 27.9721 - val_loss: 29.3188 - val_MinusLogProbMetric: 29.3188 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 392/1000
2023-10-26 03:52:08.910 
Epoch 392/1000 
	 loss: 27.9624, MinusLogProbMetric: 27.9624, val_loss: 30.2151, val_MinusLogProbMetric: 30.2151

Epoch 392: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9624 - MinusLogProbMetric: 27.9624 - val_loss: 30.2151 - val_MinusLogProbMetric: 30.2151 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 393/1000
2023-10-26 03:52:43.818 
Epoch 393/1000 
	 loss: 27.9639, MinusLogProbMetric: 27.9639, val_loss: 29.4825, val_MinusLogProbMetric: 29.4825

Epoch 393: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9639 - MinusLogProbMetric: 27.9639 - val_loss: 29.4825 - val_MinusLogProbMetric: 29.4825 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 394/1000
2023-10-26 03:53:18.676 
Epoch 394/1000 
	 loss: 27.8989, MinusLogProbMetric: 27.8989, val_loss: 28.9899, val_MinusLogProbMetric: 28.9899

Epoch 394: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.8989 - MinusLogProbMetric: 27.8989 - val_loss: 28.9899 - val_MinusLogProbMetric: 28.9899 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 395/1000
2023-10-26 03:53:53.599 
Epoch 395/1000 
	 loss: 27.9310, MinusLogProbMetric: 27.9310, val_loss: 30.2086, val_MinusLogProbMetric: 30.2086

Epoch 395: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9310 - MinusLogProbMetric: 27.9310 - val_loss: 30.2086 - val_MinusLogProbMetric: 30.2086 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 396/1000
2023-10-26 03:54:28.484 
Epoch 396/1000 
	 loss: 27.9811, MinusLogProbMetric: 27.9811, val_loss: 29.7960, val_MinusLogProbMetric: 29.7960

Epoch 396: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9811 - MinusLogProbMetric: 27.9811 - val_loss: 29.7960 - val_MinusLogProbMetric: 29.7960 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 397/1000
2023-10-26 03:55:03.081 
Epoch 397/1000 
	 loss: 28.0495, MinusLogProbMetric: 28.0495, val_loss: 29.1741, val_MinusLogProbMetric: 29.1741

Epoch 397: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0495 - MinusLogProbMetric: 28.0495 - val_loss: 29.1741 - val_MinusLogProbMetric: 29.1741 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 398/1000
2023-10-26 03:55:37.689 
Epoch 398/1000 
	 loss: 27.9873, MinusLogProbMetric: 27.9873, val_loss: 29.0602, val_MinusLogProbMetric: 29.0602

Epoch 398: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9873 - MinusLogProbMetric: 27.9873 - val_loss: 29.0602 - val_MinusLogProbMetric: 29.0602 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 399/1000
2023-10-26 03:56:12.558 
Epoch 399/1000 
	 loss: 27.9057, MinusLogProbMetric: 27.9057, val_loss: 29.3661, val_MinusLogProbMetric: 29.3661

Epoch 399: val_loss did not improve from 28.84435
196/196 - 35s - loss: 27.9057 - MinusLogProbMetric: 27.9057 - val_loss: 29.3661 - val_MinusLogProbMetric: 29.3661 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 400/1000
2023-10-26 03:56:47.261 
Epoch 400/1000 
	 loss: 28.0357, MinusLogProbMetric: 28.0357, val_loss: 29.1549, val_MinusLogProbMetric: 29.1549

Epoch 400: val_loss did not improve from 28.84435
196/196 - 35s - loss: 28.0357 - MinusLogProbMetric: 28.0357 - val_loss: 29.1549 - val_MinusLogProbMetric: 29.1549 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 401/1000
2023-10-26 03:57:22.173 
Epoch 401/1000 
	 loss: 27.8986, MinusLogProbMetric: 27.8986, val_loss: 28.8176, val_MinusLogProbMetric: 28.8176

Epoch 401: val_loss improved from 28.84435 to 28.81763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 36s - loss: 27.8986 - MinusLogProbMetric: 27.8986 - val_loss: 28.8176 - val_MinusLogProbMetric: 28.8176 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 402/1000
2023-10-26 03:57:57.506 
Epoch 402/1000 
	 loss: 28.1038, MinusLogProbMetric: 28.1038, val_loss: 29.2417, val_MinusLogProbMetric: 29.2417

Epoch 402: val_loss did not improve from 28.81763
196/196 - 35s - loss: 28.1038 - MinusLogProbMetric: 28.1038 - val_loss: 29.2417 - val_MinusLogProbMetric: 29.2417 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 403/1000
2023-10-26 03:58:32.520 
Epoch 403/1000 
	 loss: 27.9486, MinusLogProbMetric: 27.9486, val_loss: 28.9797, val_MinusLogProbMetric: 28.9797

Epoch 403: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.9486 - MinusLogProbMetric: 27.9486 - val_loss: 28.9797 - val_MinusLogProbMetric: 28.9797 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 404/1000
2023-10-26 03:59:07.192 
Epoch 404/1000 
	 loss: 27.9241, MinusLogProbMetric: 27.9241, val_loss: 29.3026, val_MinusLogProbMetric: 29.3026

Epoch 404: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.9241 - MinusLogProbMetric: 27.9241 - val_loss: 29.3026 - val_MinusLogProbMetric: 29.3026 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 405/1000
2023-10-26 03:59:42.007 
Epoch 405/1000 
	 loss: 27.9489, MinusLogProbMetric: 27.9489, val_loss: 29.1367, val_MinusLogProbMetric: 29.1367

Epoch 405: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.9489 - MinusLogProbMetric: 27.9489 - val_loss: 29.1367 - val_MinusLogProbMetric: 29.1367 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 406/1000
2023-10-26 04:00:16.708 
Epoch 406/1000 
	 loss: 27.9413, MinusLogProbMetric: 27.9413, val_loss: 29.2777, val_MinusLogProbMetric: 29.2777

Epoch 406: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.9413 - MinusLogProbMetric: 27.9413 - val_loss: 29.2777 - val_MinusLogProbMetric: 29.2777 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 407/1000
2023-10-26 04:00:51.692 
Epoch 407/1000 
	 loss: 27.9276, MinusLogProbMetric: 27.9276, val_loss: 29.6742, val_MinusLogProbMetric: 29.6742

Epoch 407: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.9276 - MinusLogProbMetric: 27.9276 - val_loss: 29.6742 - val_MinusLogProbMetric: 29.6742 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 408/1000
2023-10-26 04:01:26.301 
Epoch 408/1000 
	 loss: 27.9741, MinusLogProbMetric: 27.9741, val_loss: 29.1875, val_MinusLogProbMetric: 29.1875

Epoch 408: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.9741 - MinusLogProbMetric: 27.9741 - val_loss: 29.1875 - val_MinusLogProbMetric: 29.1875 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 409/1000
2023-10-26 04:02:00.915 
Epoch 409/1000 
	 loss: 27.9158, MinusLogProbMetric: 27.9158, val_loss: 29.0125, val_MinusLogProbMetric: 29.0125

Epoch 409: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.9158 - MinusLogProbMetric: 27.9158 - val_loss: 29.0125 - val_MinusLogProbMetric: 29.0125 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 410/1000
2023-10-26 04:02:35.569 
Epoch 410/1000 
	 loss: 27.8888, MinusLogProbMetric: 27.8888, val_loss: 28.8292, val_MinusLogProbMetric: 28.8292

Epoch 410: val_loss did not improve from 28.81763
196/196 - 35s - loss: 27.8888 - MinusLogProbMetric: 27.8888 - val_loss: 28.8292 - val_MinusLogProbMetric: 28.8292 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 411/1000
2023-10-26 04:03:10.258 
Epoch 411/1000 
	 loss: 27.8583, MinusLogProbMetric: 27.8583, val_loss: 28.8056, val_MinusLogProbMetric: 28.8056

Epoch 411: val_loss improved from 28.81763 to 28.80557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 35s - loss: 27.8583 - MinusLogProbMetric: 27.8583 - val_loss: 28.8056 - val_MinusLogProbMetric: 28.8056 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 412/1000
2023-10-26 04:03:45.846 
Epoch 412/1000 
	 loss: 27.9273, MinusLogProbMetric: 27.9273, val_loss: 29.1649, val_MinusLogProbMetric: 29.1649

Epoch 412: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.9273 - MinusLogProbMetric: 27.9273 - val_loss: 29.1649 - val_MinusLogProbMetric: 29.1649 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 413/1000
2023-10-26 04:04:20.776 
Epoch 413/1000 
	 loss: 27.8945, MinusLogProbMetric: 27.8945, val_loss: 29.4849, val_MinusLogProbMetric: 29.4849

Epoch 413: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8945 - MinusLogProbMetric: 27.8945 - val_loss: 29.4849 - val_MinusLogProbMetric: 29.4849 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 414/1000
2023-10-26 04:04:55.532 
Epoch 414/1000 
	 loss: 27.9074, MinusLogProbMetric: 27.9074, val_loss: 29.0713, val_MinusLogProbMetric: 29.0713

Epoch 414: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.9074 - MinusLogProbMetric: 27.9074 - val_loss: 29.0713 - val_MinusLogProbMetric: 29.0713 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 415/1000
2023-10-26 04:05:30.166 
Epoch 415/1000 
	 loss: 27.9447, MinusLogProbMetric: 27.9447, val_loss: 29.1399, val_MinusLogProbMetric: 29.1399

Epoch 415: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.9447 - MinusLogProbMetric: 27.9447 - val_loss: 29.1399 - val_MinusLogProbMetric: 29.1399 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 416/1000
2023-10-26 04:06:04.722 
Epoch 416/1000 
	 loss: 27.8918, MinusLogProbMetric: 27.8918, val_loss: 29.8084, val_MinusLogProbMetric: 29.8084

Epoch 416: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8918 - MinusLogProbMetric: 27.8918 - val_loss: 29.8084 - val_MinusLogProbMetric: 29.8084 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 417/1000
2023-10-26 04:06:39.647 
Epoch 417/1000 
	 loss: 27.8836, MinusLogProbMetric: 27.8836, val_loss: 28.8699, val_MinusLogProbMetric: 28.8699

Epoch 417: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8836 - MinusLogProbMetric: 27.8836 - val_loss: 28.8699 - val_MinusLogProbMetric: 28.8699 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 418/1000
2023-10-26 04:07:14.610 
Epoch 418/1000 
	 loss: 27.8839, MinusLogProbMetric: 27.8839, val_loss: 29.3560, val_MinusLogProbMetric: 29.3560

Epoch 418: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8839 - MinusLogProbMetric: 27.8839 - val_loss: 29.3560 - val_MinusLogProbMetric: 29.3560 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 419/1000
2023-10-26 04:07:49.356 
Epoch 419/1000 
	 loss: 27.8769, MinusLogProbMetric: 27.8769, val_loss: 29.4722, val_MinusLogProbMetric: 29.4722

Epoch 419: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8769 - MinusLogProbMetric: 27.8769 - val_loss: 29.4722 - val_MinusLogProbMetric: 29.4722 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 420/1000
2023-10-26 04:08:24.055 
Epoch 420/1000 
	 loss: 27.8711, MinusLogProbMetric: 27.8711, val_loss: 29.2100, val_MinusLogProbMetric: 29.2100

Epoch 420: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8711 - MinusLogProbMetric: 27.8711 - val_loss: 29.2100 - val_MinusLogProbMetric: 29.2100 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 421/1000
2023-10-26 04:08:58.849 
Epoch 421/1000 
	 loss: 27.9307, MinusLogProbMetric: 27.9307, val_loss: 29.8860, val_MinusLogProbMetric: 29.8860

Epoch 421: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.9307 - MinusLogProbMetric: 27.9307 - val_loss: 29.8860 - val_MinusLogProbMetric: 29.8860 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 422/1000
2023-10-26 04:09:33.583 
Epoch 422/1000 
	 loss: 27.8818, MinusLogProbMetric: 27.8818, val_loss: 29.3516, val_MinusLogProbMetric: 29.3516

Epoch 422: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8818 - MinusLogProbMetric: 27.8818 - val_loss: 29.3516 - val_MinusLogProbMetric: 29.3516 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 423/1000
2023-10-26 04:10:08.652 
Epoch 423/1000 
	 loss: 27.8934, MinusLogProbMetric: 27.8934, val_loss: 29.0233, val_MinusLogProbMetric: 29.0233

Epoch 423: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8934 - MinusLogProbMetric: 27.8934 - val_loss: 29.0233 - val_MinusLogProbMetric: 29.0233 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 424/1000
2023-10-26 04:10:43.535 
Epoch 424/1000 
	 loss: 27.7982, MinusLogProbMetric: 27.7982, val_loss: 29.0677, val_MinusLogProbMetric: 29.0677

Epoch 424: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.7982 - MinusLogProbMetric: 27.7982 - val_loss: 29.0677 - val_MinusLogProbMetric: 29.0677 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 425/1000
2023-10-26 04:11:17.954 
Epoch 425/1000 
	 loss: 27.8835, MinusLogProbMetric: 27.8835, val_loss: 29.3924, val_MinusLogProbMetric: 29.3924

Epoch 425: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8835 - MinusLogProbMetric: 27.8835 - val_loss: 29.3924 - val_MinusLogProbMetric: 29.3924 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 426/1000
2023-10-26 04:11:51.832 
Epoch 426/1000 
	 loss: 27.8709, MinusLogProbMetric: 27.8709, val_loss: 29.0229, val_MinusLogProbMetric: 29.0229

Epoch 426: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8709 - MinusLogProbMetric: 27.8709 - val_loss: 29.0229 - val_MinusLogProbMetric: 29.0229 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 427/1000
2023-10-26 04:12:26.058 
Epoch 427/1000 
	 loss: 27.9020, MinusLogProbMetric: 27.9020, val_loss: 28.9846, val_MinusLogProbMetric: 28.9846

Epoch 427: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.9020 - MinusLogProbMetric: 27.9020 - val_loss: 28.9846 - val_MinusLogProbMetric: 28.9846 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 428/1000
2023-10-26 04:12:58.188 
Epoch 428/1000 
	 loss: 27.8232, MinusLogProbMetric: 27.8232, val_loss: 29.4700, val_MinusLogProbMetric: 29.4700

Epoch 428: val_loss did not improve from 28.80557
196/196 - 32s - loss: 27.8232 - MinusLogProbMetric: 27.8232 - val_loss: 29.4700 - val_MinusLogProbMetric: 29.4700 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 429/1000
2023-10-26 04:13:31.597 
Epoch 429/1000 
	 loss: 27.8614, MinusLogProbMetric: 27.8614, val_loss: 29.6161, val_MinusLogProbMetric: 29.6161

Epoch 429: val_loss did not improve from 28.80557
196/196 - 33s - loss: 27.8614 - MinusLogProbMetric: 27.8614 - val_loss: 29.6161 - val_MinusLogProbMetric: 29.6161 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 430/1000
2023-10-26 04:14:03.875 
Epoch 430/1000 
	 loss: 27.9012, MinusLogProbMetric: 27.9012, val_loss: 29.0428, val_MinusLogProbMetric: 29.0428

Epoch 430: val_loss did not improve from 28.80557
196/196 - 32s - loss: 27.9012 - MinusLogProbMetric: 27.9012 - val_loss: 29.0428 - val_MinusLogProbMetric: 29.0428 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 431/1000
2023-10-26 04:14:38.361 
Epoch 431/1000 
	 loss: 27.8433, MinusLogProbMetric: 27.8433, val_loss: 29.1905, val_MinusLogProbMetric: 29.1905

Epoch 431: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8433 - MinusLogProbMetric: 27.8433 - val_loss: 29.1905 - val_MinusLogProbMetric: 29.1905 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 432/1000
2023-10-26 04:15:12.605 
Epoch 432/1000 
	 loss: 27.8831, MinusLogProbMetric: 27.8831, val_loss: 29.3866, val_MinusLogProbMetric: 29.3866

Epoch 432: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8831 - MinusLogProbMetric: 27.8831 - val_loss: 29.3866 - val_MinusLogProbMetric: 29.3866 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 433/1000
2023-10-26 04:15:46.990 
Epoch 433/1000 
	 loss: 27.8516, MinusLogProbMetric: 27.8516, val_loss: 29.4027, val_MinusLogProbMetric: 29.4027

Epoch 433: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8516 - MinusLogProbMetric: 27.8516 - val_loss: 29.4027 - val_MinusLogProbMetric: 29.4027 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 434/1000
2023-10-26 04:16:21.851 
Epoch 434/1000 
	 loss: 27.8459, MinusLogProbMetric: 27.8459, val_loss: 29.3779, val_MinusLogProbMetric: 29.3779

Epoch 434: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8459 - MinusLogProbMetric: 27.8459 - val_loss: 29.3779 - val_MinusLogProbMetric: 29.3779 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 435/1000
2023-10-26 04:16:55.932 
Epoch 435/1000 
	 loss: 27.8039, MinusLogProbMetric: 27.8039, val_loss: 28.9273, val_MinusLogProbMetric: 28.9273

Epoch 435: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8039 - MinusLogProbMetric: 27.8039 - val_loss: 28.9273 - val_MinusLogProbMetric: 28.9273 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 436/1000
2023-10-26 04:17:30.949 
Epoch 436/1000 
	 loss: 27.8871, MinusLogProbMetric: 27.8871, val_loss: 29.6425, val_MinusLogProbMetric: 29.6425

Epoch 436: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8871 - MinusLogProbMetric: 27.8871 - val_loss: 29.6425 - val_MinusLogProbMetric: 29.6425 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 437/1000
2023-10-26 04:18:05.715 
Epoch 437/1000 
	 loss: 27.8615, MinusLogProbMetric: 27.8615, val_loss: 28.8991, val_MinusLogProbMetric: 28.8991

Epoch 437: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8615 - MinusLogProbMetric: 27.8615 - val_loss: 28.8991 - val_MinusLogProbMetric: 28.8991 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 438/1000
2023-10-26 04:18:40.374 
Epoch 438/1000 
	 loss: 27.8127, MinusLogProbMetric: 27.8127, val_loss: 29.1971, val_MinusLogProbMetric: 29.1971

Epoch 438: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8127 - MinusLogProbMetric: 27.8127 - val_loss: 29.1971 - val_MinusLogProbMetric: 29.1971 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 439/1000
2023-10-26 04:19:14.967 
Epoch 439/1000 
	 loss: 27.8791, MinusLogProbMetric: 27.8791, val_loss: 29.3709, val_MinusLogProbMetric: 29.3709

Epoch 439: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8791 - MinusLogProbMetric: 27.8791 - val_loss: 29.3709 - val_MinusLogProbMetric: 29.3709 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 440/1000
2023-10-26 04:19:49.896 
Epoch 440/1000 
	 loss: 27.7921, MinusLogProbMetric: 27.7921, val_loss: 29.0354, val_MinusLogProbMetric: 29.0354

Epoch 440: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.7921 - MinusLogProbMetric: 27.7921 - val_loss: 29.0354 - val_MinusLogProbMetric: 29.0354 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 441/1000
2023-10-26 04:20:24.429 
Epoch 441/1000 
	 loss: 27.8239, MinusLogProbMetric: 27.8239, val_loss: 29.3198, val_MinusLogProbMetric: 29.3198

Epoch 441: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8239 - MinusLogProbMetric: 27.8239 - val_loss: 29.3198 - val_MinusLogProbMetric: 29.3198 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 442/1000
2023-10-26 04:20:58.927 
Epoch 442/1000 
	 loss: 27.8409, MinusLogProbMetric: 27.8409, val_loss: 29.8466, val_MinusLogProbMetric: 29.8466

Epoch 442: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8409 - MinusLogProbMetric: 27.8409 - val_loss: 29.8466 - val_MinusLogProbMetric: 29.8466 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 443/1000
2023-10-26 04:21:33.634 
Epoch 443/1000 
	 loss: 27.8352, MinusLogProbMetric: 27.8352, val_loss: 29.3604, val_MinusLogProbMetric: 29.3604

Epoch 443: val_loss did not improve from 28.80557
196/196 - 35s - loss: 27.8352 - MinusLogProbMetric: 27.8352 - val_loss: 29.3604 - val_MinusLogProbMetric: 29.3604 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 444/1000
2023-10-26 04:22:08.064 
Epoch 444/1000 
	 loss: 27.8093, MinusLogProbMetric: 27.8093, val_loss: 29.0542, val_MinusLogProbMetric: 29.0542

Epoch 444: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8093 - MinusLogProbMetric: 27.8093 - val_loss: 29.0542 - val_MinusLogProbMetric: 29.0542 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 445/1000
2023-10-26 04:22:42.529 
Epoch 445/1000 
	 loss: 27.8263, MinusLogProbMetric: 27.8263, val_loss: 30.1250, val_MinusLogProbMetric: 30.1250

Epoch 445: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8263 - MinusLogProbMetric: 27.8263 - val_loss: 30.1250 - val_MinusLogProbMetric: 30.1250 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 446/1000
2023-10-26 04:23:13.606 
Epoch 446/1000 
	 loss: 27.8874, MinusLogProbMetric: 27.8874, val_loss: 29.1284, val_MinusLogProbMetric: 29.1284

Epoch 446: val_loss did not improve from 28.80557
196/196 - 31s - loss: 27.8874 - MinusLogProbMetric: 27.8874 - val_loss: 29.1284 - val_MinusLogProbMetric: 29.1284 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 447/1000
2023-10-26 04:23:43.039 
Epoch 447/1000 
	 loss: 27.7831, MinusLogProbMetric: 27.7831, val_loss: 29.3246, val_MinusLogProbMetric: 29.3246

Epoch 447: val_loss did not improve from 28.80557
196/196 - 29s - loss: 27.7831 - MinusLogProbMetric: 27.7831 - val_loss: 29.3246 - val_MinusLogProbMetric: 29.3246 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 448/1000
2023-10-26 04:24:13.433 
Epoch 448/1000 
	 loss: 27.8208, MinusLogProbMetric: 27.8208, val_loss: 28.9994, val_MinusLogProbMetric: 28.9994

Epoch 448: val_loss did not improve from 28.80557
196/196 - 30s - loss: 27.8208 - MinusLogProbMetric: 27.8208 - val_loss: 28.9994 - val_MinusLogProbMetric: 28.9994 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 449/1000
2023-10-26 04:24:47.244 
Epoch 449/1000 
	 loss: 27.8582, MinusLogProbMetric: 27.8582, val_loss: 28.9903, val_MinusLogProbMetric: 28.9903

Epoch 449: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8582 - MinusLogProbMetric: 27.8582 - val_loss: 28.9903 - val_MinusLogProbMetric: 28.9903 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 450/1000
2023-10-26 04:25:20.760 
Epoch 450/1000 
	 loss: 27.7465, MinusLogProbMetric: 27.7465, val_loss: 29.0159, val_MinusLogProbMetric: 29.0159

Epoch 450: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.7465 - MinusLogProbMetric: 27.7465 - val_loss: 29.0159 - val_MinusLogProbMetric: 29.0159 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 451/1000
2023-10-26 04:25:50.068 
Epoch 451/1000 
	 loss: 27.8055, MinusLogProbMetric: 27.8055, val_loss: 29.7544, val_MinusLogProbMetric: 29.7544

Epoch 451: val_loss did not improve from 28.80557
196/196 - 29s - loss: 27.8055 - MinusLogProbMetric: 27.8055 - val_loss: 29.7544 - val_MinusLogProbMetric: 29.7544 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 452/1000
2023-10-26 04:26:19.758 
Epoch 452/1000 
	 loss: 27.7970, MinusLogProbMetric: 27.7970, val_loss: 29.5001, val_MinusLogProbMetric: 29.5001

Epoch 452: val_loss did not improve from 28.80557
196/196 - 30s - loss: 27.7970 - MinusLogProbMetric: 27.7970 - val_loss: 29.5001 - val_MinusLogProbMetric: 29.5001 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 453/1000
2023-10-26 04:26:49.659 
Epoch 453/1000 
	 loss: 27.7904, MinusLogProbMetric: 27.7904, val_loss: 30.1242, val_MinusLogProbMetric: 30.1242

Epoch 453: val_loss did not improve from 28.80557
196/196 - 30s - loss: 27.7904 - MinusLogProbMetric: 27.7904 - val_loss: 30.1242 - val_MinusLogProbMetric: 30.1242 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 454/1000
2023-10-26 04:27:23.835 
Epoch 454/1000 
	 loss: 27.8334, MinusLogProbMetric: 27.8334, val_loss: 29.2058, val_MinusLogProbMetric: 29.2058

Epoch 454: val_loss did not improve from 28.80557
196/196 - 34s - loss: 27.8334 - MinusLogProbMetric: 27.8334 - val_loss: 29.2058 - val_MinusLogProbMetric: 29.2058 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 455/1000
2023-10-26 04:27:55.871 
Epoch 455/1000 
	 loss: 27.7718, MinusLogProbMetric: 27.7718, val_loss: 29.0525, val_MinusLogProbMetric: 29.0525

Epoch 455: val_loss did not improve from 28.80557
196/196 - 32s - loss: 27.7718 - MinusLogProbMetric: 27.7718 - val_loss: 29.0525 - val_MinusLogProbMetric: 29.0525 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 456/1000
2023-10-26 04:28:25.228 
Epoch 456/1000 
	 loss: 27.7967, MinusLogProbMetric: 27.7967, val_loss: 28.9902, val_MinusLogProbMetric: 28.9902

Epoch 456: val_loss did not improve from 28.80557
196/196 - 29s - loss: 27.7967 - MinusLogProbMetric: 27.7967 - val_loss: 28.9902 - val_MinusLogProbMetric: 28.9902 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 457/1000
2023-10-26 04:28:54.449 
Epoch 457/1000 
	 loss: 27.6935, MinusLogProbMetric: 27.6935, val_loss: 29.2979, val_MinusLogProbMetric: 29.2979

Epoch 457: val_loss did not improve from 28.80557
196/196 - 29s - loss: 27.6935 - MinusLogProbMetric: 27.6935 - val_loss: 29.2979 - val_MinusLogProbMetric: 29.2979 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 458/1000
2023-10-26 04:29:26.446 
Epoch 458/1000 
	 loss: 27.7454, MinusLogProbMetric: 27.7454, val_loss: 29.1742, val_MinusLogProbMetric: 29.1742

Epoch 458: val_loss did not improve from 28.80557
196/196 - 32s - loss: 27.7454 - MinusLogProbMetric: 27.7454 - val_loss: 29.1742 - val_MinusLogProbMetric: 29.1742 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 459/1000
2023-10-26 04:29:56.382 
Epoch 459/1000 
	 loss: 27.7885, MinusLogProbMetric: 27.7885, val_loss: 29.1975, val_MinusLogProbMetric: 29.1975

Epoch 459: val_loss did not improve from 28.80557
196/196 - 30s - loss: 27.7885 - MinusLogProbMetric: 27.7885 - val_loss: 29.1975 - val_MinusLogProbMetric: 29.1975 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 460/1000
2023-10-26 04:30:25.934 
Epoch 460/1000 
	 loss: 27.8543, MinusLogProbMetric: 27.8543, val_loss: 30.0077, val_MinusLogProbMetric: 30.0077

Epoch 460: val_loss did not improve from 28.80557
196/196 - 30s - loss: 27.8543 - MinusLogProbMetric: 27.8543 - val_loss: 30.0077 - val_MinusLogProbMetric: 30.0077 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 461/1000
2023-10-26 04:30:55.689 
Epoch 461/1000 
	 loss: 27.8396, MinusLogProbMetric: 27.8396, val_loss: 29.5109, val_MinusLogProbMetric: 29.5109

Epoch 461: val_loss did not improve from 28.80557
196/196 - 30s - loss: 27.8396 - MinusLogProbMetric: 27.8396 - val_loss: 29.5109 - val_MinusLogProbMetric: 29.5109 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 462/1000
2023-10-26 04:31:26.135 
Epoch 462/1000 
	 loss: 27.3005, MinusLogProbMetric: 27.3005, val_loss: 28.7368, val_MinusLogProbMetric: 28.7368

Epoch 462: val_loss improved from 28.80557 to 28.73679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 31s - loss: 27.3005 - MinusLogProbMetric: 27.3005 - val_loss: 28.7368 - val_MinusLogProbMetric: 28.7368 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 463/1000
2023-10-26 04:31:58.170 
Epoch 463/1000 
	 loss: 27.2724, MinusLogProbMetric: 27.2724, val_loss: 28.7950, val_MinusLogProbMetric: 28.7950

Epoch 463: val_loss did not improve from 28.73679
196/196 - 31s - loss: 27.2724 - MinusLogProbMetric: 27.2724 - val_loss: 28.7950 - val_MinusLogProbMetric: 28.7950 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 464/1000
2023-10-26 04:32:30.860 
Epoch 464/1000 
	 loss: 27.2622, MinusLogProbMetric: 27.2622, val_loss: 28.8739, val_MinusLogProbMetric: 28.8739

Epoch 464: val_loss did not improve from 28.73679
196/196 - 33s - loss: 27.2622 - MinusLogProbMetric: 27.2622 - val_loss: 28.8739 - val_MinusLogProbMetric: 28.8739 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 465/1000
2023-10-26 04:33:00.177 
Epoch 465/1000 
	 loss: 27.2902, MinusLogProbMetric: 27.2902, val_loss: 28.8345, val_MinusLogProbMetric: 28.8345

Epoch 465: val_loss did not improve from 28.73679
196/196 - 29s - loss: 27.2902 - MinusLogProbMetric: 27.2902 - val_loss: 28.8345 - val_MinusLogProbMetric: 28.8345 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 466/1000
2023-10-26 04:33:30.008 
Epoch 466/1000 
	 loss: 27.2568, MinusLogProbMetric: 27.2568, val_loss: 28.9593, val_MinusLogProbMetric: 28.9593

Epoch 466: val_loss did not improve from 28.73679
196/196 - 30s - loss: 27.2568 - MinusLogProbMetric: 27.2568 - val_loss: 28.9593 - val_MinusLogProbMetric: 28.9593 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 467/1000
2023-10-26 04:34:01.352 
Epoch 467/1000 
	 loss: 27.2805, MinusLogProbMetric: 27.2805, val_loss: 28.6630, val_MinusLogProbMetric: 28.6630

Epoch 467: val_loss improved from 28.73679 to 28.66298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_370/weights/best_weights.h5
196/196 - 32s - loss: 27.2805 - MinusLogProbMetric: 27.2805 - val_loss: 28.6630 - val_MinusLogProbMetric: 28.6630 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 468/1000
2023-10-26 04:34:35.730 
Epoch 468/1000 
	 loss: 27.2781, MinusLogProbMetric: 27.2781, val_loss: 28.7888, val_MinusLogProbMetric: 28.7888

Epoch 468: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2781 - MinusLogProbMetric: 27.2781 - val_loss: 28.7888 - val_MinusLogProbMetric: 28.7888 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 469/1000
2023-10-26 04:35:06.525 
Epoch 469/1000 
	 loss: 27.2923, MinusLogProbMetric: 27.2923, val_loss: 28.7836, val_MinusLogProbMetric: 28.7836

Epoch 469: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.2923 - MinusLogProbMetric: 27.2923 - val_loss: 28.7836 - val_MinusLogProbMetric: 28.7836 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 470/1000
2023-10-26 04:35:35.905 
Epoch 470/1000 
	 loss: 27.2624, MinusLogProbMetric: 27.2624, val_loss: 28.7295, val_MinusLogProbMetric: 28.7295

Epoch 470: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2624 - MinusLogProbMetric: 27.2624 - val_loss: 28.7295 - val_MinusLogProbMetric: 28.7295 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 471/1000
2023-10-26 04:36:06.162 
Epoch 471/1000 
	 loss: 27.2833, MinusLogProbMetric: 27.2833, val_loss: 28.8298, val_MinusLogProbMetric: 28.8298

Epoch 471: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2833 - MinusLogProbMetric: 27.2833 - val_loss: 28.8298 - val_MinusLogProbMetric: 28.8298 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 472/1000
2023-10-26 04:36:40.327 
Epoch 472/1000 
	 loss: 27.2512, MinusLogProbMetric: 27.2512, val_loss: 28.8518, val_MinusLogProbMetric: 28.8518

Epoch 472: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2512 - MinusLogProbMetric: 27.2512 - val_loss: 28.8518 - val_MinusLogProbMetric: 28.8518 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 473/1000
2023-10-26 04:37:13.254 
Epoch 473/1000 
	 loss: 27.2610, MinusLogProbMetric: 27.2610, val_loss: 28.9185, val_MinusLogProbMetric: 28.9185

Epoch 473: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.2610 - MinusLogProbMetric: 27.2610 - val_loss: 28.9185 - val_MinusLogProbMetric: 28.9185 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 474/1000
2023-10-26 04:37:42.433 
Epoch 474/1000 
	 loss: 27.2648, MinusLogProbMetric: 27.2648, val_loss: 28.7928, val_MinusLogProbMetric: 28.7928

Epoch 474: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2648 - MinusLogProbMetric: 27.2648 - val_loss: 28.7928 - val_MinusLogProbMetric: 28.7928 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 475/1000
2023-10-26 04:38:11.931 
Epoch 475/1000 
	 loss: 27.2675, MinusLogProbMetric: 27.2675, val_loss: 28.8634, val_MinusLogProbMetric: 28.8634

Epoch 475: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2675 - MinusLogProbMetric: 27.2675 - val_loss: 28.8634 - val_MinusLogProbMetric: 28.8634 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 476/1000
2023-10-26 04:38:43.768 
Epoch 476/1000 
	 loss: 27.2633, MinusLogProbMetric: 27.2633, val_loss: 28.8247, val_MinusLogProbMetric: 28.8247

Epoch 476: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.2633 - MinusLogProbMetric: 27.2633 - val_loss: 28.8247 - val_MinusLogProbMetric: 28.8247 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 477/1000
2023-10-26 04:39:17.906 
Epoch 477/1000 
	 loss: 27.2484, MinusLogProbMetric: 27.2484, val_loss: 28.8751, val_MinusLogProbMetric: 28.8751

Epoch 477: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2484 - MinusLogProbMetric: 27.2484 - val_loss: 28.8751 - val_MinusLogProbMetric: 28.8751 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 478/1000
2023-10-26 04:39:49.115 
Epoch 478/1000 
	 loss: 27.2467, MinusLogProbMetric: 27.2467, val_loss: 28.8973, val_MinusLogProbMetric: 28.8973

Epoch 478: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.2467 - MinusLogProbMetric: 27.2467 - val_loss: 28.8973 - val_MinusLogProbMetric: 28.8973 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 479/1000
2023-10-26 04:40:18.789 
Epoch 479/1000 
	 loss: 27.2457, MinusLogProbMetric: 27.2457, val_loss: 28.9442, val_MinusLogProbMetric: 28.9442

Epoch 479: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2457 - MinusLogProbMetric: 27.2457 - val_loss: 28.9442 - val_MinusLogProbMetric: 28.9442 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 480/1000
2023-10-26 04:40:48.219 
Epoch 480/1000 
	 loss: 27.2924, MinusLogProbMetric: 27.2924, val_loss: 28.8660, val_MinusLogProbMetric: 28.8660

Epoch 480: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2924 - MinusLogProbMetric: 27.2924 - val_loss: 28.8660 - val_MinusLogProbMetric: 28.8660 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 481/1000
2023-10-26 04:41:21.891 
Epoch 481/1000 
	 loss: 27.2390, MinusLogProbMetric: 27.2390, val_loss: 28.9072, val_MinusLogProbMetric: 28.9072

Epoch 481: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2390 - MinusLogProbMetric: 27.2390 - val_loss: 28.9072 - val_MinusLogProbMetric: 28.9072 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 482/1000
2023-10-26 04:41:55.769 
Epoch 482/1000 
	 loss: 27.2465, MinusLogProbMetric: 27.2465, val_loss: 28.8705, val_MinusLogProbMetric: 28.8705

Epoch 482: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2465 - MinusLogProbMetric: 27.2465 - val_loss: 28.8705 - val_MinusLogProbMetric: 28.8705 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 483/1000
2023-10-26 04:42:26.320 
Epoch 483/1000 
	 loss: 27.2484, MinusLogProbMetric: 27.2484, val_loss: 28.8511, val_MinusLogProbMetric: 28.8511

Epoch 483: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.2484 - MinusLogProbMetric: 27.2484 - val_loss: 28.8511 - val_MinusLogProbMetric: 28.8511 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 484/1000
2023-10-26 04:42:56.065 
Epoch 484/1000 
	 loss: 27.2943, MinusLogProbMetric: 27.2943, val_loss: 28.8112, val_MinusLogProbMetric: 28.8112

Epoch 484: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2943 - MinusLogProbMetric: 27.2943 - val_loss: 28.8112 - val_MinusLogProbMetric: 28.8112 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 485/1000
2023-10-26 04:43:26.389 
Epoch 485/1000 
	 loss: 27.2628, MinusLogProbMetric: 27.2628, val_loss: 29.0745, val_MinusLogProbMetric: 29.0745

Epoch 485: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2628 - MinusLogProbMetric: 27.2628 - val_loss: 29.0745 - val_MinusLogProbMetric: 29.0745 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 486/1000
2023-10-26 04:44:00.322 
Epoch 486/1000 
	 loss: 27.2454, MinusLogProbMetric: 27.2454, val_loss: 28.7481, val_MinusLogProbMetric: 28.7481

Epoch 486: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2454 - MinusLogProbMetric: 27.2454 - val_loss: 28.7481 - val_MinusLogProbMetric: 28.7481 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 487/1000
2023-10-26 04:44:34.245 
Epoch 487/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 28.8550, val_MinusLogProbMetric: 28.8550

Epoch 487: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 28.8550 - val_MinusLogProbMetric: 28.8550 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 488/1000
2023-10-26 04:45:04.148 
Epoch 488/1000 
	 loss: 27.2773, MinusLogProbMetric: 27.2773, val_loss: 28.8570, val_MinusLogProbMetric: 28.8570

Epoch 488: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2773 - MinusLogProbMetric: 27.2773 - val_loss: 28.8570 - val_MinusLogProbMetric: 28.8570 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 489/1000
2023-10-26 04:45:33.547 
Epoch 489/1000 
	 loss: 27.2519, MinusLogProbMetric: 27.2519, val_loss: 28.7342, val_MinusLogProbMetric: 28.7342

Epoch 489: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2519 - MinusLogProbMetric: 27.2519 - val_loss: 28.7342 - val_MinusLogProbMetric: 28.7342 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 490/1000
2023-10-26 04:46:04.115 
Epoch 490/1000 
	 loss: 27.2551, MinusLogProbMetric: 27.2551, val_loss: 28.9190, val_MinusLogProbMetric: 28.9190

Epoch 490: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.2551 - MinusLogProbMetric: 27.2551 - val_loss: 28.9190 - val_MinusLogProbMetric: 28.9190 - lr: 1.6667e-04 - 31s/epoch - 156ms/step
Epoch 491/1000
2023-10-26 04:46:36.529 
Epoch 491/1000 
	 loss: 27.2635, MinusLogProbMetric: 27.2635, val_loss: 28.9604, val_MinusLogProbMetric: 28.9604

Epoch 491: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.2635 - MinusLogProbMetric: 27.2635 - val_loss: 28.9604 - val_MinusLogProbMetric: 28.9604 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 492/1000
2023-10-26 04:47:06.240 
Epoch 492/1000 
	 loss: 27.2871, MinusLogProbMetric: 27.2871, val_loss: 29.1769, val_MinusLogProbMetric: 29.1769

Epoch 492: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2871 - MinusLogProbMetric: 27.2871 - val_loss: 29.1769 - val_MinusLogProbMetric: 29.1769 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 493/1000
2023-10-26 04:47:35.546 
Epoch 493/1000 
	 loss: 27.2620, MinusLogProbMetric: 27.2620, val_loss: 28.7841, val_MinusLogProbMetric: 28.7841

Epoch 493: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2620 - MinusLogProbMetric: 27.2620 - val_loss: 28.7841 - val_MinusLogProbMetric: 28.7841 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 494/1000
2023-10-26 04:48:05.131 
Epoch 494/1000 
	 loss: 27.2473, MinusLogProbMetric: 27.2473, val_loss: 28.8779, val_MinusLogProbMetric: 28.8779

Epoch 494: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2473 - MinusLogProbMetric: 27.2473 - val_loss: 28.8779 - val_MinusLogProbMetric: 28.8779 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 495/1000
2023-10-26 04:48:37.612 
Epoch 495/1000 
	 loss: 27.2660, MinusLogProbMetric: 27.2660, val_loss: 28.7797, val_MinusLogProbMetric: 28.7797

Epoch 495: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.2660 - MinusLogProbMetric: 27.2660 - val_loss: 28.7797 - val_MinusLogProbMetric: 28.7797 - lr: 1.6667e-04 - 32s/epoch - 166ms/step
Epoch 496/1000
2023-10-26 04:49:10.396 
Epoch 496/1000 
	 loss: 27.2440, MinusLogProbMetric: 27.2440, val_loss: 28.8157, val_MinusLogProbMetric: 28.8157

Epoch 496: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.2440 - MinusLogProbMetric: 27.2440 - val_loss: 28.8157 - val_MinusLogProbMetric: 28.8157 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 497/1000
2023-10-26 04:49:39.591 
Epoch 497/1000 
	 loss: 27.2425, MinusLogProbMetric: 27.2425, val_loss: 28.7826, val_MinusLogProbMetric: 28.7826

Epoch 497: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2425 - MinusLogProbMetric: 27.2425 - val_loss: 28.7826 - val_MinusLogProbMetric: 28.7826 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 498/1000
2023-10-26 04:50:09.553 
Epoch 498/1000 
	 loss: 27.2728, MinusLogProbMetric: 27.2728, val_loss: 29.1748, val_MinusLogProbMetric: 29.1748

Epoch 498: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2728 - MinusLogProbMetric: 27.2728 - val_loss: 29.1748 - val_MinusLogProbMetric: 29.1748 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 499/1000
2023-10-26 04:50:39.689 
Epoch 499/1000 
	 loss: 27.2445, MinusLogProbMetric: 27.2445, val_loss: 28.7132, val_MinusLogProbMetric: 28.7132

Epoch 499: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2445 - MinusLogProbMetric: 27.2445 - val_loss: 28.7132 - val_MinusLogProbMetric: 28.7132 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 500/1000
2023-10-26 04:51:13.966 
Epoch 500/1000 
	 loss: 27.2382, MinusLogProbMetric: 27.2382, val_loss: 28.8840, val_MinusLogProbMetric: 28.8840

Epoch 500: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2382 - MinusLogProbMetric: 27.2382 - val_loss: 28.8840 - val_MinusLogProbMetric: 28.8840 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 501/1000
2023-10-26 04:51:47.510 
Epoch 501/1000 
	 loss: 27.2244, MinusLogProbMetric: 27.2244, val_loss: 28.7769, val_MinusLogProbMetric: 28.7769

Epoch 501: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2244 - MinusLogProbMetric: 27.2244 - val_loss: 28.7769 - val_MinusLogProbMetric: 28.7769 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 502/1000
2023-10-26 04:52:17.442 
Epoch 502/1000 
	 loss: 27.2541, MinusLogProbMetric: 27.2541, val_loss: 28.8812, val_MinusLogProbMetric: 28.8812

Epoch 502: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2541 - MinusLogProbMetric: 27.2541 - val_loss: 28.8812 - val_MinusLogProbMetric: 28.8812 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 503/1000
2023-10-26 04:52:46.768 
Epoch 503/1000 
	 loss: 27.2230, MinusLogProbMetric: 27.2230, val_loss: 29.0605, val_MinusLogProbMetric: 29.0605

Epoch 503: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2230 - MinusLogProbMetric: 27.2230 - val_loss: 29.0605 - val_MinusLogProbMetric: 29.0605 - lr: 1.6667e-04 - 29s/epoch - 150ms/step
Epoch 504/1000
2023-10-26 04:53:16.002 
Epoch 504/1000 
	 loss: 27.2488, MinusLogProbMetric: 27.2488, val_loss: 28.8509, val_MinusLogProbMetric: 28.8509

Epoch 504: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2488 - MinusLogProbMetric: 27.2488 - val_loss: 28.8509 - val_MinusLogProbMetric: 28.8509 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 505/1000
2023-10-26 04:53:45.844 
Epoch 505/1000 
	 loss: 27.2573, MinusLogProbMetric: 27.2573, val_loss: 28.8042, val_MinusLogProbMetric: 28.8042

Epoch 505: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2573 - MinusLogProbMetric: 27.2573 - val_loss: 28.8042 - val_MinusLogProbMetric: 28.8042 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 506/1000
2023-10-26 04:54:16.289 
Epoch 506/1000 
	 loss: 27.2557, MinusLogProbMetric: 27.2557, val_loss: 28.7995, val_MinusLogProbMetric: 28.7995

Epoch 506: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2557 - MinusLogProbMetric: 27.2557 - val_loss: 28.7995 - val_MinusLogProbMetric: 28.7995 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 507/1000
2023-10-26 04:54:48.272 
Epoch 507/1000 
	 loss: 27.2231, MinusLogProbMetric: 27.2231, val_loss: 29.0746, val_MinusLogProbMetric: 29.0746

Epoch 507: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.2231 - MinusLogProbMetric: 27.2231 - val_loss: 29.0746 - val_MinusLogProbMetric: 29.0746 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 508/1000
2023-10-26 04:55:17.860 
Epoch 508/1000 
	 loss: 27.2207, MinusLogProbMetric: 27.2207, val_loss: 29.1239, val_MinusLogProbMetric: 29.1239

Epoch 508: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2207 - MinusLogProbMetric: 27.2207 - val_loss: 29.1239 - val_MinusLogProbMetric: 29.1239 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 509/1000
2023-10-26 04:55:47.098 
Epoch 509/1000 
	 loss: 27.2418, MinusLogProbMetric: 27.2418, val_loss: 28.7764, val_MinusLogProbMetric: 28.7764

Epoch 509: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.2418 - MinusLogProbMetric: 27.2418 - val_loss: 28.7764 - val_MinusLogProbMetric: 28.7764 - lr: 1.6667e-04 - 29s/epoch - 149ms/step
Epoch 510/1000
2023-10-26 04:56:16.664 
Epoch 510/1000 
	 loss: 27.2393, MinusLogProbMetric: 27.2393, val_loss: 29.0623, val_MinusLogProbMetric: 29.0623

Epoch 510: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2393 - MinusLogProbMetric: 27.2393 - val_loss: 29.0623 - val_MinusLogProbMetric: 29.0623 - lr: 1.6667e-04 - 30s/epoch - 151ms/step
Epoch 511/1000
2023-10-26 04:56:48.040 
Epoch 511/1000 
	 loss: 27.2355, MinusLogProbMetric: 27.2355, val_loss: 28.7493, val_MinusLogProbMetric: 28.7493

Epoch 511: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.2355 - MinusLogProbMetric: 27.2355 - val_loss: 28.7493 - val_MinusLogProbMetric: 28.7493 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 512/1000
2023-10-26 04:57:21.465 
Epoch 512/1000 
	 loss: 27.2471, MinusLogProbMetric: 27.2471, val_loss: 28.9225, val_MinusLogProbMetric: 28.9225

Epoch 512: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.2471 - MinusLogProbMetric: 27.2471 - val_loss: 28.9225 - val_MinusLogProbMetric: 28.9225 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 513/1000
2023-10-26 04:57:53.567 
Epoch 513/1000 
	 loss: 27.2304, MinusLogProbMetric: 27.2304, val_loss: 28.8746, val_MinusLogProbMetric: 28.8746

Epoch 513: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.2304 - MinusLogProbMetric: 27.2304 - val_loss: 28.8746 - val_MinusLogProbMetric: 28.8746 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 514/1000
2023-10-26 04:58:27.907 
Epoch 514/1000 
	 loss: 27.2288, MinusLogProbMetric: 27.2288, val_loss: 28.7979, val_MinusLogProbMetric: 28.7979

Epoch 514: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.2288 - MinusLogProbMetric: 27.2288 - val_loss: 28.7979 - val_MinusLogProbMetric: 28.7979 - lr: 1.6667e-04 - 34s/epoch - 175ms/step
Epoch 515/1000
2023-10-26 04:58:59.071 
Epoch 515/1000 
	 loss: 27.1997, MinusLogProbMetric: 27.1997, val_loss: 28.7177, val_MinusLogProbMetric: 28.7177

Epoch 515: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.1997 - MinusLogProbMetric: 27.1997 - val_loss: 28.7177 - val_MinusLogProbMetric: 28.7177 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 516/1000
2023-10-26 04:59:30.921 
Epoch 516/1000 
	 loss: 27.1987, MinusLogProbMetric: 27.1987, val_loss: 28.7244, val_MinusLogProbMetric: 28.7244

Epoch 516: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.1987 - MinusLogProbMetric: 27.1987 - val_loss: 28.7244 - val_MinusLogProbMetric: 28.7244 - lr: 1.6667e-04 - 32s/epoch - 162ms/step
Epoch 517/1000
2023-10-26 05:00:00.934 
Epoch 517/1000 
	 loss: 27.2129, MinusLogProbMetric: 27.2129, val_loss: 29.4453, val_MinusLogProbMetric: 29.4453

Epoch 517: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.2129 - MinusLogProbMetric: 27.2129 - val_loss: 29.4453 - val_MinusLogProbMetric: 29.4453 - lr: 1.6667e-04 - 30s/epoch - 153ms/step
Epoch 518/1000
2023-10-26 05:00:30.352 
Epoch 518/1000 
	 loss: 27.0674, MinusLogProbMetric: 27.0674, val_loss: 28.6926, val_MinusLogProbMetric: 28.6926

Epoch 518: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.0674 - MinusLogProbMetric: 27.0674 - val_loss: 28.6926 - val_MinusLogProbMetric: 28.6926 - lr: 8.3333e-05 - 29s/epoch - 150ms/step
Epoch 519/1000
2023-10-26 05:01:03.392 
Epoch 519/1000 
	 loss: 27.0339, MinusLogProbMetric: 27.0339, val_loss: 28.7495, val_MinusLogProbMetric: 28.7495

Epoch 519: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.0339 - MinusLogProbMetric: 27.0339 - val_loss: 28.7495 - val_MinusLogProbMetric: 28.7495 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 520/1000
2023-10-26 05:01:36.274 
Epoch 520/1000 
	 loss: 27.0489, MinusLogProbMetric: 27.0489, val_loss: 28.7419, val_MinusLogProbMetric: 28.7419

Epoch 520: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.0489 - MinusLogProbMetric: 27.0489 - val_loss: 28.7419 - val_MinusLogProbMetric: 28.7419 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 521/1000
2023-10-26 05:02:06.442 
Epoch 521/1000 
	 loss: 27.0382, MinusLogProbMetric: 27.0382, val_loss: 28.7476, val_MinusLogProbMetric: 28.7476

Epoch 521: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0382 - MinusLogProbMetric: 27.0382 - val_loss: 28.7476 - val_MinusLogProbMetric: 28.7476 - lr: 8.3333e-05 - 30s/epoch - 154ms/step
Epoch 522/1000
2023-10-26 05:02:36.482 
Epoch 522/1000 
	 loss: 27.0385, MinusLogProbMetric: 27.0385, val_loss: 28.7341, val_MinusLogProbMetric: 28.7341

Epoch 522: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0385 - MinusLogProbMetric: 27.0385 - val_loss: 28.7341 - val_MinusLogProbMetric: 28.7341 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 523/1000
2023-10-26 05:03:06.042 
Epoch 523/1000 
	 loss: 27.0382, MinusLogProbMetric: 27.0382, val_loss: 28.7905, val_MinusLogProbMetric: 28.7905

Epoch 523: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0382 - MinusLogProbMetric: 27.0382 - val_loss: 28.7905 - val_MinusLogProbMetric: 28.7905 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 524/1000
2023-10-26 05:03:36.221 
Epoch 524/1000 
	 loss: 27.0353, MinusLogProbMetric: 27.0353, val_loss: 28.7525, val_MinusLogProbMetric: 28.7525

Epoch 524: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0353 - MinusLogProbMetric: 27.0353 - val_loss: 28.7525 - val_MinusLogProbMetric: 28.7525 - lr: 8.3333e-05 - 30s/epoch - 154ms/step
Epoch 525/1000
2023-10-26 05:04:09.511 
Epoch 525/1000 
	 loss: 27.0321, MinusLogProbMetric: 27.0321, val_loss: 28.7266, val_MinusLogProbMetric: 28.7266

Epoch 525: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.0321 - MinusLogProbMetric: 27.0321 - val_loss: 28.7266 - val_MinusLogProbMetric: 28.7266 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 526/1000
2023-10-26 05:04:41.562 
Epoch 526/1000 
	 loss: 27.0387, MinusLogProbMetric: 27.0387, val_loss: 28.7626, val_MinusLogProbMetric: 28.7626

Epoch 526: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.0387 - MinusLogProbMetric: 27.0387 - val_loss: 28.7626 - val_MinusLogProbMetric: 28.7626 - lr: 8.3333e-05 - 32s/epoch - 164ms/step
Epoch 527/1000
2023-10-26 05:05:15.132 
Epoch 527/1000 
	 loss: 27.0297, MinusLogProbMetric: 27.0297, val_loss: 28.7518, val_MinusLogProbMetric: 28.7518

Epoch 527: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.0297 - MinusLogProbMetric: 27.0297 - val_loss: 28.7518 - val_MinusLogProbMetric: 28.7518 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 528/1000
2023-10-26 05:05:45.711 
Epoch 528/1000 
	 loss: 27.0368, MinusLogProbMetric: 27.0368, val_loss: 28.8213, val_MinusLogProbMetric: 28.8213

Epoch 528: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.0368 - MinusLogProbMetric: 27.0368 - val_loss: 28.8213 - val_MinusLogProbMetric: 28.8213 - lr: 8.3333e-05 - 31s/epoch - 156ms/step
Epoch 529/1000
2023-10-26 05:06:15.265 
Epoch 529/1000 
	 loss: 27.0526, MinusLogProbMetric: 27.0526, val_loss: 28.7421, val_MinusLogProbMetric: 28.7421

Epoch 529: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0526 - MinusLogProbMetric: 27.0526 - val_loss: 28.7421 - val_MinusLogProbMetric: 28.7421 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 530/1000
2023-10-26 05:06:45.120 
Epoch 530/1000 
	 loss: 27.0392, MinusLogProbMetric: 27.0392, val_loss: 28.8272, val_MinusLogProbMetric: 28.8272

Epoch 530: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0392 - MinusLogProbMetric: 27.0392 - val_loss: 28.8272 - val_MinusLogProbMetric: 28.8272 - lr: 8.3333e-05 - 30s/epoch - 152ms/step
Epoch 531/1000
2023-10-26 05:07:18.887 
Epoch 531/1000 
	 loss: 27.0527, MinusLogProbMetric: 27.0527, val_loss: 28.8449, val_MinusLogProbMetric: 28.8449

Epoch 531: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.0527 - MinusLogProbMetric: 27.0527 - val_loss: 28.8449 - val_MinusLogProbMetric: 28.8449 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 532/1000
2023-10-26 05:07:52.778 
Epoch 532/1000 
	 loss: 27.0332, MinusLogProbMetric: 27.0332, val_loss: 28.7468, val_MinusLogProbMetric: 28.7468

Epoch 532: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.0332 - MinusLogProbMetric: 27.0332 - val_loss: 28.7468 - val_MinusLogProbMetric: 28.7468 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 533/1000
2023-10-26 05:08:25.596 
Epoch 533/1000 
	 loss: 27.0234, MinusLogProbMetric: 27.0234, val_loss: 28.7196, val_MinusLogProbMetric: 28.7196

Epoch 533: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.0234 - MinusLogProbMetric: 27.0234 - val_loss: 28.7196 - val_MinusLogProbMetric: 28.7196 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 534/1000
2023-10-26 05:08:55.148 
Epoch 534/1000 
	 loss: 27.0217, MinusLogProbMetric: 27.0217, val_loss: 28.7237, val_MinusLogProbMetric: 28.7237

Epoch 534: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0217 - MinusLogProbMetric: 27.0217 - val_loss: 28.7237 - val_MinusLogProbMetric: 28.7237 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 535/1000
2023-10-26 05:09:26.155 
Epoch 535/1000 
	 loss: 27.0383, MinusLogProbMetric: 27.0383, val_loss: 28.8419, val_MinusLogProbMetric: 28.8419

Epoch 535: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.0383 - MinusLogProbMetric: 27.0383 - val_loss: 28.8419 - val_MinusLogProbMetric: 28.8419 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 536/1000
2023-10-26 05:09:55.865 
Epoch 536/1000 
	 loss: 27.0350, MinusLogProbMetric: 27.0350, val_loss: 28.7717, val_MinusLogProbMetric: 28.7717

Epoch 536: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0350 - MinusLogProbMetric: 27.0350 - val_loss: 28.7717 - val_MinusLogProbMetric: 28.7717 - lr: 8.3333e-05 - 30s/epoch - 152ms/step
Epoch 537/1000
2023-10-26 05:10:26.946 
Epoch 537/1000 
	 loss: 27.0174, MinusLogProbMetric: 27.0174, val_loss: 28.7593, val_MinusLogProbMetric: 28.7593

Epoch 537: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.0174 - MinusLogProbMetric: 27.0174 - val_loss: 28.7593 - val_MinusLogProbMetric: 28.7593 - lr: 8.3333e-05 - 31s/epoch - 159ms/step
Epoch 538/1000
2023-10-26 05:11:01.789 
Epoch 538/1000 
	 loss: 27.0259, MinusLogProbMetric: 27.0259, val_loss: 28.8197, val_MinusLogProbMetric: 28.8197

Epoch 538: val_loss did not improve from 28.66298
196/196 - 35s - loss: 27.0259 - MinusLogProbMetric: 27.0259 - val_loss: 28.8197 - val_MinusLogProbMetric: 28.8197 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 539/1000
2023-10-26 05:11:33.730 
Epoch 539/1000 
	 loss: 27.0362, MinusLogProbMetric: 27.0362, val_loss: 28.7377, val_MinusLogProbMetric: 28.7377

Epoch 539: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.0362 - MinusLogProbMetric: 27.0362 - val_loss: 28.7377 - val_MinusLogProbMetric: 28.7377 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 540/1000
2023-10-26 05:12:03.755 
Epoch 540/1000 
	 loss: 27.0336, MinusLogProbMetric: 27.0336, val_loss: 28.7269, val_MinusLogProbMetric: 28.7269

Epoch 540: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0336 - MinusLogProbMetric: 27.0336 - val_loss: 28.7269 - val_MinusLogProbMetric: 28.7269 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 541/1000
2023-10-26 05:12:33.739 
Epoch 541/1000 
	 loss: 27.0248, MinusLogProbMetric: 27.0248, val_loss: 28.7658, val_MinusLogProbMetric: 28.7658

Epoch 541: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0248 - MinusLogProbMetric: 27.0248 - val_loss: 28.7658 - val_MinusLogProbMetric: 28.7658 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 542/1000
2023-10-26 05:13:03.817 
Epoch 542/1000 
	 loss: 27.0367, MinusLogProbMetric: 27.0367, val_loss: 28.7459, val_MinusLogProbMetric: 28.7459

Epoch 542: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0367 - MinusLogProbMetric: 27.0367 - val_loss: 28.7459 - val_MinusLogProbMetric: 28.7459 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 543/1000
2023-10-26 05:13:34.885 
Epoch 543/1000 
	 loss: 27.0302, MinusLogProbMetric: 27.0302, val_loss: 28.7911, val_MinusLogProbMetric: 28.7911

Epoch 543: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.0302 - MinusLogProbMetric: 27.0302 - val_loss: 28.7911 - val_MinusLogProbMetric: 28.7911 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 544/1000
2023-10-26 05:14:09.962 
Epoch 544/1000 
	 loss: 27.0222, MinusLogProbMetric: 27.0222, val_loss: 28.7854, val_MinusLogProbMetric: 28.7854

Epoch 544: val_loss did not improve from 28.66298
196/196 - 35s - loss: 27.0222 - MinusLogProbMetric: 27.0222 - val_loss: 28.7854 - val_MinusLogProbMetric: 28.7854 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 545/1000
2023-10-26 05:14:44.702 
Epoch 545/1000 
	 loss: 27.0168, MinusLogProbMetric: 27.0168, val_loss: 28.8002, val_MinusLogProbMetric: 28.8002

Epoch 545: val_loss did not improve from 28.66298
196/196 - 35s - loss: 27.0168 - MinusLogProbMetric: 27.0168 - val_loss: 28.8002 - val_MinusLogProbMetric: 28.8002 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 546/1000
2023-10-26 05:15:15.450 
Epoch 546/1000 
	 loss: 27.0126, MinusLogProbMetric: 27.0126, val_loss: 28.7466, val_MinusLogProbMetric: 28.7466

Epoch 546: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.0126 - MinusLogProbMetric: 27.0126 - val_loss: 28.7466 - val_MinusLogProbMetric: 28.7466 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 547/1000
2023-10-26 05:15:45.379 
Epoch 547/1000 
	 loss: 27.0135, MinusLogProbMetric: 27.0135, val_loss: 28.7849, val_MinusLogProbMetric: 28.7849

Epoch 547: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0135 - MinusLogProbMetric: 27.0135 - val_loss: 28.7849 - val_MinusLogProbMetric: 28.7849 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 548/1000
2023-10-26 05:16:14.959 
Epoch 548/1000 
	 loss: 27.0225, MinusLogProbMetric: 27.0225, val_loss: 28.8003, val_MinusLogProbMetric: 28.8003

Epoch 548: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0225 - MinusLogProbMetric: 27.0225 - val_loss: 28.8003 - val_MinusLogProbMetric: 28.8003 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 549/1000
2023-10-26 05:16:44.606 
Epoch 549/1000 
	 loss: 27.0374, MinusLogProbMetric: 27.0374, val_loss: 28.7248, val_MinusLogProbMetric: 28.7248

Epoch 549: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0374 - MinusLogProbMetric: 27.0374 - val_loss: 28.7248 - val_MinusLogProbMetric: 28.7248 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 550/1000
2023-10-26 05:17:19.234 
Epoch 550/1000 
	 loss: 27.0218, MinusLogProbMetric: 27.0218, val_loss: 28.8408, val_MinusLogProbMetric: 28.8408

Epoch 550: val_loss did not improve from 28.66298
196/196 - 35s - loss: 27.0218 - MinusLogProbMetric: 27.0218 - val_loss: 28.8408 - val_MinusLogProbMetric: 28.8408 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 551/1000
2023-10-26 05:17:53.599 
Epoch 551/1000 
	 loss: 27.0327, MinusLogProbMetric: 27.0327, val_loss: 28.7398, val_MinusLogProbMetric: 28.7398

Epoch 551: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.0327 - MinusLogProbMetric: 27.0327 - val_loss: 28.7398 - val_MinusLogProbMetric: 28.7398 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 552/1000
2023-10-26 05:18:24.359 
Epoch 552/1000 
	 loss: 27.0236, MinusLogProbMetric: 27.0236, val_loss: 28.7427, val_MinusLogProbMetric: 28.7427

Epoch 552: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.0236 - MinusLogProbMetric: 27.0236 - val_loss: 28.7427 - val_MinusLogProbMetric: 28.7427 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 553/1000
2023-10-26 05:18:54.131 
Epoch 553/1000 
	 loss: 27.0257, MinusLogProbMetric: 27.0257, val_loss: 28.7493, val_MinusLogProbMetric: 28.7493

Epoch 553: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0257 - MinusLogProbMetric: 27.0257 - val_loss: 28.7493 - val_MinusLogProbMetric: 28.7493 - lr: 8.3333e-05 - 30s/epoch - 152ms/step
Epoch 554/1000
2023-10-26 05:19:23.600 
Epoch 554/1000 
	 loss: 27.0422, MinusLogProbMetric: 27.0422, val_loss: 28.7640, val_MinusLogProbMetric: 28.7640

Epoch 554: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.0422 - MinusLogProbMetric: 27.0422 - val_loss: 28.7640 - val_MinusLogProbMetric: 28.7640 - lr: 8.3333e-05 - 29s/epoch - 150ms/step
Epoch 555/1000
2023-10-26 05:19:54.117 
Epoch 555/1000 
	 loss: 27.0232, MinusLogProbMetric: 27.0232, val_loss: 28.9539, val_MinusLogProbMetric: 28.9539

Epoch 555: val_loss did not improve from 28.66298
196/196 - 31s - loss: 27.0232 - MinusLogProbMetric: 27.0232 - val_loss: 28.9539 - val_MinusLogProbMetric: 28.9539 - lr: 8.3333e-05 - 31s/epoch - 156ms/step
Epoch 556/1000
2023-10-26 05:20:26.134 
Epoch 556/1000 
	 loss: 27.0424, MinusLogProbMetric: 27.0424, val_loss: 28.7853, val_MinusLogProbMetric: 28.7853

Epoch 556: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.0424 - MinusLogProbMetric: 27.0424 - val_loss: 28.7853 - val_MinusLogProbMetric: 28.7853 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 557/1000
2023-10-26 05:21:00.217 
Epoch 557/1000 
	 loss: 27.0244, MinusLogProbMetric: 27.0244, val_loss: 28.8421, val_MinusLogProbMetric: 28.8421

Epoch 557: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.0244 - MinusLogProbMetric: 27.0244 - val_loss: 28.8421 - val_MinusLogProbMetric: 28.8421 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 558/1000
2023-10-26 05:21:34.372 
Epoch 558/1000 
	 loss: 27.0255, MinusLogProbMetric: 27.0255, val_loss: 28.7983, val_MinusLogProbMetric: 28.7983

Epoch 558: val_loss did not improve from 28.66298
196/196 - 34s - loss: 27.0255 - MinusLogProbMetric: 27.0255 - val_loss: 28.7983 - val_MinusLogProbMetric: 28.7983 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 559/1000
2023-10-26 05:22:03.792 
Epoch 559/1000 
	 loss: 27.0252, MinusLogProbMetric: 27.0252, val_loss: 28.8098, val_MinusLogProbMetric: 28.8098

Epoch 559: val_loss did not improve from 28.66298
196/196 - 29s - loss: 27.0252 - MinusLogProbMetric: 27.0252 - val_loss: 28.8098 - val_MinusLogProbMetric: 28.8098 - lr: 8.3333e-05 - 29s/epoch - 150ms/step
Epoch 560/1000
2023-10-26 05:22:33.333 
Epoch 560/1000 
	 loss: 27.0162, MinusLogProbMetric: 27.0162, val_loss: 28.8602, val_MinusLogProbMetric: 28.8602

Epoch 560: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0162 - MinusLogProbMetric: 27.0162 - val_loss: 28.8602 - val_MinusLogProbMetric: 28.8602 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 561/1000
2023-10-26 05:23:03.424 
Epoch 561/1000 
	 loss: 27.0183, MinusLogProbMetric: 27.0183, val_loss: 28.7605, val_MinusLogProbMetric: 28.7605

Epoch 561: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0183 - MinusLogProbMetric: 27.0183 - val_loss: 28.7605 - val_MinusLogProbMetric: 28.7605 - lr: 8.3333e-05 - 30s/epoch - 154ms/step
Epoch 562/1000
2023-10-26 05:23:33.148 
Epoch 562/1000 
	 loss: 27.0132, MinusLogProbMetric: 27.0132, val_loss: 28.7802, val_MinusLogProbMetric: 28.7802

Epoch 562: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0132 - MinusLogProbMetric: 27.0132 - val_loss: 28.7802 - val_MinusLogProbMetric: 28.7802 - lr: 8.3333e-05 - 30s/epoch - 152ms/step
Epoch 563/1000
2023-10-26 05:24:08.097 
Epoch 563/1000 
	 loss: 27.0174, MinusLogProbMetric: 27.0174, val_loss: 28.7763, val_MinusLogProbMetric: 28.7763

Epoch 563: val_loss did not improve from 28.66298
196/196 - 35s - loss: 27.0174 - MinusLogProbMetric: 27.0174 - val_loss: 28.7763 - val_MinusLogProbMetric: 28.7763 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 564/1000
2023-10-26 05:24:41.369 
Epoch 564/1000 
	 loss: 27.0226, MinusLogProbMetric: 27.0226, val_loss: 28.7791, val_MinusLogProbMetric: 28.7791

Epoch 564: val_loss did not improve from 28.66298
196/196 - 33s - loss: 27.0226 - MinusLogProbMetric: 27.0226 - val_loss: 28.7791 - val_MinusLogProbMetric: 28.7791 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 565/1000
2023-10-26 05:25:13.171 
Epoch 565/1000 
	 loss: 27.0093, MinusLogProbMetric: 27.0093, val_loss: 28.7600, val_MinusLogProbMetric: 28.7600

Epoch 565: val_loss did not improve from 28.66298
196/196 - 32s - loss: 27.0093 - MinusLogProbMetric: 27.0093 - val_loss: 28.7600 - val_MinusLogProbMetric: 28.7600 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 566/1000
2023-10-26 05:25:42.840 
Epoch 566/1000 
	 loss: 27.0194, MinusLogProbMetric: 27.0194, val_loss: 28.7348, val_MinusLogProbMetric: 28.7348

Epoch 566: val_loss did not improve from 28.66298
196/196 - 30s - loss: 27.0194 - MinusLogProbMetric: 27.0194 - val_loss: 28.7348 - val_MinusLogProbMetric: 28.7348 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 567/1000
2023-10-26 05:26:12.397 
Epoch 567/1000 
	 loss: 27.0027, MinusLogProbMetric: 27.0027, val_loss: 28.7637, val_MinusLogProbMetric: 28.7637

Epoch 567: val_loss did not improve from 28.66298
Restoring model weights from the end of the best epoch: 467.
196/196 - 30s - loss: 27.0027 - MinusLogProbMetric: 27.0027 - val_loss: 28.7637 - val_MinusLogProbMetric: 28.7637 - lr: 8.3333e-05 - 30s/epoch - 152ms/step
Epoch 567: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 721.
Model trained in 19311.04 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.57 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.83 s.
===========
Run 370/720 done in 19528.30 s.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

===========
Generating train data for run 376.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_376
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_149"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_150 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f0c0c65ff10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c846cbb50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c846cbb50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c0423e710>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ce0561000>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ce0561570>, <keras.callbacks.ModelCheckpoint object at 0x7f0ce0561630>, <keras.callbacks.EarlyStopping object at 0x7f0ce05618a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ce05618d0>, <keras.callbacks.TerminateOnNaN object at 0x7f0ce0561510>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_376/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 376/720 with hyperparameters:
timestamp = 2023-10-26 05:26:23.592931
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:29:08.809 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6634.4780, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 165s - loss: nan - MinusLogProbMetric: 6634.4780 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 165s/epoch - 842ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 376.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_376
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_160"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_161 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f0d653c4040>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4b86fd60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4b86fd60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d4b5bb6d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d858820b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d85882620>, <keras.callbacks.ModelCheckpoint object at 0x7f0d858826e0>, <keras.callbacks.EarlyStopping object at 0x7f0d85882950>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d85882980>, <keras.callbacks.TerminateOnNaN object at 0x7f0d858825c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_376/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 376/720 with hyperparameters:
timestamp = 2023-10-26 05:29:20.078296
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 136: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:32:54.562 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3192.0442, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 214s - loss: nan - MinusLogProbMetric: 3192.0442 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 214s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 0.0001111111111111111.
===========
Generating train data for run 376.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_376
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_171"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_172 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f0b7d2a3d00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c68333e50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c68333e50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c68378670>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b7d2fed10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b7d2ff280>, <keras.callbacks.ModelCheckpoint object at 0x7f0b7d2ff340>, <keras.callbacks.EarlyStopping object at 0x7f0b7d2ff5b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b7d2ff5e0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b7d2ff220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_376/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 376/720 with hyperparameters:
timestamp = 2023-10-26 05:33:05.376593
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 05:37:00.826 
Epoch 1/1000 
	 loss: 3983.5347, MinusLogProbMetric: 3983.5347, val_loss: 2014.9442, val_MinusLogProbMetric: 2014.9442

Epoch 1: val_loss improved from inf to 2014.94421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 237s - loss: 3983.5347 - MinusLogProbMetric: 3983.5347 - val_loss: 2014.9442 - val_MinusLogProbMetric: 2014.9442 - lr: 1.1111e-04 - 237s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 77: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:37:35.109 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 1530.5674, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 2014.94421
196/196 - 32s - loss: nan - MinusLogProbMetric: 1530.5674 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 32s/epoch - 165ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 3.703703703703703e-05.
===========
Generating train data for run 376.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_376
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_182"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_183 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f0b8560f340>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d863e7a30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d863e7a30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b74202020>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b85685a80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b85685ff0>, <keras.callbacks.ModelCheckpoint object at 0x7f0b856860b0>, <keras.callbacks.EarlyStopping object at 0x7f0b85686320>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b85686350>, <keras.callbacks.TerminateOnNaN object at 0x7f0b85685f90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 376/720 with hyperparameters:
timestamp = 2023-10-26 05:37:46.907490
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 32: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 05:40:39.654 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2106.9402, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 2106.9402 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 172s/epoch - 880ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 1.2345679012345677e-05.
===========
Generating train data for run 376.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_376
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_193"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_194 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f0d276aa590>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d275f4e80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d275f4e80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d270d13f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d26f3e7a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d26f3ed10>, <keras.callbacks.ModelCheckpoint object at 0x7f0d26f3edd0>, <keras.callbacks.EarlyStopping object at 0x7f0d26f3f040>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d26f3f070>, <keras.callbacks.TerminateOnNaN object at 0x7f0d26f3ecb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 376/720 with hyperparameters:
timestamp = 2023-10-26 05:40:49.001227
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 05:44:51.156 
Epoch 1/1000 
	 loss: 1519.2849, MinusLogProbMetric: 1519.2849, val_loss: 1310.0930, val_MinusLogProbMetric: 1310.0930

Epoch 1: val_loss improved from inf to 1310.09302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 244s - loss: 1519.2849 - MinusLogProbMetric: 1519.2849 - val_loss: 1310.0930 - val_MinusLogProbMetric: 1310.0930 - lr: 1.2346e-05 - 244s/epoch - 1s/step
Epoch 2/1000
2023-10-26 05:46:16.478 
Epoch 2/1000 
	 loss: 1128.6704, MinusLogProbMetric: 1128.6704, val_loss: 1017.1287, val_MinusLogProbMetric: 1017.1287

Epoch 2: val_loss improved from 1310.09302 to 1017.12866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 1128.6704 - MinusLogProbMetric: 1128.6704 - val_loss: 1017.1287 - val_MinusLogProbMetric: 1017.1287 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 3/1000
2023-10-26 05:47:41.191 
Epoch 3/1000 
	 loss: 1084.9314, MinusLogProbMetric: 1084.9314, val_loss: 1018.9086, val_MinusLogProbMetric: 1018.9086

Epoch 3: val_loss did not improve from 1017.12866
196/196 - 83s - loss: 1084.9314 - MinusLogProbMetric: 1084.9314 - val_loss: 1018.9086 - val_MinusLogProbMetric: 1018.9086 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 4/1000
2023-10-26 05:49:04.559 
Epoch 4/1000 
	 loss: 1022.6033, MinusLogProbMetric: 1022.6033, val_loss: 921.2023, val_MinusLogProbMetric: 921.2023

Epoch 4: val_loss improved from 1017.12866 to 921.20233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 1022.6033 - MinusLogProbMetric: 1022.6033 - val_loss: 921.2023 - val_MinusLogProbMetric: 921.2023 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 5/1000
2023-10-26 05:50:28.642 
Epoch 5/1000 
	 loss: 820.5273, MinusLogProbMetric: 820.5273, val_loss: 743.1793, val_MinusLogProbMetric: 743.1793

Epoch 5: val_loss improved from 921.20233 to 743.17926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 820.5273 - MinusLogProbMetric: 820.5273 - val_loss: 743.1793 - val_MinusLogProbMetric: 743.1793 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 6/1000
2023-10-26 05:51:52.711 
Epoch 6/1000 
	 loss: 819.0115, MinusLogProbMetric: 819.0115, val_loss: 987.3358, val_MinusLogProbMetric: 987.3358

Epoch 6: val_loss did not improve from 743.17926
196/196 - 83s - loss: 819.0115 - MinusLogProbMetric: 819.0115 - val_loss: 987.3358 - val_MinusLogProbMetric: 987.3358 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 7/1000
2023-10-26 05:53:16.031 
Epoch 7/1000 
	 loss: 870.0974, MinusLogProbMetric: 870.0974, val_loss: 786.6564, val_MinusLogProbMetric: 786.6564

Epoch 7: val_loss did not improve from 743.17926
196/196 - 83s - loss: 870.0974 - MinusLogProbMetric: 870.0974 - val_loss: 786.6564 - val_MinusLogProbMetric: 786.6564 - lr: 1.2346e-05 - 83s/epoch - 425ms/step
Epoch 8/1000
2023-10-26 05:54:39.052 
Epoch 8/1000 
	 loss: 707.2665, MinusLogProbMetric: 707.2665, val_loss: 660.7780, val_MinusLogProbMetric: 660.7780

Epoch 8: val_loss improved from 743.17926 to 660.77802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 707.2665 - MinusLogProbMetric: 707.2665 - val_loss: 660.7780 - val_MinusLogProbMetric: 660.7780 - lr: 1.2346e-05 - 85s/epoch - 431ms/step
Epoch 9/1000
2023-10-26 05:56:03.118 
Epoch 9/1000 
	 loss: 647.9949, MinusLogProbMetric: 647.9949, val_loss: 619.1398, val_MinusLogProbMetric: 619.1398

Epoch 9: val_loss improved from 660.77802 to 619.13977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 647.9949 - MinusLogProbMetric: 647.9949 - val_loss: 619.1398 - val_MinusLogProbMetric: 619.1398 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 10/1000
2023-10-26 05:57:26.949 
Epoch 10/1000 
	 loss: 615.7529, MinusLogProbMetric: 615.7529, val_loss: 593.4124, val_MinusLogProbMetric: 593.4124

Epoch 10: val_loss improved from 619.13977 to 593.41235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 615.7529 - MinusLogProbMetric: 615.7529 - val_loss: 593.4124 - val_MinusLogProbMetric: 593.4124 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 11/1000
2023-10-26 05:58:51.155 
Epoch 11/1000 
	 loss: 601.8658, MinusLogProbMetric: 601.8658, val_loss: 580.6142, val_MinusLogProbMetric: 580.6142

Epoch 11: val_loss improved from 593.41235 to 580.61420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 601.8658 - MinusLogProbMetric: 601.8658 - val_loss: 580.6142 - val_MinusLogProbMetric: 580.6142 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 12/1000
2023-10-26 06:00:15.577 
Epoch 12/1000 
	 loss: 627.1432, MinusLogProbMetric: 627.1432, val_loss: 682.5540, val_MinusLogProbMetric: 682.5540

Epoch 12: val_loss did not improve from 580.61420
196/196 - 83s - loss: 627.1432 - MinusLogProbMetric: 627.1432 - val_loss: 682.5540 - val_MinusLogProbMetric: 682.5540 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 13/1000
2023-10-26 06:01:38.817 
Epoch 13/1000 
	 loss: 610.7863, MinusLogProbMetric: 610.7863, val_loss: 578.0526, val_MinusLogProbMetric: 578.0526

Epoch 13: val_loss improved from 580.61420 to 578.05255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 610.7863 - MinusLogProbMetric: 610.7863 - val_loss: 578.0526 - val_MinusLogProbMetric: 578.0526 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 14/1000
2023-10-26 06:03:02.882 
Epoch 14/1000 
	 loss: 575.9277, MinusLogProbMetric: 575.9277, val_loss: 588.7984, val_MinusLogProbMetric: 588.7984

Epoch 14: val_loss did not improve from 578.05255
196/196 - 83s - loss: 575.9277 - MinusLogProbMetric: 575.9277 - val_loss: 588.7984 - val_MinusLogProbMetric: 588.7984 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 15/1000
2023-10-26 06:04:25.866 
Epoch 15/1000 
	 loss: 561.2535, MinusLogProbMetric: 561.2535, val_loss: 544.7178, val_MinusLogProbMetric: 544.7178

Epoch 15: val_loss improved from 578.05255 to 544.71783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 561.2535 - MinusLogProbMetric: 561.2535 - val_loss: 544.7178 - val_MinusLogProbMetric: 544.7178 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 16/1000
2023-10-26 06:05:49.653 
Epoch 16/1000 
	 loss: 527.3754, MinusLogProbMetric: 527.3754, val_loss: 518.3000, val_MinusLogProbMetric: 518.3000

Epoch 16: val_loss improved from 544.71783 to 518.29999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 527.3754 - MinusLogProbMetric: 527.3754 - val_loss: 518.3000 - val_MinusLogProbMetric: 518.3000 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 17/1000
2023-10-26 06:07:13.953 
Epoch 17/1000 
	 loss: 505.2899, MinusLogProbMetric: 505.2899, val_loss: 493.5041, val_MinusLogProbMetric: 493.5041

Epoch 17: val_loss improved from 518.29999 to 493.50412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 505.2899 - MinusLogProbMetric: 505.2899 - val_loss: 493.5041 - val_MinusLogProbMetric: 493.5041 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 18/1000
2023-10-26 06:08:37.993 
Epoch 18/1000 
	 loss: 488.8622, MinusLogProbMetric: 488.8622, val_loss: 484.0395, val_MinusLogProbMetric: 484.0395

Epoch 18: val_loss improved from 493.50412 to 484.03946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 488.8622 - MinusLogProbMetric: 488.8622 - val_loss: 484.0395 - val_MinusLogProbMetric: 484.0395 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 19/1000
2023-10-26 06:10:02.932 
Epoch 19/1000 
	 loss: 493.0045, MinusLogProbMetric: 493.0045, val_loss: 518.7325, val_MinusLogProbMetric: 518.7325

Epoch 19: val_loss did not improve from 484.03946
196/196 - 83s - loss: 493.0045 - MinusLogProbMetric: 493.0045 - val_loss: 518.7325 - val_MinusLogProbMetric: 518.7325 - lr: 1.2346e-05 - 83s/epoch - 425ms/step
Epoch 20/1000
2023-10-26 06:11:26.328 
Epoch 20/1000 
	 loss: 481.8432, MinusLogProbMetric: 481.8432, val_loss: 460.3236, val_MinusLogProbMetric: 460.3236

Epoch 20: val_loss improved from 484.03946 to 460.32361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 481.8432 - MinusLogProbMetric: 481.8432 - val_loss: 460.3236 - val_MinusLogProbMetric: 460.3236 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 21/1000
2023-10-26 06:12:51.276 
Epoch 21/1000 
	 loss: 452.0691, MinusLogProbMetric: 452.0691, val_loss: 489.6290, val_MinusLogProbMetric: 489.6290

Epoch 21: val_loss did not improve from 460.32361
196/196 - 83s - loss: 452.0691 - MinusLogProbMetric: 452.0691 - val_loss: 489.6290 - val_MinusLogProbMetric: 489.6290 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 22/1000
2023-10-26 06:14:13.570 
Epoch 22/1000 
	 loss: 443.2631, MinusLogProbMetric: 443.2631, val_loss: 437.3502, val_MinusLogProbMetric: 437.3502

Epoch 22: val_loss improved from 460.32361 to 437.35019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 443.2631 - MinusLogProbMetric: 443.2631 - val_loss: 437.3502 - val_MinusLogProbMetric: 437.3502 - lr: 1.2346e-05 - 84s/epoch - 426ms/step
Epoch 23/1000
2023-10-26 06:15:37.731 
Epoch 23/1000 
	 loss: 432.7988, MinusLogProbMetric: 432.7988, val_loss: 421.3143, val_MinusLogProbMetric: 421.3143

Epoch 23: val_loss improved from 437.35019 to 421.31430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 432.7988 - MinusLogProbMetric: 432.7988 - val_loss: 421.3143 - val_MinusLogProbMetric: 421.3143 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 24/1000
2023-10-26 06:17:01.927 
Epoch 24/1000 
	 loss: 415.6909, MinusLogProbMetric: 415.6909, val_loss: 412.8749, val_MinusLogProbMetric: 412.8749

Epoch 24: val_loss improved from 421.31430 to 412.87494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 415.6909 - MinusLogProbMetric: 415.6909 - val_loss: 412.8749 - val_MinusLogProbMetric: 412.8749 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 25/1000
2023-10-26 06:18:26.085 
Epoch 25/1000 
	 loss: 406.5174, MinusLogProbMetric: 406.5174, val_loss: 442.2177, val_MinusLogProbMetric: 442.2177

Epoch 25: val_loss did not improve from 412.87494
196/196 - 83s - loss: 406.5174 - MinusLogProbMetric: 406.5174 - val_loss: 442.2177 - val_MinusLogProbMetric: 442.2177 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 26/1000
2023-10-26 06:19:47.814 
Epoch 26/1000 
	 loss: 415.3674, MinusLogProbMetric: 415.3674, val_loss: 399.4723, val_MinusLogProbMetric: 399.4723

Epoch 26: val_loss improved from 412.87494 to 399.47229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 415.3674 - MinusLogProbMetric: 415.3674 - val_loss: 399.4723 - val_MinusLogProbMetric: 399.4723 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 27/1000
2023-10-26 06:21:11.885 
Epoch 27/1000 
	 loss: 395.5735, MinusLogProbMetric: 395.5735, val_loss: 392.8910, val_MinusLogProbMetric: 392.8910

Epoch 27: val_loss improved from 399.47229 to 392.89096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 395.5735 - MinusLogProbMetric: 395.5735 - val_loss: 392.8910 - val_MinusLogProbMetric: 392.8910 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 28/1000
2023-10-26 06:22:36.257 
Epoch 28/1000 
	 loss: 392.8752, MinusLogProbMetric: 392.8752, val_loss: 388.2888, val_MinusLogProbMetric: 388.2888

Epoch 28: val_loss improved from 392.89096 to 388.28882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 392.8752 - MinusLogProbMetric: 392.8752 - val_loss: 388.2888 - val_MinusLogProbMetric: 388.2888 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 29/1000
2023-10-26 06:24:00.044 
Epoch 29/1000 
	 loss: 384.6236, MinusLogProbMetric: 384.6236, val_loss: 381.4155, val_MinusLogProbMetric: 381.4155

Epoch 29: val_loss improved from 388.28882 to 381.41547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 384.6236 - MinusLogProbMetric: 384.6236 - val_loss: 381.4155 - val_MinusLogProbMetric: 381.4155 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 30/1000
2023-10-26 06:25:24.825 
Epoch 30/1000 
	 loss: 378.6517, MinusLogProbMetric: 378.6517, val_loss: 374.5901, val_MinusLogProbMetric: 374.5901

Epoch 30: val_loss improved from 381.41547 to 374.59012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 378.6517 - MinusLogProbMetric: 378.6517 - val_loss: 374.5901 - val_MinusLogProbMetric: 374.5901 - lr: 1.2346e-05 - 85s/epoch - 431ms/step
Epoch 31/1000
2023-10-26 06:26:49.577 
Epoch 31/1000 
	 loss: 374.4904, MinusLogProbMetric: 374.4904, val_loss: 372.2737, val_MinusLogProbMetric: 372.2737

Epoch 31: val_loss improved from 374.59012 to 372.27368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 374.4904 - MinusLogProbMetric: 374.4904 - val_loss: 372.2737 - val_MinusLogProbMetric: 372.2737 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 32/1000
2023-10-26 06:28:13.266 
Epoch 32/1000 
	 loss: 371.8466, MinusLogProbMetric: 371.8466, val_loss: 367.5785, val_MinusLogProbMetric: 367.5785

Epoch 32: val_loss improved from 372.27368 to 367.57846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 371.8466 - MinusLogProbMetric: 371.8466 - val_loss: 367.5785 - val_MinusLogProbMetric: 367.5785 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 33/1000
2023-10-26 06:29:37.592 
Epoch 33/1000 
	 loss: 387.4637, MinusLogProbMetric: 387.4637, val_loss: 416.6460, val_MinusLogProbMetric: 416.6460

Epoch 33: val_loss did not improve from 367.57846
196/196 - 83s - loss: 387.4637 - MinusLogProbMetric: 387.4637 - val_loss: 416.6460 - val_MinusLogProbMetric: 416.6460 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 34/1000
2023-10-26 06:31:00.162 
Epoch 34/1000 
	 loss: 386.9716, MinusLogProbMetric: 386.9716, val_loss: 372.0393, val_MinusLogProbMetric: 372.0393

Epoch 34: val_loss did not improve from 367.57846
196/196 - 83s - loss: 386.9716 - MinusLogProbMetric: 386.9716 - val_loss: 372.0393 - val_MinusLogProbMetric: 372.0393 - lr: 1.2346e-05 - 83s/epoch - 421ms/step
Epoch 35/1000
2023-10-26 06:32:22.722 
Epoch 35/1000 
	 loss: 677.6205, MinusLogProbMetric: 677.6205, val_loss: 493.8550, val_MinusLogProbMetric: 493.8550

Epoch 35: val_loss did not improve from 367.57846
196/196 - 83s - loss: 677.6205 - MinusLogProbMetric: 677.6205 - val_loss: 493.8550 - val_MinusLogProbMetric: 493.8550 - lr: 1.2346e-05 - 83s/epoch - 421ms/step
Epoch 36/1000
2023-10-26 06:33:45.439 
Epoch 36/1000 
	 loss: 479.5454, MinusLogProbMetric: 479.5454, val_loss: 532.3253, val_MinusLogProbMetric: 532.3253

Epoch 36: val_loss did not improve from 367.57846
196/196 - 83s - loss: 479.5454 - MinusLogProbMetric: 479.5454 - val_loss: 532.3253 - val_MinusLogProbMetric: 532.3253 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 37/1000
2023-10-26 06:35:07.820 
Epoch 37/1000 
	 loss: 533.7705, MinusLogProbMetric: 533.7705, val_loss: 473.3989, val_MinusLogProbMetric: 473.3989

Epoch 37: val_loss did not improve from 367.57846
196/196 - 82s - loss: 533.7705 - MinusLogProbMetric: 533.7705 - val_loss: 473.3989 - val_MinusLogProbMetric: 473.3989 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 38/1000
2023-10-26 06:36:30.715 
Epoch 38/1000 
	 loss: 458.0887, MinusLogProbMetric: 458.0887, val_loss: 447.7994, val_MinusLogProbMetric: 447.7994

Epoch 38: val_loss did not improve from 367.57846
196/196 - 83s - loss: 458.0887 - MinusLogProbMetric: 458.0887 - val_loss: 447.7994 - val_MinusLogProbMetric: 447.7994 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 39/1000
2023-10-26 06:37:54.161 
Epoch 39/1000 
	 loss: 477.1253, MinusLogProbMetric: 477.1253, val_loss: 453.2504, val_MinusLogProbMetric: 453.2504

Epoch 39: val_loss did not improve from 367.57846
196/196 - 83s - loss: 477.1253 - MinusLogProbMetric: 477.1253 - val_loss: 453.2504 - val_MinusLogProbMetric: 453.2504 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 40/1000
2023-10-26 06:39:16.430 
Epoch 40/1000 
	 loss: 437.7466, MinusLogProbMetric: 437.7466, val_loss: 427.2120, val_MinusLogProbMetric: 427.2120

Epoch 40: val_loss did not improve from 367.57846
196/196 - 82s - loss: 437.7466 - MinusLogProbMetric: 437.7466 - val_loss: 427.2120 - val_MinusLogProbMetric: 427.2120 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 41/1000
2023-10-26 06:40:38.522 
Epoch 41/1000 
	 loss: 423.7205, MinusLogProbMetric: 423.7205, val_loss: 438.2471, val_MinusLogProbMetric: 438.2471

Epoch 41: val_loss did not improve from 367.57846
196/196 - 82s - loss: 423.7205 - MinusLogProbMetric: 423.7205 - val_loss: 438.2471 - val_MinusLogProbMetric: 438.2471 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 42/1000
2023-10-26 06:42:01.197 
Epoch 42/1000 
	 loss: 418.1468, MinusLogProbMetric: 418.1468, val_loss: 406.4445, val_MinusLogProbMetric: 406.4445

Epoch 42: val_loss did not improve from 367.57846
196/196 - 83s - loss: 418.1468 - MinusLogProbMetric: 418.1468 - val_loss: 406.4445 - val_MinusLogProbMetric: 406.4445 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 43/1000
2023-10-26 06:43:23.502 
Epoch 43/1000 
	 loss: 403.7932, MinusLogProbMetric: 403.7932, val_loss: 396.9915, val_MinusLogProbMetric: 396.9915

Epoch 43: val_loss did not improve from 367.57846
196/196 - 82s - loss: 403.7932 - MinusLogProbMetric: 403.7932 - val_loss: 396.9915 - val_MinusLogProbMetric: 396.9915 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 44/1000
2023-10-26 06:44:45.634 
Epoch 44/1000 
	 loss: 394.5314, MinusLogProbMetric: 394.5314, val_loss: 396.2253, val_MinusLogProbMetric: 396.2253

Epoch 44: val_loss did not improve from 367.57846
196/196 - 82s - loss: 394.5314 - MinusLogProbMetric: 394.5314 - val_loss: 396.2253 - val_MinusLogProbMetric: 396.2253 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 45/1000
2023-10-26 06:46:08.347 
Epoch 45/1000 
	 loss: 387.3944, MinusLogProbMetric: 387.3944, val_loss: 381.5872, val_MinusLogProbMetric: 381.5872

Epoch 45: val_loss did not improve from 367.57846
196/196 - 83s - loss: 387.3944 - MinusLogProbMetric: 387.3944 - val_loss: 381.5872 - val_MinusLogProbMetric: 381.5872 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 46/1000
2023-10-26 06:47:30.822 
Epoch 46/1000 
	 loss: 379.8329, MinusLogProbMetric: 379.8329, val_loss: 377.1878, val_MinusLogProbMetric: 377.1878

Epoch 46: val_loss did not improve from 367.57846
196/196 - 82s - loss: 379.8329 - MinusLogProbMetric: 379.8329 - val_loss: 377.1878 - val_MinusLogProbMetric: 377.1878 - lr: 1.2346e-05 - 82s/epoch - 421ms/step
Epoch 47/1000
2023-10-26 06:48:53.298 
Epoch 47/1000 
	 loss: 374.0150, MinusLogProbMetric: 374.0150, val_loss: 371.0387, val_MinusLogProbMetric: 371.0387

Epoch 47: val_loss did not improve from 367.57846
196/196 - 82s - loss: 374.0150 - MinusLogProbMetric: 374.0150 - val_loss: 371.0387 - val_MinusLogProbMetric: 371.0387 - lr: 1.2346e-05 - 82s/epoch - 421ms/step
Epoch 48/1000
2023-10-26 06:50:15.703 
Epoch 48/1000 
	 loss: 369.4175, MinusLogProbMetric: 369.4175, val_loss: 376.5807, val_MinusLogProbMetric: 376.5807

Epoch 48: val_loss did not improve from 367.57846
196/196 - 82s - loss: 369.4175 - MinusLogProbMetric: 369.4175 - val_loss: 376.5807 - val_MinusLogProbMetric: 376.5807 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 49/1000
2023-10-26 06:51:37.468 
Epoch 49/1000 
	 loss: 364.7638, MinusLogProbMetric: 364.7638, val_loss: 362.8989, val_MinusLogProbMetric: 362.8989

Epoch 49: val_loss improved from 367.57846 to 362.89890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 364.7638 - MinusLogProbMetric: 364.7638 - val_loss: 362.8989 - val_MinusLogProbMetric: 362.8989 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 50/1000
2023-10-26 06:52:56.638 
Epoch 50/1000 
	 loss: 373.3849, MinusLogProbMetric: 373.3849, val_loss: 393.9631, val_MinusLogProbMetric: 393.9631

Epoch 50: val_loss did not improve from 362.89890
196/196 - 78s - loss: 373.3849 - MinusLogProbMetric: 373.3849 - val_loss: 393.9631 - val_MinusLogProbMetric: 393.9631 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 51/1000
2023-10-26 06:54:15.507 
Epoch 51/1000 
	 loss: 373.1573, MinusLogProbMetric: 373.1573, val_loss: 359.5538, val_MinusLogProbMetric: 359.5538

Epoch 51: val_loss improved from 362.89890 to 359.55380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 373.1573 - MinusLogProbMetric: 373.1573 - val_loss: 359.5538 - val_MinusLogProbMetric: 359.5538 - lr: 1.2346e-05 - 80s/epoch - 410ms/step
Epoch 52/1000
2023-10-26 06:55:36.866 
Epoch 52/1000 
	 loss: 358.5319, MinusLogProbMetric: 358.5319, val_loss: 359.7036, val_MinusLogProbMetric: 359.7036

Epoch 52: val_loss did not improve from 359.55380
196/196 - 80s - loss: 358.5319 - MinusLogProbMetric: 358.5319 - val_loss: 359.7036 - val_MinusLogProbMetric: 359.7036 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 53/1000
2023-10-26 06:56:54.969 
Epoch 53/1000 
	 loss: 351.7621, MinusLogProbMetric: 351.7621, val_loss: 348.2982, val_MinusLogProbMetric: 348.2982

Epoch 53: val_loss improved from 359.55380 to 348.29819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 79s - loss: 351.7621 - MinusLogProbMetric: 351.7621 - val_loss: 348.2982 - val_MinusLogProbMetric: 348.2982 - lr: 1.2346e-05 - 79s/epoch - 405ms/step
Epoch 54/1000
2023-10-26 06:58:12.805 
Epoch 54/1000 
	 loss: 345.5361, MinusLogProbMetric: 345.5361, val_loss: 343.6823, val_MinusLogProbMetric: 343.6823

Epoch 54: val_loss improved from 348.29819 to 343.68228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 78s - loss: 345.5361 - MinusLogProbMetric: 345.5361 - val_loss: 343.6823 - val_MinusLogProbMetric: 343.6823 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 55/1000
2023-10-26 06:59:35.931 
Epoch 55/1000 
	 loss: 341.6001, MinusLogProbMetric: 341.6001, val_loss: 336.2148, val_MinusLogProbMetric: 336.2148

Epoch 55: val_loss improved from 343.68228 to 336.21475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 341.6001 - MinusLogProbMetric: 341.6001 - val_loss: 336.2148 - val_MinusLogProbMetric: 336.2148 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 56/1000
2023-10-26 07:00:58.722 
Epoch 56/1000 
	 loss: 340.2598, MinusLogProbMetric: 340.2598, val_loss: 336.3449, val_MinusLogProbMetric: 336.3449

Epoch 56: val_loss did not improve from 336.21475
196/196 - 81s - loss: 340.2598 - MinusLogProbMetric: 340.2598 - val_loss: 336.3449 - val_MinusLogProbMetric: 336.3449 - lr: 1.2346e-05 - 81s/epoch - 416ms/step
Epoch 57/1000
2023-10-26 07:02:20.073 
Epoch 57/1000 
	 loss: 336.6096, MinusLogProbMetric: 336.6096, val_loss: 333.9248, val_MinusLogProbMetric: 333.9248

Epoch 57: val_loss improved from 336.21475 to 333.92480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 336.6096 - MinusLogProbMetric: 336.6096 - val_loss: 333.9248 - val_MinusLogProbMetric: 333.9248 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 58/1000
2023-10-26 07:03:43.499 
Epoch 58/1000 
	 loss: 333.1482, MinusLogProbMetric: 333.1482, val_loss: 331.9549, val_MinusLogProbMetric: 331.9549

Epoch 58: val_loss improved from 333.92480 to 331.95486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 333.1482 - MinusLogProbMetric: 333.1482 - val_loss: 331.9549 - val_MinusLogProbMetric: 331.9549 - lr: 1.2346e-05 - 84s/epoch - 426ms/step
Epoch 59/1000
2023-10-26 07:05:07.231 
Epoch 59/1000 
	 loss: 331.3244, MinusLogProbMetric: 331.3244, val_loss: 327.2670, val_MinusLogProbMetric: 327.2670

Epoch 59: val_loss improved from 331.95486 to 327.26697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 331.3244 - MinusLogProbMetric: 331.3244 - val_loss: 327.2670 - val_MinusLogProbMetric: 327.2670 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 60/1000
2023-10-26 07:06:30.759 
Epoch 60/1000 
	 loss: 328.0218, MinusLogProbMetric: 328.0218, val_loss: 333.7038, val_MinusLogProbMetric: 333.7038

Epoch 60: val_loss did not improve from 327.26697
196/196 - 82s - loss: 328.0218 - MinusLogProbMetric: 328.0218 - val_loss: 333.7038 - val_MinusLogProbMetric: 333.7038 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 61/1000
2023-10-26 07:07:53.272 
Epoch 61/1000 
	 loss: 466.2056, MinusLogProbMetric: 466.2056, val_loss: 532.7397, val_MinusLogProbMetric: 532.7397

Epoch 61: val_loss did not improve from 327.26697
196/196 - 83s - loss: 466.2056 - MinusLogProbMetric: 466.2056 - val_loss: 532.7397 - val_MinusLogProbMetric: 532.7397 - lr: 1.2346e-05 - 83s/epoch - 421ms/step
Epoch 62/1000
2023-10-26 07:09:15.203 
Epoch 62/1000 
	 loss: 469.3219, MinusLogProbMetric: 469.3219, val_loss: 443.7571, val_MinusLogProbMetric: 443.7571

Epoch 62: val_loss did not improve from 327.26697
196/196 - 82s - loss: 469.3219 - MinusLogProbMetric: 469.3219 - val_loss: 443.7571 - val_MinusLogProbMetric: 443.7571 - lr: 1.2346e-05 - 82s/epoch - 418ms/step
Epoch 63/1000
2023-10-26 07:10:37.167 
Epoch 63/1000 
	 loss: 424.8936, MinusLogProbMetric: 424.8936, val_loss: 410.7099, val_MinusLogProbMetric: 410.7099

Epoch 63: val_loss did not improve from 327.26697
196/196 - 82s - loss: 424.8936 - MinusLogProbMetric: 424.8936 - val_loss: 410.7099 - val_MinusLogProbMetric: 410.7099 - lr: 1.2346e-05 - 82s/epoch - 418ms/step
Epoch 64/1000
2023-10-26 07:11:59.613 
Epoch 64/1000 
	 loss: 407.2848, MinusLogProbMetric: 407.2848, val_loss: 406.2948, val_MinusLogProbMetric: 406.2948

Epoch 64: val_loss did not improve from 327.26697
196/196 - 82s - loss: 407.2848 - MinusLogProbMetric: 407.2848 - val_loss: 406.2948 - val_MinusLogProbMetric: 406.2948 - lr: 1.2346e-05 - 82s/epoch - 421ms/step
Epoch 65/1000
2023-10-26 07:13:21.685 
Epoch 65/1000 
	 loss: 413.9041, MinusLogProbMetric: 413.9041, val_loss: 402.5872, val_MinusLogProbMetric: 402.5872

Epoch 65: val_loss did not improve from 327.26697
196/196 - 82s - loss: 413.9041 - MinusLogProbMetric: 413.9041 - val_loss: 402.5872 - val_MinusLogProbMetric: 402.5872 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 66/1000
2023-10-26 07:14:43.092 
Epoch 66/1000 
	 loss: 389.9277, MinusLogProbMetric: 389.9277, val_loss: 385.1862, val_MinusLogProbMetric: 385.1862

Epoch 66: val_loss did not improve from 327.26697
196/196 - 81s - loss: 389.9277 - MinusLogProbMetric: 389.9277 - val_loss: 385.1862 - val_MinusLogProbMetric: 385.1862 - lr: 1.2346e-05 - 81s/epoch - 415ms/step
Epoch 67/1000
2023-10-26 07:16:04.874 
Epoch 67/1000 
	 loss: 378.9219, MinusLogProbMetric: 378.9219, val_loss: 373.9440, val_MinusLogProbMetric: 373.9440

Epoch 67: val_loss did not improve from 327.26697
196/196 - 82s - loss: 378.9219 - MinusLogProbMetric: 378.9219 - val_loss: 373.9440 - val_MinusLogProbMetric: 373.9440 - lr: 1.2346e-05 - 82s/epoch - 417ms/step
Epoch 68/1000
2023-10-26 07:17:27.064 
Epoch 68/1000 
	 loss: 368.4621, MinusLogProbMetric: 368.4621, val_loss: 363.9834, val_MinusLogProbMetric: 363.9834

Epoch 68: val_loss did not improve from 327.26697
196/196 - 82s - loss: 368.4621 - MinusLogProbMetric: 368.4621 - val_loss: 363.9834 - val_MinusLogProbMetric: 363.9834 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 69/1000
2023-10-26 07:18:49.037 
Epoch 69/1000 
	 loss: 360.5687, MinusLogProbMetric: 360.5687, val_loss: 357.5047, val_MinusLogProbMetric: 357.5047

Epoch 69: val_loss did not improve from 327.26697
196/196 - 82s - loss: 360.5687 - MinusLogProbMetric: 360.5687 - val_loss: 357.5047 - val_MinusLogProbMetric: 357.5047 - lr: 1.2346e-05 - 82s/epoch - 418ms/step
Epoch 70/1000
2023-10-26 07:20:12.009 
Epoch 70/1000 
	 loss: 356.8036, MinusLogProbMetric: 356.8036, val_loss: 354.4742, val_MinusLogProbMetric: 354.4742

Epoch 70: val_loss did not improve from 327.26697
196/196 - 83s - loss: 356.8036 - MinusLogProbMetric: 356.8036 - val_loss: 354.4742 - val_MinusLogProbMetric: 354.4742 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 71/1000
2023-10-26 07:21:34.130 
Epoch 71/1000 
	 loss: 406.4096, MinusLogProbMetric: 406.4096, val_loss: 367.3058, val_MinusLogProbMetric: 367.3058

Epoch 71: val_loss did not improve from 327.26697
196/196 - 82s - loss: 406.4096 - MinusLogProbMetric: 406.4096 - val_loss: 367.3058 - val_MinusLogProbMetric: 367.3058 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 72/1000
2023-10-26 07:22:56.399 
Epoch 72/1000 
	 loss: 357.8064, MinusLogProbMetric: 357.8064, val_loss: 351.5657, val_MinusLogProbMetric: 351.5657

Epoch 72: val_loss did not improve from 327.26697
196/196 - 82s - loss: 357.8064 - MinusLogProbMetric: 357.8064 - val_loss: 351.5657 - val_MinusLogProbMetric: 351.5657 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 73/1000
2023-10-26 07:24:18.669 
Epoch 73/1000 
	 loss: 349.1741, MinusLogProbMetric: 349.1741, val_loss: 345.5274, val_MinusLogProbMetric: 345.5274

Epoch 73: val_loss did not improve from 327.26697
196/196 - 82s - loss: 349.1741 - MinusLogProbMetric: 349.1741 - val_loss: 345.5274 - val_MinusLogProbMetric: 345.5274 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 74/1000
2023-10-26 07:25:40.724 
Epoch 74/1000 
	 loss: 347.7222, MinusLogProbMetric: 347.7222, val_loss: 350.0453, val_MinusLogProbMetric: 350.0453

Epoch 74: val_loss did not improve from 327.26697
196/196 - 82s - loss: 347.7222 - MinusLogProbMetric: 347.7222 - val_loss: 350.0453 - val_MinusLogProbMetric: 350.0453 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 75/1000
2023-10-26 07:27:02.992 
Epoch 75/1000 
	 loss: 342.7190, MinusLogProbMetric: 342.7190, val_loss: 337.2240, val_MinusLogProbMetric: 337.2240

Epoch 75: val_loss did not improve from 327.26697
196/196 - 82s - loss: 342.7190 - MinusLogProbMetric: 342.7190 - val_loss: 337.2240 - val_MinusLogProbMetric: 337.2240 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 76/1000
2023-10-26 07:28:25.004 
Epoch 76/1000 
	 loss: 336.8940, MinusLogProbMetric: 336.8940, val_loss: 334.5051, val_MinusLogProbMetric: 334.5051

Epoch 76: val_loss did not improve from 327.26697
196/196 - 82s - loss: 336.8940 - MinusLogProbMetric: 336.8940 - val_loss: 334.5051 - val_MinusLogProbMetric: 334.5051 - lr: 1.2346e-05 - 82s/epoch - 418ms/step
Epoch 77/1000
2023-10-26 07:29:46.723 
Epoch 77/1000 
	 loss: 332.9513, MinusLogProbMetric: 332.9513, val_loss: 341.0590, val_MinusLogProbMetric: 341.0590

Epoch 77: val_loss did not improve from 327.26697
196/196 - 82s - loss: 332.9513 - MinusLogProbMetric: 332.9513 - val_loss: 341.0590 - val_MinusLogProbMetric: 341.0590 - lr: 1.2346e-05 - 82s/epoch - 417ms/step
Epoch 78/1000
2023-10-26 07:31:05.218 
Epoch 78/1000 
	 loss: 329.3679, MinusLogProbMetric: 329.3679, val_loss: 325.8543, val_MinusLogProbMetric: 325.8543

Epoch 78: val_loss improved from 327.26697 to 325.85431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 329.3679 - MinusLogProbMetric: 329.3679 - val_loss: 325.8543 - val_MinusLogProbMetric: 325.8543 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 79/1000
2023-10-26 07:32:25.911 
Epoch 79/1000 
	 loss: 327.2988, MinusLogProbMetric: 327.2988, val_loss: 330.3054, val_MinusLogProbMetric: 330.3054

Epoch 79: val_loss did not improve from 325.85431
196/196 - 79s - loss: 327.2988 - MinusLogProbMetric: 327.2988 - val_loss: 330.3054 - val_MinusLogProbMetric: 330.3054 - lr: 1.2346e-05 - 79s/epoch - 405ms/step
Epoch 80/1000
2023-10-26 07:33:48.298 
Epoch 80/1000 
	 loss: 325.2849, MinusLogProbMetric: 325.2849, val_loss: 322.2318, val_MinusLogProbMetric: 322.2318

Epoch 80: val_loss improved from 325.85431 to 322.23184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 325.2849 - MinusLogProbMetric: 325.2849 - val_loss: 322.2318 - val_MinusLogProbMetric: 322.2318 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 81/1000
2023-10-26 07:35:12.114 
Epoch 81/1000 
	 loss: 321.7669, MinusLogProbMetric: 321.7669, val_loss: 325.9429, val_MinusLogProbMetric: 325.9429

Epoch 81: val_loss did not improve from 322.23184
196/196 - 82s - loss: 321.7669 - MinusLogProbMetric: 321.7669 - val_loss: 325.9429 - val_MinusLogProbMetric: 325.9429 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 82/1000
2023-10-26 07:36:34.173 
Epoch 82/1000 
	 loss: 321.8233, MinusLogProbMetric: 321.8233, val_loss: 322.6986, val_MinusLogProbMetric: 322.6986

Epoch 82: val_loss did not improve from 322.23184
196/196 - 82s - loss: 321.8233 - MinusLogProbMetric: 321.8233 - val_loss: 322.6986 - val_MinusLogProbMetric: 322.6986 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 83/1000
2023-10-26 07:37:56.795 
Epoch 83/1000 
	 loss: 316.9250, MinusLogProbMetric: 316.9250, val_loss: 314.3101, val_MinusLogProbMetric: 314.3101

Epoch 83: val_loss improved from 322.23184 to 314.31006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 316.9250 - MinusLogProbMetric: 316.9250 - val_loss: 314.3101 - val_MinusLogProbMetric: 314.3101 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 84/1000
2023-10-26 07:39:19.940 
Epoch 84/1000 
	 loss: 315.9728, MinusLogProbMetric: 315.9728, val_loss: 311.9827, val_MinusLogProbMetric: 311.9827

Epoch 84: val_loss improved from 314.31006 to 311.98267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 315.9728 - MinusLogProbMetric: 315.9728 - val_loss: 311.9827 - val_MinusLogProbMetric: 311.9827 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 85/1000
2023-10-26 07:40:43.352 
Epoch 85/1000 
	 loss: 311.2413, MinusLogProbMetric: 311.2413, val_loss: 310.2999, val_MinusLogProbMetric: 310.2999

Epoch 85: val_loss improved from 311.98267 to 310.29987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 311.2413 - MinusLogProbMetric: 311.2413 - val_loss: 310.2999 - val_MinusLogProbMetric: 310.2999 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 86/1000
2023-10-26 07:42:06.947 
Epoch 86/1000 
	 loss: 311.1354, MinusLogProbMetric: 311.1354, val_loss: 309.9104, val_MinusLogProbMetric: 309.9104

Epoch 86: val_loss improved from 310.29987 to 309.91040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 311.1354 - MinusLogProbMetric: 311.1354 - val_loss: 309.9104 - val_MinusLogProbMetric: 309.9104 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 87/1000
2023-10-26 07:43:30.808 
Epoch 87/1000 
	 loss: 307.2471, MinusLogProbMetric: 307.2471, val_loss: 310.3536, val_MinusLogProbMetric: 310.3536

Epoch 87: val_loss did not improve from 309.91040
196/196 - 83s - loss: 307.2471 - MinusLogProbMetric: 307.2471 - val_loss: 310.3536 - val_MinusLogProbMetric: 310.3536 - lr: 1.2346e-05 - 83s/epoch - 421ms/step
Epoch 88/1000
2023-10-26 07:44:53.534 
Epoch 88/1000 
	 loss: 305.8638, MinusLogProbMetric: 305.8638, val_loss: 304.7681, val_MinusLogProbMetric: 304.7681

Epoch 88: val_loss improved from 309.91040 to 304.76813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 305.8638 - MinusLogProbMetric: 305.8638 - val_loss: 304.7681 - val_MinusLogProbMetric: 304.7681 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 89/1000
2023-10-26 07:46:18.344 
Epoch 89/1000 
	 loss: 303.7765, MinusLogProbMetric: 303.7765, val_loss: 302.1034, val_MinusLogProbMetric: 302.1034

Epoch 89: val_loss improved from 304.76813 to 302.10342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 85s - loss: 303.7765 - MinusLogProbMetric: 303.7765 - val_loss: 302.1034 - val_MinusLogProbMetric: 302.1034 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 90/1000
2023-10-26 07:47:42.276 
Epoch 90/1000 
	 loss: 306.5851, MinusLogProbMetric: 306.5851, val_loss: 302.2314, val_MinusLogProbMetric: 302.2314

Epoch 90: val_loss did not improve from 302.10342
196/196 - 83s - loss: 306.5851 - MinusLogProbMetric: 306.5851 - val_loss: 302.2314 - val_MinusLogProbMetric: 302.2314 - lr: 1.2346e-05 - 83s/epoch - 421ms/step
Epoch 91/1000
2023-10-26 07:49:05.292 
Epoch 91/1000 
	 loss: 301.0302, MinusLogProbMetric: 301.0302, val_loss: 298.0955, val_MinusLogProbMetric: 298.0955

Epoch 91: val_loss improved from 302.10342 to 298.09552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 84s - loss: 301.0302 - MinusLogProbMetric: 301.0302 - val_loss: 298.0955 - val_MinusLogProbMetric: 298.0955 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 92/1000
2023-10-26 07:50:28.296 
Epoch 92/1000 
	 loss: 320.3067, MinusLogProbMetric: 320.3067, val_loss: 303.3674, val_MinusLogProbMetric: 303.3674

Epoch 92: val_loss did not improve from 298.09552
196/196 - 82s - loss: 320.3067 - MinusLogProbMetric: 320.3067 - val_loss: 303.3674 - val_MinusLogProbMetric: 303.3674 - lr: 1.2346e-05 - 82s/epoch - 417ms/step
Epoch 93/1000
2023-10-26 07:51:44.582 
Epoch 93/1000 
	 loss: 519.4424, MinusLogProbMetric: 519.4424, val_loss: 607.8625, val_MinusLogProbMetric: 607.8625

Epoch 93: val_loss did not improve from 298.09552
196/196 - 76s - loss: 519.4424 - MinusLogProbMetric: 519.4424 - val_loss: 607.8625 - val_MinusLogProbMetric: 607.8625 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 94/1000
2023-10-26 07:53:05.271 
Epoch 94/1000 
	 loss: 784.4168, MinusLogProbMetric: 784.4168, val_loss: 773.8858, val_MinusLogProbMetric: 773.8858

Epoch 94: val_loss did not improve from 298.09552
196/196 - 81s - loss: 784.4168 - MinusLogProbMetric: 784.4168 - val_loss: 773.8858 - val_MinusLogProbMetric: 773.8858 - lr: 1.2346e-05 - 81s/epoch - 412ms/step
Epoch 95/1000
2023-10-26 07:54:26.923 
Epoch 95/1000 
	 loss: 641.3065, MinusLogProbMetric: 641.3065, val_loss: 565.4890, val_MinusLogProbMetric: 565.4890

Epoch 95: val_loss did not improve from 298.09552
196/196 - 82s - loss: 641.3065 - MinusLogProbMetric: 641.3065 - val_loss: 565.4890 - val_MinusLogProbMetric: 565.4890 - lr: 1.2346e-05 - 82s/epoch - 417ms/step
Epoch 96/1000
2023-10-26 07:55:48.765 
Epoch 96/1000 
	 loss: 542.3209, MinusLogProbMetric: 542.3209, val_loss: 510.3471, val_MinusLogProbMetric: 510.3471

Epoch 96: val_loss did not improve from 298.09552
196/196 - 82s - loss: 542.3209 - MinusLogProbMetric: 542.3209 - val_loss: 510.3471 - val_MinusLogProbMetric: 510.3471 - lr: 1.2346e-05 - 82s/epoch - 418ms/step
Epoch 97/1000
2023-10-26 07:57:11.406 
Epoch 97/1000 
	 loss: 492.5150, MinusLogProbMetric: 492.5150, val_loss: 477.1315, val_MinusLogProbMetric: 477.1315

Epoch 97: val_loss did not improve from 298.09552
196/196 - 83s - loss: 492.5150 - MinusLogProbMetric: 492.5150 - val_loss: 477.1315 - val_MinusLogProbMetric: 477.1315 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 98/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 84: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 07:57:50.300 
Epoch 98/1000 
	 loss: nan, MinusLogProbMetric: 473.6122, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 98: val_loss did not improve from 298.09552
196/196 - 39s - loss: nan - MinusLogProbMetric: 473.6122 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 39s/epoch - 198ms/step
The loss history contains NaN values.
Training failed: trying again with seed 821433 and lr 4.115226337448558e-06.
===========
Generating train data for run 376.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_376/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_376
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_204"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_205 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f0d4af1e6b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4a8e6b90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4a8e6b90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1460e74df0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b7d74bd90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b7d700340>, <keras.callbacks.ModelCheckpoint object at 0x7f0b7d700400>, <keras.callbacks.EarlyStopping object at 0x7f0b7d700670>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b7d7006a0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b7d7002e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 376/720 with hyperparameters:
timestamp = 2023-10-26 07:57:59.908225
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 08:01:58.726 
Epoch 1/1000 
	 loss: 306.8955, MinusLogProbMetric: 306.8955, val_loss: 291.7420, val_MinusLogProbMetric: 291.7420

Epoch 1: val_loss improved from inf to 291.74203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 240s - loss: 306.8955 - MinusLogProbMetric: 306.8955 - val_loss: 291.7420 - val_MinusLogProbMetric: 291.7420 - lr: 4.1152e-06 - 240s/epoch - 1s/step
Epoch 2/1000
2023-10-26 08:03:23.041 
Epoch 2/1000 
	 loss: 347.2110, MinusLogProbMetric: 347.2110, val_loss: 414.4089, val_MinusLogProbMetric: 414.4089

Epoch 2: val_loss did not improve from 291.74203
196/196 - 82s - loss: 347.2110 - MinusLogProbMetric: 347.2110 - val_loss: 414.4089 - val_MinusLogProbMetric: 414.4089 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 3/1000
2023-10-26 08:04:45.079 
Epoch 3/1000 
	 loss: 574.9985, MinusLogProbMetric: 574.9985, val_loss: 634.7379, val_MinusLogProbMetric: 634.7379

Epoch 3: val_loss did not improve from 291.74203
196/196 - 82s - loss: 574.9985 - MinusLogProbMetric: 574.9985 - val_loss: 634.7379 - val_MinusLogProbMetric: 634.7379 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 4/1000
2023-10-26 08:06:07.095 
Epoch 4/1000 
	 loss: 544.6341, MinusLogProbMetric: 544.6341, val_loss: 512.1378, val_MinusLogProbMetric: 512.1378

Epoch 4: val_loss did not improve from 291.74203
196/196 - 82s - loss: 544.6341 - MinusLogProbMetric: 544.6341 - val_loss: 512.1378 - val_MinusLogProbMetric: 512.1378 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 5/1000
2023-10-26 08:07:29.379 
Epoch 5/1000 
	 loss: 486.3659, MinusLogProbMetric: 486.3659, val_loss: 479.6637, val_MinusLogProbMetric: 479.6637

Epoch 5: val_loss did not improve from 291.74203
196/196 - 82s - loss: 486.3659 - MinusLogProbMetric: 486.3659 - val_loss: 479.6637 - val_MinusLogProbMetric: 479.6637 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 6/1000
2023-10-26 08:08:52.168 
Epoch 6/1000 
	 loss: 421.4355, MinusLogProbMetric: 421.4355, val_loss: 396.6380, val_MinusLogProbMetric: 396.6380

Epoch 6: val_loss did not improve from 291.74203
196/196 - 83s - loss: 421.4355 - MinusLogProbMetric: 421.4355 - val_loss: 396.6380 - val_MinusLogProbMetric: 396.6380 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 7/1000
2023-10-26 08:10:11.479 
Epoch 7/1000 
	 loss: 411.4847, MinusLogProbMetric: 411.4847, val_loss: 442.2205, val_MinusLogProbMetric: 442.2205

Epoch 7: val_loss did not improve from 291.74203
196/196 - 79s - loss: 411.4847 - MinusLogProbMetric: 411.4847 - val_loss: 442.2205 - val_MinusLogProbMetric: 442.2205 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 8/1000
2023-10-26 08:11:19.018 
Epoch 8/1000 
	 loss: 389.0976, MinusLogProbMetric: 389.0976, val_loss: 365.0875, val_MinusLogProbMetric: 365.0875

Epoch 8: val_loss did not improve from 291.74203
196/196 - 68s - loss: 389.0976 - MinusLogProbMetric: 389.0976 - val_loss: 365.0875 - val_MinusLogProbMetric: 365.0875 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 9/1000
2023-10-26 08:12:26.838 
Epoch 9/1000 
	 loss: 352.1252, MinusLogProbMetric: 352.1252, val_loss: 341.8432, val_MinusLogProbMetric: 341.8432

Epoch 9: val_loss did not improve from 291.74203
196/196 - 68s - loss: 352.1252 - MinusLogProbMetric: 352.1252 - val_loss: 341.8432 - val_MinusLogProbMetric: 341.8432 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 10/1000
2023-10-26 08:13:38.336 
Epoch 10/1000 
	 loss: 336.4390, MinusLogProbMetric: 336.4390, val_loss: 331.6882, val_MinusLogProbMetric: 331.6882

Epoch 10: val_loss did not improve from 291.74203
196/196 - 71s - loss: 336.4390 - MinusLogProbMetric: 336.4390 - val_loss: 331.6882 - val_MinusLogProbMetric: 331.6882 - lr: 4.1152e-06 - 71s/epoch - 365ms/step
Epoch 11/1000
2023-10-26 08:14:52.712 
Epoch 11/1000 
	 loss: 327.2543, MinusLogProbMetric: 327.2543, val_loss: 321.6309, val_MinusLogProbMetric: 321.6309

Epoch 11: val_loss did not improve from 291.74203
196/196 - 74s - loss: 327.2543 - MinusLogProbMetric: 327.2543 - val_loss: 321.6309 - val_MinusLogProbMetric: 321.6309 - lr: 4.1152e-06 - 74s/epoch - 379ms/step
Epoch 12/1000
2023-10-26 08:16:04.476 
Epoch 12/1000 
	 loss: 320.3327, MinusLogProbMetric: 320.3327, val_loss: 330.4865, val_MinusLogProbMetric: 330.4865

Epoch 12: val_loss did not improve from 291.74203
196/196 - 72s - loss: 320.3327 - MinusLogProbMetric: 320.3327 - val_loss: 330.4865 - val_MinusLogProbMetric: 330.4865 - lr: 4.1152e-06 - 72s/epoch - 366ms/step
Epoch 13/1000
2023-10-26 08:17:12.086 
Epoch 13/1000 
	 loss: 314.4163, MinusLogProbMetric: 314.4163, val_loss: 311.3282, val_MinusLogProbMetric: 311.3282

Epoch 13: val_loss did not improve from 291.74203
196/196 - 68s - loss: 314.4163 - MinusLogProbMetric: 314.4163 - val_loss: 311.3282 - val_MinusLogProbMetric: 311.3282 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 14/1000
2023-10-26 08:18:25.663 
Epoch 14/1000 
	 loss: 307.7960, MinusLogProbMetric: 307.7960, val_loss: 304.1440, val_MinusLogProbMetric: 304.1440

Epoch 14: val_loss did not improve from 291.74203
196/196 - 74s - loss: 307.7960 - MinusLogProbMetric: 307.7960 - val_loss: 304.1440 - val_MinusLogProbMetric: 304.1440 - lr: 4.1152e-06 - 74s/epoch - 375ms/step
Epoch 15/1000
2023-10-26 08:19:40.104 
Epoch 15/1000 
	 loss: 310.4554, MinusLogProbMetric: 310.4554, val_loss: 327.0725, val_MinusLogProbMetric: 327.0725

Epoch 15: val_loss did not improve from 291.74203
196/196 - 74s - loss: 310.4554 - MinusLogProbMetric: 310.4554 - val_loss: 327.0725 - val_MinusLogProbMetric: 327.0725 - lr: 4.1152e-06 - 74s/epoch - 380ms/step
Epoch 16/1000
2023-10-26 08:20:50.965 
Epoch 16/1000 
	 loss: 327.3168, MinusLogProbMetric: 327.3168, val_loss: 389.3578, val_MinusLogProbMetric: 389.3578

Epoch 16: val_loss did not improve from 291.74203
196/196 - 71s - loss: 327.3168 - MinusLogProbMetric: 327.3168 - val_loss: 389.3578 - val_MinusLogProbMetric: 389.3578 - lr: 4.1152e-06 - 71s/epoch - 362ms/step
Epoch 17/1000
2023-10-26 08:21:58.600 
Epoch 17/1000 
	 loss: 317.7855, MinusLogProbMetric: 317.7855, val_loss: 317.3784, val_MinusLogProbMetric: 317.3784

Epoch 17: val_loss did not improve from 291.74203
196/196 - 68s - loss: 317.7855 - MinusLogProbMetric: 317.7855 - val_loss: 317.3784 - val_MinusLogProbMetric: 317.3784 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 18/1000
2023-10-26 08:23:10.968 
Epoch 18/1000 
	 loss: 386.8653, MinusLogProbMetric: 386.8653, val_loss: 355.9261, val_MinusLogProbMetric: 355.9261

Epoch 18: val_loss did not improve from 291.74203
196/196 - 72s - loss: 386.8653 - MinusLogProbMetric: 386.8653 - val_loss: 355.9261 - val_MinusLogProbMetric: 355.9261 - lr: 4.1152e-06 - 72s/epoch - 369ms/step
Epoch 19/1000
2023-10-26 08:24:22.204 
Epoch 19/1000 
	 loss: 337.6608, MinusLogProbMetric: 337.6608, val_loss: 311.4207, val_MinusLogProbMetric: 311.4207

Epoch 19: val_loss did not improve from 291.74203
196/196 - 71s - loss: 337.6608 - MinusLogProbMetric: 337.6608 - val_loss: 311.4207 - val_MinusLogProbMetric: 311.4207 - lr: 4.1152e-06 - 71s/epoch - 363ms/step
Epoch 20/1000
2023-10-26 08:25:32.711 
Epoch 20/1000 
	 loss: 305.5625, MinusLogProbMetric: 305.5625, val_loss: 301.7363, val_MinusLogProbMetric: 301.7363

Epoch 20: val_loss did not improve from 291.74203
196/196 - 71s - loss: 305.5625 - MinusLogProbMetric: 305.5625 - val_loss: 301.7363 - val_MinusLogProbMetric: 301.7363 - lr: 4.1152e-06 - 71s/epoch - 360ms/step
Epoch 21/1000
2023-10-26 08:26:45.006 
Epoch 21/1000 
	 loss: 296.9234, MinusLogProbMetric: 296.9234, val_loss: 293.7689, val_MinusLogProbMetric: 293.7689

Epoch 21: val_loss did not improve from 291.74203
196/196 - 72s - loss: 296.9234 - MinusLogProbMetric: 296.9234 - val_loss: 293.7689 - val_MinusLogProbMetric: 293.7689 - lr: 4.1152e-06 - 72s/epoch - 369ms/step
Epoch 22/1000
2023-10-26 08:27:57.748 
Epoch 22/1000 
	 loss: 294.1600, MinusLogProbMetric: 294.1600, val_loss: 316.8869, val_MinusLogProbMetric: 316.8869

Epoch 22: val_loss did not improve from 291.74203
196/196 - 73s - loss: 294.1600 - MinusLogProbMetric: 294.1600 - val_loss: 316.8869 - val_MinusLogProbMetric: 316.8869 - lr: 4.1152e-06 - 73s/epoch - 371ms/step
Epoch 23/1000
2023-10-26 08:29:10.438 
Epoch 23/1000 
	 loss: 310.0052, MinusLogProbMetric: 310.0052, val_loss: 303.2255, val_MinusLogProbMetric: 303.2255

Epoch 23: val_loss did not improve from 291.74203
196/196 - 73s - loss: 310.0052 - MinusLogProbMetric: 310.0052 - val_loss: 303.2255 - val_MinusLogProbMetric: 303.2255 - lr: 4.1152e-06 - 73s/epoch - 371ms/step
Epoch 24/1000
2023-10-26 08:30:30.019 
Epoch 24/1000 
	 loss: 298.2175, MinusLogProbMetric: 298.2175, val_loss: 293.9515, val_MinusLogProbMetric: 293.9515

Epoch 24: val_loss did not improve from 291.74203
196/196 - 80s - loss: 298.2175 - MinusLogProbMetric: 298.2175 - val_loss: 293.9515 - val_MinusLogProbMetric: 293.9515 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 25/1000
2023-10-26 08:31:50.600 
Epoch 25/1000 
	 loss: 289.6698, MinusLogProbMetric: 289.6698, val_loss: 286.4203, val_MinusLogProbMetric: 286.4203

Epoch 25: val_loss improved from 291.74203 to 286.42026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 289.6698 - MinusLogProbMetric: 289.6698 - val_loss: 286.4203 - val_MinusLogProbMetric: 286.4203 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 26/1000
2023-10-26 08:33:12.055 
Epoch 26/1000 
	 loss: 283.6026, MinusLogProbMetric: 283.6026, val_loss: 281.7478, val_MinusLogProbMetric: 281.7478

Epoch 26: val_loss improved from 286.42026 to 281.74783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 283.6026 - MinusLogProbMetric: 283.6026 - val_loss: 281.7478 - val_MinusLogProbMetric: 281.7478 - lr: 4.1152e-06 - 81s/epoch - 416ms/step
Epoch 27/1000
2023-10-26 08:34:33.569 
Epoch 27/1000 
	 loss: 575.8650, MinusLogProbMetric: 575.8650, val_loss: 534.5731, val_MinusLogProbMetric: 534.5731

Epoch 27: val_loss did not improve from 281.74783
196/196 - 80s - loss: 575.8650 - MinusLogProbMetric: 575.8650 - val_loss: 534.5731 - val_MinusLogProbMetric: 534.5731 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 28/1000
2023-10-26 08:35:54.036 
Epoch 28/1000 
	 loss: 467.3370, MinusLogProbMetric: 467.3370, val_loss: 440.4956, val_MinusLogProbMetric: 440.4956

Epoch 28: val_loss did not improve from 281.74783
196/196 - 80s - loss: 467.3370 - MinusLogProbMetric: 467.3370 - val_loss: 440.4956 - val_MinusLogProbMetric: 440.4956 - lr: 4.1152e-06 - 80s/epoch - 411ms/step
Epoch 29/1000
2023-10-26 08:37:14.342 
Epoch 29/1000 
	 loss: 402.6901, MinusLogProbMetric: 402.6901, val_loss: 382.7876, val_MinusLogProbMetric: 382.7876

Epoch 29: val_loss did not improve from 281.74783
196/196 - 80s - loss: 402.6901 - MinusLogProbMetric: 402.6901 - val_loss: 382.7876 - val_MinusLogProbMetric: 382.7876 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 30/1000
2023-10-26 08:38:34.330 
Epoch 30/1000 
	 loss: 377.4269, MinusLogProbMetric: 377.4269, val_loss: 369.5207, val_MinusLogProbMetric: 369.5207

Epoch 30: val_loss did not improve from 281.74783
196/196 - 80s - loss: 377.4269 - MinusLogProbMetric: 377.4269 - val_loss: 369.5207 - val_MinusLogProbMetric: 369.5207 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 31/1000
2023-10-26 08:39:55.056 
Epoch 31/1000 
	 loss: 361.0542, MinusLogProbMetric: 361.0542, val_loss: 353.1370, val_MinusLogProbMetric: 353.1370

Epoch 31: val_loss did not improve from 281.74783
196/196 - 81s - loss: 361.0542 - MinusLogProbMetric: 361.0542 - val_loss: 353.1370 - val_MinusLogProbMetric: 353.1370 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 32/1000
2023-10-26 08:41:14.180 
Epoch 32/1000 
	 loss: 348.1960, MinusLogProbMetric: 348.1960, val_loss: 341.1119, val_MinusLogProbMetric: 341.1119

Epoch 32: val_loss did not improve from 281.74783
196/196 - 79s - loss: 348.1960 - MinusLogProbMetric: 348.1960 - val_loss: 341.1119 - val_MinusLogProbMetric: 341.1119 - lr: 4.1152e-06 - 79s/epoch - 404ms/step
Epoch 33/1000
2023-10-26 08:42:33.613 
Epoch 33/1000 
	 loss: 337.3249, MinusLogProbMetric: 337.3249, val_loss: 334.3814, val_MinusLogProbMetric: 334.3814

Epoch 33: val_loss did not improve from 281.74783
196/196 - 79s - loss: 337.3249 - MinusLogProbMetric: 337.3249 - val_loss: 334.3814 - val_MinusLogProbMetric: 334.3814 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 34/1000
2023-10-26 08:43:54.824 
Epoch 34/1000 
	 loss: 331.0083, MinusLogProbMetric: 331.0083, val_loss: 328.6112, val_MinusLogProbMetric: 328.6112

Epoch 34: val_loss did not improve from 281.74783
196/196 - 81s - loss: 331.0083 - MinusLogProbMetric: 331.0083 - val_loss: 328.6112 - val_MinusLogProbMetric: 328.6112 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 35/1000
2023-10-26 08:45:14.086 
Epoch 35/1000 
	 loss: 326.3983, MinusLogProbMetric: 326.3983, val_loss: 323.6722, val_MinusLogProbMetric: 323.6722

Epoch 35: val_loss did not improve from 281.74783
196/196 - 79s - loss: 326.3983 - MinusLogProbMetric: 326.3983 - val_loss: 323.6722 - val_MinusLogProbMetric: 323.6722 - lr: 4.1152e-06 - 79s/epoch - 404ms/step
Epoch 36/1000
2023-10-26 08:46:34.731 
Epoch 36/1000 
	 loss: 320.4351, MinusLogProbMetric: 320.4351, val_loss: 317.8370, val_MinusLogProbMetric: 317.8370

Epoch 36: val_loss did not improve from 281.74783
196/196 - 81s - loss: 320.4351 - MinusLogProbMetric: 320.4351 - val_loss: 317.8370 - val_MinusLogProbMetric: 317.8370 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 37/1000
2023-10-26 08:47:55.255 
Epoch 37/1000 
	 loss: 315.9295, MinusLogProbMetric: 315.9295, val_loss: 313.8745, val_MinusLogProbMetric: 313.8745

Epoch 37: val_loss did not improve from 281.74783
196/196 - 81s - loss: 315.9295 - MinusLogProbMetric: 315.9295 - val_loss: 313.8745 - val_MinusLogProbMetric: 313.8745 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 38/1000
2023-10-26 08:49:16.089 
Epoch 38/1000 
	 loss: 314.0377, MinusLogProbMetric: 314.0377, val_loss: 312.7416, val_MinusLogProbMetric: 312.7416

Epoch 38: val_loss did not improve from 281.74783
196/196 - 81s - loss: 314.0377 - MinusLogProbMetric: 314.0377 - val_loss: 312.7416 - val_MinusLogProbMetric: 312.7416 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 39/1000
2023-10-26 08:50:36.320 
Epoch 39/1000 
	 loss: 352.7084, MinusLogProbMetric: 352.7084, val_loss: 334.2242, val_MinusLogProbMetric: 334.2242

Epoch 39: val_loss did not improve from 281.74783
196/196 - 80s - loss: 352.7084 - MinusLogProbMetric: 352.7084 - val_loss: 334.2242 - val_MinusLogProbMetric: 334.2242 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 40/1000
2023-10-26 08:51:56.525 
Epoch 40/1000 
	 loss: 324.1541, MinusLogProbMetric: 324.1541, val_loss: 316.7576, val_MinusLogProbMetric: 316.7576

Epoch 40: val_loss did not improve from 281.74783
196/196 - 80s - loss: 324.1541 - MinusLogProbMetric: 324.1541 - val_loss: 316.7576 - val_MinusLogProbMetric: 316.7576 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 41/1000
2023-10-26 08:53:17.072 
Epoch 41/1000 
	 loss: 312.4786, MinusLogProbMetric: 312.4786, val_loss: 310.6825, val_MinusLogProbMetric: 310.6825

Epoch 41: val_loss did not improve from 281.74783
196/196 - 81s - loss: 312.4786 - MinusLogProbMetric: 312.4786 - val_loss: 310.6825 - val_MinusLogProbMetric: 310.6825 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 42/1000
2023-10-26 08:54:38.451 
Epoch 42/1000 
	 loss: 308.2974, MinusLogProbMetric: 308.2974, val_loss: 305.9099, val_MinusLogProbMetric: 305.9099

Epoch 42: val_loss did not improve from 281.74783
196/196 - 81s - loss: 308.2974 - MinusLogProbMetric: 308.2974 - val_loss: 305.9099 - val_MinusLogProbMetric: 305.9099 - lr: 4.1152e-06 - 81s/epoch - 415ms/step
Epoch 43/1000
2023-10-26 08:55:58.968 
Epoch 43/1000 
	 loss: 302.2130, MinusLogProbMetric: 302.2130, val_loss: 300.8350, val_MinusLogProbMetric: 300.8350

Epoch 43: val_loss did not improve from 281.74783
196/196 - 81s - loss: 302.2130 - MinusLogProbMetric: 302.2130 - val_loss: 300.8350 - val_MinusLogProbMetric: 300.8350 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 44/1000
2023-10-26 08:57:19.207 
Epoch 44/1000 
	 loss: 297.9228, MinusLogProbMetric: 297.9228, val_loss: 295.7748, val_MinusLogProbMetric: 295.7748

Epoch 44: val_loss did not improve from 281.74783
196/196 - 80s - loss: 297.9228 - MinusLogProbMetric: 297.9228 - val_loss: 295.7748 - val_MinusLogProbMetric: 295.7748 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 45/1000
2023-10-26 08:58:38.660 
Epoch 45/1000 
	 loss: 301.8548, MinusLogProbMetric: 301.8548, val_loss: 292.6419, val_MinusLogProbMetric: 292.6419

Epoch 45: val_loss did not improve from 281.74783
196/196 - 79s - loss: 301.8548 - MinusLogProbMetric: 301.8548 - val_loss: 292.6419 - val_MinusLogProbMetric: 292.6419 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 46/1000
2023-10-26 08:59:58.624 
Epoch 46/1000 
	 loss: 291.2241, MinusLogProbMetric: 291.2241, val_loss: 289.1120, val_MinusLogProbMetric: 289.1120

Epoch 46: val_loss did not improve from 281.74783
196/196 - 80s - loss: 291.2241 - MinusLogProbMetric: 291.2241 - val_loss: 289.1120 - val_MinusLogProbMetric: 289.1120 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 47/1000
2023-10-26 09:01:19.513 
Epoch 47/1000 
	 loss: 287.7057, MinusLogProbMetric: 287.7057, val_loss: 285.3272, val_MinusLogProbMetric: 285.3272

Epoch 47: val_loss did not improve from 281.74783
196/196 - 81s - loss: 287.7057 - MinusLogProbMetric: 287.7057 - val_loss: 285.3272 - val_MinusLogProbMetric: 285.3272 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 48/1000
2023-10-26 09:02:38.463 
Epoch 48/1000 
	 loss: 284.4142, MinusLogProbMetric: 284.4142, val_loss: 282.9856, val_MinusLogProbMetric: 282.9856

Epoch 48: val_loss did not improve from 281.74783
196/196 - 79s - loss: 284.4142 - MinusLogProbMetric: 284.4142 - val_loss: 282.9856 - val_MinusLogProbMetric: 282.9856 - lr: 4.1152e-06 - 79s/epoch - 403ms/step
Epoch 49/1000
2023-10-26 09:03:58.584 
Epoch 49/1000 
	 loss: 282.4828, MinusLogProbMetric: 282.4828, val_loss: 282.5111, val_MinusLogProbMetric: 282.5111

Epoch 49: val_loss did not improve from 281.74783
196/196 - 80s - loss: 282.4828 - MinusLogProbMetric: 282.4828 - val_loss: 282.5111 - val_MinusLogProbMetric: 282.5111 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 50/1000
2023-10-26 09:05:18.471 
Epoch 50/1000 
	 loss: 283.3221, MinusLogProbMetric: 283.3221, val_loss: 279.0807, val_MinusLogProbMetric: 279.0807

Epoch 50: val_loss improved from 281.74783 to 279.08075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 283.3221 - MinusLogProbMetric: 283.3221 - val_loss: 279.0807 - val_MinusLogProbMetric: 279.0807 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 51/1000
2023-10-26 09:06:40.530 
Epoch 51/1000 
	 loss: 276.5050, MinusLogProbMetric: 276.5050, val_loss: 275.3567, val_MinusLogProbMetric: 275.3567

Epoch 51: val_loss improved from 279.08075 to 275.35669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 276.5050 - MinusLogProbMetric: 276.5050 - val_loss: 275.3567 - val_MinusLogProbMetric: 275.3567 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 52/1000
2023-10-26 09:08:02.300 
Epoch 52/1000 
	 loss: 273.9293, MinusLogProbMetric: 273.9293, val_loss: 273.3982, val_MinusLogProbMetric: 273.3982

Epoch 52: val_loss improved from 275.35669 to 273.39816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 273.9293 - MinusLogProbMetric: 273.9293 - val_loss: 273.3982 - val_MinusLogProbMetric: 273.3982 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 53/1000
2023-10-26 09:09:23.094 
Epoch 53/1000 
	 loss: 272.0174, MinusLogProbMetric: 272.0174, val_loss: 271.8132, val_MinusLogProbMetric: 271.8132

Epoch 53: val_loss improved from 273.39816 to 271.81317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 272.0174 - MinusLogProbMetric: 272.0174 - val_loss: 271.8132 - val_MinusLogProbMetric: 271.8132 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 54/1000
2023-10-26 09:10:44.625 
Epoch 54/1000 
	 loss: 270.9189, MinusLogProbMetric: 270.9189, val_loss: 270.5175, val_MinusLogProbMetric: 270.5175

Epoch 54: val_loss improved from 271.81317 to 270.51755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 270.9189 - MinusLogProbMetric: 270.9189 - val_loss: 270.5175 - val_MinusLogProbMetric: 270.5175 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 55/1000
2023-10-26 09:12:06.618 
Epoch 55/1000 
	 loss: 269.5165, MinusLogProbMetric: 269.5165, val_loss: 268.5800, val_MinusLogProbMetric: 268.5800

Epoch 55: val_loss improved from 270.51755 to 268.57996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 269.5165 - MinusLogProbMetric: 269.5165 - val_loss: 268.5800 - val_MinusLogProbMetric: 268.5800 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 56/1000
2023-10-26 09:13:28.173 
Epoch 56/1000 
	 loss: 311.4750, MinusLogProbMetric: 311.4750, val_loss: 331.1611, val_MinusLogProbMetric: 331.1611

Epoch 56: val_loss did not improve from 268.57996
196/196 - 80s - loss: 311.4750 - MinusLogProbMetric: 311.4750 - val_loss: 331.1611 - val_MinusLogProbMetric: 331.1611 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 57/1000
2023-10-26 09:14:48.575 
Epoch 57/1000 
	 loss: 283.1234, MinusLogProbMetric: 283.1234, val_loss: 273.5078, val_MinusLogProbMetric: 273.5078

Epoch 57: val_loss did not improve from 268.57996
196/196 - 80s - loss: 283.1234 - MinusLogProbMetric: 283.1234 - val_loss: 273.5078 - val_MinusLogProbMetric: 273.5078 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 58/1000
2023-10-26 09:16:09.850 
Epoch 58/1000 
	 loss: 270.5932, MinusLogProbMetric: 270.5932, val_loss: 269.3027, val_MinusLogProbMetric: 269.3027

Epoch 58: val_loss did not improve from 268.57996
196/196 - 81s - loss: 270.5932 - MinusLogProbMetric: 270.5932 - val_loss: 269.3027 - val_MinusLogProbMetric: 269.3027 - lr: 4.1152e-06 - 81s/epoch - 415ms/step
Epoch 59/1000
2023-10-26 09:17:30.430 
Epoch 59/1000 
	 loss: 267.9193, MinusLogProbMetric: 267.9193, val_loss: 266.1094, val_MinusLogProbMetric: 266.1094

Epoch 59: val_loss improved from 268.57996 to 266.10938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 267.9193 - MinusLogProbMetric: 267.9193 - val_loss: 266.1094 - val_MinusLogProbMetric: 266.1094 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 60/1000
2023-10-26 09:18:51.099 
Epoch 60/1000 
	 loss: 265.2916, MinusLogProbMetric: 265.2916, val_loss: 263.6267, val_MinusLogProbMetric: 263.6267

Epoch 60: val_loss improved from 266.10938 to 263.62671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 265.2916 - MinusLogProbMetric: 265.2916 - val_loss: 263.6267 - val_MinusLogProbMetric: 263.6267 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 61/1000
2023-10-26 09:20:11.961 
Epoch 61/1000 
	 loss: 262.4856, MinusLogProbMetric: 262.4856, val_loss: 263.1547, val_MinusLogProbMetric: 263.1547

Epoch 61: val_loss improved from 263.62671 to 263.15466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 262.4856 - MinusLogProbMetric: 262.4856 - val_loss: 263.1547 - val_MinusLogProbMetric: 263.1547 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 62/1000
2023-10-26 09:21:33.603 
Epoch 62/1000 
	 loss: 260.7061, MinusLogProbMetric: 260.7061, val_loss: 260.0703, val_MinusLogProbMetric: 260.0703

Epoch 62: val_loss improved from 263.15466 to 260.07034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 260.7061 - MinusLogProbMetric: 260.7061 - val_loss: 260.0703 - val_MinusLogProbMetric: 260.0703 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 63/1000
2023-10-26 09:22:55.890 
Epoch 63/1000 
	 loss: 259.2480, MinusLogProbMetric: 259.2480, val_loss: 259.3640, val_MinusLogProbMetric: 259.3640

Epoch 63: val_loss improved from 260.07034 to 259.36395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 259.2480 - MinusLogProbMetric: 259.2480 - val_loss: 259.3640 - val_MinusLogProbMetric: 259.3640 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 64/1000
2023-10-26 09:24:18.436 
Epoch 64/1000 
	 loss: 258.4185, MinusLogProbMetric: 258.4185, val_loss: 257.5442, val_MinusLogProbMetric: 257.5442

Epoch 64: val_loss improved from 259.36395 to 257.54422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 258.4185 - MinusLogProbMetric: 258.4185 - val_loss: 257.5442 - val_MinusLogProbMetric: 257.5442 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 65/1000
2023-10-26 09:25:41.034 
Epoch 65/1000 
	 loss: 257.2450, MinusLogProbMetric: 257.2450, val_loss: 258.9342, val_MinusLogProbMetric: 258.9342

Epoch 65: val_loss did not improve from 257.54422
196/196 - 81s - loss: 257.2450 - MinusLogProbMetric: 257.2450 - val_loss: 258.9342 - val_MinusLogProbMetric: 258.9342 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 66/1000
2023-10-26 09:27:00.834 
Epoch 66/1000 
	 loss: 257.1766, MinusLogProbMetric: 257.1766, val_loss: 258.5040, val_MinusLogProbMetric: 258.5040

Epoch 66: val_loss did not improve from 257.54422
196/196 - 80s - loss: 257.1766 - MinusLogProbMetric: 257.1766 - val_loss: 258.5040 - val_MinusLogProbMetric: 258.5040 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 67/1000
2023-10-26 09:28:21.699 
Epoch 67/1000 
	 loss: 256.7885, MinusLogProbMetric: 256.7885, val_loss: 255.4996, val_MinusLogProbMetric: 255.4996

Epoch 67: val_loss improved from 257.54422 to 255.49962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 256.7885 - MinusLogProbMetric: 256.7885 - val_loss: 255.4996 - val_MinusLogProbMetric: 255.4996 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 68/1000
2023-10-26 09:29:43.949 
Epoch 68/1000 
	 loss: 254.3124, MinusLogProbMetric: 254.3124, val_loss: 254.0469, val_MinusLogProbMetric: 254.0469

Epoch 68: val_loss improved from 255.49962 to 254.04686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 254.3124 - MinusLogProbMetric: 254.3124 - val_loss: 254.0469 - val_MinusLogProbMetric: 254.0469 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 69/1000
2023-10-26 09:31:05.064 
Epoch 69/1000 
	 loss: 252.4503, MinusLogProbMetric: 252.4503, val_loss: 251.8526, val_MinusLogProbMetric: 251.8526

Epoch 69: val_loss improved from 254.04686 to 251.85258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 252.4503 - MinusLogProbMetric: 252.4503 - val_loss: 251.8526 - val_MinusLogProbMetric: 251.8526 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 70/1000
2023-10-26 09:32:26.459 
Epoch 70/1000 
	 loss: 253.3419, MinusLogProbMetric: 253.3419, val_loss: 251.1756, val_MinusLogProbMetric: 251.1756

Epoch 70: val_loss improved from 251.85258 to 251.17560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 253.3419 - MinusLogProbMetric: 253.3419 - val_loss: 251.1756 - val_MinusLogProbMetric: 251.1756 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 71/1000
2023-10-26 09:33:48.993 
Epoch 71/1000 
	 loss: 264.8562, MinusLogProbMetric: 264.8562, val_loss: 256.7111, val_MinusLogProbMetric: 256.7111

Epoch 71: val_loss did not improve from 251.17560
196/196 - 81s - loss: 264.8562 - MinusLogProbMetric: 264.8562 - val_loss: 256.7111 - val_MinusLogProbMetric: 256.7111 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 72/1000
2023-10-26 09:35:09.410 
Epoch 72/1000 
	 loss: 280.6417, MinusLogProbMetric: 280.6417, val_loss: 275.7772, val_MinusLogProbMetric: 275.7772

Epoch 72: val_loss did not improve from 251.17560
196/196 - 80s - loss: 280.6417 - MinusLogProbMetric: 280.6417 - val_loss: 275.7772 - val_MinusLogProbMetric: 275.7772 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 73/1000
2023-10-26 09:36:29.985 
Epoch 73/1000 
	 loss: 265.9901, MinusLogProbMetric: 265.9901, val_loss: 258.5643, val_MinusLogProbMetric: 258.5643

Epoch 73: val_loss did not improve from 251.17560
196/196 - 81s - loss: 265.9901 - MinusLogProbMetric: 265.9901 - val_loss: 258.5643 - val_MinusLogProbMetric: 258.5643 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 74/1000
2023-10-26 09:37:50.374 
Epoch 74/1000 
	 loss: 253.9475, MinusLogProbMetric: 253.9475, val_loss: 252.1422, val_MinusLogProbMetric: 252.1422

Epoch 74: val_loss did not improve from 251.17560
196/196 - 80s - loss: 253.9475 - MinusLogProbMetric: 253.9475 - val_loss: 252.1422 - val_MinusLogProbMetric: 252.1422 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 75/1000
2023-10-26 09:39:10.061 
Epoch 75/1000 
	 loss: 250.3147, MinusLogProbMetric: 250.3147, val_loss: 249.3918, val_MinusLogProbMetric: 249.3918

Epoch 75: val_loss improved from 251.17560 to 249.39178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 250.3147 - MinusLogProbMetric: 250.3147 - val_loss: 249.3918 - val_MinusLogProbMetric: 249.3918 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 76/1000
2023-10-26 09:40:31.667 
Epoch 76/1000 
	 loss: 248.5133, MinusLogProbMetric: 248.5133, val_loss: 248.7599, val_MinusLogProbMetric: 248.7599

Epoch 76: val_loss improved from 249.39178 to 248.75992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 248.5133 - MinusLogProbMetric: 248.5133 - val_loss: 248.7599 - val_MinusLogProbMetric: 248.7599 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 77/1000
2023-10-26 09:41:53.722 
Epoch 77/1000 
	 loss: 295.9273, MinusLogProbMetric: 295.9273, val_loss: 294.2859, val_MinusLogProbMetric: 294.2859

Epoch 77: val_loss did not improve from 248.75992
196/196 - 81s - loss: 295.9273 - MinusLogProbMetric: 295.9273 - val_loss: 294.2859 - val_MinusLogProbMetric: 294.2859 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 78/1000
2023-10-26 09:43:14.210 
Epoch 78/1000 
	 loss: 256.1692, MinusLogProbMetric: 256.1692, val_loss: 246.0573, val_MinusLogProbMetric: 246.0573

Epoch 78: val_loss improved from 248.75992 to 246.05728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 256.1692 - MinusLogProbMetric: 256.1692 - val_loss: 246.0573 - val_MinusLogProbMetric: 246.0573 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 79/1000
2023-10-26 09:44:35.832 
Epoch 79/1000 
	 loss: 244.7082, MinusLogProbMetric: 244.7082, val_loss: 244.3941, val_MinusLogProbMetric: 244.3941

Epoch 79: val_loss improved from 246.05728 to 244.39412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 244.7082 - MinusLogProbMetric: 244.7082 - val_loss: 244.3941 - val_MinusLogProbMetric: 244.3941 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 80/1000
2023-10-26 09:45:57.629 
Epoch 80/1000 
	 loss: 245.1430, MinusLogProbMetric: 245.1430, val_loss: 244.5403, val_MinusLogProbMetric: 244.5403

Epoch 80: val_loss did not improve from 244.39412
196/196 - 80s - loss: 245.1430 - MinusLogProbMetric: 245.1430 - val_loss: 244.5403 - val_MinusLogProbMetric: 244.5403 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 81/1000
2023-10-26 09:47:18.675 
Epoch 81/1000 
	 loss: 242.4249, MinusLogProbMetric: 242.4249, val_loss: 241.7598, val_MinusLogProbMetric: 241.7598

Epoch 81: val_loss improved from 244.39412 to 241.75981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 242.4249 - MinusLogProbMetric: 242.4249 - val_loss: 241.7598 - val_MinusLogProbMetric: 241.7598 - lr: 4.1152e-06 - 83s/epoch - 421ms/step
Epoch 82/1000
2023-10-26 09:48:41.086 
Epoch 82/1000 
	 loss: 241.4890, MinusLogProbMetric: 241.4890, val_loss: 242.4523, val_MinusLogProbMetric: 242.4523

Epoch 82: val_loss did not improve from 241.75981
196/196 - 81s - loss: 241.4890 - MinusLogProbMetric: 241.4890 - val_loss: 242.4523 - val_MinusLogProbMetric: 242.4523 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 83/1000
2023-10-26 09:50:01.687 
Epoch 83/1000 
	 loss: 240.6987, MinusLogProbMetric: 240.6987, val_loss: 240.4925, val_MinusLogProbMetric: 240.4925

Epoch 83: val_loss improved from 241.75981 to 240.49252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 240.6987 - MinusLogProbMetric: 240.6987 - val_loss: 240.4925 - val_MinusLogProbMetric: 240.4925 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 84/1000
2023-10-26 09:51:22.525 
Epoch 84/1000 
	 loss: 239.6378, MinusLogProbMetric: 239.6378, val_loss: 238.8440, val_MinusLogProbMetric: 238.8440

Epoch 84: val_loss improved from 240.49252 to 238.84396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 239.6378 - MinusLogProbMetric: 239.6378 - val_loss: 238.8440 - val_MinusLogProbMetric: 238.8440 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 85/1000
2023-10-26 09:52:44.231 
Epoch 85/1000 
	 loss: 494.5572, MinusLogProbMetric: 494.5572, val_loss: 514.8829, val_MinusLogProbMetric: 514.8829

Epoch 85: val_loss did not improve from 238.84396
196/196 - 80s - loss: 494.5572 - MinusLogProbMetric: 494.5572 - val_loss: 514.8829 - val_MinusLogProbMetric: 514.8829 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 86/1000
2023-10-26 09:54:04.555 
Epoch 86/1000 
	 loss: 426.6614, MinusLogProbMetric: 426.6614, val_loss: 385.4055, val_MinusLogProbMetric: 385.4055

Epoch 86: val_loss did not improve from 238.84396
196/196 - 80s - loss: 426.6614 - MinusLogProbMetric: 426.6614 - val_loss: 385.4055 - val_MinusLogProbMetric: 385.4055 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 87/1000
2023-10-26 09:55:24.844 
Epoch 87/1000 
	 loss: 367.4272, MinusLogProbMetric: 367.4272, val_loss: 356.1592, val_MinusLogProbMetric: 356.1592

Epoch 87: val_loss did not improve from 238.84396
196/196 - 80s - loss: 367.4272 - MinusLogProbMetric: 367.4272 - val_loss: 356.1592 - val_MinusLogProbMetric: 356.1592 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 88/1000
2023-10-26 09:56:45.012 
Epoch 88/1000 
	 loss: 349.5727, MinusLogProbMetric: 349.5727, val_loss: 342.9780, val_MinusLogProbMetric: 342.9780

Epoch 88: val_loss did not improve from 238.84396
196/196 - 80s - loss: 349.5727 - MinusLogProbMetric: 349.5727 - val_loss: 342.9780 - val_MinusLogProbMetric: 342.9780 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 89/1000
2023-10-26 09:58:06.181 
Epoch 89/1000 
	 loss: 464.9283, MinusLogProbMetric: 464.9283, val_loss: 455.9658, val_MinusLogProbMetric: 455.9658

Epoch 89: val_loss did not improve from 238.84396
196/196 - 81s - loss: 464.9283 - MinusLogProbMetric: 464.9283 - val_loss: 455.9658 - val_MinusLogProbMetric: 455.9658 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 90/1000
2023-10-26 09:59:26.557 
Epoch 90/1000 
	 loss: 380.8656, MinusLogProbMetric: 380.8656, val_loss: 350.5590, val_MinusLogProbMetric: 350.5590

Epoch 90: val_loss did not improve from 238.84396
196/196 - 80s - loss: 380.8656 - MinusLogProbMetric: 380.8656 - val_loss: 350.5590 - val_MinusLogProbMetric: 350.5590 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 91/1000
2023-10-26 10:00:42.435 
Epoch 91/1000 
	 loss: 336.5022, MinusLogProbMetric: 336.5022, val_loss: 327.0322, val_MinusLogProbMetric: 327.0322

Epoch 91: val_loss did not improve from 238.84396
196/196 - 76s - loss: 336.5022 - MinusLogProbMetric: 336.5022 - val_loss: 327.0322 - val_MinusLogProbMetric: 327.0322 - lr: 4.1152e-06 - 76s/epoch - 387ms/step
Epoch 92/1000
2023-10-26 10:01:45.180 
Epoch 92/1000 
	 loss: 320.2040, MinusLogProbMetric: 320.2040, val_loss: 314.9416, val_MinusLogProbMetric: 314.9416

Epoch 92: val_loss did not improve from 238.84396
196/196 - 63s - loss: 320.2040 - MinusLogProbMetric: 320.2040 - val_loss: 314.9416 - val_MinusLogProbMetric: 314.9416 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 93/1000
2023-10-26 10:03:00.645 
Epoch 93/1000 
	 loss: 309.8729, MinusLogProbMetric: 309.8729, val_loss: 306.1780, val_MinusLogProbMetric: 306.1780

Epoch 93: val_loss did not improve from 238.84396
196/196 - 75s - loss: 309.8729 - MinusLogProbMetric: 309.8729 - val_loss: 306.1780 - val_MinusLogProbMetric: 306.1780 - lr: 4.1152e-06 - 75s/epoch - 385ms/step
Epoch 94/1000
2023-10-26 10:04:19.487 
Epoch 94/1000 
	 loss: 304.9018, MinusLogProbMetric: 304.9018, val_loss: 299.3004, val_MinusLogProbMetric: 299.3004

Epoch 94: val_loss did not improve from 238.84396
196/196 - 79s - loss: 304.9018 - MinusLogProbMetric: 304.9018 - val_loss: 299.3004 - val_MinusLogProbMetric: 299.3004 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 95/1000
2023-10-26 10:05:40.431 
Epoch 95/1000 
	 loss: 296.0027, MinusLogProbMetric: 296.0027, val_loss: 293.4938, val_MinusLogProbMetric: 293.4938

Epoch 95: val_loss did not improve from 238.84396
196/196 - 81s - loss: 296.0027 - MinusLogProbMetric: 296.0027 - val_loss: 293.4938 - val_MinusLogProbMetric: 293.4938 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 96/1000
2023-10-26 10:07:01.159 
Epoch 96/1000 
	 loss: 291.5716, MinusLogProbMetric: 291.5716, val_loss: 289.5658, val_MinusLogProbMetric: 289.5658

Epoch 96: val_loss did not improve from 238.84396
196/196 - 81s - loss: 291.5716 - MinusLogProbMetric: 291.5716 - val_loss: 289.5658 - val_MinusLogProbMetric: 289.5658 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 97/1000
2023-10-26 10:08:22.403 
Epoch 97/1000 
	 loss: 286.8007, MinusLogProbMetric: 286.8007, val_loss: 285.0148, val_MinusLogProbMetric: 285.0148

Epoch 97: val_loss did not improve from 238.84396
196/196 - 81s - loss: 286.8007 - MinusLogProbMetric: 286.8007 - val_loss: 285.0148 - val_MinusLogProbMetric: 285.0148 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 98/1000
2023-10-26 10:09:41.293 
Epoch 98/1000 
	 loss: 284.0837, MinusLogProbMetric: 284.0837, val_loss: 281.5099, val_MinusLogProbMetric: 281.5099

Epoch 98: val_loss did not improve from 238.84396
196/196 - 79s - loss: 284.0837 - MinusLogProbMetric: 284.0837 - val_loss: 281.5099 - val_MinusLogProbMetric: 281.5099 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 99/1000
2023-10-26 10:11:01.693 
Epoch 99/1000 
	 loss: 286.8671, MinusLogProbMetric: 286.8671, val_loss: 285.0417, val_MinusLogProbMetric: 285.0417

Epoch 99: val_loss did not improve from 238.84396
196/196 - 80s - loss: 286.8671 - MinusLogProbMetric: 286.8671 - val_loss: 285.0417 - val_MinusLogProbMetric: 285.0417 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 100/1000
2023-10-26 10:12:22.620 
Epoch 100/1000 
	 loss: 281.4809, MinusLogProbMetric: 281.4809, val_loss: 278.6620, val_MinusLogProbMetric: 278.6620

Epoch 100: val_loss did not improve from 238.84396
196/196 - 81s - loss: 281.4809 - MinusLogProbMetric: 281.4809 - val_loss: 278.6620 - val_MinusLogProbMetric: 278.6620 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 101/1000
2023-10-26 10:13:43.455 
Epoch 101/1000 
	 loss: 276.0309, MinusLogProbMetric: 276.0309, val_loss: 274.8527, val_MinusLogProbMetric: 274.8527

Epoch 101: val_loss did not improve from 238.84396
196/196 - 81s - loss: 276.0309 - MinusLogProbMetric: 276.0309 - val_loss: 274.8527 - val_MinusLogProbMetric: 274.8527 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 102/1000
2023-10-26 10:15:03.745 
Epoch 102/1000 
	 loss: 273.3761, MinusLogProbMetric: 273.3761, val_loss: 273.1936, val_MinusLogProbMetric: 273.1936

Epoch 102: val_loss did not improve from 238.84396
196/196 - 80s - loss: 273.3761 - MinusLogProbMetric: 273.3761 - val_loss: 273.1936 - val_MinusLogProbMetric: 273.1936 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 103/1000
2023-10-26 10:16:24.412 
Epoch 103/1000 
	 loss: 270.9590, MinusLogProbMetric: 270.9590, val_loss: 269.6420, val_MinusLogProbMetric: 269.6420

Epoch 103: val_loss did not improve from 238.84396
196/196 - 81s - loss: 270.9590 - MinusLogProbMetric: 270.9590 - val_loss: 269.6420 - val_MinusLogProbMetric: 269.6420 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 104/1000
2023-10-26 10:17:45.215 
Epoch 104/1000 
	 loss: 268.4406, MinusLogProbMetric: 268.4406, val_loss: 268.3210, val_MinusLogProbMetric: 268.3210

Epoch 104: val_loss did not improve from 238.84396
196/196 - 81s - loss: 268.4406 - MinusLogProbMetric: 268.4406 - val_loss: 268.3210 - val_MinusLogProbMetric: 268.3210 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 105/1000
2023-10-26 10:19:06.333 
Epoch 105/1000 
	 loss: 300.0965, MinusLogProbMetric: 300.0965, val_loss: 286.6678, val_MinusLogProbMetric: 286.6678

Epoch 105: val_loss did not improve from 238.84396
196/196 - 81s - loss: 300.0965 - MinusLogProbMetric: 300.0965 - val_loss: 286.6678 - val_MinusLogProbMetric: 286.6678 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 106/1000
2023-10-26 10:20:22.613 
Epoch 106/1000 
	 loss: 280.4586, MinusLogProbMetric: 280.4586, val_loss: 276.6230, val_MinusLogProbMetric: 276.6230

Epoch 106: val_loss did not improve from 238.84396
196/196 - 76s - loss: 280.4586 - MinusLogProbMetric: 280.4586 - val_loss: 276.6230 - val_MinusLogProbMetric: 276.6230 - lr: 4.1152e-06 - 76s/epoch - 389ms/step
Epoch 107/1000
2023-10-26 10:21:43.641 
Epoch 107/1000 
	 loss: 274.2014, MinusLogProbMetric: 274.2014, val_loss: 272.9350, val_MinusLogProbMetric: 272.9350

Epoch 107: val_loss did not improve from 238.84396
196/196 - 81s - loss: 274.2014 - MinusLogProbMetric: 274.2014 - val_loss: 272.9350 - val_MinusLogProbMetric: 272.9350 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 108/1000
2023-10-26 10:23:04.067 
Epoch 108/1000 
	 loss: 271.0161, MinusLogProbMetric: 271.0161, val_loss: 271.0537, val_MinusLogProbMetric: 271.0537

Epoch 108: val_loss did not improve from 238.84396
196/196 - 80s - loss: 271.0161 - MinusLogProbMetric: 271.0161 - val_loss: 271.0537 - val_MinusLogProbMetric: 271.0537 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 109/1000
2023-10-26 10:24:25.235 
Epoch 109/1000 
	 loss: 270.2098, MinusLogProbMetric: 270.2098, val_loss: 269.3141, val_MinusLogProbMetric: 269.3141

Epoch 109: val_loss did not improve from 238.84396
196/196 - 81s - loss: 270.2098 - MinusLogProbMetric: 270.2098 - val_loss: 269.3141 - val_MinusLogProbMetric: 269.3141 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 110/1000
2023-10-26 10:25:45.731 
Epoch 110/1000 
	 loss: 267.0034, MinusLogProbMetric: 267.0034, val_loss: 267.1075, val_MinusLogProbMetric: 267.1075

Epoch 110: val_loss did not improve from 238.84396
196/196 - 80s - loss: 267.0034 - MinusLogProbMetric: 267.0034 - val_loss: 267.1075 - val_MinusLogProbMetric: 267.1075 - lr: 4.1152e-06 - 80s/epoch - 411ms/step
Epoch 111/1000
2023-10-26 10:27:06.024 
Epoch 111/1000 
	 loss: 263.5661, MinusLogProbMetric: 263.5661, val_loss: 262.6389, val_MinusLogProbMetric: 262.6389

Epoch 111: val_loss did not improve from 238.84396
196/196 - 80s - loss: 263.5661 - MinusLogProbMetric: 263.5661 - val_loss: 262.6389 - val_MinusLogProbMetric: 262.6389 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 112/1000
2023-10-26 10:28:26.938 
Epoch 112/1000 
	 loss: 267.9834, MinusLogProbMetric: 267.9834, val_loss: 265.5067, val_MinusLogProbMetric: 265.5067

Epoch 112: val_loss did not improve from 238.84396
196/196 - 81s - loss: 267.9834 - MinusLogProbMetric: 267.9834 - val_loss: 265.5067 - val_MinusLogProbMetric: 265.5067 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 113/1000
2023-10-26 10:29:47.480 
Epoch 113/1000 
	 loss: 262.9332, MinusLogProbMetric: 262.9332, val_loss: 259.9616, val_MinusLogProbMetric: 259.9616

Epoch 113: val_loss did not improve from 238.84396
196/196 - 81s - loss: 262.9332 - MinusLogProbMetric: 262.9332 - val_loss: 259.9616 - val_MinusLogProbMetric: 259.9616 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 114/1000
2023-10-26 10:31:07.176 
Epoch 114/1000 
	 loss: 258.3922, MinusLogProbMetric: 258.3922, val_loss: 257.1576, val_MinusLogProbMetric: 257.1576

Epoch 114: val_loss did not improve from 238.84396
196/196 - 80s - loss: 258.3922 - MinusLogProbMetric: 258.3922 - val_loss: 257.1576 - val_MinusLogProbMetric: 257.1576 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 115/1000
2023-10-26 10:32:28.067 
Epoch 115/1000 
	 loss: 256.7508, MinusLogProbMetric: 256.7508, val_loss: 255.8982, val_MinusLogProbMetric: 255.8982

Epoch 115: val_loss did not improve from 238.84396
196/196 - 81s - loss: 256.7508 - MinusLogProbMetric: 256.7508 - val_loss: 255.8982 - val_MinusLogProbMetric: 255.8982 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 116/1000
2023-10-26 10:33:48.833 
Epoch 116/1000 
	 loss: 254.8184, MinusLogProbMetric: 254.8184, val_loss: 254.3867, val_MinusLogProbMetric: 254.3867

Epoch 116: val_loss did not improve from 238.84396
196/196 - 81s - loss: 254.8184 - MinusLogProbMetric: 254.8184 - val_loss: 254.3867 - val_MinusLogProbMetric: 254.3867 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 117/1000
2023-10-26 10:35:09.179 
Epoch 117/1000 
	 loss: 253.3450, MinusLogProbMetric: 253.3450, val_loss: 252.7749, val_MinusLogProbMetric: 252.7749

Epoch 117: val_loss did not improve from 238.84396
196/196 - 80s - loss: 253.3450 - MinusLogProbMetric: 253.3450 - val_loss: 252.7749 - val_MinusLogProbMetric: 252.7749 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 118/1000
2023-10-26 10:36:28.733 
Epoch 118/1000 
	 loss: 252.1151, MinusLogProbMetric: 252.1151, val_loss: 251.7154, val_MinusLogProbMetric: 251.7154

Epoch 118: val_loss did not improve from 238.84396
196/196 - 80s - loss: 252.1151 - MinusLogProbMetric: 252.1151 - val_loss: 251.7154 - val_MinusLogProbMetric: 251.7154 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 119/1000
2023-10-26 10:37:48.346 
Epoch 119/1000 
	 loss: 251.9648, MinusLogProbMetric: 251.9648, val_loss: 251.0288, val_MinusLogProbMetric: 251.0288

Epoch 119: val_loss did not improve from 238.84396
196/196 - 80s - loss: 251.9648 - MinusLogProbMetric: 251.9648 - val_loss: 251.0288 - val_MinusLogProbMetric: 251.0288 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 120/1000
2023-10-26 10:39:08.849 
Epoch 120/1000 
	 loss: 250.1255, MinusLogProbMetric: 250.1255, val_loss: 249.8268, val_MinusLogProbMetric: 249.8268

Epoch 120: val_loss did not improve from 238.84396
196/196 - 81s - loss: 250.1255 - MinusLogProbMetric: 250.1255 - val_loss: 249.8268 - val_MinusLogProbMetric: 249.8268 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 121/1000
2023-10-26 10:40:29.579 
Epoch 121/1000 
	 loss: 248.9016, MinusLogProbMetric: 248.9016, val_loss: 248.6415, val_MinusLogProbMetric: 248.6415

Epoch 121: val_loss did not improve from 238.84396
196/196 - 81s - loss: 248.9016 - MinusLogProbMetric: 248.9016 - val_loss: 248.6415 - val_MinusLogProbMetric: 248.6415 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 122/1000
2023-10-26 10:41:49.947 
Epoch 122/1000 
	 loss: 247.7829, MinusLogProbMetric: 247.7829, val_loss: 247.3229, val_MinusLogProbMetric: 247.3229

Epoch 122: val_loss did not improve from 238.84396
196/196 - 80s - loss: 247.7829 - MinusLogProbMetric: 247.7829 - val_loss: 247.3229 - val_MinusLogProbMetric: 247.3229 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 123/1000
2023-10-26 10:43:10.654 
Epoch 123/1000 
	 loss: 246.9310, MinusLogProbMetric: 246.9310, val_loss: 246.9026, val_MinusLogProbMetric: 246.9026

Epoch 123: val_loss did not improve from 238.84396
196/196 - 81s - loss: 246.9310 - MinusLogProbMetric: 246.9310 - val_loss: 246.9026 - val_MinusLogProbMetric: 246.9026 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 124/1000
2023-10-26 10:44:30.483 
Epoch 124/1000 
	 loss: 246.0142, MinusLogProbMetric: 246.0142, val_loss: 245.5542, val_MinusLogProbMetric: 245.5542

Epoch 124: val_loss did not improve from 238.84396
196/196 - 80s - loss: 246.0142 - MinusLogProbMetric: 246.0142 - val_loss: 245.5542 - val_MinusLogProbMetric: 245.5542 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 125/1000
2023-10-26 10:45:49.588 
Epoch 125/1000 
	 loss: 245.1220, MinusLogProbMetric: 245.1220, val_loss: 246.2184, val_MinusLogProbMetric: 246.2184

Epoch 125: val_loss did not improve from 238.84396
196/196 - 79s - loss: 245.1220 - MinusLogProbMetric: 245.1220 - val_loss: 246.2184 - val_MinusLogProbMetric: 246.2184 - lr: 4.1152e-06 - 79s/epoch - 404ms/step
Epoch 126/1000
2023-10-26 10:47:09.894 
Epoch 126/1000 
	 loss: 244.5501, MinusLogProbMetric: 244.5501, val_loss: 243.8411, val_MinusLogProbMetric: 243.8411

Epoch 126: val_loss did not improve from 238.84396
196/196 - 80s - loss: 244.5501 - MinusLogProbMetric: 244.5501 - val_loss: 243.8411 - val_MinusLogProbMetric: 243.8411 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 127/1000
2023-10-26 10:48:30.560 
Epoch 127/1000 
	 loss: 243.4500, MinusLogProbMetric: 243.4500, val_loss: 242.7926, val_MinusLogProbMetric: 242.7926

Epoch 127: val_loss did not improve from 238.84396
196/196 - 81s - loss: 243.4500 - MinusLogProbMetric: 243.4500 - val_loss: 242.7926 - val_MinusLogProbMetric: 242.7926 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 128/1000
2023-10-26 10:49:50.994 
Epoch 128/1000 
	 loss: 242.6591, MinusLogProbMetric: 242.6591, val_loss: 242.2326, val_MinusLogProbMetric: 242.2326

Epoch 128: val_loss did not improve from 238.84396
196/196 - 80s - loss: 242.6591 - MinusLogProbMetric: 242.6591 - val_loss: 242.2326 - val_MinusLogProbMetric: 242.2326 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 129/1000
2023-10-26 10:51:11.504 
Epoch 129/1000 
	 loss: 244.7914, MinusLogProbMetric: 244.7914, val_loss: 241.5765, val_MinusLogProbMetric: 241.5765

Epoch 129: val_loss did not improve from 238.84396
196/196 - 81s - loss: 244.7914 - MinusLogProbMetric: 244.7914 - val_loss: 241.5765 - val_MinusLogProbMetric: 241.5765 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 130/1000
2023-10-26 10:52:31.913 
Epoch 130/1000 
	 loss: 240.9983, MinusLogProbMetric: 240.9983, val_loss: 240.7136, val_MinusLogProbMetric: 240.7136

Epoch 130: val_loss did not improve from 238.84396
196/196 - 80s - loss: 240.9983 - MinusLogProbMetric: 240.9983 - val_loss: 240.7136 - val_MinusLogProbMetric: 240.7136 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 131/1000
2023-10-26 10:53:52.383 
Epoch 131/1000 
	 loss: 241.4919, MinusLogProbMetric: 241.4919, val_loss: 240.6368, val_MinusLogProbMetric: 240.6368

Epoch 131: val_loss did not improve from 238.84396
196/196 - 80s - loss: 241.4919 - MinusLogProbMetric: 241.4919 - val_loss: 240.6368 - val_MinusLogProbMetric: 240.6368 - lr: 4.1152e-06 - 80s/epoch - 411ms/step
Epoch 132/1000
2023-10-26 10:55:13.659 
Epoch 132/1000 
	 loss: 239.7927, MinusLogProbMetric: 239.7927, val_loss: 239.4269, val_MinusLogProbMetric: 239.4269

Epoch 132: val_loss did not improve from 238.84396
196/196 - 81s - loss: 239.7927 - MinusLogProbMetric: 239.7927 - val_loss: 239.4269 - val_MinusLogProbMetric: 239.4269 - lr: 4.1152e-06 - 81s/epoch - 415ms/step
Epoch 133/1000
2023-10-26 10:56:34.213 
Epoch 133/1000 
	 loss: 238.9603, MinusLogProbMetric: 238.9603, val_loss: 238.3524, val_MinusLogProbMetric: 238.3524

Epoch 133: val_loss improved from 238.84396 to 238.35243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 238.9603 - MinusLogProbMetric: 238.9603 - val_loss: 238.3524 - val_MinusLogProbMetric: 238.3524 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 134/1000
2023-10-26 10:57:56.696 
Epoch 134/1000 
	 loss: 238.1072, MinusLogProbMetric: 238.1072, val_loss: 238.1010, val_MinusLogProbMetric: 238.1010

Epoch 134: val_loss improved from 238.35243 to 238.10101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 238.1072 - MinusLogProbMetric: 238.1072 - val_loss: 238.1010 - val_MinusLogProbMetric: 238.1010 - lr: 4.1152e-06 - 82s/epoch - 421ms/step
Epoch 135/1000
2023-10-26 10:59:18.592 
Epoch 135/1000 
	 loss: 237.1614, MinusLogProbMetric: 237.1614, val_loss: 236.6508, val_MinusLogProbMetric: 236.6508

Epoch 135: val_loss improved from 238.10101 to 236.65085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 237.1614 - MinusLogProbMetric: 237.1614 - val_loss: 236.6508 - val_MinusLogProbMetric: 236.6508 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 136/1000
2023-10-26 11:00:40.442 
Epoch 136/1000 
	 loss: 235.5695, MinusLogProbMetric: 235.5695, val_loss: 235.1770, val_MinusLogProbMetric: 235.1770

Epoch 136: val_loss improved from 236.65085 to 235.17703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 235.5695 - MinusLogProbMetric: 235.5695 - val_loss: 235.1770 - val_MinusLogProbMetric: 235.1770 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 137/1000
2023-10-26 11:02:02.599 
Epoch 137/1000 
	 loss: 286.7226, MinusLogProbMetric: 286.7226, val_loss: 305.3199, val_MinusLogProbMetric: 305.3199

Epoch 137: val_loss did not improve from 235.17703
196/196 - 81s - loss: 286.7226 - MinusLogProbMetric: 286.7226 - val_loss: 305.3199 - val_MinusLogProbMetric: 305.3199 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 138/1000
2023-10-26 11:03:22.740 
Epoch 138/1000 
	 loss: 290.7910, MinusLogProbMetric: 290.7910, val_loss: 281.1613, val_MinusLogProbMetric: 281.1613

Epoch 138: val_loss did not improve from 235.17703
196/196 - 80s - loss: 290.7910 - MinusLogProbMetric: 290.7910 - val_loss: 281.1613 - val_MinusLogProbMetric: 281.1613 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 139/1000
2023-10-26 11:04:43.166 
Epoch 139/1000 
	 loss: 274.7957, MinusLogProbMetric: 274.7957, val_loss: 270.1738, val_MinusLogProbMetric: 270.1738

Epoch 139: val_loss did not improve from 235.17703
196/196 - 80s - loss: 274.7957 - MinusLogProbMetric: 274.7957 - val_loss: 270.1738 - val_MinusLogProbMetric: 270.1738 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 140/1000
2023-10-26 11:06:03.373 
Epoch 140/1000 
	 loss: 266.2646, MinusLogProbMetric: 266.2646, val_loss: 262.9774, val_MinusLogProbMetric: 262.9774

Epoch 140: val_loss did not improve from 235.17703
196/196 - 80s - loss: 266.2646 - MinusLogProbMetric: 266.2646 - val_loss: 262.9774 - val_MinusLogProbMetric: 262.9774 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 141/1000
2023-10-26 11:07:23.838 
Epoch 141/1000 
	 loss: 260.2318, MinusLogProbMetric: 260.2318, val_loss: 258.1649, val_MinusLogProbMetric: 258.1649

Epoch 141: val_loss did not improve from 235.17703
196/196 - 80s - loss: 260.2318 - MinusLogProbMetric: 260.2318 - val_loss: 258.1649 - val_MinusLogProbMetric: 258.1649 - lr: 4.1152e-06 - 80s/epoch - 411ms/step
Epoch 142/1000
2023-10-26 11:08:44.725 
Epoch 142/1000 
	 loss: 255.7281, MinusLogProbMetric: 255.7281, val_loss: 253.8821, val_MinusLogProbMetric: 253.8821

Epoch 142: val_loss did not improve from 235.17703
196/196 - 81s - loss: 255.7281 - MinusLogProbMetric: 255.7281 - val_loss: 253.8821 - val_MinusLogProbMetric: 253.8821 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 143/1000
2023-10-26 11:10:05.620 
Epoch 143/1000 
	 loss: 295.3591, MinusLogProbMetric: 295.3591, val_loss: 499.4295, val_MinusLogProbMetric: 499.4295

Epoch 143: val_loss did not improve from 235.17703
196/196 - 81s - loss: 295.3591 - MinusLogProbMetric: 295.3591 - val_loss: 499.4295 - val_MinusLogProbMetric: 499.4295 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 144/1000
2023-10-26 11:11:26.671 
Epoch 144/1000 
	 loss: 379.5603, MinusLogProbMetric: 379.5603, val_loss: 320.0387, val_MinusLogProbMetric: 320.0387

Epoch 144: val_loss did not improve from 235.17703
196/196 - 81s - loss: 379.5603 - MinusLogProbMetric: 379.5603 - val_loss: 320.0387 - val_MinusLogProbMetric: 320.0387 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 145/1000
2023-10-26 11:12:46.630 
Epoch 145/1000 
	 loss: 300.3939, MinusLogProbMetric: 300.3939, val_loss: 289.4972, val_MinusLogProbMetric: 289.4972

Epoch 145: val_loss did not improve from 235.17703
196/196 - 80s - loss: 300.3939 - MinusLogProbMetric: 300.3939 - val_loss: 289.4972 - val_MinusLogProbMetric: 289.4972 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 146/1000
2023-10-26 11:14:06.860 
Epoch 146/1000 
	 loss: 283.3404, MinusLogProbMetric: 283.3404, val_loss: 278.6006, val_MinusLogProbMetric: 278.6006

Epoch 146: val_loss did not improve from 235.17703
196/196 - 80s - loss: 283.3404 - MinusLogProbMetric: 283.3404 - val_loss: 278.6006 - val_MinusLogProbMetric: 278.6006 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 147/1000
2023-10-26 11:15:25.643 
Epoch 147/1000 
	 loss: 275.0275, MinusLogProbMetric: 275.0275, val_loss: 272.2260, val_MinusLogProbMetric: 272.2260

Epoch 147: val_loss did not improve from 235.17703
196/196 - 79s - loss: 275.0275 - MinusLogProbMetric: 275.0275 - val_loss: 272.2260 - val_MinusLogProbMetric: 272.2260 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 148/1000
2023-10-26 11:16:45.266 
Epoch 148/1000 
	 loss: 269.9740, MinusLogProbMetric: 269.9740, val_loss: 267.8761, val_MinusLogProbMetric: 267.8761

Epoch 148: val_loss did not improve from 235.17703
196/196 - 80s - loss: 269.9740 - MinusLogProbMetric: 269.9740 - val_loss: 267.8761 - val_MinusLogProbMetric: 267.8761 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 149/1000
2023-10-26 11:18:05.790 
Epoch 149/1000 
	 loss: 267.1093, MinusLogProbMetric: 267.1093, val_loss: 264.5697, val_MinusLogProbMetric: 264.5697

Epoch 149: val_loss did not improve from 235.17703
196/196 - 81s - loss: 267.1093 - MinusLogProbMetric: 267.1093 - val_loss: 264.5697 - val_MinusLogProbMetric: 264.5697 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 150/1000
2023-10-26 11:19:25.824 
Epoch 150/1000 
	 loss: 263.1082, MinusLogProbMetric: 263.1082, val_loss: 262.3158, val_MinusLogProbMetric: 262.3158

Epoch 150: val_loss did not improve from 235.17703
196/196 - 80s - loss: 263.1082 - MinusLogProbMetric: 263.1082 - val_loss: 262.3158 - val_MinusLogProbMetric: 262.3158 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 151/1000
2023-10-26 11:20:46.879 
Epoch 151/1000 
	 loss: 260.5799, MinusLogProbMetric: 260.5799, val_loss: 259.5389, val_MinusLogProbMetric: 259.5389

Epoch 151: val_loss did not improve from 235.17703
196/196 - 81s - loss: 260.5799 - MinusLogProbMetric: 260.5799 - val_loss: 259.5389 - val_MinusLogProbMetric: 259.5389 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 152/1000
2023-10-26 11:22:06.600 
Epoch 152/1000 
	 loss: 258.1917, MinusLogProbMetric: 258.1917, val_loss: 257.1951, val_MinusLogProbMetric: 257.1951

Epoch 152: val_loss did not improve from 235.17703
196/196 - 80s - loss: 258.1917 - MinusLogProbMetric: 258.1917 - val_loss: 257.1951 - val_MinusLogProbMetric: 257.1951 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 153/1000
2023-10-26 11:23:27.107 
Epoch 153/1000 
	 loss: 256.5761, MinusLogProbMetric: 256.5761, val_loss: 255.1407, val_MinusLogProbMetric: 255.1407

Epoch 153: val_loss did not improve from 235.17703
196/196 - 81s - loss: 256.5761 - MinusLogProbMetric: 256.5761 - val_loss: 255.1407 - val_MinusLogProbMetric: 255.1407 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 154/1000
2023-10-26 11:24:47.503 
Epoch 154/1000 
	 loss: 254.1539, MinusLogProbMetric: 254.1539, val_loss: 253.0631, val_MinusLogProbMetric: 253.0631

Epoch 154: val_loss did not improve from 235.17703
196/196 - 80s - loss: 254.1539 - MinusLogProbMetric: 254.1539 - val_loss: 253.0631 - val_MinusLogProbMetric: 253.0631 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 155/1000
2023-10-26 11:26:07.716 
Epoch 155/1000 
	 loss: 251.9502, MinusLogProbMetric: 251.9502, val_loss: 251.2626, val_MinusLogProbMetric: 251.2626

Epoch 155: val_loss did not improve from 235.17703
196/196 - 80s - loss: 251.9502 - MinusLogProbMetric: 251.9502 - val_loss: 251.2626 - val_MinusLogProbMetric: 251.2626 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 156/1000
2023-10-26 11:27:28.224 
Epoch 156/1000 
	 loss: 250.2976, MinusLogProbMetric: 250.2976, val_loss: 249.4609, val_MinusLogProbMetric: 249.4609

Epoch 156: val_loss did not improve from 235.17703
196/196 - 81s - loss: 250.2976 - MinusLogProbMetric: 250.2976 - val_loss: 249.4609 - val_MinusLogProbMetric: 249.4609 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 157/1000
2023-10-26 11:28:48.913 
Epoch 157/1000 
	 loss: 249.1057, MinusLogProbMetric: 249.1057, val_loss: 248.9768, val_MinusLogProbMetric: 248.9768

Epoch 157: val_loss did not improve from 235.17703
196/196 - 81s - loss: 249.1057 - MinusLogProbMetric: 249.1057 - val_loss: 248.9768 - val_MinusLogProbMetric: 248.9768 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 158/1000
2023-10-26 11:30:07.742 
Epoch 158/1000 
	 loss: 248.0913, MinusLogProbMetric: 248.0913, val_loss: 247.5359, val_MinusLogProbMetric: 247.5359

Epoch 158: val_loss did not improve from 235.17703
196/196 - 79s - loss: 248.0913 - MinusLogProbMetric: 248.0913 - val_loss: 247.5359 - val_MinusLogProbMetric: 247.5359 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 159/1000
2023-10-26 11:31:27.024 
Epoch 159/1000 
	 loss: 246.5603, MinusLogProbMetric: 246.5603, val_loss: 245.9131, val_MinusLogProbMetric: 245.9131

Epoch 159: val_loss did not improve from 235.17703
196/196 - 79s - loss: 246.5603 - MinusLogProbMetric: 246.5603 - val_loss: 245.9131 - val_MinusLogProbMetric: 245.9131 - lr: 4.1152e-06 - 79s/epoch - 404ms/step
Epoch 160/1000
2023-10-26 11:32:47.576 
Epoch 160/1000 
	 loss: 245.3891, MinusLogProbMetric: 245.3891, val_loss: 244.8401, val_MinusLogProbMetric: 244.8401

Epoch 160: val_loss did not improve from 235.17703
196/196 - 81s - loss: 245.3891 - MinusLogProbMetric: 245.3891 - val_loss: 244.8401 - val_MinusLogProbMetric: 244.8401 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 161/1000
2023-10-26 11:34:07.296 
Epoch 161/1000 
	 loss: 243.8976, MinusLogProbMetric: 243.8976, val_loss: 243.6496, val_MinusLogProbMetric: 243.6496

Epoch 161: val_loss did not improve from 235.17703
196/196 - 80s - loss: 243.8976 - MinusLogProbMetric: 243.8976 - val_loss: 243.6496 - val_MinusLogProbMetric: 243.6496 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 162/1000
2023-10-26 11:35:27.754 
Epoch 162/1000 
	 loss: 242.6668, MinusLogProbMetric: 242.6668, val_loss: 242.3728, val_MinusLogProbMetric: 242.3728

Epoch 162: val_loss did not improve from 235.17703
196/196 - 80s - loss: 242.6668 - MinusLogProbMetric: 242.6668 - val_loss: 242.3728 - val_MinusLogProbMetric: 242.3728 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 163/1000
2023-10-26 11:36:47.491 
Epoch 163/1000 
	 loss: 241.3891, MinusLogProbMetric: 241.3891, val_loss: 240.8475, val_MinusLogProbMetric: 240.8475

Epoch 163: val_loss did not improve from 235.17703
196/196 - 80s - loss: 241.3891 - MinusLogProbMetric: 241.3891 - val_loss: 240.8475 - val_MinusLogProbMetric: 240.8475 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 164/1000
2023-10-26 11:38:07.899 
Epoch 164/1000 
	 loss: 240.0135, MinusLogProbMetric: 240.0135, val_loss: 239.4045, val_MinusLogProbMetric: 239.4045

Epoch 164: val_loss did not improve from 235.17703
196/196 - 80s - loss: 240.0135 - MinusLogProbMetric: 240.0135 - val_loss: 239.4045 - val_MinusLogProbMetric: 239.4045 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 165/1000
2023-10-26 11:39:28.128 
Epoch 165/1000 
	 loss: 238.6459, MinusLogProbMetric: 238.6459, val_loss: 238.1259, val_MinusLogProbMetric: 238.1259

Epoch 165: val_loss did not improve from 235.17703
196/196 - 80s - loss: 238.6459 - MinusLogProbMetric: 238.6459 - val_loss: 238.1259 - val_MinusLogProbMetric: 238.1259 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 166/1000
2023-10-26 11:40:49.043 
Epoch 166/1000 
	 loss: 237.4894, MinusLogProbMetric: 237.4894, val_loss: 237.2134, val_MinusLogProbMetric: 237.2134

Epoch 166: val_loss did not improve from 235.17703
196/196 - 81s - loss: 237.4894 - MinusLogProbMetric: 237.4894 - val_loss: 237.2134 - val_MinusLogProbMetric: 237.2134 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 167/1000
2023-10-26 11:42:09.040 
Epoch 167/1000 
	 loss: 236.6584, MinusLogProbMetric: 236.6584, val_loss: 237.5512, val_MinusLogProbMetric: 237.5512

Epoch 167: val_loss did not improve from 235.17703
196/196 - 80s - loss: 236.6584 - MinusLogProbMetric: 236.6584 - val_loss: 237.5512 - val_MinusLogProbMetric: 237.5512 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 168/1000
2023-10-26 11:43:29.913 
Epoch 168/1000 
	 loss: 235.8289, MinusLogProbMetric: 235.8289, val_loss: 235.4028, val_MinusLogProbMetric: 235.4028

Epoch 168: val_loss did not improve from 235.17703
196/196 - 81s - loss: 235.8289 - MinusLogProbMetric: 235.8289 - val_loss: 235.4028 - val_MinusLogProbMetric: 235.4028 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 169/1000
2023-10-26 11:44:50.060 
Epoch 169/1000 
	 loss: 234.9865, MinusLogProbMetric: 234.9865, val_loss: 234.7205, val_MinusLogProbMetric: 234.7205

Epoch 169: val_loss improved from 235.17703 to 234.72050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 234.9865 - MinusLogProbMetric: 234.9865 - val_loss: 234.7205 - val_MinusLogProbMetric: 234.7205 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 170/1000
2023-10-26 11:46:11.353 
Epoch 170/1000 
	 loss: 234.2452, MinusLogProbMetric: 234.2452, val_loss: 233.8509, val_MinusLogProbMetric: 233.8509

Epoch 170: val_loss improved from 234.72050 to 233.85094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 234.2452 - MinusLogProbMetric: 234.2452 - val_loss: 233.8509 - val_MinusLogProbMetric: 233.8509 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 171/1000
2023-10-26 11:47:33.452 
Epoch 171/1000 
	 loss: 233.2640, MinusLogProbMetric: 233.2640, val_loss: 233.3443, val_MinusLogProbMetric: 233.3443

Epoch 171: val_loss improved from 233.85094 to 233.34435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 233.2640 - MinusLogProbMetric: 233.2640 - val_loss: 233.3443 - val_MinusLogProbMetric: 233.3443 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 172/1000
2023-10-26 11:48:56.054 
Epoch 172/1000 
	 loss: 232.5245, MinusLogProbMetric: 232.5245, val_loss: 232.4636, val_MinusLogProbMetric: 232.4636

Epoch 172: val_loss improved from 233.34435 to 232.46359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 232.5245 - MinusLogProbMetric: 232.5245 - val_loss: 232.4636 - val_MinusLogProbMetric: 232.4636 - lr: 4.1152e-06 - 82s/epoch - 421ms/step
Epoch 173/1000
2023-10-26 11:50:18.230 
Epoch 173/1000 
	 loss: 231.8571, MinusLogProbMetric: 231.8571, val_loss: 231.7000, val_MinusLogProbMetric: 231.7000

Epoch 173: val_loss improved from 232.46359 to 231.70003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 231.8571 - MinusLogProbMetric: 231.8571 - val_loss: 231.7000 - val_MinusLogProbMetric: 231.7000 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 174/1000
2023-10-26 11:51:40.763 
Epoch 174/1000 
	 loss: 231.2773, MinusLogProbMetric: 231.2773, val_loss: 231.0509, val_MinusLogProbMetric: 231.0509

Epoch 174: val_loss improved from 231.70003 to 231.05086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 231.2773 - MinusLogProbMetric: 231.2773 - val_loss: 231.0509 - val_MinusLogProbMetric: 231.0509 - lr: 4.1152e-06 - 83s/epoch - 421ms/step
Epoch 175/1000
2023-10-26 11:53:01.501 
Epoch 175/1000 
	 loss: 230.6275, MinusLogProbMetric: 230.6275, val_loss: 230.7178, val_MinusLogProbMetric: 230.7178

Epoch 175: val_loss improved from 231.05086 to 230.71780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 230.6275 - MinusLogProbMetric: 230.6275 - val_loss: 230.7178 - val_MinusLogProbMetric: 230.7178 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 176/1000
2023-10-26 11:54:23.099 
Epoch 176/1000 
	 loss: 229.8765, MinusLogProbMetric: 229.8765, val_loss: 230.4918, val_MinusLogProbMetric: 230.4918

Epoch 176: val_loss improved from 230.71780 to 230.49182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 229.8765 - MinusLogProbMetric: 229.8765 - val_loss: 230.4918 - val_MinusLogProbMetric: 230.4918 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 177/1000
2023-10-26 11:55:44.769 
Epoch 177/1000 
	 loss: 229.2152, MinusLogProbMetric: 229.2152, val_loss: 229.0214, val_MinusLogProbMetric: 229.0214

Epoch 177: val_loss improved from 230.49182 to 229.02138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 229.2152 - MinusLogProbMetric: 229.2152 - val_loss: 229.0214 - val_MinusLogProbMetric: 229.0214 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 178/1000
2023-10-26 11:57:07.446 
Epoch 178/1000 
	 loss: 228.5533, MinusLogProbMetric: 228.5533, val_loss: 228.5593, val_MinusLogProbMetric: 228.5593

Epoch 178: val_loss improved from 229.02138 to 228.55930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 228.5533 - MinusLogProbMetric: 228.5533 - val_loss: 228.5593 - val_MinusLogProbMetric: 228.5593 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 179/1000
2023-10-26 11:58:28.684 
Epoch 179/1000 
	 loss: 227.8816, MinusLogProbMetric: 227.8816, val_loss: 227.8290, val_MinusLogProbMetric: 227.8290

Epoch 179: val_loss improved from 228.55930 to 227.82899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 227.8816 - MinusLogProbMetric: 227.8816 - val_loss: 227.8290 - val_MinusLogProbMetric: 227.8290 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 180/1000
2023-10-26 11:59:48.961 
Epoch 180/1000 
	 loss: 227.1957, MinusLogProbMetric: 227.1957, val_loss: 228.3501, val_MinusLogProbMetric: 228.3501

Epoch 180: val_loss did not improve from 227.82899
196/196 - 79s - loss: 227.1957 - MinusLogProbMetric: 227.1957 - val_loss: 228.3501 - val_MinusLogProbMetric: 228.3501 - lr: 4.1152e-06 - 79s/epoch - 403ms/step
Epoch 181/1000
2023-10-26 12:01:09.331 
Epoch 181/1000 
	 loss: 226.6337, MinusLogProbMetric: 226.6337, val_loss: 226.5601, val_MinusLogProbMetric: 226.5601

Epoch 181: val_loss improved from 227.82899 to 226.56013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 226.6337 - MinusLogProbMetric: 226.6337 - val_loss: 226.5601 - val_MinusLogProbMetric: 226.5601 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 182/1000
2023-10-26 12:02:30.940 
Epoch 182/1000 
	 loss: 225.7773, MinusLogProbMetric: 225.7773, val_loss: 225.5354, val_MinusLogProbMetric: 225.5354

Epoch 182: val_loss improved from 226.56013 to 225.53542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 225.7773 - MinusLogProbMetric: 225.7773 - val_loss: 225.5354 - val_MinusLogProbMetric: 225.5354 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 183/1000
2023-10-26 12:03:53.107 
Epoch 183/1000 
	 loss: 225.1018, MinusLogProbMetric: 225.1018, val_loss: 225.2136, val_MinusLogProbMetric: 225.2136

Epoch 183: val_loss improved from 225.53542 to 225.21362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 225.1018 - MinusLogProbMetric: 225.1018 - val_loss: 225.2136 - val_MinusLogProbMetric: 225.2136 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 184/1000
2023-10-26 12:05:15.598 
Epoch 184/1000 
	 loss: 224.8018, MinusLogProbMetric: 224.8018, val_loss: 226.5991, val_MinusLogProbMetric: 226.5991

Epoch 184: val_loss did not improve from 225.21362
196/196 - 81s - loss: 224.8018 - MinusLogProbMetric: 224.8018 - val_loss: 226.5991 - val_MinusLogProbMetric: 226.5991 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 185/1000
2023-10-26 12:06:35.816 
Epoch 185/1000 
	 loss: 225.8655, MinusLogProbMetric: 225.8655, val_loss: 225.5313, val_MinusLogProbMetric: 225.5313

Epoch 185: val_loss did not improve from 225.21362
196/196 - 80s - loss: 225.8655 - MinusLogProbMetric: 225.8655 - val_loss: 225.5313 - val_MinusLogProbMetric: 225.5313 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 186/1000
2023-10-26 12:07:57.013 
Epoch 186/1000 
	 loss: 224.5468, MinusLogProbMetric: 224.5468, val_loss: 223.7765, val_MinusLogProbMetric: 223.7765

Epoch 186: val_loss improved from 225.21362 to 223.77649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 224.5468 - MinusLogProbMetric: 224.5468 - val_loss: 223.7765 - val_MinusLogProbMetric: 223.7765 - lr: 4.1152e-06 - 82s/epoch - 421ms/step
Epoch 187/1000
2023-10-26 12:09:17.867 
Epoch 187/1000 
	 loss: 223.3037, MinusLogProbMetric: 223.3037, val_loss: 223.0673, val_MinusLogProbMetric: 223.0673

Epoch 187: val_loss improved from 223.77649 to 223.06731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 223.3037 - MinusLogProbMetric: 223.3037 - val_loss: 223.0673 - val_MinusLogProbMetric: 223.0673 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 188/1000
2023-10-26 12:10:40.173 
Epoch 188/1000 
	 loss: 222.6697, MinusLogProbMetric: 222.6697, val_loss: 223.6418, val_MinusLogProbMetric: 223.6418

Epoch 188: val_loss did not improve from 223.06731
196/196 - 81s - loss: 222.6697 - MinusLogProbMetric: 222.6697 - val_loss: 223.6418 - val_MinusLogProbMetric: 223.6418 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 189/1000
2023-10-26 12:11:57.911 
Epoch 189/1000 
	 loss: 222.1047, MinusLogProbMetric: 222.1047, val_loss: 222.4254, val_MinusLogProbMetric: 222.4254

Epoch 189: val_loss improved from 223.06731 to 222.42540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 79s - loss: 222.1047 - MinusLogProbMetric: 222.1047 - val_loss: 222.4254 - val_MinusLogProbMetric: 222.4254 - lr: 4.1152e-06 - 79s/epoch - 403ms/step
Epoch 190/1000
2023-10-26 12:13:17.814 
Epoch 190/1000 
	 loss: 221.5136, MinusLogProbMetric: 221.5136, val_loss: 221.7384, val_MinusLogProbMetric: 221.7384

Epoch 190: val_loss improved from 222.42540 to 221.73843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 221.5136 - MinusLogProbMetric: 221.5136 - val_loss: 221.7384 - val_MinusLogProbMetric: 221.7384 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 191/1000
2023-10-26 12:14:39.770 
Epoch 191/1000 
	 loss: 220.9460, MinusLogProbMetric: 220.9460, val_loss: 221.0177, val_MinusLogProbMetric: 221.0177

Epoch 191: val_loss improved from 221.73843 to 221.01765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 220.9460 - MinusLogProbMetric: 220.9460 - val_loss: 221.0177 - val_MinusLogProbMetric: 221.0177 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 192/1000
2023-10-26 12:16:00.351 
Epoch 192/1000 
	 loss: 220.5317, MinusLogProbMetric: 220.5317, val_loss: 220.4649, val_MinusLogProbMetric: 220.4649

Epoch 192: val_loss improved from 221.01765 to 220.46492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 220.5317 - MinusLogProbMetric: 220.5317 - val_loss: 220.4649 - val_MinusLogProbMetric: 220.4649 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 193/1000
2023-10-26 12:17:21.631 
Epoch 193/1000 
	 loss: 220.0196, MinusLogProbMetric: 220.0196, val_loss: 219.9541, val_MinusLogProbMetric: 219.9541

Epoch 193: val_loss improved from 220.46492 to 219.95409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 220.0196 - MinusLogProbMetric: 220.0196 - val_loss: 219.9541 - val_MinusLogProbMetric: 219.9541 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 194/1000
2023-10-26 12:18:43.053 
Epoch 194/1000 
	 loss: 219.4878, MinusLogProbMetric: 219.4878, val_loss: 219.3181, val_MinusLogProbMetric: 219.3181

Epoch 194: val_loss improved from 219.95409 to 219.31805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 219.4878 - MinusLogProbMetric: 219.4878 - val_loss: 219.3181 - val_MinusLogProbMetric: 219.3181 - lr: 4.1152e-06 - 81s/epoch - 416ms/step
Epoch 195/1000
2023-10-26 12:20:04.964 
Epoch 195/1000 
	 loss: 219.0143, MinusLogProbMetric: 219.0143, val_loss: 220.5126, val_MinusLogProbMetric: 220.5126

Epoch 195: val_loss did not improve from 219.31805
196/196 - 81s - loss: 219.0143 - MinusLogProbMetric: 219.0143 - val_loss: 220.5126 - val_MinusLogProbMetric: 220.5126 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 196/1000
2023-10-26 12:21:26.116 
Epoch 196/1000 
	 loss: 218.5807, MinusLogProbMetric: 218.5807, val_loss: 218.8073, val_MinusLogProbMetric: 218.8073

Epoch 196: val_loss improved from 219.31805 to 218.80734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 218.5807 - MinusLogProbMetric: 218.5807 - val_loss: 218.8073 - val_MinusLogProbMetric: 218.8073 - lr: 4.1152e-06 - 82s/epoch - 421ms/step
Epoch 197/1000
2023-10-26 12:22:48.142 
Epoch 197/1000 
	 loss: 218.1629, MinusLogProbMetric: 218.1629, val_loss: 218.7107, val_MinusLogProbMetric: 218.7107

Epoch 197: val_loss improved from 218.80734 to 218.71069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 218.1629 - MinusLogProbMetric: 218.1629 - val_loss: 218.7107 - val_MinusLogProbMetric: 218.7107 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 198/1000
2023-10-26 12:24:10.544 
Epoch 198/1000 
	 loss: 217.6515, MinusLogProbMetric: 217.6515, val_loss: 217.5307, val_MinusLogProbMetric: 217.5307

Epoch 198: val_loss improved from 218.71069 to 217.53075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 217.6515 - MinusLogProbMetric: 217.6515 - val_loss: 217.5307 - val_MinusLogProbMetric: 217.5307 - lr: 4.1152e-06 - 83s/epoch - 421ms/step
Epoch 199/1000
2023-10-26 12:25:32.384 
Epoch 199/1000 
	 loss: 217.0658, MinusLogProbMetric: 217.0658, val_loss: 216.9111, val_MinusLogProbMetric: 216.9111

Epoch 199: val_loss improved from 217.53075 to 216.91106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 217.0658 - MinusLogProbMetric: 217.0658 - val_loss: 216.9111 - val_MinusLogProbMetric: 216.9111 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 200/1000
2023-10-26 12:26:44.060 
Epoch 200/1000 
	 loss: 216.6904, MinusLogProbMetric: 216.6904, val_loss: 216.5318, val_MinusLogProbMetric: 216.5318

Epoch 200: val_loss improved from 216.91106 to 216.53180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 216.6904 - MinusLogProbMetric: 216.6904 - val_loss: 216.5318 - val_MinusLogProbMetric: 216.5318 - lr: 4.1152e-06 - 71s/epoch - 364ms/step
Epoch 201/1000
2023-10-26 12:27:59.613 
Epoch 201/1000 
	 loss: 216.2382, MinusLogProbMetric: 216.2382, val_loss: 216.1140, val_MinusLogProbMetric: 216.1140

Epoch 201: val_loss improved from 216.53180 to 216.11404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 76s - loss: 216.2382 - MinusLogProbMetric: 216.2382 - val_loss: 216.1140 - val_MinusLogProbMetric: 216.1140 - lr: 4.1152e-06 - 76s/epoch - 387ms/step
Epoch 202/1000
2023-10-26 12:29:09.279 
Epoch 202/1000 
	 loss: 215.7258, MinusLogProbMetric: 215.7258, val_loss: 215.5646, val_MinusLogProbMetric: 215.5646

Epoch 202: val_loss improved from 216.11404 to 215.56456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 215.7258 - MinusLogProbMetric: 215.7258 - val_loss: 215.5646 - val_MinusLogProbMetric: 215.5646 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 203/1000
2023-10-26 12:30:24.687 
Epoch 203/1000 
	 loss: 215.3369, MinusLogProbMetric: 215.3369, val_loss: 216.0011, val_MinusLogProbMetric: 216.0011

Epoch 203: val_loss did not improve from 215.56456
196/196 - 74s - loss: 215.3369 - MinusLogProbMetric: 215.3369 - val_loss: 216.0011 - val_MinusLogProbMetric: 216.0011 - lr: 4.1152e-06 - 74s/epoch - 379ms/step
Epoch 204/1000
2023-10-26 12:31:32.266 
Epoch 204/1000 
	 loss: 214.9399, MinusLogProbMetric: 214.9399, val_loss: 214.8633, val_MinusLogProbMetric: 214.8633

Epoch 204: val_loss improved from 215.56456 to 214.86328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 214.9399 - MinusLogProbMetric: 214.9399 - val_loss: 214.8633 - val_MinusLogProbMetric: 214.8633 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 205/1000
2023-10-26 12:32:47.141 
Epoch 205/1000 
	 loss: 214.5073, MinusLogProbMetric: 214.5073, val_loss: 214.5138, val_MinusLogProbMetric: 214.5138

Epoch 205: val_loss improved from 214.86328 to 214.51384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 214.5073 - MinusLogProbMetric: 214.5073 - val_loss: 214.5138 - val_MinusLogProbMetric: 214.5138 - lr: 4.1152e-06 - 75s/epoch - 382ms/step
Epoch 206/1000
2023-10-26 12:33:56.305 
Epoch 206/1000 
	 loss: 214.1669, MinusLogProbMetric: 214.1669, val_loss: 214.3620, val_MinusLogProbMetric: 214.3620

Epoch 206: val_loss improved from 214.51384 to 214.36195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 214.1669 - MinusLogProbMetric: 214.1669 - val_loss: 214.3620 - val_MinusLogProbMetric: 214.3620 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 207/1000
2023-10-26 12:35:11.077 
Epoch 207/1000 
	 loss: 213.7440, MinusLogProbMetric: 213.7440, val_loss: 214.1481, val_MinusLogProbMetric: 214.1481

Epoch 207: val_loss improved from 214.36195 to 214.14813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 213.7440 - MinusLogProbMetric: 213.7440 - val_loss: 214.1481 - val_MinusLogProbMetric: 214.1481 - lr: 4.1152e-06 - 75s/epoch - 382ms/step
Epoch 208/1000
2023-10-26 12:36:18.976 
Epoch 208/1000 
	 loss: 213.4130, MinusLogProbMetric: 213.4130, val_loss: 213.1497, val_MinusLogProbMetric: 213.1497

Epoch 208: val_loss improved from 214.14813 to 213.14969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 213.4130 - MinusLogProbMetric: 213.4130 - val_loss: 213.1497 - val_MinusLogProbMetric: 213.1497 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 209/1000
2023-10-26 12:37:32.821 
Epoch 209/1000 
	 loss: 213.0607, MinusLogProbMetric: 213.0607, val_loss: 213.0195, val_MinusLogProbMetric: 213.0195

Epoch 209: val_loss improved from 213.14969 to 213.01949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 74s - loss: 213.0607 - MinusLogProbMetric: 213.0607 - val_loss: 213.0195 - val_MinusLogProbMetric: 213.0195 - lr: 4.1152e-06 - 74s/epoch - 376ms/step
Epoch 210/1000
2023-10-26 12:38:41.195 
Epoch 210/1000 
	 loss: 212.6421, MinusLogProbMetric: 212.6421, val_loss: 212.4344, val_MinusLogProbMetric: 212.4344

Epoch 210: val_loss improved from 213.01949 to 212.43437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 212.6421 - MinusLogProbMetric: 212.6421 - val_loss: 212.4344 - val_MinusLogProbMetric: 212.4344 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 211/1000
2023-10-26 12:39:54.415 
Epoch 211/1000 
	 loss: 212.2654, MinusLogProbMetric: 212.2654, val_loss: 212.4618, val_MinusLogProbMetric: 212.4618

Epoch 211: val_loss did not improve from 212.43437
196/196 - 72s - loss: 212.2654 - MinusLogProbMetric: 212.2654 - val_loss: 212.4618 - val_MinusLogProbMetric: 212.4618 - lr: 4.1152e-06 - 72s/epoch - 367ms/step
Epoch 212/1000
2023-10-26 12:41:01.381 
Epoch 212/1000 
	 loss: 211.9520, MinusLogProbMetric: 211.9520, val_loss: 211.9174, val_MinusLogProbMetric: 211.9174

Epoch 212: val_loss improved from 212.43437 to 211.91745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 211.9520 - MinusLogProbMetric: 211.9520 - val_loss: 211.9174 - val_MinusLogProbMetric: 211.9174 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 213/1000
2023-10-26 12:42:14.542 
Epoch 213/1000 
	 loss: 211.6492, MinusLogProbMetric: 211.6492, val_loss: 211.4197, val_MinusLogProbMetric: 211.4197

Epoch 213: val_loss improved from 211.91745 to 211.41966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 73s - loss: 211.6492 - MinusLogProbMetric: 211.6492 - val_loss: 211.4197 - val_MinusLogProbMetric: 211.4197 - lr: 4.1152e-06 - 73s/epoch - 372ms/step
Epoch 214/1000
2023-10-26 12:43:24.401 
Epoch 214/1000 
	 loss: 211.2987, MinusLogProbMetric: 211.2987, val_loss: 211.1185, val_MinusLogProbMetric: 211.1185

Epoch 214: val_loss improved from 211.41966 to 211.11853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 211.2987 - MinusLogProbMetric: 211.2987 - val_loss: 211.1185 - val_MinusLogProbMetric: 211.1185 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 215/1000
2023-10-26 12:44:38.481 
Epoch 215/1000 
	 loss: 210.9481, MinusLogProbMetric: 210.9481, val_loss: 211.1640, val_MinusLogProbMetric: 211.1640

Epoch 215: val_loss did not improve from 211.11853
196/196 - 73s - loss: 210.9481 - MinusLogProbMetric: 210.9481 - val_loss: 211.1640 - val_MinusLogProbMetric: 211.1640 - lr: 4.1152e-06 - 73s/epoch - 371ms/step
Epoch 216/1000
2023-10-26 12:45:40.919 
Epoch 216/1000 
	 loss: 210.6591, MinusLogProbMetric: 210.6591, val_loss: 210.5285, val_MinusLogProbMetric: 210.5285

Epoch 216: val_loss improved from 211.11853 to 210.52853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 64s - loss: 210.6591 - MinusLogProbMetric: 210.6591 - val_loss: 210.5285 - val_MinusLogProbMetric: 210.5285 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 217/1000
2023-10-26 12:46:42.932 
Epoch 217/1000 
	 loss: 210.3339, MinusLogProbMetric: 210.3339, val_loss: 210.0859, val_MinusLogProbMetric: 210.0859

Epoch 217: val_loss improved from 210.52853 to 210.08588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 62s - loss: 210.3339 - MinusLogProbMetric: 210.3339 - val_loss: 210.0859 - val_MinusLogProbMetric: 210.0859 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 218/1000
2023-10-26 12:47:46.512 
Epoch 218/1000 
	 loss: 209.9987, MinusLogProbMetric: 209.9987, val_loss: 209.8380, val_MinusLogProbMetric: 209.8380

Epoch 218: val_loss improved from 210.08588 to 209.83801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 64s - loss: 209.9987 - MinusLogProbMetric: 209.9987 - val_loss: 209.8380 - val_MinusLogProbMetric: 209.8380 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 219/1000
2023-10-26 12:48:48.977 
Epoch 219/1000 
	 loss: 209.6395, MinusLogProbMetric: 209.6395, val_loss: 209.7377, val_MinusLogProbMetric: 209.7377

Epoch 219: val_loss improved from 209.83801 to 209.73775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 62s - loss: 209.6395 - MinusLogProbMetric: 209.6395 - val_loss: 209.7377 - val_MinusLogProbMetric: 209.7377 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 220/1000
2023-10-26 12:49:51.855 
Epoch 220/1000 
	 loss: 209.3360, MinusLogProbMetric: 209.3360, val_loss: 209.1769, val_MinusLogProbMetric: 209.1769

Epoch 220: val_loss improved from 209.73775 to 209.17686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 63s - loss: 209.3360 - MinusLogProbMetric: 209.3360 - val_loss: 209.1769 - val_MinusLogProbMetric: 209.1769 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 221/1000
2023-10-26 12:50:54.432 
Epoch 221/1000 
	 loss: 209.0485, MinusLogProbMetric: 209.0485, val_loss: 208.9406, val_MinusLogProbMetric: 208.9406

Epoch 221: val_loss improved from 209.17686 to 208.94055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 63s - loss: 209.0485 - MinusLogProbMetric: 209.0485 - val_loss: 208.9406 - val_MinusLogProbMetric: 208.9406 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 222/1000
2023-10-26 12:52:02.356 
Epoch 222/1000 
	 loss: 208.6653, MinusLogProbMetric: 208.6653, val_loss: 208.6390, val_MinusLogProbMetric: 208.6390

Epoch 222: val_loss improved from 208.94055 to 208.63902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 208.6653 - MinusLogProbMetric: 208.6653 - val_loss: 208.6390 - val_MinusLogProbMetric: 208.6390 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 223/1000
2023-10-26 12:53:15.256 
Epoch 223/1000 
	 loss: 208.3255, MinusLogProbMetric: 208.3255, val_loss: 208.5340, val_MinusLogProbMetric: 208.5340

Epoch 223: val_loss improved from 208.63902 to 208.53398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 73s - loss: 208.3255 - MinusLogProbMetric: 208.3255 - val_loss: 208.5340 - val_MinusLogProbMetric: 208.5340 - lr: 4.1152e-06 - 73s/epoch - 373ms/step
Epoch 224/1000
2023-10-26 12:54:26.715 
Epoch 224/1000 
	 loss: 208.0031, MinusLogProbMetric: 208.0031, val_loss: 208.1411, val_MinusLogProbMetric: 208.1411

Epoch 224: val_loss improved from 208.53398 to 208.14111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 208.0031 - MinusLogProbMetric: 208.0031 - val_loss: 208.1411 - val_MinusLogProbMetric: 208.1411 - lr: 4.1152e-06 - 71s/epoch - 364ms/step
Epoch 225/1000
2023-10-26 12:55:36.309 
Epoch 225/1000 
	 loss: 207.5983, MinusLogProbMetric: 207.5983, val_loss: 207.5258, val_MinusLogProbMetric: 207.5258

Epoch 225: val_loss improved from 208.14111 to 207.52579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 207.5983 - MinusLogProbMetric: 207.5983 - val_loss: 207.5258 - val_MinusLogProbMetric: 207.5258 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 226/1000
2023-10-26 12:56:50.264 
Epoch 226/1000 
	 loss: 207.2583, MinusLogProbMetric: 207.2583, val_loss: 207.3106, val_MinusLogProbMetric: 207.3106

Epoch 226: val_loss improved from 207.52579 to 207.31056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 74s - loss: 207.2583 - MinusLogProbMetric: 207.2583 - val_loss: 207.3106 - val_MinusLogProbMetric: 207.3106 - lr: 4.1152e-06 - 74s/epoch - 378ms/step
Epoch 227/1000
2023-10-26 12:57:57.443 
Epoch 227/1000 
	 loss: 206.9743, MinusLogProbMetric: 206.9743, val_loss: 207.0501, val_MinusLogProbMetric: 207.0501

Epoch 227: val_loss improved from 207.31056 to 207.05009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 67s - loss: 206.9743 - MinusLogProbMetric: 206.9743 - val_loss: 207.0501 - val_MinusLogProbMetric: 207.0501 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 228/1000
2023-10-26 12:58:59.824 
Epoch 228/1000 
	 loss: 206.5982, MinusLogProbMetric: 206.5982, val_loss: 206.5372, val_MinusLogProbMetric: 206.5372

Epoch 228: val_loss improved from 207.05009 to 206.53717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 62s - loss: 206.5982 - MinusLogProbMetric: 206.5982 - val_loss: 206.5372 - val_MinusLogProbMetric: 206.5372 - lr: 4.1152e-06 - 62s/epoch - 319ms/step
Epoch 229/1000
2023-10-26 13:00:05.720 
Epoch 229/1000 
	 loss: 206.9982, MinusLogProbMetric: 206.9982, val_loss: 207.2519, val_MinusLogProbMetric: 207.2519

Epoch 229: val_loss did not improve from 206.53717
196/196 - 65s - loss: 206.9982 - MinusLogProbMetric: 206.9982 - val_loss: 207.2519 - val_MinusLogProbMetric: 207.2519 - lr: 4.1152e-06 - 65s/epoch - 330ms/step
Epoch 230/1000
2023-10-26 13:01:13.203 
Epoch 230/1000 
	 loss: 206.5026, MinusLogProbMetric: 206.5026, val_loss: 206.1823, val_MinusLogProbMetric: 206.1823

Epoch 230: val_loss improved from 206.53717 to 206.18233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 206.5026 - MinusLogProbMetric: 206.5026 - val_loss: 206.1823 - val_MinusLogProbMetric: 206.1823 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 231/1000
2023-10-26 13:02:28.679 
Epoch 231/1000 
	 loss: 205.7141, MinusLogProbMetric: 205.7141, val_loss: 205.4543, val_MinusLogProbMetric: 205.4543

Epoch 231: val_loss improved from 206.18233 to 205.45425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 205.7141 - MinusLogProbMetric: 205.7141 - val_loss: 205.4543 - val_MinusLogProbMetric: 205.4543 - lr: 4.1152e-06 - 75s/epoch - 384ms/step
Epoch 232/1000
2023-10-26 13:03:37.044 
Epoch 232/1000 
	 loss: 205.1442, MinusLogProbMetric: 205.1442, val_loss: 205.1561, val_MinusLogProbMetric: 205.1561

Epoch 232: val_loss improved from 205.45425 to 205.15608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 205.1442 - MinusLogProbMetric: 205.1442 - val_loss: 205.1561 - val_MinusLogProbMetric: 205.1561 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 233/1000
2023-10-26 13:04:52.136 
Epoch 233/1000 
	 loss: 204.7789, MinusLogProbMetric: 204.7789, val_loss: 204.8920, val_MinusLogProbMetric: 204.8920

Epoch 233: val_loss improved from 205.15608 to 204.89203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 204.7789 - MinusLogProbMetric: 204.7789 - val_loss: 204.8920 - val_MinusLogProbMetric: 204.8920 - lr: 4.1152e-06 - 75s/epoch - 385ms/step
Epoch 234/1000
2023-10-26 13:06:03.444 
Epoch 234/1000 
	 loss: 204.5258, MinusLogProbMetric: 204.5258, val_loss: 204.6398, val_MinusLogProbMetric: 204.6398

Epoch 234: val_loss improved from 204.89203 to 204.63983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 204.5258 - MinusLogProbMetric: 204.5258 - val_loss: 204.6398 - val_MinusLogProbMetric: 204.6398 - lr: 4.1152e-06 - 71s/epoch - 363ms/step
Epoch 235/1000
2023-10-26 13:07:12.868 
Epoch 235/1000 
	 loss: 204.2329, MinusLogProbMetric: 204.2329, val_loss: 204.4455, val_MinusLogProbMetric: 204.4455

Epoch 235: val_loss improved from 204.63983 to 204.44547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 204.2329 - MinusLogProbMetric: 204.2329 - val_loss: 204.4455 - val_MinusLogProbMetric: 204.4455 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 236/1000
2023-10-26 13:08:28.434 
Epoch 236/1000 
	 loss: 203.7806, MinusLogProbMetric: 203.7806, val_loss: 203.7380, val_MinusLogProbMetric: 203.7380

Epoch 236: val_loss improved from 204.44547 to 203.73796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 203.7806 - MinusLogProbMetric: 203.7806 - val_loss: 203.7380 - val_MinusLogProbMetric: 203.7380 - lr: 4.1152e-06 - 75s/epoch - 384ms/step
Epoch 237/1000
2023-10-26 13:09:35.835 
Epoch 237/1000 
	 loss: 203.6397, MinusLogProbMetric: 203.6397, val_loss: 203.5983, val_MinusLogProbMetric: 203.5983

Epoch 237: val_loss improved from 203.73796 to 203.59827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 67s - loss: 203.6397 - MinusLogProbMetric: 203.6397 - val_loss: 203.5983 - val_MinusLogProbMetric: 203.5983 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 238/1000
2023-10-26 13:10:45.000 
Epoch 238/1000 
	 loss: 203.2651, MinusLogProbMetric: 203.2651, val_loss: 203.1786, val_MinusLogProbMetric: 203.1786

Epoch 238: val_loss improved from 203.59827 to 203.17865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 203.2651 - MinusLogProbMetric: 203.2651 - val_loss: 203.1786 - val_MinusLogProbMetric: 203.1786 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 239/1000
2023-10-26 13:11:54.874 
Epoch 239/1000 
	 loss: 202.9933, MinusLogProbMetric: 202.9933, val_loss: 203.4130, val_MinusLogProbMetric: 203.4130

Epoch 239: val_loss did not improve from 203.17865
196/196 - 69s - loss: 202.9933 - MinusLogProbMetric: 202.9933 - val_loss: 203.4130 - val_MinusLogProbMetric: 203.4130 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 240/1000
2023-10-26 13:13:02.311 
Epoch 240/1000 
	 loss: 202.7805, MinusLogProbMetric: 202.7805, val_loss: 202.7131, val_MinusLogProbMetric: 202.7131

Epoch 240: val_loss improved from 203.17865 to 202.71313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 202.7805 - MinusLogProbMetric: 202.7805 - val_loss: 202.7131 - val_MinusLogProbMetric: 202.7131 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 241/1000
2023-10-26 13:14:18.680 
Epoch 241/1000 
	 loss: 202.5125, MinusLogProbMetric: 202.5125, val_loss: 202.5620, val_MinusLogProbMetric: 202.5620

Epoch 241: val_loss improved from 202.71313 to 202.56201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 76s - loss: 202.5125 - MinusLogProbMetric: 202.5125 - val_loss: 202.5620 - val_MinusLogProbMetric: 202.5620 - lr: 4.1152e-06 - 76s/epoch - 388ms/step
Epoch 242/1000
2023-10-26 13:15:27.318 
Epoch 242/1000 
	 loss: 202.1282, MinusLogProbMetric: 202.1282, val_loss: 202.0400, val_MinusLogProbMetric: 202.0400

Epoch 242: val_loss improved from 202.56201 to 202.04002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 202.1282 - MinusLogProbMetric: 202.1282 - val_loss: 202.0400 - val_MinusLogProbMetric: 202.0400 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 243/1000
2023-10-26 13:16:43.449 
Epoch 243/1000 
	 loss: 201.8813, MinusLogProbMetric: 201.8813, val_loss: 202.0379, val_MinusLogProbMetric: 202.0379

Epoch 243: val_loss improved from 202.04002 to 202.03786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 76s - loss: 201.8813 - MinusLogProbMetric: 201.8813 - val_loss: 202.0379 - val_MinusLogProbMetric: 202.0379 - lr: 4.1152e-06 - 76s/epoch - 389ms/step
Epoch 244/1000
2023-10-26 13:17:52.901 
Epoch 244/1000 
	 loss: 201.6468, MinusLogProbMetric: 201.6468, val_loss: 201.5499, val_MinusLogProbMetric: 201.5499

Epoch 244: val_loss improved from 202.03786 to 201.54987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 201.6468 - MinusLogProbMetric: 201.6468 - val_loss: 201.5499 - val_MinusLogProbMetric: 201.5499 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 245/1000
2023-10-26 13:19:03.724 
Epoch 245/1000 
	 loss: 201.3823, MinusLogProbMetric: 201.3823, val_loss: 202.5005, val_MinusLogProbMetric: 202.5005

Epoch 245: val_loss did not improve from 201.54987
196/196 - 70s - loss: 201.3823 - MinusLogProbMetric: 201.3823 - val_loss: 202.5005 - val_MinusLogProbMetric: 202.5005 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 246/1000
2023-10-26 13:20:17.215 
Epoch 246/1000 
	 loss: 201.2041, MinusLogProbMetric: 201.2041, val_loss: 201.2838, val_MinusLogProbMetric: 201.2838

Epoch 246: val_loss improved from 201.54987 to 201.28378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 201.2041 - MinusLogProbMetric: 201.2041 - val_loss: 201.2838 - val_MinusLogProbMetric: 201.2838 - lr: 4.1152e-06 - 75s/epoch - 381ms/step
Epoch 247/1000
2023-10-26 13:21:26.507 
Epoch 247/1000 
	 loss: 200.9143, MinusLogProbMetric: 200.9143, val_loss: 200.9858, val_MinusLogProbMetric: 200.9858

Epoch 247: val_loss improved from 201.28378 to 200.98584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 200.9143 - MinusLogProbMetric: 200.9143 - val_loss: 200.9858 - val_MinusLogProbMetric: 200.9858 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 248/1000
2023-10-26 13:22:40.436 
Epoch 248/1000 
	 loss: 200.6402, MinusLogProbMetric: 200.6402, val_loss: 200.4936, val_MinusLogProbMetric: 200.4936

Epoch 248: val_loss improved from 200.98584 to 200.49362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 74s - loss: 200.6402 - MinusLogProbMetric: 200.6402 - val_loss: 200.4936 - val_MinusLogProbMetric: 200.4936 - lr: 4.1152e-06 - 74s/epoch - 378ms/step
Epoch 249/1000
2023-10-26 13:23:51.224 
Epoch 249/1000 
	 loss: 200.4053, MinusLogProbMetric: 200.4053, val_loss: 200.4919, val_MinusLogProbMetric: 200.4919

Epoch 249: val_loss improved from 200.49362 to 200.49187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 200.4053 - MinusLogProbMetric: 200.4053 - val_loss: 200.4919 - val_MinusLogProbMetric: 200.4919 - lr: 4.1152e-06 - 71s/epoch - 361ms/step
Epoch 250/1000
2023-10-26 13:25:00.326 
Epoch 250/1000 
	 loss: 200.0623, MinusLogProbMetric: 200.0623, val_loss: 200.0653, val_MinusLogProbMetric: 200.0653

Epoch 250: val_loss improved from 200.49187 to 200.06528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 200.0623 - MinusLogProbMetric: 200.0623 - val_loss: 200.0653 - val_MinusLogProbMetric: 200.0653 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 251/1000
2023-10-26 13:26:15.580 
Epoch 251/1000 
	 loss: 199.8538, MinusLogProbMetric: 199.8538, val_loss: 199.8695, val_MinusLogProbMetric: 199.8695

Epoch 251: val_loss improved from 200.06528 to 199.86952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 199.8538 - MinusLogProbMetric: 199.8538 - val_loss: 199.8695 - val_MinusLogProbMetric: 199.8695 - lr: 4.1152e-06 - 75s/epoch - 384ms/step
Epoch 252/1000
2023-10-26 13:27:23.626 
Epoch 252/1000 
	 loss: 199.5254, MinusLogProbMetric: 199.5254, val_loss: 199.4807, val_MinusLogProbMetric: 199.4807

Epoch 252: val_loss improved from 199.86952 to 199.48068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 199.5254 - MinusLogProbMetric: 199.5254 - val_loss: 199.4807 - val_MinusLogProbMetric: 199.4807 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 253/1000
2023-10-26 13:28:37.777 
Epoch 253/1000 
	 loss: 199.3035, MinusLogProbMetric: 199.3035, val_loss: 199.2661, val_MinusLogProbMetric: 199.2661

Epoch 253: val_loss improved from 199.48068 to 199.26610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 74s - loss: 199.3035 - MinusLogProbMetric: 199.3035 - val_loss: 199.2661 - val_MinusLogProbMetric: 199.2661 - lr: 4.1152e-06 - 74s/epoch - 379ms/step
Epoch 254/1000
2023-10-26 13:29:49.385 
Epoch 254/1000 
	 loss: 199.0208, MinusLogProbMetric: 199.0208, val_loss: 199.0875, val_MinusLogProbMetric: 199.0875

Epoch 254: val_loss improved from 199.26610 to 199.08754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 72s - loss: 199.0208 - MinusLogProbMetric: 199.0208 - val_loss: 199.0875 - val_MinusLogProbMetric: 199.0875 - lr: 4.1152e-06 - 72s/epoch - 365ms/step
Epoch 255/1000
2023-10-26 13:30:57.222 
Epoch 255/1000 
	 loss: 198.8075, MinusLogProbMetric: 198.8075, val_loss: 199.0396, val_MinusLogProbMetric: 199.0396

Epoch 255: val_loss improved from 199.08754 to 199.03961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 198.8075 - MinusLogProbMetric: 198.8075 - val_loss: 199.0396 - val_MinusLogProbMetric: 199.0396 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 256/1000
2023-10-26 13:32:12.503 
Epoch 256/1000 
	 loss: 198.6005, MinusLogProbMetric: 198.6005, val_loss: 199.1908, val_MinusLogProbMetric: 199.1908

Epoch 256: val_loss did not improve from 199.03961
196/196 - 74s - loss: 198.6005 - MinusLogProbMetric: 198.6005 - val_loss: 199.1908 - val_MinusLogProbMetric: 199.1908 - lr: 4.1152e-06 - 74s/epoch - 378ms/step
Epoch 257/1000
2023-10-26 13:33:19.037 
Epoch 257/1000 
	 loss: 198.2483, MinusLogProbMetric: 198.2483, val_loss: 199.0082, val_MinusLogProbMetric: 199.0082

Epoch 257: val_loss improved from 199.03961 to 199.00816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 198.2483 - MinusLogProbMetric: 198.2483 - val_loss: 199.0082 - val_MinusLogProbMetric: 199.0082 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 258/1000
2023-10-26 13:34:29.061 
Epoch 258/1000 
	 loss: 197.9856, MinusLogProbMetric: 197.9856, val_loss: 198.2829, val_MinusLogProbMetric: 198.2829

Epoch 258: val_loss improved from 199.00816 to 198.28285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 197.9856 - MinusLogProbMetric: 197.9856 - val_loss: 198.2829 - val_MinusLogProbMetric: 198.2829 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 259/1000
2023-10-26 13:35:42.992 
Epoch 259/1000 
	 loss: 197.7139, MinusLogProbMetric: 197.7139, val_loss: 198.7470, val_MinusLogProbMetric: 198.7470

Epoch 259: val_loss did not improve from 198.28285
196/196 - 73s - loss: 197.7139 - MinusLogProbMetric: 197.7139 - val_loss: 198.7470 - val_MinusLogProbMetric: 198.7470 - lr: 4.1152e-06 - 73s/epoch - 370ms/step
Epoch 260/1000
2023-10-26 13:36:50.229 
Epoch 260/1000 
	 loss: 197.5107, MinusLogProbMetric: 197.5107, val_loss: 197.6396, val_MinusLogProbMetric: 197.6396

Epoch 260: val_loss improved from 198.28285 to 197.63959, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 197.5107 - MinusLogProbMetric: 197.5107 - val_loss: 197.6396 - val_MinusLogProbMetric: 197.6396 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 261/1000
2023-10-26 13:38:06.264 
Epoch 261/1000 
	 loss: 197.2061, MinusLogProbMetric: 197.2061, val_loss: 197.3889, val_MinusLogProbMetric: 197.3889

Epoch 261: val_loss improved from 197.63959 to 197.38895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 76s - loss: 197.2061 - MinusLogProbMetric: 197.2061 - val_loss: 197.3889 - val_MinusLogProbMetric: 197.3889 - lr: 4.1152e-06 - 76s/epoch - 389ms/step
Epoch 262/1000
2023-10-26 13:39:18.885 
Epoch 262/1000 
	 loss: 197.0095, MinusLogProbMetric: 197.0095, val_loss: 197.6583, val_MinusLogProbMetric: 197.6583

Epoch 262: val_loss did not improve from 197.38895
196/196 - 71s - loss: 197.0095 - MinusLogProbMetric: 197.0095 - val_loss: 197.6583 - val_MinusLogProbMetric: 197.6583 - lr: 4.1152e-06 - 71s/epoch - 364ms/step
Epoch 263/1000
2023-10-26 13:40:27.981 
Epoch 263/1000 
	 loss: 196.7626, MinusLogProbMetric: 196.7626, val_loss: 197.0249, val_MinusLogProbMetric: 197.0249

Epoch 263: val_loss improved from 197.38895 to 197.02493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 196.7626 - MinusLogProbMetric: 196.7626 - val_loss: 197.0249 - val_MinusLogProbMetric: 197.0249 - lr: 4.1152e-06 - 71s/epoch - 360ms/step
Epoch 264/1000
2023-10-26 13:41:47.870 
Epoch 264/1000 
	 loss: 196.5378, MinusLogProbMetric: 196.5378, val_loss: 196.8388, val_MinusLogProbMetric: 196.8388

Epoch 264: val_loss improved from 197.02493 to 196.83882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 196.5378 - MinusLogProbMetric: 196.5378 - val_loss: 196.8388 - val_MinusLogProbMetric: 196.8388 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 265/1000
2023-10-26 13:42:58.059 
Epoch 265/1000 
	 loss: 196.3223, MinusLogProbMetric: 196.3223, val_loss: 196.3527, val_MinusLogProbMetric: 196.3527

Epoch 265: val_loss improved from 196.83882 to 196.35272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 196.3223 - MinusLogProbMetric: 196.3223 - val_loss: 196.3527 - val_MinusLogProbMetric: 196.3527 - lr: 4.1152e-06 - 70s/epoch - 358ms/step
Epoch 266/1000
2023-10-26 13:44:11.076 
Epoch 266/1000 
	 loss: 196.0657, MinusLogProbMetric: 196.0657, val_loss: 196.0261, val_MinusLogProbMetric: 196.0261

Epoch 266: val_loss improved from 196.35272 to 196.02609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 73s - loss: 196.0657 - MinusLogProbMetric: 196.0657 - val_loss: 196.0261 - val_MinusLogProbMetric: 196.0261 - lr: 4.1152e-06 - 73s/epoch - 374ms/step
Epoch 267/1000
2023-10-26 13:45:27.353 
Epoch 267/1000 
	 loss: 195.8221, MinusLogProbMetric: 195.8221, val_loss: 195.9707, val_MinusLogProbMetric: 195.9707

Epoch 267: val_loss improved from 196.02609 to 195.97075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 76s - loss: 195.8221 - MinusLogProbMetric: 195.8221 - val_loss: 195.9707 - val_MinusLogProbMetric: 195.9707 - lr: 4.1152e-06 - 76s/epoch - 389ms/step
Epoch 268/1000
2023-10-26 13:46:37.076 
Epoch 268/1000 
	 loss: 195.5388, MinusLogProbMetric: 195.5388, val_loss: 195.5513, val_MinusLogProbMetric: 195.5513

Epoch 268: val_loss improved from 195.97075 to 195.55130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 195.5388 - MinusLogProbMetric: 195.5388 - val_loss: 195.5513 - val_MinusLogProbMetric: 195.5513 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 269/1000
2023-10-26 13:47:55.997 
Epoch 269/1000 
	 loss: 195.2663, MinusLogProbMetric: 195.2663, val_loss: 195.5012, val_MinusLogProbMetric: 195.5012

Epoch 269: val_loss improved from 195.55130 to 195.50119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 79s - loss: 195.2663 - MinusLogProbMetric: 195.2663 - val_loss: 195.5012 - val_MinusLogProbMetric: 195.5012 - lr: 4.1152e-06 - 79s/epoch - 403ms/step
Epoch 270/1000
2023-10-26 13:49:08.290 
Epoch 270/1000 
	 loss: 195.1069, MinusLogProbMetric: 195.1069, val_loss: 195.0034, val_MinusLogProbMetric: 195.0034

Epoch 270: val_loss improved from 195.50119 to 195.00340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 72s - loss: 195.1069 - MinusLogProbMetric: 195.1069 - val_loss: 195.0034 - val_MinusLogProbMetric: 195.0034 - lr: 4.1152e-06 - 72s/epoch - 368ms/step
Epoch 271/1000
2023-10-26 13:50:17.808 
Epoch 271/1000 
	 loss: 194.8215, MinusLogProbMetric: 194.8215, val_loss: 195.0374, val_MinusLogProbMetric: 195.0374

Epoch 271: val_loss did not improve from 195.00340
196/196 - 68s - loss: 194.8215 - MinusLogProbMetric: 194.8215 - val_loss: 195.0374 - val_MinusLogProbMetric: 195.0374 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 272/1000
2023-10-26 13:51:34.241 
Epoch 272/1000 
	 loss: 194.5771, MinusLogProbMetric: 194.5771, val_loss: 194.7591, val_MinusLogProbMetric: 194.7591

Epoch 272: val_loss improved from 195.00340 to 194.75914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 78s - loss: 194.5771 - MinusLogProbMetric: 194.5771 - val_loss: 194.7591 - val_MinusLogProbMetric: 194.7591 - lr: 4.1152e-06 - 78s/epoch - 397ms/step
Epoch 273/1000
2023-10-26 13:52:45.024 
Epoch 273/1000 
	 loss: 194.4247, MinusLogProbMetric: 194.4247, val_loss: 195.1593, val_MinusLogProbMetric: 195.1593

Epoch 273: val_loss did not improve from 194.75914
196/196 - 69s - loss: 194.4247 - MinusLogProbMetric: 194.4247 - val_loss: 195.1593 - val_MinusLogProbMetric: 195.1593 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 274/1000
2023-10-26 13:53:52.570 
Epoch 274/1000 
	 loss: 194.2056, MinusLogProbMetric: 194.2056, val_loss: 194.0500, val_MinusLogProbMetric: 194.0500

Epoch 274: val_loss improved from 194.75914 to 194.04997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 194.2056 - MinusLogProbMetric: 194.2056 - val_loss: 194.0500 - val_MinusLogProbMetric: 194.0500 - lr: 4.1152e-06 - 69s/epoch - 351ms/step
Epoch 275/1000
2023-10-26 13:55:12.946 
Epoch 275/1000 
	 loss: 193.9372, MinusLogProbMetric: 193.9372, val_loss: 194.5863, val_MinusLogProbMetric: 194.5863

Epoch 275: val_loss did not improve from 194.04997
196/196 - 79s - loss: 193.9372 - MinusLogProbMetric: 193.9372 - val_loss: 194.5863 - val_MinusLogProbMetric: 194.5863 - lr: 4.1152e-06 - 79s/epoch - 404ms/step
Epoch 276/1000
2023-10-26 13:56:21.272 
Epoch 276/1000 
	 loss: 193.7463, MinusLogProbMetric: 193.7463, val_loss: 194.0991, val_MinusLogProbMetric: 194.0991

Epoch 276: val_loss did not improve from 194.04997
196/196 - 68s - loss: 193.7463 - MinusLogProbMetric: 193.7463 - val_loss: 194.0991 - val_MinusLogProbMetric: 194.0991 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 277/1000
2023-10-26 13:57:34.385 
Epoch 277/1000 
	 loss: 193.6014, MinusLogProbMetric: 193.6014, val_loss: 193.6009, val_MinusLogProbMetric: 193.6009

Epoch 277: val_loss improved from 194.04997 to 193.60091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 74s - loss: 193.6014 - MinusLogProbMetric: 193.6014 - val_loss: 193.6009 - val_MinusLogProbMetric: 193.6009 - lr: 4.1152e-06 - 74s/epoch - 380ms/step
Epoch 278/1000
2023-10-26 13:58:51.423 
Epoch 278/1000 
	 loss: 193.3541, MinusLogProbMetric: 193.3541, val_loss: 193.3874, val_MinusLogProbMetric: 193.3874

Epoch 278: val_loss improved from 193.60091 to 193.38736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 77s - loss: 193.3541 - MinusLogProbMetric: 193.3541 - val_loss: 193.3874 - val_MinusLogProbMetric: 193.3874 - lr: 4.1152e-06 - 77s/epoch - 393ms/step
Epoch 279/1000
2023-10-26 14:00:05.001 
Epoch 279/1000 
	 loss: 193.0505, MinusLogProbMetric: 193.0505, val_loss: 193.1262, val_MinusLogProbMetric: 193.1262

Epoch 279: val_loss improved from 193.38736 to 193.12619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 73s - loss: 193.0505 - MinusLogProbMetric: 193.0505 - val_loss: 193.1262 - val_MinusLogProbMetric: 193.1262 - lr: 4.1152e-06 - 73s/epoch - 374ms/step
Epoch 280/1000
2023-10-26 14:01:17.447 
Epoch 280/1000 
	 loss: 192.9560, MinusLogProbMetric: 192.9560, val_loss: 193.1742, val_MinusLogProbMetric: 193.1742

Epoch 280: val_loss did not improve from 193.12619
196/196 - 71s - loss: 192.9560 - MinusLogProbMetric: 192.9560 - val_loss: 193.1742 - val_MinusLogProbMetric: 193.1742 - lr: 4.1152e-06 - 71s/epoch - 364ms/step
Epoch 281/1000
2023-10-26 14:02:32.539 
Epoch 281/1000 
	 loss: 192.6933, MinusLogProbMetric: 192.6933, val_loss: 193.2639, val_MinusLogProbMetric: 193.2639

Epoch 281: val_loss did not improve from 193.12619
196/196 - 75s - loss: 192.6933 - MinusLogProbMetric: 192.6933 - val_loss: 193.2639 - val_MinusLogProbMetric: 193.2639 - lr: 4.1152e-06 - 75s/epoch - 383ms/step
Epoch 282/1000
2023-10-26 14:03:52.852 
Epoch 282/1000 
	 loss: 192.4908, MinusLogProbMetric: 192.4908, val_loss: 192.3024, val_MinusLogProbMetric: 192.3024

Epoch 282: val_loss improved from 193.12619 to 192.30244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 192.4908 - MinusLogProbMetric: 192.4908 - val_loss: 192.3024 - val_MinusLogProbMetric: 192.3024 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 283/1000
2023-10-26 14:05:14.308 
Epoch 283/1000 
	 loss: 192.3314, MinusLogProbMetric: 192.3314, val_loss: 192.5654, val_MinusLogProbMetric: 192.5654

Epoch 283: val_loss did not improve from 192.30244
196/196 - 80s - loss: 192.3314 - MinusLogProbMetric: 192.3314 - val_loss: 192.5654 - val_MinusLogProbMetric: 192.5654 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 284/1000
2023-10-26 14:06:34.940 
Epoch 284/1000 
	 loss: 192.1412, MinusLogProbMetric: 192.1412, val_loss: 192.1121, val_MinusLogProbMetric: 192.1121

Epoch 284: val_loss improved from 192.30244 to 192.11214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 192.1412 - MinusLogProbMetric: 192.1412 - val_loss: 192.1121 - val_MinusLogProbMetric: 192.1121 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 285/1000
2023-10-26 14:07:56.372 
Epoch 285/1000 
	 loss: 191.7981, MinusLogProbMetric: 191.7981, val_loss: 191.8562, val_MinusLogProbMetric: 191.8562

Epoch 285: val_loss improved from 192.11214 to 191.85622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 191.7981 - MinusLogProbMetric: 191.7981 - val_loss: 191.8562 - val_MinusLogProbMetric: 191.8562 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 286/1000
2023-10-26 14:09:18.572 
Epoch 286/1000 
	 loss: 191.5144, MinusLogProbMetric: 191.5144, val_loss: 191.7036, val_MinusLogProbMetric: 191.7036

Epoch 286: val_loss improved from 191.85622 to 191.70358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 191.5144 - MinusLogProbMetric: 191.5144 - val_loss: 191.7036 - val_MinusLogProbMetric: 191.7036 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 287/1000
2023-10-26 14:10:26.783 
Epoch 287/1000 
	 loss: 191.3599, MinusLogProbMetric: 191.3599, val_loss: 191.4770, val_MinusLogProbMetric: 191.4770

Epoch 287: val_loss improved from 191.70358 to 191.47699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 68s - loss: 191.3599 - MinusLogProbMetric: 191.3599 - val_loss: 191.4770 - val_MinusLogProbMetric: 191.4770 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 288/1000
2023-10-26 14:11:36.474 
Epoch 288/1000 
	 loss: 191.1316, MinusLogProbMetric: 191.1316, val_loss: 191.5106, val_MinusLogProbMetric: 191.5106

Epoch 288: val_loss did not improve from 191.47699
196/196 - 69s - loss: 191.1316 - MinusLogProbMetric: 191.1316 - val_loss: 191.5106 - val_MinusLogProbMetric: 191.5106 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 289/1000
2023-10-26 14:12:56.132 
Epoch 289/1000 
	 loss: 191.0358, MinusLogProbMetric: 191.0358, val_loss: 190.9362, val_MinusLogProbMetric: 190.9362

Epoch 289: val_loss improved from 191.47699 to 190.93616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 191.0358 - MinusLogProbMetric: 191.0358 - val_loss: 190.9362 - val_MinusLogProbMetric: 190.9362 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 290/1000
2023-10-26 14:14:17.058 
Epoch 290/1000 
	 loss: 190.6259, MinusLogProbMetric: 190.6259, val_loss: 190.6885, val_MinusLogProbMetric: 190.6885

Epoch 290: val_loss improved from 190.93616 to 190.68851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 190.6259 - MinusLogProbMetric: 190.6259 - val_loss: 190.6885 - val_MinusLogProbMetric: 190.6885 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 291/1000
2023-10-26 14:15:37.528 
Epoch 291/1000 
	 loss: 190.3952, MinusLogProbMetric: 190.3952, val_loss: 190.6004, val_MinusLogProbMetric: 190.6004

Epoch 291: val_loss improved from 190.68851 to 190.60040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 190.3952 - MinusLogProbMetric: 190.3952 - val_loss: 190.6004 - val_MinusLogProbMetric: 190.6004 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 292/1000
2023-10-26 14:16:59.425 
Epoch 292/1000 
	 loss: 190.0851, MinusLogProbMetric: 190.0851, val_loss: 190.3947, val_MinusLogProbMetric: 190.3947

Epoch 292: val_loss improved from 190.60040 to 190.39473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 190.0851 - MinusLogProbMetric: 190.0851 - val_loss: 190.3947 - val_MinusLogProbMetric: 190.3947 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 293/1000
2023-10-26 14:18:15.429 
Epoch 293/1000 
	 loss: 189.9198, MinusLogProbMetric: 189.9198, val_loss: 190.5886, val_MinusLogProbMetric: 190.5886

Epoch 293: val_loss did not improve from 190.39473
196/196 - 75s - loss: 189.9198 - MinusLogProbMetric: 189.9198 - val_loss: 190.5886 - val_MinusLogProbMetric: 190.5886 - lr: 4.1152e-06 - 75s/epoch - 380ms/step
Epoch 294/1000
2023-10-26 14:19:19.376 
Epoch 294/1000 
	 loss: 189.6863, MinusLogProbMetric: 189.6863, val_loss: 189.7816, val_MinusLogProbMetric: 189.7816

Epoch 294: val_loss improved from 190.39473 to 189.78160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 65s - loss: 189.6863 - MinusLogProbMetric: 189.6863 - val_loss: 189.7816 - val_MinusLogProbMetric: 189.7816 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 295/1000
2023-10-26 14:20:40.191 
Epoch 295/1000 
	 loss: 189.5090, MinusLogProbMetric: 189.5090, val_loss: 189.8085, val_MinusLogProbMetric: 189.8085

Epoch 295: val_loss did not improve from 189.78160
196/196 - 79s - loss: 189.5090 - MinusLogProbMetric: 189.5090 - val_loss: 189.8085 - val_MinusLogProbMetric: 189.8085 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 296/1000
2023-10-26 14:21:58.846 
Epoch 296/1000 
	 loss: 189.4284, MinusLogProbMetric: 189.4284, val_loss: 189.2745, val_MinusLogProbMetric: 189.2745

Epoch 296: val_loss improved from 189.78160 to 189.27451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 189.4284 - MinusLogProbMetric: 189.4284 - val_loss: 189.2745 - val_MinusLogProbMetric: 189.2745 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 297/1000
2023-10-26 14:23:20.747 
Epoch 297/1000 
	 loss: 188.9574, MinusLogProbMetric: 188.9574, val_loss: 188.9045, val_MinusLogProbMetric: 188.9045

Epoch 297: val_loss improved from 189.27451 to 188.90450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 188.9574 - MinusLogProbMetric: 188.9574 - val_loss: 188.9045 - val_MinusLogProbMetric: 188.9045 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 298/1000
2023-10-26 14:24:43.273 
Epoch 298/1000 
	 loss: 188.6910, MinusLogProbMetric: 188.6910, val_loss: 188.5396, val_MinusLogProbMetric: 188.5396

Epoch 298: val_loss improved from 188.90450 to 188.53955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 188.6910 - MinusLogProbMetric: 188.6910 - val_loss: 188.5396 - val_MinusLogProbMetric: 188.5396 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 299/1000
2023-10-26 14:26:03.453 
Epoch 299/1000 
	 loss: 188.3900, MinusLogProbMetric: 188.3900, val_loss: 188.4846, val_MinusLogProbMetric: 188.4846

Epoch 299: val_loss improved from 188.53955 to 188.48459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 188.3900 - MinusLogProbMetric: 188.3900 - val_loss: 188.4846 - val_MinusLogProbMetric: 188.4846 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 300/1000
2023-10-26 14:27:25.300 
Epoch 300/1000 
	 loss: 188.0198, MinusLogProbMetric: 188.0198, val_loss: 187.9851, val_MinusLogProbMetric: 187.9851

Epoch 300: val_loss improved from 188.48459 to 187.98508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 188.0198 - MinusLogProbMetric: 188.0198 - val_loss: 187.9851 - val_MinusLogProbMetric: 187.9851 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 301/1000
2023-10-26 14:28:46.222 
Epoch 301/1000 
	 loss: 187.7585, MinusLogProbMetric: 187.7585, val_loss: 187.7847, val_MinusLogProbMetric: 187.7847

Epoch 301: val_loss improved from 187.98508 to 187.78468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 187.7585 - MinusLogProbMetric: 187.7585 - val_loss: 187.7847 - val_MinusLogProbMetric: 187.7847 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 302/1000
2023-10-26 14:30:08.140 
Epoch 302/1000 
	 loss: 187.4780, MinusLogProbMetric: 187.4780, val_loss: 187.7858, val_MinusLogProbMetric: 187.7858

Epoch 302: val_loss did not improve from 187.78468
196/196 - 80s - loss: 187.4780 - MinusLogProbMetric: 187.4780 - val_loss: 187.7858 - val_MinusLogProbMetric: 187.7858 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 303/1000
2023-10-26 14:31:28.856 
Epoch 303/1000 
	 loss: 187.2300, MinusLogProbMetric: 187.2300, val_loss: 188.0073, val_MinusLogProbMetric: 188.0073

Epoch 303: val_loss did not improve from 187.78468
196/196 - 81s - loss: 187.2300 - MinusLogProbMetric: 187.2300 - val_loss: 188.0073 - val_MinusLogProbMetric: 188.0073 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 304/1000
2023-10-26 14:32:47.889 
Epoch 304/1000 
	 loss: 187.1508, MinusLogProbMetric: 187.1508, val_loss: 187.3167, val_MinusLogProbMetric: 187.3167

Epoch 304: val_loss improved from 187.78468 to 187.31668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 187.1508 - MinusLogProbMetric: 187.1508 - val_loss: 187.3167 - val_MinusLogProbMetric: 187.3167 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 305/1000
2023-10-26 14:34:09.638 
Epoch 305/1000 
	 loss: 186.8697, MinusLogProbMetric: 186.8697, val_loss: 187.2317, val_MinusLogProbMetric: 187.2317

Epoch 305: val_loss improved from 187.31668 to 187.23172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 186.8697 - MinusLogProbMetric: 186.8697 - val_loss: 187.2317 - val_MinusLogProbMetric: 187.2317 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 306/1000
2023-10-26 14:35:30.022 
Epoch 306/1000 
	 loss: 186.6870, MinusLogProbMetric: 186.6870, val_loss: 186.9108, val_MinusLogProbMetric: 186.9108

Epoch 306: val_loss improved from 187.23172 to 186.91075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 186.6870 - MinusLogProbMetric: 186.6870 - val_loss: 186.9108 - val_MinusLogProbMetric: 186.9108 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 307/1000
2023-10-26 14:36:50.661 
Epoch 307/1000 
	 loss: 186.3548, MinusLogProbMetric: 186.3548, val_loss: 186.8750, val_MinusLogProbMetric: 186.8750

Epoch 307: val_loss improved from 186.91075 to 186.87502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 186.3548 - MinusLogProbMetric: 186.3548 - val_loss: 186.8750 - val_MinusLogProbMetric: 186.8750 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 308/1000
2023-10-26 14:38:11.736 
Epoch 308/1000 
	 loss: 185.9011, MinusLogProbMetric: 185.9011, val_loss: 186.1267, val_MinusLogProbMetric: 186.1267

Epoch 308: val_loss improved from 186.87502 to 186.12674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 185.9011 - MinusLogProbMetric: 185.9011 - val_loss: 186.1267 - val_MinusLogProbMetric: 186.1267 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 309/1000
2023-10-26 14:39:33.059 
Epoch 309/1000 
	 loss: 185.7086, MinusLogProbMetric: 185.7086, val_loss: 185.7520, val_MinusLogProbMetric: 185.7520

Epoch 309: val_loss improved from 186.12674 to 185.75195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 185.7086 - MinusLogProbMetric: 185.7086 - val_loss: 185.7520 - val_MinusLogProbMetric: 185.7520 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 310/1000
2023-10-26 14:40:53.610 
Epoch 310/1000 
	 loss: 185.4862, MinusLogProbMetric: 185.4862, val_loss: 185.5482, val_MinusLogProbMetric: 185.5482

Epoch 310: val_loss improved from 185.75195 to 185.54816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 185.4862 - MinusLogProbMetric: 185.4862 - val_loss: 185.5482 - val_MinusLogProbMetric: 185.5482 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 311/1000
2023-10-26 14:42:13.162 
Epoch 311/1000 
	 loss: 185.2316, MinusLogProbMetric: 185.2316, val_loss: 185.6578, val_MinusLogProbMetric: 185.6578

Epoch 311: val_loss did not improve from 185.54816
196/196 - 78s - loss: 185.2316 - MinusLogProbMetric: 185.2316 - val_loss: 185.6578 - val_MinusLogProbMetric: 185.6578 - lr: 4.1152e-06 - 78s/epoch - 398ms/step
Epoch 312/1000
2023-10-26 14:43:34.145 
Epoch 312/1000 
	 loss: 184.9645, MinusLogProbMetric: 184.9645, val_loss: 185.2955, val_MinusLogProbMetric: 185.2955

Epoch 312: val_loss improved from 185.54816 to 185.29546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 184.9645 - MinusLogProbMetric: 184.9645 - val_loss: 185.2955 - val_MinusLogProbMetric: 185.2955 - lr: 4.1152e-06 - 83s/epoch - 421ms/step
Epoch 313/1000
2023-10-26 14:44:54.461 
Epoch 313/1000 
	 loss: 184.7580, MinusLogProbMetric: 184.7580, val_loss: 185.2866, val_MinusLogProbMetric: 185.2866

Epoch 313: val_loss improved from 185.29546 to 185.28656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 184.7580 - MinusLogProbMetric: 184.7580 - val_loss: 185.2866 - val_MinusLogProbMetric: 185.2866 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 314/1000
2023-10-26 14:46:15.901 
Epoch 314/1000 
	 loss: 184.6758, MinusLogProbMetric: 184.6758, val_loss: 185.4802, val_MinusLogProbMetric: 185.4802

Epoch 314: val_loss did not improve from 185.28656
196/196 - 80s - loss: 184.6758 - MinusLogProbMetric: 184.6758 - val_loss: 185.4802 - val_MinusLogProbMetric: 185.4802 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 315/1000
2023-10-26 14:47:36.215 
Epoch 315/1000 
	 loss: 184.3616, MinusLogProbMetric: 184.3616, val_loss: 184.6909, val_MinusLogProbMetric: 184.6909

Epoch 315: val_loss improved from 185.28656 to 184.69095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 184.3616 - MinusLogProbMetric: 184.3616 - val_loss: 184.6909 - val_MinusLogProbMetric: 184.6909 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 316/1000
2023-10-26 14:48:57.859 
Epoch 316/1000 
	 loss: 184.0989, MinusLogProbMetric: 184.0989, val_loss: 184.4111, val_MinusLogProbMetric: 184.4111

Epoch 316: val_loss improved from 184.69095 to 184.41109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 184.0989 - MinusLogProbMetric: 184.0989 - val_loss: 184.4111 - val_MinusLogProbMetric: 184.4111 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 317/1000
2023-10-26 14:50:19.579 
Epoch 317/1000 
	 loss: 183.9308, MinusLogProbMetric: 183.9308, val_loss: 184.0753, val_MinusLogProbMetric: 184.0753

Epoch 317: val_loss improved from 184.41109 to 184.07526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 183.9308 - MinusLogProbMetric: 183.9308 - val_loss: 184.0753 - val_MinusLogProbMetric: 184.0753 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 318/1000
2023-10-26 14:51:41.454 
Epoch 318/1000 
	 loss: 183.8421, MinusLogProbMetric: 183.8421, val_loss: 184.0899, val_MinusLogProbMetric: 184.0899

Epoch 318: val_loss did not improve from 184.07526
196/196 - 80s - loss: 183.8421 - MinusLogProbMetric: 183.8421 - val_loss: 184.0899 - val_MinusLogProbMetric: 184.0899 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 319/1000
2023-10-26 14:53:01.671 
Epoch 319/1000 
	 loss: 183.5594, MinusLogProbMetric: 183.5594, val_loss: 184.0868, val_MinusLogProbMetric: 184.0868

Epoch 319: val_loss did not improve from 184.07526
196/196 - 80s - loss: 183.5594 - MinusLogProbMetric: 183.5594 - val_loss: 184.0868 - val_MinusLogProbMetric: 184.0868 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 320/1000
2023-10-26 14:54:22.939 
Epoch 320/1000 
	 loss: 183.3960, MinusLogProbMetric: 183.3960, val_loss: 183.6959, val_MinusLogProbMetric: 183.6959

Epoch 320: val_loss improved from 184.07526 to 183.69588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 183.3960 - MinusLogProbMetric: 183.3960 - val_loss: 183.6959 - val_MinusLogProbMetric: 183.6959 - lr: 4.1152e-06 - 83s/epoch - 421ms/step
Epoch 321/1000
2023-10-26 14:55:44.232 
Epoch 321/1000 
	 loss: 183.2766, MinusLogProbMetric: 183.2766, val_loss: 183.4539, val_MinusLogProbMetric: 183.4539

Epoch 321: val_loss improved from 183.69588 to 183.45390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 183.2766 - MinusLogProbMetric: 183.2766 - val_loss: 183.4539 - val_MinusLogProbMetric: 183.4539 - lr: 4.1152e-06 - 81s/epoch - 415ms/step
Epoch 322/1000
2023-10-26 14:57:05.470 
Epoch 322/1000 
	 loss: 183.1661, MinusLogProbMetric: 183.1661, val_loss: 183.4362, val_MinusLogProbMetric: 183.4362

Epoch 322: val_loss improved from 183.45390 to 183.43625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 81s - loss: 183.1661 - MinusLogProbMetric: 183.1661 - val_loss: 183.4362 - val_MinusLogProbMetric: 183.4362 - lr: 4.1152e-06 - 81s/epoch - 415ms/step
Epoch 323/1000
2023-10-26 14:58:27.913 
Epoch 323/1000 
	 loss: 182.8307, MinusLogProbMetric: 182.8307, val_loss: 183.0487, val_MinusLogProbMetric: 183.0487

Epoch 323: val_loss improved from 183.43625 to 183.04871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 182.8307 - MinusLogProbMetric: 182.8307 - val_loss: 183.0487 - val_MinusLogProbMetric: 183.0487 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 324/1000
2023-10-26 14:59:49.766 
Epoch 324/1000 
	 loss: 182.6096, MinusLogProbMetric: 182.6096, val_loss: 182.7927, val_MinusLogProbMetric: 182.7927

Epoch 324: val_loss improved from 183.04871 to 182.79266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 182.6096 - MinusLogProbMetric: 182.6096 - val_loss: 182.7927 - val_MinusLogProbMetric: 182.7927 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 325/1000
2023-10-26 15:01:12.181 
Epoch 325/1000 
	 loss: 182.4454, MinusLogProbMetric: 182.4454, val_loss: 182.7540, val_MinusLogProbMetric: 182.7540

Epoch 325: val_loss improved from 182.79266 to 182.75395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 182.4454 - MinusLogProbMetric: 182.4454 - val_loss: 182.7540 - val_MinusLogProbMetric: 182.7540 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 326/1000
2023-10-26 15:02:19.367 
Epoch 326/1000 
	 loss: 182.3236, MinusLogProbMetric: 182.3236, val_loss: 182.6946, val_MinusLogProbMetric: 182.6946

Epoch 326: val_loss improved from 182.75395 to 182.69463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 67s - loss: 182.3236 - MinusLogProbMetric: 182.3236 - val_loss: 182.6946 - val_MinusLogProbMetric: 182.6946 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 327/1000
2023-10-26 15:03:29.656 
Epoch 327/1000 
	 loss: 182.1179, MinusLogProbMetric: 182.1179, val_loss: 182.4422, val_MinusLogProbMetric: 182.4422

Epoch 327: val_loss improved from 182.69463 to 182.44220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 182.1179 - MinusLogProbMetric: 182.1179 - val_loss: 182.4422 - val_MinusLogProbMetric: 182.4422 - lr: 4.1152e-06 - 70s/epoch - 359ms/step
Epoch 328/1000
2023-10-26 15:04:49.444 
Epoch 328/1000 
	 loss: 182.0676, MinusLogProbMetric: 182.0676, val_loss: 182.3279, val_MinusLogProbMetric: 182.3279

Epoch 328: val_loss improved from 182.44220 to 182.32791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 182.0676 - MinusLogProbMetric: 182.0676 - val_loss: 182.3279 - val_MinusLogProbMetric: 182.3279 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 329/1000
2023-10-26 15:06:09.854 
Epoch 329/1000 
	 loss: 181.8161, MinusLogProbMetric: 181.8161, val_loss: 182.1848, val_MinusLogProbMetric: 182.1848

Epoch 329: val_loss improved from 182.32791 to 182.18480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 80s - loss: 181.8161 - MinusLogProbMetric: 181.8161 - val_loss: 182.1848 - val_MinusLogProbMetric: 182.1848 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 330/1000
2023-10-26 15:07:31.529 
Epoch 330/1000 
	 loss: 181.6004, MinusLogProbMetric: 181.6004, val_loss: 181.7153, val_MinusLogProbMetric: 181.7153

Epoch 330: val_loss improved from 182.18480 to 181.71527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 181.6004 - MinusLogProbMetric: 181.6004 - val_loss: 181.7153 - val_MinusLogProbMetric: 181.7153 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 331/1000
2023-10-26 15:08:53.256 
Epoch 331/1000 
	 loss: 181.3642, MinusLogProbMetric: 181.3642, val_loss: 181.6288, val_MinusLogProbMetric: 181.6288

Epoch 331: val_loss improved from 181.71527 to 181.62881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 181.3642 - MinusLogProbMetric: 181.3642 - val_loss: 181.6288 - val_MinusLogProbMetric: 181.6288 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 332/1000
2023-10-26 15:10:05.732 
Epoch 332/1000 
	 loss: 181.1034, MinusLogProbMetric: 181.1034, val_loss: 181.4763, val_MinusLogProbMetric: 181.4763

Epoch 332: val_loss improved from 181.62881 to 181.47633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 72s - loss: 181.1034 - MinusLogProbMetric: 181.1034 - val_loss: 181.4763 - val_MinusLogProbMetric: 181.4763 - lr: 4.1152e-06 - 72s/epoch - 370ms/step
Epoch 333/1000
2023-10-26 15:11:26.349 
Epoch 333/1000 
	 loss: 180.8733, MinusLogProbMetric: 180.8733, val_loss: 181.5925, val_MinusLogProbMetric: 181.5925

Epoch 333: val_loss did not improve from 181.47633
196/196 - 79s - loss: 180.8733 - MinusLogProbMetric: 180.8733 - val_loss: 181.5925 - val_MinusLogProbMetric: 181.5925 - lr: 4.1152e-06 - 79s/epoch - 404ms/step
Epoch 334/1000
2023-10-26 15:12:46.960 
Epoch 334/1000 
	 loss: 180.6582, MinusLogProbMetric: 180.6582, val_loss: 180.8840, val_MinusLogProbMetric: 180.8840

Epoch 334: val_loss improved from 181.47633 to 180.88397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 180.6582 - MinusLogProbMetric: 180.6582 - val_loss: 180.8840 - val_MinusLogProbMetric: 180.8840 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 335/1000
2023-10-26 15:14:09.987 
Epoch 335/1000 
	 loss: 180.3691, MinusLogProbMetric: 180.3691, val_loss: 180.5141, val_MinusLogProbMetric: 180.5141

Epoch 335: val_loss improved from 180.88397 to 180.51411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 180.3691 - MinusLogProbMetric: 180.3691 - val_loss: 180.5141 - val_MinusLogProbMetric: 180.5141 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 336/1000
2023-10-26 15:15:31.764 
Epoch 336/1000 
	 loss: 180.1662, MinusLogProbMetric: 180.1662, val_loss: 180.3049, val_MinusLogProbMetric: 180.3049

Epoch 336: val_loss improved from 180.51411 to 180.30490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 180.1662 - MinusLogProbMetric: 180.1662 - val_loss: 180.3049 - val_MinusLogProbMetric: 180.3049 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 337/1000
2023-10-26 15:16:53.870 
Epoch 337/1000 
	 loss: 179.8892, MinusLogProbMetric: 179.8892, val_loss: 180.3089, val_MinusLogProbMetric: 180.3089

Epoch 337: val_loss did not improve from 180.30490
196/196 - 81s - loss: 179.8892 - MinusLogProbMetric: 179.8892 - val_loss: 180.3089 - val_MinusLogProbMetric: 180.3089 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 338/1000
2023-10-26 15:18:14.898 
Epoch 338/1000 
	 loss: 179.7210, MinusLogProbMetric: 179.7210, val_loss: 180.4801, val_MinusLogProbMetric: 180.4801

Epoch 338: val_loss did not improve from 180.30490
196/196 - 81s - loss: 179.7210 - MinusLogProbMetric: 179.7210 - val_loss: 180.4801 - val_MinusLogProbMetric: 180.4801 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 339/1000
2023-10-26 15:19:35.304 
Epoch 339/1000 
	 loss: 179.5690, MinusLogProbMetric: 179.5690, val_loss: 180.1869, val_MinusLogProbMetric: 180.1869

Epoch 339: val_loss improved from 180.30490 to 180.18692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 82s - loss: 179.5690 - MinusLogProbMetric: 179.5690 - val_loss: 180.1869 - val_MinusLogProbMetric: 180.1869 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 340/1000
2023-10-26 15:20:58.016 
Epoch 340/1000 
	 loss: 179.6460, MinusLogProbMetric: 179.6460, val_loss: 179.9468, val_MinusLogProbMetric: 179.9468

Epoch 340: val_loss improved from 180.18692 to 179.94679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 179.6460 - MinusLogProbMetric: 179.6460 - val_loss: 179.9468 - val_MinusLogProbMetric: 179.9468 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 341/1000
2023-10-26 15:22:20.278 
Epoch 341/1000 
	 loss: 179.5025, MinusLogProbMetric: 179.5025, val_loss: 180.2959, val_MinusLogProbMetric: 180.2959

Epoch 341: val_loss did not improve from 179.94679
196/196 - 81s - loss: 179.5025 - MinusLogProbMetric: 179.5025 - val_loss: 180.2959 - val_MinusLogProbMetric: 180.2959 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 342/1000
2023-10-26 15:23:41.418 
Epoch 342/1000 
	 loss: 179.2334, MinusLogProbMetric: 179.2334, val_loss: 179.2811, val_MinusLogProbMetric: 179.2811

Epoch 342: val_loss improved from 179.94679 to 179.28114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 179.2334 - MinusLogProbMetric: 179.2334 - val_loss: 179.2811 - val_MinusLogProbMetric: 179.2811 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 343/1000
2023-10-26 15:25:04.568 
Epoch 343/1000 
	 loss: 178.9998, MinusLogProbMetric: 178.9998, val_loss: 179.8568, val_MinusLogProbMetric: 179.8568

Epoch 343: val_loss did not improve from 179.28114
196/196 - 82s - loss: 178.9998 - MinusLogProbMetric: 178.9998 - val_loss: 179.8568 - val_MinusLogProbMetric: 179.8568 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 344/1000
2023-10-26 15:26:22.547 
Epoch 344/1000 
	 loss: 178.7916, MinusLogProbMetric: 178.7916, val_loss: 179.5634, val_MinusLogProbMetric: 179.5634

Epoch 344: val_loss did not improve from 179.28114
196/196 - 78s - loss: 178.7916 - MinusLogProbMetric: 178.7916 - val_loss: 179.5634 - val_MinusLogProbMetric: 179.5634 - lr: 4.1152e-06 - 78s/epoch - 398ms/step
Epoch 345/1000
2023-10-26 15:27:25.240 
Epoch 345/1000 
	 loss: 178.6148, MinusLogProbMetric: 178.6148, val_loss: 179.0535, val_MinusLogProbMetric: 179.0535

Epoch 345: val_loss improved from 179.28114 to 179.05350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 64s - loss: 178.6148 - MinusLogProbMetric: 178.6148 - val_loss: 179.0535 - val_MinusLogProbMetric: 179.0535 - lr: 4.1152e-06 - 64s/epoch - 325ms/step
Epoch 346/1000
2023-10-26 15:28:41.665 
Epoch 346/1000 
	 loss: 178.4982, MinusLogProbMetric: 178.4982, val_loss: 178.6252, val_MinusLogProbMetric: 178.6252

Epoch 346: val_loss improved from 179.05350 to 178.62520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 77s - loss: 178.4982 - MinusLogProbMetric: 178.4982 - val_loss: 178.6252 - val_MinusLogProbMetric: 178.6252 - lr: 4.1152e-06 - 77s/epoch - 391ms/step
Epoch 347/1000
2023-10-26 15:29:58.433 
Epoch 347/1000 
	 loss: 178.1098, MinusLogProbMetric: 178.1098, val_loss: 178.3882, val_MinusLogProbMetric: 178.3882

Epoch 347: val_loss improved from 178.62520 to 178.38817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 76s - loss: 178.1098 - MinusLogProbMetric: 178.1098 - val_loss: 178.3882 - val_MinusLogProbMetric: 178.3882 - lr: 4.1152e-06 - 76s/epoch - 390ms/step
Epoch 348/1000
2023-10-26 15:31:02.952 
Epoch 348/1000 
	 loss: 177.9246, MinusLogProbMetric: 177.9246, val_loss: 181.0835, val_MinusLogProbMetric: 181.0835

Epoch 348: val_loss did not improve from 178.38817
196/196 - 63s - loss: 177.9246 - MinusLogProbMetric: 177.9246 - val_loss: 181.0835 - val_MinusLogProbMetric: 181.0835 - lr: 4.1152e-06 - 63s/epoch - 324ms/step
Epoch 349/1000
2023-10-26 15:32:16.014 
Epoch 349/1000 
	 loss: 177.7943, MinusLogProbMetric: 177.7943, val_loss: 178.0845, val_MinusLogProbMetric: 178.0845

Epoch 349: val_loss improved from 178.38817 to 178.08455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 74s - loss: 177.7943 - MinusLogProbMetric: 177.7943 - val_loss: 178.0845 - val_MinusLogProbMetric: 178.0845 - lr: 4.1152e-06 - 74s/epoch - 378ms/step
Epoch 350/1000
2023-10-26 15:33:20.446 
Epoch 350/1000 
	 loss: 177.8634, MinusLogProbMetric: 177.8634, val_loss: 179.1035, val_MinusLogProbMetric: 179.1035

Epoch 350: val_loss did not improve from 178.08455
196/196 - 63s - loss: 177.8634 - MinusLogProbMetric: 177.8634 - val_loss: 179.1035 - val_MinusLogProbMetric: 179.1035 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 351/1000
2023-10-26 15:34:34.373 
Epoch 351/1000 
	 loss: 178.2161, MinusLogProbMetric: 178.2161, val_loss: 178.3542, val_MinusLogProbMetric: 178.3542

Epoch 351: val_loss did not improve from 178.08455
196/196 - 74s - loss: 178.2161 - MinusLogProbMetric: 178.2161 - val_loss: 178.3542 - val_MinusLogProbMetric: 178.3542 - lr: 4.1152e-06 - 74s/epoch - 377ms/step
Epoch 352/1000
2023-10-26 15:35:39.554 
Epoch 352/1000 
	 loss: 177.9053, MinusLogProbMetric: 177.9053, val_loss: 177.9066, val_MinusLogProbMetric: 177.9066

Epoch 352: val_loss improved from 178.08455 to 177.90657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 66s - loss: 177.9053 - MinusLogProbMetric: 177.9053 - val_loss: 177.9066 - val_MinusLogProbMetric: 177.9066 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 353/1000
2023-10-26 15:36:52.713 
Epoch 353/1000 
	 loss: 177.5973, MinusLogProbMetric: 177.5973, val_loss: 177.9105, val_MinusLogProbMetric: 177.9105

Epoch 353: val_loss did not improve from 177.90657
196/196 - 72s - loss: 177.5973 - MinusLogProbMetric: 177.5973 - val_loss: 177.9105 - val_MinusLogProbMetric: 177.9105 - lr: 4.1152e-06 - 72s/epoch - 367ms/step
Epoch 354/1000
2023-10-26 15:37:59.875 
Epoch 354/1000 
	 loss: 177.4732, MinusLogProbMetric: 177.4732, val_loss: 178.7862, val_MinusLogProbMetric: 178.7862

Epoch 354: val_loss did not improve from 177.90657
196/196 - 67s - loss: 177.4732 - MinusLogProbMetric: 177.4732 - val_loss: 178.7862 - val_MinusLogProbMetric: 178.7862 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 355/1000
2023-10-26 15:39:10.838 
Epoch 355/1000 
	 loss: 177.2107, MinusLogProbMetric: 177.2107, val_loss: 177.3593, val_MinusLogProbMetric: 177.3593

Epoch 355: val_loss improved from 177.90657 to 177.35927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 72s - loss: 177.2107 - MinusLogProbMetric: 177.2107 - val_loss: 177.3593 - val_MinusLogProbMetric: 177.3593 - lr: 4.1152e-06 - 72s/epoch - 369ms/step
Epoch 356/1000
2023-10-26 15:40:22.518 
Epoch 356/1000 
	 loss: 176.9533, MinusLogProbMetric: 176.9533, val_loss: 177.5040, val_MinusLogProbMetric: 177.5040

Epoch 356: val_loss did not improve from 177.35927
196/196 - 70s - loss: 176.9533 - MinusLogProbMetric: 176.9533 - val_loss: 177.5040 - val_MinusLogProbMetric: 177.5040 - lr: 4.1152e-06 - 70s/epoch - 359ms/step
Epoch 357/1000
2023-10-26 15:41:33.558 
Epoch 357/1000 
	 loss: 176.6959, MinusLogProbMetric: 176.6959, val_loss: 177.0287, val_MinusLogProbMetric: 177.0287

Epoch 357: val_loss improved from 177.35927 to 177.02875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 72s - loss: 176.6959 - MinusLogProbMetric: 176.6959 - val_loss: 177.0287 - val_MinusLogProbMetric: 177.0287 - lr: 4.1152e-06 - 72s/epoch - 368ms/step
Epoch 358/1000
2023-10-26 15:42:44.511 
Epoch 358/1000 
	 loss: 176.4752, MinusLogProbMetric: 176.4752, val_loss: 176.6398, val_MinusLogProbMetric: 176.6398

Epoch 358: val_loss improved from 177.02875 to 176.63983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 176.4752 - MinusLogProbMetric: 176.4752 - val_loss: 176.6398 - val_MinusLogProbMetric: 176.6398 - lr: 4.1152e-06 - 71s/epoch - 364ms/step
Epoch 359/1000
2023-10-26 15:43:55.543 
Epoch 359/1000 
	 loss: 176.2545, MinusLogProbMetric: 176.2545, val_loss: 176.3329, val_MinusLogProbMetric: 176.3329

Epoch 359: val_loss improved from 176.63983 to 176.33290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 176.2545 - MinusLogProbMetric: 176.2545 - val_loss: 176.3329 - val_MinusLogProbMetric: 176.3329 - lr: 4.1152e-06 - 71s/epoch - 362ms/step
Epoch 360/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:45:05.573 
Epoch 360/1000 
	 loss: 176.1304, MinusLogProbMetric: 176.1304, val_loss: 176.2470, val_MinusLogProbMetric: 176.2471

Epoch 360: val_loss improved from 176.33290 to 176.24704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 176.1304 - MinusLogProbMetric: 176.1304 - val_loss: 176.2470 - val_MinusLogProbMetric: 176.2471 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 361/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:46:18.281 
Epoch 361/1000 
	 loss: 175.8848, MinusLogProbMetric: 175.8848, val_loss: 176.1669, val_MinusLogProbMetric: 176.1669

Epoch 361: val_loss improved from 176.24704 to 176.16685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 73s - loss: 175.8848 - MinusLogProbMetric: 175.8848 - val_loss: 176.1669 - val_MinusLogProbMetric: 176.1669 - lr: 4.1152e-06 - 73s/epoch - 371ms/step
Epoch 362/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:47:29.458 
Epoch 362/1000 
	 loss: 175.7687, MinusLogProbMetric: 175.7687, val_loss: 175.6704, val_MinusLogProbMetric: 175.6705

Epoch 362: val_loss improved from 176.16685 to 175.67044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 175.7687 - MinusLogProbMetric: 175.7687 - val_loss: 175.6704 - val_MinusLogProbMetric: 175.6705 - lr: 4.1152e-06 - 71s/epoch - 365ms/step
Epoch 363/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:48:43.498 
Epoch 363/1000 
	 loss: 175.5028, MinusLogProbMetric: 175.5028, val_loss: 175.7788, val_MinusLogProbMetric: 175.7789

Epoch 363: val_loss did not improve from 175.67044
196/196 - 73s - loss: 175.5028 - MinusLogProbMetric: 175.5028 - val_loss: 175.7788 - val_MinusLogProbMetric: 175.7789 - lr: 4.1152e-06 - 73s/epoch - 371ms/step
Epoch 364/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:49:54.631 
Epoch 364/1000 
	 loss: 175.4213, MinusLogProbMetric: 175.4213, val_loss: 175.5439, val_MinusLogProbMetric: 175.5439

Epoch 364: val_loss improved from 175.67044 to 175.54390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 73s - loss: 175.4213 - MinusLogProbMetric: 175.4213 - val_loss: 175.5439 - val_MinusLogProbMetric: 175.5439 - lr: 4.1152e-06 - 73s/epoch - 370ms/step
Epoch 365/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:51:05.029 
Epoch 365/1000 
	 loss: 175.2084, MinusLogProbMetric: 175.2084, val_loss: 175.4386, val_MinusLogProbMetric: 175.4386

Epoch 365: val_loss improved from 175.54390 to 175.43858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 70s - loss: 175.2084 - MinusLogProbMetric: 175.2084 - val_loss: 175.4386 - val_MinusLogProbMetric: 175.4386 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 366/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:52:20.119 
Epoch 366/1000 
	 loss: 175.0453, MinusLogProbMetric: 175.0453, val_loss: 175.1125, val_MinusLogProbMetric: 175.1126

Epoch 366: val_loss improved from 175.43858 to 175.11255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 75s - loss: 175.0453 - MinusLogProbMetric: 175.0453 - val_loss: 175.1125 - val_MinusLogProbMetric: 175.1126 - lr: 4.1152e-06 - 75s/epoch - 383ms/step
Epoch 367/1000
2023-10-26 15:53:35.386 
Epoch 367/1000 
	 loss: 174.8027, MinusLogProbMetric: 174.8027, val_loss: 174.9251, val_MinusLogProbMetric: 174.9251

Epoch 367: val_loss improved from 175.11255 to 174.92511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 76s - loss: 174.8027 - MinusLogProbMetric: 174.8027 - val_loss: 174.9251 - val_MinusLogProbMetric: 174.9251 - lr: 4.1152e-06 - 76s/epoch - 386ms/step
Epoch 368/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:54:49.330 
Epoch 368/1000 
	 loss: 174.7214, MinusLogProbMetric: 174.7214, val_loss: 175.1172, val_MinusLogProbMetric: 175.1173

Epoch 368: val_loss did not improve from 174.92511
196/196 - 72s - loss: 174.7214 - MinusLogProbMetric: 174.7214 - val_loss: 175.1172 - val_MinusLogProbMetric: 175.1173 - lr: 4.1152e-06 - 72s/epoch - 369ms/step
Epoch 369/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:56:11.014 
Epoch 369/1000 
	 loss: 174.6067, MinusLogProbMetric: 174.6067, val_loss: 174.6501, val_MinusLogProbMetric: 174.6501

Epoch 369: val_loss improved from 174.92511 to 174.65005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 83s - loss: 174.6067 - MinusLogProbMetric: 174.6067 - val_loss: 174.6501 - val_MinusLogProbMetric: 174.6501 - lr: 4.1152e-06 - 83s/epoch - 425ms/step
Epoch 370/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:57:35.868 
Epoch 370/1000 
	 loss: 174.3461, MinusLogProbMetric: 174.3461, val_loss: 174.6535, val_MinusLogProbMetric: 174.6536

Epoch 370: val_loss did not improve from 174.65005
196/196 - 83s - loss: 174.3461 - MinusLogProbMetric: 174.3461 - val_loss: 174.6535 - val_MinusLogProbMetric: 174.6536 - lr: 4.1152e-06 - 83s/epoch - 425ms/step
Epoch 371/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:58:45.949 
Epoch 371/1000 
	 loss: 174.1757, MinusLogProbMetric: 174.1757, val_loss: 174.3085, val_MinusLogProbMetric: 174.3086

Epoch 371: val_loss improved from 174.65005 to 174.30852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 71s - loss: 174.1757 - MinusLogProbMetric: 174.1757 - val_loss: 174.3085 - val_MinusLogProbMetric: 174.3086 - lr: 4.1152e-06 - 71s/epoch - 363ms/step
Epoch 372/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 15:59:55.124 
Epoch 372/1000 
	 loss: 174.0226, MinusLogProbMetric: 174.0226, val_loss: 174.2057, val_MinusLogProbMetric: 174.2057

Epoch 372: val_loss improved from 174.30852 to 174.20567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 69s - loss: 174.0226 - MinusLogProbMetric: 174.0226 - val_loss: 174.2057 - val_MinusLogProbMetric: 174.2057 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 373/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:01:12.394 
Epoch 373/1000 
	 loss: 173.9183, MinusLogProbMetric: 173.9183, val_loss: 174.8042, val_MinusLogProbMetric: 174.8042

Epoch 373: val_loss did not improve from 174.20567
196/196 - 76s - loss: 173.9183 - MinusLogProbMetric: 173.9183 - val_loss: 174.8042 - val_MinusLogProbMetric: 174.8042 - lr: 4.1152e-06 - 76s/epoch - 389ms/step
Epoch 374/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:02:24.525 
Epoch 374/1000 
	 loss: 173.8875, MinusLogProbMetric: 173.8875, val_loss: 174.5944, val_MinusLogProbMetric: 174.5945

Epoch 374: val_loss did not improve from 174.20567
196/196 - 72s - loss: 173.8875 - MinusLogProbMetric: 173.8875 - val_loss: 174.5944 - val_MinusLogProbMetric: 174.5945 - lr: 4.1152e-06 - 72s/epoch - 368ms/step
Epoch 375/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:03:37.405 
Epoch 375/1000 
	 loss: 173.8381, MinusLogProbMetric: 173.8381, val_loss: 174.0085, val_MinusLogProbMetric: 174.0085

Epoch 375: val_loss improved from 174.20567 to 174.00845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_376/weights/best_weights.h5
196/196 - 74s - loss: 173.8381 - MinusLogProbMetric: 173.8381 - val_loss: 174.0085 - val_MinusLogProbMetric: 174.0085 - lr: 4.1152e-06 - 74s/epoch - 379ms/step
Epoch 376/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:04:50.517 
Epoch 376/1000 
	 loss: 174.2230, MinusLogProbMetric: 174.2230, val_loss: 174.8245, val_MinusLogProbMetric: 174.8245

Epoch 376: val_loss did not improve from 174.00845
196/196 - 72s - loss: 174.2230 - MinusLogProbMetric: 174.2230 - val_loss: 174.8245 - val_MinusLogProbMetric: 174.8245 - lr: 4.1152e-06 - 72s/epoch - 366ms/step
Epoch 377/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:06:03.343 
Epoch 377/1000 
	 loss: 173.8161, MinusLogProbMetric: 173.8161, val_loss: 192.5617, val_MinusLogProbMetric: 192.5618

Epoch 377: val_loss did not improve from 174.00845
196/196 - 73s - loss: 173.8161 - MinusLogProbMetric: 173.8161 - val_loss: 192.5617 - val_MinusLogProbMetric: 192.5618 - lr: 4.1152e-06 - 73s/epoch - 372ms/step
Epoch 378/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:07:22.667 
Epoch 378/1000 
	 loss: 218.4672, MinusLogProbMetric: 218.4672, val_loss: 196.6761, val_MinusLogProbMetric: 196.6762

Epoch 378: val_loss did not improve from 174.00845
196/196 - 79s - loss: 218.4672 - MinusLogProbMetric: 218.4672 - val_loss: 196.6761 - val_MinusLogProbMetric: 196.6762 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 379/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:08:38.867 
Epoch 379/1000 
	 loss: 190.5379, MinusLogProbMetric: 190.5379, val_loss: 183.9986, val_MinusLogProbMetric: 183.9987

Epoch 379: val_loss did not improve from 174.00845
196/196 - 76s - loss: 190.5379 - MinusLogProbMetric: 190.5379 - val_loss: 183.9986 - val_MinusLogProbMetric: 183.9987 - lr: 4.1152e-06 - 76s/epoch - 389ms/step
Epoch 380/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:10:00.391 
Epoch 380/1000 
	 loss: 180.4904, MinusLogProbMetric: 180.4904, val_loss: 178.7631, val_MinusLogProbMetric: 178.7631

Epoch 380: val_loss did not improve from 174.00845
196/196 - 82s - loss: 180.4904 - MinusLogProbMetric: 180.4904 - val_loss: 178.7631 - val_MinusLogProbMetric: 178.7631 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 381/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:11:17.176 
Epoch 381/1000 
	 loss: 177.8419, MinusLogProbMetric: 177.8419, val_loss: 177.6874, val_MinusLogProbMetric: 177.6874

Epoch 381: val_loss did not improve from 174.00845
196/196 - 77s - loss: 177.8419 - MinusLogProbMetric: 177.8419 - val_loss: 177.6874 - val_MinusLogProbMetric: 177.6874 - lr: 4.1152e-06 - 77s/epoch - 392ms/step
Epoch 382/1000
2023-10-26 16:12:38.229 
Epoch 382/1000 
	 loss: 176.6927, MinusLogProbMetric: 176.6927, val_loss: 176.7092, val_MinusLogProbMetric: 176.7092

Epoch 382: val_loss did not improve from 174.00845
196/196 - 81s - loss: 176.6927 - MinusLogProbMetric: 176.6927 - val_loss: 176.7092 - val_MinusLogProbMetric: 176.7092 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 383/1000
2023-10-26 16:13:58.287 
Epoch 383/1000 
	 loss: 176.0287, MinusLogProbMetric: 176.0287, val_loss: 176.9915, val_MinusLogProbMetric: 176.9915

Epoch 383: val_loss did not improve from 174.00845
196/196 - 80s - loss: 176.0287 - MinusLogProbMetric: 176.0287 - val_loss: 176.9915 - val_MinusLogProbMetric: 176.9915 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 384/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:15:14.766 
Epoch 384/1000 
	 loss: 175.5229, MinusLogProbMetric: 175.5229, val_loss: 175.4599, val_MinusLogProbMetric: 175.4599

Epoch 384: val_loss did not improve from 174.00845
196/196 - 76s - loss: 175.5229 - MinusLogProbMetric: 175.5229 - val_loss: 175.4599 - val_MinusLogProbMetric: 175.4599 - lr: 4.1152e-06 - 76s/epoch - 390ms/step
Epoch 385/1000
2023-10-26 16:16:35.991 
Epoch 385/1000 
	 loss: 175.2257, MinusLogProbMetric: 175.2257, val_loss: 175.6700, val_MinusLogProbMetric: 175.6700

Epoch 385: val_loss did not improve from 174.00845
196/196 - 81s - loss: 175.2257 - MinusLogProbMetric: 175.2257 - val_loss: 175.6700 - val_MinusLogProbMetric: 175.6700 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 386/1000
2023-10-26 16:17:51.867 
Epoch 386/1000 
	 loss: 174.8177, MinusLogProbMetric: 174.8177, val_loss: 174.8664, val_MinusLogProbMetric: 174.8664

Epoch 386: val_loss did not improve from 174.00845
196/196 - 76s - loss: 174.8177 - MinusLogProbMetric: 174.8177 - val_loss: 174.8664 - val_MinusLogProbMetric: 174.8664 - lr: 4.1152e-06 - 76s/epoch - 387ms/step
Epoch 387/1000
2023-10-26 16:19:12.596 
Epoch 387/1000 
	 loss: 174.3786, MinusLogProbMetric: 174.3786, val_loss: 174.4263, val_MinusLogProbMetric: 174.4263

Epoch 387: val_loss did not improve from 174.00845
196/196 - 81s - loss: 174.3786 - MinusLogProbMetric: 174.3786 - val_loss: 174.4263 - val_MinusLogProbMetric: 174.4263 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 388/1000
2023-10-26 16:20:30.956 
Epoch 388/1000 
	 loss: 173.6792, MinusLogProbMetric: 173.6792, val_loss: 174.2177, val_MinusLogProbMetric: 174.2177

Epoch 388: val_loss did not improve from 174.00845
196/196 - 78s - loss: 173.6792 - MinusLogProbMetric: 173.6792 - val_loss: 174.2177 - val_MinusLogProbMetric: 174.2177 - lr: 4.1152e-06 - 78s/epoch - 400ms/step
Epoch 389/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:21:51.803 
Epoch 389/1000 
	 loss: 226.5053, MinusLogProbMetric: 226.5053, val_loss: 263.6983, val_MinusLogProbMetric: 263.6979

Epoch 389: val_loss did not improve from 174.00845
196/196 - 81s - loss: 226.5053 - MinusLogProbMetric: 226.5053 - val_loss: 263.6983 - val_MinusLogProbMetric: 263.6979 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 390/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-26 16:23:10.176 
Epoch 390/1000 
	 loss: 878.7029, MinusLogProbMetric: 878.7029, val_loss: 776.5336, val_MinusLogProbMetric: 776.5341

Epoch 390: val_loss did not improve from 174.00845
196/196 - 78s - loss: 878.7029 - MinusLogProbMetric: 878.7029 - val_loss: 776.5336 - val_MinusLogProbMetric: 776.5341 - lr: 4.1152e-06 - 78s/epoch - 400ms/step
Epoch 391/1000
2023-10-26 16:24:25.060 
Epoch 391/1000 
	 loss: 626.1294, MinusLogProbMetric: 626.1294, val_loss: 521.8467, val_MinusLogProbMetric: 521.8467

Epoch 391: val_loss did not improve from 174.00845
196/196 - 75s - loss: 626.1294 - MinusLogProbMetric: 626.1294 - val_loss: 521.8467 - val_MinusLogProbMetric: 521.8467 - lr: 4.1152e-06 - 75s/epoch - 382ms/step
Epoch 392/1000
2023-10-26 16:25:35.017 
Epoch 392/1000 
	 loss: 481.3975, MinusLogProbMetric: 481.3975, val_loss: 453.0395, val_MinusLogProbMetric: 453.0395

Epoch 392: val_loss did not improve from 174.00845
196/196 - 70s - loss: 481.3975 - MinusLogProbMetric: 481.3975 - val_loss: 453.0395 - val_MinusLogProbMetric: 453.0395 - lr: 4.1152e-06 - 70s/epoch - 357ms/step
Epoch 393/1000
2023-10-26 16:26:55.631 
Epoch 393/1000 
	 loss: 436.5157, MinusLogProbMetric: 436.5157, val_loss: 423.8645, val_MinusLogProbMetric: 423.8645

Epoch 393: val_loss did not improve from 174.00845
196/196 - 81s - loss: 436.5157 - MinusLogProbMetric: 436.5157 - val_loss: 423.8645 - val_MinusLogProbMetric: 423.8645 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 394/1000
2023-10-26 16:28:14.779 
Epoch 394/1000 
	 loss: 412.8624, MinusLogProbMetric: 412.8624, val_loss: 408.6122, val_MinusLogProbMetric: 408.6122

Epoch 394: val_loss did not improve from 174.00845
196/196 - 79s - loss: 412.8624 - MinusLogProbMetric: 412.8624 - val_loss: 408.6122 - val_MinusLogProbMetric: 408.6122 - lr: 4.1152e-06 - 79s/epoch - 404ms/step
Epoch 395/1000
2023-10-26 16:29:33.590 
Epoch 395/1000 
	 loss: 400.6068, MinusLogProbMetric: 400.6068, val_loss: 393.7704, val_MinusLogProbMetric: 393.7704

Epoch 395: val_loss did not improve from 174.00845
196/196 - 79s - loss: 400.6068 - MinusLogProbMetric: 400.6068 - val_loss: 393.7704 - val_MinusLogProbMetric: 393.7704 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 396/1000
2023-10-26 16:30:51.015 
Epoch 396/1000 
	 loss: 388.1061, MinusLogProbMetric: 388.1061, val_loss: 384.4457, val_MinusLogProbMetric: 384.4457

Epoch 396: val_loss did not improve from 174.00845
196/196 - 77s - loss: 388.1061 - MinusLogProbMetric: 388.1061 - val_loss: 384.4457 - val_MinusLogProbMetric: 384.4457 - lr: 4.1152e-06 - 77s/epoch - 395ms/step
Epoch 397/1000
2023-10-26 16:32:09.722 
Epoch 397/1000 
	 loss: 379.2616, MinusLogProbMetric: 379.2616, val_loss: 375.7324, val_MinusLogProbMetric: 375.7324

Epoch 397: val_loss did not improve from 174.00845
196/196 - 79s - loss: 379.2616 - MinusLogProbMetric: 379.2616 - val_loss: 375.7324 - val_MinusLogProbMetric: 375.7324 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 398/1000
2023-10-26 16:33:27.784 
Epoch 398/1000 
	 loss: 371.4651, MinusLogProbMetric: 371.4651, val_loss: 368.6303, val_MinusLogProbMetric: 368.6303

Epoch 398: val_loss did not improve from 174.00845
196/196 - 78s - loss: 371.4651 - MinusLogProbMetric: 371.4651 - val_loss: 368.6303 - val_MinusLogProbMetric: 368.6303 - lr: 4.1152e-06 - 78s/epoch - 398ms/step
Epoch 399/1000
2023-10-26 16:35:24.970 
Epoch 399/1000 
	 loss: 365.4778, MinusLogProbMetric: 365.4778, val_loss: 363.3439, val_MinusLogProbMetric: 363.3439

Epoch 399: val_loss did not improve from 174.00845
196/196 - 117s - loss: 365.4778 - MinusLogProbMetric: 365.4778 - val_loss: 363.3439 - val_MinusLogProbMetric: 363.3439 - lr: 4.1152e-06 - 117s/epoch - 598ms/step
Epoch 400/1000
2023-10-26 16:36:46.066 
Epoch 400/1000 
	 loss: 360.8546, MinusLogProbMetric: 360.8546, val_loss: 358.8127, val_MinusLogProbMetric: 358.8127

Epoch 400: val_loss did not improve from 174.00845
196/196 - 81s - loss: 360.8546 - MinusLogProbMetric: 360.8546 - val_loss: 358.8127 - val_MinusLogProbMetric: 358.8127 - lr: 4.1152e-06 - 81s/epoch - 414ms/step
Epoch 401/1000
2023-10-26 16:38:01.093 
Epoch 401/1000 
	 loss: 356.0957, MinusLogProbMetric: 356.0957, val_loss: 354.6192, val_MinusLogProbMetric: 354.6192

Epoch 401: val_loss did not improve from 174.00845
196/196 - 75s - loss: 356.0957 - MinusLogProbMetric: 356.0957 - val_loss: 354.6192 - val_MinusLogProbMetric: 354.6192 - lr: 4.1152e-06 - 75s/epoch - 383ms/step
Epoch 402/1000
2023-10-26 16:39:18.633 
Epoch 402/1000 
	 loss: 352.2224, MinusLogProbMetric: 352.2224, val_loss: 350.4405, val_MinusLogProbMetric: 350.4405

Epoch 402: val_loss did not improve from 174.00845
196/196 - 78s - loss: 352.2224 - MinusLogProbMetric: 352.2224 - val_loss: 350.4405 - val_MinusLogProbMetric: 350.4405 - lr: 4.1152e-06 - 78s/epoch - 396ms/step
Epoch 403/1000
2023-10-26 16:40:37.963 
Epoch 403/1000 
	 loss: 348.3952, MinusLogProbMetric: 348.3952, val_loss: 346.8924, val_MinusLogProbMetric: 346.8924

Epoch 403: val_loss did not improve from 174.00845
196/196 - 79s - loss: 348.3952 - MinusLogProbMetric: 348.3952 - val_loss: 346.8924 - val_MinusLogProbMetric: 346.8924 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 404/1000
2023-10-26 16:41:56.474 
Epoch 404/1000 
	 loss: 345.8895, MinusLogProbMetric: 345.8895, val_loss: 362.2996, val_MinusLogProbMetric: 362.2996

Epoch 404: val_loss did not improve from 174.00845
196/196 - 79s - loss: 345.8895 - MinusLogProbMetric: 345.8895 - val_loss: 362.2996 - val_MinusLogProbMetric: 362.2996 - lr: 4.1152e-06 - 79s/epoch - 401ms/step
Epoch 405/1000
2023-10-26 16:43:16.557 
Epoch 405/1000 
	 loss: 345.1965, MinusLogProbMetric: 345.1965, val_loss: 340.3780, val_MinusLogProbMetric: 340.3780

Epoch 405: val_loss did not improve from 174.00845
196/196 - 80s - loss: 345.1965 - MinusLogProbMetric: 345.1965 - val_loss: 340.3780 - val_MinusLogProbMetric: 340.3780 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 406/1000
2023-10-26 16:44:37.469 
Epoch 406/1000 
	 loss: 337.3919, MinusLogProbMetric: 337.3919, val_loss: 335.8244, val_MinusLogProbMetric: 335.8244

Epoch 406: val_loss did not improve from 174.00845
196/196 - 81s - loss: 337.3919 - MinusLogProbMetric: 337.3919 - val_loss: 335.8244 - val_MinusLogProbMetric: 335.8244 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 407/1000
2023-10-26 16:45:55.416 
Epoch 407/1000 
	 loss: 332.8140, MinusLogProbMetric: 332.8140, val_loss: 331.1117, val_MinusLogProbMetric: 331.1117

Epoch 407: val_loss did not improve from 174.00845
196/196 - 78s - loss: 332.8140 - MinusLogProbMetric: 332.8140 - val_loss: 331.1117 - val_MinusLogProbMetric: 331.1117 - lr: 4.1152e-06 - 78s/epoch - 398ms/step
Epoch 408/1000
2023-10-26 16:47:12.771 
Epoch 408/1000 
	 loss: 328.8175, MinusLogProbMetric: 328.8175, val_loss: 327.3703, val_MinusLogProbMetric: 327.3703

Epoch 408: val_loss did not improve from 174.00845
196/196 - 77s - loss: 328.8175 - MinusLogProbMetric: 328.8175 - val_loss: 327.3703 - val_MinusLogProbMetric: 327.3703 - lr: 4.1152e-06 - 77s/epoch - 395ms/step
Epoch 409/1000
2023-10-26 16:48:33.088 
Epoch 409/1000 
	 loss: 325.7253, MinusLogProbMetric: 325.7253, val_loss: 324.7444, val_MinusLogProbMetric: 324.7444

Epoch 409: val_loss did not improve from 174.00845
196/196 - 80s - loss: 325.7253 - MinusLogProbMetric: 325.7253 - val_loss: 324.7444 - val_MinusLogProbMetric: 324.7444 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 410/1000
2023-10-26 16:49:48.382 
Epoch 410/1000 
	 loss: 323.1041, MinusLogProbMetric: 323.1041, val_loss: 321.9141, val_MinusLogProbMetric: 321.9141

Epoch 410: val_loss did not improve from 174.00845
196/196 - 75s - loss: 323.1041 - MinusLogProbMetric: 323.1041 - val_loss: 321.9141 - val_MinusLogProbMetric: 321.9141 - lr: 4.1152e-06 - 75s/epoch - 384ms/step
Epoch 411/1000
2023-10-26 16:51:01.888 
Epoch 411/1000 
	 loss: 344.9492, MinusLogProbMetric: 344.9492, val_loss: 548.6180, val_MinusLogProbMetric: 548.6180

Epoch 411: val_loss did not improve from 174.00845
196/196 - 74s - loss: 344.9492 - MinusLogProbMetric: 344.9492 - val_loss: 548.6180 - val_MinusLogProbMetric: 548.6180 - lr: 4.1152e-06 - 74s/epoch - 375ms/step
Epoch 412/1000
2023-10-26 16:52:21.887 
Epoch 412/1000 
	 loss: 421.4273, MinusLogProbMetric: 421.4273, val_loss: 365.9350, val_MinusLogProbMetric: 365.9350

Epoch 412: val_loss did not improve from 174.00845
196/196 - 80s - loss: 421.4273 - MinusLogProbMetric: 421.4273 - val_loss: 365.9350 - val_MinusLogProbMetric: 365.9350 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 413/1000
2023-10-26 16:53:34.058 
Epoch 413/1000 
	 loss: 350.6953, MinusLogProbMetric: 350.6953, val_loss: 340.9419, val_MinusLogProbMetric: 340.9419

Epoch 413: val_loss did not improve from 174.00845
196/196 - 72s - loss: 350.6953 - MinusLogProbMetric: 350.6953 - val_loss: 340.9419 - val_MinusLogProbMetric: 340.9419 - lr: 4.1152e-06 - 72s/epoch - 368ms/step
Epoch 414/1000
2023-10-26 16:54:48.070 
Epoch 414/1000 
	 loss: 341.4470, MinusLogProbMetric: 341.4470, val_loss: 333.2756, val_MinusLogProbMetric: 333.2756

Epoch 414: val_loss did not improve from 174.00845
196/196 - 74s - loss: 341.4470 - MinusLogProbMetric: 341.4470 - val_loss: 333.2756 - val_MinusLogProbMetric: 333.2756 - lr: 4.1152e-06 - 74s/epoch - 378ms/step
Epoch 415/1000
2023-10-26 16:56:09.712 
Epoch 415/1000 
	 loss: 324.6154, MinusLogProbMetric: 324.6154, val_loss: 319.0818, val_MinusLogProbMetric: 319.0818

Epoch 415: val_loss did not improve from 174.00845
196/196 - 82s - loss: 324.6154 - MinusLogProbMetric: 324.6154 - val_loss: 319.0818 - val_MinusLogProbMetric: 319.0818 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 416/1000
2023-10-26 16:57:30.606 
Epoch 416/1000 
	 loss: 314.6685, MinusLogProbMetric: 314.6685, val_loss: 311.7473, val_MinusLogProbMetric: 311.7473

Epoch 416: val_loss did not improve from 174.00845
196/196 - 81s - loss: 314.6685 - MinusLogProbMetric: 314.6685 - val_loss: 311.7473 - val_MinusLogProbMetric: 311.7473 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 417/1000
2023-10-26 16:58:51.508 
Epoch 417/1000 
	 loss: 308.7775, MinusLogProbMetric: 308.7775, val_loss: 306.5997, val_MinusLogProbMetric: 306.5997

Epoch 417: val_loss did not improve from 174.00845
196/196 - 81s - loss: 308.7775 - MinusLogProbMetric: 308.7775 - val_loss: 306.5997 - val_MinusLogProbMetric: 306.5997 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 418/1000
2023-10-26 17:00:12.317 
Epoch 418/1000 
	 loss: 304.4192, MinusLogProbMetric: 304.4192, val_loss: 302.8575, val_MinusLogProbMetric: 302.8575

Epoch 418: val_loss did not improve from 174.00845
196/196 - 81s - loss: 304.4192 - MinusLogProbMetric: 304.4192 - val_loss: 302.8575 - val_MinusLogProbMetric: 302.8575 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 419/1000
2023-10-26 17:01:31.009 
Epoch 419/1000 
	 loss: 302.1628, MinusLogProbMetric: 302.1628, val_loss: 300.4071, val_MinusLogProbMetric: 300.4071

Epoch 419: val_loss did not improve from 174.00845
196/196 - 79s - loss: 302.1628 - MinusLogProbMetric: 302.1628 - val_loss: 300.4071 - val_MinusLogProbMetric: 300.4071 - lr: 4.1152e-06 - 79s/epoch - 401ms/step
Epoch 420/1000
2023-10-26 17:02:47.688 
Epoch 420/1000 
	 loss: 298.1704, MinusLogProbMetric: 298.1704, val_loss: 296.4922, val_MinusLogProbMetric: 296.4922

Epoch 420: val_loss did not improve from 174.00845
196/196 - 77s - loss: 298.1704 - MinusLogProbMetric: 298.1704 - val_loss: 296.4922 - val_MinusLogProbMetric: 296.4922 - lr: 4.1152e-06 - 77s/epoch - 391ms/step
Epoch 421/1000
2023-10-26 17:04:09.367 
Epoch 421/1000 
	 loss: 294.6893, MinusLogProbMetric: 294.6893, val_loss: 298.3819, val_MinusLogProbMetric: 298.3819

Epoch 421: val_loss did not improve from 174.00845
196/196 - 82s - loss: 294.6893 - MinusLogProbMetric: 294.6893 - val_loss: 298.3819 - val_MinusLogProbMetric: 298.3819 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 422/1000
2023-10-26 17:05:24.422 
Epoch 422/1000 
	 loss: 292.8181, MinusLogProbMetric: 292.8181, val_loss: 291.2795, val_MinusLogProbMetric: 291.2795

Epoch 422: val_loss did not improve from 174.00845
196/196 - 75s - loss: 292.8181 - MinusLogProbMetric: 292.8181 - val_loss: 291.2795 - val_MinusLogProbMetric: 291.2795 - lr: 4.1152e-06 - 75s/epoch - 383ms/step
Epoch 423/1000
2023-10-26 17:06:41.905 
Epoch 423/1000 
	 loss: 289.5753, MinusLogProbMetric: 289.5753, val_loss: 288.8583, val_MinusLogProbMetric: 288.8583

Epoch 423: val_loss did not improve from 174.00845
196/196 - 77s - loss: 289.5753 - MinusLogProbMetric: 289.5753 - val_loss: 288.8583 - val_MinusLogProbMetric: 288.8583 - lr: 4.1152e-06 - 77s/epoch - 395ms/step
Epoch 424/1000
2023-10-26 17:08:02.492 
Epoch 424/1000 
	 loss: 287.2256, MinusLogProbMetric: 287.2256, val_loss: 286.5071, val_MinusLogProbMetric: 286.5071

Epoch 424: val_loss did not improve from 174.00845
196/196 - 81s - loss: 287.2256 - MinusLogProbMetric: 287.2256 - val_loss: 286.5071 - val_MinusLogProbMetric: 286.5071 - lr: 4.1152e-06 - 81s/epoch - 411ms/step
Epoch 425/1000
2023-10-26 17:09:21.351 
Epoch 425/1000 
	 loss: 285.4236, MinusLogProbMetric: 285.4236, val_loss: 284.3219, val_MinusLogProbMetric: 284.3219

Epoch 425: val_loss did not improve from 174.00845
196/196 - 79s - loss: 285.4236 - MinusLogProbMetric: 285.4236 - val_loss: 284.3219 - val_MinusLogProbMetric: 284.3219 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 426/1000
2023-10-26 17:10:40.537 
Epoch 426/1000 
	 loss: 283.4873, MinusLogProbMetric: 283.4873, val_loss: 283.4053, val_MinusLogProbMetric: 283.4053

Epoch 426: val_loss did not improve from 174.00845
196/196 - 79s - loss: 283.4873 - MinusLogProbMetric: 283.4873 - val_loss: 283.4053 - val_MinusLogProbMetric: 283.4053 - lr: 2.0576e-06 - 79s/epoch - 404ms/step
Epoch 427/1000
2023-10-26 17:12:01.667 
Epoch 427/1000 
	 loss: 282.6171, MinusLogProbMetric: 282.6171, val_loss: 282.6715, val_MinusLogProbMetric: 282.6715

Epoch 427: val_loss did not improve from 174.00845
196/196 - 81s - loss: 282.6171 - MinusLogProbMetric: 282.6171 - val_loss: 282.6715 - val_MinusLogProbMetric: 282.6715 - lr: 2.0576e-06 - 81s/epoch - 414ms/step
Epoch 428/1000
2023-10-26 17:13:17.470 
Epoch 428/1000 
	 loss: 281.8687, MinusLogProbMetric: 281.8687, val_loss: 282.0675, val_MinusLogProbMetric: 282.0675

Epoch 428: val_loss did not improve from 174.00845
196/196 - 76s - loss: 281.8687 - MinusLogProbMetric: 281.8687 - val_loss: 282.0675 - val_MinusLogProbMetric: 282.0675 - lr: 2.0576e-06 - 76s/epoch - 387ms/step
Epoch 429/1000
2023-10-26 17:14:35.920 
Epoch 429/1000 
	 loss: 280.7868, MinusLogProbMetric: 280.7868, val_loss: 280.5515, val_MinusLogProbMetric: 280.5515

Epoch 429: val_loss did not improve from 174.00845
196/196 - 78s - loss: 280.7868 - MinusLogProbMetric: 280.7868 - val_loss: 280.5515 - val_MinusLogProbMetric: 280.5515 - lr: 2.0576e-06 - 78s/epoch - 400ms/step
Epoch 430/1000
2023-10-26 17:15:53.231 
Epoch 430/1000 
	 loss: 280.0092, MinusLogProbMetric: 280.0092, val_loss: 279.7914, val_MinusLogProbMetric: 279.7914

Epoch 430: val_loss did not improve from 174.00845
196/196 - 77s - loss: 280.0092 - MinusLogProbMetric: 280.0092 - val_loss: 279.7914 - val_MinusLogProbMetric: 279.7914 - lr: 2.0576e-06 - 77s/epoch - 394ms/step
Epoch 431/1000
2023-10-26 17:17:13.314 
Epoch 431/1000 
	 loss: 279.1011, MinusLogProbMetric: 279.1011, val_loss: 279.0131, val_MinusLogProbMetric: 279.0131

Epoch 431: val_loss did not improve from 174.00845
196/196 - 80s - loss: 279.1011 - MinusLogProbMetric: 279.1011 - val_loss: 279.0131 - val_MinusLogProbMetric: 279.0131 - lr: 2.0576e-06 - 80s/epoch - 409ms/step
Epoch 432/1000
2023-10-26 17:18:32.534 
Epoch 432/1000 
	 loss: 280.3494, MinusLogProbMetric: 280.3494, val_loss: 280.1233, val_MinusLogProbMetric: 280.1233

Epoch 432: val_loss did not improve from 174.00845
196/196 - 79s - loss: 280.3494 - MinusLogProbMetric: 280.3494 - val_loss: 280.1233 - val_MinusLogProbMetric: 280.1233 - lr: 2.0576e-06 - 79s/epoch - 404ms/step
Epoch 433/1000
2023-10-26 17:19:45.114 
Epoch 433/1000 
	 loss: 279.1750, MinusLogProbMetric: 279.1750, val_loss: 278.8410, val_MinusLogProbMetric: 278.8410

Epoch 433: val_loss did not improve from 174.00845
196/196 - 73s - loss: 279.1750 - MinusLogProbMetric: 279.1750 - val_loss: 278.8410 - val_MinusLogProbMetric: 278.8410 - lr: 2.0576e-06 - 73s/epoch - 370ms/step
Epoch 434/1000
2023-10-26 17:21:05.382 
Epoch 434/1000 
	 loss: 278.2248, MinusLogProbMetric: 278.2248, val_loss: 278.2044, val_MinusLogProbMetric: 278.2044

Epoch 434: val_loss did not improve from 174.00845
196/196 - 80s - loss: 278.2248 - MinusLogProbMetric: 278.2248 - val_loss: 278.2044 - val_MinusLogProbMetric: 278.2044 - lr: 2.0576e-06 - 80s/epoch - 410ms/step
Epoch 435/1000
2023-10-26 17:22:22.682 
Epoch 435/1000 
	 loss: 277.4306, MinusLogProbMetric: 277.4306, val_loss: 277.2653, val_MinusLogProbMetric: 277.2653

Epoch 435: val_loss did not improve from 174.00845
196/196 - 77s - loss: 277.4306 - MinusLogProbMetric: 277.4306 - val_loss: 277.2653 - val_MinusLogProbMetric: 277.2653 - lr: 2.0576e-06 - 77s/epoch - 394ms/step
Epoch 436/1000
2023-10-26 17:23:42.311 
Epoch 436/1000 
	 loss: 276.5887, MinusLogProbMetric: 276.5887, val_loss: 276.5882, val_MinusLogProbMetric: 276.5882

Epoch 436: val_loss did not improve from 174.00845
196/196 - 80s - loss: 276.5887 - MinusLogProbMetric: 276.5887 - val_loss: 276.5882 - val_MinusLogProbMetric: 276.5882 - lr: 2.0576e-06 - 80s/epoch - 406ms/step
Epoch 437/1000
2023-10-26 17:25:00.935 
Epoch 437/1000 
	 loss: 275.7536, MinusLogProbMetric: 275.7536, val_loss: 275.7082, val_MinusLogProbMetric: 275.7082

Epoch 437: val_loss did not improve from 174.00845
196/196 - 79s - loss: 275.7536 - MinusLogProbMetric: 275.7536 - val_loss: 275.7082 - val_MinusLogProbMetric: 275.7082 - lr: 2.0576e-06 - 79s/epoch - 401ms/step
Epoch 438/1000
2023-10-26 17:26:19.594 
Epoch 438/1000 
	 loss: 274.9932, MinusLogProbMetric: 274.9932, val_loss: 274.8589, val_MinusLogProbMetric: 274.8589

Epoch 438: val_loss did not improve from 174.00845
196/196 - 79s - loss: 274.9932 - MinusLogProbMetric: 274.9932 - val_loss: 274.8589 - val_MinusLogProbMetric: 274.8589 - lr: 2.0576e-06 - 79s/epoch - 401ms/step
Epoch 439/1000
2023-10-26 17:27:36.309 
Epoch 439/1000 
	 loss: 274.4883, MinusLogProbMetric: 274.4883, val_loss: 274.5611, val_MinusLogProbMetric: 274.5611

Epoch 439: val_loss did not improve from 174.00845
196/196 - 77s - loss: 274.4883 - MinusLogProbMetric: 274.4883 - val_loss: 274.5611 - val_MinusLogProbMetric: 274.5611 - lr: 2.0576e-06 - 77s/epoch - 391ms/step
Epoch 440/1000
2023-10-26 17:28:54.741 
Epoch 440/1000 
	 loss: 273.8665, MinusLogProbMetric: 273.8665, val_loss: 273.8086, val_MinusLogProbMetric: 273.8086

Epoch 440: val_loss did not improve from 174.00845
196/196 - 78s - loss: 273.8665 - MinusLogProbMetric: 273.8665 - val_loss: 273.8086 - val_MinusLogProbMetric: 273.8086 - lr: 2.0576e-06 - 78s/epoch - 400ms/step
Epoch 441/1000
2023-10-26 17:30:14.226 
Epoch 441/1000 
	 loss: 273.1925, MinusLogProbMetric: 273.1925, val_loss: 273.1498, val_MinusLogProbMetric: 273.1498

Epoch 441: val_loss did not improve from 174.00845
196/196 - 79s - loss: 273.1925 - MinusLogProbMetric: 273.1925 - val_loss: 273.1498 - val_MinusLogProbMetric: 273.1498 - lr: 2.0576e-06 - 79s/epoch - 406ms/step
Epoch 442/1000
2023-10-26 17:31:34.271 
Epoch 442/1000 
	 loss: 272.5549, MinusLogProbMetric: 272.5549, val_loss: 272.5233, val_MinusLogProbMetric: 272.5233

Epoch 442: val_loss did not improve from 174.00845
196/196 - 80s - loss: 272.5549 - MinusLogProbMetric: 272.5549 - val_loss: 272.5233 - val_MinusLogProbMetric: 272.5233 - lr: 2.0576e-06 - 80s/epoch - 408ms/step
Epoch 443/1000
2023-10-26 17:32:54.630 
Epoch 443/1000 
	 loss: 272.9637, MinusLogProbMetric: 272.9637, val_loss: 271.9462, val_MinusLogProbMetric: 271.9462

Epoch 443: val_loss did not improve from 174.00845
196/196 - 80s - loss: 272.9637 - MinusLogProbMetric: 272.9637 - val_loss: 271.9462 - val_MinusLogProbMetric: 271.9462 - lr: 2.0576e-06 - 80s/epoch - 410ms/step
Epoch 444/1000
2023-10-26 17:34:12.948 
Epoch 444/1000 
	 loss: 271.7117, MinusLogProbMetric: 271.7117, val_loss: 271.7515, val_MinusLogProbMetric: 271.7515

Epoch 444: val_loss did not improve from 174.00845
196/196 - 78s - loss: 271.7117 - MinusLogProbMetric: 271.7117 - val_loss: 271.7515 - val_MinusLogProbMetric: 271.7515 - lr: 2.0576e-06 - 78s/epoch - 400ms/step
Epoch 445/1000
2023-10-26 17:35:32.619 
Epoch 445/1000 
	 loss: 271.0562, MinusLogProbMetric: 271.0562, val_loss: 271.2818, val_MinusLogProbMetric: 271.2818

Epoch 445: val_loss did not improve from 174.00845
196/196 - 80s - loss: 271.0562 - MinusLogProbMetric: 271.0562 - val_loss: 271.2818 - val_MinusLogProbMetric: 271.2818 - lr: 2.0576e-06 - 80s/epoch - 406ms/step
Epoch 446/1000
2023-10-26 17:36:53.853 
Epoch 446/1000 
	 loss: 270.3995, MinusLogProbMetric: 270.3995, val_loss: 270.5211, val_MinusLogProbMetric: 270.5211

Epoch 446: val_loss did not improve from 174.00845
196/196 - 81s - loss: 270.3995 - MinusLogProbMetric: 270.3995 - val_loss: 270.5211 - val_MinusLogProbMetric: 270.5211 - lr: 2.0576e-06 - 81s/epoch - 414ms/step
Epoch 447/1000
2023-10-26 17:38:15.941 
Epoch 447/1000 
	 loss: 274.4215, MinusLogProbMetric: 274.4215, val_loss: 272.0864, val_MinusLogProbMetric: 272.0864

Epoch 447: val_loss did not improve from 174.00845
196/196 - 82s - loss: 274.4215 - MinusLogProbMetric: 274.4215 - val_loss: 272.0864 - val_MinusLogProbMetric: 272.0864 - lr: 2.0576e-06 - 82s/epoch - 419ms/step
Epoch 448/1000
2023-10-26 17:39:37.757 
Epoch 448/1000 
	 loss: 270.0342, MinusLogProbMetric: 270.0342, val_loss: 269.2322, val_MinusLogProbMetric: 269.2322

Epoch 448: val_loss did not improve from 174.00845
196/196 - 82s - loss: 270.0342 - MinusLogProbMetric: 270.0342 - val_loss: 269.2322 - val_MinusLogProbMetric: 269.2322 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 449/1000
2023-10-26 17:40:59.747 
Epoch 449/1000 
	 loss: 268.3765, MinusLogProbMetric: 268.3765, val_loss: 267.9730, val_MinusLogProbMetric: 267.9730

Epoch 449: val_loss did not improve from 174.00845
196/196 - 82s - loss: 268.3765 - MinusLogProbMetric: 268.3765 - val_loss: 267.9730 - val_MinusLogProbMetric: 267.9730 - lr: 2.0576e-06 - 82s/epoch - 418ms/step
Epoch 450/1000
2023-10-26 17:42:21.390 
Epoch 450/1000 
	 loss: 267.8016, MinusLogProbMetric: 267.8016, val_loss: 271.2076, val_MinusLogProbMetric: 271.2076

Epoch 450: val_loss did not improve from 174.00845
196/196 - 82s - loss: 267.8016 - MinusLogProbMetric: 267.8016 - val_loss: 271.2076 - val_MinusLogProbMetric: 271.2076 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 451/1000
2023-10-26 17:43:40.454 
Epoch 451/1000 
	 loss: 267.8733, MinusLogProbMetric: 267.8733, val_loss: 268.6055, val_MinusLogProbMetric: 268.6055

Epoch 451: val_loss did not improve from 174.00845
196/196 - 79s - loss: 267.8733 - MinusLogProbMetric: 267.8733 - val_loss: 268.6055 - val_MinusLogProbMetric: 268.6055 - lr: 2.0576e-06 - 79s/epoch - 403ms/step
Epoch 452/1000
2023-10-26 17:44:55.578 
Epoch 452/1000 
	 loss: 266.8302, MinusLogProbMetric: 266.8302, val_loss: 266.4436, val_MinusLogProbMetric: 266.4436

Epoch 452: val_loss did not improve from 174.00845
196/196 - 75s - loss: 266.8302 - MinusLogProbMetric: 266.8302 - val_loss: 266.4436 - val_MinusLogProbMetric: 266.4436 - lr: 2.0576e-06 - 75s/epoch - 383ms/step
Epoch 453/1000
2023-10-26 17:46:17.291 
Epoch 453/1000 
	 loss: 265.7932, MinusLogProbMetric: 265.7932, val_loss: 265.5501, val_MinusLogProbMetric: 265.5501

Epoch 453: val_loss did not improve from 174.00845
196/196 - 82s - loss: 265.7932 - MinusLogProbMetric: 265.7932 - val_loss: 265.5501 - val_MinusLogProbMetric: 265.5501 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 454/1000
2023-10-26 17:47:38.988 
Epoch 454/1000 
	 loss: 264.9377, MinusLogProbMetric: 264.9377, val_loss: 264.9063, val_MinusLogProbMetric: 264.9063

Epoch 454: val_loss did not improve from 174.00845
196/196 - 82s - loss: 264.9377 - MinusLogProbMetric: 264.9377 - val_loss: 264.9063 - val_MinusLogProbMetric: 264.9063 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 455/1000
2023-10-26 17:49:00.437 
Epoch 455/1000 
	 loss: 264.3697, MinusLogProbMetric: 264.3697, val_loss: 264.2936, val_MinusLogProbMetric: 264.2936

Epoch 455: val_loss did not improve from 174.00845
196/196 - 81s - loss: 264.3697 - MinusLogProbMetric: 264.3697 - val_loss: 264.2936 - val_MinusLogProbMetric: 264.2936 - lr: 2.0576e-06 - 81s/epoch - 416ms/step
Epoch 456/1000
2023-10-26 17:50:22.314 
Epoch 456/1000 
	 loss: 264.1120, MinusLogProbMetric: 264.1120, val_loss: 264.0140, val_MinusLogProbMetric: 264.0140

Epoch 456: val_loss did not improve from 174.00845
196/196 - 82s - loss: 264.1120 - MinusLogProbMetric: 264.1120 - val_loss: 264.0140 - val_MinusLogProbMetric: 264.0140 - lr: 2.0576e-06 - 82s/epoch - 418ms/step
Epoch 457/1000
2023-10-26 17:51:44.521 
Epoch 457/1000 
	 loss: 263.4283, MinusLogProbMetric: 263.4283, val_loss: 263.3690, val_MinusLogProbMetric: 263.3690

Epoch 457: val_loss did not improve from 174.00845
196/196 - 82s - loss: 263.4283 - MinusLogProbMetric: 263.4283 - val_loss: 263.3690 - val_MinusLogProbMetric: 263.3690 - lr: 2.0576e-06 - 82s/epoch - 419ms/step
Epoch 458/1000
2023-10-26 17:53:05.906 
Epoch 458/1000 
	 loss: 262.8835, MinusLogProbMetric: 262.8835, val_loss: 262.9025, val_MinusLogProbMetric: 262.9025

Epoch 458: val_loss did not improve from 174.00845
196/196 - 81s - loss: 262.8835 - MinusLogProbMetric: 262.8835 - val_loss: 262.9025 - val_MinusLogProbMetric: 262.9025 - lr: 2.0576e-06 - 81s/epoch - 415ms/step
Epoch 459/1000
2023-10-26 17:54:27.553 
Epoch 459/1000 
	 loss: 262.4039, MinusLogProbMetric: 262.4039, val_loss: 262.4089, val_MinusLogProbMetric: 262.4089

Epoch 459: val_loss did not improve from 174.00845
196/196 - 82s - loss: 262.4039 - MinusLogProbMetric: 262.4039 - val_loss: 262.4089 - val_MinusLogProbMetric: 262.4089 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 460/1000
2023-10-26 17:55:49.293 
Epoch 460/1000 
	 loss: 261.9611, MinusLogProbMetric: 261.9611, val_loss: 261.9827, val_MinusLogProbMetric: 261.9827

Epoch 460: val_loss did not improve from 174.00845
196/196 - 82s - loss: 261.9611 - MinusLogProbMetric: 261.9611 - val_loss: 261.9827 - val_MinusLogProbMetric: 261.9827 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 461/1000
2023-10-26 17:57:11.249 
Epoch 461/1000 
	 loss: 261.5371, MinusLogProbMetric: 261.5371, val_loss: 261.5013, val_MinusLogProbMetric: 261.5013

Epoch 461: val_loss did not improve from 174.00845
196/196 - 82s - loss: 261.5371 - MinusLogProbMetric: 261.5371 - val_loss: 261.5013 - val_MinusLogProbMetric: 261.5013 - lr: 2.0576e-06 - 82s/epoch - 418ms/step
Epoch 462/1000
2023-10-26 17:58:33.361 
Epoch 462/1000 
	 loss: 260.9852, MinusLogProbMetric: 260.9852, val_loss: 260.9901, val_MinusLogProbMetric: 260.9901

Epoch 462: val_loss did not improve from 174.00845
196/196 - 82s - loss: 260.9852 - MinusLogProbMetric: 260.9852 - val_loss: 260.9901 - val_MinusLogProbMetric: 260.9901 - lr: 2.0576e-06 - 82s/epoch - 419ms/step
Epoch 463/1000
2023-10-26 17:59:55.719 
Epoch 463/1000 
	 loss: 260.4930, MinusLogProbMetric: 260.4930, val_loss: 260.5086, val_MinusLogProbMetric: 260.5086

Epoch 463: val_loss did not improve from 174.00845
196/196 - 82s - loss: 260.4930 - MinusLogProbMetric: 260.4930 - val_loss: 260.5086 - val_MinusLogProbMetric: 260.5086 - lr: 2.0576e-06 - 82s/epoch - 420ms/step
Epoch 464/1000
2023-10-26 18:01:17.525 
Epoch 464/1000 
	 loss: 260.0691, MinusLogProbMetric: 260.0691, val_loss: 260.1555, val_MinusLogProbMetric: 260.1555

Epoch 464: val_loss did not improve from 174.00845
196/196 - 82s - loss: 260.0691 - MinusLogProbMetric: 260.0691 - val_loss: 260.1555 - val_MinusLogProbMetric: 260.1555 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 465/1000
2023-10-26 18:02:38.922 
Epoch 465/1000 
	 loss: 259.6446, MinusLogProbMetric: 259.6446, val_loss: 259.7527, val_MinusLogProbMetric: 259.7527

Epoch 465: val_loss did not improve from 174.00845
196/196 - 81s - loss: 259.6446 - MinusLogProbMetric: 259.6446 - val_loss: 259.7527 - val_MinusLogProbMetric: 259.7527 - lr: 2.0576e-06 - 81s/epoch - 415ms/step
Epoch 466/1000
2023-10-26 18:04:01.035 
Epoch 466/1000 
	 loss: 259.2822, MinusLogProbMetric: 259.2822, val_loss: 259.3171, val_MinusLogProbMetric: 259.3171

Epoch 466: val_loss did not improve from 174.00845
196/196 - 82s - loss: 259.2822 - MinusLogProbMetric: 259.2822 - val_loss: 259.3171 - val_MinusLogProbMetric: 259.3171 - lr: 2.0576e-06 - 82s/epoch - 419ms/step
Epoch 467/1000
2023-10-26 18:05:22.710 
Epoch 467/1000 
	 loss: 258.8626, MinusLogProbMetric: 258.8626, val_loss: 259.0683, val_MinusLogProbMetric: 259.0683

Epoch 467: val_loss did not improve from 174.00845
196/196 - 82s - loss: 258.8626 - MinusLogProbMetric: 258.8626 - val_loss: 259.0683 - val_MinusLogProbMetric: 259.0683 - lr: 2.0576e-06 - 82s/epoch - 417ms/step
Epoch 468/1000
2023-10-26 18:06:40.162 
Epoch 468/1000 
	 loss: 258.5325, MinusLogProbMetric: 258.5325, val_loss: 258.5753, val_MinusLogProbMetric: 258.5753

Epoch 468: val_loss did not improve from 174.00845
196/196 - 77s - loss: 258.5325 - MinusLogProbMetric: 258.5325 - val_loss: 258.5753 - val_MinusLogProbMetric: 258.5753 - lr: 2.0576e-06 - 77s/epoch - 395ms/step
Epoch 469/1000
2023-10-26 18:07:55.840 
Epoch 469/1000 
	 loss: 258.1044, MinusLogProbMetric: 258.1044, val_loss: 258.1658, val_MinusLogProbMetric: 258.1658

Epoch 469: val_loss did not improve from 174.00845
196/196 - 76s - loss: 258.1044 - MinusLogProbMetric: 258.1044 - val_loss: 258.1658 - val_MinusLogProbMetric: 258.1658 - lr: 2.0576e-06 - 76s/epoch - 386ms/step
Epoch 470/1000
2023-10-26 18:09:16.733 
Epoch 470/1000 
	 loss: 257.7656, MinusLogProbMetric: 257.7656, val_loss: 257.8458, val_MinusLogProbMetric: 257.8458

Epoch 470: val_loss did not improve from 174.00845
196/196 - 81s - loss: 257.7656 - MinusLogProbMetric: 257.7656 - val_loss: 257.8458 - val_MinusLogProbMetric: 257.8458 - lr: 2.0576e-06 - 81s/epoch - 413ms/step
Epoch 471/1000
2023-10-26 18:10:38.904 
Epoch 471/1000 
	 loss: 257.4353, MinusLogProbMetric: 257.4353, val_loss: 257.4947, val_MinusLogProbMetric: 257.4947

Epoch 471: val_loss did not improve from 174.00845
196/196 - 82s - loss: 257.4353 - MinusLogProbMetric: 257.4353 - val_loss: 257.4947 - val_MinusLogProbMetric: 257.4947 - lr: 2.0576e-06 - 82s/epoch - 419ms/step
Epoch 472/1000
2023-10-26 18:12:01.190 
Epoch 472/1000 
	 loss: 257.0866, MinusLogProbMetric: 257.0866, val_loss: 257.1421, val_MinusLogProbMetric: 257.1421

Epoch 472: val_loss did not improve from 174.00845
196/196 - 82s - loss: 257.0866 - MinusLogProbMetric: 257.0866 - val_loss: 257.1421 - val_MinusLogProbMetric: 257.1421 - lr: 2.0576e-06 - 82s/epoch - 420ms/step
Epoch 473/1000
2023-10-26 18:13:23.812 
Epoch 473/1000 
	 loss: 256.7994, MinusLogProbMetric: 256.7994, val_loss: 256.9423, val_MinusLogProbMetric: 256.9423

Epoch 473: val_loss did not improve from 174.00845
196/196 - 83s - loss: 256.7994 - MinusLogProbMetric: 256.7994 - val_loss: 256.9423 - val_MinusLogProbMetric: 256.9423 - lr: 2.0576e-06 - 83s/epoch - 422ms/step
Epoch 474/1000
2023-10-26 18:14:45.758 
Epoch 474/1000 
	 loss: 256.4655, MinusLogProbMetric: 256.4655, val_loss: 256.5883, val_MinusLogProbMetric: 256.5883

Epoch 474: val_loss did not improve from 174.00845
196/196 - 82s - loss: 256.4655 - MinusLogProbMetric: 256.4655 - val_loss: 256.5883 - val_MinusLogProbMetric: 256.5883 - lr: 2.0576e-06 - 82s/epoch - 418ms/step
Epoch 475/1000
2023-10-26 18:16:07.880 
Epoch 475/1000 
	 loss: 256.1772, MinusLogProbMetric: 256.1772, val_loss: 256.1446, val_MinusLogProbMetric: 256.1446

Epoch 475: val_loss did not improve from 174.00845
Restoring model weights from the end of the best epoch: 375.
196/196 - 83s - loss: 256.1772 - MinusLogProbMetric: 256.1772 - val_loss: 256.1446 - val_MinusLogProbMetric: 256.1446 - lr: 2.0576e-06 - 83s/epoch - 423ms/step
Epoch 475: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 721.
Model trained in 37088.88 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.62 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.95 s.
===========
Run 376/720 done in 46197.27 s.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

===========
Generating train data for run 393.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_393/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_393/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_393/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_393
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_210"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_211 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f0d4ab69150>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d6578e890>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d6578e890>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d04201870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b7cefae90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b7cefbd60>, <keras.callbacks.ModelCheckpoint object at 0x7f0b7cefae60>, <keras.callbacks.EarlyStopping object at 0x7f0b7cef9450>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b7cefb2b0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b7cef80d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_393/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 393/720 with hyperparameters:
timestamp = 2023-10-26 18:16:15.605158
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-26 18:17:51.024 
Epoch 1/1000 
	 loss: 761.8929, MinusLogProbMetric: 761.8929, val_loss: 170.7003, val_MinusLogProbMetric: 170.7003

Epoch 1: val_loss improved from inf to 170.70027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 96s - loss: 761.8929 - MinusLogProbMetric: 761.8929 - val_loss: 170.7003 - val_MinusLogProbMetric: 170.7003 - lr: 0.0010 - 96s/epoch - 488ms/step
Epoch 2/1000
2023-10-26 18:18:23.585 
Epoch 2/1000 
	 loss: 140.0499, MinusLogProbMetric: 140.0499, val_loss: 115.4229, val_MinusLogProbMetric: 115.4229

Epoch 2: val_loss improved from 170.70027 to 115.42294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 32s - loss: 140.0499 - MinusLogProbMetric: 140.0499 - val_loss: 115.4229 - val_MinusLogProbMetric: 115.4229 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 3/1000
2023-10-26 18:18:54.042 
Epoch 3/1000 
	 loss: 103.6029, MinusLogProbMetric: 103.6029, val_loss: 187.5869, val_MinusLogProbMetric: 187.5869

Epoch 3: val_loss did not improve from 115.42294
196/196 - 30s - loss: 103.6029 - MinusLogProbMetric: 103.6029 - val_loss: 187.5869 - val_MinusLogProbMetric: 187.5869 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 4/1000
2023-10-26 18:19:27.318 
Epoch 4/1000 
	 loss: 91.1589, MinusLogProbMetric: 91.1589, val_loss: 75.9776, val_MinusLogProbMetric: 75.9776

Epoch 4: val_loss improved from 115.42294 to 75.97762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 34s - loss: 91.1589 - MinusLogProbMetric: 91.1589 - val_loss: 75.9776 - val_MinusLogProbMetric: 75.9776 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 5/1000
2023-10-26 18:20:02.901 
Epoch 5/1000 
	 loss: 70.4722, MinusLogProbMetric: 70.4722, val_loss: 70.9169, val_MinusLogProbMetric: 70.9169

Epoch 5: val_loss improved from 75.97762 to 70.91689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 70.4722 - MinusLogProbMetric: 70.4722 - val_loss: 70.9169 - val_MinusLogProbMetric: 70.9169 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 6/1000
2023-10-26 18:20:38.715 
Epoch 6/1000 
	 loss: 62.4692, MinusLogProbMetric: 62.4692, val_loss: 58.7230, val_MinusLogProbMetric: 58.7230

Epoch 6: val_loss improved from 70.91689 to 58.72300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 62.4692 - MinusLogProbMetric: 62.4692 - val_loss: 58.7230 - val_MinusLogProbMetric: 58.7230 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 7/1000
2023-10-26 18:21:14.611 
Epoch 7/1000 
	 loss: 56.5520, MinusLogProbMetric: 56.5520, val_loss: 57.9205, val_MinusLogProbMetric: 57.9205

Epoch 7: val_loss improved from 58.72300 to 57.92051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 56.5520 - MinusLogProbMetric: 56.5520 - val_loss: 57.9205 - val_MinusLogProbMetric: 57.9205 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 8/1000
2023-10-26 18:21:50.049 
Epoch 8/1000 
	 loss: 52.7463, MinusLogProbMetric: 52.7463, val_loss: 51.0264, val_MinusLogProbMetric: 51.0264

Epoch 8: val_loss improved from 57.92051 to 51.02641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 52.7463 - MinusLogProbMetric: 52.7463 - val_loss: 51.0264 - val_MinusLogProbMetric: 51.0264 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 9/1000
2023-10-26 18:22:25.278 
Epoch 9/1000 
	 loss: 49.6416, MinusLogProbMetric: 49.6416, val_loss: 48.1797, val_MinusLogProbMetric: 48.1797

Epoch 9: val_loss improved from 51.02641 to 48.17973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 49.6416 - MinusLogProbMetric: 49.6416 - val_loss: 48.1797 - val_MinusLogProbMetric: 48.1797 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 10/1000
2023-10-26 18:23:00.558 
Epoch 10/1000 
	 loss: 47.6025, MinusLogProbMetric: 47.6025, val_loss: 47.4117, val_MinusLogProbMetric: 47.4117

Epoch 10: val_loss improved from 48.17973 to 47.41171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 47.6025 - MinusLogProbMetric: 47.6025 - val_loss: 47.4117 - val_MinusLogProbMetric: 47.4117 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 11/1000
2023-10-26 18:23:36.370 
Epoch 11/1000 
	 loss: 45.8899, MinusLogProbMetric: 45.8899, val_loss: 44.1842, val_MinusLogProbMetric: 44.1842

Epoch 11: val_loss improved from 47.41171 to 44.18422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 45.8899 - MinusLogProbMetric: 45.8899 - val_loss: 44.1842 - val_MinusLogProbMetric: 44.1842 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 12/1000
2023-10-26 18:24:11.857 
Epoch 12/1000 
	 loss: 44.3568, MinusLogProbMetric: 44.3568, val_loss: 44.8915, val_MinusLogProbMetric: 44.8915

Epoch 12: val_loss did not improve from 44.18422
196/196 - 35s - loss: 44.3568 - MinusLogProbMetric: 44.3568 - val_loss: 44.8915 - val_MinusLogProbMetric: 44.8915 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 13/1000
2023-10-26 18:24:46.994 
Epoch 13/1000 
	 loss: 45.6859, MinusLogProbMetric: 45.6859, val_loss: 42.7856, val_MinusLogProbMetric: 42.7856

Epoch 13: val_loss improved from 44.18422 to 42.78556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 45.6859 - MinusLogProbMetric: 45.6859 - val_loss: 42.7856 - val_MinusLogProbMetric: 42.7856 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 14/1000
2023-10-26 18:25:22.360 
Epoch 14/1000 
	 loss: 41.9447, MinusLogProbMetric: 41.9447, val_loss: 41.8231, val_MinusLogProbMetric: 41.8231

Epoch 14: val_loss improved from 42.78556 to 41.82306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 41.9447 - MinusLogProbMetric: 41.9447 - val_loss: 41.8231 - val_MinusLogProbMetric: 41.8231 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 15/1000
2023-10-26 18:25:57.796 
Epoch 15/1000 
	 loss: 41.4362, MinusLogProbMetric: 41.4362, val_loss: 40.6926, val_MinusLogProbMetric: 40.6926

Epoch 15: val_loss improved from 41.82306 to 40.69262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 41.4362 - MinusLogProbMetric: 41.4362 - val_loss: 40.6926 - val_MinusLogProbMetric: 40.6926 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 16/1000
2023-10-26 18:26:33.122 
Epoch 16/1000 
	 loss: 41.3139, MinusLogProbMetric: 41.3139, val_loss: 39.2829, val_MinusLogProbMetric: 39.2829

Epoch 16: val_loss improved from 40.69262 to 39.28286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 41.3139 - MinusLogProbMetric: 41.3139 - val_loss: 39.2829 - val_MinusLogProbMetric: 39.2829 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 17/1000
2023-10-26 18:27:08.608 
Epoch 17/1000 
	 loss: 39.9901, MinusLogProbMetric: 39.9901, val_loss: 39.4710, val_MinusLogProbMetric: 39.4710

Epoch 17: val_loss did not improve from 39.28286
196/196 - 35s - loss: 39.9901 - MinusLogProbMetric: 39.9901 - val_loss: 39.4710 - val_MinusLogProbMetric: 39.4710 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 18/1000
2023-10-26 18:27:43.560 
Epoch 18/1000 
	 loss: 40.3263, MinusLogProbMetric: 40.3263, val_loss: 38.3065, val_MinusLogProbMetric: 38.3065

Epoch 18: val_loss improved from 39.28286 to 38.30645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 40.3263 - MinusLogProbMetric: 40.3263 - val_loss: 38.3065 - val_MinusLogProbMetric: 38.3065 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 19/1000
2023-10-26 18:28:18.834 
Epoch 19/1000 
	 loss: 38.9108, MinusLogProbMetric: 38.9108, val_loss: 39.4276, val_MinusLogProbMetric: 39.4276

Epoch 19: val_loss did not improve from 38.30645
196/196 - 35s - loss: 38.9108 - MinusLogProbMetric: 38.9108 - val_loss: 39.4276 - val_MinusLogProbMetric: 39.4276 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 20/1000
2023-10-26 18:28:53.557 
Epoch 20/1000 
	 loss: 40.1244, MinusLogProbMetric: 40.1244, val_loss: 39.3252, val_MinusLogProbMetric: 39.3252

Epoch 20: val_loss did not improve from 38.30645
196/196 - 35s - loss: 40.1244 - MinusLogProbMetric: 40.1244 - val_loss: 39.3252 - val_MinusLogProbMetric: 39.3252 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 21/1000
2023-10-26 18:29:28.700 
Epoch 21/1000 
	 loss: 38.0758, MinusLogProbMetric: 38.0758, val_loss: 38.1189, val_MinusLogProbMetric: 38.1189

Epoch 21: val_loss improved from 38.30645 to 38.11888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 38.0758 - MinusLogProbMetric: 38.0758 - val_loss: 38.1189 - val_MinusLogProbMetric: 38.1189 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 22/1000
2023-10-26 18:30:04.177 
Epoch 22/1000 
	 loss: 38.1636, MinusLogProbMetric: 38.1636, val_loss: 38.3299, val_MinusLogProbMetric: 38.3299

Epoch 22: val_loss did not improve from 38.11888
196/196 - 35s - loss: 38.1636 - MinusLogProbMetric: 38.1636 - val_loss: 38.3299 - val_MinusLogProbMetric: 38.3299 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 23/1000
2023-10-26 18:30:39.266 
Epoch 23/1000 
	 loss: 37.2868, MinusLogProbMetric: 37.2868, val_loss: 37.0775, val_MinusLogProbMetric: 37.0775

Epoch 23: val_loss improved from 38.11888 to 37.07754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 37.2868 - MinusLogProbMetric: 37.2868 - val_loss: 37.0775 - val_MinusLogProbMetric: 37.0775 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 24/1000
2023-10-26 18:31:14.820 
Epoch 24/1000 
	 loss: 42.1468, MinusLogProbMetric: 42.1468, val_loss: 36.6245, val_MinusLogProbMetric: 36.6245

Epoch 24: val_loss improved from 37.07754 to 36.62449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 42.1468 - MinusLogProbMetric: 42.1468 - val_loss: 36.6245 - val_MinusLogProbMetric: 36.6245 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 25/1000
2023-10-26 18:31:50.650 
Epoch 25/1000 
	 loss: 36.3377, MinusLogProbMetric: 36.3377, val_loss: 36.4700, val_MinusLogProbMetric: 36.4700

Epoch 25: val_loss improved from 36.62449 to 36.46997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 36.3377 - MinusLogProbMetric: 36.3377 - val_loss: 36.4700 - val_MinusLogProbMetric: 36.4700 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 26/1000
2023-10-26 18:32:26.300 
Epoch 26/1000 
	 loss: 36.5407, MinusLogProbMetric: 36.5407, val_loss: 37.3093, val_MinusLogProbMetric: 37.3093

Epoch 26: val_loss did not improve from 36.46997
196/196 - 35s - loss: 36.5407 - MinusLogProbMetric: 36.5407 - val_loss: 37.3093 - val_MinusLogProbMetric: 37.3093 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 27/1000
2023-10-26 18:33:01.257 
Epoch 27/1000 
	 loss: 36.1945, MinusLogProbMetric: 36.1945, val_loss: 38.3940, val_MinusLogProbMetric: 38.3940

Epoch 27: val_loss did not improve from 36.46997
196/196 - 35s - loss: 36.1945 - MinusLogProbMetric: 36.1945 - val_loss: 38.3940 - val_MinusLogProbMetric: 38.3940 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 28/1000
2023-10-26 18:33:36.386 
Epoch 28/1000 
	 loss: 35.9754, MinusLogProbMetric: 35.9754, val_loss: 35.8640, val_MinusLogProbMetric: 35.8640

Epoch 28: val_loss improved from 36.46997 to 35.86404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 35.9754 - MinusLogProbMetric: 35.9754 - val_loss: 35.8640 - val_MinusLogProbMetric: 35.8640 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 29/1000
2023-10-26 18:34:11.985 
Epoch 29/1000 
	 loss: 36.0663, MinusLogProbMetric: 36.0663, val_loss: 35.7505, val_MinusLogProbMetric: 35.7505

Epoch 29: val_loss improved from 35.86404 to 35.75046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 36.0663 - MinusLogProbMetric: 36.0663 - val_loss: 35.7505 - val_MinusLogProbMetric: 35.7505 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 30/1000
2023-10-26 18:34:47.635 
Epoch 30/1000 
	 loss: 35.6684, MinusLogProbMetric: 35.6684, val_loss: 34.9916, val_MinusLogProbMetric: 34.9916

Epoch 30: val_loss improved from 35.75046 to 34.99155, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 35.6684 - MinusLogProbMetric: 35.6684 - val_loss: 34.9916 - val_MinusLogProbMetric: 34.9916 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 31/1000
2023-10-26 18:35:23.026 
Epoch 31/1000 
	 loss: 35.6984, MinusLogProbMetric: 35.6984, val_loss: 36.6614, val_MinusLogProbMetric: 36.6614

Epoch 31: val_loss did not improve from 34.99155
196/196 - 35s - loss: 35.6984 - MinusLogProbMetric: 35.6984 - val_loss: 36.6614 - val_MinusLogProbMetric: 36.6614 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 32/1000
2023-10-26 18:35:57.958 
Epoch 32/1000 
	 loss: 35.6816, MinusLogProbMetric: 35.6816, val_loss: 34.4549, val_MinusLogProbMetric: 34.4549

Epoch 32: val_loss improved from 34.99155 to 34.45486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 35.6816 - MinusLogProbMetric: 35.6816 - val_loss: 34.4549 - val_MinusLogProbMetric: 34.4549 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 33/1000
2023-10-26 18:36:33.324 
Epoch 33/1000 
	 loss: 35.0278, MinusLogProbMetric: 35.0278, val_loss: 35.6926, val_MinusLogProbMetric: 35.6926

Epoch 33: val_loss did not improve from 34.45486
196/196 - 35s - loss: 35.0278 - MinusLogProbMetric: 35.0278 - val_loss: 35.6926 - val_MinusLogProbMetric: 35.6926 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 34/1000
2023-10-26 18:37:08.070 
Epoch 34/1000 
	 loss: 35.2389, MinusLogProbMetric: 35.2389, val_loss: 34.9334, val_MinusLogProbMetric: 34.9334

Epoch 34: val_loss did not improve from 34.45486
196/196 - 35s - loss: 35.2389 - MinusLogProbMetric: 35.2389 - val_loss: 34.9334 - val_MinusLogProbMetric: 34.9334 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 35/1000
2023-10-26 18:37:43.090 
Epoch 35/1000 
	 loss: 34.7107, MinusLogProbMetric: 34.7107, val_loss: 34.4566, val_MinusLogProbMetric: 34.4566

Epoch 35: val_loss did not improve from 34.45486
196/196 - 35s - loss: 34.7107 - MinusLogProbMetric: 34.7107 - val_loss: 34.4566 - val_MinusLogProbMetric: 34.4566 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 36/1000
2023-10-26 18:38:18.066 
Epoch 36/1000 
	 loss: 34.7023, MinusLogProbMetric: 34.7023, val_loss: 34.2505, val_MinusLogProbMetric: 34.2505

Epoch 36: val_loss improved from 34.45486 to 34.25051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 34.7023 - MinusLogProbMetric: 34.7023 - val_loss: 34.2505 - val_MinusLogProbMetric: 34.2505 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 37/1000
2023-10-26 18:38:53.678 
Epoch 37/1000 
	 loss: 34.5757, MinusLogProbMetric: 34.5757, val_loss: 34.6755, val_MinusLogProbMetric: 34.6755

Epoch 37: val_loss did not improve from 34.25051
196/196 - 35s - loss: 34.5757 - MinusLogProbMetric: 34.5757 - val_loss: 34.6755 - val_MinusLogProbMetric: 34.6755 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 38/1000
2023-10-26 18:39:28.404 
Epoch 38/1000 
	 loss: 34.3825, MinusLogProbMetric: 34.3825, val_loss: 34.3028, val_MinusLogProbMetric: 34.3028

Epoch 38: val_loss did not improve from 34.25051
196/196 - 35s - loss: 34.3825 - MinusLogProbMetric: 34.3825 - val_loss: 34.3028 - val_MinusLogProbMetric: 34.3028 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 39/1000
2023-10-26 18:40:03.323 
Epoch 39/1000 
	 loss: 34.6534, MinusLogProbMetric: 34.6534, val_loss: 33.8890, val_MinusLogProbMetric: 33.8890

Epoch 39: val_loss improved from 34.25051 to 33.88900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 34.6534 - MinusLogProbMetric: 34.6534 - val_loss: 33.8890 - val_MinusLogProbMetric: 33.8890 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 40/1000
2023-10-26 18:40:38.696 
Epoch 40/1000 
	 loss: 34.4108, MinusLogProbMetric: 34.4108, val_loss: 35.5907, val_MinusLogProbMetric: 35.5907

Epoch 40: val_loss did not improve from 33.88900
196/196 - 35s - loss: 34.4108 - MinusLogProbMetric: 34.4108 - val_loss: 35.5907 - val_MinusLogProbMetric: 35.5907 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 41/1000
2023-10-26 18:41:13.388 
Epoch 41/1000 
	 loss: 34.0681, MinusLogProbMetric: 34.0681, val_loss: 33.8828, val_MinusLogProbMetric: 33.8828

Epoch 41: val_loss improved from 33.88900 to 33.88283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 34.0681 - MinusLogProbMetric: 34.0681 - val_loss: 33.8828 - val_MinusLogProbMetric: 33.8828 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 42/1000
2023-10-26 18:41:48.968 
Epoch 42/1000 
	 loss: 34.1743, MinusLogProbMetric: 34.1743, val_loss: 34.2988, val_MinusLogProbMetric: 34.2988

Epoch 42: val_loss did not improve from 33.88283
196/196 - 35s - loss: 34.1743 - MinusLogProbMetric: 34.1743 - val_loss: 34.2988 - val_MinusLogProbMetric: 34.2988 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 43/1000
2023-10-26 18:42:24.221 
Epoch 43/1000 
	 loss: 33.8603, MinusLogProbMetric: 33.8603, val_loss: 33.8961, val_MinusLogProbMetric: 33.8961

Epoch 43: val_loss did not improve from 33.88283
196/196 - 35s - loss: 33.8603 - MinusLogProbMetric: 33.8603 - val_loss: 33.8961 - val_MinusLogProbMetric: 33.8961 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 44/1000
2023-10-26 18:42:59.221 
Epoch 44/1000 
	 loss: 33.7830, MinusLogProbMetric: 33.7830, val_loss: 32.7886, val_MinusLogProbMetric: 32.7886

Epoch 44: val_loss improved from 33.88283 to 32.78862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 33.7830 - MinusLogProbMetric: 33.7830 - val_loss: 32.7886 - val_MinusLogProbMetric: 32.7886 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 45/1000
2023-10-26 18:43:35.062 
Epoch 45/1000 
	 loss: 33.6532, MinusLogProbMetric: 33.6532, val_loss: 33.2949, val_MinusLogProbMetric: 33.2949

Epoch 45: val_loss did not improve from 32.78862
196/196 - 35s - loss: 33.6532 - MinusLogProbMetric: 33.6532 - val_loss: 33.2949 - val_MinusLogProbMetric: 33.2949 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 46/1000
2023-10-26 18:44:10.215 
Epoch 46/1000 
	 loss: 33.6414, MinusLogProbMetric: 33.6414, val_loss: 34.8095, val_MinusLogProbMetric: 34.8095

Epoch 46: val_loss did not improve from 32.78862
196/196 - 35s - loss: 33.6414 - MinusLogProbMetric: 33.6414 - val_loss: 34.8095 - val_MinusLogProbMetric: 34.8095 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 47/1000
2023-10-26 18:44:45.128 
Epoch 47/1000 
	 loss: 33.6224, MinusLogProbMetric: 33.6224, val_loss: 33.8419, val_MinusLogProbMetric: 33.8419

Epoch 47: val_loss did not improve from 32.78862
196/196 - 35s - loss: 33.6224 - MinusLogProbMetric: 33.6224 - val_loss: 33.8419 - val_MinusLogProbMetric: 33.8419 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 48/1000
2023-10-26 18:45:20.115 
Epoch 48/1000 
	 loss: 33.7034, MinusLogProbMetric: 33.7034, val_loss: 34.8128, val_MinusLogProbMetric: 34.8128

Epoch 48: val_loss did not improve from 32.78862
196/196 - 35s - loss: 33.7034 - MinusLogProbMetric: 33.7034 - val_loss: 34.8128 - val_MinusLogProbMetric: 34.8128 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 49/1000
2023-10-26 18:45:55.348 
Epoch 49/1000 
	 loss: 33.5811, MinusLogProbMetric: 33.5811, val_loss: 33.5497, val_MinusLogProbMetric: 33.5497

Epoch 49: val_loss did not improve from 32.78862
196/196 - 35s - loss: 33.5811 - MinusLogProbMetric: 33.5811 - val_loss: 33.5497 - val_MinusLogProbMetric: 33.5497 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 50/1000
2023-10-26 18:46:30.726 
Epoch 50/1000 
	 loss: 33.0337, MinusLogProbMetric: 33.0337, val_loss: 33.1502, val_MinusLogProbMetric: 33.1502

Epoch 50: val_loss did not improve from 32.78862
196/196 - 35s - loss: 33.0337 - MinusLogProbMetric: 33.0337 - val_loss: 33.1502 - val_MinusLogProbMetric: 33.1502 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 51/1000
2023-10-26 18:47:05.884 
Epoch 51/1000 
	 loss: 32.9896, MinusLogProbMetric: 32.9896, val_loss: 32.7502, val_MinusLogProbMetric: 32.7502

Epoch 51: val_loss improved from 32.78862 to 32.75017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 32.9896 - MinusLogProbMetric: 32.9896 - val_loss: 32.7502 - val_MinusLogProbMetric: 32.7502 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 52/1000
2023-10-26 18:47:41.227 
Epoch 52/1000 
	 loss: 33.0702, MinusLogProbMetric: 33.0702, val_loss: 32.3571, val_MinusLogProbMetric: 32.3571

Epoch 52: val_loss improved from 32.75017 to 32.35712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 33.0702 - MinusLogProbMetric: 33.0702 - val_loss: 32.3571 - val_MinusLogProbMetric: 32.3571 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 53/1000
2023-10-26 18:48:16.951 
Epoch 53/1000 
	 loss: 32.9711, MinusLogProbMetric: 32.9711, val_loss: 32.9628, val_MinusLogProbMetric: 32.9628

Epoch 53: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.9711 - MinusLogProbMetric: 32.9711 - val_loss: 32.9628 - val_MinusLogProbMetric: 32.9628 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 54/1000
2023-10-26 18:48:52.434 
Epoch 54/1000 
	 loss: 32.8438, MinusLogProbMetric: 32.8438, val_loss: 32.5830, val_MinusLogProbMetric: 32.5830

Epoch 54: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.8438 - MinusLogProbMetric: 32.8438 - val_loss: 32.5830 - val_MinusLogProbMetric: 32.5830 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 55/1000
2023-10-26 18:49:27.244 
Epoch 55/1000 
	 loss: 32.8817, MinusLogProbMetric: 32.8817, val_loss: 33.9663, val_MinusLogProbMetric: 33.9663

Epoch 55: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.8817 - MinusLogProbMetric: 32.8817 - val_loss: 33.9663 - val_MinusLogProbMetric: 33.9663 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 56/1000
2023-10-26 18:50:02.375 
Epoch 56/1000 
	 loss: 32.9738, MinusLogProbMetric: 32.9738, val_loss: 38.1223, val_MinusLogProbMetric: 38.1223

Epoch 56: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.9738 - MinusLogProbMetric: 32.9738 - val_loss: 38.1223 - val_MinusLogProbMetric: 38.1223 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 57/1000
2023-10-26 18:50:37.168 
Epoch 57/1000 
	 loss: 32.9017, MinusLogProbMetric: 32.9017, val_loss: 33.5022, val_MinusLogProbMetric: 33.5022

Epoch 57: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.9017 - MinusLogProbMetric: 32.9017 - val_loss: 33.5022 - val_MinusLogProbMetric: 33.5022 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 58/1000
2023-10-26 18:51:12.307 
Epoch 58/1000 
	 loss: 32.6437, MinusLogProbMetric: 32.6437, val_loss: 32.5325, val_MinusLogProbMetric: 32.5325

Epoch 58: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.6437 - MinusLogProbMetric: 32.6437 - val_loss: 32.5325 - val_MinusLogProbMetric: 32.5325 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 59/1000
2023-10-26 18:51:47.697 
Epoch 59/1000 
	 loss: 32.7188, MinusLogProbMetric: 32.7188, val_loss: 32.7007, val_MinusLogProbMetric: 32.7007

Epoch 59: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.7188 - MinusLogProbMetric: 32.7188 - val_loss: 32.7007 - val_MinusLogProbMetric: 32.7007 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 60/1000
2023-10-26 18:52:23.046 
Epoch 60/1000 
	 loss: 32.7267, MinusLogProbMetric: 32.7267, val_loss: 34.9999, val_MinusLogProbMetric: 34.9999

Epoch 60: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.7267 - MinusLogProbMetric: 32.7267 - val_loss: 34.9999 - val_MinusLogProbMetric: 34.9999 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 61/1000
2023-10-26 18:52:58.392 
Epoch 61/1000 
	 loss: 32.5276, MinusLogProbMetric: 32.5276, val_loss: 33.2692, val_MinusLogProbMetric: 33.2692

Epoch 61: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.5276 - MinusLogProbMetric: 32.5276 - val_loss: 33.2692 - val_MinusLogProbMetric: 33.2692 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 62/1000
2023-10-26 18:53:32.923 
Epoch 62/1000 
	 loss: 32.4248, MinusLogProbMetric: 32.4248, val_loss: 32.7273, val_MinusLogProbMetric: 32.7273

Epoch 62: val_loss did not improve from 32.35712
196/196 - 35s - loss: 32.4248 - MinusLogProbMetric: 32.4248 - val_loss: 32.7273 - val_MinusLogProbMetric: 32.7273 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 63/1000
2023-10-26 18:54:07.901 
Epoch 63/1000 
	 loss: 32.3600, MinusLogProbMetric: 32.3600, val_loss: 32.0662, val_MinusLogProbMetric: 32.0662

Epoch 63: val_loss improved from 32.35712 to 32.06624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 32.3600 - MinusLogProbMetric: 32.3600 - val_loss: 32.0662 - val_MinusLogProbMetric: 32.0662 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 64/1000
2023-10-26 18:54:43.526 
Epoch 64/1000 
	 loss: 32.0844, MinusLogProbMetric: 32.0844, val_loss: 32.3865, val_MinusLogProbMetric: 32.3865

Epoch 64: val_loss did not improve from 32.06624
196/196 - 35s - loss: 32.0844 - MinusLogProbMetric: 32.0844 - val_loss: 32.3865 - val_MinusLogProbMetric: 32.3865 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 65/1000
2023-10-26 18:55:18.662 
Epoch 65/1000 
	 loss: 32.2387, MinusLogProbMetric: 32.2387, val_loss: 32.5204, val_MinusLogProbMetric: 32.5204

Epoch 65: val_loss did not improve from 32.06624
196/196 - 35s - loss: 32.2387 - MinusLogProbMetric: 32.2387 - val_loss: 32.5204 - val_MinusLogProbMetric: 32.5204 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 66/1000
2023-10-26 18:55:53.752 
Epoch 66/1000 
	 loss: 32.2771, MinusLogProbMetric: 32.2771, val_loss: 31.9000, val_MinusLogProbMetric: 31.9000

Epoch 66: val_loss improved from 32.06624 to 31.90001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 32.2771 - MinusLogProbMetric: 32.2771 - val_loss: 31.9000 - val_MinusLogProbMetric: 31.9000 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 67/1000
2023-10-26 18:56:29.356 
Epoch 67/1000 
	 loss: 32.1547, MinusLogProbMetric: 32.1547, val_loss: 31.8213, val_MinusLogProbMetric: 31.8213

Epoch 67: val_loss improved from 31.90001 to 31.82133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 32.1547 - MinusLogProbMetric: 32.1547 - val_loss: 31.8213 - val_MinusLogProbMetric: 31.8213 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 68/1000
2023-10-26 18:57:05.352 
Epoch 68/1000 
	 loss: 32.1800, MinusLogProbMetric: 32.1800, val_loss: 32.8473, val_MinusLogProbMetric: 32.8473

Epoch 68: val_loss did not improve from 31.82133
196/196 - 35s - loss: 32.1800 - MinusLogProbMetric: 32.1800 - val_loss: 32.8473 - val_MinusLogProbMetric: 32.8473 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 69/1000
2023-10-26 18:57:40.806 
Epoch 69/1000 
	 loss: 32.0607, MinusLogProbMetric: 32.0607, val_loss: 32.0393, val_MinusLogProbMetric: 32.0393

Epoch 69: val_loss did not improve from 31.82133
196/196 - 35s - loss: 32.0607 - MinusLogProbMetric: 32.0607 - val_loss: 32.0393 - val_MinusLogProbMetric: 32.0393 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 70/1000
2023-10-26 18:58:15.934 
Epoch 70/1000 
	 loss: 32.0924, MinusLogProbMetric: 32.0924, val_loss: 31.7767, val_MinusLogProbMetric: 31.7767

Epoch 70: val_loss improved from 31.82133 to 31.77674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 32.0924 - MinusLogProbMetric: 32.0924 - val_loss: 31.7767 - val_MinusLogProbMetric: 31.7767 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 71/1000
2023-10-26 18:58:51.828 
Epoch 71/1000 
	 loss: 32.2254, MinusLogProbMetric: 32.2254, val_loss: 32.0438, val_MinusLogProbMetric: 32.0438

Epoch 71: val_loss did not improve from 31.77674
196/196 - 35s - loss: 32.2254 - MinusLogProbMetric: 32.2254 - val_loss: 32.0438 - val_MinusLogProbMetric: 32.0438 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 72/1000
2023-10-26 18:59:26.628 
Epoch 72/1000 
	 loss: 31.9364, MinusLogProbMetric: 31.9364, val_loss: 32.5917, val_MinusLogProbMetric: 32.5917

Epoch 72: val_loss did not improve from 31.77674
196/196 - 35s - loss: 31.9364 - MinusLogProbMetric: 31.9364 - val_loss: 32.5917 - val_MinusLogProbMetric: 32.5917 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 73/1000
2023-10-26 19:00:01.682 
Epoch 73/1000 
	 loss: 31.8188, MinusLogProbMetric: 31.8188, val_loss: 31.2987, val_MinusLogProbMetric: 31.2987

Epoch 73: val_loss improved from 31.77674 to 31.29869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 31.8188 - MinusLogProbMetric: 31.8188 - val_loss: 31.2987 - val_MinusLogProbMetric: 31.2987 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 74/1000
2023-10-26 19:00:37.288 
Epoch 74/1000 
	 loss: 31.9610, MinusLogProbMetric: 31.9610, val_loss: 31.6954, val_MinusLogProbMetric: 31.6954

Epoch 74: val_loss did not improve from 31.29869
196/196 - 35s - loss: 31.9610 - MinusLogProbMetric: 31.9610 - val_loss: 31.6954 - val_MinusLogProbMetric: 31.6954 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 75/1000
2023-10-26 19:01:11.959 
Epoch 75/1000 
	 loss: 31.7699, MinusLogProbMetric: 31.7699, val_loss: 32.4898, val_MinusLogProbMetric: 32.4898

Epoch 75: val_loss did not improve from 31.29869
196/196 - 35s - loss: 31.7699 - MinusLogProbMetric: 31.7699 - val_loss: 32.4898 - val_MinusLogProbMetric: 32.4898 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 76/1000
2023-10-26 19:01:46.834 
Epoch 76/1000 
	 loss: 31.8832, MinusLogProbMetric: 31.8832, val_loss: 31.5027, val_MinusLogProbMetric: 31.5027

Epoch 76: val_loss did not improve from 31.29869
196/196 - 35s - loss: 31.8832 - MinusLogProbMetric: 31.8832 - val_loss: 31.5027 - val_MinusLogProbMetric: 31.5027 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 77/1000
2023-10-26 19:02:21.878 
Epoch 77/1000 
	 loss: 31.5780, MinusLogProbMetric: 31.5780, val_loss: 32.5639, val_MinusLogProbMetric: 32.5639

Epoch 77: val_loss did not improve from 31.29869
196/196 - 35s - loss: 31.5780 - MinusLogProbMetric: 31.5780 - val_loss: 32.5639 - val_MinusLogProbMetric: 32.5639 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 78/1000
2023-10-26 19:02:57.024 
Epoch 78/1000 
	 loss: 31.8426, MinusLogProbMetric: 31.8426, val_loss: 31.7299, val_MinusLogProbMetric: 31.7299

Epoch 78: val_loss did not improve from 31.29869
196/196 - 35s - loss: 31.8426 - MinusLogProbMetric: 31.8426 - val_loss: 31.7299 - val_MinusLogProbMetric: 31.7299 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 79/1000
2023-10-26 19:03:32.115 
Epoch 79/1000 
	 loss: 31.6282, MinusLogProbMetric: 31.6282, val_loss: 31.2025, val_MinusLogProbMetric: 31.2025

Epoch 79: val_loss improved from 31.29869 to 31.20248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 31.6282 - MinusLogProbMetric: 31.6282 - val_loss: 31.2025 - val_MinusLogProbMetric: 31.2025 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 80/1000
2023-10-26 19:04:07.365 
Epoch 80/1000 
	 loss: 31.6435, MinusLogProbMetric: 31.6435, val_loss: 31.4439, val_MinusLogProbMetric: 31.4439

Epoch 80: val_loss did not improve from 31.20248
196/196 - 35s - loss: 31.6435 - MinusLogProbMetric: 31.6435 - val_loss: 31.4439 - val_MinusLogProbMetric: 31.4439 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 81/1000
2023-10-26 19:04:42.064 
Epoch 81/1000 
	 loss: 31.5193, MinusLogProbMetric: 31.5193, val_loss: 31.9674, val_MinusLogProbMetric: 31.9674

Epoch 81: val_loss did not improve from 31.20248
196/196 - 35s - loss: 31.5193 - MinusLogProbMetric: 31.5193 - val_loss: 31.9674 - val_MinusLogProbMetric: 31.9674 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 82/1000
2023-10-26 19:05:17.011 
Epoch 82/1000 
	 loss: 31.4949, MinusLogProbMetric: 31.4949, val_loss: 32.7047, val_MinusLogProbMetric: 32.7047

Epoch 82: val_loss did not improve from 31.20248
196/196 - 35s - loss: 31.4949 - MinusLogProbMetric: 31.4949 - val_loss: 32.7047 - val_MinusLogProbMetric: 32.7047 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 83/1000
2023-10-26 19:05:52.027 
Epoch 83/1000 
	 loss: 31.3746, MinusLogProbMetric: 31.3746, val_loss: 31.2412, val_MinusLogProbMetric: 31.2412

Epoch 83: val_loss did not improve from 31.20248
196/196 - 35s - loss: 31.3746 - MinusLogProbMetric: 31.3746 - val_loss: 31.2412 - val_MinusLogProbMetric: 31.2412 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 84/1000
2023-10-26 19:06:26.526 
Epoch 84/1000 
	 loss: 31.3259, MinusLogProbMetric: 31.3259, val_loss: 31.8980, val_MinusLogProbMetric: 31.8980

Epoch 84: val_loss did not improve from 31.20248
196/196 - 34s - loss: 31.3259 - MinusLogProbMetric: 31.3259 - val_loss: 31.8980 - val_MinusLogProbMetric: 31.8980 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 85/1000
2023-10-26 19:07:00.631 
Epoch 85/1000 
	 loss: 31.5493, MinusLogProbMetric: 31.5493, val_loss: 33.1183, val_MinusLogProbMetric: 33.1183

Epoch 85: val_loss did not improve from 31.20248
196/196 - 34s - loss: 31.5493 - MinusLogProbMetric: 31.5493 - val_loss: 33.1183 - val_MinusLogProbMetric: 33.1183 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 86/1000
2023-10-26 19:07:33.440 
Epoch 86/1000 
	 loss: 31.5182, MinusLogProbMetric: 31.5182, val_loss: 31.1282, val_MinusLogProbMetric: 31.1282

Epoch 86: val_loss improved from 31.20248 to 31.12821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 33s - loss: 31.5182 - MinusLogProbMetric: 31.5182 - val_loss: 31.1282 - val_MinusLogProbMetric: 31.1282 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 87/1000
2023-10-26 19:08:07.639 
Epoch 87/1000 
	 loss: 31.4416, MinusLogProbMetric: 31.4416, val_loss: 32.3778, val_MinusLogProbMetric: 32.3778

Epoch 87: val_loss did not improve from 31.12821
196/196 - 34s - loss: 31.4416 - MinusLogProbMetric: 31.4416 - val_loss: 32.3778 - val_MinusLogProbMetric: 32.3778 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 88/1000
2023-10-26 19:08:39.027 
Epoch 88/1000 
	 loss: 31.2077, MinusLogProbMetric: 31.2077, val_loss: 30.8261, val_MinusLogProbMetric: 30.8261

Epoch 88: val_loss improved from 31.12821 to 30.82614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 32s - loss: 31.2077 - MinusLogProbMetric: 31.2077 - val_loss: 30.8261 - val_MinusLogProbMetric: 30.8261 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 89/1000
2023-10-26 19:09:11.485 
Epoch 89/1000 
	 loss: 31.2184, MinusLogProbMetric: 31.2184, val_loss: 31.6094, val_MinusLogProbMetric: 31.6094

Epoch 89: val_loss did not improve from 30.82614
196/196 - 32s - loss: 31.2184 - MinusLogProbMetric: 31.2184 - val_loss: 31.6094 - val_MinusLogProbMetric: 31.6094 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 90/1000
2023-10-26 19:09:46.469 
Epoch 90/1000 
	 loss: 31.2784, MinusLogProbMetric: 31.2784, val_loss: 31.1969, val_MinusLogProbMetric: 31.1969

Epoch 90: val_loss did not improve from 30.82614
196/196 - 35s - loss: 31.2784 - MinusLogProbMetric: 31.2784 - val_loss: 31.1969 - val_MinusLogProbMetric: 31.1969 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 91/1000
2023-10-26 19:10:21.553 
Epoch 91/1000 
	 loss: 31.3129, MinusLogProbMetric: 31.3129, val_loss: 31.2026, val_MinusLogProbMetric: 31.2026

Epoch 91: val_loss did not improve from 30.82614
196/196 - 35s - loss: 31.3129 - MinusLogProbMetric: 31.3129 - val_loss: 31.2026 - val_MinusLogProbMetric: 31.2026 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 92/1000
2023-10-26 19:10:56.282 
Epoch 92/1000 
	 loss: 31.2049, MinusLogProbMetric: 31.2049, val_loss: 31.1580, val_MinusLogProbMetric: 31.1580

Epoch 92: val_loss did not improve from 30.82614
196/196 - 35s - loss: 31.2049 - MinusLogProbMetric: 31.2049 - val_loss: 31.1580 - val_MinusLogProbMetric: 31.1580 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 93/1000
2023-10-26 19:11:30.544 
Epoch 93/1000 
	 loss: 31.1917, MinusLogProbMetric: 31.1917, val_loss: 31.0048, val_MinusLogProbMetric: 31.0048

Epoch 93: val_loss did not improve from 30.82614
196/196 - 34s - loss: 31.1917 - MinusLogProbMetric: 31.1917 - val_loss: 31.0048 - val_MinusLogProbMetric: 31.0048 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 94/1000
2023-10-26 19:12:05.395 
Epoch 94/1000 
	 loss: 31.1425, MinusLogProbMetric: 31.1425, val_loss: 31.7383, val_MinusLogProbMetric: 31.7383

Epoch 94: val_loss did not improve from 30.82614
196/196 - 35s - loss: 31.1425 - MinusLogProbMetric: 31.1425 - val_loss: 31.7383 - val_MinusLogProbMetric: 31.7383 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 95/1000
2023-10-26 19:12:40.358 
Epoch 95/1000 
	 loss: 31.1190, MinusLogProbMetric: 31.1190, val_loss: 30.9983, val_MinusLogProbMetric: 30.9983

Epoch 95: val_loss did not improve from 30.82614
196/196 - 35s - loss: 31.1190 - MinusLogProbMetric: 31.1190 - val_loss: 30.9983 - val_MinusLogProbMetric: 30.9983 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 96/1000
2023-10-26 19:13:15.376 
Epoch 96/1000 
	 loss: 30.9131, MinusLogProbMetric: 30.9131, val_loss: 31.0200, val_MinusLogProbMetric: 31.0200

Epoch 96: val_loss did not improve from 30.82614
196/196 - 35s - loss: 30.9131 - MinusLogProbMetric: 30.9131 - val_loss: 31.0200 - val_MinusLogProbMetric: 31.0200 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 97/1000
2023-10-26 19:13:50.273 
Epoch 97/1000 
	 loss: 31.0823, MinusLogProbMetric: 31.0823, val_loss: 30.5128, val_MinusLogProbMetric: 30.5128

Epoch 97: val_loss improved from 30.82614 to 30.51285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 31.0823 - MinusLogProbMetric: 31.0823 - val_loss: 30.5128 - val_MinusLogProbMetric: 30.5128 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 98/1000
2023-10-26 19:14:25.322 
Epoch 98/1000 
	 loss: 30.9512, MinusLogProbMetric: 30.9512, val_loss: 31.7584, val_MinusLogProbMetric: 31.7584

Epoch 98: val_loss did not improve from 30.51285
196/196 - 35s - loss: 30.9512 - MinusLogProbMetric: 30.9512 - val_loss: 31.7584 - val_MinusLogProbMetric: 31.7584 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 99/1000
2023-10-26 19:15:00.157 
Epoch 99/1000 
	 loss: 30.7840, MinusLogProbMetric: 30.7840, val_loss: 30.5037, val_MinusLogProbMetric: 30.5037

Epoch 99: val_loss improved from 30.51285 to 30.50371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 30.7840 - MinusLogProbMetric: 30.7840 - val_loss: 30.5037 - val_MinusLogProbMetric: 30.5037 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 100/1000
2023-10-26 19:15:35.262 
Epoch 100/1000 
	 loss: 30.9699, MinusLogProbMetric: 30.9699, val_loss: 31.1126, val_MinusLogProbMetric: 31.1126

Epoch 100: val_loss did not improve from 30.50371
196/196 - 35s - loss: 30.9699 - MinusLogProbMetric: 30.9699 - val_loss: 31.1126 - val_MinusLogProbMetric: 31.1126 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 101/1000
2023-10-26 19:16:09.863 
Epoch 101/1000 
	 loss: 30.7824, MinusLogProbMetric: 30.7824, val_loss: 30.1952, val_MinusLogProbMetric: 30.1952

Epoch 101: val_loss improved from 30.50371 to 30.19523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 30.7824 - MinusLogProbMetric: 30.7824 - val_loss: 30.1952 - val_MinusLogProbMetric: 30.1952 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 102/1000
2023-10-26 19:16:45.257 
Epoch 102/1000 
	 loss: 31.0691, MinusLogProbMetric: 31.0691, val_loss: 30.7032, val_MinusLogProbMetric: 30.7032

Epoch 102: val_loss did not improve from 30.19523
196/196 - 35s - loss: 31.0691 - MinusLogProbMetric: 31.0691 - val_loss: 30.7032 - val_MinusLogProbMetric: 30.7032 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 103/1000
2023-10-26 19:17:20.076 
Epoch 103/1000 
	 loss: 31.0072, MinusLogProbMetric: 31.0072, val_loss: 31.0283, val_MinusLogProbMetric: 31.0283

Epoch 103: val_loss did not improve from 30.19523
196/196 - 35s - loss: 31.0072 - MinusLogProbMetric: 31.0072 - val_loss: 31.0283 - val_MinusLogProbMetric: 31.0283 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 104/1000
2023-10-26 19:17:54.799 
Epoch 104/1000 
	 loss: 30.7059, MinusLogProbMetric: 30.7059, val_loss: 30.7945, val_MinusLogProbMetric: 30.7945

Epoch 104: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.7059 - MinusLogProbMetric: 30.7059 - val_loss: 30.7945 - val_MinusLogProbMetric: 30.7945 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 105/1000
2023-10-26 19:18:29.577 
Epoch 105/1000 
	 loss: 30.7774, MinusLogProbMetric: 30.7774, val_loss: 30.2737, val_MinusLogProbMetric: 30.2737

Epoch 105: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.7774 - MinusLogProbMetric: 30.7774 - val_loss: 30.2737 - val_MinusLogProbMetric: 30.2737 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 106/1000
2023-10-26 19:19:04.243 
Epoch 106/1000 
	 loss: 30.7221, MinusLogProbMetric: 30.7221, val_loss: 31.1128, val_MinusLogProbMetric: 31.1128

Epoch 106: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.7221 - MinusLogProbMetric: 30.7221 - val_loss: 31.1128 - val_MinusLogProbMetric: 31.1128 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 107/1000
2023-10-26 19:19:38.933 
Epoch 107/1000 
	 loss: 30.5943, MinusLogProbMetric: 30.5943, val_loss: 31.1966, val_MinusLogProbMetric: 31.1966

Epoch 107: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.5943 - MinusLogProbMetric: 30.5943 - val_loss: 31.1966 - val_MinusLogProbMetric: 31.1966 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 108/1000
2023-10-26 19:20:13.841 
Epoch 108/1000 
	 loss: 30.7278, MinusLogProbMetric: 30.7278, val_loss: 30.4023, val_MinusLogProbMetric: 30.4023

Epoch 108: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.7278 - MinusLogProbMetric: 30.7278 - val_loss: 30.4023 - val_MinusLogProbMetric: 30.4023 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 109/1000
2023-10-26 19:20:48.708 
Epoch 109/1000 
	 loss: 30.6623, MinusLogProbMetric: 30.6623, val_loss: 30.8555, val_MinusLogProbMetric: 30.8555

Epoch 109: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.6623 - MinusLogProbMetric: 30.6623 - val_loss: 30.8555 - val_MinusLogProbMetric: 30.8555 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 110/1000
2023-10-26 19:21:23.514 
Epoch 110/1000 
	 loss: 30.7694, MinusLogProbMetric: 30.7694, val_loss: 30.8296, val_MinusLogProbMetric: 30.8296

Epoch 110: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.7694 - MinusLogProbMetric: 30.7694 - val_loss: 30.8296 - val_MinusLogProbMetric: 30.8296 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 111/1000
2023-10-26 19:21:58.062 
Epoch 111/1000 
	 loss: 30.5101, MinusLogProbMetric: 30.5101, val_loss: 30.7272, val_MinusLogProbMetric: 30.7272

Epoch 111: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.5101 - MinusLogProbMetric: 30.5101 - val_loss: 30.7272 - val_MinusLogProbMetric: 30.7272 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 112/1000
2023-10-26 19:22:32.640 
Epoch 112/1000 
	 loss: 30.7220, MinusLogProbMetric: 30.7220, val_loss: 30.6887, val_MinusLogProbMetric: 30.6887

Epoch 112: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.7220 - MinusLogProbMetric: 30.7220 - val_loss: 30.6887 - val_MinusLogProbMetric: 30.6887 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 113/1000
2023-10-26 19:23:07.632 
Epoch 113/1000 
	 loss: 30.5879, MinusLogProbMetric: 30.5879, val_loss: 31.3010, val_MinusLogProbMetric: 31.3010

Epoch 113: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.5879 - MinusLogProbMetric: 30.5879 - val_loss: 31.3010 - val_MinusLogProbMetric: 31.3010 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 114/1000
2023-10-26 19:23:42.332 
Epoch 114/1000 
	 loss: 30.4853, MinusLogProbMetric: 30.4853, val_loss: 32.2701, val_MinusLogProbMetric: 32.2701

Epoch 114: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.4853 - MinusLogProbMetric: 30.4853 - val_loss: 32.2701 - val_MinusLogProbMetric: 32.2701 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 115/1000
2023-10-26 19:24:16.962 
Epoch 115/1000 
	 loss: 30.5350, MinusLogProbMetric: 30.5350, val_loss: 30.5678, val_MinusLogProbMetric: 30.5678

Epoch 115: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.5350 - MinusLogProbMetric: 30.5350 - val_loss: 30.5678 - val_MinusLogProbMetric: 30.5678 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 116/1000
2023-10-26 19:24:51.715 
Epoch 116/1000 
	 loss: 30.5641, MinusLogProbMetric: 30.5641, val_loss: 31.4046, val_MinusLogProbMetric: 31.4046

Epoch 116: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.5641 - MinusLogProbMetric: 30.5641 - val_loss: 31.4046 - val_MinusLogProbMetric: 31.4046 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 117/1000
2023-10-26 19:25:26.366 
Epoch 117/1000 
	 loss: 30.5123, MinusLogProbMetric: 30.5123, val_loss: 31.1606, val_MinusLogProbMetric: 31.1606

Epoch 117: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.5123 - MinusLogProbMetric: 30.5123 - val_loss: 31.1606 - val_MinusLogProbMetric: 31.1606 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 118/1000
2023-10-26 19:26:01.045 
Epoch 118/1000 
	 loss: 30.5033, MinusLogProbMetric: 30.5033, val_loss: 31.2009, val_MinusLogProbMetric: 31.2009

Epoch 118: val_loss did not improve from 30.19523
196/196 - 35s - loss: 30.5033 - MinusLogProbMetric: 30.5033 - val_loss: 31.2009 - val_MinusLogProbMetric: 31.2009 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 119/1000
2023-10-26 19:26:35.447 
Epoch 119/1000 
	 loss: 30.4693, MinusLogProbMetric: 30.4693, val_loss: 31.0954, val_MinusLogProbMetric: 31.0954

Epoch 119: val_loss did not improve from 30.19523
196/196 - 34s - loss: 30.4693 - MinusLogProbMetric: 30.4693 - val_loss: 31.0954 - val_MinusLogProbMetric: 31.0954 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 120/1000
2023-10-26 19:27:10.173 
Epoch 120/1000 
	 loss: 30.2916, MinusLogProbMetric: 30.2916, val_loss: 30.0908, val_MinusLogProbMetric: 30.0908

Epoch 120: val_loss improved from 30.19523 to 30.09083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 30.2916 - MinusLogProbMetric: 30.2916 - val_loss: 30.0908 - val_MinusLogProbMetric: 30.0908 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 121/1000
2023-10-26 19:27:45.267 
Epoch 121/1000 
	 loss: 30.3207, MinusLogProbMetric: 30.3207, val_loss: 30.7929, val_MinusLogProbMetric: 30.7929

Epoch 121: val_loss did not improve from 30.09083
196/196 - 35s - loss: 30.3207 - MinusLogProbMetric: 30.3207 - val_loss: 30.7929 - val_MinusLogProbMetric: 30.7929 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 122/1000
2023-10-26 19:28:20.022 
Epoch 122/1000 
	 loss: 30.4935, MinusLogProbMetric: 30.4935, val_loss: 30.8376, val_MinusLogProbMetric: 30.8376

Epoch 122: val_loss did not improve from 30.09083
196/196 - 35s - loss: 30.4935 - MinusLogProbMetric: 30.4935 - val_loss: 30.8376 - val_MinusLogProbMetric: 30.8376 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 123/1000
2023-10-26 19:28:54.860 
Epoch 123/1000 
	 loss: 30.3323, MinusLogProbMetric: 30.3323, val_loss: 30.4725, val_MinusLogProbMetric: 30.4725

Epoch 123: val_loss did not improve from 30.09083
196/196 - 35s - loss: 30.3323 - MinusLogProbMetric: 30.3323 - val_loss: 30.4725 - val_MinusLogProbMetric: 30.4725 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 124/1000
2023-10-26 19:29:29.685 
Epoch 124/1000 
	 loss: 30.2981, MinusLogProbMetric: 30.2981, val_loss: 30.5093, val_MinusLogProbMetric: 30.5093

Epoch 124: val_loss did not improve from 30.09083
196/196 - 35s - loss: 30.2981 - MinusLogProbMetric: 30.2981 - val_loss: 30.5093 - val_MinusLogProbMetric: 30.5093 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 125/1000
2023-10-26 19:30:02.555 
Epoch 125/1000 
	 loss: 30.3116, MinusLogProbMetric: 30.3116, val_loss: 32.2965, val_MinusLogProbMetric: 32.2965

Epoch 125: val_loss did not improve from 30.09083
196/196 - 33s - loss: 30.3116 - MinusLogProbMetric: 30.3116 - val_loss: 32.2965 - val_MinusLogProbMetric: 32.2965 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 126/1000
2023-10-26 19:30:32.361 
Epoch 126/1000 
	 loss: 30.2627, MinusLogProbMetric: 30.2627, val_loss: 30.1264, val_MinusLogProbMetric: 30.1264

Epoch 126: val_loss did not improve from 30.09083
196/196 - 30s - loss: 30.2627 - MinusLogProbMetric: 30.2627 - val_loss: 30.1264 - val_MinusLogProbMetric: 30.1264 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 127/1000
2023-10-26 19:31:04.892 
Epoch 127/1000 
	 loss: 30.4454, MinusLogProbMetric: 30.4454, val_loss: 30.3127, val_MinusLogProbMetric: 30.3127

Epoch 127: val_loss did not improve from 30.09083
196/196 - 33s - loss: 30.4454 - MinusLogProbMetric: 30.4454 - val_loss: 30.3127 - val_MinusLogProbMetric: 30.3127 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 128/1000
2023-10-26 19:31:39.707 
Epoch 128/1000 
	 loss: 30.2752, MinusLogProbMetric: 30.2752, val_loss: 29.8985, val_MinusLogProbMetric: 29.8985

Epoch 128: val_loss improved from 30.09083 to 29.89852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 30.2752 - MinusLogProbMetric: 30.2752 - val_loss: 29.8985 - val_MinusLogProbMetric: 29.8985 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 129/1000
2023-10-26 19:32:14.563 
Epoch 129/1000 
	 loss: 30.1503, MinusLogProbMetric: 30.1503, val_loss: 30.2595, val_MinusLogProbMetric: 30.2595

Epoch 129: val_loss did not improve from 29.89852
196/196 - 34s - loss: 30.1503 - MinusLogProbMetric: 30.1503 - val_loss: 30.2595 - val_MinusLogProbMetric: 30.2595 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 130/1000
2023-10-26 19:32:49.314 
Epoch 130/1000 
	 loss: 30.1281, MinusLogProbMetric: 30.1281, val_loss: 30.1453, val_MinusLogProbMetric: 30.1453

Epoch 130: val_loss did not improve from 29.89852
196/196 - 35s - loss: 30.1281 - MinusLogProbMetric: 30.1281 - val_loss: 30.1453 - val_MinusLogProbMetric: 30.1453 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 131/1000
2023-10-26 19:33:24.387 
Epoch 131/1000 
	 loss: 30.1075, MinusLogProbMetric: 30.1075, val_loss: 29.7994, val_MinusLogProbMetric: 29.7994

Epoch 131: val_loss improved from 29.89852 to 29.79940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 30.1075 - MinusLogProbMetric: 30.1075 - val_loss: 29.7994 - val_MinusLogProbMetric: 29.7994 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 132/1000
2023-10-26 19:33:59.812 
Epoch 132/1000 
	 loss: 30.2570, MinusLogProbMetric: 30.2570, val_loss: 30.7225, val_MinusLogProbMetric: 30.7225

Epoch 132: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.2570 - MinusLogProbMetric: 30.2570 - val_loss: 30.7225 - val_MinusLogProbMetric: 30.7225 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 133/1000
2023-10-26 19:34:34.807 
Epoch 133/1000 
	 loss: 30.2606, MinusLogProbMetric: 30.2606, val_loss: 30.1513, val_MinusLogProbMetric: 30.1513

Epoch 133: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.2606 - MinusLogProbMetric: 30.2606 - val_loss: 30.1513 - val_MinusLogProbMetric: 30.1513 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 134/1000
2023-10-26 19:35:09.368 
Epoch 134/1000 
	 loss: 30.0873, MinusLogProbMetric: 30.0873, val_loss: 31.5680, val_MinusLogProbMetric: 31.5680

Epoch 134: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.0873 - MinusLogProbMetric: 30.0873 - val_loss: 31.5680 - val_MinusLogProbMetric: 31.5680 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 135/1000
2023-10-26 19:35:44.313 
Epoch 135/1000 
	 loss: 30.1587, MinusLogProbMetric: 30.1587, val_loss: 30.7303, val_MinusLogProbMetric: 30.7303

Epoch 135: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.1587 - MinusLogProbMetric: 30.1587 - val_loss: 30.7303 - val_MinusLogProbMetric: 30.7303 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 136/1000
2023-10-26 19:36:18.958 
Epoch 136/1000 
	 loss: 30.2077, MinusLogProbMetric: 30.2077, val_loss: 30.3791, val_MinusLogProbMetric: 30.3791

Epoch 136: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.2077 - MinusLogProbMetric: 30.2077 - val_loss: 30.3791 - val_MinusLogProbMetric: 30.3791 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 137/1000
2023-10-26 19:36:54.012 
Epoch 137/1000 
	 loss: 30.0350, MinusLogProbMetric: 30.0350, val_loss: 30.1845, val_MinusLogProbMetric: 30.1845

Epoch 137: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.0350 - MinusLogProbMetric: 30.0350 - val_loss: 30.1845 - val_MinusLogProbMetric: 30.1845 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 138/1000
2023-10-26 19:37:29.076 
Epoch 138/1000 
	 loss: 30.0517, MinusLogProbMetric: 30.0517, val_loss: 29.8787, val_MinusLogProbMetric: 29.8787

Epoch 138: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.0517 - MinusLogProbMetric: 30.0517 - val_loss: 29.8787 - val_MinusLogProbMetric: 29.8787 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 139/1000
2023-10-26 19:38:03.710 
Epoch 139/1000 
	 loss: 30.0076, MinusLogProbMetric: 30.0076, val_loss: 29.9635, val_MinusLogProbMetric: 29.9635

Epoch 139: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.0076 - MinusLogProbMetric: 30.0076 - val_loss: 29.9635 - val_MinusLogProbMetric: 29.9635 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 140/1000
2023-10-26 19:38:38.617 
Epoch 140/1000 
	 loss: 30.1543, MinusLogProbMetric: 30.1543, val_loss: 30.1410, val_MinusLogProbMetric: 30.1410

Epoch 140: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.1543 - MinusLogProbMetric: 30.1543 - val_loss: 30.1410 - val_MinusLogProbMetric: 30.1410 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 141/1000
2023-10-26 19:39:13.179 
Epoch 141/1000 
	 loss: 30.0167, MinusLogProbMetric: 30.0167, val_loss: 29.9802, val_MinusLogProbMetric: 29.9802

Epoch 141: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.0167 - MinusLogProbMetric: 30.0167 - val_loss: 29.9802 - val_MinusLogProbMetric: 29.9802 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 142/1000
2023-10-26 19:39:48.034 
Epoch 142/1000 
	 loss: 29.9536, MinusLogProbMetric: 29.9536, val_loss: 29.9544, val_MinusLogProbMetric: 29.9544

Epoch 142: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.9536 - MinusLogProbMetric: 29.9536 - val_loss: 29.9544 - val_MinusLogProbMetric: 29.9544 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 143/1000
2023-10-26 19:40:23.262 
Epoch 143/1000 
	 loss: 30.0227, MinusLogProbMetric: 30.0227, val_loss: 29.9961, val_MinusLogProbMetric: 29.9961

Epoch 143: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.0227 - MinusLogProbMetric: 30.0227 - val_loss: 29.9961 - val_MinusLogProbMetric: 29.9961 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 144/1000
2023-10-26 19:40:58.031 
Epoch 144/1000 
	 loss: 29.9372, MinusLogProbMetric: 29.9372, val_loss: 30.1208, val_MinusLogProbMetric: 30.1208

Epoch 144: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.9372 - MinusLogProbMetric: 29.9372 - val_loss: 30.1208 - val_MinusLogProbMetric: 30.1208 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 145/1000
2023-10-26 19:41:32.858 
Epoch 145/1000 
	 loss: 29.9248, MinusLogProbMetric: 29.9248, val_loss: 29.8942, val_MinusLogProbMetric: 29.8942

Epoch 145: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.9248 - MinusLogProbMetric: 29.9248 - val_loss: 29.8942 - val_MinusLogProbMetric: 29.8942 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 146/1000
2023-10-26 19:42:08.347 
Epoch 146/1000 
	 loss: 29.9206, MinusLogProbMetric: 29.9206, val_loss: 30.5912, val_MinusLogProbMetric: 30.5912

Epoch 146: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.9206 - MinusLogProbMetric: 29.9206 - val_loss: 30.5912 - val_MinusLogProbMetric: 30.5912 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 147/1000
2023-10-26 19:42:43.179 
Epoch 147/1000 
	 loss: 29.8598, MinusLogProbMetric: 29.8598, val_loss: 31.0875, val_MinusLogProbMetric: 31.0875

Epoch 147: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.8598 - MinusLogProbMetric: 29.8598 - val_loss: 31.0875 - val_MinusLogProbMetric: 31.0875 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 148/1000
2023-10-26 19:43:18.065 
Epoch 148/1000 
	 loss: 30.0178, MinusLogProbMetric: 30.0178, val_loss: 30.3993, val_MinusLogProbMetric: 30.3993

Epoch 148: val_loss did not improve from 29.79940
196/196 - 35s - loss: 30.0178 - MinusLogProbMetric: 30.0178 - val_loss: 30.3993 - val_MinusLogProbMetric: 30.3993 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 149/1000
2023-10-26 19:43:52.750 
Epoch 149/1000 
	 loss: 29.9420, MinusLogProbMetric: 29.9420, val_loss: 30.6149, val_MinusLogProbMetric: 30.6149

Epoch 149: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.9420 - MinusLogProbMetric: 29.9420 - val_loss: 30.6149 - val_MinusLogProbMetric: 30.6149 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 150/1000
2023-10-26 19:44:27.726 
Epoch 150/1000 
	 loss: 29.8736, MinusLogProbMetric: 29.8736, val_loss: 29.9168, val_MinusLogProbMetric: 29.9168

Epoch 150: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.8736 - MinusLogProbMetric: 29.8736 - val_loss: 29.9168 - val_MinusLogProbMetric: 29.9168 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 151/1000
2023-10-26 19:45:02.557 
Epoch 151/1000 
	 loss: 29.9574, MinusLogProbMetric: 29.9574, val_loss: 30.3440, val_MinusLogProbMetric: 30.3440

Epoch 151: val_loss did not improve from 29.79940
196/196 - 35s - loss: 29.9574 - MinusLogProbMetric: 29.9574 - val_loss: 30.3440 - val_MinusLogProbMetric: 30.3440 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 152/1000
2023-10-26 19:45:37.751 
Epoch 152/1000 
	 loss: 29.9574, MinusLogProbMetric: 29.9574, val_loss: 29.5875, val_MinusLogProbMetric: 29.5875

Epoch 152: val_loss improved from 29.79940 to 29.58749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 29.9574 - MinusLogProbMetric: 29.9574 - val_loss: 29.5875 - val_MinusLogProbMetric: 29.5875 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 153/1000
2023-10-26 19:46:12.837 
Epoch 153/1000 
	 loss: 29.8854, MinusLogProbMetric: 29.8854, val_loss: 29.5804, val_MinusLogProbMetric: 29.5804

Epoch 153: val_loss improved from 29.58749 to 29.58036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 29.8854 - MinusLogProbMetric: 29.8854 - val_loss: 29.5804 - val_MinusLogProbMetric: 29.5804 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 154/1000
2023-10-26 19:46:48.003 
Epoch 154/1000 
	 loss: 29.8958, MinusLogProbMetric: 29.8958, val_loss: 29.9791, val_MinusLogProbMetric: 29.9791

Epoch 154: val_loss did not improve from 29.58036
196/196 - 35s - loss: 29.8958 - MinusLogProbMetric: 29.8958 - val_loss: 29.9791 - val_MinusLogProbMetric: 29.9791 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 155/1000
2023-10-26 19:47:22.919 
Epoch 155/1000 
	 loss: 29.7998, MinusLogProbMetric: 29.7998, val_loss: 29.6485, val_MinusLogProbMetric: 29.6485

Epoch 155: val_loss did not improve from 29.58036
196/196 - 35s - loss: 29.7998 - MinusLogProbMetric: 29.7998 - val_loss: 29.6485 - val_MinusLogProbMetric: 29.6485 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 156/1000
2023-10-26 19:47:58.027 
Epoch 156/1000 
	 loss: 29.6916, MinusLogProbMetric: 29.6916, val_loss: 29.5365, val_MinusLogProbMetric: 29.5365

Epoch 156: val_loss improved from 29.58036 to 29.53651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 29.6916 - MinusLogProbMetric: 29.6916 - val_loss: 29.5365 - val_MinusLogProbMetric: 29.5365 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 157/1000
2023-10-26 19:48:33.165 
Epoch 157/1000 
	 loss: 29.8461, MinusLogProbMetric: 29.8461, val_loss: 29.7281, val_MinusLogProbMetric: 29.7281

Epoch 157: val_loss did not improve from 29.53651
196/196 - 35s - loss: 29.8461 - MinusLogProbMetric: 29.8461 - val_loss: 29.7281 - val_MinusLogProbMetric: 29.7281 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 158/1000
2023-10-26 19:49:08.371 
Epoch 158/1000 
	 loss: 29.7372, MinusLogProbMetric: 29.7372, val_loss: 30.2665, val_MinusLogProbMetric: 30.2665

Epoch 158: val_loss did not improve from 29.53651
196/196 - 35s - loss: 29.7372 - MinusLogProbMetric: 29.7372 - val_loss: 30.2665 - val_MinusLogProbMetric: 30.2665 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 159/1000
2023-10-26 19:49:43.243 
Epoch 159/1000 
	 loss: 29.7578, MinusLogProbMetric: 29.7578, val_loss: 29.8269, val_MinusLogProbMetric: 29.8269

Epoch 159: val_loss did not improve from 29.53651
196/196 - 35s - loss: 29.7578 - MinusLogProbMetric: 29.7578 - val_loss: 29.8269 - val_MinusLogProbMetric: 29.8269 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 160/1000
2023-10-26 19:50:18.364 
Epoch 160/1000 
	 loss: 29.7836, MinusLogProbMetric: 29.7836, val_loss: 29.5605, val_MinusLogProbMetric: 29.5605

Epoch 160: val_loss did not improve from 29.53651
196/196 - 35s - loss: 29.7836 - MinusLogProbMetric: 29.7836 - val_loss: 29.5605 - val_MinusLogProbMetric: 29.5605 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 161/1000
2023-10-26 19:50:53.039 
Epoch 161/1000 
	 loss: 29.7333, MinusLogProbMetric: 29.7333, val_loss: 30.4585, val_MinusLogProbMetric: 30.4585

Epoch 161: val_loss did not improve from 29.53651
196/196 - 35s - loss: 29.7333 - MinusLogProbMetric: 29.7333 - val_loss: 30.4585 - val_MinusLogProbMetric: 30.4585 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 162/1000
2023-10-26 19:51:28.036 
Epoch 162/1000 
	 loss: 29.7117, MinusLogProbMetric: 29.7117, val_loss: 30.1434, val_MinusLogProbMetric: 30.1434

Epoch 162: val_loss did not improve from 29.53651
196/196 - 35s - loss: 29.7117 - MinusLogProbMetric: 29.7117 - val_loss: 30.1434 - val_MinusLogProbMetric: 30.1434 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 163/1000
2023-10-26 19:52:03.053 
Epoch 163/1000 
	 loss: 29.9334, MinusLogProbMetric: 29.9334, val_loss: 29.6372, val_MinusLogProbMetric: 29.6372

Epoch 163: val_loss did not improve from 29.53651
196/196 - 35s - loss: 29.9334 - MinusLogProbMetric: 29.9334 - val_loss: 29.6372 - val_MinusLogProbMetric: 29.6372 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 164/1000
2023-10-26 19:52:37.928 
Epoch 164/1000 
	 loss: 29.5922, MinusLogProbMetric: 29.5922, val_loss: 29.4857, val_MinusLogProbMetric: 29.4857

Epoch 164: val_loss improved from 29.53651 to 29.48575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 29.5922 - MinusLogProbMetric: 29.5922 - val_loss: 29.4857 - val_MinusLogProbMetric: 29.4857 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 165/1000
2023-10-26 19:53:13.377 
Epoch 165/1000 
	 loss: 29.6835, MinusLogProbMetric: 29.6835, val_loss: 29.7353, val_MinusLogProbMetric: 29.7353

Epoch 165: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.6835 - MinusLogProbMetric: 29.6835 - val_loss: 29.7353 - val_MinusLogProbMetric: 29.7353 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 166/1000
2023-10-26 19:53:48.372 
Epoch 166/1000 
	 loss: 29.8336, MinusLogProbMetric: 29.8336, val_loss: 29.7450, val_MinusLogProbMetric: 29.7450

Epoch 166: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.8336 - MinusLogProbMetric: 29.8336 - val_loss: 29.7450 - val_MinusLogProbMetric: 29.7450 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 167/1000
2023-10-26 19:54:23.701 
Epoch 167/1000 
	 loss: 29.6746, MinusLogProbMetric: 29.6746, val_loss: 30.0500, val_MinusLogProbMetric: 30.0500

Epoch 167: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.6746 - MinusLogProbMetric: 29.6746 - val_loss: 30.0500 - val_MinusLogProbMetric: 30.0500 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 168/1000
2023-10-26 19:54:58.885 
Epoch 168/1000 
	 loss: 29.8049, MinusLogProbMetric: 29.8049, val_loss: 29.5291, val_MinusLogProbMetric: 29.5291

Epoch 168: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.8049 - MinusLogProbMetric: 29.8049 - val_loss: 29.5291 - val_MinusLogProbMetric: 29.5291 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 169/1000
2023-10-26 19:55:33.920 
Epoch 169/1000 
	 loss: 29.7045, MinusLogProbMetric: 29.7045, val_loss: 29.4904, val_MinusLogProbMetric: 29.4904

Epoch 169: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.7045 - MinusLogProbMetric: 29.7045 - val_loss: 29.4904 - val_MinusLogProbMetric: 29.4904 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 170/1000
2023-10-26 19:56:08.985 
Epoch 170/1000 
	 loss: 29.7093, MinusLogProbMetric: 29.7093, val_loss: 29.9929, val_MinusLogProbMetric: 29.9929

Epoch 170: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.7093 - MinusLogProbMetric: 29.7093 - val_loss: 29.9929 - val_MinusLogProbMetric: 29.9929 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 171/1000
2023-10-26 19:56:44.125 
Epoch 171/1000 
	 loss: 29.5547, MinusLogProbMetric: 29.5547, val_loss: 29.5675, val_MinusLogProbMetric: 29.5675

Epoch 171: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.5547 - MinusLogProbMetric: 29.5547 - val_loss: 29.5675 - val_MinusLogProbMetric: 29.5675 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 172/1000
2023-10-26 19:57:19.214 
Epoch 172/1000 
	 loss: 29.5768, MinusLogProbMetric: 29.5768, val_loss: 30.1872, val_MinusLogProbMetric: 30.1872

Epoch 172: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.5768 - MinusLogProbMetric: 29.5768 - val_loss: 30.1872 - val_MinusLogProbMetric: 30.1872 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 173/1000
2023-10-26 19:57:54.203 
Epoch 173/1000 
	 loss: 29.6270, MinusLogProbMetric: 29.6270, val_loss: 29.7636, val_MinusLogProbMetric: 29.7636

Epoch 173: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.6270 - MinusLogProbMetric: 29.6270 - val_loss: 29.7636 - val_MinusLogProbMetric: 29.7636 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 174/1000
2023-10-26 19:58:29.353 
Epoch 174/1000 
	 loss: 29.6300, MinusLogProbMetric: 29.6300, val_loss: 29.7050, val_MinusLogProbMetric: 29.7050

Epoch 174: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.6300 - MinusLogProbMetric: 29.6300 - val_loss: 29.7050 - val_MinusLogProbMetric: 29.7050 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 175/1000
2023-10-26 19:59:04.587 
Epoch 175/1000 
	 loss: 29.4996, MinusLogProbMetric: 29.4996, val_loss: 29.5782, val_MinusLogProbMetric: 29.5782

Epoch 175: val_loss did not improve from 29.48575
196/196 - 35s - loss: 29.4996 - MinusLogProbMetric: 29.4996 - val_loss: 29.5782 - val_MinusLogProbMetric: 29.5782 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 176/1000
2023-10-26 19:59:39.258 
Epoch 176/1000 
	 loss: 29.5198, MinusLogProbMetric: 29.5198, val_loss: 29.4645, val_MinusLogProbMetric: 29.4645

Epoch 176: val_loss improved from 29.48575 to 29.46454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 29.5198 - MinusLogProbMetric: 29.5198 - val_loss: 29.4645 - val_MinusLogProbMetric: 29.4645 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 177/1000
2023-10-26 20:00:14.701 
Epoch 177/1000 
	 loss: 29.5738, MinusLogProbMetric: 29.5738, val_loss: 30.5710, val_MinusLogProbMetric: 30.5710

Epoch 177: val_loss did not improve from 29.46454
196/196 - 35s - loss: 29.5738 - MinusLogProbMetric: 29.5738 - val_loss: 30.5710 - val_MinusLogProbMetric: 30.5710 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 178/1000
2023-10-26 20:00:49.575 
Epoch 178/1000 
	 loss: 29.4290, MinusLogProbMetric: 29.4290, val_loss: 29.4435, val_MinusLogProbMetric: 29.4435

Epoch 178: val_loss improved from 29.46454 to 29.44353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 29.4290 - MinusLogProbMetric: 29.4290 - val_loss: 29.4435 - val_MinusLogProbMetric: 29.4435 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 179/1000
2023-10-26 20:01:25.090 
Epoch 179/1000 
	 loss: 29.5652, MinusLogProbMetric: 29.5652, val_loss: 29.7692, val_MinusLogProbMetric: 29.7692

Epoch 179: val_loss did not improve from 29.44353
196/196 - 35s - loss: 29.5652 - MinusLogProbMetric: 29.5652 - val_loss: 29.7692 - val_MinusLogProbMetric: 29.7692 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 180/1000
2023-10-26 20:01:59.815 
Epoch 180/1000 
	 loss: 29.7284, MinusLogProbMetric: 29.7284, val_loss: 29.5316, val_MinusLogProbMetric: 29.5316

Epoch 180: val_loss did not improve from 29.44353
196/196 - 35s - loss: 29.7284 - MinusLogProbMetric: 29.7284 - val_loss: 29.5316 - val_MinusLogProbMetric: 29.5316 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 181/1000
2023-10-26 20:02:34.889 
Epoch 181/1000 
	 loss: 29.4824, MinusLogProbMetric: 29.4824, val_loss: 29.3994, val_MinusLogProbMetric: 29.3994

Epoch 181: val_loss improved from 29.44353 to 29.39935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 29.4824 - MinusLogProbMetric: 29.4824 - val_loss: 29.3994 - val_MinusLogProbMetric: 29.3994 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 182/1000
2023-10-26 20:03:10.259 
Epoch 182/1000 
	 loss: 29.5196, MinusLogProbMetric: 29.5196, val_loss: 29.9248, val_MinusLogProbMetric: 29.9248

Epoch 182: val_loss did not improve from 29.39935
196/196 - 35s - loss: 29.5196 - MinusLogProbMetric: 29.5196 - val_loss: 29.9248 - val_MinusLogProbMetric: 29.9248 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 183/1000
2023-10-26 20:03:45.462 
Epoch 183/1000 
	 loss: 29.4511, MinusLogProbMetric: 29.4511, val_loss: 29.2492, val_MinusLogProbMetric: 29.2492

Epoch 183: val_loss improved from 29.39935 to 29.24922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 29.4511 - MinusLogProbMetric: 29.4511 - val_loss: 29.2492 - val_MinusLogProbMetric: 29.2492 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 184/1000
2023-10-26 20:04:21.182 
Epoch 184/1000 
	 loss: 29.5662, MinusLogProbMetric: 29.5662, val_loss: 29.4742, val_MinusLogProbMetric: 29.4742

Epoch 184: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.5662 - MinusLogProbMetric: 29.5662 - val_loss: 29.4742 - val_MinusLogProbMetric: 29.4742 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 185/1000
2023-10-26 20:04:56.102 
Epoch 185/1000 
	 loss: 29.5231, MinusLogProbMetric: 29.5231, val_loss: 29.7989, val_MinusLogProbMetric: 29.7989

Epoch 185: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.5231 - MinusLogProbMetric: 29.5231 - val_loss: 29.7989 - val_MinusLogProbMetric: 29.7989 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 186/1000
2023-10-26 20:05:31.263 
Epoch 186/1000 
	 loss: 29.3984, MinusLogProbMetric: 29.3984, val_loss: 29.3785, val_MinusLogProbMetric: 29.3785

Epoch 186: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.3984 - MinusLogProbMetric: 29.3984 - val_loss: 29.3785 - val_MinusLogProbMetric: 29.3785 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 187/1000
2023-10-26 20:06:06.398 
Epoch 187/1000 
	 loss: 29.4351, MinusLogProbMetric: 29.4351, val_loss: 29.2618, val_MinusLogProbMetric: 29.2618

Epoch 187: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.4351 - MinusLogProbMetric: 29.4351 - val_loss: 29.2618 - val_MinusLogProbMetric: 29.2618 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 188/1000
2023-10-26 20:06:41.776 
Epoch 188/1000 
	 loss: 29.4832, MinusLogProbMetric: 29.4832, val_loss: 29.5853, val_MinusLogProbMetric: 29.5853

Epoch 188: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.4832 - MinusLogProbMetric: 29.4832 - val_loss: 29.5853 - val_MinusLogProbMetric: 29.5853 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 189/1000
2023-10-26 20:07:16.839 
Epoch 189/1000 
	 loss: 29.4925, MinusLogProbMetric: 29.4925, val_loss: 29.3767, val_MinusLogProbMetric: 29.3767

Epoch 189: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.4925 - MinusLogProbMetric: 29.4925 - val_loss: 29.3767 - val_MinusLogProbMetric: 29.3767 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 190/1000
2023-10-26 20:07:51.909 
Epoch 190/1000 
	 loss: 29.4353, MinusLogProbMetric: 29.4353, val_loss: 30.8836, val_MinusLogProbMetric: 30.8836

Epoch 190: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.4353 - MinusLogProbMetric: 29.4353 - val_loss: 30.8836 - val_MinusLogProbMetric: 30.8836 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 191/1000
2023-10-26 20:08:27.088 
Epoch 191/1000 
	 loss: 29.6052, MinusLogProbMetric: 29.6052, val_loss: 30.6132, val_MinusLogProbMetric: 30.6132

Epoch 191: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.6052 - MinusLogProbMetric: 29.6052 - val_loss: 30.6132 - val_MinusLogProbMetric: 30.6132 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 192/1000
2023-10-26 20:09:01.847 
Epoch 192/1000 
	 loss: 29.3268, MinusLogProbMetric: 29.3268, val_loss: 30.7657, val_MinusLogProbMetric: 30.7657

Epoch 192: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.3268 - MinusLogProbMetric: 29.3268 - val_loss: 30.7657 - val_MinusLogProbMetric: 30.7657 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 193/1000
2023-10-26 20:09:36.739 
Epoch 193/1000 
	 loss: 29.4597, MinusLogProbMetric: 29.4597, val_loss: 29.5373, val_MinusLogProbMetric: 29.5373

Epoch 193: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.4597 - MinusLogProbMetric: 29.4597 - val_loss: 29.5373 - val_MinusLogProbMetric: 29.5373 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 194/1000
2023-10-26 20:10:11.850 
Epoch 194/1000 
	 loss: 29.3957, MinusLogProbMetric: 29.3957, val_loss: 29.3222, val_MinusLogProbMetric: 29.3222

Epoch 194: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.3957 - MinusLogProbMetric: 29.3957 - val_loss: 29.3222 - val_MinusLogProbMetric: 29.3222 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 195/1000
2023-10-26 20:10:46.939 
Epoch 195/1000 
	 loss: 29.3426, MinusLogProbMetric: 29.3426, val_loss: 29.9460, val_MinusLogProbMetric: 29.9460

Epoch 195: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.3426 - MinusLogProbMetric: 29.3426 - val_loss: 29.9460 - val_MinusLogProbMetric: 29.9460 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 196/1000
2023-10-26 20:11:21.510 
Epoch 196/1000 
	 loss: 29.2394, MinusLogProbMetric: 29.2394, val_loss: 29.6976, val_MinusLogProbMetric: 29.6976

Epoch 196: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.2394 - MinusLogProbMetric: 29.2394 - val_loss: 29.6976 - val_MinusLogProbMetric: 29.6976 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 197/1000
2023-10-26 20:11:56.471 
Epoch 197/1000 
	 loss: 29.3114, MinusLogProbMetric: 29.3114, val_loss: 29.2802, val_MinusLogProbMetric: 29.2802

Epoch 197: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.3114 - MinusLogProbMetric: 29.3114 - val_loss: 29.2802 - val_MinusLogProbMetric: 29.2802 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 198/1000
2023-10-26 20:12:31.388 
Epoch 198/1000 
	 loss: 29.6014, MinusLogProbMetric: 29.6014, val_loss: 30.5086, val_MinusLogProbMetric: 30.5086

Epoch 198: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.6014 - MinusLogProbMetric: 29.6014 - val_loss: 30.5086 - val_MinusLogProbMetric: 30.5086 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 199/1000
2023-10-26 20:13:06.233 
Epoch 199/1000 
	 loss: 29.5579, MinusLogProbMetric: 29.5579, val_loss: 30.1469, val_MinusLogProbMetric: 30.1469

Epoch 199: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.5579 - MinusLogProbMetric: 29.5579 - val_loss: 30.1469 - val_MinusLogProbMetric: 30.1469 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 200/1000
2023-10-26 20:13:41.349 
Epoch 200/1000 
	 loss: 29.4028, MinusLogProbMetric: 29.4028, val_loss: 30.5397, val_MinusLogProbMetric: 30.5397

Epoch 200: val_loss did not improve from 29.24922
196/196 - 35s - loss: 29.4028 - MinusLogProbMetric: 29.4028 - val_loss: 30.5397 - val_MinusLogProbMetric: 30.5397 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 201/1000
2023-10-26 20:14:16.089 
Epoch 201/1000 
	 loss: 29.4244, MinusLogProbMetric: 29.4244, val_loss: 29.1907, val_MinusLogProbMetric: 29.1907

Epoch 201: val_loss improved from 29.24922 to 29.19069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 29.4244 - MinusLogProbMetric: 29.4244 - val_loss: 29.1907 - val_MinusLogProbMetric: 29.1907 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 202/1000
2023-10-26 20:14:51.895 
Epoch 202/1000 
	 loss: 29.2738, MinusLogProbMetric: 29.2738, val_loss: 29.3236, val_MinusLogProbMetric: 29.3236

Epoch 202: val_loss did not improve from 29.19069
196/196 - 35s - loss: 29.2738 - MinusLogProbMetric: 29.2738 - val_loss: 29.3236 - val_MinusLogProbMetric: 29.3236 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 203/1000
2023-10-26 20:15:26.720 
Epoch 203/1000 
	 loss: 29.3434, MinusLogProbMetric: 29.3434, val_loss: 29.8794, val_MinusLogProbMetric: 29.8794

Epoch 203: val_loss did not improve from 29.19069
196/196 - 35s - loss: 29.3434 - MinusLogProbMetric: 29.3434 - val_loss: 29.8794 - val_MinusLogProbMetric: 29.8794 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 204/1000
2023-10-26 20:16:01.807 
Epoch 204/1000 
	 loss: 29.3699, MinusLogProbMetric: 29.3699, val_loss: 29.2088, val_MinusLogProbMetric: 29.2088

Epoch 204: val_loss did not improve from 29.19069
196/196 - 35s - loss: 29.3699 - MinusLogProbMetric: 29.3699 - val_loss: 29.2088 - val_MinusLogProbMetric: 29.2088 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 205/1000
2023-10-26 20:16:37.045 
Epoch 205/1000 
	 loss: 29.3822, MinusLogProbMetric: 29.3822, val_loss: 29.2693, val_MinusLogProbMetric: 29.2693

Epoch 205: val_loss did not improve from 29.19069
196/196 - 35s - loss: 29.3822 - MinusLogProbMetric: 29.3822 - val_loss: 29.2693 - val_MinusLogProbMetric: 29.2693 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 206/1000
2023-10-26 20:17:12.066 
Epoch 206/1000 
	 loss: 29.2084, MinusLogProbMetric: 29.2084, val_loss: 29.2764, val_MinusLogProbMetric: 29.2764

Epoch 206: val_loss did not improve from 29.19069
196/196 - 35s - loss: 29.2084 - MinusLogProbMetric: 29.2084 - val_loss: 29.2764 - val_MinusLogProbMetric: 29.2764 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 207/1000
2023-10-26 20:17:47.199 
Epoch 207/1000 
	 loss: 29.2867, MinusLogProbMetric: 29.2867, val_loss: 29.6611, val_MinusLogProbMetric: 29.6611

Epoch 207: val_loss did not improve from 29.19069
196/196 - 35s - loss: 29.2867 - MinusLogProbMetric: 29.2867 - val_loss: 29.6611 - val_MinusLogProbMetric: 29.6611 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 208/1000
2023-10-26 20:18:22.328 
Epoch 208/1000 
	 loss: 29.3318, MinusLogProbMetric: 29.3318, val_loss: 29.3754, val_MinusLogProbMetric: 29.3754

Epoch 208: val_loss did not improve from 29.19069
196/196 - 35s - loss: 29.3318 - MinusLogProbMetric: 29.3318 - val_loss: 29.3754 - val_MinusLogProbMetric: 29.3754 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 209/1000
2023-10-26 20:18:57.153 
Epoch 209/1000 
	 loss: 29.2377, MinusLogProbMetric: 29.2377, val_loss: 29.1523, val_MinusLogProbMetric: 29.1523

Epoch 209: val_loss improved from 29.19069 to 29.15232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 29.2377 - MinusLogProbMetric: 29.2377 - val_loss: 29.1523 - val_MinusLogProbMetric: 29.1523 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 210/1000
2023-10-26 20:19:32.761 
Epoch 210/1000 
	 loss: 29.2594, MinusLogProbMetric: 29.2594, val_loss: 29.0817, val_MinusLogProbMetric: 29.0817

Epoch 210: val_loss improved from 29.15232 to 29.08173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 29.2594 - MinusLogProbMetric: 29.2594 - val_loss: 29.0817 - val_MinusLogProbMetric: 29.0817 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 211/1000
2023-10-26 20:20:08.116 
Epoch 211/1000 
	 loss: 29.1256, MinusLogProbMetric: 29.1256, val_loss: 30.1152, val_MinusLogProbMetric: 30.1152

Epoch 211: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1256 - MinusLogProbMetric: 29.1256 - val_loss: 30.1152 - val_MinusLogProbMetric: 30.1152 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 212/1000
2023-10-26 20:20:43.226 
Epoch 212/1000 
	 loss: 29.2990, MinusLogProbMetric: 29.2990, val_loss: 29.6040, val_MinusLogProbMetric: 29.6040

Epoch 212: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.2990 - MinusLogProbMetric: 29.2990 - val_loss: 29.6040 - val_MinusLogProbMetric: 29.6040 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 213/1000
2023-10-26 20:21:18.184 
Epoch 213/1000 
	 loss: 29.2501, MinusLogProbMetric: 29.2501, val_loss: 29.6939, val_MinusLogProbMetric: 29.6939

Epoch 213: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.2501 - MinusLogProbMetric: 29.2501 - val_loss: 29.6939 - val_MinusLogProbMetric: 29.6939 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 214/1000
2023-10-26 20:21:53.204 
Epoch 214/1000 
	 loss: 29.4354, MinusLogProbMetric: 29.4354, val_loss: 29.9693, val_MinusLogProbMetric: 29.9693

Epoch 214: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.4354 - MinusLogProbMetric: 29.4354 - val_loss: 29.9693 - val_MinusLogProbMetric: 29.9693 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 215/1000
2023-10-26 20:22:28.459 
Epoch 215/1000 
	 loss: 29.2277, MinusLogProbMetric: 29.2277, val_loss: 29.4369, val_MinusLogProbMetric: 29.4369

Epoch 215: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.2277 - MinusLogProbMetric: 29.2277 - val_loss: 29.4369 - val_MinusLogProbMetric: 29.4369 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 216/1000
2023-10-26 20:23:03.540 
Epoch 216/1000 
	 loss: 29.1805, MinusLogProbMetric: 29.1805, val_loss: 29.1520, val_MinusLogProbMetric: 29.1520

Epoch 216: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1805 - MinusLogProbMetric: 29.1805 - val_loss: 29.1520 - val_MinusLogProbMetric: 29.1520 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 217/1000
2023-10-26 20:23:38.444 
Epoch 217/1000 
	 loss: 29.1484, MinusLogProbMetric: 29.1484, val_loss: 29.4752, val_MinusLogProbMetric: 29.4752

Epoch 217: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1484 - MinusLogProbMetric: 29.1484 - val_loss: 29.4752 - val_MinusLogProbMetric: 29.4752 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 218/1000
2023-10-26 20:24:13.478 
Epoch 218/1000 
	 loss: 29.2472, MinusLogProbMetric: 29.2472, val_loss: 30.3525, val_MinusLogProbMetric: 30.3525

Epoch 218: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.2472 - MinusLogProbMetric: 29.2472 - val_loss: 30.3525 - val_MinusLogProbMetric: 30.3525 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 219/1000
2023-10-26 20:24:48.505 
Epoch 219/1000 
	 loss: 29.2841, MinusLogProbMetric: 29.2841, val_loss: 29.5791, val_MinusLogProbMetric: 29.5791

Epoch 219: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.2841 - MinusLogProbMetric: 29.2841 - val_loss: 29.5791 - val_MinusLogProbMetric: 29.5791 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 220/1000
2023-10-26 20:25:23.551 
Epoch 220/1000 
	 loss: 29.1476, MinusLogProbMetric: 29.1476, val_loss: 29.5127, val_MinusLogProbMetric: 29.5127

Epoch 220: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1476 - MinusLogProbMetric: 29.1476 - val_loss: 29.5127 - val_MinusLogProbMetric: 29.5127 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 221/1000
2023-10-26 20:25:58.645 
Epoch 221/1000 
	 loss: 29.1905, MinusLogProbMetric: 29.1905, val_loss: 29.2075, val_MinusLogProbMetric: 29.2075

Epoch 221: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1905 - MinusLogProbMetric: 29.1905 - val_loss: 29.2075 - val_MinusLogProbMetric: 29.2075 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 222/1000
2023-10-26 20:26:33.870 
Epoch 222/1000 
	 loss: 29.1968, MinusLogProbMetric: 29.1968, val_loss: 30.4834, val_MinusLogProbMetric: 30.4834

Epoch 222: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1968 - MinusLogProbMetric: 29.1968 - val_loss: 30.4834 - val_MinusLogProbMetric: 30.4834 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 223/1000
2023-10-26 20:27:08.829 
Epoch 223/1000 
	 loss: 29.1673, MinusLogProbMetric: 29.1673, val_loss: 29.5168, val_MinusLogProbMetric: 29.5168

Epoch 223: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1673 - MinusLogProbMetric: 29.1673 - val_loss: 29.5168 - val_MinusLogProbMetric: 29.5168 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 224/1000
2023-10-26 20:27:43.702 
Epoch 224/1000 
	 loss: 29.1835, MinusLogProbMetric: 29.1835, val_loss: 29.5739, val_MinusLogProbMetric: 29.5739

Epoch 224: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1835 - MinusLogProbMetric: 29.1835 - val_loss: 29.5739 - val_MinusLogProbMetric: 29.5739 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 225/1000
2023-10-26 20:28:18.439 
Epoch 225/1000 
	 loss: 29.1293, MinusLogProbMetric: 29.1293, val_loss: 29.7134, val_MinusLogProbMetric: 29.7134

Epoch 225: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1293 - MinusLogProbMetric: 29.1293 - val_loss: 29.7134 - val_MinusLogProbMetric: 29.7134 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 226/1000
2023-10-26 20:28:53.181 
Epoch 226/1000 
	 loss: 29.1273, MinusLogProbMetric: 29.1273, val_loss: 29.5200, val_MinusLogProbMetric: 29.5200

Epoch 226: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1273 - MinusLogProbMetric: 29.1273 - val_loss: 29.5200 - val_MinusLogProbMetric: 29.5200 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 227/1000
2023-10-26 20:29:28.148 
Epoch 227/1000 
	 loss: 29.1782, MinusLogProbMetric: 29.1782, val_loss: 29.5715, val_MinusLogProbMetric: 29.5715

Epoch 227: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.1782 - MinusLogProbMetric: 29.1782 - val_loss: 29.5715 - val_MinusLogProbMetric: 29.5715 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 228/1000
2023-10-26 20:30:03.004 
Epoch 228/1000 
	 loss: 29.2826, MinusLogProbMetric: 29.2826, val_loss: 29.2355, val_MinusLogProbMetric: 29.2355

Epoch 228: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.2826 - MinusLogProbMetric: 29.2826 - val_loss: 29.2355 - val_MinusLogProbMetric: 29.2355 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 229/1000
2023-10-26 20:30:38.011 
Epoch 229/1000 
	 loss: 29.0855, MinusLogProbMetric: 29.0855, val_loss: 29.1626, val_MinusLogProbMetric: 29.1626

Epoch 229: val_loss did not improve from 29.08173
196/196 - 35s - loss: 29.0855 - MinusLogProbMetric: 29.0855 - val_loss: 29.1626 - val_MinusLogProbMetric: 29.1626 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 230/1000
2023-10-26 20:31:12.812 
Epoch 230/1000 
	 loss: 29.0655, MinusLogProbMetric: 29.0655, val_loss: 28.8710, val_MinusLogProbMetric: 28.8710

Epoch 230: val_loss improved from 29.08173 to 28.87105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 29.0655 - MinusLogProbMetric: 29.0655 - val_loss: 28.8710 - val_MinusLogProbMetric: 28.8710 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 231/1000
2023-10-26 20:31:48.321 
Epoch 231/1000 
	 loss: 29.1455, MinusLogProbMetric: 29.1455, val_loss: 29.2960, val_MinusLogProbMetric: 29.2960

Epoch 231: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.1455 - MinusLogProbMetric: 29.1455 - val_loss: 29.2960 - val_MinusLogProbMetric: 29.2960 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 232/1000
2023-10-26 20:32:23.215 
Epoch 232/1000 
	 loss: 29.0950, MinusLogProbMetric: 29.0950, val_loss: 29.4593, val_MinusLogProbMetric: 29.4593

Epoch 232: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.0950 - MinusLogProbMetric: 29.0950 - val_loss: 29.4593 - val_MinusLogProbMetric: 29.4593 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 233/1000
2023-10-26 20:32:58.308 
Epoch 233/1000 
	 loss: 29.1789, MinusLogProbMetric: 29.1789, val_loss: 29.5253, val_MinusLogProbMetric: 29.5253

Epoch 233: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.1789 - MinusLogProbMetric: 29.1789 - val_loss: 29.5253 - val_MinusLogProbMetric: 29.5253 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 234/1000
2023-10-26 20:33:33.204 
Epoch 234/1000 
	 loss: 29.0117, MinusLogProbMetric: 29.0117, val_loss: 28.9990, val_MinusLogProbMetric: 28.9990

Epoch 234: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.0117 - MinusLogProbMetric: 29.0117 - val_loss: 28.9990 - val_MinusLogProbMetric: 28.9990 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 235/1000
2023-10-26 20:34:08.243 
Epoch 235/1000 
	 loss: 29.0067, MinusLogProbMetric: 29.0067, val_loss: 29.0767, val_MinusLogProbMetric: 29.0767

Epoch 235: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.0067 - MinusLogProbMetric: 29.0067 - val_loss: 29.0767 - val_MinusLogProbMetric: 29.0767 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 236/1000
2023-10-26 20:34:43.544 
Epoch 236/1000 
	 loss: 29.0981, MinusLogProbMetric: 29.0981, val_loss: 29.4011, val_MinusLogProbMetric: 29.4011

Epoch 236: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.0981 - MinusLogProbMetric: 29.0981 - val_loss: 29.4011 - val_MinusLogProbMetric: 29.4011 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 237/1000
2023-10-26 20:35:18.689 
Epoch 237/1000 
	 loss: 29.2841, MinusLogProbMetric: 29.2841, val_loss: 29.8344, val_MinusLogProbMetric: 29.8344

Epoch 237: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.2841 - MinusLogProbMetric: 29.2841 - val_loss: 29.8344 - val_MinusLogProbMetric: 29.8344 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 238/1000
2023-10-26 20:35:53.785 
Epoch 238/1000 
	 loss: 29.0171, MinusLogProbMetric: 29.0171, val_loss: 29.1969, val_MinusLogProbMetric: 29.1969

Epoch 238: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.0171 - MinusLogProbMetric: 29.0171 - val_loss: 29.1969 - val_MinusLogProbMetric: 29.1969 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 239/1000
2023-10-26 20:36:29.054 
Epoch 239/1000 
	 loss: 29.0815, MinusLogProbMetric: 29.0815, val_loss: 29.2904, val_MinusLogProbMetric: 29.2904

Epoch 239: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.0815 - MinusLogProbMetric: 29.0815 - val_loss: 29.2904 - val_MinusLogProbMetric: 29.2904 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 240/1000
2023-10-26 20:37:04.332 
Epoch 240/1000 
	 loss: 29.1254, MinusLogProbMetric: 29.1254, val_loss: 29.4509, val_MinusLogProbMetric: 29.4509

Epoch 240: val_loss did not improve from 28.87105
196/196 - 35s - loss: 29.1254 - MinusLogProbMetric: 29.1254 - val_loss: 29.4509 - val_MinusLogProbMetric: 29.4509 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 241/1000
2023-10-26 20:37:39.573 
Epoch 241/1000 
	 loss: 29.0989, MinusLogProbMetric: 29.0989, val_loss: 28.7339, val_MinusLogProbMetric: 28.7339

Epoch 241: val_loss improved from 28.87105 to 28.73390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 29.0989 - MinusLogProbMetric: 29.0989 - val_loss: 28.7339 - val_MinusLogProbMetric: 28.7339 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 242/1000
2023-10-26 20:38:15.352 
Epoch 242/1000 
	 loss: 28.9953, MinusLogProbMetric: 28.9953, val_loss: 28.8553, val_MinusLogProbMetric: 28.8553

Epoch 242: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9953 - MinusLogProbMetric: 28.9953 - val_loss: 28.8553 - val_MinusLogProbMetric: 28.8553 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 243/1000
2023-10-26 20:38:50.499 
Epoch 243/1000 
	 loss: 29.0368, MinusLogProbMetric: 29.0368, val_loss: 29.0605, val_MinusLogProbMetric: 29.0605

Epoch 243: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0368 - MinusLogProbMetric: 29.0368 - val_loss: 29.0605 - val_MinusLogProbMetric: 29.0605 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 244/1000
2023-10-26 20:39:25.781 
Epoch 244/1000 
	 loss: 29.0767, MinusLogProbMetric: 29.0767, val_loss: 29.0488, val_MinusLogProbMetric: 29.0488

Epoch 244: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0767 - MinusLogProbMetric: 29.0767 - val_loss: 29.0488 - val_MinusLogProbMetric: 29.0488 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 245/1000
2023-10-26 20:40:01.025 
Epoch 245/1000 
	 loss: 29.0033, MinusLogProbMetric: 29.0033, val_loss: 28.9890, val_MinusLogProbMetric: 28.9890

Epoch 245: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0033 - MinusLogProbMetric: 29.0033 - val_loss: 28.9890 - val_MinusLogProbMetric: 28.9890 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 246/1000
2023-10-26 20:40:36.254 
Epoch 246/1000 
	 loss: 29.0022, MinusLogProbMetric: 29.0022, val_loss: 29.0866, val_MinusLogProbMetric: 29.0866

Epoch 246: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0022 - MinusLogProbMetric: 29.0022 - val_loss: 29.0866 - val_MinusLogProbMetric: 29.0866 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 247/1000
2023-10-26 20:41:11.448 
Epoch 247/1000 
	 loss: 29.0441, MinusLogProbMetric: 29.0441, val_loss: 29.0132, val_MinusLogProbMetric: 29.0132

Epoch 247: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0441 - MinusLogProbMetric: 29.0441 - val_loss: 29.0132 - val_MinusLogProbMetric: 29.0132 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 248/1000
2023-10-26 20:41:46.681 
Epoch 248/1000 
	 loss: 29.0269, MinusLogProbMetric: 29.0269, val_loss: 29.2905, val_MinusLogProbMetric: 29.2905

Epoch 248: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0269 - MinusLogProbMetric: 29.0269 - val_loss: 29.2905 - val_MinusLogProbMetric: 29.2905 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 249/1000
2023-10-26 20:42:21.612 
Epoch 249/1000 
	 loss: 29.0058, MinusLogProbMetric: 29.0058, val_loss: 29.4627, val_MinusLogProbMetric: 29.4627

Epoch 249: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0058 - MinusLogProbMetric: 29.0058 - val_loss: 29.4627 - val_MinusLogProbMetric: 29.4627 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 250/1000
2023-10-26 20:42:56.834 
Epoch 250/1000 
	 loss: 29.0728, MinusLogProbMetric: 29.0728, val_loss: 29.2169, val_MinusLogProbMetric: 29.2169

Epoch 250: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0728 - MinusLogProbMetric: 29.0728 - val_loss: 29.2169 - val_MinusLogProbMetric: 29.2169 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 251/1000
2023-10-26 20:43:31.837 
Epoch 251/1000 
	 loss: 28.9644, MinusLogProbMetric: 28.9644, val_loss: 28.9975, val_MinusLogProbMetric: 28.9975

Epoch 251: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9644 - MinusLogProbMetric: 28.9644 - val_loss: 28.9975 - val_MinusLogProbMetric: 28.9975 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 252/1000
2023-10-26 20:44:06.505 
Epoch 252/1000 
	 loss: 29.0162, MinusLogProbMetric: 29.0162, val_loss: 30.0566, val_MinusLogProbMetric: 30.0566

Epoch 252: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0162 - MinusLogProbMetric: 29.0162 - val_loss: 30.0566 - val_MinusLogProbMetric: 30.0566 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 253/1000
2023-10-26 20:44:41.718 
Epoch 253/1000 
	 loss: 28.9763, MinusLogProbMetric: 28.9763, val_loss: 29.1042, val_MinusLogProbMetric: 29.1042

Epoch 253: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9763 - MinusLogProbMetric: 28.9763 - val_loss: 29.1042 - val_MinusLogProbMetric: 29.1042 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 254/1000
2023-10-26 20:45:16.848 
Epoch 254/1000 
	 loss: 29.0572, MinusLogProbMetric: 29.0572, val_loss: 28.8729, val_MinusLogProbMetric: 28.8729

Epoch 254: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0572 - MinusLogProbMetric: 29.0572 - val_loss: 28.8729 - val_MinusLogProbMetric: 28.8729 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 255/1000
2023-10-26 20:45:51.879 
Epoch 255/1000 
	 loss: 28.9985, MinusLogProbMetric: 28.9985, val_loss: 30.2507, val_MinusLogProbMetric: 30.2507

Epoch 255: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9985 - MinusLogProbMetric: 28.9985 - val_loss: 30.2507 - val_MinusLogProbMetric: 30.2507 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 256/1000
2023-10-26 20:46:26.761 
Epoch 256/1000 
	 loss: 29.0351, MinusLogProbMetric: 29.0351, val_loss: 28.9713, val_MinusLogProbMetric: 28.9713

Epoch 256: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0351 - MinusLogProbMetric: 29.0351 - val_loss: 28.9713 - val_MinusLogProbMetric: 28.9713 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 257/1000
2023-10-26 20:47:01.829 
Epoch 257/1000 
	 loss: 28.9306, MinusLogProbMetric: 28.9306, val_loss: 29.7034, val_MinusLogProbMetric: 29.7034

Epoch 257: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9306 - MinusLogProbMetric: 28.9306 - val_loss: 29.7034 - val_MinusLogProbMetric: 29.7034 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 258/1000
2023-10-26 20:47:36.910 
Epoch 258/1000 
	 loss: 28.9695, MinusLogProbMetric: 28.9695, val_loss: 30.1410, val_MinusLogProbMetric: 30.1410

Epoch 258: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9695 - MinusLogProbMetric: 28.9695 - val_loss: 30.1410 - val_MinusLogProbMetric: 30.1410 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 259/1000
2023-10-26 20:48:11.892 
Epoch 259/1000 
	 loss: 28.9579, MinusLogProbMetric: 28.9579, val_loss: 29.1616, val_MinusLogProbMetric: 29.1616

Epoch 259: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9579 - MinusLogProbMetric: 28.9579 - val_loss: 29.1616 - val_MinusLogProbMetric: 29.1616 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 260/1000
2023-10-26 20:48:47.145 
Epoch 260/1000 
	 loss: 29.0959, MinusLogProbMetric: 29.0959, val_loss: 29.1398, val_MinusLogProbMetric: 29.1398

Epoch 260: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0959 - MinusLogProbMetric: 29.0959 - val_loss: 29.1398 - val_MinusLogProbMetric: 29.1398 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 261/1000
2023-10-26 20:49:22.392 
Epoch 261/1000 
	 loss: 28.9553, MinusLogProbMetric: 28.9553, val_loss: 28.9742, val_MinusLogProbMetric: 28.9742

Epoch 261: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9553 - MinusLogProbMetric: 28.9553 - val_loss: 28.9742 - val_MinusLogProbMetric: 28.9742 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 262/1000
2023-10-26 20:49:57.430 
Epoch 262/1000 
	 loss: 28.9467, MinusLogProbMetric: 28.9467, val_loss: 28.8245, val_MinusLogProbMetric: 28.8245

Epoch 262: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9467 - MinusLogProbMetric: 28.9467 - val_loss: 28.8245 - val_MinusLogProbMetric: 28.8245 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 263/1000
2023-10-26 20:50:32.577 
Epoch 263/1000 
	 loss: 29.1466, MinusLogProbMetric: 29.1466, val_loss: 29.4545, val_MinusLogProbMetric: 29.4545

Epoch 263: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.1466 - MinusLogProbMetric: 29.1466 - val_loss: 29.4545 - val_MinusLogProbMetric: 29.4545 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 264/1000
2023-10-26 20:51:07.841 
Epoch 264/1000 
	 loss: 28.9070, MinusLogProbMetric: 28.9070, val_loss: 28.8992, val_MinusLogProbMetric: 28.8992

Epoch 264: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9070 - MinusLogProbMetric: 28.9070 - val_loss: 28.8992 - val_MinusLogProbMetric: 28.8992 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 265/1000
2023-10-26 20:51:42.781 
Epoch 265/1000 
	 loss: 29.0351, MinusLogProbMetric: 29.0351, val_loss: 29.8028, val_MinusLogProbMetric: 29.8028

Epoch 265: val_loss did not improve from 28.73390
196/196 - 35s - loss: 29.0351 - MinusLogProbMetric: 29.0351 - val_loss: 29.8028 - val_MinusLogProbMetric: 29.8028 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 266/1000
2023-10-26 20:52:17.875 
Epoch 266/1000 
	 loss: 28.9310, MinusLogProbMetric: 28.9310, val_loss: 29.5104, val_MinusLogProbMetric: 29.5104

Epoch 266: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9310 - MinusLogProbMetric: 28.9310 - val_loss: 29.5104 - val_MinusLogProbMetric: 29.5104 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 267/1000
2023-10-26 20:52:52.758 
Epoch 267/1000 
	 loss: 28.8779, MinusLogProbMetric: 28.8779, val_loss: 28.8026, val_MinusLogProbMetric: 28.8026

Epoch 267: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.8779 - MinusLogProbMetric: 28.8779 - val_loss: 28.8026 - val_MinusLogProbMetric: 28.8026 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 268/1000
2023-10-26 20:53:27.699 
Epoch 268/1000 
	 loss: 28.9573, MinusLogProbMetric: 28.9573, val_loss: 29.0297, val_MinusLogProbMetric: 29.0297

Epoch 268: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9573 - MinusLogProbMetric: 28.9573 - val_loss: 29.0297 - val_MinusLogProbMetric: 29.0297 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 269/1000
2023-10-26 20:54:02.807 
Epoch 269/1000 
	 loss: 28.9496, MinusLogProbMetric: 28.9496, val_loss: 29.6083, val_MinusLogProbMetric: 29.6083

Epoch 269: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9496 - MinusLogProbMetric: 28.9496 - val_loss: 29.6083 - val_MinusLogProbMetric: 29.6083 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 270/1000
2023-10-26 20:54:37.520 
Epoch 270/1000 
	 loss: 28.8589, MinusLogProbMetric: 28.8589, val_loss: 29.8239, val_MinusLogProbMetric: 29.8239

Epoch 270: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.8589 - MinusLogProbMetric: 28.8589 - val_loss: 29.8239 - val_MinusLogProbMetric: 29.8239 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 271/1000
2023-10-26 20:55:12.419 
Epoch 271/1000 
	 loss: 28.9298, MinusLogProbMetric: 28.9298, val_loss: 29.1752, val_MinusLogProbMetric: 29.1752

Epoch 271: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.9298 - MinusLogProbMetric: 28.9298 - val_loss: 29.1752 - val_MinusLogProbMetric: 29.1752 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 272/1000
2023-10-26 20:55:47.518 
Epoch 272/1000 
	 loss: 28.8937, MinusLogProbMetric: 28.8937, val_loss: 29.2097, val_MinusLogProbMetric: 29.2097

Epoch 272: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.8937 - MinusLogProbMetric: 28.8937 - val_loss: 29.2097 - val_MinusLogProbMetric: 29.2097 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 273/1000
2023-10-26 20:56:22.842 
Epoch 273/1000 
	 loss: 28.8874, MinusLogProbMetric: 28.8874, val_loss: 28.7427, val_MinusLogProbMetric: 28.7427

Epoch 273: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.8874 - MinusLogProbMetric: 28.8874 - val_loss: 28.7427 - val_MinusLogProbMetric: 28.7427 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 274/1000
2023-10-26 20:56:58.004 
Epoch 274/1000 
	 loss: 28.7889, MinusLogProbMetric: 28.7889, val_loss: 30.1802, val_MinusLogProbMetric: 30.1802

Epoch 274: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.7889 - MinusLogProbMetric: 28.7889 - val_loss: 30.1802 - val_MinusLogProbMetric: 30.1802 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 275/1000
2023-10-26 20:57:33.075 
Epoch 275/1000 
	 loss: 28.8355, MinusLogProbMetric: 28.8355, val_loss: 28.8847, val_MinusLogProbMetric: 28.8847

Epoch 275: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.8355 - MinusLogProbMetric: 28.8355 - val_loss: 28.8847 - val_MinusLogProbMetric: 28.8847 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 276/1000
2023-10-26 20:58:08.170 
Epoch 276/1000 
	 loss: 28.8638, MinusLogProbMetric: 28.8638, val_loss: 28.9140, val_MinusLogProbMetric: 28.9140

Epoch 276: val_loss did not improve from 28.73390
196/196 - 35s - loss: 28.8638 - MinusLogProbMetric: 28.8638 - val_loss: 28.9140 - val_MinusLogProbMetric: 28.9140 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 277/1000
2023-10-26 20:58:42.893 
Epoch 277/1000 
	 loss: 28.9548, MinusLogProbMetric: 28.9548, val_loss: 28.6748, val_MinusLogProbMetric: 28.6748

Epoch 277: val_loss improved from 28.73390 to 28.67480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 28.9548 - MinusLogProbMetric: 28.9548 - val_loss: 28.6748 - val_MinusLogProbMetric: 28.6748 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 278/1000
2023-10-26 20:59:18.465 
Epoch 278/1000 
	 loss: 28.9098, MinusLogProbMetric: 28.9098, val_loss: 29.3133, val_MinusLogProbMetric: 29.3133

Epoch 278: val_loss did not improve from 28.67480
196/196 - 35s - loss: 28.9098 - MinusLogProbMetric: 28.9098 - val_loss: 29.3133 - val_MinusLogProbMetric: 29.3133 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 279/1000
2023-10-26 20:59:53.541 
Epoch 279/1000 
	 loss: 28.9867, MinusLogProbMetric: 28.9867, val_loss: 28.8618, val_MinusLogProbMetric: 28.8618

Epoch 279: val_loss did not improve from 28.67480
196/196 - 35s - loss: 28.9867 - MinusLogProbMetric: 28.9867 - val_loss: 28.8618 - val_MinusLogProbMetric: 28.8618 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 280/1000
2023-10-26 21:00:28.691 
Epoch 280/1000 
	 loss: 28.8681, MinusLogProbMetric: 28.8681, val_loss: 28.8012, val_MinusLogProbMetric: 28.8012

Epoch 280: val_loss did not improve from 28.67480
196/196 - 35s - loss: 28.8681 - MinusLogProbMetric: 28.8681 - val_loss: 28.8012 - val_MinusLogProbMetric: 28.8012 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 281/1000
2023-10-26 21:01:03.917 
Epoch 281/1000 
	 loss: 28.8713, MinusLogProbMetric: 28.8713, val_loss: 28.6595, val_MinusLogProbMetric: 28.6595

Epoch 281: val_loss improved from 28.67480 to 28.65946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 28.8713 - MinusLogProbMetric: 28.8713 - val_loss: 28.6595 - val_MinusLogProbMetric: 28.6595 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 282/1000
2023-10-26 21:01:39.557 
Epoch 282/1000 
	 loss: 29.1026, MinusLogProbMetric: 29.1026, val_loss: 29.3303, val_MinusLogProbMetric: 29.3303

Epoch 282: val_loss did not improve from 28.65946
196/196 - 35s - loss: 29.1026 - MinusLogProbMetric: 29.1026 - val_loss: 29.3303 - val_MinusLogProbMetric: 29.3303 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 283/1000
2023-10-26 21:02:14.244 
Epoch 283/1000 
	 loss: 28.8386, MinusLogProbMetric: 28.8386, val_loss: 29.3999, val_MinusLogProbMetric: 29.3999

Epoch 283: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8386 - MinusLogProbMetric: 28.8386 - val_loss: 29.3999 - val_MinusLogProbMetric: 29.3999 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 284/1000
2023-10-26 21:02:49.024 
Epoch 284/1000 
	 loss: 28.8130, MinusLogProbMetric: 28.8130, val_loss: 28.6756, val_MinusLogProbMetric: 28.6756

Epoch 284: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8130 - MinusLogProbMetric: 28.8130 - val_loss: 28.6756 - val_MinusLogProbMetric: 28.6756 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 285/1000
2023-10-26 21:03:24.230 
Epoch 285/1000 
	 loss: 28.8641, MinusLogProbMetric: 28.8641, val_loss: 29.3803, val_MinusLogProbMetric: 29.3803

Epoch 285: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8641 - MinusLogProbMetric: 28.8641 - val_loss: 29.3803 - val_MinusLogProbMetric: 29.3803 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 286/1000
2023-10-26 21:03:59.391 
Epoch 286/1000 
	 loss: 28.8440, MinusLogProbMetric: 28.8440, val_loss: 29.4211, val_MinusLogProbMetric: 29.4211

Epoch 286: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8440 - MinusLogProbMetric: 28.8440 - val_loss: 29.4211 - val_MinusLogProbMetric: 29.4211 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 287/1000
2023-10-26 21:04:34.594 
Epoch 287/1000 
	 loss: 29.0080, MinusLogProbMetric: 29.0080, val_loss: 29.8371, val_MinusLogProbMetric: 29.8371

Epoch 287: val_loss did not improve from 28.65946
196/196 - 35s - loss: 29.0080 - MinusLogProbMetric: 29.0080 - val_loss: 29.8371 - val_MinusLogProbMetric: 29.8371 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 288/1000
2023-10-26 21:05:09.720 
Epoch 288/1000 
	 loss: 28.7860, MinusLogProbMetric: 28.7860, val_loss: 28.8543, val_MinusLogProbMetric: 28.8543

Epoch 288: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.7860 - MinusLogProbMetric: 28.7860 - val_loss: 28.8543 - val_MinusLogProbMetric: 28.8543 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 289/1000
2023-10-26 21:05:44.813 
Epoch 289/1000 
	 loss: 28.8250, MinusLogProbMetric: 28.8250, val_loss: 29.3171, val_MinusLogProbMetric: 29.3171

Epoch 289: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8250 - MinusLogProbMetric: 28.8250 - val_loss: 29.3171 - val_MinusLogProbMetric: 29.3171 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 290/1000
2023-10-26 21:06:20.065 
Epoch 290/1000 
	 loss: 28.8316, MinusLogProbMetric: 28.8316, val_loss: 29.1422, val_MinusLogProbMetric: 29.1422

Epoch 290: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8316 - MinusLogProbMetric: 28.8316 - val_loss: 29.1422 - val_MinusLogProbMetric: 29.1422 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 291/1000
2023-10-26 21:06:55.312 
Epoch 291/1000 
	 loss: 28.8478, MinusLogProbMetric: 28.8478, val_loss: 28.8774, val_MinusLogProbMetric: 28.8774

Epoch 291: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8478 - MinusLogProbMetric: 28.8478 - val_loss: 28.8774 - val_MinusLogProbMetric: 28.8774 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 292/1000
2023-10-26 21:07:30.339 
Epoch 292/1000 
	 loss: 28.8782, MinusLogProbMetric: 28.8782, val_loss: 29.3885, val_MinusLogProbMetric: 29.3885

Epoch 292: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8782 - MinusLogProbMetric: 28.8782 - val_loss: 29.3885 - val_MinusLogProbMetric: 29.3885 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 293/1000
2023-10-26 21:08:05.475 
Epoch 293/1000 
	 loss: 28.8353, MinusLogProbMetric: 28.8353, val_loss: 29.8341, val_MinusLogProbMetric: 29.8341

Epoch 293: val_loss did not improve from 28.65946
196/196 - 35s - loss: 28.8353 - MinusLogProbMetric: 28.8353 - val_loss: 29.8341 - val_MinusLogProbMetric: 29.8341 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 294/1000
2023-10-26 21:08:40.458 
Epoch 294/1000 
	 loss: 28.7984, MinusLogProbMetric: 28.7984, val_loss: 28.6085, val_MinusLogProbMetric: 28.6085

Epoch 294: val_loss improved from 28.65946 to 28.60848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 28.7984 - MinusLogProbMetric: 28.7984 - val_loss: 28.6085 - val_MinusLogProbMetric: 28.6085 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 295/1000
2023-10-26 21:09:15.900 
Epoch 295/1000 
	 loss: 28.7384, MinusLogProbMetric: 28.7384, val_loss: 29.1221, val_MinusLogProbMetric: 29.1221

Epoch 295: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7384 - MinusLogProbMetric: 28.7384 - val_loss: 29.1221 - val_MinusLogProbMetric: 29.1221 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 296/1000
2023-10-26 21:09:50.525 
Epoch 296/1000 
	 loss: 28.8321, MinusLogProbMetric: 28.8321, val_loss: 29.0005, val_MinusLogProbMetric: 29.0005

Epoch 296: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8321 - MinusLogProbMetric: 28.8321 - val_loss: 29.0005 - val_MinusLogProbMetric: 29.0005 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 297/1000
2023-10-26 21:10:25.532 
Epoch 297/1000 
	 loss: 28.8217, MinusLogProbMetric: 28.8217, val_loss: 28.9910, val_MinusLogProbMetric: 28.9910

Epoch 297: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8217 - MinusLogProbMetric: 28.8217 - val_loss: 28.9910 - val_MinusLogProbMetric: 28.9910 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 298/1000
2023-10-26 21:11:00.472 
Epoch 298/1000 
	 loss: 28.8111, MinusLogProbMetric: 28.8111, val_loss: 29.8023, val_MinusLogProbMetric: 29.8023

Epoch 298: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8111 - MinusLogProbMetric: 28.8111 - val_loss: 29.8023 - val_MinusLogProbMetric: 29.8023 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 299/1000
2023-10-26 21:11:35.468 
Epoch 299/1000 
	 loss: 28.7839, MinusLogProbMetric: 28.7839, val_loss: 29.5367, val_MinusLogProbMetric: 29.5367

Epoch 299: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7839 - MinusLogProbMetric: 28.7839 - val_loss: 29.5367 - val_MinusLogProbMetric: 29.5367 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 300/1000
2023-10-26 21:12:10.254 
Epoch 300/1000 
	 loss: 28.9611, MinusLogProbMetric: 28.9611, val_loss: 29.1413, val_MinusLogProbMetric: 29.1413

Epoch 300: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.9611 - MinusLogProbMetric: 28.9611 - val_loss: 29.1413 - val_MinusLogProbMetric: 29.1413 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 301/1000
2023-10-26 21:12:45.402 
Epoch 301/1000 
	 loss: 28.8303, MinusLogProbMetric: 28.8303, val_loss: 28.7387, val_MinusLogProbMetric: 28.7387

Epoch 301: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8303 - MinusLogProbMetric: 28.8303 - val_loss: 28.7387 - val_MinusLogProbMetric: 28.7387 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 302/1000
2023-10-26 21:13:20.330 
Epoch 302/1000 
	 loss: 28.6998, MinusLogProbMetric: 28.6998, val_loss: 28.7761, val_MinusLogProbMetric: 28.7761

Epoch 302: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.6998 - MinusLogProbMetric: 28.6998 - val_loss: 28.7761 - val_MinusLogProbMetric: 28.7761 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 303/1000
2023-10-26 21:13:55.240 
Epoch 303/1000 
	 loss: 28.9406, MinusLogProbMetric: 28.9406, val_loss: 29.0169, val_MinusLogProbMetric: 29.0169

Epoch 303: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.9406 - MinusLogProbMetric: 28.9406 - val_loss: 29.0169 - val_MinusLogProbMetric: 29.0169 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 304/1000
2023-10-26 21:14:30.523 
Epoch 304/1000 
	 loss: 28.7264, MinusLogProbMetric: 28.7264, val_loss: 29.8485, val_MinusLogProbMetric: 29.8485

Epoch 304: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7264 - MinusLogProbMetric: 28.7264 - val_loss: 29.8485 - val_MinusLogProbMetric: 29.8485 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 305/1000
2023-10-26 21:15:05.790 
Epoch 305/1000 
	 loss: 28.8349, MinusLogProbMetric: 28.8349, val_loss: 29.1691, val_MinusLogProbMetric: 29.1691

Epoch 305: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8349 - MinusLogProbMetric: 28.8349 - val_loss: 29.1691 - val_MinusLogProbMetric: 29.1691 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 306/1000
2023-10-26 21:15:40.848 
Epoch 306/1000 
	 loss: 28.8707, MinusLogProbMetric: 28.8707, val_loss: 29.3740, val_MinusLogProbMetric: 29.3740

Epoch 306: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8707 - MinusLogProbMetric: 28.8707 - val_loss: 29.3740 - val_MinusLogProbMetric: 29.3740 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 307/1000
2023-10-26 21:16:16.157 
Epoch 307/1000 
	 loss: 28.8566, MinusLogProbMetric: 28.8566, val_loss: 29.0442, val_MinusLogProbMetric: 29.0442

Epoch 307: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8566 - MinusLogProbMetric: 28.8566 - val_loss: 29.0442 - val_MinusLogProbMetric: 29.0442 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 308/1000
2023-10-26 21:16:51.576 
Epoch 308/1000 
	 loss: 28.7952, MinusLogProbMetric: 28.7952, val_loss: 28.6277, val_MinusLogProbMetric: 28.6277

Epoch 308: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7952 - MinusLogProbMetric: 28.7952 - val_loss: 28.6277 - val_MinusLogProbMetric: 28.6277 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 309/1000
2023-10-26 21:17:26.730 
Epoch 309/1000 
	 loss: 28.7668, MinusLogProbMetric: 28.7668, val_loss: 30.4353, val_MinusLogProbMetric: 30.4353

Epoch 309: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7668 - MinusLogProbMetric: 28.7668 - val_loss: 30.4353 - val_MinusLogProbMetric: 30.4353 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 310/1000
2023-10-26 21:18:01.829 
Epoch 310/1000 
	 loss: 28.7315, MinusLogProbMetric: 28.7315, val_loss: 28.7091, val_MinusLogProbMetric: 28.7091

Epoch 310: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7315 - MinusLogProbMetric: 28.7315 - val_loss: 28.7091 - val_MinusLogProbMetric: 28.7091 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 311/1000
2023-10-26 21:18:36.763 
Epoch 311/1000 
	 loss: 28.7557, MinusLogProbMetric: 28.7557, val_loss: 28.9053, val_MinusLogProbMetric: 28.9053

Epoch 311: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7557 - MinusLogProbMetric: 28.7557 - val_loss: 28.9053 - val_MinusLogProbMetric: 28.9053 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 312/1000
2023-10-26 21:19:11.788 
Epoch 312/1000 
	 loss: 28.8302, MinusLogProbMetric: 28.8302, val_loss: 29.0271, val_MinusLogProbMetric: 29.0271

Epoch 312: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.8302 - MinusLogProbMetric: 28.8302 - val_loss: 29.0271 - val_MinusLogProbMetric: 29.0271 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 313/1000
2023-10-26 21:19:46.738 
Epoch 313/1000 
	 loss: 28.7695, MinusLogProbMetric: 28.7695, val_loss: 28.8272, val_MinusLogProbMetric: 28.8272

Epoch 313: val_loss did not improve from 28.60848
196/196 - 35s - loss: 28.7695 - MinusLogProbMetric: 28.7695 - val_loss: 28.8272 - val_MinusLogProbMetric: 28.8272 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 314/1000
2023-10-26 21:20:21.792 
Epoch 314/1000 
	 loss: 28.8339, MinusLogProbMetric: 28.8339, val_loss: 28.5305, val_MinusLogProbMetric: 28.5305

Epoch 314: val_loss improved from 28.60848 to 28.53052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 28.8339 - MinusLogProbMetric: 28.8339 - val_loss: 28.5305 - val_MinusLogProbMetric: 28.5305 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 315/1000
2023-10-26 21:20:57.531 
Epoch 315/1000 
	 loss: 28.6964, MinusLogProbMetric: 28.6964, val_loss: 29.0618, val_MinusLogProbMetric: 29.0618

Epoch 315: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.6964 - MinusLogProbMetric: 28.6964 - val_loss: 29.0618 - val_MinusLogProbMetric: 29.0618 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 316/1000
2023-10-26 21:21:32.961 
Epoch 316/1000 
	 loss: 28.9044, MinusLogProbMetric: 28.9044, val_loss: 28.7140, val_MinusLogProbMetric: 28.7140

Epoch 316: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.9044 - MinusLogProbMetric: 28.9044 - val_loss: 28.7140 - val_MinusLogProbMetric: 28.7140 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 317/1000
2023-10-26 21:22:08.297 
Epoch 317/1000 
	 loss: 28.6661, MinusLogProbMetric: 28.6661, val_loss: 28.5433, val_MinusLogProbMetric: 28.5433

Epoch 317: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.6661 - MinusLogProbMetric: 28.6661 - val_loss: 28.5433 - val_MinusLogProbMetric: 28.5433 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 318/1000
2023-10-26 21:22:43.420 
Epoch 318/1000 
	 loss: 28.8140, MinusLogProbMetric: 28.8140, val_loss: 29.3751, val_MinusLogProbMetric: 29.3751

Epoch 318: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.8140 - MinusLogProbMetric: 28.8140 - val_loss: 29.3751 - val_MinusLogProbMetric: 29.3751 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 319/1000
2023-10-26 21:23:18.438 
Epoch 319/1000 
	 loss: 28.7040, MinusLogProbMetric: 28.7040, val_loss: 28.7126, val_MinusLogProbMetric: 28.7126

Epoch 319: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.7040 - MinusLogProbMetric: 28.7040 - val_loss: 28.7126 - val_MinusLogProbMetric: 28.7126 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 320/1000
2023-10-26 21:23:53.563 
Epoch 320/1000 
	 loss: 28.6835, MinusLogProbMetric: 28.6835, val_loss: 29.2438, val_MinusLogProbMetric: 29.2438

Epoch 320: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.6835 - MinusLogProbMetric: 28.6835 - val_loss: 29.2438 - val_MinusLogProbMetric: 29.2438 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 321/1000
2023-10-26 21:24:28.934 
Epoch 321/1000 
	 loss: 28.8319, MinusLogProbMetric: 28.8319, val_loss: 29.2052, val_MinusLogProbMetric: 29.2052

Epoch 321: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.8319 - MinusLogProbMetric: 28.8319 - val_loss: 29.2052 - val_MinusLogProbMetric: 29.2052 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 322/1000
2023-10-26 21:25:04.038 
Epoch 322/1000 
	 loss: 28.6840, MinusLogProbMetric: 28.6840, val_loss: 28.7657, val_MinusLogProbMetric: 28.7657

Epoch 322: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.6840 - MinusLogProbMetric: 28.6840 - val_loss: 28.7657 - val_MinusLogProbMetric: 28.7657 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 323/1000
2023-10-26 21:25:39.061 
Epoch 323/1000 
	 loss: 28.6804, MinusLogProbMetric: 28.6804, val_loss: 29.9342, val_MinusLogProbMetric: 29.9342

Epoch 323: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.6804 - MinusLogProbMetric: 28.6804 - val_loss: 29.9342 - val_MinusLogProbMetric: 29.9342 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 324/1000
2023-10-26 21:26:14.248 
Epoch 324/1000 
	 loss: 28.7865, MinusLogProbMetric: 28.7865, val_loss: 28.8964, val_MinusLogProbMetric: 28.8964

Epoch 324: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.7865 - MinusLogProbMetric: 28.7865 - val_loss: 28.8964 - val_MinusLogProbMetric: 28.8964 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 325/1000
2023-10-26 21:26:49.399 
Epoch 325/1000 
	 loss: 28.6785, MinusLogProbMetric: 28.6785, val_loss: 29.3632, val_MinusLogProbMetric: 29.3632

Epoch 325: val_loss did not improve from 28.53052
196/196 - 35s - loss: 28.6785 - MinusLogProbMetric: 28.6785 - val_loss: 29.3632 - val_MinusLogProbMetric: 29.3632 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 326/1000
2023-10-26 21:27:24.087 
Epoch 326/1000 
	 loss: 28.6853, MinusLogProbMetric: 28.6853, val_loss: 28.5086, val_MinusLogProbMetric: 28.5086

Epoch 326: val_loss improved from 28.53052 to 28.50857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 28.6853 - MinusLogProbMetric: 28.6853 - val_loss: 28.5086 - val_MinusLogProbMetric: 28.5086 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 327/1000
2023-10-26 21:27:59.564 
Epoch 327/1000 
	 loss: 28.7888, MinusLogProbMetric: 28.7888, val_loss: 29.2512, val_MinusLogProbMetric: 29.2512

Epoch 327: val_loss did not improve from 28.50857
196/196 - 35s - loss: 28.7888 - MinusLogProbMetric: 28.7888 - val_loss: 29.2512 - val_MinusLogProbMetric: 29.2512 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 328/1000
2023-10-26 21:28:34.336 
Epoch 328/1000 
	 loss: 28.6503, MinusLogProbMetric: 28.6503, val_loss: 28.4793, val_MinusLogProbMetric: 28.4793

Epoch 328: val_loss improved from 28.50857 to 28.47930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 28.6503 - MinusLogProbMetric: 28.6503 - val_loss: 28.4793 - val_MinusLogProbMetric: 28.4793 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 329/1000
2023-10-26 21:29:10.058 
Epoch 329/1000 
	 loss: 28.7536, MinusLogProbMetric: 28.7536, val_loss: 28.8475, val_MinusLogProbMetric: 28.8475

Epoch 329: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.7536 - MinusLogProbMetric: 28.7536 - val_loss: 28.8475 - val_MinusLogProbMetric: 28.8475 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 330/1000
2023-10-26 21:29:45.015 
Epoch 330/1000 
	 loss: 28.6148, MinusLogProbMetric: 28.6148, val_loss: 28.7959, val_MinusLogProbMetric: 28.7959

Epoch 330: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6148 - MinusLogProbMetric: 28.6148 - val_loss: 28.7959 - val_MinusLogProbMetric: 28.7959 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 331/1000
2023-10-26 21:30:20.115 
Epoch 331/1000 
	 loss: 28.7626, MinusLogProbMetric: 28.7626, val_loss: 29.7639, val_MinusLogProbMetric: 29.7639

Epoch 331: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.7626 - MinusLogProbMetric: 28.7626 - val_loss: 29.7639 - val_MinusLogProbMetric: 29.7639 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 332/1000
2023-10-26 21:30:54.940 
Epoch 332/1000 
	 loss: 28.7122, MinusLogProbMetric: 28.7122, val_loss: 28.8156, val_MinusLogProbMetric: 28.8156

Epoch 332: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.7122 - MinusLogProbMetric: 28.7122 - val_loss: 28.8156 - val_MinusLogProbMetric: 28.8156 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 333/1000
2023-10-26 21:31:29.804 
Epoch 333/1000 
	 loss: 28.6710, MinusLogProbMetric: 28.6710, val_loss: 29.0966, val_MinusLogProbMetric: 29.0966

Epoch 333: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6710 - MinusLogProbMetric: 28.6710 - val_loss: 29.0966 - val_MinusLogProbMetric: 29.0966 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 334/1000
2023-10-26 21:32:04.942 
Epoch 334/1000 
	 loss: 28.7411, MinusLogProbMetric: 28.7411, val_loss: 28.9651, val_MinusLogProbMetric: 28.9651

Epoch 334: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.7411 - MinusLogProbMetric: 28.7411 - val_loss: 28.9651 - val_MinusLogProbMetric: 28.9651 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 335/1000
2023-10-26 21:32:40.015 
Epoch 335/1000 
	 loss: 28.7255, MinusLogProbMetric: 28.7255, val_loss: 29.8871, val_MinusLogProbMetric: 29.8871

Epoch 335: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.7255 - MinusLogProbMetric: 28.7255 - val_loss: 29.8871 - val_MinusLogProbMetric: 29.8871 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 336/1000
2023-10-26 21:33:15.229 
Epoch 336/1000 
	 loss: 28.7077, MinusLogProbMetric: 28.7077, val_loss: 29.4231, val_MinusLogProbMetric: 29.4231

Epoch 336: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.7077 - MinusLogProbMetric: 28.7077 - val_loss: 29.4231 - val_MinusLogProbMetric: 29.4231 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 337/1000
2023-10-26 21:33:50.359 
Epoch 337/1000 
	 loss: 28.7666, MinusLogProbMetric: 28.7666, val_loss: 28.8750, val_MinusLogProbMetric: 28.8750

Epoch 337: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.7666 - MinusLogProbMetric: 28.7666 - val_loss: 28.8750 - val_MinusLogProbMetric: 28.8750 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 338/1000
2023-10-26 21:34:25.374 
Epoch 338/1000 
	 loss: 28.6532, MinusLogProbMetric: 28.6532, val_loss: 29.0399, val_MinusLogProbMetric: 29.0399

Epoch 338: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6532 - MinusLogProbMetric: 28.6532 - val_loss: 29.0399 - val_MinusLogProbMetric: 29.0399 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 339/1000
2023-10-26 21:35:00.727 
Epoch 339/1000 
	 loss: 28.6760, MinusLogProbMetric: 28.6760, val_loss: 29.1726, val_MinusLogProbMetric: 29.1726

Epoch 339: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6760 - MinusLogProbMetric: 28.6760 - val_loss: 29.1726 - val_MinusLogProbMetric: 29.1726 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 340/1000
2023-10-26 21:35:35.641 
Epoch 340/1000 
	 loss: 28.6500, MinusLogProbMetric: 28.6500, val_loss: 28.8343, val_MinusLogProbMetric: 28.8343

Epoch 340: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6500 - MinusLogProbMetric: 28.6500 - val_loss: 28.8343 - val_MinusLogProbMetric: 28.8343 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 341/1000
2023-10-26 21:36:10.718 
Epoch 341/1000 
	 loss: 28.6303, MinusLogProbMetric: 28.6303, val_loss: 28.9238, val_MinusLogProbMetric: 28.9238

Epoch 341: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6303 - MinusLogProbMetric: 28.6303 - val_loss: 28.9238 - val_MinusLogProbMetric: 28.9238 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 342/1000
2023-10-26 21:36:45.675 
Epoch 342/1000 
	 loss: 28.6908, MinusLogProbMetric: 28.6908, val_loss: 29.0801, val_MinusLogProbMetric: 29.0801

Epoch 342: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6908 - MinusLogProbMetric: 28.6908 - val_loss: 29.0801 - val_MinusLogProbMetric: 29.0801 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 343/1000
2023-10-26 21:37:20.688 
Epoch 343/1000 
	 loss: 28.6715, MinusLogProbMetric: 28.6715, val_loss: 28.8690, val_MinusLogProbMetric: 28.8690

Epoch 343: val_loss did not improve from 28.47930
196/196 - 35s - loss: 28.6715 - MinusLogProbMetric: 28.6715 - val_loss: 28.8690 - val_MinusLogProbMetric: 28.8690 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 344/1000
2023-10-26 21:37:55.091 
Epoch 344/1000 
	 loss: 28.6427, MinusLogProbMetric: 28.6427, val_loss: 28.4603, val_MinusLogProbMetric: 28.4603

Epoch 344: val_loss improved from 28.47930 to 28.46034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 28.6427 - MinusLogProbMetric: 28.6427 - val_loss: 28.4603 - val_MinusLogProbMetric: 28.4603 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 345/1000
2023-10-26 21:38:30.310 
Epoch 345/1000 
	 loss: 28.6703, MinusLogProbMetric: 28.6703, val_loss: 28.5242, val_MinusLogProbMetric: 28.5242

Epoch 345: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6703 - MinusLogProbMetric: 28.6703 - val_loss: 28.5242 - val_MinusLogProbMetric: 28.5242 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 346/1000
2023-10-26 21:39:05.100 
Epoch 346/1000 
	 loss: 28.6804, MinusLogProbMetric: 28.6804, val_loss: 30.3622, val_MinusLogProbMetric: 30.3622

Epoch 346: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6804 - MinusLogProbMetric: 28.6804 - val_loss: 30.3622 - val_MinusLogProbMetric: 30.3622 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 347/1000
2023-10-26 21:39:39.762 
Epoch 347/1000 
	 loss: 28.7225, MinusLogProbMetric: 28.7225, val_loss: 29.2171, val_MinusLogProbMetric: 29.2171

Epoch 347: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.7225 - MinusLogProbMetric: 28.7225 - val_loss: 29.2171 - val_MinusLogProbMetric: 29.2171 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 348/1000
2023-10-26 21:40:14.628 
Epoch 348/1000 
	 loss: 28.7917, MinusLogProbMetric: 28.7917, val_loss: 29.0141, val_MinusLogProbMetric: 29.0141

Epoch 348: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.7917 - MinusLogProbMetric: 28.7917 - val_loss: 29.0141 - val_MinusLogProbMetric: 29.0141 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 349/1000
2023-10-26 21:40:49.730 
Epoch 349/1000 
	 loss: 28.5841, MinusLogProbMetric: 28.5841, val_loss: 29.4724, val_MinusLogProbMetric: 29.4724

Epoch 349: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5841 - MinusLogProbMetric: 28.5841 - val_loss: 29.4724 - val_MinusLogProbMetric: 29.4724 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 350/1000
2023-10-26 21:41:24.670 
Epoch 350/1000 
	 loss: 28.5837, MinusLogProbMetric: 28.5837, val_loss: 28.5338, val_MinusLogProbMetric: 28.5338

Epoch 350: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5837 - MinusLogProbMetric: 28.5837 - val_loss: 28.5338 - val_MinusLogProbMetric: 28.5338 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 351/1000
2023-10-26 21:41:59.605 
Epoch 351/1000 
	 loss: 28.6291, MinusLogProbMetric: 28.6291, val_loss: 28.5497, val_MinusLogProbMetric: 28.5497

Epoch 351: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6291 - MinusLogProbMetric: 28.6291 - val_loss: 28.5497 - val_MinusLogProbMetric: 28.5497 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 352/1000
2023-10-26 21:42:34.420 
Epoch 352/1000 
	 loss: 28.6124, MinusLogProbMetric: 28.6124, val_loss: 28.6085, val_MinusLogProbMetric: 28.6085

Epoch 352: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6124 - MinusLogProbMetric: 28.6124 - val_loss: 28.6085 - val_MinusLogProbMetric: 28.6085 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 353/1000
2023-10-26 21:43:09.690 
Epoch 353/1000 
	 loss: 28.6282, MinusLogProbMetric: 28.6282, val_loss: 29.9179, val_MinusLogProbMetric: 29.9179

Epoch 353: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6282 - MinusLogProbMetric: 28.6282 - val_loss: 29.9179 - val_MinusLogProbMetric: 29.9179 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 354/1000
2023-10-26 21:43:44.697 
Epoch 354/1000 
	 loss: 28.6855, MinusLogProbMetric: 28.6855, val_loss: 29.2406, val_MinusLogProbMetric: 29.2406

Epoch 354: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6855 - MinusLogProbMetric: 28.6855 - val_loss: 29.2406 - val_MinusLogProbMetric: 29.2406 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 355/1000
2023-10-26 21:44:19.651 
Epoch 355/1000 
	 loss: 28.6160, MinusLogProbMetric: 28.6160, val_loss: 29.2114, val_MinusLogProbMetric: 29.2114

Epoch 355: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6160 - MinusLogProbMetric: 28.6160 - val_loss: 29.2114 - val_MinusLogProbMetric: 29.2114 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 356/1000
2023-10-26 21:44:55.020 
Epoch 356/1000 
	 loss: 28.6824, MinusLogProbMetric: 28.6824, val_loss: 28.8004, val_MinusLogProbMetric: 28.8004

Epoch 356: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6824 - MinusLogProbMetric: 28.6824 - val_loss: 28.8004 - val_MinusLogProbMetric: 28.8004 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 357/1000
2023-10-26 21:45:30.662 
Epoch 357/1000 
	 loss: 28.5806, MinusLogProbMetric: 28.5806, val_loss: 28.4785, val_MinusLogProbMetric: 28.4785

Epoch 357: val_loss did not improve from 28.46034
196/196 - 36s - loss: 28.5806 - MinusLogProbMetric: 28.5806 - val_loss: 28.4785 - val_MinusLogProbMetric: 28.4785 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 358/1000
2023-10-26 21:46:05.945 
Epoch 358/1000 
	 loss: 28.6044, MinusLogProbMetric: 28.6044, val_loss: 29.2891, val_MinusLogProbMetric: 29.2891

Epoch 358: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6044 - MinusLogProbMetric: 28.6044 - val_loss: 29.2891 - val_MinusLogProbMetric: 29.2891 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 359/1000
2023-10-26 21:46:41.401 
Epoch 359/1000 
	 loss: 28.7767, MinusLogProbMetric: 28.7767, val_loss: 28.9765, val_MinusLogProbMetric: 28.9765

Epoch 359: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.7767 - MinusLogProbMetric: 28.7767 - val_loss: 28.9765 - val_MinusLogProbMetric: 28.9765 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 360/1000
2023-10-26 21:47:16.563 
Epoch 360/1000 
	 loss: 28.6140, MinusLogProbMetric: 28.6140, val_loss: 28.7403, val_MinusLogProbMetric: 28.7403

Epoch 360: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6140 - MinusLogProbMetric: 28.6140 - val_loss: 28.7403 - val_MinusLogProbMetric: 28.7403 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 361/1000
2023-10-26 21:47:51.766 
Epoch 361/1000 
	 loss: 28.6686, MinusLogProbMetric: 28.6686, val_loss: 29.4518, val_MinusLogProbMetric: 29.4518

Epoch 361: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6686 - MinusLogProbMetric: 28.6686 - val_loss: 29.4518 - val_MinusLogProbMetric: 29.4518 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 362/1000
2023-10-26 21:48:26.938 
Epoch 362/1000 
	 loss: 28.7032, MinusLogProbMetric: 28.7032, val_loss: 29.3146, val_MinusLogProbMetric: 29.3146

Epoch 362: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.7032 - MinusLogProbMetric: 28.7032 - val_loss: 29.3146 - val_MinusLogProbMetric: 29.3146 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 363/1000
2023-10-26 21:49:01.927 
Epoch 363/1000 
	 loss: 28.6160, MinusLogProbMetric: 28.6160, val_loss: 28.7631, val_MinusLogProbMetric: 28.7631

Epoch 363: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6160 - MinusLogProbMetric: 28.6160 - val_loss: 28.7631 - val_MinusLogProbMetric: 28.7631 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 364/1000
2023-10-26 21:49:36.911 
Epoch 364/1000 
	 loss: 28.5902, MinusLogProbMetric: 28.5902, val_loss: 28.6216, val_MinusLogProbMetric: 28.6216

Epoch 364: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5902 - MinusLogProbMetric: 28.5902 - val_loss: 28.6216 - val_MinusLogProbMetric: 28.6216 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 365/1000
2023-10-26 21:50:12.125 
Epoch 365/1000 
	 loss: 28.6916, MinusLogProbMetric: 28.6916, val_loss: 29.2214, val_MinusLogProbMetric: 29.2214

Epoch 365: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6916 - MinusLogProbMetric: 28.6916 - val_loss: 29.2214 - val_MinusLogProbMetric: 29.2214 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 366/1000
2023-10-26 21:50:46.880 
Epoch 366/1000 
	 loss: 28.6433, MinusLogProbMetric: 28.6433, val_loss: 29.9739, val_MinusLogProbMetric: 29.9739

Epoch 366: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6433 - MinusLogProbMetric: 28.6433 - val_loss: 29.9739 - val_MinusLogProbMetric: 29.9739 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 367/1000
2023-10-26 21:51:22.004 
Epoch 367/1000 
	 loss: 28.6438, MinusLogProbMetric: 28.6438, val_loss: 28.5579, val_MinusLogProbMetric: 28.5579

Epoch 367: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6438 - MinusLogProbMetric: 28.6438 - val_loss: 28.5579 - val_MinusLogProbMetric: 28.5579 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 368/1000
2023-10-26 21:51:57.164 
Epoch 368/1000 
	 loss: 28.5902, MinusLogProbMetric: 28.5902, val_loss: 28.5436, val_MinusLogProbMetric: 28.5436

Epoch 368: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5902 - MinusLogProbMetric: 28.5902 - val_loss: 28.5436 - val_MinusLogProbMetric: 28.5436 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 369/1000
2023-10-26 21:52:32.294 
Epoch 369/1000 
	 loss: 28.6496, MinusLogProbMetric: 28.6496, val_loss: 28.6392, val_MinusLogProbMetric: 28.6392

Epoch 369: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6496 - MinusLogProbMetric: 28.6496 - val_loss: 28.6392 - val_MinusLogProbMetric: 28.6392 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 370/1000
2023-10-26 21:53:07.339 
Epoch 370/1000 
	 loss: 28.6133, MinusLogProbMetric: 28.6133, val_loss: 29.7423, val_MinusLogProbMetric: 29.7423

Epoch 370: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6133 - MinusLogProbMetric: 28.6133 - val_loss: 29.7423 - val_MinusLogProbMetric: 29.7423 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 371/1000
2023-10-26 21:53:42.261 
Epoch 371/1000 
	 loss: 28.6421, MinusLogProbMetric: 28.6421, val_loss: 29.1231, val_MinusLogProbMetric: 29.1231

Epoch 371: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6421 - MinusLogProbMetric: 28.6421 - val_loss: 29.1231 - val_MinusLogProbMetric: 29.1231 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 372/1000
2023-10-26 21:54:17.628 
Epoch 372/1000 
	 loss: 28.6594, MinusLogProbMetric: 28.6594, val_loss: 28.6469, val_MinusLogProbMetric: 28.6469

Epoch 372: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6594 - MinusLogProbMetric: 28.6594 - val_loss: 28.6469 - val_MinusLogProbMetric: 28.6469 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 373/1000
2023-10-26 21:54:52.826 
Epoch 373/1000 
	 loss: 28.6146, MinusLogProbMetric: 28.6146, val_loss: 28.9925, val_MinusLogProbMetric: 28.9925

Epoch 373: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6146 - MinusLogProbMetric: 28.6146 - val_loss: 28.9925 - val_MinusLogProbMetric: 28.9925 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 374/1000
2023-10-26 21:55:28.054 
Epoch 374/1000 
	 loss: 28.4986, MinusLogProbMetric: 28.4986, val_loss: 28.6874, val_MinusLogProbMetric: 28.6874

Epoch 374: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.4986 - MinusLogProbMetric: 28.4986 - val_loss: 28.6874 - val_MinusLogProbMetric: 28.6874 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 375/1000
2023-10-26 21:56:03.450 
Epoch 375/1000 
	 loss: 28.5988, MinusLogProbMetric: 28.5988, val_loss: 28.7171, val_MinusLogProbMetric: 28.7171

Epoch 375: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5988 - MinusLogProbMetric: 28.5988 - val_loss: 28.7171 - val_MinusLogProbMetric: 28.7171 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 376/1000
2023-10-26 21:56:38.639 
Epoch 376/1000 
	 loss: 28.6012, MinusLogProbMetric: 28.6012, val_loss: 29.0026, val_MinusLogProbMetric: 29.0026

Epoch 376: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.6012 - MinusLogProbMetric: 28.6012 - val_loss: 29.0026 - val_MinusLogProbMetric: 29.0026 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 377/1000
2023-10-26 21:57:13.483 
Epoch 377/1000 
	 loss: 28.5207, MinusLogProbMetric: 28.5207, val_loss: 28.9509, val_MinusLogProbMetric: 28.9509

Epoch 377: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5207 - MinusLogProbMetric: 28.5207 - val_loss: 28.9509 - val_MinusLogProbMetric: 28.9509 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 378/1000
2023-10-26 21:57:46.527 
Epoch 378/1000 
	 loss: 28.5921, MinusLogProbMetric: 28.5921, val_loss: 29.7828, val_MinusLogProbMetric: 29.7828

Epoch 378: val_loss did not improve from 28.46034
196/196 - 33s - loss: 28.5921 - MinusLogProbMetric: 28.5921 - val_loss: 29.7828 - val_MinusLogProbMetric: 29.7828 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 379/1000
2023-10-26 21:58:16.763 
Epoch 379/1000 
	 loss: 28.6346, MinusLogProbMetric: 28.6346, val_loss: 28.7764, val_MinusLogProbMetric: 28.7764

Epoch 379: val_loss did not improve from 28.46034
196/196 - 30s - loss: 28.6346 - MinusLogProbMetric: 28.6346 - val_loss: 28.7764 - val_MinusLogProbMetric: 28.7764 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 380/1000
2023-10-26 21:58:46.170 
Epoch 380/1000 
	 loss: 28.4691, MinusLogProbMetric: 28.4691, val_loss: 29.1461, val_MinusLogProbMetric: 29.1461

Epoch 380: val_loss did not improve from 28.46034
196/196 - 29s - loss: 28.4691 - MinusLogProbMetric: 28.4691 - val_loss: 29.1461 - val_MinusLogProbMetric: 29.1461 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 381/1000
2023-10-26 21:59:18.947 
Epoch 381/1000 
	 loss: 28.5898, MinusLogProbMetric: 28.5898, val_loss: 29.2428, val_MinusLogProbMetric: 29.2428

Epoch 381: val_loss did not improve from 28.46034
196/196 - 33s - loss: 28.5898 - MinusLogProbMetric: 28.5898 - val_loss: 29.2428 - val_MinusLogProbMetric: 29.2428 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 382/1000
2023-10-26 21:59:53.823 
Epoch 382/1000 
	 loss: 28.5155, MinusLogProbMetric: 28.5155, val_loss: 28.4936, val_MinusLogProbMetric: 28.4936

Epoch 382: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5155 - MinusLogProbMetric: 28.5155 - val_loss: 28.4936 - val_MinusLogProbMetric: 28.4936 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 383/1000
2023-10-26 22:00:28.971 
Epoch 383/1000 
	 loss: 28.4601, MinusLogProbMetric: 28.4601, val_loss: 28.7997, val_MinusLogProbMetric: 28.7997

Epoch 383: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.4601 - MinusLogProbMetric: 28.4601 - val_loss: 28.7997 - val_MinusLogProbMetric: 28.7997 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 384/1000
2023-10-26 22:01:03.041 
Epoch 384/1000 
	 loss: 28.5469, MinusLogProbMetric: 28.5469, val_loss: 28.4992, val_MinusLogProbMetric: 28.4992

Epoch 384: val_loss did not improve from 28.46034
196/196 - 34s - loss: 28.5469 - MinusLogProbMetric: 28.5469 - val_loss: 28.4992 - val_MinusLogProbMetric: 28.4992 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 385/1000
2023-10-26 22:01:37.957 
Epoch 385/1000 
	 loss: 28.5350, MinusLogProbMetric: 28.5350, val_loss: 29.0237, val_MinusLogProbMetric: 29.0237

Epoch 385: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.5350 - MinusLogProbMetric: 28.5350 - val_loss: 29.0237 - val_MinusLogProbMetric: 29.0237 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 386/1000
2023-10-26 22:02:12.675 
Epoch 386/1000 
	 loss: 28.7150, MinusLogProbMetric: 28.7150, val_loss: 28.6874, val_MinusLogProbMetric: 28.6874

Epoch 386: val_loss did not improve from 28.46034
196/196 - 35s - loss: 28.7150 - MinusLogProbMetric: 28.7150 - val_loss: 28.6874 - val_MinusLogProbMetric: 28.6874 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 387/1000
2023-10-26 22:02:47.239 
Epoch 387/1000 
	 loss: 28.5865, MinusLogProbMetric: 28.5865, val_loss: 28.4063, val_MinusLogProbMetric: 28.4063

Epoch 387: val_loss improved from 28.46034 to 28.40634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 28.5865 - MinusLogProbMetric: 28.5865 - val_loss: 28.4063 - val_MinusLogProbMetric: 28.4063 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 388/1000
2023-10-26 22:03:22.711 
Epoch 388/1000 
	 loss: 28.5126, MinusLogProbMetric: 28.5126, val_loss: 28.8837, val_MinusLogProbMetric: 28.8837

Epoch 388: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5126 - MinusLogProbMetric: 28.5126 - val_loss: 28.8837 - val_MinusLogProbMetric: 28.8837 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 389/1000
2023-10-26 22:03:57.955 
Epoch 389/1000 
	 loss: 28.6072, MinusLogProbMetric: 28.6072, val_loss: 28.7001, val_MinusLogProbMetric: 28.7001

Epoch 389: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.6072 - MinusLogProbMetric: 28.6072 - val_loss: 28.7001 - val_MinusLogProbMetric: 28.7001 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 390/1000
2023-10-26 22:04:32.782 
Epoch 390/1000 
	 loss: 28.6348, MinusLogProbMetric: 28.6348, val_loss: 28.6011, val_MinusLogProbMetric: 28.6011

Epoch 390: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.6348 - MinusLogProbMetric: 28.6348 - val_loss: 28.6011 - val_MinusLogProbMetric: 28.6011 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 391/1000
2023-10-26 22:05:07.559 
Epoch 391/1000 
	 loss: 28.5474, MinusLogProbMetric: 28.5474, val_loss: 28.8853, val_MinusLogProbMetric: 28.8853

Epoch 391: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5474 - MinusLogProbMetric: 28.5474 - val_loss: 28.8853 - val_MinusLogProbMetric: 28.8853 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 392/1000
2023-10-26 22:05:42.286 
Epoch 392/1000 
	 loss: 28.6064, MinusLogProbMetric: 28.6064, val_loss: 28.7974, val_MinusLogProbMetric: 28.7974

Epoch 392: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.6064 - MinusLogProbMetric: 28.6064 - val_loss: 28.7974 - val_MinusLogProbMetric: 28.7974 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 393/1000
2023-10-26 22:06:17.201 
Epoch 393/1000 
	 loss: 28.5415, MinusLogProbMetric: 28.5415, val_loss: 28.6612, val_MinusLogProbMetric: 28.6612

Epoch 393: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5415 - MinusLogProbMetric: 28.5415 - val_loss: 28.6612 - val_MinusLogProbMetric: 28.6612 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 394/1000
2023-10-26 22:06:51.959 
Epoch 394/1000 
	 loss: 28.5563, MinusLogProbMetric: 28.5563, val_loss: 28.5676, val_MinusLogProbMetric: 28.5676

Epoch 394: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5563 - MinusLogProbMetric: 28.5563 - val_loss: 28.5676 - val_MinusLogProbMetric: 28.5676 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 395/1000
2023-10-26 22:07:26.896 
Epoch 395/1000 
	 loss: 28.5236, MinusLogProbMetric: 28.5236, val_loss: 28.5659, val_MinusLogProbMetric: 28.5659

Epoch 395: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5236 - MinusLogProbMetric: 28.5236 - val_loss: 28.5659 - val_MinusLogProbMetric: 28.5659 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 396/1000
2023-10-26 22:08:01.750 
Epoch 396/1000 
	 loss: 28.5782, MinusLogProbMetric: 28.5782, val_loss: 28.4805, val_MinusLogProbMetric: 28.4805

Epoch 396: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5782 - MinusLogProbMetric: 28.5782 - val_loss: 28.4805 - val_MinusLogProbMetric: 28.4805 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 397/1000
2023-10-26 22:08:36.941 
Epoch 397/1000 
	 loss: 28.5592, MinusLogProbMetric: 28.5592, val_loss: 29.0020, val_MinusLogProbMetric: 29.0020

Epoch 397: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5592 - MinusLogProbMetric: 28.5592 - val_loss: 29.0020 - val_MinusLogProbMetric: 29.0020 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 398/1000
2023-10-26 22:09:11.705 
Epoch 398/1000 
	 loss: 28.5242, MinusLogProbMetric: 28.5242, val_loss: 28.4757, val_MinusLogProbMetric: 28.4757

Epoch 398: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5242 - MinusLogProbMetric: 28.5242 - val_loss: 28.4757 - val_MinusLogProbMetric: 28.4757 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 399/1000
2023-10-26 22:09:46.473 
Epoch 399/1000 
	 loss: 28.5581, MinusLogProbMetric: 28.5581, val_loss: 28.6696, val_MinusLogProbMetric: 28.6696

Epoch 399: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5581 - MinusLogProbMetric: 28.5581 - val_loss: 28.6696 - val_MinusLogProbMetric: 28.6696 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 400/1000
2023-10-26 22:10:21.307 
Epoch 400/1000 
	 loss: 28.5287, MinusLogProbMetric: 28.5287, val_loss: 28.7979, val_MinusLogProbMetric: 28.7979

Epoch 400: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5287 - MinusLogProbMetric: 28.5287 - val_loss: 28.7979 - val_MinusLogProbMetric: 28.7979 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 401/1000
2023-10-26 22:10:56.362 
Epoch 401/1000 
	 loss: 28.6545, MinusLogProbMetric: 28.6545, val_loss: 28.7951, val_MinusLogProbMetric: 28.7951

Epoch 401: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.6545 - MinusLogProbMetric: 28.6545 - val_loss: 28.7951 - val_MinusLogProbMetric: 28.7951 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 402/1000
2023-10-26 22:11:31.521 
Epoch 402/1000 
	 loss: 28.5335, MinusLogProbMetric: 28.5335, val_loss: 29.1228, val_MinusLogProbMetric: 29.1228

Epoch 402: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5335 - MinusLogProbMetric: 28.5335 - val_loss: 29.1228 - val_MinusLogProbMetric: 29.1228 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 403/1000
2023-10-26 22:12:06.368 
Epoch 403/1000 
	 loss: 28.4846, MinusLogProbMetric: 28.4846, val_loss: 28.8070, val_MinusLogProbMetric: 28.8070

Epoch 403: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.4846 - MinusLogProbMetric: 28.4846 - val_loss: 28.8070 - val_MinusLogProbMetric: 28.8070 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 404/1000
2023-10-26 22:12:41.348 
Epoch 404/1000 
	 loss: 28.5297, MinusLogProbMetric: 28.5297, val_loss: 28.5259, val_MinusLogProbMetric: 28.5259

Epoch 404: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5297 - MinusLogProbMetric: 28.5297 - val_loss: 28.5259 - val_MinusLogProbMetric: 28.5259 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 405/1000
2023-10-26 22:13:16.091 
Epoch 405/1000 
	 loss: 28.5158, MinusLogProbMetric: 28.5158, val_loss: 28.5476, val_MinusLogProbMetric: 28.5476

Epoch 405: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5158 - MinusLogProbMetric: 28.5158 - val_loss: 28.5476 - val_MinusLogProbMetric: 28.5476 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 406/1000
2023-10-26 22:13:50.936 
Epoch 406/1000 
	 loss: 28.4918, MinusLogProbMetric: 28.4918, val_loss: 28.7116, val_MinusLogProbMetric: 28.7116

Epoch 406: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.4918 - MinusLogProbMetric: 28.4918 - val_loss: 28.7116 - val_MinusLogProbMetric: 28.7116 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 407/1000
2023-10-26 22:14:25.959 
Epoch 407/1000 
	 loss: 28.5434, MinusLogProbMetric: 28.5434, val_loss: 28.5866, val_MinusLogProbMetric: 28.5866

Epoch 407: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.5434 - MinusLogProbMetric: 28.5434 - val_loss: 28.5866 - val_MinusLogProbMetric: 28.5866 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 408/1000
2023-10-26 22:15:01.021 
Epoch 408/1000 
	 loss: 28.4369, MinusLogProbMetric: 28.4369, val_loss: 29.0372, val_MinusLogProbMetric: 29.0372

Epoch 408: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.4369 - MinusLogProbMetric: 28.4369 - val_loss: 29.0372 - val_MinusLogProbMetric: 29.0372 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 409/1000
2023-10-26 22:15:35.987 
Epoch 409/1000 
	 loss: 28.4880, MinusLogProbMetric: 28.4880, val_loss: 28.5173, val_MinusLogProbMetric: 28.5173

Epoch 409: val_loss did not improve from 28.40634
196/196 - 35s - loss: 28.4880 - MinusLogProbMetric: 28.4880 - val_loss: 28.5173 - val_MinusLogProbMetric: 28.5173 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 410/1000
2023-10-26 22:16:11.178 
Epoch 410/1000 
	 loss: 28.4735, MinusLogProbMetric: 28.4735, val_loss: 28.3961, val_MinusLogProbMetric: 28.3961

Epoch 410: val_loss improved from 28.40634 to 28.39615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 28.4735 - MinusLogProbMetric: 28.4735 - val_loss: 28.3961 - val_MinusLogProbMetric: 28.3961 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 411/1000
2023-10-26 22:16:46.611 
Epoch 411/1000 
	 loss: 28.5144, MinusLogProbMetric: 28.5144, val_loss: 28.7496, val_MinusLogProbMetric: 28.7496

Epoch 411: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.5144 - MinusLogProbMetric: 28.5144 - val_loss: 28.7496 - val_MinusLogProbMetric: 28.7496 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 412/1000
2023-10-26 22:17:21.388 
Epoch 412/1000 
	 loss: 28.6234, MinusLogProbMetric: 28.6234, val_loss: 29.1180, val_MinusLogProbMetric: 29.1180

Epoch 412: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.6234 - MinusLogProbMetric: 28.6234 - val_loss: 29.1180 - val_MinusLogProbMetric: 29.1180 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 413/1000
2023-10-26 22:17:56.381 
Epoch 413/1000 
	 loss: 28.6400, MinusLogProbMetric: 28.6400, val_loss: 28.8014, val_MinusLogProbMetric: 28.8014

Epoch 413: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.6400 - MinusLogProbMetric: 28.6400 - val_loss: 28.8014 - val_MinusLogProbMetric: 28.8014 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 414/1000
2023-10-26 22:18:31.421 
Epoch 414/1000 
	 loss: 28.5385, MinusLogProbMetric: 28.5385, val_loss: 28.6256, val_MinusLogProbMetric: 28.6256

Epoch 414: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.5385 - MinusLogProbMetric: 28.5385 - val_loss: 28.6256 - val_MinusLogProbMetric: 28.6256 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 415/1000
2023-10-26 22:19:06.584 
Epoch 415/1000 
	 loss: 28.5898, MinusLogProbMetric: 28.5898, val_loss: 28.9317, val_MinusLogProbMetric: 28.9317

Epoch 415: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.5898 - MinusLogProbMetric: 28.5898 - val_loss: 28.9317 - val_MinusLogProbMetric: 28.9317 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 416/1000
2023-10-26 22:19:41.415 
Epoch 416/1000 
	 loss: 28.4876, MinusLogProbMetric: 28.4876, val_loss: 28.4639, val_MinusLogProbMetric: 28.4639

Epoch 416: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.4876 - MinusLogProbMetric: 28.4876 - val_loss: 28.4639 - val_MinusLogProbMetric: 28.4639 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 417/1000
2023-10-26 22:20:16.516 
Epoch 417/1000 
	 loss: 28.5611, MinusLogProbMetric: 28.5611, val_loss: 28.6051, val_MinusLogProbMetric: 28.6051

Epoch 417: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.5611 - MinusLogProbMetric: 28.5611 - val_loss: 28.6051 - val_MinusLogProbMetric: 28.6051 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 418/1000
2023-10-26 22:20:51.381 
Epoch 418/1000 
	 loss: 28.5440, MinusLogProbMetric: 28.5440, val_loss: 28.6233, val_MinusLogProbMetric: 28.6233

Epoch 418: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.5440 - MinusLogProbMetric: 28.5440 - val_loss: 28.6233 - val_MinusLogProbMetric: 28.6233 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 419/1000
2023-10-26 22:21:26.177 
Epoch 419/1000 
	 loss: 28.5003, MinusLogProbMetric: 28.5003, val_loss: 28.6054, val_MinusLogProbMetric: 28.6054

Epoch 419: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.5003 - MinusLogProbMetric: 28.5003 - val_loss: 28.6054 - val_MinusLogProbMetric: 28.6054 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 420/1000
2023-10-26 22:22:01.238 
Epoch 420/1000 
	 loss: 28.5626, MinusLogProbMetric: 28.5626, val_loss: 28.5953, val_MinusLogProbMetric: 28.5953

Epoch 420: val_loss did not improve from 28.39615
196/196 - 35s - loss: 28.5626 - MinusLogProbMetric: 28.5626 - val_loss: 28.5953 - val_MinusLogProbMetric: 28.5953 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 421/1000
2023-10-26 22:22:36.033 
Epoch 421/1000 
	 loss: 28.5399, MinusLogProbMetric: 28.5399, val_loss: 28.3858, val_MinusLogProbMetric: 28.3858

Epoch 421: val_loss improved from 28.39615 to 28.38579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 28.5399 - MinusLogProbMetric: 28.5399 - val_loss: 28.3858 - val_MinusLogProbMetric: 28.3858 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 422/1000
2023-10-26 22:23:11.449 
Epoch 422/1000 
	 loss: 28.5663, MinusLogProbMetric: 28.5663, val_loss: 28.8542, val_MinusLogProbMetric: 28.8542

Epoch 422: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.5663 - MinusLogProbMetric: 28.5663 - val_loss: 28.8542 - val_MinusLogProbMetric: 28.8542 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 423/1000
2023-10-26 22:23:46.331 
Epoch 423/1000 
	 loss: 28.4526, MinusLogProbMetric: 28.4526, val_loss: 28.6631, val_MinusLogProbMetric: 28.6631

Epoch 423: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.4526 - MinusLogProbMetric: 28.4526 - val_loss: 28.6631 - val_MinusLogProbMetric: 28.6631 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 424/1000
2023-10-26 22:24:21.152 
Epoch 424/1000 
	 loss: 28.6294, MinusLogProbMetric: 28.6294, val_loss: 28.6286, val_MinusLogProbMetric: 28.6286

Epoch 424: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.6294 - MinusLogProbMetric: 28.6294 - val_loss: 28.6286 - val_MinusLogProbMetric: 28.6286 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 425/1000
2023-10-26 22:24:55.955 
Epoch 425/1000 
	 loss: 28.4719, MinusLogProbMetric: 28.4719, val_loss: 28.7562, val_MinusLogProbMetric: 28.7562

Epoch 425: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.4719 - MinusLogProbMetric: 28.4719 - val_loss: 28.7562 - val_MinusLogProbMetric: 28.7562 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 426/1000
2023-10-26 22:25:30.956 
Epoch 426/1000 
	 loss: 28.5125, MinusLogProbMetric: 28.5125, val_loss: 28.5552, val_MinusLogProbMetric: 28.5552

Epoch 426: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.5125 - MinusLogProbMetric: 28.5125 - val_loss: 28.5552 - val_MinusLogProbMetric: 28.5552 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 427/1000
2023-10-26 22:26:05.710 
Epoch 427/1000 
	 loss: 28.5526, MinusLogProbMetric: 28.5526, val_loss: 28.4573, val_MinusLogProbMetric: 28.4573

Epoch 427: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.5526 - MinusLogProbMetric: 28.5526 - val_loss: 28.4573 - val_MinusLogProbMetric: 28.4573 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 428/1000
2023-10-26 22:26:40.839 
Epoch 428/1000 
	 loss: 28.4832, MinusLogProbMetric: 28.4832, val_loss: 28.4145, val_MinusLogProbMetric: 28.4145

Epoch 428: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.4832 - MinusLogProbMetric: 28.4832 - val_loss: 28.4145 - val_MinusLogProbMetric: 28.4145 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 429/1000
2023-10-26 22:27:15.760 
Epoch 429/1000 
	 loss: 28.5422, MinusLogProbMetric: 28.5422, val_loss: 28.9724, val_MinusLogProbMetric: 28.9724

Epoch 429: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.5422 - MinusLogProbMetric: 28.5422 - val_loss: 28.9724 - val_MinusLogProbMetric: 28.9724 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 430/1000
2023-10-26 22:27:50.842 
Epoch 430/1000 
	 loss: 28.4684, MinusLogProbMetric: 28.4684, val_loss: 28.6443, val_MinusLogProbMetric: 28.6443

Epoch 430: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.4684 - MinusLogProbMetric: 28.4684 - val_loss: 28.6443 - val_MinusLogProbMetric: 28.6443 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 431/1000
2023-10-26 22:28:25.503 
Epoch 431/1000 
	 loss: 28.5343, MinusLogProbMetric: 28.5343, val_loss: 28.9451, val_MinusLogProbMetric: 28.9451

Epoch 431: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.5343 - MinusLogProbMetric: 28.5343 - val_loss: 28.9451 - val_MinusLogProbMetric: 28.9451 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 432/1000
2023-10-26 22:29:00.337 
Epoch 432/1000 
	 loss: 28.4344, MinusLogProbMetric: 28.4344, val_loss: 28.6666, val_MinusLogProbMetric: 28.6666

Epoch 432: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.4344 - MinusLogProbMetric: 28.4344 - val_loss: 28.6666 - val_MinusLogProbMetric: 28.6666 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 433/1000
2023-10-26 22:29:35.155 
Epoch 433/1000 
	 loss: 28.5963, MinusLogProbMetric: 28.5963, val_loss: 28.5804, val_MinusLogProbMetric: 28.5804

Epoch 433: val_loss did not improve from 28.38579
196/196 - 35s - loss: 28.5963 - MinusLogProbMetric: 28.5963 - val_loss: 28.5804 - val_MinusLogProbMetric: 28.5804 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 434/1000
2023-10-26 22:30:10.278 
Epoch 434/1000 
	 loss: 28.4861, MinusLogProbMetric: 28.4861, val_loss: 28.2601, val_MinusLogProbMetric: 28.2601

Epoch 434: val_loss improved from 28.38579 to 28.26012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 28.4861 - MinusLogProbMetric: 28.4861 - val_loss: 28.2601 - val_MinusLogProbMetric: 28.2601 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 435/1000
2023-10-26 22:30:45.625 
Epoch 435/1000 
	 loss: 28.4113, MinusLogProbMetric: 28.4113, val_loss: 28.4036, val_MinusLogProbMetric: 28.4036

Epoch 435: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4113 - MinusLogProbMetric: 28.4113 - val_loss: 28.4036 - val_MinusLogProbMetric: 28.4036 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 436/1000
2023-10-26 22:31:20.643 
Epoch 436/1000 
	 loss: 28.4861, MinusLogProbMetric: 28.4861, val_loss: 28.7652, val_MinusLogProbMetric: 28.7652

Epoch 436: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4861 - MinusLogProbMetric: 28.4861 - val_loss: 28.7652 - val_MinusLogProbMetric: 28.7652 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 437/1000
2023-10-26 22:31:55.628 
Epoch 437/1000 
	 loss: 28.5234, MinusLogProbMetric: 28.5234, val_loss: 28.6437, val_MinusLogProbMetric: 28.6437

Epoch 437: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5234 - MinusLogProbMetric: 28.5234 - val_loss: 28.6437 - val_MinusLogProbMetric: 28.6437 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 438/1000
2023-10-26 22:32:30.715 
Epoch 438/1000 
	 loss: 28.4625, MinusLogProbMetric: 28.4625, val_loss: 28.5174, val_MinusLogProbMetric: 28.5174

Epoch 438: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4625 - MinusLogProbMetric: 28.4625 - val_loss: 28.5174 - val_MinusLogProbMetric: 28.5174 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 439/1000
2023-10-26 22:33:05.824 
Epoch 439/1000 
	 loss: 28.4360, MinusLogProbMetric: 28.4360, val_loss: 28.8308, val_MinusLogProbMetric: 28.8308

Epoch 439: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4360 - MinusLogProbMetric: 28.4360 - val_loss: 28.8308 - val_MinusLogProbMetric: 28.8308 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 440/1000
2023-10-26 22:33:41.198 
Epoch 440/1000 
	 loss: 28.5005, MinusLogProbMetric: 28.5005, val_loss: 29.7989, val_MinusLogProbMetric: 29.7989

Epoch 440: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5005 - MinusLogProbMetric: 28.5005 - val_loss: 29.7989 - val_MinusLogProbMetric: 29.7989 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 441/1000
2023-10-26 22:34:16.239 
Epoch 441/1000 
	 loss: 28.6527, MinusLogProbMetric: 28.6527, val_loss: 28.7996, val_MinusLogProbMetric: 28.7996

Epoch 441: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.6527 - MinusLogProbMetric: 28.6527 - val_loss: 28.7996 - val_MinusLogProbMetric: 28.7996 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 442/1000
2023-10-26 22:34:51.460 
Epoch 442/1000 
	 loss: 28.4857, MinusLogProbMetric: 28.4857, val_loss: 28.4934, val_MinusLogProbMetric: 28.4934

Epoch 442: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4857 - MinusLogProbMetric: 28.4857 - val_loss: 28.4934 - val_MinusLogProbMetric: 28.4934 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 443/1000
2023-10-26 22:35:26.191 
Epoch 443/1000 
	 loss: 28.5253, MinusLogProbMetric: 28.5253, val_loss: 28.9303, val_MinusLogProbMetric: 28.9303

Epoch 443: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5253 - MinusLogProbMetric: 28.5253 - val_loss: 28.9303 - val_MinusLogProbMetric: 28.9303 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 444/1000
2023-10-26 22:36:01.164 
Epoch 444/1000 
	 loss: 28.4571, MinusLogProbMetric: 28.4571, val_loss: 28.5591, val_MinusLogProbMetric: 28.5591

Epoch 444: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4571 - MinusLogProbMetric: 28.4571 - val_loss: 28.5591 - val_MinusLogProbMetric: 28.5591 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 445/1000
2023-10-26 22:36:36.379 
Epoch 445/1000 
	 loss: 28.4195, MinusLogProbMetric: 28.4195, val_loss: 28.5618, val_MinusLogProbMetric: 28.5618

Epoch 445: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4195 - MinusLogProbMetric: 28.4195 - val_loss: 28.5618 - val_MinusLogProbMetric: 28.5618 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 446/1000
2023-10-26 22:37:11.645 
Epoch 446/1000 
	 loss: 28.4125, MinusLogProbMetric: 28.4125, val_loss: 28.6942, val_MinusLogProbMetric: 28.6942

Epoch 446: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4125 - MinusLogProbMetric: 28.4125 - val_loss: 28.6942 - val_MinusLogProbMetric: 28.6942 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 447/1000
2023-10-26 22:37:46.601 
Epoch 447/1000 
	 loss: 28.5236, MinusLogProbMetric: 28.5236, val_loss: 29.0530, val_MinusLogProbMetric: 29.0530

Epoch 447: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5236 - MinusLogProbMetric: 28.5236 - val_loss: 29.0530 - val_MinusLogProbMetric: 29.0530 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 448/1000
2023-10-26 22:38:22.346 
Epoch 448/1000 
	 loss: 28.4679, MinusLogProbMetric: 28.4679, val_loss: 28.8595, val_MinusLogProbMetric: 28.8595

Epoch 448: val_loss did not improve from 28.26012
196/196 - 36s - loss: 28.4679 - MinusLogProbMetric: 28.4679 - val_loss: 28.8595 - val_MinusLogProbMetric: 28.8595 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 449/1000
2023-10-26 22:38:57.724 
Epoch 449/1000 
	 loss: 28.4846, MinusLogProbMetric: 28.4846, val_loss: 29.1718, val_MinusLogProbMetric: 29.1718

Epoch 449: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4846 - MinusLogProbMetric: 28.4846 - val_loss: 29.1718 - val_MinusLogProbMetric: 29.1718 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 450/1000
2023-10-26 22:39:33.023 
Epoch 450/1000 
	 loss: 28.4310, MinusLogProbMetric: 28.4310, val_loss: 28.5175, val_MinusLogProbMetric: 28.5175

Epoch 450: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4310 - MinusLogProbMetric: 28.4310 - val_loss: 28.5175 - val_MinusLogProbMetric: 28.5175 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 451/1000
2023-10-26 22:40:08.231 
Epoch 451/1000 
	 loss: 28.5996, MinusLogProbMetric: 28.5996, val_loss: 28.5246, val_MinusLogProbMetric: 28.5246

Epoch 451: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5996 - MinusLogProbMetric: 28.5996 - val_loss: 28.5246 - val_MinusLogProbMetric: 28.5246 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 452/1000
2023-10-26 22:40:43.514 
Epoch 452/1000 
	 loss: 28.4634, MinusLogProbMetric: 28.4634, val_loss: 28.5318, val_MinusLogProbMetric: 28.5318

Epoch 452: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4634 - MinusLogProbMetric: 28.4634 - val_loss: 28.5318 - val_MinusLogProbMetric: 28.5318 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 453/1000
2023-10-26 22:41:18.710 
Epoch 453/1000 
	 loss: 28.4418, MinusLogProbMetric: 28.4418, val_loss: 28.5800, val_MinusLogProbMetric: 28.5800

Epoch 453: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4418 - MinusLogProbMetric: 28.4418 - val_loss: 28.5800 - val_MinusLogProbMetric: 28.5800 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 454/1000
2023-10-26 22:41:53.909 
Epoch 454/1000 
	 loss: 28.4631, MinusLogProbMetric: 28.4631, val_loss: 28.7578, val_MinusLogProbMetric: 28.7578

Epoch 454: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4631 - MinusLogProbMetric: 28.4631 - val_loss: 28.7578 - val_MinusLogProbMetric: 28.7578 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 455/1000
2023-10-26 22:42:28.977 
Epoch 455/1000 
	 loss: 28.4036, MinusLogProbMetric: 28.4036, val_loss: 28.8852, val_MinusLogProbMetric: 28.8852

Epoch 455: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4036 - MinusLogProbMetric: 28.4036 - val_loss: 28.8852 - val_MinusLogProbMetric: 28.8852 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 456/1000
2023-10-26 22:43:04.024 
Epoch 456/1000 
	 loss: 28.3647, MinusLogProbMetric: 28.3647, val_loss: 29.2237, val_MinusLogProbMetric: 29.2237

Epoch 456: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3647 - MinusLogProbMetric: 28.3647 - val_loss: 29.2237 - val_MinusLogProbMetric: 29.2237 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 457/1000
2023-10-26 22:43:39.013 
Epoch 457/1000 
	 loss: 28.3968, MinusLogProbMetric: 28.3968, val_loss: 28.5847, val_MinusLogProbMetric: 28.5847

Epoch 457: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3968 - MinusLogProbMetric: 28.3968 - val_loss: 28.5847 - val_MinusLogProbMetric: 28.5847 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 458/1000
2023-10-26 22:44:14.242 
Epoch 458/1000 
	 loss: 28.4227, MinusLogProbMetric: 28.4227, val_loss: 28.7225, val_MinusLogProbMetric: 28.7225

Epoch 458: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4227 - MinusLogProbMetric: 28.4227 - val_loss: 28.7225 - val_MinusLogProbMetric: 28.7225 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 459/1000
2023-10-26 22:44:49.377 
Epoch 459/1000 
	 loss: 28.4170, MinusLogProbMetric: 28.4170, val_loss: 28.4764, val_MinusLogProbMetric: 28.4764

Epoch 459: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4170 - MinusLogProbMetric: 28.4170 - val_loss: 28.4764 - val_MinusLogProbMetric: 28.4764 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 460/1000
2023-10-26 22:45:24.232 
Epoch 460/1000 
	 loss: 28.5238, MinusLogProbMetric: 28.5238, val_loss: 28.6279, val_MinusLogProbMetric: 28.6279

Epoch 460: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5238 - MinusLogProbMetric: 28.5238 - val_loss: 28.6279 - val_MinusLogProbMetric: 28.6279 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 461/1000
2023-10-26 22:45:59.368 
Epoch 461/1000 
	 loss: 28.3870, MinusLogProbMetric: 28.3870, val_loss: 28.8841, val_MinusLogProbMetric: 28.8841

Epoch 461: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3870 - MinusLogProbMetric: 28.3870 - val_loss: 28.8841 - val_MinusLogProbMetric: 28.8841 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 462/1000
2023-10-26 22:46:34.388 
Epoch 462/1000 
	 loss: 28.3831, MinusLogProbMetric: 28.3831, val_loss: 28.5468, val_MinusLogProbMetric: 28.5468

Epoch 462: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3831 - MinusLogProbMetric: 28.3831 - val_loss: 28.5468 - val_MinusLogProbMetric: 28.5468 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 463/1000
2023-10-26 22:47:09.634 
Epoch 463/1000 
	 loss: 28.4391, MinusLogProbMetric: 28.4391, val_loss: 28.2696, val_MinusLogProbMetric: 28.2696

Epoch 463: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4391 - MinusLogProbMetric: 28.4391 - val_loss: 28.2696 - val_MinusLogProbMetric: 28.2696 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 464/1000
2023-10-26 22:47:44.694 
Epoch 464/1000 
	 loss: 28.4667, MinusLogProbMetric: 28.4667, val_loss: 28.3958, val_MinusLogProbMetric: 28.3958

Epoch 464: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4667 - MinusLogProbMetric: 28.4667 - val_loss: 28.3958 - val_MinusLogProbMetric: 28.3958 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 465/1000
2023-10-26 22:48:19.544 
Epoch 465/1000 
	 loss: 28.4004, MinusLogProbMetric: 28.4004, val_loss: 29.6420, val_MinusLogProbMetric: 29.6420

Epoch 465: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4004 - MinusLogProbMetric: 28.4004 - val_loss: 29.6420 - val_MinusLogProbMetric: 29.6420 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 466/1000
2023-10-26 22:48:54.740 
Epoch 466/1000 
	 loss: 28.4258, MinusLogProbMetric: 28.4258, val_loss: 28.4199, val_MinusLogProbMetric: 28.4199

Epoch 466: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4258 - MinusLogProbMetric: 28.4258 - val_loss: 28.4199 - val_MinusLogProbMetric: 28.4199 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 467/1000
2023-10-26 22:49:29.834 
Epoch 467/1000 
	 loss: 28.5079, MinusLogProbMetric: 28.5079, val_loss: 28.9657, val_MinusLogProbMetric: 28.9657

Epoch 467: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5079 - MinusLogProbMetric: 28.5079 - val_loss: 28.9657 - val_MinusLogProbMetric: 28.9657 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 468/1000
2023-10-26 22:50:04.951 
Epoch 468/1000 
	 loss: 28.4497, MinusLogProbMetric: 28.4497, val_loss: 28.6077, val_MinusLogProbMetric: 28.6077

Epoch 468: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4497 - MinusLogProbMetric: 28.4497 - val_loss: 28.6077 - val_MinusLogProbMetric: 28.6077 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 469/1000
2023-10-26 22:50:39.257 
Epoch 469/1000 
	 loss: 28.3879, MinusLogProbMetric: 28.3879, val_loss: 28.8844, val_MinusLogProbMetric: 28.8844

Epoch 469: val_loss did not improve from 28.26012
196/196 - 34s - loss: 28.3879 - MinusLogProbMetric: 28.3879 - val_loss: 28.8844 - val_MinusLogProbMetric: 28.8844 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 470/1000
2023-10-26 22:51:13.998 
Epoch 470/1000 
	 loss: 28.3929, MinusLogProbMetric: 28.3929, val_loss: 28.4256, val_MinusLogProbMetric: 28.4256

Epoch 470: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3929 - MinusLogProbMetric: 28.3929 - val_loss: 28.4256 - val_MinusLogProbMetric: 28.4256 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 471/1000
2023-10-26 22:51:48.755 
Epoch 471/1000 
	 loss: 28.5181, MinusLogProbMetric: 28.5181, val_loss: 28.4671, val_MinusLogProbMetric: 28.4671

Epoch 471: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.5181 - MinusLogProbMetric: 28.5181 - val_loss: 28.4671 - val_MinusLogProbMetric: 28.4671 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 472/1000
2023-10-26 22:52:23.670 
Epoch 472/1000 
	 loss: 28.3916, MinusLogProbMetric: 28.3916, val_loss: 28.3754, val_MinusLogProbMetric: 28.3754

Epoch 472: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3916 - MinusLogProbMetric: 28.3916 - val_loss: 28.3754 - val_MinusLogProbMetric: 28.3754 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 473/1000
2023-10-26 22:52:58.401 
Epoch 473/1000 
	 loss: 28.4717, MinusLogProbMetric: 28.4717, val_loss: 28.5128, val_MinusLogProbMetric: 28.5128

Epoch 473: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4717 - MinusLogProbMetric: 28.4717 - val_loss: 28.5128 - val_MinusLogProbMetric: 28.5128 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 474/1000
2023-10-26 22:53:33.721 
Epoch 474/1000 
	 loss: 28.4148, MinusLogProbMetric: 28.4148, val_loss: 29.3315, val_MinusLogProbMetric: 29.3315

Epoch 474: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4148 - MinusLogProbMetric: 28.4148 - val_loss: 29.3315 - val_MinusLogProbMetric: 29.3315 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 475/1000
2023-10-26 22:54:09.077 
Epoch 475/1000 
	 loss: 28.4476, MinusLogProbMetric: 28.4476, val_loss: 28.9205, val_MinusLogProbMetric: 28.9205

Epoch 475: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4476 - MinusLogProbMetric: 28.4476 - val_loss: 28.9205 - val_MinusLogProbMetric: 28.9205 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 476/1000
2023-10-26 22:54:44.066 
Epoch 476/1000 
	 loss: 28.4510, MinusLogProbMetric: 28.4510, val_loss: 28.5694, val_MinusLogProbMetric: 28.5694

Epoch 476: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4510 - MinusLogProbMetric: 28.4510 - val_loss: 28.5694 - val_MinusLogProbMetric: 28.5694 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 477/1000
2023-10-26 22:55:18.990 
Epoch 477/1000 
	 loss: 28.4111, MinusLogProbMetric: 28.4111, val_loss: 28.6457, val_MinusLogProbMetric: 28.6457

Epoch 477: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4111 - MinusLogProbMetric: 28.4111 - val_loss: 28.6457 - val_MinusLogProbMetric: 28.6457 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 478/1000
2023-10-26 22:55:54.168 
Epoch 478/1000 
	 loss: 28.4438, MinusLogProbMetric: 28.4438, val_loss: 28.4530, val_MinusLogProbMetric: 28.4530

Epoch 478: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4438 - MinusLogProbMetric: 28.4438 - val_loss: 28.4530 - val_MinusLogProbMetric: 28.4530 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 479/1000
2023-10-26 22:56:29.271 
Epoch 479/1000 
	 loss: 28.3324, MinusLogProbMetric: 28.3324, val_loss: 28.9734, val_MinusLogProbMetric: 28.9734

Epoch 479: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3324 - MinusLogProbMetric: 28.3324 - val_loss: 28.9734 - val_MinusLogProbMetric: 28.9734 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 480/1000
2023-10-26 22:57:04.448 
Epoch 480/1000 
	 loss: 28.4244, MinusLogProbMetric: 28.4244, val_loss: 28.5380, val_MinusLogProbMetric: 28.5380

Epoch 480: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4244 - MinusLogProbMetric: 28.4244 - val_loss: 28.5380 - val_MinusLogProbMetric: 28.5380 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 481/1000
2023-10-26 22:57:39.512 
Epoch 481/1000 
	 loss: 28.3914, MinusLogProbMetric: 28.3914, val_loss: 28.8144, val_MinusLogProbMetric: 28.8144

Epoch 481: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.3914 - MinusLogProbMetric: 28.3914 - val_loss: 28.8144 - val_MinusLogProbMetric: 28.8144 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 482/1000
2023-10-26 22:58:14.563 
Epoch 482/1000 
	 loss: 28.4103, MinusLogProbMetric: 28.4103, val_loss: 29.7383, val_MinusLogProbMetric: 29.7383

Epoch 482: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4103 - MinusLogProbMetric: 28.4103 - val_loss: 29.7383 - val_MinusLogProbMetric: 29.7383 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 483/1000
2023-10-26 22:58:49.965 
Epoch 483/1000 
	 loss: 28.4150, MinusLogProbMetric: 28.4150, val_loss: 28.4492, val_MinusLogProbMetric: 28.4492

Epoch 483: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4150 - MinusLogProbMetric: 28.4150 - val_loss: 28.4492 - val_MinusLogProbMetric: 28.4492 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 484/1000
2023-10-26 22:59:24.969 
Epoch 484/1000 
	 loss: 28.4068, MinusLogProbMetric: 28.4068, val_loss: 28.7671, val_MinusLogProbMetric: 28.7671

Epoch 484: val_loss did not improve from 28.26012
196/196 - 35s - loss: 28.4068 - MinusLogProbMetric: 28.4068 - val_loss: 28.7671 - val_MinusLogProbMetric: 28.7671 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 485/1000
2023-10-26 23:00:00.155 
Epoch 485/1000 
	 loss: 27.9376, MinusLogProbMetric: 27.9376, val_loss: 28.0900, val_MinusLogProbMetric: 28.0900

Epoch 485: val_loss improved from 28.26012 to 28.08999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.9376 - MinusLogProbMetric: 27.9376 - val_loss: 28.0900 - val_MinusLogProbMetric: 28.0900 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 486/1000
2023-10-26 23:00:35.780 
Epoch 486/1000 
	 loss: 27.9481, MinusLogProbMetric: 27.9481, val_loss: 28.1005, val_MinusLogProbMetric: 28.1005

Epoch 486: val_loss did not improve from 28.08999
196/196 - 35s - loss: 27.9481 - MinusLogProbMetric: 27.9481 - val_loss: 28.1005 - val_MinusLogProbMetric: 28.1005 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 487/1000
2023-10-26 23:01:10.753 
Epoch 487/1000 
	 loss: 27.9569, MinusLogProbMetric: 27.9569, val_loss: 28.2248, val_MinusLogProbMetric: 28.2248

Epoch 487: val_loss did not improve from 28.08999
196/196 - 35s - loss: 27.9569 - MinusLogProbMetric: 27.9569 - val_loss: 28.2248 - val_MinusLogProbMetric: 28.2248 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 488/1000
2023-10-26 23:01:45.610 
Epoch 488/1000 
	 loss: 27.9267, MinusLogProbMetric: 27.9267, val_loss: 28.0586, val_MinusLogProbMetric: 28.0586

Epoch 488: val_loss improved from 28.08999 to 28.05860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.9267 - MinusLogProbMetric: 27.9267 - val_loss: 28.0586 - val_MinusLogProbMetric: 28.0586 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 489/1000
2023-10-26 23:02:21.297 
Epoch 489/1000 
	 loss: 27.9579, MinusLogProbMetric: 27.9579, val_loss: 28.5392, val_MinusLogProbMetric: 28.5392

Epoch 489: val_loss did not improve from 28.05860
196/196 - 35s - loss: 27.9579 - MinusLogProbMetric: 27.9579 - val_loss: 28.5392 - val_MinusLogProbMetric: 28.5392 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 490/1000
2023-10-26 23:02:56.281 
Epoch 490/1000 
	 loss: 27.9392, MinusLogProbMetric: 27.9392, val_loss: 28.1123, val_MinusLogProbMetric: 28.1123

Epoch 490: val_loss did not improve from 28.05860
196/196 - 35s - loss: 27.9392 - MinusLogProbMetric: 27.9392 - val_loss: 28.1123 - val_MinusLogProbMetric: 28.1123 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 491/1000
2023-10-26 23:03:31.565 
Epoch 491/1000 
	 loss: 27.9744, MinusLogProbMetric: 27.9744, val_loss: 28.0501, val_MinusLogProbMetric: 28.0501

Epoch 491: val_loss improved from 28.05860 to 28.05010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.9744 - MinusLogProbMetric: 27.9744 - val_loss: 28.0501 - val_MinusLogProbMetric: 28.0501 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 492/1000
2023-10-26 23:04:07.058 
Epoch 492/1000 
	 loss: 27.9550, MinusLogProbMetric: 27.9550, val_loss: 28.1355, val_MinusLogProbMetric: 28.1355

Epoch 492: val_loss did not improve from 28.05010
196/196 - 35s - loss: 27.9550 - MinusLogProbMetric: 27.9550 - val_loss: 28.1355 - val_MinusLogProbMetric: 28.1355 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 493/1000
2023-10-26 23:04:42.243 
Epoch 493/1000 
	 loss: 27.9428, MinusLogProbMetric: 27.9428, val_loss: 28.2913, val_MinusLogProbMetric: 28.2913

Epoch 493: val_loss did not improve from 28.05010
196/196 - 35s - loss: 27.9428 - MinusLogProbMetric: 27.9428 - val_loss: 28.2913 - val_MinusLogProbMetric: 28.2913 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 494/1000
2023-10-26 23:05:17.584 
Epoch 494/1000 
	 loss: 27.9172, MinusLogProbMetric: 27.9172, val_loss: 28.0945, val_MinusLogProbMetric: 28.0945

Epoch 494: val_loss did not improve from 28.05010
196/196 - 35s - loss: 27.9172 - MinusLogProbMetric: 27.9172 - val_loss: 28.0945 - val_MinusLogProbMetric: 28.0945 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 495/1000
2023-10-26 23:05:52.810 
Epoch 495/1000 
	 loss: 27.9604, MinusLogProbMetric: 27.9604, val_loss: 28.0929, val_MinusLogProbMetric: 28.0929

Epoch 495: val_loss did not improve from 28.05010
196/196 - 35s - loss: 27.9604 - MinusLogProbMetric: 27.9604 - val_loss: 28.0929 - val_MinusLogProbMetric: 28.0929 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 496/1000
2023-10-26 23:06:28.001 
Epoch 496/1000 
	 loss: 27.9318, MinusLogProbMetric: 27.9318, val_loss: 28.0137, val_MinusLogProbMetric: 28.0137

Epoch 496: val_loss improved from 28.05010 to 28.01368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.9318 - MinusLogProbMetric: 27.9318 - val_loss: 28.0137 - val_MinusLogProbMetric: 28.0137 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 497/1000
2023-10-26 23:07:03.544 
Epoch 497/1000 
	 loss: 27.9952, MinusLogProbMetric: 27.9952, val_loss: 28.3077, val_MinusLogProbMetric: 28.3077

Epoch 497: val_loss did not improve from 28.01368
196/196 - 35s - loss: 27.9952 - MinusLogProbMetric: 27.9952 - val_loss: 28.3077 - val_MinusLogProbMetric: 28.3077 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 498/1000
2023-10-26 23:07:38.750 
Epoch 498/1000 
	 loss: 27.9252, MinusLogProbMetric: 27.9252, val_loss: 28.0851, val_MinusLogProbMetric: 28.0851

Epoch 498: val_loss did not improve from 28.01368
196/196 - 35s - loss: 27.9252 - MinusLogProbMetric: 27.9252 - val_loss: 28.0851 - val_MinusLogProbMetric: 28.0851 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 499/1000
2023-10-26 23:08:13.969 
Epoch 499/1000 
	 loss: 27.9213, MinusLogProbMetric: 27.9213, val_loss: 28.1089, val_MinusLogProbMetric: 28.1089

Epoch 499: val_loss did not improve from 28.01368
196/196 - 35s - loss: 27.9213 - MinusLogProbMetric: 27.9213 - val_loss: 28.1089 - val_MinusLogProbMetric: 28.1089 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 500/1000
2023-10-26 23:08:48.959 
Epoch 500/1000 
	 loss: 27.9594, MinusLogProbMetric: 27.9594, val_loss: 28.4165, val_MinusLogProbMetric: 28.4165

Epoch 500: val_loss did not improve from 28.01368
196/196 - 35s - loss: 27.9594 - MinusLogProbMetric: 27.9594 - val_loss: 28.4165 - val_MinusLogProbMetric: 28.4165 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 501/1000
2023-10-26 23:09:23.571 
Epoch 501/1000 
	 loss: 27.9272, MinusLogProbMetric: 27.9272, val_loss: 28.1563, val_MinusLogProbMetric: 28.1563

Epoch 501: val_loss did not improve from 28.01368
196/196 - 35s - loss: 27.9272 - MinusLogProbMetric: 27.9272 - val_loss: 28.1563 - val_MinusLogProbMetric: 28.1563 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 502/1000
2023-10-26 23:09:58.578 
Epoch 502/1000 
	 loss: 28.0009, MinusLogProbMetric: 28.0009, val_loss: 28.1056, val_MinusLogProbMetric: 28.1056

Epoch 502: val_loss did not improve from 28.01368
196/196 - 35s - loss: 28.0009 - MinusLogProbMetric: 28.0009 - val_loss: 28.1056 - val_MinusLogProbMetric: 28.1056 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 503/1000
2023-10-26 23:10:33.462 
Epoch 503/1000 
	 loss: 27.8525, MinusLogProbMetric: 27.8525, val_loss: 27.9953, val_MinusLogProbMetric: 27.9953

Epoch 503: val_loss improved from 28.01368 to 27.99528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.8525 - MinusLogProbMetric: 27.8525 - val_loss: 27.9953 - val_MinusLogProbMetric: 27.9953 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 504/1000
2023-10-26 23:11:09.122 
Epoch 504/1000 
	 loss: 27.9397, MinusLogProbMetric: 27.9397, val_loss: 28.1122, val_MinusLogProbMetric: 28.1122

Epoch 504: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9397 - MinusLogProbMetric: 27.9397 - val_loss: 28.1122 - val_MinusLogProbMetric: 28.1122 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 505/1000
2023-10-26 23:11:44.006 
Epoch 505/1000 
	 loss: 27.9216, MinusLogProbMetric: 27.9216, val_loss: 28.0423, val_MinusLogProbMetric: 28.0423

Epoch 505: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9216 - MinusLogProbMetric: 27.9216 - val_loss: 28.0423 - val_MinusLogProbMetric: 28.0423 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 506/1000
2023-10-26 23:12:18.948 
Epoch 506/1000 
	 loss: 27.9095, MinusLogProbMetric: 27.9095, val_loss: 27.9964, val_MinusLogProbMetric: 27.9964

Epoch 506: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9095 - MinusLogProbMetric: 27.9095 - val_loss: 27.9964 - val_MinusLogProbMetric: 27.9964 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 507/1000
2023-10-26 23:12:53.841 
Epoch 507/1000 
	 loss: 27.9473, MinusLogProbMetric: 27.9473, val_loss: 28.0260, val_MinusLogProbMetric: 28.0260

Epoch 507: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9473 - MinusLogProbMetric: 27.9473 - val_loss: 28.0260 - val_MinusLogProbMetric: 28.0260 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 508/1000
2023-10-26 23:13:28.877 
Epoch 508/1000 
	 loss: 28.0014, MinusLogProbMetric: 28.0014, val_loss: 28.6553, val_MinusLogProbMetric: 28.6553

Epoch 508: val_loss did not improve from 27.99528
196/196 - 35s - loss: 28.0014 - MinusLogProbMetric: 28.0014 - val_loss: 28.6553 - val_MinusLogProbMetric: 28.6553 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 509/1000
2023-10-26 23:14:04.183 
Epoch 509/1000 
	 loss: 27.9244, MinusLogProbMetric: 27.9244, val_loss: 28.2182, val_MinusLogProbMetric: 28.2182

Epoch 509: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9244 - MinusLogProbMetric: 27.9244 - val_loss: 28.2182 - val_MinusLogProbMetric: 28.2182 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 510/1000
2023-10-26 23:14:39.229 
Epoch 510/1000 
	 loss: 27.8860, MinusLogProbMetric: 27.8860, val_loss: 28.1170, val_MinusLogProbMetric: 28.1170

Epoch 510: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.8860 - MinusLogProbMetric: 27.8860 - val_loss: 28.1170 - val_MinusLogProbMetric: 28.1170 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 511/1000
2023-10-26 23:15:14.324 
Epoch 511/1000 
	 loss: 27.9229, MinusLogProbMetric: 27.9229, val_loss: 28.2029, val_MinusLogProbMetric: 28.2029

Epoch 511: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9229 - MinusLogProbMetric: 27.9229 - val_loss: 28.2029 - val_MinusLogProbMetric: 28.2029 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 512/1000
2023-10-26 23:15:49.529 
Epoch 512/1000 
	 loss: 27.8940, MinusLogProbMetric: 27.8940, val_loss: 28.0355, val_MinusLogProbMetric: 28.0355

Epoch 512: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.8940 - MinusLogProbMetric: 27.8940 - val_loss: 28.0355 - val_MinusLogProbMetric: 28.0355 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 513/1000
2023-10-26 23:16:24.803 
Epoch 513/1000 
	 loss: 27.9505, MinusLogProbMetric: 27.9505, val_loss: 28.0888, val_MinusLogProbMetric: 28.0888

Epoch 513: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9505 - MinusLogProbMetric: 27.9505 - val_loss: 28.0888 - val_MinusLogProbMetric: 28.0888 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 514/1000
2023-10-26 23:16:59.828 
Epoch 514/1000 
	 loss: 27.8962, MinusLogProbMetric: 27.8962, val_loss: 28.0055, val_MinusLogProbMetric: 28.0055

Epoch 514: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.8962 - MinusLogProbMetric: 27.8962 - val_loss: 28.0055 - val_MinusLogProbMetric: 28.0055 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 515/1000
2023-10-26 23:17:35.144 
Epoch 515/1000 
	 loss: 27.9293, MinusLogProbMetric: 27.9293, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 515: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9293 - MinusLogProbMetric: 27.9293 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 516/1000
2023-10-26 23:18:10.285 
Epoch 516/1000 
	 loss: 27.9155, MinusLogProbMetric: 27.9155, val_loss: 28.0704, val_MinusLogProbMetric: 28.0704

Epoch 516: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9155 - MinusLogProbMetric: 27.9155 - val_loss: 28.0704 - val_MinusLogProbMetric: 28.0704 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 517/1000
2023-10-26 23:18:45.342 
Epoch 517/1000 
	 loss: 27.8946, MinusLogProbMetric: 27.8946, val_loss: 28.1018, val_MinusLogProbMetric: 28.1018

Epoch 517: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.8946 - MinusLogProbMetric: 27.8946 - val_loss: 28.1018 - val_MinusLogProbMetric: 28.1018 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 518/1000
2023-10-26 23:19:20.658 
Epoch 518/1000 
	 loss: 27.9110, MinusLogProbMetric: 27.9110, val_loss: 28.3496, val_MinusLogProbMetric: 28.3496

Epoch 518: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9110 - MinusLogProbMetric: 27.9110 - val_loss: 28.3496 - val_MinusLogProbMetric: 28.3496 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 519/1000
2023-10-26 23:19:55.906 
Epoch 519/1000 
	 loss: 27.9232, MinusLogProbMetric: 27.9232, val_loss: 28.0560, val_MinusLogProbMetric: 28.0560

Epoch 519: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9232 - MinusLogProbMetric: 27.9232 - val_loss: 28.0560 - val_MinusLogProbMetric: 28.0560 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 520/1000
2023-10-26 23:20:31.224 
Epoch 520/1000 
	 loss: 27.9000, MinusLogProbMetric: 27.9000, val_loss: 28.1527, val_MinusLogProbMetric: 28.1527

Epoch 520: val_loss did not improve from 27.99528
196/196 - 35s - loss: 27.9000 - MinusLogProbMetric: 27.9000 - val_loss: 28.1527 - val_MinusLogProbMetric: 28.1527 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 521/1000
2023-10-26 23:21:06.399 
Epoch 521/1000 
	 loss: 27.9142, MinusLogProbMetric: 27.9142, val_loss: 27.9807, val_MinusLogProbMetric: 27.9807

Epoch 521: val_loss improved from 27.99528 to 27.98072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.9142 - MinusLogProbMetric: 27.9142 - val_loss: 27.9807 - val_MinusLogProbMetric: 27.9807 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 522/1000
2023-10-26 23:21:42.194 
Epoch 522/1000 
	 loss: 27.9012, MinusLogProbMetric: 27.9012, val_loss: 28.0449, val_MinusLogProbMetric: 28.0449

Epoch 522: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.9012 - MinusLogProbMetric: 27.9012 - val_loss: 28.0449 - val_MinusLogProbMetric: 28.0449 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 523/1000
2023-10-26 23:22:17.428 
Epoch 523/1000 
	 loss: 27.9271, MinusLogProbMetric: 27.9271, val_loss: 28.3781, val_MinusLogProbMetric: 28.3781

Epoch 523: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.9271 - MinusLogProbMetric: 27.9271 - val_loss: 28.3781 - val_MinusLogProbMetric: 28.3781 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 524/1000
2023-10-26 23:22:52.774 
Epoch 524/1000 
	 loss: 27.9237, MinusLogProbMetric: 27.9237, val_loss: 28.0969, val_MinusLogProbMetric: 28.0969

Epoch 524: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.9237 - MinusLogProbMetric: 27.9237 - val_loss: 28.0969 - val_MinusLogProbMetric: 28.0969 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 525/1000
2023-10-26 23:23:27.791 
Epoch 525/1000 
	 loss: 27.9935, MinusLogProbMetric: 27.9935, val_loss: 28.0897, val_MinusLogProbMetric: 28.0897

Epoch 525: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.9935 - MinusLogProbMetric: 27.9935 - val_loss: 28.0897 - val_MinusLogProbMetric: 28.0897 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 526/1000
2023-10-26 23:24:03.066 
Epoch 526/1000 
	 loss: 27.8698, MinusLogProbMetric: 27.8698, val_loss: 28.1242, val_MinusLogProbMetric: 28.1242

Epoch 526: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.8698 - MinusLogProbMetric: 27.8698 - val_loss: 28.1242 - val_MinusLogProbMetric: 28.1242 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 527/1000
2023-10-26 23:24:38.382 
Epoch 527/1000 
	 loss: 27.9523, MinusLogProbMetric: 27.9523, val_loss: 28.2057, val_MinusLogProbMetric: 28.2057

Epoch 527: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.9523 - MinusLogProbMetric: 27.9523 - val_loss: 28.2057 - val_MinusLogProbMetric: 28.2057 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 528/1000
2023-10-26 23:25:13.919 
Epoch 528/1000 
	 loss: 28.0347, MinusLogProbMetric: 28.0347, val_loss: 28.0139, val_MinusLogProbMetric: 28.0139

Epoch 528: val_loss did not improve from 27.98072
196/196 - 36s - loss: 28.0347 - MinusLogProbMetric: 28.0347 - val_loss: 28.0139 - val_MinusLogProbMetric: 28.0139 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 529/1000
2023-10-26 23:25:48.878 
Epoch 529/1000 
	 loss: 27.8444, MinusLogProbMetric: 27.8444, val_loss: 28.1051, val_MinusLogProbMetric: 28.1051

Epoch 529: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.8444 - MinusLogProbMetric: 27.8444 - val_loss: 28.1051 - val_MinusLogProbMetric: 28.1051 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 530/1000
2023-10-26 23:26:23.846 
Epoch 530/1000 
	 loss: 27.8948, MinusLogProbMetric: 27.8948, val_loss: 27.9817, val_MinusLogProbMetric: 27.9817

Epoch 530: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.8948 - MinusLogProbMetric: 27.8948 - val_loss: 27.9817 - val_MinusLogProbMetric: 27.9817 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 531/1000
2023-10-26 23:26:59.043 
Epoch 531/1000 
	 loss: 27.9159, MinusLogProbMetric: 27.9159, val_loss: 28.0357, val_MinusLogProbMetric: 28.0357

Epoch 531: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.9159 - MinusLogProbMetric: 27.9159 - val_loss: 28.0357 - val_MinusLogProbMetric: 28.0357 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 532/1000
2023-10-26 23:27:34.195 
Epoch 532/1000 
	 loss: 27.8916, MinusLogProbMetric: 27.8916, val_loss: 28.0117, val_MinusLogProbMetric: 28.0117

Epoch 532: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.8916 - MinusLogProbMetric: 27.8916 - val_loss: 28.0117 - val_MinusLogProbMetric: 28.0117 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 533/1000
2023-10-26 23:28:09.729 
Epoch 533/1000 
	 loss: 27.9584, MinusLogProbMetric: 27.9584, val_loss: 28.0174, val_MinusLogProbMetric: 28.0174

Epoch 533: val_loss did not improve from 27.98072
196/196 - 36s - loss: 27.9584 - MinusLogProbMetric: 27.9584 - val_loss: 28.0174 - val_MinusLogProbMetric: 28.0174 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 534/1000
2023-10-26 23:28:44.891 
Epoch 534/1000 
	 loss: 27.8756, MinusLogProbMetric: 27.8756, val_loss: 28.0761, val_MinusLogProbMetric: 28.0761

Epoch 534: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.8756 - MinusLogProbMetric: 27.8756 - val_loss: 28.0761 - val_MinusLogProbMetric: 28.0761 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 535/1000
2023-10-26 23:29:19.851 
Epoch 535/1000 
	 loss: 27.9344, MinusLogProbMetric: 27.9344, val_loss: 27.9893, val_MinusLogProbMetric: 27.9893

Epoch 535: val_loss did not improve from 27.98072
196/196 - 35s - loss: 27.9344 - MinusLogProbMetric: 27.9344 - val_loss: 27.9893 - val_MinusLogProbMetric: 27.9893 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 536/1000
2023-10-26 23:29:54.849 
Epoch 536/1000 
	 loss: 27.8944, MinusLogProbMetric: 27.8944, val_loss: 27.9687, val_MinusLogProbMetric: 27.9687

Epoch 536: val_loss improved from 27.98072 to 27.96872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.8944 - MinusLogProbMetric: 27.8944 - val_loss: 27.9687 - val_MinusLogProbMetric: 27.9687 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 537/1000
2023-10-26 23:30:30.000 
Epoch 537/1000 
	 loss: 27.8705, MinusLogProbMetric: 27.8705, val_loss: 28.3006, val_MinusLogProbMetric: 28.3006

Epoch 537: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.8705 - MinusLogProbMetric: 27.8705 - val_loss: 28.3006 - val_MinusLogProbMetric: 28.3006 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 538/1000
2023-10-26 23:31:04.784 
Epoch 538/1000 
	 loss: 27.9084, MinusLogProbMetric: 27.9084, val_loss: 28.0603, val_MinusLogProbMetric: 28.0603

Epoch 538: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9084 - MinusLogProbMetric: 27.9084 - val_loss: 28.0603 - val_MinusLogProbMetric: 28.0603 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 539/1000
2023-10-26 23:31:39.559 
Epoch 539/1000 
	 loss: 27.9143, MinusLogProbMetric: 27.9143, val_loss: 28.5146, val_MinusLogProbMetric: 28.5146

Epoch 539: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9143 - MinusLogProbMetric: 27.9143 - val_loss: 28.5146 - val_MinusLogProbMetric: 28.5146 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 540/1000
2023-10-26 23:32:14.356 
Epoch 540/1000 
	 loss: 27.8768, MinusLogProbMetric: 27.8768, val_loss: 28.1438, val_MinusLogProbMetric: 28.1438

Epoch 540: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.8768 - MinusLogProbMetric: 27.8768 - val_loss: 28.1438 - val_MinusLogProbMetric: 28.1438 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 541/1000
2023-10-26 23:32:49.292 
Epoch 541/1000 
	 loss: 27.9510, MinusLogProbMetric: 27.9510, val_loss: 28.0748, val_MinusLogProbMetric: 28.0748

Epoch 541: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9510 - MinusLogProbMetric: 27.9510 - val_loss: 28.0748 - val_MinusLogProbMetric: 28.0748 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 542/1000
2023-10-26 23:33:24.640 
Epoch 542/1000 
	 loss: 27.9024, MinusLogProbMetric: 27.9024, val_loss: 28.3888, val_MinusLogProbMetric: 28.3888

Epoch 542: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9024 - MinusLogProbMetric: 27.9024 - val_loss: 28.3888 - val_MinusLogProbMetric: 28.3888 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 543/1000
2023-10-26 23:33:59.164 
Epoch 543/1000 
	 loss: 27.9163, MinusLogProbMetric: 27.9163, val_loss: 28.1399, val_MinusLogProbMetric: 28.1399

Epoch 543: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9163 - MinusLogProbMetric: 27.9163 - val_loss: 28.1399 - val_MinusLogProbMetric: 28.1399 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 544/1000
2023-10-26 23:34:34.409 
Epoch 544/1000 
	 loss: 27.9125, MinusLogProbMetric: 27.9125, val_loss: 27.9694, val_MinusLogProbMetric: 27.9694

Epoch 544: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9125 - MinusLogProbMetric: 27.9125 - val_loss: 27.9694 - val_MinusLogProbMetric: 27.9694 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 545/1000
2023-10-26 23:35:09.260 
Epoch 545/1000 
	 loss: 27.8679, MinusLogProbMetric: 27.8679, val_loss: 28.2267, val_MinusLogProbMetric: 28.2267

Epoch 545: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.8679 - MinusLogProbMetric: 27.8679 - val_loss: 28.2267 - val_MinusLogProbMetric: 28.2267 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 546/1000
2023-10-26 23:35:44.347 
Epoch 546/1000 
	 loss: 27.9500, MinusLogProbMetric: 27.9500, val_loss: 28.2973, val_MinusLogProbMetric: 28.2973

Epoch 546: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9500 - MinusLogProbMetric: 27.9500 - val_loss: 28.2973 - val_MinusLogProbMetric: 28.2973 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 547/1000
2023-10-26 23:36:19.407 
Epoch 547/1000 
	 loss: 27.9052, MinusLogProbMetric: 27.9052, val_loss: 28.1056, val_MinusLogProbMetric: 28.1056

Epoch 547: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9052 - MinusLogProbMetric: 27.9052 - val_loss: 28.1056 - val_MinusLogProbMetric: 28.1056 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 548/1000
2023-10-26 23:36:54.345 
Epoch 548/1000 
	 loss: 27.9110, MinusLogProbMetric: 27.9110, val_loss: 28.1713, val_MinusLogProbMetric: 28.1713

Epoch 548: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9110 - MinusLogProbMetric: 27.9110 - val_loss: 28.1713 - val_MinusLogProbMetric: 28.1713 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 549/1000
2023-10-26 23:37:29.674 
Epoch 549/1000 
	 loss: 27.9055, MinusLogProbMetric: 27.9055, val_loss: 28.2189, val_MinusLogProbMetric: 28.2189

Epoch 549: val_loss did not improve from 27.96872
196/196 - 35s - loss: 27.9055 - MinusLogProbMetric: 27.9055 - val_loss: 28.2189 - val_MinusLogProbMetric: 28.2189 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 550/1000
2023-10-26 23:38:04.825 
Epoch 550/1000 
	 loss: 27.8775, MinusLogProbMetric: 27.8775, val_loss: 27.9543, val_MinusLogProbMetric: 27.9543

Epoch 550: val_loss improved from 27.96872 to 27.95427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.8775 - MinusLogProbMetric: 27.8775 - val_loss: 27.9543 - val_MinusLogProbMetric: 27.9543 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 551/1000
2023-10-26 23:38:40.332 
Epoch 551/1000 
	 loss: 27.9615, MinusLogProbMetric: 27.9615, val_loss: 28.0018, val_MinusLogProbMetric: 28.0018

Epoch 551: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9615 - MinusLogProbMetric: 27.9615 - val_loss: 28.0018 - val_MinusLogProbMetric: 28.0018 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 552/1000
2023-10-26 23:39:15.570 
Epoch 552/1000 
	 loss: 27.9331, MinusLogProbMetric: 27.9331, val_loss: 28.0929, val_MinusLogProbMetric: 28.0929

Epoch 552: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9331 - MinusLogProbMetric: 27.9331 - val_loss: 28.0929 - val_MinusLogProbMetric: 28.0929 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 553/1000
2023-10-26 23:39:50.848 
Epoch 553/1000 
	 loss: 27.8673, MinusLogProbMetric: 27.8673, val_loss: 28.2114, val_MinusLogProbMetric: 28.2114

Epoch 553: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8673 - MinusLogProbMetric: 27.8673 - val_loss: 28.2114 - val_MinusLogProbMetric: 28.2114 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 554/1000
2023-10-26 23:40:25.627 
Epoch 554/1000 
	 loss: 27.8937, MinusLogProbMetric: 27.8937, val_loss: 28.0500, val_MinusLogProbMetric: 28.0500

Epoch 554: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8937 - MinusLogProbMetric: 27.8937 - val_loss: 28.0500 - val_MinusLogProbMetric: 28.0500 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 555/1000
2023-10-26 23:41:00.968 
Epoch 555/1000 
	 loss: 27.9382, MinusLogProbMetric: 27.9382, val_loss: 28.1403, val_MinusLogProbMetric: 28.1403

Epoch 555: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9382 - MinusLogProbMetric: 27.9382 - val_loss: 28.1403 - val_MinusLogProbMetric: 28.1403 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 556/1000
2023-10-26 23:41:36.197 
Epoch 556/1000 
	 loss: 27.9076, MinusLogProbMetric: 27.9076, val_loss: 28.2888, val_MinusLogProbMetric: 28.2888

Epoch 556: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9076 - MinusLogProbMetric: 27.9076 - val_loss: 28.2888 - val_MinusLogProbMetric: 28.2888 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 557/1000
2023-10-26 23:42:11.399 
Epoch 557/1000 
	 loss: 27.8710, MinusLogProbMetric: 27.8710, val_loss: 28.1950, val_MinusLogProbMetric: 28.1950

Epoch 557: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8710 - MinusLogProbMetric: 27.8710 - val_loss: 28.1950 - val_MinusLogProbMetric: 28.1950 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 558/1000
2023-10-26 23:42:46.403 
Epoch 558/1000 
	 loss: 27.9211, MinusLogProbMetric: 27.9211, val_loss: 27.9920, val_MinusLogProbMetric: 27.9920

Epoch 558: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9211 - MinusLogProbMetric: 27.9211 - val_loss: 27.9920 - val_MinusLogProbMetric: 27.9920 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 559/1000
2023-10-26 23:43:21.596 
Epoch 559/1000 
	 loss: 27.8894, MinusLogProbMetric: 27.8894, val_loss: 28.2911, val_MinusLogProbMetric: 28.2911

Epoch 559: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8894 - MinusLogProbMetric: 27.8894 - val_loss: 28.2911 - val_MinusLogProbMetric: 28.2911 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 560/1000
2023-10-26 23:43:56.555 
Epoch 560/1000 
	 loss: 27.9312, MinusLogProbMetric: 27.9312, val_loss: 27.9964, val_MinusLogProbMetric: 27.9964

Epoch 560: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9312 - MinusLogProbMetric: 27.9312 - val_loss: 27.9964 - val_MinusLogProbMetric: 27.9964 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 561/1000
2023-10-26 23:44:31.831 
Epoch 561/1000 
	 loss: 27.9344, MinusLogProbMetric: 27.9344, val_loss: 28.0731, val_MinusLogProbMetric: 28.0731

Epoch 561: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9344 - MinusLogProbMetric: 27.9344 - val_loss: 28.0731 - val_MinusLogProbMetric: 28.0731 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 562/1000
2023-10-26 23:45:07.168 
Epoch 562/1000 
	 loss: 27.8991, MinusLogProbMetric: 27.8991, val_loss: 28.1302, val_MinusLogProbMetric: 28.1302

Epoch 562: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8991 - MinusLogProbMetric: 27.8991 - val_loss: 28.1302 - val_MinusLogProbMetric: 28.1302 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 563/1000
2023-10-26 23:45:42.224 
Epoch 563/1000 
	 loss: 27.9091, MinusLogProbMetric: 27.9091, val_loss: 27.9805, val_MinusLogProbMetric: 27.9805

Epoch 563: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9091 - MinusLogProbMetric: 27.9091 - val_loss: 27.9805 - val_MinusLogProbMetric: 27.9805 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 564/1000
2023-10-26 23:46:16.980 
Epoch 564/1000 
	 loss: 27.9192, MinusLogProbMetric: 27.9192, val_loss: 28.1343, val_MinusLogProbMetric: 28.1343

Epoch 564: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9192 - MinusLogProbMetric: 27.9192 - val_loss: 28.1343 - val_MinusLogProbMetric: 28.1343 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 565/1000
2023-10-26 23:46:51.994 
Epoch 565/1000 
	 loss: 27.9295, MinusLogProbMetric: 27.9295, val_loss: 28.2105, val_MinusLogProbMetric: 28.2105

Epoch 565: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9295 - MinusLogProbMetric: 27.9295 - val_loss: 28.2105 - val_MinusLogProbMetric: 28.2105 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 566/1000
2023-10-26 23:47:27.068 
Epoch 566/1000 
	 loss: 27.8933, MinusLogProbMetric: 27.8933, val_loss: 28.1569, val_MinusLogProbMetric: 28.1569

Epoch 566: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8933 - MinusLogProbMetric: 27.8933 - val_loss: 28.1569 - val_MinusLogProbMetric: 28.1569 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 567/1000
2023-10-26 23:48:02.197 
Epoch 567/1000 
	 loss: 27.8984, MinusLogProbMetric: 27.8984, val_loss: 28.2313, val_MinusLogProbMetric: 28.2313

Epoch 567: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8984 - MinusLogProbMetric: 27.8984 - val_loss: 28.2313 - val_MinusLogProbMetric: 28.2313 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 568/1000
2023-10-26 23:48:37.269 
Epoch 568/1000 
	 loss: 27.9084, MinusLogProbMetric: 27.9084, val_loss: 28.0890, val_MinusLogProbMetric: 28.0890

Epoch 568: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9084 - MinusLogProbMetric: 27.9084 - val_loss: 28.0890 - val_MinusLogProbMetric: 28.0890 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 569/1000
2023-10-26 23:49:12.489 
Epoch 569/1000 
	 loss: 27.9354, MinusLogProbMetric: 27.9354, val_loss: 28.0588, val_MinusLogProbMetric: 28.0588

Epoch 569: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.9354 - MinusLogProbMetric: 27.9354 - val_loss: 28.0588 - val_MinusLogProbMetric: 28.0588 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 570/1000
2023-10-26 23:49:47.705 
Epoch 570/1000 
	 loss: 27.8762, MinusLogProbMetric: 27.8762, val_loss: 28.0392, val_MinusLogProbMetric: 28.0392

Epoch 570: val_loss did not improve from 27.95427
196/196 - 35s - loss: 27.8762 - MinusLogProbMetric: 27.8762 - val_loss: 28.0392 - val_MinusLogProbMetric: 28.0392 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 571/1000
2023-10-26 23:50:22.634 
Epoch 571/1000 
	 loss: 27.8910, MinusLogProbMetric: 27.8910, val_loss: 27.9538, val_MinusLogProbMetric: 27.9538

Epoch 571: val_loss improved from 27.95427 to 27.95380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.8910 - MinusLogProbMetric: 27.8910 - val_loss: 27.9538 - val_MinusLogProbMetric: 27.9538 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 572/1000
2023-10-26 23:50:57.879 
Epoch 572/1000 
	 loss: 27.8680, MinusLogProbMetric: 27.8680, val_loss: 28.3392, val_MinusLogProbMetric: 28.3392

Epoch 572: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8680 - MinusLogProbMetric: 27.8680 - val_loss: 28.3392 - val_MinusLogProbMetric: 28.3392 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 573/1000
2023-10-26 23:51:32.803 
Epoch 573/1000 
	 loss: 27.8976, MinusLogProbMetric: 27.8976, val_loss: 28.0552, val_MinusLogProbMetric: 28.0552

Epoch 573: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8976 - MinusLogProbMetric: 27.8976 - val_loss: 28.0552 - val_MinusLogProbMetric: 28.0552 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 574/1000
2023-10-26 23:52:07.345 
Epoch 574/1000 
	 loss: 27.8912, MinusLogProbMetric: 27.8912, val_loss: 28.3985, val_MinusLogProbMetric: 28.3985

Epoch 574: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8912 - MinusLogProbMetric: 27.8912 - val_loss: 28.3985 - val_MinusLogProbMetric: 28.3985 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 575/1000
2023-10-26 23:52:42.406 
Epoch 575/1000 
	 loss: 27.9016, MinusLogProbMetric: 27.9016, val_loss: 28.4197, val_MinusLogProbMetric: 28.4197

Epoch 575: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9016 - MinusLogProbMetric: 27.9016 - val_loss: 28.4197 - val_MinusLogProbMetric: 28.4197 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 576/1000
2023-10-26 23:53:17.569 
Epoch 576/1000 
	 loss: 27.8925, MinusLogProbMetric: 27.8925, val_loss: 28.0477, val_MinusLogProbMetric: 28.0477

Epoch 576: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8925 - MinusLogProbMetric: 27.8925 - val_loss: 28.0477 - val_MinusLogProbMetric: 28.0477 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 577/1000
2023-10-26 23:53:52.521 
Epoch 577/1000 
	 loss: 27.9236, MinusLogProbMetric: 27.9236, val_loss: 28.0295, val_MinusLogProbMetric: 28.0295

Epoch 577: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9236 - MinusLogProbMetric: 27.9236 - val_loss: 28.0295 - val_MinusLogProbMetric: 28.0295 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 578/1000
2023-10-26 23:54:27.604 
Epoch 578/1000 
	 loss: 27.8633, MinusLogProbMetric: 27.8633, val_loss: 28.0209, val_MinusLogProbMetric: 28.0209

Epoch 578: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8633 - MinusLogProbMetric: 27.8633 - val_loss: 28.0209 - val_MinusLogProbMetric: 28.0209 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 579/1000
2023-10-26 23:55:02.532 
Epoch 579/1000 
	 loss: 27.8650, MinusLogProbMetric: 27.8650, val_loss: 29.8657, val_MinusLogProbMetric: 29.8657

Epoch 579: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8650 - MinusLogProbMetric: 27.8650 - val_loss: 29.8657 - val_MinusLogProbMetric: 29.8657 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 580/1000
2023-10-26 23:55:37.477 
Epoch 580/1000 
	 loss: 27.9119, MinusLogProbMetric: 27.9119, val_loss: 28.5165, val_MinusLogProbMetric: 28.5165

Epoch 580: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9119 - MinusLogProbMetric: 27.9119 - val_loss: 28.5165 - val_MinusLogProbMetric: 28.5165 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 581/1000
2023-10-26 23:56:12.415 
Epoch 581/1000 
	 loss: 27.9313, MinusLogProbMetric: 27.9313, val_loss: 28.6210, val_MinusLogProbMetric: 28.6210

Epoch 581: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9313 - MinusLogProbMetric: 27.9313 - val_loss: 28.6210 - val_MinusLogProbMetric: 28.6210 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 582/1000
2023-10-26 23:56:48.094 
Epoch 582/1000 
	 loss: 27.9043, MinusLogProbMetric: 27.9043, val_loss: 28.0573, val_MinusLogProbMetric: 28.0573

Epoch 582: val_loss did not improve from 27.95380
196/196 - 36s - loss: 27.9043 - MinusLogProbMetric: 27.9043 - val_loss: 28.0573 - val_MinusLogProbMetric: 28.0573 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 583/1000
2023-10-26 23:57:23.215 
Epoch 583/1000 
	 loss: 27.9218, MinusLogProbMetric: 27.9218, val_loss: 28.1340, val_MinusLogProbMetric: 28.1340

Epoch 583: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9218 - MinusLogProbMetric: 27.9218 - val_loss: 28.1340 - val_MinusLogProbMetric: 28.1340 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 584/1000
2023-10-26 23:57:58.305 
Epoch 584/1000 
	 loss: 27.9174, MinusLogProbMetric: 27.9174, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 584: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9174 - MinusLogProbMetric: 27.9174 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 585/1000
2023-10-26 23:58:33.582 
Epoch 585/1000 
	 loss: 27.8771, MinusLogProbMetric: 27.8771, val_loss: 27.9983, val_MinusLogProbMetric: 27.9983

Epoch 585: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8771 - MinusLogProbMetric: 27.8771 - val_loss: 27.9983 - val_MinusLogProbMetric: 27.9983 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 586/1000
2023-10-26 23:59:08.574 
Epoch 586/1000 
	 loss: 27.8945, MinusLogProbMetric: 27.8945, val_loss: 28.2819, val_MinusLogProbMetric: 28.2819

Epoch 586: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8945 - MinusLogProbMetric: 27.8945 - val_loss: 28.2819 - val_MinusLogProbMetric: 28.2819 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 587/1000
2023-10-26 23:59:44.028 
Epoch 587/1000 
	 loss: 27.8950, MinusLogProbMetric: 27.8950, val_loss: 28.1161, val_MinusLogProbMetric: 28.1161

Epoch 587: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8950 - MinusLogProbMetric: 27.8950 - val_loss: 28.1161 - val_MinusLogProbMetric: 28.1161 - lr: 5.0000e-04 - 35s/epoch - 181ms/step
Epoch 588/1000
2023-10-27 00:00:19.200 
Epoch 588/1000 
	 loss: 27.9137, MinusLogProbMetric: 27.9137, val_loss: 28.0552, val_MinusLogProbMetric: 28.0552

Epoch 588: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9137 - MinusLogProbMetric: 27.9137 - val_loss: 28.0552 - val_MinusLogProbMetric: 28.0552 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 589/1000
2023-10-27 00:00:54.176 
Epoch 589/1000 
	 loss: 27.9003, MinusLogProbMetric: 27.9003, val_loss: 28.0869, val_MinusLogProbMetric: 28.0869

Epoch 589: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9003 - MinusLogProbMetric: 27.9003 - val_loss: 28.0869 - val_MinusLogProbMetric: 28.0869 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 590/1000
2023-10-27 00:01:29.302 
Epoch 590/1000 
	 loss: 27.8490, MinusLogProbMetric: 27.8490, val_loss: 28.0018, val_MinusLogProbMetric: 28.0018

Epoch 590: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8490 - MinusLogProbMetric: 27.8490 - val_loss: 28.0018 - val_MinusLogProbMetric: 28.0018 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 591/1000
2023-10-27 00:02:04.628 
Epoch 591/1000 
	 loss: 27.8446, MinusLogProbMetric: 27.8446, val_loss: 28.1021, val_MinusLogProbMetric: 28.1021

Epoch 591: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8446 - MinusLogProbMetric: 27.8446 - val_loss: 28.1021 - val_MinusLogProbMetric: 28.1021 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 592/1000
2023-10-27 00:02:39.491 
Epoch 592/1000 
	 loss: 27.8931, MinusLogProbMetric: 27.8931, val_loss: 28.0370, val_MinusLogProbMetric: 28.0370

Epoch 592: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8931 - MinusLogProbMetric: 27.8931 - val_loss: 28.0370 - val_MinusLogProbMetric: 28.0370 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 593/1000
2023-10-27 00:03:15.010 
Epoch 593/1000 
	 loss: 27.8799, MinusLogProbMetric: 27.8799, val_loss: 28.4025, val_MinusLogProbMetric: 28.4025

Epoch 593: val_loss did not improve from 27.95380
196/196 - 36s - loss: 27.8799 - MinusLogProbMetric: 27.8799 - val_loss: 28.4025 - val_MinusLogProbMetric: 28.4025 - lr: 5.0000e-04 - 36s/epoch - 181ms/step
Epoch 594/1000
2023-10-27 00:03:50.295 
Epoch 594/1000 
	 loss: 27.8982, MinusLogProbMetric: 27.8982, val_loss: 28.0026, val_MinusLogProbMetric: 28.0026

Epoch 594: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8982 - MinusLogProbMetric: 27.8982 - val_loss: 28.0026 - val_MinusLogProbMetric: 28.0026 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 595/1000
2023-10-27 00:04:25.593 
Epoch 595/1000 
	 loss: 27.9346, MinusLogProbMetric: 27.9346, val_loss: 28.1735, val_MinusLogProbMetric: 28.1735

Epoch 595: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9346 - MinusLogProbMetric: 27.9346 - val_loss: 28.1735 - val_MinusLogProbMetric: 28.1735 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 596/1000
2023-10-27 00:05:00.745 
Epoch 596/1000 
	 loss: 27.9107, MinusLogProbMetric: 27.9107, val_loss: 28.0579, val_MinusLogProbMetric: 28.0579

Epoch 596: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9107 - MinusLogProbMetric: 27.9107 - val_loss: 28.0579 - val_MinusLogProbMetric: 28.0579 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 597/1000
2023-10-27 00:05:35.961 
Epoch 597/1000 
	 loss: 27.9369, MinusLogProbMetric: 27.9369, val_loss: 28.0221, val_MinusLogProbMetric: 28.0221

Epoch 597: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9369 - MinusLogProbMetric: 27.9369 - val_loss: 28.0221 - val_MinusLogProbMetric: 28.0221 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 598/1000
2023-10-27 00:06:11.139 
Epoch 598/1000 
	 loss: 27.8484, MinusLogProbMetric: 27.8484, val_loss: 27.9716, val_MinusLogProbMetric: 27.9716

Epoch 598: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8484 - MinusLogProbMetric: 27.8484 - val_loss: 27.9716 - val_MinusLogProbMetric: 27.9716 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 599/1000
2023-10-27 00:06:46.076 
Epoch 599/1000 
	 loss: 27.9160, MinusLogProbMetric: 27.9160, val_loss: 28.1453, val_MinusLogProbMetric: 28.1453

Epoch 599: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9160 - MinusLogProbMetric: 27.9160 - val_loss: 28.1453 - val_MinusLogProbMetric: 28.1453 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 600/1000
2023-10-27 00:07:21.196 
Epoch 600/1000 
	 loss: 27.8878, MinusLogProbMetric: 27.8878, val_loss: 28.2453, val_MinusLogProbMetric: 28.2453

Epoch 600: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8878 - MinusLogProbMetric: 27.8878 - val_loss: 28.2453 - val_MinusLogProbMetric: 28.2453 - lr: 5.0000e-04 - 35s/epoch - 179ms/step
Epoch 601/1000
2023-10-27 00:07:56.082 
Epoch 601/1000 
	 loss: 27.8647, MinusLogProbMetric: 27.8647, val_loss: 28.1176, val_MinusLogProbMetric: 28.1176

Epoch 601: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8647 - MinusLogProbMetric: 27.8647 - val_loss: 28.1176 - val_MinusLogProbMetric: 28.1176 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 602/1000
2023-10-27 00:08:31.061 
Epoch 602/1000 
	 loss: 27.9379, MinusLogProbMetric: 27.9379, val_loss: 28.1680, val_MinusLogProbMetric: 28.1680

Epoch 602: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9379 - MinusLogProbMetric: 27.9379 - val_loss: 28.1680 - val_MinusLogProbMetric: 28.1680 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 603/1000
2023-10-27 00:09:06.310 
Epoch 603/1000 
	 loss: 27.9072, MinusLogProbMetric: 27.9072, val_loss: 28.0307, val_MinusLogProbMetric: 28.0307

Epoch 603: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9072 - MinusLogProbMetric: 27.9072 - val_loss: 28.0307 - val_MinusLogProbMetric: 28.0307 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 604/1000
2023-10-27 00:09:41.610 
Epoch 604/1000 
	 loss: 27.8914, MinusLogProbMetric: 27.8914, val_loss: 28.0463, val_MinusLogProbMetric: 28.0463

Epoch 604: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8914 - MinusLogProbMetric: 27.8914 - val_loss: 28.0463 - val_MinusLogProbMetric: 28.0463 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 605/1000
2023-10-27 00:10:16.921 
Epoch 605/1000 
	 loss: 27.8453, MinusLogProbMetric: 27.8453, val_loss: 28.2041, val_MinusLogProbMetric: 28.2041

Epoch 605: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8453 - MinusLogProbMetric: 27.8453 - val_loss: 28.2041 - val_MinusLogProbMetric: 28.2041 - lr: 5.0000e-04 - 35s/epoch - 180ms/step
Epoch 606/1000
2023-10-27 00:10:51.825 
Epoch 606/1000 
	 loss: 27.8869, MinusLogProbMetric: 27.8869, val_loss: 28.1688, val_MinusLogProbMetric: 28.1688

Epoch 606: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8869 - MinusLogProbMetric: 27.8869 - val_loss: 28.1688 - val_MinusLogProbMetric: 28.1688 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 607/1000
2023-10-27 00:11:26.490 
Epoch 607/1000 
	 loss: 27.8853, MinusLogProbMetric: 27.8853, val_loss: 28.1542, val_MinusLogProbMetric: 28.1542

Epoch 607: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8853 - MinusLogProbMetric: 27.8853 - val_loss: 28.1542 - val_MinusLogProbMetric: 28.1542 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 608/1000
2023-10-27 00:12:01.375 
Epoch 608/1000 
	 loss: 27.8821, MinusLogProbMetric: 27.8821, val_loss: 28.0862, val_MinusLogProbMetric: 28.0862

Epoch 608: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8821 - MinusLogProbMetric: 27.8821 - val_loss: 28.0862 - val_MinusLogProbMetric: 28.0862 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 609/1000
2023-10-27 00:12:36.087 
Epoch 609/1000 
	 loss: 27.8650, MinusLogProbMetric: 27.8650, val_loss: 28.0336, val_MinusLogProbMetric: 28.0336

Epoch 609: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8650 - MinusLogProbMetric: 27.8650 - val_loss: 28.0336 - val_MinusLogProbMetric: 28.0336 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 610/1000
2023-10-27 00:13:11.068 
Epoch 610/1000 
	 loss: 27.9141, MinusLogProbMetric: 27.9141, val_loss: 28.0305, val_MinusLogProbMetric: 28.0305

Epoch 610: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9141 - MinusLogProbMetric: 27.9141 - val_loss: 28.0305 - val_MinusLogProbMetric: 28.0305 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 611/1000
2023-10-27 00:13:45.908 
Epoch 611/1000 
	 loss: 27.8792, MinusLogProbMetric: 27.8792, val_loss: 28.1576, val_MinusLogProbMetric: 28.1576

Epoch 611: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8792 - MinusLogProbMetric: 27.8792 - val_loss: 28.1576 - val_MinusLogProbMetric: 28.1576 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 612/1000
2023-10-27 00:14:20.629 
Epoch 612/1000 
	 loss: 27.9265, MinusLogProbMetric: 27.9265, val_loss: 28.3600, val_MinusLogProbMetric: 28.3600

Epoch 612: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9265 - MinusLogProbMetric: 27.9265 - val_loss: 28.3600 - val_MinusLogProbMetric: 28.3600 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 613/1000
2023-10-27 00:14:52.276 
Epoch 613/1000 
	 loss: 27.8697, MinusLogProbMetric: 27.8697, val_loss: 28.0192, val_MinusLogProbMetric: 28.0192

Epoch 613: val_loss did not improve from 27.95380
196/196 - 32s - loss: 27.8697 - MinusLogProbMetric: 27.8697 - val_loss: 28.0192 - val_MinusLogProbMetric: 28.0192 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 614/1000
2023-10-27 00:15:23.937 
Epoch 614/1000 
	 loss: 27.9404, MinusLogProbMetric: 27.9404, val_loss: 28.0736, val_MinusLogProbMetric: 28.0736

Epoch 614: val_loss did not improve from 27.95380
196/196 - 32s - loss: 27.9404 - MinusLogProbMetric: 27.9404 - val_loss: 28.0736 - val_MinusLogProbMetric: 28.0736 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 615/1000
2023-10-27 00:15:58.469 
Epoch 615/1000 
	 loss: 27.8903, MinusLogProbMetric: 27.8903, val_loss: 28.2297, val_MinusLogProbMetric: 28.2297

Epoch 615: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8903 - MinusLogProbMetric: 27.8903 - val_loss: 28.2297 - val_MinusLogProbMetric: 28.2297 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 616/1000
2023-10-27 00:16:32.955 
Epoch 616/1000 
	 loss: 27.8720, MinusLogProbMetric: 27.8720, val_loss: 28.0088, val_MinusLogProbMetric: 28.0088

Epoch 616: val_loss did not improve from 27.95380
196/196 - 34s - loss: 27.8720 - MinusLogProbMetric: 27.8720 - val_loss: 28.0088 - val_MinusLogProbMetric: 28.0088 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 617/1000
2023-10-27 00:17:07.824 
Epoch 617/1000 
	 loss: 27.8690, MinusLogProbMetric: 27.8690, val_loss: 27.9876, val_MinusLogProbMetric: 27.9876

Epoch 617: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8690 - MinusLogProbMetric: 27.8690 - val_loss: 27.9876 - val_MinusLogProbMetric: 27.9876 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 618/1000
2023-10-27 00:17:42.535 
Epoch 618/1000 
	 loss: 27.8919, MinusLogProbMetric: 27.8919, val_loss: 27.9579, val_MinusLogProbMetric: 27.9579

Epoch 618: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8919 - MinusLogProbMetric: 27.8919 - val_loss: 27.9579 - val_MinusLogProbMetric: 27.9579 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 619/1000
2023-10-27 00:18:17.488 
Epoch 619/1000 
	 loss: 27.9726, MinusLogProbMetric: 27.9726, val_loss: 28.3136, val_MinusLogProbMetric: 28.3136

Epoch 619: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.9726 - MinusLogProbMetric: 27.9726 - val_loss: 28.3136 - val_MinusLogProbMetric: 28.3136 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 620/1000
2023-10-27 00:18:52.078 
Epoch 620/1000 
	 loss: 27.8930, MinusLogProbMetric: 27.8930, val_loss: 28.4203, val_MinusLogProbMetric: 28.4203

Epoch 620: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8930 - MinusLogProbMetric: 27.8930 - val_loss: 28.4203 - val_MinusLogProbMetric: 28.4203 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 621/1000
2023-10-27 00:19:26.647 
Epoch 621/1000 
	 loss: 27.8486, MinusLogProbMetric: 27.8486, val_loss: 28.0258, val_MinusLogProbMetric: 28.0258

Epoch 621: val_loss did not improve from 27.95380
196/196 - 35s - loss: 27.8486 - MinusLogProbMetric: 27.8486 - val_loss: 28.0258 - val_MinusLogProbMetric: 28.0258 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 622/1000
2023-10-27 00:20:01.372 
Epoch 622/1000 
	 loss: 27.6655, MinusLogProbMetric: 27.6655, val_loss: 27.8664, val_MinusLogProbMetric: 27.8664

Epoch 622: val_loss improved from 27.95380 to 27.86644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.6655 - MinusLogProbMetric: 27.6655 - val_loss: 27.8664 - val_MinusLogProbMetric: 27.8664 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 623/1000
2023-10-27 00:20:36.480 
Epoch 623/1000 
	 loss: 27.6706, MinusLogProbMetric: 27.6706, val_loss: 27.8628, val_MinusLogProbMetric: 27.8628

Epoch 623: val_loss improved from 27.86644 to 27.86281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.6706 - MinusLogProbMetric: 27.6706 - val_loss: 27.8628 - val_MinusLogProbMetric: 27.8628 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 624/1000
2023-10-27 00:21:11.826 
Epoch 624/1000 
	 loss: 27.6609, MinusLogProbMetric: 27.6609, val_loss: 27.8775, val_MinusLogProbMetric: 27.8775

Epoch 624: val_loss did not improve from 27.86281
196/196 - 35s - loss: 27.6609 - MinusLogProbMetric: 27.6609 - val_loss: 27.8775 - val_MinusLogProbMetric: 27.8775 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 625/1000
2023-10-27 00:21:46.868 
Epoch 625/1000 
	 loss: 27.6590, MinusLogProbMetric: 27.6590, val_loss: 28.1092, val_MinusLogProbMetric: 28.1092

Epoch 625: val_loss did not improve from 27.86281
196/196 - 35s - loss: 27.6590 - MinusLogProbMetric: 27.6590 - val_loss: 28.1092 - val_MinusLogProbMetric: 28.1092 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 626/1000
2023-10-27 00:22:21.703 
Epoch 626/1000 
	 loss: 27.6793, MinusLogProbMetric: 27.6793, val_loss: 27.8766, val_MinusLogProbMetric: 27.8766

Epoch 626: val_loss did not improve from 27.86281
196/196 - 35s - loss: 27.6793 - MinusLogProbMetric: 27.6793 - val_loss: 27.8766 - val_MinusLogProbMetric: 27.8766 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 627/1000
2023-10-27 00:22:56.643 
Epoch 627/1000 
	 loss: 27.6450, MinusLogProbMetric: 27.6450, val_loss: 27.9021, val_MinusLogProbMetric: 27.9021

Epoch 627: val_loss did not improve from 27.86281
196/196 - 35s - loss: 27.6450 - MinusLogProbMetric: 27.6450 - val_loss: 27.9021 - val_MinusLogProbMetric: 27.9021 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 628/1000
2023-10-27 00:23:31.191 
Epoch 628/1000 
	 loss: 27.6848, MinusLogProbMetric: 27.6848, val_loss: 27.8332, val_MinusLogProbMetric: 27.8332

Epoch 628: val_loss improved from 27.86281 to 27.83315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.6848 - MinusLogProbMetric: 27.6848 - val_loss: 27.8332 - val_MinusLogProbMetric: 27.8332 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 629/1000
2023-10-27 00:24:06.507 
Epoch 629/1000 
	 loss: 27.6557, MinusLogProbMetric: 27.6557, val_loss: 27.8505, val_MinusLogProbMetric: 27.8505

Epoch 629: val_loss did not improve from 27.83315
196/196 - 35s - loss: 27.6557 - MinusLogProbMetric: 27.6557 - val_loss: 27.8505 - val_MinusLogProbMetric: 27.8505 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 630/1000
2023-10-27 00:24:41.311 
Epoch 630/1000 
	 loss: 27.6735, MinusLogProbMetric: 27.6735, val_loss: 27.8026, val_MinusLogProbMetric: 27.8026

Epoch 630: val_loss improved from 27.83315 to 27.80256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.6735 - MinusLogProbMetric: 27.6735 - val_loss: 27.8026 - val_MinusLogProbMetric: 27.8026 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 631/1000
2023-10-27 00:25:16.525 
Epoch 631/1000 
	 loss: 27.6518, MinusLogProbMetric: 27.6518, val_loss: 27.9944, val_MinusLogProbMetric: 27.9944

Epoch 631: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6518 - MinusLogProbMetric: 27.6518 - val_loss: 27.9944 - val_MinusLogProbMetric: 27.9944 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 632/1000
2023-10-27 00:25:51.387 
Epoch 632/1000 
	 loss: 27.6631, MinusLogProbMetric: 27.6631, val_loss: 28.3179, val_MinusLogProbMetric: 28.3179

Epoch 632: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6631 - MinusLogProbMetric: 27.6631 - val_loss: 28.3179 - val_MinusLogProbMetric: 28.3179 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 633/1000
2023-10-27 00:26:25.798 
Epoch 633/1000 
	 loss: 27.6797, MinusLogProbMetric: 27.6797, val_loss: 27.8910, val_MinusLogProbMetric: 27.8910

Epoch 633: val_loss did not improve from 27.80256
196/196 - 34s - loss: 27.6797 - MinusLogProbMetric: 27.6797 - val_loss: 27.8910 - val_MinusLogProbMetric: 27.8910 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 634/1000
2023-10-27 00:27:00.457 
Epoch 634/1000 
	 loss: 27.6993, MinusLogProbMetric: 27.6993, val_loss: 27.9639, val_MinusLogProbMetric: 27.9639

Epoch 634: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6993 - MinusLogProbMetric: 27.6993 - val_loss: 27.9639 - val_MinusLogProbMetric: 27.9639 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 635/1000
2023-10-27 00:27:35.060 
Epoch 635/1000 
	 loss: 27.6773, MinusLogProbMetric: 27.6773, val_loss: 27.9151, val_MinusLogProbMetric: 27.9151

Epoch 635: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6773 - MinusLogProbMetric: 27.6773 - val_loss: 27.9151 - val_MinusLogProbMetric: 27.9151 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 636/1000
2023-10-27 00:28:09.698 
Epoch 636/1000 
	 loss: 27.6577, MinusLogProbMetric: 27.6577, val_loss: 27.8393, val_MinusLogProbMetric: 27.8393

Epoch 636: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6577 - MinusLogProbMetric: 27.6577 - val_loss: 27.8393 - val_MinusLogProbMetric: 27.8393 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 637/1000
2023-10-27 00:28:44.334 
Epoch 637/1000 
	 loss: 27.6854, MinusLogProbMetric: 27.6854, val_loss: 27.9535, val_MinusLogProbMetric: 27.9535

Epoch 637: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6854 - MinusLogProbMetric: 27.6854 - val_loss: 27.9535 - val_MinusLogProbMetric: 27.9535 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 638/1000
2023-10-27 00:29:19.062 
Epoch 638/1000 
	 loss: 27.6672, MinusLogProbMetric: 27.6672, val_loss: 27.8268, val_MinusLogProbMetric: 27.8268

Epoch 638: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6672 - MinusLogProbMetric: 27.6672 - val_loss: 27.8268 - val_MinusLogProbMetric: 27.8268 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 639/1000
2023-10-27 00:29:53.786 
Epoch 639/1000 
	 loss: 27.6759, MinusLogProbMetric: 27.6759, val_loss: 27.8254, val_MinusLogProbMetric: 27.8254

Epoch 639: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6759 - MinusLogProbMetric: 27.6759 - val_loss: 27.8254 - val_MinusLogProbMetric: 27.8254 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 640/1000
2023-10-27 00:30:28.264 
Epoch 640/1000 
	 loss: 27.6879, MinusLogProbMetric: 27.6879, val_loss: 27.9659, val_MinusLogProbMetric: 27.9659

Epoch 640: val_loss did not improve from 27.80256
196/196 - 34s - loss: 27.6879 - MinusLogProbMetric: 27.6879 - val_loss: 27.9659 - val_MinusLogProbMetric: 27.9659 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 641/1000
2023-10-27 00:31:02.659 
Epoch 641/1000 
	 loss: 27.6876, MinusLogProbMetric: 27.6876, val_loss: 27.8634, val_MinusLogProbMetric: 27.8634

Epoch 641: val_loss did not improve from 27.80256
196/196 - 34s - loss: 27.6876 - MinusLogProbMetric: 27.6876 - val_loss: 27.8634 - val_MinusLogProbMetric: 27.8634 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 642/1000
2023-10-27 00:31:37.456 
Epoch 642/1000 
	 loss: 27.6743, MinusLogProbMetric: 27.6743, val_loss: 27.8378, val_MinusLogProbMetric: 27.8378

Epoch 642: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6743 - MinusLogProbMetric: 27.6743 - val_loss: 27.8378 - val_MinusLogProbMetric: 27.8378 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 643/1000
2023-10-27 00:32:11.661 
Epoch 643/1000 
	 loss: 27.6716, MinusLogProbMetric: 27.6716, val_loss: 27.8916, val_MinusLogProbMetric: 27.8916

Epoch 643: val_loss did not improve from 27.80256
196/196 - 34s - loss: 27.6716 - MinusLogProbMetric: 27.6716 - val_loss: 27.8916 - val_MinusLogProbMetric: 27.8916 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 644/1000
2023-10-27 00:32:46.350 
Epoch 644/1000 
	 loss: 27.6852, MinusLogProbMetric: 27.6852, val_loss: 27.8904, val_MinusLogProbMetric: 27.8904

Epoch 644: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6852 - MinusLogProbMetric: 27.6852 - val_loss: 27.8904 - val_MinusLogProbMetric: 27.8904 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 645/1000
2023-10-27 00:33:21.020 
Epoch 645/1000 
	 loss: 27.6766, MinusLogProbMetric: 27.6766, val_loss: 28.2785, val_MinusLogProbMetric: 28.2785

Epoch 645: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6766 - MinusLogProbMetric: 27.6766 - val_loss: 28.2785 - val_MinusLogProbMetric: 28.2785 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 646/1000
2023-10-27 00:33:55.819 
Epoch 646/1000 
	 loss: 27.6860, MinusLogProbMetric: 27.6860, val_loss: 27.8334, val_MinusLogProbMetric: 27.8334

Epoch 646: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6860 - MinusLogProbMetric: 27.6860 - val_loss: 27.8334 - val_MinusLogProbMetric: 27.8334 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 647/1000
2023-10-27 00:34:30.599 
Epoch 647/1000 
	 loss: 27.6840, MinusLogProbMetric: 27.6840, val_loss: 27.9037, val_MinusLogProbMetric: 27.9037

Epoch 647: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6840 - MinusLogProbMetric: 27.6840 - val_loss: 27.9037 - val_MinusLogProbMetric: 27.9037 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 648/1000
2023-10-27 00:35:05.548 
Epoch 648/1000 
	 loss: 27.6461, MinusLogProbMetric: 27.6461, val_loss: 27.9062, val_MinusLogProbMetric: 27.9062

Epoch 648: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6461 - MinusLogProbMetric: 27.6461 - val_loss: 27.9062 - val_MinusLogProbMetric: 27.9062 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 649/1000
2023-10-27 00:35:40.268 
Epoch 649/1000 
	 loss: 27.6936, MinusLogProbMetric: 27.6936, val_loss: 27.8462, val_MinusLogProbMetric: 27.8462

Epoch 649: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6936 - MinusLogProbMetric: 27.6936 - val_loss: 27.8462 - val_MinusLogProbMetric: 27.8462 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 650/1000
2023-10-27 00:36:15.003 
Epoch 650/1000 
	 loss: 27.6747, MinusLogProbMetric: 27.6747, val_loss: 27.8390, val_MinusLogProbMetric: 27.8390

Epoch 650: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6747 - MinusLogProbMetric: 27.6747 - val_loss: 27.8390 - val_MinusLogProbMetric: 27.8390 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 651/1000
2023-10-27 00:36:49.761 
Epoch 651/1000 
	 loss: 27.6916, MinusLogProbMetric: 27.6916, val_loss: 27.8668, val_MinusLogProbMetric: 27.8668

Epoch 651: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6916 - MinusLogProbMetric: 27.6916 - val_loss: 27.8668 - val_MinusLogProbMetric: 27.8668 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 652/1000
2023-10-27 00:37:24.601 
Epoch 652/1000 
	 loss: 27.6972, MinusLogProbMetric: 27.6972, val_loss: 27.8746, val_MinusLogProbMetric: 27.8746

Epoch 652: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6972 - MinusLogProbMetric: 27.6972 - val_loss: 27.8746 - val_MinusLogProbMetric: 27.8746 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 653/1000
2023-10-27 00:37:59.139 
Epoch 653/1000 
	 loss: 27.6521, MinusLogProbMetric: 27.6521, val_loss: 27.8216, val_MinusLogProbMetric: 27.8216

Epoch 653: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6521 - MinusLogProbMetric: 27.6521 - val_loss: 27.8216 - val_MinusLogProbMetric: 27.8216 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 654/1000
2023-10-27 00:38:33.979 
Epoch 654/1000 
	 loss: 27.7012, MinusLogProbMetric: 27.7012, val_loss: 27.8147, val_MinusLogProbMetric: 27.8147

Epoch 654: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.7012 - MinusLogProbMetric: 27.7012 - val_loss: 27.8147 - val_MinusLogProbMetric: 27.8147 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 655/1000
2023-10-27 00:39:08.598 
Epoch 655/1000 
	 loss: 27.6684, MinusLogProbMetric: 27.6684, val_loss: 27.8523, val_MinusLogProbMetric: 27.8523

Epoch 655: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6684 - MinusLogProbMetric: 27.6684 - val_loss: 27.8523 - val_MinusLogProbMetric: 27.8523 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 656/1000
2023-10-27 00:39:43.630 
Epoch 656/1000 
	 loss: 27.6776, MinusLogProbMetric: 27.6776, val_loss: 27.8606, val_MinusLogProbMetric: 27.8606

Epoch 656: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6776 - MinusLogProbMetric: 27.6776 - val_loss: 27.8606 - val_MinusLogProbMetric: 27.8606 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 657/1000
2023-10-27 00:40:18.403 
Epoch 657/1000 
	 loss: 27.6682, MinusLogProbMetric: 27.6682, val_loss: 27.9086, val_MinusLogProbMetric: 27.9086

Epoch 657: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6682 - MinusLogProbMetric: 27.6682 - val_loss: 27.9086 - val_MinusLogProbMetric: 27.9086 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 658/1000
2023-10-27 00:40:53.419 
Epoch 658/1000 
	 loss: 27.6804, MinusLogProbMetric: 27.6804, val_loss: 27.8789, val_MinusLogProbMetric: 27.8789

Epoch 658: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6804 - MinusLogProbMetric: 27.6804 - val_loss: 27.8789 - val_MinusLogProbMetric: 27.8789 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 659/1000
2023-10-27 00:41:28.064 
Epoch 659/1000 
	 loss: 27.7208, MinusLogProbMetric: 27.7208, val_loss: 27.9947, val_MinusLogProbMetric: 27.9947

Epoch 659: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.7208 - MinusLogProbMetric: 27.7208 - val_loss: 27.9947 - val_MinusLogProbMetric: 27.9947 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 660/1000
2023-10-27 00:42:02.656 
Epoch 660/1000 
	 loss: 27.6534, MinusLogProbMetric: 27.6534, val_loss: 27.8344, val_MinusLogProbMetric: 27.8344

Epoch 660: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6534 - MinusLogProbMetric: 27.6534 - val_loss: 27.8344 - val_MinusLogProbMetric: 27.8344 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 661/1000
2023-10-27 00:42:37.197 
Epoch 661/1000 
	 loss: 27.6534, MinusLogProbMetric: 27.6534, val_loss: 27.8236, val_MinusLogProbMetric: 27.8236

Epoch 661: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6534 - MinusLogProbMetric: 27.6534 - val_loss: 27.8236 - val_MinusLogProbMetric: 27.8236 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 662/1000
2023-10-27 00:43:12.049 
Epoch 662/1000 
	 loss: 27.6665, MinusLogProbMetric: 27.6665, val_loss: 28.0364, val_MinusLogProbMetric: 28.0364

Epoch 662: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6665 - MinusLogProbMetric: 27.6665 - val_loss: 28.0364 - val_MinusLogProbMetric: 28.0364 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 663/1000
2023-10-27 00:43:46.842 
Epoch 663/1000 
	 loss: 27.6601, MinusLogProbMetric: 27.6601, val_loss: 27.8619, val_MinusLogProbMetric: 27.8619

Epoch 663: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6601 - MinusLogProbMetric: 27.6601 - val_loss: 27.8619 - val_MinusLogProbMetric: 27.8619 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 664/1000
2023-10-27 00:44:21.721 
Epoch 664/1000 
	 loss: 27.6437, MinusLogProbMetric: 27.6437, val_loss: 27.9520, val_MinusLogProbMetric: 27.9520

Epoch 664: val_loss did not improve from 27.80256
196/196 - 35s - loss: 27.6437 - MinusLogProbMetric: 27.6437 - val_loss: 27.9520 - val_MinusLogProbMetric: 27.9520 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 665/1000
2023-10-27 00:44:56.520 
Epoch 665/1000 
	 loss: 27.6705, MinusLogProbMetric: 27.6705, val_loss: 27.7967, val_MinusLogProbMetric: 27.7967

Epoch 665: val_loss improved from 27.80256 to 27.79667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.6705 - MinusLogProbMetric: 27.6705 - val_loss: 27.7967 - val_MinusLogProbMetric: 27.7967 - lr: 2.5000e-04 - 36s/epoch - 181ms/step
Epoch 666/1000
2023-10-27 00:45:31.789 
Epoch 666/1000 
	 loss: 27.6626, MinusLogProbMetric: 27.6626, val_loss: 27.8794, val_MinusLogProbMetric: 27.8794

Epoch 666: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6626 - MinusLogProbMetric: 27.6626 - val_loss: 27.8794 - val_MinusLogProbMetric: 27.8794 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 667/1000
2023-10-27 00:46:06.745 
Epoch 667/1000 
	 loss: 27.7088, MinusLogProbMetric: 27.7088, val_loss: 28.1198, val_MinusLogProbMetric: 28.1198

Epoch 667: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.7088 - MinusLogProbMetric: 27.7088 - val_loss: 28.1198 - val_MinusLogProbMetric: 28.1198 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 668/1000
2023-10-27 00:46:41.798 
Epoch 668/1000 
	 loss: 27.6685, MinusLogProbMetric: 27.6685, val_loss: 27.8022, val_MinusLogProbMetric: 27.8022

Epoch 668: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6685 - MinusLogProbMetric: 27.6685 - val_loss: 27.8022 - val_MinusLogProbMetric: 27.8022 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 669/1000
2023-10-27 00:47:16.614 
Epoch 669/1000 
	 loss: 27.6870, MinusLogProbMetric: 27.6870, val_loss: 27.9564, val_MinusLogProbMetric: 27.9564

Epoch 669: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6870 - MinusLogProbMetric: 27.6870 - val_loss: 27.9564 - val_MinusLogProbMetric: 27.9564 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 670/1000
2023-10-27 00:47:51.101 
Epoch 670/1000 
	 loss: 27.6700, MinusLogProbMetric: 27.6700, val_loss: 27.8863, val_MinusLogProbMetric: 27.8863

Epoch 670: val_loss did not improve from 27.79667
196/196 - 34s - loss: 27.6700 - MinusLogProbMetric: 27.6700 - val_loss: 27.8863 - val_MinusLogProbMetric: 27.8863 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 671/1000
2023-10-27 00:48:25.760 
Epoch 671/1000 
	 loss: 27.6870, MinusLogProbMetric: 27.6870, val_loss: 27.9229, val_MinusLogProbMetric: 27.9229

Epoch 671: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6870 - MinusLogProbMetric: 27.6870 - val_loss: 27.9229 - val_MinusLogProbMetric: 27.9229 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 672/1000
2023-10-27 00:49:00.369 
Epoch 672/1000 
	 loss: 27.7141, MinusLogProbMetric: 27.7141, val_loss: 27.8476, val_MinusLogProbMetric: 27.8476

Epoch 672: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.7141 - MinusLogProbMetric: 27.7141 - val_loss: 27.8476 - val_MinusLogProbMetric: 27.8476 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 673/1000
2023-10-27 00:49:35.001 
Epoch 673/1000 
	 loss: 27.6595, MinusLogProbMetric: 27.6595, val_loss: 27.8261, val_MinusLogProbMetric: 27.8261

Epoch 673: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6595 - MinusLogProbMetric: 27.6595 - val_loss: 27.8261 - val_MinusLogProbMetric: 27.8261 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 674/1000
2023-10-27 00:50:09.821 
Epoch 674/1000 
	 loss: 27.6360, MinusLogProbMetric: 27.6360, val_loss: 27.8221, val_MinusLogProbMetric: 27.8221

Epoch 674: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6360 - MinusLogProbMetric: 27.6360 - val_loss: 27.8221 - val_MinusLogProbMetric: 27.8221 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 675/1000
2023-10-27 00:50:44.351 
Epoch 675/1000 
	 loss: 27.6730, MinusLogProbMetric: 27.6730, val_loss: 28.0063, val_MinusLogProbMetric: 28.0063

Epoch 675: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6730 - MinusLogProbMetric: 27.6730 - val_loss: 28.0063 - val_MinusLogProbMetric: 28.0063 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 676/1000
2023-10-27 00:51:19.161 
Epoch 676/1000 
	 loss: 27.6665, MinusLogProbMetric: 27.6665, val_loss: 27.9227, val_MinusLogProbMetric: 27.9227

Epoch 676: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6665 - MinusLogProbMetric: 27.6665 - val_loss: 27.9227 - val_MinusLogProbMetric: 27.9227 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 677/1000
2023-10-27 00:51:53.762 
Epoch 677/1000 
	 loss: 27.6663, MinusLogProbMetric: 27.6663, val_loss: 27.8265, val_MinusLogProbMetric: 27.8265

Epoch 677: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6663 - MinusLogProbMetric: 27.6663 - val_loss: 27.8265 - val_MinusLogProbMetric: 27.8265 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 678/1000
2023-10-27 00:52:28.097 
Epoch 678/1000 
	 loss: 27.6622, MinusLogProbMetric: 27.6622, val_loss: 27.8830, val_MinusLogProbMetric: 27.8830

Epoch 678: val_loss did not improve from 27.79667
196/196 - 34s - loss: 27.6622 - MinusLogProbMetric: 27.6622 - val_loss: 27.8830 - val_MinusLogProbMetric: 27.8830 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 679/1000
2023-10-27 00:53:02.609 
Epoch 679/1000 
	 loss: 27.7061, MinusLogProbMetric: 27.7061, val_loss: 27.8892, val_MinusLogProbMetric: 27.8892

Epoch 679: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.7061 - MinusLogProbMetric: 27.7061 - val_loss: 27.8892 - val_MinusLogProbMetric: 27.8892 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 680/1000
2023-10-27 00:53:37.173 
Epoch 680/1000 
	 loss: 27.6392, MinusLogProbMetric: 27.6392, val_loss: 27.8734, val_MinusLogProbMetric: 27.8734

Epoch 680: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6392 - MinusLogProbMetric: 27.6392 - val_loss: 27.8734 - val_MinusLogProbMetric: 27.8734 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 681/1000
2023-10-27 00:54:11.910 
Epoch 681/1000 
	 loss: 27.6717, MinusLogProbMetric: 27.6717, val_loss: 27.8135, val_MinusLogProbMetric: 27.8135

Epoch 681: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6717 - MinusLogProbMetric: 27.6717 - val_loss: 27.8135 - val_MinusLogProbMetric: 27.8135 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 682/1000
2023-10-27 00:54:46.638 
Epoch 682/1000 
	 loss: 27.7076, MinusLogProbMetric: 27.7076, val_loss: 27.8798, val_MinusLogProbMetric: 27.8798

Epoch 682: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.7076 - MinusLogProbMetric: 27.7076 - val_loss: 27.8798 - val_MinusLogProbMetric: 27.8798 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 683/1000
2023-10-27 00:55:21.110 
Epoch 683/1000 
	 loss: 27.6725, MinusLogProbMetric: 27.6725, val_loss: 27.8284, val_MinusLogProbMetric: 27.8284

Epoch 683: val_loss did not improve from 27.79667
196/196 - 34s - loss: 27.6725 - MinusLogProbMetric: 27.6725 - val_loss: 27.8284 - val_MinusLogProbMetric: 27.8284 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 684/1000
2023-10-27 00:55:55.660 
Epoch 684/1000 
	 loss: 27.6836, MinusLogProbMetric: 27.6836, val_loss: 27.8367, val_MinusLogProbMetric: 27.8367

Epoch 684: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6836 - MinusLogProbMetric: 27.6836 - val_loss: 27.8367 - val_MinusLogProbMetric: 27.8367 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 685/1000
2023-10-27 00:56:30.355 
Epoch 685/1000 
	 loss: 27.6640, MinusLogProbMetric: 27.6640, val_loss: 27.8918, val_MinusLogProbMetric: 27.8918

Epoch 685: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6640 - MinusLogProbMetric: 27.6640 - val_loss: 27.8918 - val_MinusLogProbMetric: 27.8918 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 686/1000
2023-10-27 00:57:04.903 
Epoch 686/1000 
	 loss: 27.6660, MinusLogProbMetric: 27.6660, val_loss: 27.8210, val_MinusLogProbMetric: 27.8210

Epoch 686: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6660 - MinusLogProbMetric: 27.6660 - val_loss: 27.8210 - val_MinusLogProbMetric: 27.8210 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 687/1000
2023-10-27 00:57:39.555 
Epoch 687/1000 
	 loss: 27.6559, MinusLogProbMetric: 27.6559, val_loss: 27.8038, val_MinusLogProbMetric: 27.8038

Epoch 687: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6559 - MinusLogProbMetric: 27.6559 - val_loss: 27.8038 - val_MinusLogProbMetric: 27.8038 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 688/1000
2023-10-27 00:58:14.019 
Epoch 688/1000 
	 loss: 27.6847, MinusLogProbMetric: 27.6847, val_loss: 27.8520, val_MinusLogProbMetric: 27.8520

Epoch 688: val_loss did not improve from 27.79667
196/196 - 34s - loss: 27.6847 - MinusLogProbMetric: 27.6847 - val_loss: 27.8520 - val_MinusLogProbMetric: 27.8520 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 689/1000
2023-10-27 00:58:48.865 
Epoch 689/1000 
	 loss: 27.6589, MinusLogProbMetric: 27.6589, val_loss: 28.0098, val_MinusLogProbMetric: 28.0098

Epoch 689: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6589 - MinusLogProbMetric: 27.6589 - val_loss: 28.0098 - val_MinusLogProbMetric: 28.0098 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 690/1000
2023-10-27 00:59:23.893 
Epoch 690/1000 
	 loss: 27.6918, MinusLogProbMetric: 27.6918, val_loss: 27.9785, val_MinusLogProbMetric: 27.9785

Epoch 690: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6918 - MinusLogProbMetric: 27.6918 - val_loss: 27.9785 - val_MinusLogProbMetric: 27.9785 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 691/1000
2023-10-27 00:59:58.630 
Epoch 691/1000 
	 loss: 27.6519, MinusLogProbMetric: 27.6519, val_loss: 27.9522, val_MinusLogProbMetric: 27.9522

Epoch 691: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6519 - MinusLogProbMetric: 27.6519 - val_loss: 27.9522 - val_MinusLogProbMetric: 27.9522 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 692/1000
2023-10-27 01:00:33.333 
Epoch 692/1000 
	 loss: 27.6393, MinusLogProbMetric: 27.6393, val_loss: 27.9296, val_MinusLogProbMetric: 27.9296

Epoch 692: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6393 - MinusLogProbMetric: 27.6393 - val_loss: 27.9296 - val_MinusLogProbMetric: 27.9296 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 693/1000
2023-10-27 01:01:08.047 
Epoch 693/1000 
	 loss: 27.6981, MinusLogProbMetric: 27.6981, val_loss: 27.8904, val_MinusLogProbMetric: 27.8904

Epoch 693: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6981 - MinusLogProbMetric: 27.6981 - val_loss: 27.8904 - val_MinusLogProbMetric: 27.8904 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 694/1000
2023-10-27 01:01:42.604 
Epoch 694/1000 
	 loss: 27.6715, MinusLogProbMetric: 27.6715, val_loss: 27.8634, val_MinusLogProbMetric: 27.8634

Epoch 694: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6715 - MinusLogProbMetric: 27.6715 - val_loss: 27.8634 - val_MinusLogProbMetric: 27.8634 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 695/1000
2023-10-27 01:02:17.529 
Epoch 695/1000 
	 loss: 27.6443, MinusLogProbMetric: 27.6443, val_loss: 28.0787, val_MinusLogProbMetric: 28.0787

Epoch 695: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6443 - MinusLogProbMetric: 27.6443 - val_loss: 28.0787 - val_MinusLogProbMetric: 28.0787 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 696/1000
2023-10-27 01:02:52.228 
Epoch 696/1000 
	 loss: 27.7001, MinusLogProbMetric: 27.7001, val_loss: 27.8514, val_MinusLogProbMetric: 27.8514

Epoch 696: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.7001 - MinusLogProbMetric: 27.7001 - val_loss: 27.8514 - val_MinusLogProbMetric: 27.8514 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 697/1000
2023-10-27 01:03:26.603 
Epoch 697/1000 
	 loss: 27.6278, MinusLogProbMetric: 27.6278, val_loss: 27.8597, val_MinusLogProbMetric: 27.8597

Epoch 697: val_loss did not improve from 27.79667
196/196 - 34s - loss: 27.6278 - MinusLogProbMetric: 27.6278 - val_loss: 27.8597 - val_MinusLogProbMetric: 27.8597 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 698/1000
2023-10-27 01:04:01.441 
Epoch 698/1000 
	 loss: 27.6793, MinusLogProbMetric: 27.6793, val_loss: 28.2526, val_MinusLogProbMetric: 28.2526

Epoch 698: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6793 - MinusLogProbMetric: 27.6793 - val_loss: 28.2526 - val_MinusLogProbMetric: 28.2526 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 699/1000
2023-10-27 01:04:36.363 
Epoch 699/1000 
	 loss: 27.6667, MinusLogProbMetric: 27.6667, val_loss: 27.8805, val_MinusLogProbMetric: 27.8805

Epoch 699: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6667 - MinusLogProbMetric: 27.6667 - val_loss: 27.8805 - val_MinusLogProbMetric: 27.8805 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 700/1000
2023-10-27 01:05:11.356 
Epoch 700/1000 
	 loss: 27.6828, MinusLogProbMetric: 27.6828, val_loss: 27.9417, val_MinusLogProbMetric: 27.9417

Epoch 700: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6828 - MinusLogProbMetric: 27.6828 - val_loss: 27.9417 - val_MinusLogProbMetric: 27.9417 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 701/1000
2023-10-27 01:05:46.234 
Epoch 701/1000 
	 loss: 27.6606, MinusLogProbMetric: 27.6606, val_loss: 27.9877, val_MinusLogProbMetric: 27.9877

Epoch 701: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6606 - MinusLogProbMetric: 27.6606 - val_loss: 27.9877 - val_MinusLogProbMetric: 27.9877 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 702/1000
2023-10-27 01:06:21.358 
Epoch 702/1000 
	 loss: 27.6469, MinusLogProbMetric: 27.6469, val_loss: 27.9781, val_MinusLogProbMetric: 27.9781

Epoch 702: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6469 - MinusLogProbMetric: 27.6469 - val_loss: 27.9781 - val_MinusLogProbMetric: 27.9781 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 703/1000
2023-10-27 01:06:56.076 
Epoch 703/1000 
	 loss: 27.6620, MinusLogProbMetric: 27.6620, val_loss: 27.9577, val_MinusLogProbMetric: 27.9577

Epoch 703: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6620 - MinusLogProbMetric: 27.6620 - val_loss: 27.9577 - val_MinusLogProbMetric: 27.9577 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 704/1000
2023-10-27 01:07:30.573 
Epoch 704/1000 
	 loss: 27.6738, MinusLogProbMetric: 27.6738, val_loss: 28.1611, val_MinusLogProbMetric: 28.1611

Epoch 704: val_loss did not improve from 27.79667
196/196 - 34s - loss: 27.6738 - MinusLogProbMetric: 27.6738 - val_loss: 28.1611 - val_MinusLogProbMetric: 28.1611 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 705/1000
2023-10-27 01:08:05.586 
Epoch 705/1000 
	 loss: 27.6385, MinusLogProbMetric: 27.6385, val_loss: 27.8577, val_MinusLogProbMetric: 27.8577

Epoch 705: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6385 - MinusLogProbMetric: 27.6385 - val_loss: 27.8577 - val_MinusLogProbMetric: 27.8577 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 706/1000
2023-10-27 01:08:40.214 
Epoch 706/1000 
	 loss: 27.6604, MinusLogProbMetric: 27.6604, val_loss: 27.8686, val_MinusLogProbMetric: 27.8686

Epoch 706: val_loss did not improve from 27.79667
196/196 - 35s - loss: 27.6604 - MinusLogProbMetric: 27.6604 - val_loss: 27.8686 - val_MinusLogProbMetric: 27.8686 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 707/1000
2023-10-27 01:09:14.992 
Epoch 707/1000 
	 loss: 27.6564, MinusLogProbMetric: 27.6564, val_loss: 27.7931, val_MinusLogProbMetric: 27.7931

Epoch 707: val_loss improved from 27.79667 to 27.79309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.6564 - MinusLogProbMetric: 27.6564 - val_loss: 27.7931 - val_MinusLogProbMetric: 27.7931 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 708/1000
2023-10-27 01:09:50.568 
Epoch 708/1000 
	 loss: 27.6643, MinusLogProbMetric: 27.6643, val_loss: 27.8602, val_MinusLogProbMetric: 27.8602

Epoch 708: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6643 - MinusLogProbMetric: 27.6643 - val_loss: 27.8602 - val_MinusLogProbMetric: 27.8602 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 709/1000
2023-10-27 01:10:25.395 
Epoch 709/1000 
	 loss: 27.6610, MinusLogProbMetric: 27.6610, val_loss: 27.8403, val_MinusLogProbMetric: 27.8403

Epoch 709: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6610 - MinusLogProbMetric: 27.6610 - val_loss: 27.8403 - val_MinusLogProbMetric: 27.8403 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 710/1000
2023-10-27 01:11:00.230 
Epoch 710/1000 
	 loss: 27.6527, MinusLogProbMetric: 27.6527, val_loss: 27.8788, val_MinusLogProbMetric: 27.8788

Epoch 710: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6527 - MinusLogProbMetric: 27.6527 - val_loss: 27.8788 - val_MinusLogProbMetric: 27.8788 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 711/1000
2023-10-27 01:11:34.801 
Epoch 711/1000 
	 loss: 27.6601, MinusLogProbMetric: 27.6601, val_loss: 27.8627, val_MinusLogProbMetric: 27.8627

Epoch 711: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6601 - MinusLogProbMetric: 27.6601 - val_loss: 27.8627 - val_MinusLogProbMetric: 27.8627 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 712/1000
2023-10-27 01:12:09.534 
Epoch 712/1000 
	 loss: 27.6705, MinusLogProbMetric: 27.6705, val_loss: 27.9336, val_MinusLogProbMetric: 27.9336

Epoch 712: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6705 - MinusLogProbMetric: 27.6705 - val_loss: 27.9336 - val_MinusLogProbMetric: 27.9336 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 713/1000
2023-10-27 01:12:44.803 
Epoch 713/1000 
	 loss: 27.6680, MinusLogProbMetric: 27.6680, val_loss: 27.9200, val_MinusLogProbMetric: 27.9200

Epoch 713: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6680 - MinusLogProbMetric: 27.6680 - val_loss: 27.9200 - val_MinusLogProbMetric: 27.9200 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 714/1000
2023-10-27 01:13:19.388 
Epoch 714/1000 
	 loss: 27.6764, MinusLogProbMetric: 27.6764, val_loss: 27.8693, val_MinusLogProbMetric: 27.8693

Epoch 714: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6764 - MinusLogProbMetric: 27.6764 - val_loss: 27.8693 - val_MinusLogProbMetric: 27.8693 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 715/1000
2023-10-27 01:13:54.223 
Epoch 715/1000 
	 loss: 27.6528, MinusLogProbMetric: 27.6528, val_loss: 27.8577, val_MinusLogProbMetric: 27.8577

Epoch 715: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6528 - MinusLogProbMetric: 27.6528 - val_loss: 27.8577 - val_MinusLogProbMetric: 27.8577 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 716/1000
2023-10-27 01:14:29.265 
Epoch 716/1000 
	 loss: 27.6617, MinusLogProbMetric: 27.6617, val_loss: 28.0427, val_MinusLogProbMetric: 28.0427

Epoch 716: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6617 - MinusLogProbMetric: 27.6617 - val_loss: 28.0427 - val_MinusLogProbMetric: 28.0427 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 717/1000
2023-10-27 01:15:03.885 
Epoch 717/1000 
	 loss: 27.6858, MinusLogProbMetric: 27.6858, val_loss: 27.8734, val_MinusLogProbMetric: 27.8734

Epoch 717: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6858 - MinusLogProbMetric: 27.6858 - val_loss: 27.8734 - val_MinusLogProbMetric: 27.8734 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 718/1000
2023-10-27 01:15:38.818 
Epoch 718/1000 
	 loss: 27.6249, MinusLogProbMetric: 27.6249, val_loss: 27.8584, val_MinusLogProbMetric: 27.8584

Epoch 718: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6249 - MinusLogProbMetric: 27.6249 - val_loss: 27.8584 - val_MinusLogProbMetric: 27.8584 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 719/1000
2023-10-27 01:16:13.374 
Epoch 719/1000 
	 loss: 27.6573, MinusLogProbMetric: 27.6573, val_loss: 27.8536, val_MinusLogProbMetric: 27.8536

Epoch 719: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6573 - MinusLogProbMetric: 27.6573 - val_loss: 27.8536 - val_MinusLogProbMetric: 27.8536 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 720/1000
2023-10-27 01:16:48.139 
Epoch 720/1000 
	 loss: 27.6778, MinusLogProbMetric: 27.6778, val_loss: 27.9470, val_MinusLogProbMetric: 27.9470

Epoch 720: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6778 - MinusLogProbMetric: 27.6778 - val_loss: 27.9470 - val_MinusLogProbMetric: 27.9470 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 721/1000
2023-10-27 01:17:22.351 
Epoch 721/1000 
	 loss: 27.6540, MinusLogProbMetric: 27.6540, val_loss: 27.9077, val_MinusLogProbMetric: 27.9077

Epoch 721: val_loss did not improve from 27.79309
196/196 - 34s - loss: 27.6540 - MinusLogProbMetric: 27.6540 - val_loss: 27.9077 - val_MinusLogProbMetric: 27.9077 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 722/1000
2023-10-27 01:17:57.317 
Epoch 722/1000 
	 loss: 27.6745, MinusLogProbMetric: 27.6745, val_loss: 27.8929, val_MinusLogProbMetric: 27.8929

Epoch 722: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6745 - MinusLogProbMetric: 27.6745 - val_loss: 27.8929 - val_MinusLogProbMetric: 27.8929 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 723/1000
2023-10-27 01:18:32.309 
Epoch 723/1000 
	 loss: 27.6320, MinusLogProbMetric: 27.6320, val_loss: 27.8666, val_MinusLogProbMetric: 27.8666

Epoch 723: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6320 - MinusLogProbMetric: 27.6320 - val_loss: 27.8666 - val_MinusLogProbMetric: 27.8666 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 724/1000
2023-10-27 01:19:07.102 
Epoch 724/1000 
	 loss: 27.6658, MinusLogProbMetric: 27.6658, val_loss: 27.8438, val_MinusLogProbMetric: 27.8438

Epoch 724: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6658 - MinusLogProbMetric: 27.6658 - val_loss: 27.8438 - val_MinusLogProbMetric: 27.8438 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 725/1000
2023-10-27 01:19:41.769 
Epoch 725/1000 
	 loss: 27.6592, MinusLogProbMetric: 27.6592, val_loss: 27.8489, val_MinusLogProbMetric: 27.8489

Epoch 725: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6592 - MinusLogProbMetric: 27.6592 - val_loss: 27.8489 - val_MinusLogProbMetric: 27.8489 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 726/1000
2023-10-27 01:20:16.539 
Epoch 726/1000 
	 loss: 27.6601, MinusLogProbMetric: 27.6601, val_loss: 28.1532, val_MinusLogProbMetric: 28.1532

Epoch 726: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6601 - MinusLogProbMetric: 27.6601 - val_loss: 28.1532 - val_MinusLogProbMetric: 28.1532 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 727/1000
2023-10-27 01:20:51.412 
Epoch 727/1000 
	 loss: 27.6751, MinusLogProbMetric: 27.6751, val_loss: 27.9475, val_MinusLogProbMetric: 27.9475

Epoch 727: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6751 - MinusLogProbMetric: 27.6751 - val_loss: 27.9475 - val_MinusLogProbMetric: 27.9475 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 728/1000
2023-10-27 01:21:26.177 
Epoch 728/1000 
	 loss: 27.6850, MinusLogProbMetric: 27.6850, val_loss: 27.8770, val_MinusLogProbMetric: 27.8770

Epoch 728: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6850 - MinusLogProbMetric: 27.6850 - val_loss: 27.8770 - val_MinusLogProbMetric: 27.8770 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 729/1000
2023-10-27 01:22:00.903 
Epoch 729/1000 
	 loss: 27.6411, MinusLogProbMetric: 27.6411, val_loss: 28.0111, val_MinusLogProbMetric: 28.0111

Epoch 729: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6411 - MinusLogProbMetric: 27.6411 - val_loss: 28.0111 - val_MinusLogProbMetric: 28.0111 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 730/1000
2023-10-27 01:22:35.469 
Epoch 730/1000 
	 loss: 27.6653, MinusLogProbMetric: 27.6653, val_loss: 27.9799, val_MinusLogProbMetric: 27.9799

Epoch 730: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6653 - MinusLogProbMetric: 27.6653 - val_loss: 27.9799 - val_MinusLogProbMetric: 27.9799 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 731/1000
2023-10-27 01:23:10.058 
Epoch 731/1000 
	 loss: 27.6714, MinusLogProbMetric: 27.6714, val_loss: 27.9395, val_MinusLogProbMetric: 27.9395

Epoch 731: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6714 - MinusLogProbMetric: 27.6714 - val_loss: 27.9395 - val_MinusLogProbMetric: 27.9395 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 732/1000
2023-10-27 01:23:44.828 
Epoch 732/1000 
	 loss: 27.6610, MinusLogProbMetric: 27.6610, val_loss: 27.8545, val_MinusLogProbMetric: 27.8545

Epoch 732: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6610 - MinusLogProbMetric: 27.6610 - val_loss: 27.8545 - val_MinusLogProbMetric: 27.8545 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 733/1000
2023-10-27 01:24:19.456 
Epoch 733/1000 
	 loss: 27.6638, MinusLogProbMetric: 27.6638, val_loss: 27.8612, val_MinusLogProbMetric: 27.8612

Epoch 733: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6638 - MinusLogProbMetric: 27.6638 - val_loss: 27.8612 - val_MinusLogProbMetric: 27.8612 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 734/1000
2023-10-27 01:24:54.358 
Epoch 734/1000 
	 loss: 27.6637, MinusLogProbMetric: 27.6637, val_loss: 27.8848, val_MinusLogProbMetric: 27.8848

Epoch 734: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6637 - MinusLogProbMetric: 27.6637 - val_loss: 27.8848 - val_MinusLogProbMetric: 27.8848 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 735/1000
2023-10-27 01:25:29.254 
Epoch 735/1000 
	 loss: 27.6582, MinusLogProbMetric: 27.6582, val_loss: 27.9478, val_MinusLogProbMetric: 27.9478

Epoch 735: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6582 - MinusLogProbMetric: 27.6582 - val_loss: 27.9478 - val_MinusLogProbMetric: 27.9478 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 736/1000
2023-10-27 01:26:04.052 
Epoch 736/1000 
	 loss: 27.6527, MinusLogProbMetric: 27.6527, val_loss: 27.8582, val_MinusLogProbMetric: 27.8582

Epoch 736: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6527 - MinusLogProbMetric: 27.6527 - val_loss: 27.8582 - val_MinusLogProbMetric: 27.8582 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 737/1000
2023-10-27 01:26:38.603 
Epoch 737/1000 
	 loss: 27.6517, MinusLogProbMetric: 27.6517, val_loss: 27.9565, val_MinusLogProbMetric: 27.9565

Epoch 737: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6517 - MinusLogProbMetric: 27.6517 - val_loss: 27.9565 - val_MinusLogProbMetric: 27.9565 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 738/1000
2023-10-27 01:27:13.454 
Epoch 738/1000 
	 loss: 27.6499, MinusLogProbMetric: 27.6499, val_loss: 27.9396, val_MinusLogProbMetric: 27.9396

Epoch 738: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6499 - MinusLogProbMetric: 27.6499 - val_loss: 27.9396 - val_MinusLogProbMetric: 27.9396 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 739/1000
2023-10-27 01:27:48.285 
Epoch 739/1000 
	 loss: 27.6457, MinusLogProbMetric: 27.6457, val_loss: 27.9007, val_MinusLogProbMetric: 27.9007

Epoch 739: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6457 - MinusLogProbMetric: 27.6457 - val_loss: 27.9007 - val_MinusLogProbMetric: 27.9007 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 740/1000
2023-10-27 01:28:23.098 
Epoch 740/1000 
	 loss: 27.6558, MinusLogProbMetric: 27.6558, val_loss: 27.9881, val_MinusLogProbMetric: 27.9881

Epoch 740: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6558 - MinusLogProbMetric: 27.6558 - val_loss: 27.9881 - val_MinusLogProbMetric: 27.9881 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 741/1000
2023-10-27 01:28:58.028 
Epoch 741/1000 
	 loss: 27.6836, MinusLogProbMetric: 27.6836, val_loss: 27.8870, val_MinusLogProbMetric: 27.8870

Epoch 741: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6836 - MinusLogProbMetric: 27.6836 - val_loss: 27.8870 - val_MinusLogProbMetric: 27.8870 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 742/1000
2023-10-27 01:29:32.805 
Epoch 742/1000 
	 loss: 27.6564, MinusLogProbMetric: 27.6564, val_loss: 28.1610, val_MinusLogProbMetric: 28.1610

Epoch 742: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6564 - MinusLogProbMetric: 27.6564 - val_loss: 28.1610 - val_MinusLogProbMetric: 28.1610 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 743/1000
2023-10-27 01:30:07.908 
Epoch 743/1000 
	 loss: 27.6607, MinusLogProbMetric: 27.6607, val_loss: 27.8628, val_MinusLogProbMetric: 27.8628

Epoch 743: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6607 - MinusLogProbMetric: 27.6607 - val_loss: 27.8628 - val_MinusLogProbMetric: 27.8628 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 744/1000
2023-10-27 01:30:42.979 
Epoch 744/1000 
	 loss: 27.6379, MinusLogProbMetric: 27.6379, val_loss: 27.8101, val_MinusLogProbMetric: 27.8101

Epoch 744: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6379 - MinusLogProbMetric: 27.6379 - val_loss: 27.8101 - val_MinusLogProbMetric: 27.8101 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 745/1000
2023-10-27 01:31:17.826 
Epoch 745/1000 
	 loss: 27.6968, MinusLogProbMetric: 27.6968, val_loss: 28.0660, val_MinusLogProbMetric: 28.0660

Epoch 745: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6968 - MinusLogProbMetric: 27.6968 - val_loss: 28.0660 - val_MinusLogProbMetric: 28.0660 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 746/1000
2023-10-27 01:31:52.495 
Epoch 746/1000 
	 loss: 27.6587, MinusLogProbMetric: 27.6587, val_loss: 27.8150, val_MinusLogProbMetric: 27.8150

Epoch 746: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6587 - MinusLogProbMetric: 27.6587 - val_loss: 27.8150 - val_MinusLogProbMetric: 27.8150 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 747/1000
2023-10-27 01:32:27.364 
Epoch 747/1000 
	 loss: 27.6693, MinusLogProbMetric: 27.6693, val_loss: 28.1198, val_MinusLogProbMetric: 28.1198

Epoch 747: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6693 - MinusLogProbMetric: 27.6693 - val_loss: 28.1198 - val_MinusLogProbMetric: 28.1198 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 748/1000
2023-10-27 01:33:01.820 
Epoch 748/1000 
	 loss: 27.6452, MinusLogProbMetric: 27.6452, val_loss: 27.8611, val_MinusLogProbMetric: 27.8611

Epoch 748: val_loss did not improve from 27.79309
196/196 - 34s - loss: 27.6452 - MinusLogProbMetric: 27.6452 - val_loss: 27.8611 - val_MinusLogProbMetric: 27.8611 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 749/1000
2023-10-27 01:33:36.454 
Epoch 749/1000 
	 loss: 27.6405, MinusLogProbMetric: 27.6405, val_loss: 27.9419, val_MinusLogProbMetric: 27.9419

Epoch 749: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6405 - MinusLogProbMetric: 27.6405 - val_loss: 27.9419 - val_MinusLogProbMetric: 27.9419 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 750/1000
2023-10-27 01:34:11.294 
Epoch 750/1000 
	 loss: 27.6554, MinusLogProbMetric: 27.6554, val_loss: 27.8630, val_MinusLogProbMetric: 27.8630

Epoch 750: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6554 - MinusLogProbMetric: 27.6554 - val_loss: 27.8630 - val_MinusLogProbMetric: 27.8630 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 751/1000
2023-10-27 01:34:46.491 
Epoch 751/1000 
	 loss: 27.6534, MinusLogProbMetric: 27.6534, val_loss: 27.9838, val_MinusLogProbMetric: 27.9838

Epoch 751: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6534 - MinusLogProbMetric: 27.6534 - val_loss: 27.9838 - val_MinusLogProbMetric: 27.9838 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 752/1000
2023-10-27 01:35:21.492 
Epoch 752/1000 
	 loss: 27.6379, MinusLogProbMetric: 27.6379, val_loss: 27.8952, val_MinusLogProbMetric: 27.8952

Epoch 752: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6379 - MinusLogProbMetric: 27.6379 - val_loss: 27.8952 - val_MinusLogProbMetric: 27.8952 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 753/1000
2023-10-27 01:35:56.151 
Epoch 753/1000 
	 loss: 27.6392, MinusLogProbMetric: 27.6392, val_loss: 27.8154, val_MinusLogProbMetric: 27.8154

Epoch 753: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6392 - MinusLogProbMetric: 27.6392 - val_loss: 27.8154 - val_MinusLogProbMetric: 27.8154 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 754/1000
2023-10-27 01:36:31.207 
Epoch 754/1000 
	 loss: 27.6639, MinusLogProbMetric: 27.6639, val_loss: 28.2320, val_MinusLogProbMetric: 28.2320

Epoch 754: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6639 - MinusLogProbMetric: 27.6639 - val_loss: 28.2320 - val_MinusLogProbMetric: 28.2320 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 755/1000
2023-10-27 01:37:06.009 
Epoch 755/1000 
	 loss: 27.6303, MinusLogProbMetric: 27.6303, val_loss: 27.8254, val_MinusLogProbMetric: 27.8254

Epoch 755: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6303 - MinusLogProbMetric: 27.6303 - val_loss: 27.8254 - val_MinusLogProbMetric: 27.8254 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 756/1000
2023-10-27 01:37:41.104 
Epoch 756/1000 
	 loss: 27.6803, MinusLogProbMetric: 27.6803, val_loss: 27.8373, val_MinusLogProbMetric: 27.8373

Epoch 756: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6803 - MinusLogProbMetric: 27.6803 - val_loss: 27.8373 - val_MinusLogProbMetric: 27.8373 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 757/1000
2023-10-27 01:38:16.231 
Epoch 757/1000 
	 loss: 27.6406, MinusLogProbMetric: 27.6406, val_loss: 27.9150, val_MinusLogProbMetric: 27.9150

Epoch 757: val_loss did not improve from 27.79309
196/196 - 35s - loss: 27.6406 - MinusLogProbMetric: 27.6406 - val_loss: 27.9150 - val_MinusLogProbMetric: 27.9150 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 758/1000
2023-10-27 01:38:51.502 
Epoch 758/1000 
	 loss: 27.5628, MinusLogProbMetric: 27.5628, val_loss: 27.7858, val_MinusLogProbMetric: 27.7858

Epoch 758: val_loss improved from 27.79309 to 27.78579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.5628 - MinusLogProbMetric: 27.5628 - val_loss: 27.7858 - val_MinusLogProbMetric: 27.7858 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 759/1000
2023-10-27 01:39:27.273 
Epoch 759/1000 
	 loss: 27.5457, MinusLogProbMetric: 27.5457, val_loss: 27.7973, val_MinusLogProbMetric: 27.7973

Epoch 759: val_loss did not improve from 27.78579
196/196 - 35s - loss: 27.5457 - MinusLogProbMetric: 27.5457 - val_loss: 27.7973 - val_MinusLogProbMetric: 27.7973 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 760/1000
2023-10-27 01:40:02.316 
Epoch 760/1000 
	 loss: 27.5481, MinusLogProbMetric: 27.5481, val_loss: 27.7840, val_MinusLogProbMetric: 27.7840

Epoch 760: val_loss improved from 27.78579 to 27.78403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.5481 - MinusLogProbMetric: 27.5481 - val_loss: 27.7840 - val_MinusLogProbMetric: 27.7840 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 761/1000
2023-10-27 01:40:37.766 
Epoch 761/1000 
	 loss: 27.5408, MinusLogProbMetric: 27.5408, val_loss: 27.7616, val_MinusLogProbMetric: 27.7616

Epoch 761: val_loss improved from 27.78403 to 27.76163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.5408 - MinusLogProbMetric: 27.5408 - val_loss: 27.7616 - val_MinusLogProbMetric: 27.7616 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 762/1000
2023-10-27 01:41:13.025 
Epoch 762/1000 
	 loss: 27.5455, MinusLogProbMetric: 27.5455, val_loss: 27.7804, val_MinusLogProbMetric: 27.7804

Epoch 762: val_loss did not improve from 27.76163
196/196 - 35s - loss: 27.5455 - MinusLogProbMetric: 27.5455 - val_loss: 27.7804 - val_MinusLogProbMetric: 27.7804 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 763/1000
2023-10-27 01:41:47.965 
Epoch 763/1000 
	 loss: 27.5429, MinusLogProbMetric: 27.5429, val_loss: 27.8032, val_MinusLogProbMetric: 27.8032

Epoch 763: val_loss did not improve from 27.76163
196/196 - 35s - loss: 27.5429 - MinusLogProbMetric: 27.5429 - val_loss: 27.8032 - val_MinusLogProbMetric: 27.8032 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 764/1000
2023-10-27 01:42:23.321 
Epoch 764/1000 
	 loss: 27.5416, MinusLogProbMetric: 27.5416, val_loss: 27.7645, val_MinusLogProbMetric: 27.7645

Epoch 764: val_loss did not improve from 27.76163
196/196 - 35s - loss: 27.5416 - MinusLogProbMetric: 27.5416 - val_loss: 27.7645 - val_MinusLogProbMetric: 27.7645 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 765/1000
2023-10-27 01:42:58.229 
Epoch 765/1000 
	 loss: 27.5457, MinusLogProbMetric: 27.5457, val_loss: 27.7608, val_MinusLogProbMetric: 27.7608

Epoch 765: val_loss improved from 27.76163 to 27.76081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.5457 - MinusLogProbMetric: 27.5457 - val_loss: 27.7608 - val_MinusLogProbMetric: 27.7608 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 766/1000
2023-10-27 01:43:34.090 
Epoch 766/1000 
	 loss: 27.5463, MinusLogProbMetric: 27.5463, val_loss: 27.8001, val_MinusLogProbMetric: 27.8001

Epoch 766: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5463 - MinusLogProbMetric: 27.5463 - val_loss: 27.8001 - val_MinusLogProbMetric: 27.8001 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 767/1000
2023-10-27 01:44:09.304 
Epoch 767/1000 
	 loss: 27.5612, MinusLogProbMetric: 27.5612, val_loss: 27.7613, val_MinusLogProbMetric: 27.7613

Epoch 767: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5612 - MinusLogProbMetric: 27.5612 - val_loss: 27.7613 - val_MinusLogProbMetric: 27.7613 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 768/1000
2023-10-27 01:44:44.752 
Epoch 768/1000 
	 loss: 27.5481, MinusLogProbMetric: 27.5481, val_loss: 27.7953, val_MinusLogProbMetric: 27.7953

Epoch 768: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5481 - MinusLogProbMetric: 27.5481 - val_loss: 27.7953 - val_MinusLogProbMetric: 27.7953 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 769/1000
2023-10-27 01:45:19.794 
Epoch 769/1000 
	 loss: 27.5439, MinusLogProbMetric: 27.5439, val_loss: 27.7611, val_MinusLogProbMetric: 27.7611

Epoch 769: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5439 - MinusLogProbMetric: 27.5439 - val_loss: 27.7611 - val_MinusLogProbMetric: 27.7611 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 770/1000
2023-10-27 01:45:52.293 
Epoch 770/1000 
	 loss: 27.5570, MinusLogProbMetric: 27.5570, val_loss: 27.7942, val_MinusLogProbMetric: 27.7942

Epoch 770: val_loss did not improve from 27.76081
196/196 - 32s - loss: 27.5570 - MinusLogProbMetric: 27.5570 - val_loss: 27.7942 - val_MinusLogProbMetric: 27.7942 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 771/1000
2023-10-27 01:46:27.250 
Epoch 771/1000 
	 loss: 27.5520, MinusLogProbMetric: 27.5520, val_loss: 27.7879, val_MinusLogProbMetric: 27.7879

Epoch 771: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5520 - MinusLogProbMetric: 27.5520 - val_loss: 27.7879 - val_MinusLogProbMetric: 27.7879 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 772/1000
2023-10-27 01:47:00.600 
Epoch 772/1000 
	 loss: 27.5420, MinusLogProbMetric: 27.5420, val_loss: 27.8563, val_MinusLogProbMetric: 27.8563

Epoch 772: val_loss did not improve from 27.76081
196/196 - 33s - loss: 27.5420 - MinusLogProbMetric: 27.5420 - val_loss: 27.8563 - val_MinusLogProbMetric: 27.8563 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 773/1000
2023-10-27 01:47:33.362 
Epoch 773/1000 
	 loss: 27.5776, MinusLogProbMetric: 27.5776, val_loss: 27.7823, val_MinusLogProbMetric: 27.7823

Epoch 773: val_loss did not improve from 27.76081
196/196 - 33s - loss: 27.5776 - MinusLogProbMetric: 27.5776 - val_loss: 27.7823 - val_MinusLogProbMetric: 27.7823 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 774/1000
2023-10-27 01:48:07.142 
Epoch 774/1000 
	 loss: 27.5440, MinusLogProbMetric: 27.5440, val_loss: 27.7898, val_MinusLogProbMetric: 27.7898

Epoch 774: val_loss did not improve from 27.76081
196/196 - 34s - loss: 27.5440 - MinusLogProbMetric: 27.5440 - val_loss: 27.7898 - val_MinusLogProbMetric: 27.7898 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 775/1000
2023-10-27 01:48:41.934 
Epoch 775/1000 
	 loss: 27.5593, MinusLogProbMetric: 27.5593, val_loss: 27.7942, val_MinusLogProbMetric: 27.7942

Epoch 775: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5593 - MinusLogProbMetric: 27.5593 - val_loss: 27.7942 - val_MinusLogProbMetric: 27.7942 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 776/1000
2023-10-27 01:49:16.707 
Epoch 776/1000 
	 loss: 27.5544, MinusLogProbMetric: 27.5544, val_loss: 27.7835, val_MinusLogProbMetric: 27.7835

Epoch 776: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5544 - MinusLogProbMetric: 27.5544 - val_loss: 27.7835 - val_MinusLogProbMetric: 27.7835 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 777/1000
2023-10-27 01:49:51.978 
Epoch 777/1000 
	 loss: 27.5602, MinusLogProbMetric: 27.5602, val_loss: 27.7660, val_MinusLogProbMetric: 27.7660

Epoch 777: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5602 - MinusLogProbMetric: 27.5602 - val_loss: 27.7660 - val_MinusLogProbMetric: 27.7660 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 778/1000
2023-10-27 01:50:26.871 
Epoch 778/1000 
	 loss: 27.5562, MinusLogProbMetric: 27.5562, val_loss: 27.7928, val_MinusLogProbMetric: 27.7928

Epoch 778: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5562 - MinusLogProbMetric: 27.5562 - val_loss: 27.7928 - val_MinusLogProbMetric: 27.7928 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 779/1000
2023-10-27 01:51:01.980 
Epoch 779/1000 
	 loss: 27.5492, MinusLogProbMetric: 27.5492, val_loss: 27.8385, val_MinusLogProbMetric: 27.8385

Epoch 779: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5492 - MinusLogProbMetric: 27.5492 - val_loss: 27.8385 - val_MinusLogProbMetric: 27.8385 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 780/1000
2023-10-27 01:51:37.343 
Epoch 780/1000 
	 loss: 27.5514, MinusLogProbMetric: 27.5514, val_loss: 27.7877, val_MinusLogProbMetric: 27.7877

Epoch 780: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5514 - MinusLogProbMetric: 27.5514 - val_loss: 27.7877 - val_MinusLogProbMetric: 27.7877 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 781/1000
2023-10-27 01:52:12.749 
Epoch 781/1000 
	 loss: 27.5521, MinusLogProbMetric: 27.5521, val_loss: 27.7815, val_MinusLogProbMetric: 27.7815

Epoch 781: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5521 - MinusLogProbMetric: 27.5521 - val_loss: 27.7815 - val_MinusLogProbMetric: 27.7815 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 782/1000
2023-10-27 01:52:48.316 
Epoch 782/1000 
	 loss: 27.5489, MinusLogProbMetric: 27.5489, val_loss: 27.8166, val_MinusLogProbMetric: 27.8166

Epoch 782: val_loss did not improve from 27.76081
196/196 - 36s - loss: 27.5489 - MinusLogProbMetric: 27.5489 - val_loss: 27.8166 - val_MinusLogProbMetric: 27.8166 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 783/1000
2023-10-27 01:53:23.726 
Epoch 783/1000 
	 loss: 27.5513, MinusLogProbMetric: 27.5513, val_loss: 27.7880, val_MinusLogProbMetric: 27.7880

Epoch 783: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5513 - MinusLogProbMetric: 27.5513 - val_loss: 27.7880 - val_MinusLogProbMetric: 27.7880 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 784/1000
2023-10-27 01:53:59.118 
Epoch 784/1000 
	 loss: 27.5529, MinusLogProbMetric: 27.5529, val_loss: 27.7660, val_MinusLogProbMetric: 27.7660

Epoch 784: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5529 - MinusLogProbMetric: 27.5529 - val_loss: 27.7660 - val_MinusLogProbMetric: 27.7660 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 785/1000
2023-10-27 01:54:34.064 
Epoch 785/1000 
	 loss: 27.5506, MinusLogProbMetric: 27.5506, val_loss: 27.7913, val_MinusLogProbMetric: 27.7913

Epoch 785: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5506 - MinusLogProbMetric: 27.5506 - val_loss: 27.7913 - val_MinusLogProbMetric: 27.7913 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 786/1000
2023-10-27 01:55:09.335 
Epoch 786/1000 
	 loss: 27.5504, MinusLogProbMetric: 27.5504, val_loss: 27.9231, val_MinusLogProbMetric: 27.9231

Epoch 786: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5504 - MinusLogProbMetric: 27.5504 - val_loss: 27.9231 - val_MinusLogProbMetric: 27.9231 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 787/1000
2023-10-27 01:55:44.449 
Epoch 787/1000 
	 loss: 27.5523, MinusLogProbMetric: 27.5523, val_loss: 27.7682, val_MinusLogProbMetric: 27.7682

Epoch 787: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5523 - MinusLogProbMetric: 27.5523 - val_loss: 27.7682 - val_MinusLogProbMetric: 27.7682 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 788/1000
2023-10-27 01:56:19.626 
Epoch 788/1000 
	 loss: 27.5501, MinusLogProbMetric: 27.5501, val_loss: 27.7804, val_MinusLogProbMetric: 27.7804

Epoch 788: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5501 - MinusLogProbMetric: 27.5501 - val_loss: 27.7804 - val_MinusLogProbMetric: 27.7804 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 789/1000
2023-10-27 01:56:54.357 
Epoch 789/1000 
	 loss: 27.5595, MinusLogProbMetric: 27.5595, val_loss: 27.7712, val_MinusLogProbMetric: 27.7712

Epoch 789: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5595 - MinusLogProbMetric: 27.5595 - val_loss: 27.7712 - val_MinusLogProbMetric: 27.7712 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 790/1000
2023-10-27 01:57:29.505 
Epoch 790/1000 
	 loss: 27.5524, MinusLogProbMetric: 27.5524, val_loss: 27.8043, val_MinusLogProbMetric: 27.8043

Epoch 790: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5524 - MinusLogProbMetric: 27.5524 - val_loss: 27.8043 - val_MinusLogProbMetric: 27.8043 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 791/1000
2023-10-27 01:58:04.786 
Epoch 791/1000 
	 loss: 27.5661, MinusLogProbMetric: 27.5661, val_loss: 27.7764, val_MinusLogProbMetric: 27.7764

Epoch 791: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5661 - MinusLogProbMetric: 27.5661 - val_loss: 27.7764 - val_MinusLogProbMetric: 27.7764 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 792/1000
2023-10-27 01:58:39.741 
Epoch 792/1000 
	 loss: 27.5566, MinusLogProbMetric: 27.5566, val_loss: 27.8089, val_MinusLogProbMetric: 27.8089

Epoch 792: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5566 - MinusLogProbMetric: 27.5566 - val_loss: 27.8089 - val_MinusLogProbMetric: 27.8089 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 793/1000
2023-10-27 01:59:15.040 
Epoch 793/1000 
	 loss: 27.5526, MinusLogProbMetric: 27.5526, val_loss: 27.7713, val_MinusLogProbMetric: 27.7713

Epoch 793: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5526 - MinusLogProbMetric: 27.5526 - val_loss: 27.7713 - val_MinusLogProbMetric: 27.7713 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 794/1000
2023-10-27 01:59:50.389 
Epoch 794/1000 
	 loss: 27.5394, MinusLogProbMetric: 27.5394, val_loss: 27.7836, val_MinusLogProbMetric: 27.7836

Epoch 794: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5394 - MinusLogProbMetric: 27.5394 - val_loss: 27.7836 - val_MinusLogProbMetric: 27.7836 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 795/1000
2023-10-27 02:00:25.125 
Epoch 795/1000 
	 loss: 27.5392, MinusLogProbMetric: 27.5392, val_loss: 27.7620, val_MinusLogProbMetric: 27.7620

Epoch 795: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5392 - MinusLogProbMetric: 27.5392 - val_loss: 27.7620 - val_MinusLogProbMetric: 27.7620 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 796/1000
2023-10-27 02:01:00.171 
Epoch 796/1000 
	 loss: 27.5462, MinusLogProbMetric: 27.5462, val_loss: 27.7910, val_MinusLogProbMetric: 27.7910

Epoch 796: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5462 - MinusLogProbMetric: 27.5462 - val_loss: 27.7910 - val_MinusLogProbMetric: 27.7910 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 797/1000
2023-10-27 02:01:34.772 
Epoch 797/1000 
	 loss: 27.5499, MinusLogProbMetric: 27.5499, val_loss: 27.7757, val_MinusLogProbMetric: 27.7757

Epoch 797: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5499 - MinusLogProbMetric: 27.5499 - val_loss: 27.7757 - val_MinusLogProbMetric: 27.7757 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 798/1000
2023-10-27 02:02:09.978 
Epoch 798/1000 
	 loss: 27.5621, MinusLogProbMetric: 27.5621, val_loss: 27.7653, val_MinusLogProbMetric: 27.7653

Epoch 798: val_loss did not improve from 27.76081
196/196 - 35s - loss: 27.5621 - MinusLogProbMetric: 27.5621 - val_loss: 27.7653 - val_MinusLogProbMetric: 27.7653 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 799/1000
2023-10-27 02:02:45.438 
Epoch 799/1000 
	 loss: 27.5435, MinusLogProbMetric: 27.5435, val_loss: 27.7554, val_MinusLogProbMetric: 27.7554

Epoch 799: val_loss improved from 27.76081 to 27.75540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.5435 - MinusLogProbMetric: 27.5435 - val_loss: 27.7554 - val_MinusLogProbMetric: 27.7554 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 800/1000
2023-10-27 02:03:20.538 
Epoch 800/1000 
	 loss: 27.5565, MinusLogProbMetric: 27.5565, val_loss: 27.7923, val_MinusLogProbMetric: 27.7923

Epoch 800: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5565 - MinusLogProbMetric: 27.5565 - val_loss: 27.7923 - val_MinusLogProbMetric: 27.7923 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 801/1000
2023-10-27 02:03:50.379 
Epoch 801/1000 
	 loss: 27.5462, MinusLogProbMetric: 27.5462, val_loss: 27.7605, val_MinusLogProbMetric: 27.7605

Epoch 801: val_loss did not improve from 27.75540
196/196 - 30s - loss: 27.5462 - MinusLogProbMetric: 27.5462 - val_loss: 27.7605 - val_MinusLogProbMetric: 27.7605 - lr: 1.2500e-04 - 30s/epoch - 152ms/step
Epoch 802/1000
2023-10-27 02:04:23.430 
Epoch 802/1000 
	 loss: 27.5474, MinusLogProbMetric: 27.5474, val_loss: 27.7951, val_MinusLogProbMetric: 27.7951

Epoch 802: val_loss did not improve from 27.75540
196/196 - 33s - loss: 27.5474 - MinusLogProbMetric: 27.5474 - val_loss: 27.7951 - val_MinusLogProbMetric: 27.7951 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 803/1000
2023-10-27 02:04:58.610 
Epoch 803/1000 
	 loss: 27.5687, MinusLogProbMetric: 27.5687, val_loss: 27.7681, val_MinusLogProbMetric: 27.7681

Epoch 803: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5687 - MinusLogProbMetric: 27.5687 - val_loss: 27.7681 - val_MinusLogProbMetric: 27.7681 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 804/1000
2023-10-27 02:05:33.260 
Epoch 804/1000 
	 loss: 27.5615, MinusLogProbMetric: 27.5615, val_loss: 27.8067, val_MinusLogProbMetric: 27.8067

Epoch 804: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5615 - MinusLogProbMetric: 27.5615 - val_loss: 27.8067 - val_MinusLogProbMetric: 27.8067 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 805/1000
2023-10-27 02:06:08.586 
Epoch 805/1000 
	 loss: 27.5688, MinusLogProbMetric: 27.5688, val_loss: 27.7655, val_MinusLogProbMetric: 27.7655

Epoch 805: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5688 - MinusLogProbMetric: 27.5688 - val_loss: 27.7655 - val_MinusLogProbMetric: 27.7655 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 806/1000
2023-10-27 02:06:43.837 
Epoch 806/1000 
	 loss: 27.5603, MinusLogProbMetric: 27.5603, val_loss: 27.7889, val_MinusLogProbMetric: 27.7889

Epoch 806: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5603 - MinusLogProbMetric: 27.5603 - val_loss: 27.7889 - val_MinusLogProbMetric: 27.7889 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 807/1000
2023-10-27 02:07:18.935 
Epoch 807/1000 
	 loss: 27.5468, MinusLogProbMetric: 27.5468, val_loss: 27.8702, val_MinusLogProbMetric: 27.8702

Epoch 807: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5468 - MinusLogProbMetric: 27.5468 - val_loss: 27.8702 - val_MinusLogProbMetric: 27.8702 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 808/1000
2023-10-27 02:07:53.937 
Epoch 808/1000 
	 loss: 27.5479, MinusLogProbMetric: 27.5479, val_loss: 27.8337, val_MinusLogProbMetric: 27.8337

Epoch 808: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5479 - MinusLogProbMetric: 27.5479 - val_loss: 27.8337 - val_MinusLogProbMetric: 27.8337 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 809/1000
2023-10-27 02:08:29.160 
Epoch 809/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 27.7976, val_MinusLogProbMetric: 27.7976

Epoch 809: val_loss did not improve from 27.75540
196/196 - 35s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 27.7976 - val_MinusLogProbMetric: 27.7976 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 810/1000
2023-10-27 02:09:04.299 
Epoch 810/1000 
	 loss: 27.5356, MinusLogProbMetric: 27.5356, val_loss: 27.7489, val_MinusLogProbMetric: 27.7489

Epoch 810: val_loss improved from 27.75540 to 27.74889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.5356 - MinusLogProbMetric: 27.5356 - val_loss: 27.7489 - val_MinusLogProbMetric: 27.7489 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 811/1000
2023-10-27 02:09:39.881 
Epoch 811/1000 
	 loss: 27.5572, MinusLogProbMetric: 27.5572, val_loss: 27.8209, val_MinusLogProbMetric: 27.8209

Epoch 811: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5572 - MinusLogProbMetric: 27.5572 - val_loss: 27.8209 - val_MinusLogProbMetric: 27.8209 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 812/1000
2023-10-27 02:10:14.899 
Epoch 812/1000 
	 loss: 27.5532, MinusLogProbMetric: 27.5532, val_loss: 27.8789, val_MinusLogProbMetric: 27.8789

Epoch 812: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5532 - MinusLogProbMetric: 27.5532 - val_loss: 27.8789 - val_MinusLogProbMetric: 27.8789 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 813/1000
2023-10-27 02:10:49.759 
Epoch 813/1000 
	 loss: 27.5561, MinusLogProbMetric: 27.5561, val_loss: 27.7782, val_MinusLogProbMetric: 27.7782

Epoch 813: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5561 - MinusLogProbMetric: 27.5561 - val_loss: 27.7782 - val_MinusLogProbMetric: 27.7782 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 814/1000
2023-10-27 02:11:24.962 
Epoch 814/1000 
	 loss: 27.5430, MinusLogProbMetric: 27.5430, val_loss: 27.7579, val_MinusLogProbMetric: 27.7579

Epoch 814: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5430 - MinusLogProbMetric: 27.5430 - val_loss: 27.7579 - val_MinusLogProbMetric: 27.7579 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 815/1000
2023-10-27 02:12:00.050 
Epoch 815/1000 
	 loss: 27.5418, MinusLogProbMetric: 27.5418, val_loss: 27.7797, val_MinusLogProbMetric: 27.7797

Epoch 815: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5418 - MinusLogProbMetric: 27.5418 - val_loss: 27.7797 - val_MinusLogProbMetric: 27.7797 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 816/1000
2023-10-27 02:12:35.100 
Epoch 816/1000 
	 loss: 27.5427, MinusLogProbMetric: 27.5427, val_loss: 27.7749, val_MinusLogProbMetric: 27.7749

Epoch 816: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5427 - MinusLogProbMetric: 27.5427 - val_loss: 27.7749 - val_MinusLogProbMetric: 27.7749 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 817/1000
2023-10-27 02:13:09.981 
Epoch 817/1000 
	 loss: 27.5523, MinusLogProbMetric: 27.5523, val_loss: 27.7650, val_MinusLogProbMetric: 27.7650

Epoch 817: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5523 - MinusLogProbMetric: 27.5523 - val_loss: 27.7650 - val_MinusLogProbMetric: 27.7650 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 818/1000
2023-10-27 02:13:45.012 
Epoch 818/1000 
	 loss: 27.5651, MinusLogProbMetric: 27.5651, val_loss: 27.7930, val_MinusLogProbMetric: 27.7930

Epoch 818: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5651 - MinusLogProbMetric: 27.5651 - val_loss: 27.7930 - val_MinusLogProbMetric: 27.7930 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 819/1000
2023-10-27 02:14:19.985 
Epoch 819/1000 
	 loss: 27.5548, MinusLogProbMetric: 27.5548, val_loss: 27.7791, val_MinusLogProbMetric: 27.7791

Epoch 819: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5548 - MinusLogProbMetric: 27.5548 - val_loss: 27.7791 - val_MinusLogProbMetric: 27.7791 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 820/1000
2023-10-27 02:14:55.205 
Epoch 820/1000 
	 loss: 27.5504, MinusLogProbMetric: 27.5504, val_loss: 27.7926, val_MinusLogProbMetric: 27.7926

Epoch 820: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5504 - MinusLogProbMetric: 27.5504 - val_loss: 27.7926 - val_MinusLogProbMetric: 27.7926 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 821/1000
2023-10-27 02:15:30.505 
Epoch 821/1000 
	 loss: 27.5550, MinusLogProbMetric: 27.5550, val_loss: 27.8002, val_MinusLogProbMetric: 27.8002

Epoch 821: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5550 - MinusLogProbMetric: 27.5550 - val_loss: 27.8002 - val_MinusLogProbMetric: 27.8002 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 822/1000
2023-10-27 02:16:05.533 
Epoch 822/1000 
	 loss: 27.5508, MinusLogProbMetric: 27.5508, val_loss: 27.7626, val_MinusLogProbMetric: 27.7626

Epoch 822: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5508 - MinusLogProbMetric: 27.5508 - val_loss: 27.7626 - val_MinusLogProbMetric: 27.7626 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 823/1000
2023-10-27 02:16:40.470 
Epoch 823/1000 
	 loss: 27.5385, MinusLogProbMetric: 27.5385, val_loss: 27.7556, val_MinusLogProbMetric: 27.7556

Epoch 823: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5385 - MinusLogProbMetric: 27.5385 - val_loss: 27.7556 - val_MinusLogProbMetric: 27.7556 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 824/1000
2023-10-27 02:17:15.640 
Epoch 824/1000 
	 loss: 27.5444, MinusLogProbMetric: 27.5444, val_loss: 27.7775, val_MinusLogProbMetric: 27.7775

Epoch 824: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5444 - MinusLogProbMetric: 27.5444 - val_loss: 27.7775 - val_MinusLogProbMetric: 27.7775 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 825/1000
2023-10-27 02:17:50.883 
Epoch 825/1000 
	 loss: 27.5558, MinusLogProbMetric: 27.5558, val_loss: 27.7897, val_MinusLogProbMetric: 27.7897

Epoch 825: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5558 - MinusLogProbMetric: 27.5558 - val_loss: 27.7897 - val_MinusLogProbMetric: 27.7897 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 826/1000
2023-10-27 02:18:25.626 
Epoch 826/1000 
	 loss: 27.5329, MinusLogProbMetric: 27.5329, val_loss: 27.7650, val_MinusLogProbMetric: 27.7650

Epoch 826: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5329 - MinusLogProbMetric: 27.5329 - val_loss: 27.7650 - val_MinusLogProbMetric: 27.7650 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 827/1000
2023-10-27 02:19:00.696 
Epoch 827/1000 
	 loss: 27.5442, MinusLogProbMetric: 27.5442, val_loss: 27.7654, val_MinusLogProbMetric: 27.7654

Epoch 827: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5442 - MinusLogProbMetric: 27.5442 - val_loss: 27.7654 - val_MinusLogProbMetric: 27.7654 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 828/1000
2023-10-27 02:19:35.383 
Epoch 828/1000 
	 loss: 27.5428, MinusLogProbMetric: 27.5428, val_loss: 27.7667, val_MinusLogProbMetric: 27.7667

Epoch 828: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5428 - MinusLogProbMetric: 27.5428 - val_loss: 27.7667 - val_MinusLogProbMetric: 27.7667 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 829/1000
2023-10-27 02:20:10.590 
Epoch 829/1000 
	 loss: 27.5500, MinusLogProbMetric: 27.5500, val_loss: 27.8412, val_MinusLogProbMetric: 27.8412

Epoch 829: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5500 - MinusLogProbMetric: 27.5500 - val_loss: 27.8412 - val_MinusLogProbMetric: 27.8412 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 830/1000
2023-10-27 02:20:45.649 
Epoch 830/1000 
	 loss: 27.5375, MinusLogProbMetric: 27.5375, val_loss: 27.7958, val_MinusLogProbMetric: 27.7958

Epoch 830: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5375 - MinusLogProbMetric: 27.5375 - val_loss: 27.7958 - val_MinusLogProbMetric: 27.7958 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 831/1000
2023-10-27 02:21:20.845 
Epoch 831/1000 
	 loss: 27.5484, MinusLogProbMetric: 27.5484, val_loss: 27.7654, val_MinusLogProbMetric: 27.7654

Epoch 831: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5484 - MinusLogProbMetric: 27.5484 - val_loss: 27.7654 - val_MinusLogProbMetric: 27.7654 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 832/1000
2023-10-27 02:21:56.288 
Epoch 832/1000 
	 loss: 27.5406, MinusLogProbMetric: 27.5406, val_loss: 27.7980, val_MinusLogProbMetric: 27.7980

Epoch 832: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5406 - MinusLogProbMetric: 27.5406 - val_loss: 27.7980 - val_MinusLogProbMetric: 27.7980 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 833/1000
2023-10-27 02:22:31.348 
Epoch 833/1000 
	 loss: 27.5514, MinusLogProbMetric: 27.5514, val_loss: 27.8096, val_MinusLogProbMetric: 27.8096

Epoch 833: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5514 - MinusLogProbMetric: 27.5514 - val_loss: 27.8096 - val_MinusLogProbMetric: 27.8096 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 834/1000
2023-10-27 02:23:06.438 
Epoch 834/1000 
	 loss: 27.5399, MinusLogProbMetric: 27.5399, val_loss: 27.8214, val_MinusLogProbMetric: 27.8214

Epoch 834: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5399 - MinusLogProbMetric: 27.5399 - val_loss: 27.8214 - val_MinusLogProbMetric: 27.8214 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 835/1000
2023-10-27 02:23:41.265 
Epoch 835/1000 
	 loss: 27.5414, MinusLogProbMetric: 27.5414, val_loss: 27.8046, val_MinusLogProbMetric: 27.8046

Epoch 835: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5414 - MinusLogProbMetric: 27.5414 - val_loss: 27.8046 - val_MinusLogProbMetric: 27.8046 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 836/1000
2023-10-27 02:24:16.411 
Epoch 836/1000 
	 loss: 27.5427, MinusLogProbMetric: 27.5427, val_loss: 27.7917, val_MinusLogProbMetric: 27.7917

Epoch 836: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5427 - MinusLogProbMetric: 27.5427 - val_loss: 27.7917 - val_MinusLogProbMetric: 27.7917 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 837/1000
2023-10-27 02:24:51.579 
Epoch 837/1000 
	 loss: 27.5440, MinusLogProbMetric: 27.5440, val_loss: 27.7583, val_MinusLogProbMetric: 27.7583

Epoch 837: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5440 - MinusLogProbMetric: 27.5440 - val_loss: 27.7583 - val_MinusLogProbMetric: 27.7583 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 838/1000
2023-10-27 02:25:27.020 
Epoch 838/1000 
	 loss: 27.5543, MinusLogProbMetric: 27.5543, val_loss: 27.7779, val_MinusLogProbMetric: 27.7779

Epoch 838: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5543 - MinusLogProbMetric: 27.5543 - val_loss: 27.7779 - val_MinusLogProbMetric: 27.7779 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 839/1000
2023-10-27 02:26:02.273 
Epoch 839/1000 
	 loss: 27.5491, MinusLogProbMetric: 27.5491, val_loss: 27.7695, val_MinusLogProbMetric: 27.7695

Epoch 839: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5491 - MinusLogProbMetric: 27.5491 - val_loss: 27.7695 - val_MinusLogProbMetric: 27.7695 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 840/1000
2023-10-27 02:26:37.140 
Epoch 840/1000 
	 loss: 27.5343, MinusLogProbMetric: 27.5343, val_loss: 27.8048, val_MinusLogProbMetric: 27.8048

Epoch 840: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5343 - MinusLogProbMetric: 27.5343 - val_loss: 27.8048 - val_MinusLogProbMetric: 27.8048 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 841/1000
2023-10-27 02:27:12.441 
Epoch 841/1000 
	 loss: 27.5574, MinusLogProbMetric: 27.5574, val_loss: 27.7931, val_MinusLogProbMetric: 27.7931

Epoch 841: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5574 - MinusLogProbMetric: 27.5574 - val_loss: 27.7931 - val_MinusLogProbMetric: 27.7931 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 842/1000
2023-10-27 02:27:47.546 
Epoch 842/1000 
	 loss: 27.5394, MinusLogProbMetric: 27.5394, val_loss: 27.7818, val_MinusLogProbMetric: 27.7818

Epoch 842: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5394 - MinusLogProbMetric: 27.5394 - val_loss: 27.7818 - val_MinusLogProbMetric: 27.7818 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 843/1000
2023-10-27 02:28:22.127 
Epoch 843/1000 
	 loss: 27.5357, MinusLogProbMetric: 27.5357, val_loss: 27.7629, val_MinusLogProbMetric: 27.7629

Epoch 843: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5357 - MinusLogProbMetric: 27.5357 - val_loss: 27.7629 - val_MinusLogProbMetric: 27.7629 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 844/1000
2023-10-27 02:28:57.174 
Epoch 844/1000 
	 loss: 27.5379, MinusLogProbMetric: 27.5379, val_loss: 27.7969, val_MinusLogProbMetric: 27.7969

Epoch 844: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5379 - MinusLogProbMetric: 27.5379 - val_loss: 27.7969 - val_MinusLogProbMetric: 27.7969 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 845/1000
2023-10-27 02:29:32.383 
Epoch 845/1000 
	 loss: 27.5539, MinusLogProbMetric: 27.5539, val_loss: 27.7744, val_MinusLogProbMetric: 27.7744

Epoch 845: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5539 - MinusLogProbMetric: 27.5539 - val_loss: 27.7744 - val_MinusLogProbMetric: 27.7744 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 846/1000
2023-10-27 02:30:07.456 
Epoch 846/1000 
	 loss: 27.5473, MinusLogProbMetric: 27.5473, val_loss: 27.7535, val_MinusLogProbMetric: 27.7535

Epoch 846: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5473 - MinusLogProbMetric: 27.5473 - val_loss: 27.7535 - val_MinusLogProbMetric: 27.7535 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 847/1000
2023-10-27 02:30:42.592 
Epoch 847/1000 
	 loss: 27.5425, MinusLogProbMetric: 27.5425, val_loss: 27.8207, val_MinusLogProbMetric: 27.8207

Epoch 847: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5425 - MinusLogProbMetric: 27.5425 - val_loss: 27.8207 - val_MinusLogProbMetric: 27.8207 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 848/1000
2023-10-27 02:31:17.607 
Epoch 848/1000 
	 loss: 27.5276, MinusLogProbMetric: 27.5276, val_loss: 27.7649, val_MinusLogProbMetric: 27.7649

Epoch 848: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5276 - MinusLogProbMetric: 27.5276 - val_loss: 27.7649 - val_MinusLogProbMetric: 27.7649 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 849/1000
2023-10-27 02:31:52.704 
Epoch 849/1000 
	 loss: 27.5395, MinusLogProbMetric: 27.5395, val_loss: 27.7882, val_MinusLogProbMetric: 27.7882

Epoch 849: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5395 - MinusLogProbMetric: 27.5395 - val_loss: 27.7882 - val_MinusLogProbMetric: 27.7882 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 850/1000
2023-10-27 02:32:27.512 
Epoch 850/1000 
	 loss: 27.5432, MinusLogProbMetric: 27.5432, val_loss: 27.8032, val_MinusLogProbMetric: 27.8032

Epoch 850: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5432 - MinusLogProbMetric: 27.5432 - val_loss: 27.8032 - val_MinusLogProbMetric: 27.8032 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 851/1000
2023-10-27 02:33:02.263 
Epoch 851/1000 
	 loss: 27.5356, MinusLogProbMetric: 27.5356, val_loss: 27.7751, val_MinusLogProbMetric: 27.7751

Epoch 851: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5356 - MinusLogProbMetric: 27.5356 - val_loss: 27.7751 - val_MinusLogProbMetric: 27.7751 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 852/1000
2023-10-27 02:33:37.188 
Epoch 852/1000 
	 loss: 27.5501, MinusLogProbMetric: 27.5501, val_loss: 27.7725, val_MinusLogProbMetric: 27.7725

Epoch 852: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5501 - MinusLogProbMetric: 27.5501 - val_loss: 27.7725 - val_MinusLogProbMetric: 27.7725 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 853/1000
2023-10-27 02:34:12.089 
Epoch 853/1000 
	 loss: 27.5377, MinusLogProbMetric: 27.5377, val_loss: 27.7885, val_MinusLogProbMetric: 27.7885

Epoch 853: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5377 - MinusLogProbMetric: 27.5377 - val_loss: 27.7885 - val_MinusLogProbMetric: 27.7885 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 854/1000
2023-10-27 02:34:47.282 
Epoch 854/1000 
	 loss: 27.5439, MinusLogProbMetric: 27.5439, val_loss: 27.7723, val_MinusLogProbMetric: 27.7723

Epoch 854: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5439 - MinusLogProbMetric: 27.5439 - val_loss: 27.7723 - val_MinusLogProbMetric: 27.7723 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 855/1000
2023-10-27 02:35:22.432 
Epoch 855/1000 
	 loss: 27.5343, MinusLogProbMetric: 27.5343, val_loss: 27.8875, val_MinusLogProbMetric: 27.8875

Epoch 855: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5343 - MinusLogProbMetric: 27.5343 - val_loss: 27.8875 - val_MinusLogProbMetric: 27.8875 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 856/1000
2023-10-27 02:35:57.511 
Epoch 856/1000 
	 loss: 27.5357, MinusLogProbMetric: 27.5357, val_loss: 27.8065, val_MinusLogProbMetric: 27.8065

Epoch 856: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5357 - MinusLogProbMetric: 27.5357 - val_loss: 27.8065 - val_MinusLogProbMetric: 27.8065 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 857/1000
2023-10-27 02:36:32.627 
Epoch 857/1000 
	 loss: 27.5471, MinusLogProbMetric: 27.5471, val_loss: 27.7987, val_MinusLogProbMetric: 27.7987

Epoch 857: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5471 - MinusLogProbMetric: 27.5471 - val_loss: 27.7987 - val_MinusLogProbMetric: 27.7987 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 858/1000
2023-10-27 02:37:07.364 
Epoch 858/1000 
	 loss: 27.5431, MinusLogProbMetric: 27.5431, val_loss: 27.8403, val_MinusLogProbMetric: 27.8403

Epoch 858: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5431 - MinusLogProbMetric: 27.5431 - val_loss: 27.8403 - val_MinusLogProbMetric: 27.8403 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 859/1000
2023-10-27 02:37:42.080 
Epoch 859/1000 
	 loss: 27.5525, MinusLogProbMetric: 27.5525, val_loss: 27.7636, val_MinusLogProbMetric: 27.7636

Epoch 859: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5525 - MinusLogProbMetric: 27.5525 - val_loss: 27.7636 - val_MinusLogProbMetric: 27.7636 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 860/1000
2023-10-27 02:38:17.061 
Epoch 860/1000 
	 loss: 27.5392, MinusLogProbMetric: 27.5392, val_loss: 27.7843, val_MinusLogProbMetric: 27.7843

Epoch 860: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5392 - MinusLogProbMetric: 27.5392 - val_loss: 27.7843 - val_MinusLogProbMetric: 27.7843 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 861/1000
2023-10-27 02:38:52.056 
Epoch 861/1000 
	 loss: 27.5035, MinusLogProbMetric: 27.5035, val_loss: 27.7585, val_MinusLogProbMetric: 27.7585

Epoch 861: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5035 - MinusLogProbMetric: 27.5035 - val_loss: 27.7585 - val_MinusLogProbMetric: 27.7585 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 862/1000
2023-10-27 02:39:27.062 
Epoch 862/1000 
	 loss: 27.5008, MinusLogProbMetric: 27.5008, val_loss: 27.7834, val_MinusLogProbMetric: 27.7834

Epoch 862: val_loss did not improve from 27.74889
196/196 - 35s - loss: 27.5008 - MinusLogProbMetric: 27.5008 - val_loss: 27.7834 - val_MinusLogProbMetric: 27.7834 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 863/1000
2023-10-27 02:40:01.952 
Epoch 863/1000 
	 loss: 27.5028, MinusLogProbMetric: 27.5028, val_loss: 27.7357, val_MinusLogProbMetric: 27.7357

Epoch 863: val_loss improved from 27.74889 to 27.73572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.5028 - MinusLogProbMetric: 27.5028 - val_loss: 27.7357 - val_MinusLogProbMetric: 27.7357 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 864/1000
2023-10-27 02:40:37.923 
Epoch 864/1000 
	 loss: 27.5035, MinusLogProbMetric: 27.5035, val_loss: 27.7688, val_MinusLogProbMetric: 27.7688

Epoch 864: val_loss did not improve from 27.73572
196/196 - 35s - loss: 27.5035 - MinusLogProbMetric: 27.5035 - val_loss: 27.7688 - val_MinusLogProbMetric: 27.7688 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 865/1000
2023-10-27 02:41:12.600 
Epoch 865/1000 
	 loss: 27.5026, MinusLogProbMetric: 27.5026, val_loss: 27.7433, val_MinusLogProbMetric: 27.7433

Epoch 865: val_loss did not improve from 27.73572
196/196 - 35s - loss: 27.5026 - MinusLogProbMetric: 27.5026 - val_loss: 27.7433 - val_MinusLogProbMetric: 27.7433 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 866/1000
2023-10-27 02:41:47.558 
Epoch 866/1000 
	 loss: 27.4996, MinusLogProbMetric: 27.4996, val_loss: 27.7345, val_MinusLogProbMetric: 27.7345

Epoch 866: val_loss improved from 27.73572 to 27.73448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 36s - loss: 27.4996 - MinusLogProbMetric: 27.4996 - val_loss: 27.7345 - val_MinusLogProbMetric: 27.7345 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 867/1000
2023-10-27 02:42:23.334 
Epoch 867/1000 
	 loss: 27.5034, MinusLogProbMetric: 27.5034, val_loss: 27.7553, val_MinusLogProbMetric: 27.7553

Epoch 867: val_loss did not improve from 27.73448
196/196 - 35s - loss: 27.5034 - MinusLogProbMetric: 27.5034 - val_loss: 27.7553 - val_MinusLogProbMetric: 27.7553 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 868/1000
2023-10-27 02:42:58.294 
Epoch 868/1000 
	 loss: 27.5014, MinusLogProbMetric: 27.5014, val_loss: 27.7521, val_MinusLogProbMetric: 27.7521

Epoch 868: val_loss did not improve from 27.73448
196/196 - 35s - loss: 27.5014 - MinusLogProbMetric: 27.5014 - val_loss: 27.7521 - val_MinusLogProbMetric: 27.7521 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 869/1000
2023-10-27 02:43:33.218 
Epoch 869/1000 
	 loss: 27.5066, MinusLogProbMetric: 27.5066, val_loss: 27.7414, val_MinusLogProbMetric: 27.7414

Epoch 869: val_loss did not improve from 27.73448
196/196 - 35s - loss: 27.5066 - MinusLogProbMetric: 27.5066 - val_loss: 27.7414 - val_MinusLogProbMetric: 27.7414 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 870/1000
2023-10-27 02:44:08.118 
Epoch 870/1000 
	 loss: 27.5015, MinusLogProbMetric: 27.5015, val_loss: 27.7341, val_MinusLogProbMetric: 27.7341

Epoch 870: val_loss improved from 27.73448 to 27.73414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.5015 - MinusLogProbMetric: 27.5015 - val_loss: 27.7341 - val_MinusLogProbMetric: 27.7341 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 871/1000
2023-10-27 02:44:43.691 
Epoch 871/1000 
	 loss: 27.5008, MinusLogProbMetric: 27.5008, val_loss: 27.7421, val_MinusLogProbMetric: 27.7421

Epoch 871: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5008 - MinusLogProbMetric: 27.5008 - val_loss: 27.7421 - val_MinusLogProbMetric: 27.7421 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 872/1000
2023-10-27 02:45:18.712 
Epoch 872/1000 
	 loss: 27.5057, MinusLogProbMetric: 27.5057, val_loss: 27.7491, val_MinusLogProbMetric: 27.7491

Epoch 872: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5057 - MinusLogProbMetric: 27.5057 - val_loss: 27.7491 - val_MinusLogProbMetric: 27.7491 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 873/1000
2023-10-27 02:45:53.545 
Epoch 873/1000 
	 loss: 27.5007, MinusLogProbMetric: 27.5007, val_loss: 27.7384, val_MinusLogProbMetric: 27.7384

Epoch 873: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5007 - MinusLogProbMetric: 27.5007 - val_loss: 27.7384 - val_MinusLogProbMetric: 27.7384 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 874/1000
2023-10-27 02:46:28.662 
Epoch 874/1000 
	 loss: 27.5038, MinusLogProbMetric: 27.5038, val_loss: 27.7758, val_MinusLogProbMetric: 27.7758

Epoch 874: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5038 - MinusLogProbMetric: 27.5038 - val_loss: 27.7758 - val_MinusLogProbMetric: 27.7758 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 875/1000
2023-10-27 02:47:03.879 
Epoch 875/1000 
	 loss: 27.5013, MinusLogProbMetric: 27.5013, val_loss: 27.7450, val_MinusLogProbMetric: 27.7450

Epoch 875: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5013 - MinusLogProbMetric: 27.5013 - val_loss: 27.7450 - val_MinusLogProbMetric: 27.7450 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 876/1000
2023-10-27 02:47:38.727 
Epoch 876/1000 
	 loss: 27.4979, MinusLogProbMetric: 27.4979, val_loss: 27.7586, val_MinusLogProbMetric: 27.7586

Epoch 876: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.4979 - MinusLogProbMetric: 27.4979 - val_loss: 27.7586 - val_MinusLogProbMetric: 27.7586 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 877/1000
2023-10-27 02:48:13.694 
Epoch 877/1000 
	 loss: 27.5037, MinusLogProbMetric: 27.5037, val_loss: 27.7453, val_MinusLogProbMetric: 27.7453

Epoch 877: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5037 - MinusLogProbMetric: 27.5037 - val_loss: 27.7453 - val_MinusLogProbMetric: 27.7453 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 878/1000
2023-10-27 02:48:48.451 
Epoch 878/1000 
	 loss: 27.5044, MinusLogProbMetric: 27.5044, val_loss: 27.7702, val_MinusLogProbMetric: 27.7702

Epoch 878: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5044 - MinusLogProbMetric: 27.5044 - val_loss: 27.7702 - val_MinusLogProbMetric: 27.7702 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 879/1000
2023-10-27 02:49:23.425 
Epoch 879/1000 
	 loss: 27.5012, MinusLogProbMetric: 27.5012, val_loss: 27.7461, val_MinusLogProbMetric: 27.7461

Epoch 879: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.5012 - MinusLogProbMetric: 27.5012 - val_loss: 27.7461 - val_MinusLogProbMetric: 27.7461 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 880/1000
2023-10-27 02:49:58.059 
Epoch 880/1000 
	 loss: 27.4998, MinusLogProbMetric: 27.4998, val_loss: 27.7368, val_MinusLogProbMetric: 27.7368

Epoch 880: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.4998 - MinusLogProbMetric: 27.4998 - val_loss: 27.7368 - val_MinusLogProbMetric: 27.7368 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 881/1000
2023-10-27 02:50:30.574 
Epoch 881/1000 
	 loss: 27.4979, MinusLogProbMetric: 27.4979, val_loss: 27.7642, val_MinusLogProbMetric: 27.7642

Epoch 881: val_loss did not improve from 27.73414
196/196 - 33s - loss: 27.4979 - MinusLogProbMetric: 27.4979 - val_loss: 27.7642 - val_MinusLogProbMetric: 27.7642 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 882/1000
2023-10-27 02:51:03.355 
Epoch 882/1000 
	 loss: 27.5014, MinusLogProbMetric: 27.5014, val_loss: 27.7559, val_MinusLogProbMetric: 27.7559

Epoch 882: val_loss did not improve from 27.73414
196/196 - 33s - loss: 27.5014 - MinusLogProbMetric: 27.5014 - val_loss: 27.7559 - val_MinusLogProbMetric: 27.7559 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 883/1000
2023-10-27 02:51:33.253 
Epoch 883/1000 
	 loss: 27.5025, MinusLogProbMetric: 27.5025, val_loss: 27.7400, val_MinusLogProbMetric: 27.7400

Epoch 883: val_loss did not improve from 27.73414
196/196 - 30s - loss: 27.5025 - MinusLogProbMetric: 27.5025 - val_loss: 27.7400 - val_MinusLogProbMetric: 27.7400 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 884/1000
2023-10-27 02:52:02.482 
Epoch 884/1000 
	 loss: 27.5015, MinusLogProbMetric: 27.5015, val_loss: 27.7447, val_MinusLogProbMetric: 27.7447

Epoch 884: val_loss did not improve from 27.73414
196/196 - 29s - loss: 27.5015 - MinusLogProbMetric: 27.5015 - val_loss: 27.7447 - val_MinusLogProbMetric: 27.7447 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 885/1000
2023-10-27 02:52:32.373 
Epoch 885/1000 
	 loss: 27.5022, MinusLogProbMetric: 27.5022, val_loss: 27.7385, val_MinusLogProbMetric: 27.7385

Epoch 885: val_loss did not improve from 27.73414
196/196 - 30s - loss: 27.5022 - MinusLogProbMetric: 27.5022 - val_loss: 27.7385 - val_MinusLogProbMetric: 27.7385 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 886/1000
2023-10-27 02:53:02.362 
Epoch 886/1000 
	 loss: 27.5028, MinusLogProbMetric: 27.5028, val_loss: 27.7387, val_MinusLogProbMetric: 27.7387

Epoch 886: val_loss did not improve from 27.73414
196/196 - 30s - loss: 27.5028 - MinusLogProbMetric: 27.5028 - val_loss: 27.7387 - val_MinusLogProbMetric: 27.7387 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 887/1000
2023-10-27 02:53:37.420 
Epoch 887/1000 
	 loss: 27.4999, MinusLogProbMetric: 27.4999, val_loss: 27.7453, val_MinusLogProbMetric: 27.7453

Epoch 887: val_loss did not improve from 27.73414
196/196 - 35s - loss: 27.4999 - MinusLogProbMetric: 27.4999 - val_loss: 27.7453 - val_MinusLogProbMetric: 27.7453 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 888/1000
2023-10-27 02:54:11.752 
Epoch 888/1000 
	 loss: 27.4995, MinusLogProbMetric: 27.4995, val_loss: 27.7601, val_MinusLogProbMetric: 27.7601

Epoch 888: val_loss did not improve from 27.73414
196/196 - 34s - loss: 27.4995 - MinusLogProbMetric: 27.4995 - val_loss: 27.7601 - val_MinusLogProbMetric: 27.7601 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 889/1000
2023-10-27 02:54:41.558 
Epoch 889/1000 
	 loss: 27.5029, MinusLogProbMetric: 27.5029, val_loss: 27.7490, val_MinusLogProbMetric: 27.7490

Epoch 889: val_loss did not improve from 27.73414
196/196 - 30s - loss: 27.5029 - MinusLogProbMetric: 27.5029 - val_loss: 27.7490 - val_MinusLogProbMetric: 27.7490 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 890/1000
2023-10-27 02:55:12.373 
Epoch 890/1000 
	 loss: 27.4981, MinusLogProbMetric: 27.4981, val_loss: 27.7380, val_MinusLogProbMetric: 27.7380

Epoch 890: val_loss did not improve from 27.73414
196/196 - 31s - loss: 27.4981 - MinusLogProbMetric: 27.4981 - val_loss: 27.7380 - val_MinusLogProbMetric: 27.7380 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 891/1000
2023-10-27 02:55:42.855 
Epoch 891/1000 
	 loss: 27.4999, MinusLogProbMetric: 27.4999, val_loss: 27.8059, val_MinusLogProbMetric: 27.8059

Epoch 891: val_loss did not improve from 27.73414
196/196 - 30s - loss: 27.4999 - MinusLogProbMetric: 27.4999 - val_loss: 27.8059 - val_MinusLogProbMetric: 27.8059 - lr: 6.2500e-05 - 30s/epoch - 156ms/step
Epoch 892/1000
2023-10-27 02:56:15.111 
Epoch 892/1000 
	 loss: 27.5040, MinusLogProbMetric: 27.5040, val_loss: 27.7822, val_MinusLogProbMetric: 27.7822

Epoch 892: val_loss did not improve from 27.73414
196/196 - 32s - loss: 27.5040 - MinusLogProbMetric: 27.5040 - val_loss: 27.7822 - val_MinusLogProbMetric: 27.7822 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 893/1000
2023-10-27 02:56:48.772 
Epoch 893/1000 
	 loss: 27.5023, MinusLogProbMetric: 27.5023, val_loss: 27.7322, val_MinusLogProbMetric: 27.7322

Epoch 893: val_loss improved from 27.73414 to 27.73217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 34s - loss: 27.5023 - MinusLogProbMetric: 27.5023 - val_loss: 27.7322 - val_MinusLogProbMetric: 27.7322 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 894/1000
2023-10-27 02:57:21.530 
Epoch 894/1000 
	 loss: 27.5001, MinusLogProbMetric: 27.5001, val_loss: 27.7545, val_MinusLogProbMetric: 27.7545

Epoch 894: val_loss did not improve from 27.73217
196/196 - 32s - loss: 27.5001 - MinusLogProbMetric: 27.5001 - val_loss: 27.7545 - val_MinusLogProbMetric: 27.7545 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 895/1000
2023-10-27 02:57:51.340 
Epoch 895/1000 
	 loss: 27.4998, MinusLogProbMetric: 27.4998, val_loss: 27.8146, val_MinusLogProbMetric: 27.8146

Epoch 895: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4998 - MinusLogProbMetric: 27.4998 - val_loss: 27.8146 - val_MinusLogProbMetric: 27.8146 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 896/1000
2023-10-27 02:58:24.455 
Epoch 896/1000 
	 loss: 27.5026, MinusLogProbMetric: 27.5026, val_loss: 27.7343, val_MinusLogProbMetric: 27.7343

Epoch 896: val_loss did not improve from 27.73217
196/196 - 33s - loss: 27.5026 - MinusLogProbMetric: 27.5026 - val_loss: 27.7343 - val_MinusLogProbMetric: 27.7343 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 897/1000
2023-10-27 02:58:56.757 
Epoch 897/1000 
	 loss: 27.5017, MinusLogProbMetric: 27.5017, val_loss: 27.7479, val_MinusLogProbMetric: 27.7479

Epoch 897: val_loss did not improve from 27.73217
196/196 - 32s - loss: 27.5017 - MinusLogProbMetric: 27.5017 - val_loss: 27.7479 - val_MinusLogProbMetric: 27.7479 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 898/1000
2023-10-27 02:59:30.370 
Epoch 898/1000 
	 loss: 27.5009, MinusLogProbMetric: 27.5009, val_loss: 27.7704, val_MinusLogProbMetric: 27.7704

Epoch 898: val_loss did not improve from 27.73217
196/196 - 34s - loss: 27.5009 - MinusLogProbMetric: 27.5009 - val_loss: 27.7704 - val_MinusLogProbMetric: 27.7704 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 899/1000
2023-10-27 03:00:00.802 
Epoch 899/1000 
	 loss: 27.4994, MinusLogProbMetric: 27.4994, val_loss: 27.7605, val_MinusLogProbMetric: 27.7605

Epoch 899: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4994 - MinusLogProbMetric: 27.4994 - val_loss: 27.7605 - val_MinusLogProbMetric: 27.7605 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 900/1000
2023-10-27 03:00:30.200 
Epoch 900/1000 
	 loss: 27.5044, MinusLogProbMetric: 27.5044, val_loss: 27.7443, val_MinusLogProbMetric: 27.7443

Epoch 900: val_loss did not improve from 27.73217
196/196 - 29s - loss: 27.5044 - MinusLogProbMetric: 27.5044 - val_loss: 27.7443 - val_MinusLogProbMetric: 27.7443 - lr: 6.2500e-05 - 29s/epoch - 150ms/step
Epoch 901/1000
2023-10-27 03:01:00.405 
Epoch 901/1000 
	 loss: 27.5057, MinusLogProbMetric: 27.5057, val_loss: 27.7528, val_MinusLogProbMetric: 27.7528

Epoch 901: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.5057 - MinusLogProbMetric: 27.5057 - val_loss: 27.7528 - val_MinusLogProbMetric: 27.7528 - lr: 6.2500e-05 - 30s/epoch - 154ms/step
Epoch 902/1000
2023-10-27 03:01:32.028 
Epoch 902/1000 
	 loss: 27.5013, MinusLogProbMetric: 27.5013, val_loss: 27.7457, val_MinusLogProbMetric: 27.7457

Epoch 902: val_loss did not improve from 27.73217
196/196 - 32s - loss: 27.5013 - MinusLogProbMetric: 27.5013 - val_loss: 27.7457 - val_MinusLogProbMetric: 27.7457 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 903/1000
2023-10-27 03:02:05.862 
Epoch 903/1000 
	 loss: 27.4958, MinusLogProbMetric: 27.4958, val_loss: 27.7428, val_MinusLogProbMetric: 27.7428

Epoch 903: val_loss did not improve from 27.73217
196/196 - 34s - loss: 27.4958 - MinusLogProbMetric: 27.4958 - val_loss: 27.7428 - val_MinusLogProbMetric: 27.7428 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 904/1000
2023-10-27 03:02:37.130 
Epoch 904/1000 
	 loss: 27.4974, MinusLogProbMetric: 27.4974, val_loss: 27.7658, val_MinusLogProbMetric: 27.7658

Epoch 904: val_loss did not improve from 27.73217
196/196 - 31s - loss: 27.4974 - MinusLogProbMetric: 27.4974 - val_loss: 27.7658 - val_MinusLogProbMetric: 27.7658 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 905/1000
2023-10-27 03:03:08.863 
Epoch 905/1000 
	 loss: 27.4989, MinusLogProbMetric: 27.4989, val_loss: 27.7568, val_MinusLogProbMetric: 27.7568

Epoch 905: val_loss did not improve from 27.73217
196/196 - 32s - loss: 27.4989 - MinusLogProbMetric: 27.4989 - val_loss: 27.7568 - val_MinusLogProbMetric: 27.7568 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 906/1000
2023-10-27 03:03:38.507 
Epoch 906/1000 
	 loss: 27.4987, MinusLogProbMetric: 27.4987, val_loss: 27.7439, val_MinusLogProbMetric: 27.7439

Epoch 906: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4987 - MinusLogProbMetric: 27.4987 - val_loss: 27.7439 - val_MinusLogProbMetric: 27.7439 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 907/1000
2023-10-27 03:04:12.229 
Epoch 907/1000 
	 loss: 27.4978, MinusLogProbMetric: 27.4978, val_loss: 27.7375, val_MinusLogProbMetric: 27.7375

Epoch 907: val_loss did not improve from 27.73217
196/196 - 34s - loss: 27.4978 - MinusLogProbMetric: 27.4978 - val_loss: 27.7375 - val_MinusLogProbMetric: 27.7375 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 908/1000
2023-10-27 03:04:45.049 
Epoch 908/1000 
	 loss: 27.4989, MinusLogProbMetric: 27.4989, val_loss: 27.7567, val_MinusLogProbMetric: 27.7567

Epoch 908: val_loss did not improve from 27.73217
196/196 - 33s - loss: 27.4989 - MinusLogProbMetric: 27.4989 - val_loss: 27.7567 - val_MinusLogProbMetric: 27.7567 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 909/1000
2023-10-27 03:05:14.626 
Epoch 909/1000 
	 loss: 27.4959, MinusLogProbMetric: 27.4959, val_loss: 27.7466, val_MinusLogProbMetric: 27.7466

Epoch 909: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4959 - MinusLogProbMetric: 27.4959 - val_loss: 27.7466 - val_MinusLogProbMetric: 27.7466 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 910/1000
2023-10-27 03:05:43.781 
Epoch 910/1000 
	 loss: 27.4951, MinusLogProbMetric: 27.4951, val_loss: 27.7563, val_MinusLogProbMetric: 27.7563

Epoch 910: val_loss did not improve from 27.73217
196/196 - 29s - loss: 27.4951 - MinusLogProbMetric: 27.4951 - val_loss: 27.7563 - val_MinusLogProbMetric: 27.7563 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 911/1000
2023-10-27 03:06:15.668 
Epoch 911/1000 
	 loss: 27.4985, MinusLogProbMetric: 27.4985, val_loss: 27.7445, val_MinusLogProbMetric: 27.7445

Epoch 911: val_loss did not improve from 27.73217
196/196 - 32s - loss: 27.4985 - MinusLogProbMetric: 27.4985 - val_loss: 27.7445 - val_MinusLogProbMetric: 27.7445 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 912/1000
2023-10-27 03:06:50.312 
Epoch 912/1000 
	 loss: 27.4986, MinusLogProbMetric: 27.4986, val_loss: 27.7398, val_MinusLogProbMetric: 27.7398

Epoch 912: val_loss did not improve from 27.73217
196/196 - 35s - loss: 27.4986 - MinusLogProbMetric: 27.4986 - val_loss: 27.7398 - val_MinusLogProbMetric: 27.7398 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 913/1000
2023-10-27 03:07:23.542 
Epoch 913/1000 
	 loss: 27.4967, MinusLogProbMetric: 27.4967, val_loss: 27.7428, val_MinusLogProbMetric: 27.7428

Epoch 913: val_loss did not improve from 27.73217
196/196 - 33s - loss: 27.4967 - MinusLogProbMetric: 27.4967 - val_loss: 27.7428 - val_MinusLogProbMetric: 27.7428 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 914/1000
2023-10-27 03:07:53.283 
Epoch 914/1000 
	 loss: 27.4956, MinusLogProbMetric: 27.4956, val_loss: 27.7415, val_MinusLogProbMetric: 27.7415

Epoch 914: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4956 - MinusLogProbMetric: 27.4956 - val_loss: 27.7415 - val_MinusLogProbMetric: 27.7415 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 915/1000
2023-10-27 03:08:22.996 
Epoch 915/1000 
	 loss: 27.4960, MinusLogProbMetric: 27.4960, val_loss: 27.7418, val_MinusLogProbMetric: 27.7418

Epoch 915: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4960 - MinusLogProbMetric: 27.4960 - val_loss: 27.7418 - val_MinusLogProbMetric: 27.7418 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 916/1000
2023-10-27 03:08:54.006 
Epoch 916/1000 
	 loss: 27.5014, MinusLogProbMetric: 27.5014, val_loss: 27.7539, val_MinusLogProbMetric: 27.7539

Epoch 916: val_loss did not improve from 27.73217
196/196 - 31s - loss: 27.5014 - MinusLogProbMetric: 27.5014 - val_loss: 27.7539 - val_MinusLogProbMetric: 27.7539 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 917/1000
2023-10-27 03:09:26.852 
Epoch 917/1000 
	 loss: 27.4987, MinusLogProbMetric: 27.4987, val_loss: 27.7396, val_MinusLogProbMetric: 27.7396

Epoch 917: val_loss did not improve from 27.73217
196/196 - 33s - loss: 27.4987 - MinusLogProbMetric: 27.4987 - val_loss: 27.7396 - val_MinusLogProbMetric: 27.7396 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 918/1000
2023-10-27 03:10:00.022 
Epoch 918/1000 
	 loss: 27.4989, MinusLogProbMetric: 27.4989, val_loss: 27.7431, val_MinusLogProbMetric: 27.7431

Epoch 918: val_loss did not improve from 27.73217
196/196 - 33s - loss: 27.4989 - MinusLogProbMetric: 27.4989 - val_loss: 27.7431 - val_MinusLogProbMetric: 27.7431 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 919/1000
2023-10-27 03:10:30.917 
Epoch 919/1000 
	 loss: 27.4968, MinusLogProbMetric: 27.4968, val_loss: 27.7629, val_MinusLogProbMetric: 27.7629

Epoch 919: val_loss did not improve from 27.73217
196/196 - 31s - loss: 27.4968 - MinusLogProbMetric: 27.4968 - val_loss: 27.7629 - val_MinusLogProbMetric: 27.7629 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 920/1000
2023-10-27 03:11:00.447 
Epoch 920/1000 
	 loss: 27.4962, MinusLogProbMetric: 27.4962, val_loss: 27.7506, val_MinusLogProbMetric: 27.7506

Epoch 920: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4962 - MinusLogProbMetric: 27.4962 - val_loss: 27.7506 - val_MinusLogProbMetric: 27.7506 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 921/1000
2023-10-27 03:11:34.057 
Epoch 921/1000 
	 loss: 27.4997, MinusLogProbMetric: 27.4997, val_loss: 27.7435, val_MinusLogProbMetric: 27.7435

Epoch 921: val_loss did not improve from 27.73217
196/196 - 34s - loss: 27.4997 - MinusLogProbMetric: 27.4997 - val_loss: 27.7435 - val_MinusLogProbMetric: 27.7435 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 922/1000
2023-10-27 03:12:07.168 
Epoch 922/1000 
	 loss: 27.4985, MinusLogProbMetric: 27.4985, val_loss: 27.7816, val_MinusLogProbMetric: 27.7816

Epoch 922: val_loss did not improve from 27.73217
196/196 - 33s - loss: 27.4985 - MinusLogProbMetric: 27.4985 - val_loss: 27.7816 - val_MinusLogProbMetric: 27.7816 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 923/1000
2023-10-27 03:12:36.867 
Epoch 923/1000 
	 loss: 27.4996, MinusLogProbMetric: 27.4996, val_loss: 27.7338, val_MinusLogProbMetric: 27.7338

Epoch 923: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.4996 - MinusLogProbMetric: 27.4996 - val_loss: 27.7338 - val_MinusLogProbMetric: 27.7338 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 924/1000
2023-10-27 03:13:06.435 
Epoch 924/1000 
	 loss: 27.5003, MinusLogProbMetric: 27.5003, val_loss: 27.7405, val_MinusLogProbMetric: 27.7405

Epoch 924: val_loss did not improve from 27.73217
196/196 - 30s - loss: 27.5003 - MinusLogProbMetric: 27.5003 - val_loss: 27.7405 - val_MinusLogProbMetric: 27.7405 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 925/1000
2023-10-27 03:13:35.884 
Epoch 925/1000 
	 loss: 27.5001, MinusLogProbMetric: 27.5001, val_loss: 27.7322, val_MinusLogProbMetric: 27.7322

Epoch 925: val_loss improved from 27.73217 to 27.73215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 30s - loss: 27.5001 - MinusLogProbMetric: 27.5001 - val_loss: 27.7322 - val_MinusLogProbMetric: 27.7322 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 926/1000
2023-10-27 03:14:10.907 
Epoch 926/1000 
	 loss: 27.4991, MinusLogProbMetric: 27.4991, val_loss: 27.7416, val_MinusLogProbMetric: 27.7416

Epoch 926: val_loss did not improve from 27.73215
196/196 - 35s - loss: 27.4991 - MinusLogProbMetric: 27.4991 - val_loss: 27.7416 - val_MinusLogProbMetric: 27.7416 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 927/1000
2023-10-27 03:14:43.267 
Epoch 927/1000 
	 loss: 27.4986, MinusLogProbMetric: 27.4986, val_loss: 27.7392, val_MinusLogProbMetric: 27.7392

Epoch 927: val_loss did not improve from 27.73215
196/196 - 32s - loss: 27.4986 - MinusLogProbMetric: 27.4986 - val_loss: 27.7392 - val_MinusLogProbMetric: 27.7392 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 928/1000
2023-10-27 03:15:12.814 
Epoch 928/1000 
	 loss: 27.4996, MinusLogProbMetric: 27.4996, val_loss: 27.7521, val_MinusLogProbMetric: 27.7521

Epoch 928: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.4996 - MinusLogProbMetric: 27.4996 - val_loss: 27.7521 - val_MinusLogProbMetric: 27.7521 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 929/1000
2023-10-27 03:15:42.095 
Epoch 929/1000 
	 loss: 27.4993, MinusLogProbMetric: 27.4993, val_loss: 27.7459, val_MinusLogProbMetric: 27.7459

Epoch 929: val_loss did not improve from 27.73215
196/196 - 29s - loss: 27.4993 - MinusLogProbMetric: 27.4993 - val_loss: 27.7459 - val_MinusLogProbMetric: 27.7459 - lr: 6.2500e-05 - 29s/epoch - 149ms/step
Epoch 930/1000
2023-10-27 03:16:12.503 
Epoch 930/1000 
	 loss: 27.4984, MinusLogProbMetric: 27.4984, val_loss: 27.7491, val_MinusLogProbMetric: 27.7491

Epoch 930: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.4984 - MinusLogProbMetric: 27.4984 - val_loss: 27.7491 - val_MinusLogProbMetric: 27.7491 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 931/1000
2023-10-27 03:16:46.528 
Epoch 931/1000 
	 loss: 27.4988, MinusLogProbMetric: 27.4988, val_loss: 27.7466, val_MinusLogProbMetric: 27.7466

Epoch 931: val_loss did not improve from 27.73215
196/196 - 34s - loss: 27.4988 - MinusLogProbMetric: 27.4988 - val_loss: 27.7466 - val_MinusLogProbMetric: 27.7466 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 932/1000
2023-10-27 03:17:17.008 
Epoch 932/1000 
	 loss: 27.4976, MinusLogProbMetric: 27.4976, val_loss: 27.7635, val_MinusLogProbMetric: 27.7635

Epoch 932: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.4976 - MinusLogProbMetric: 27.4976 - val_loss: 27.7635 - val_MinusLogProbMetric: 27.7635 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 933/1000
2023-10-27 03:17:46.729 
Epoch 933/1000 
	 loss: 27.4971, MinusLogProbMetric: 27.4971, val_loss: 27.7481, val_MinusLogProbMetric: 27.7481

Epoch 933: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.4971 - MinusLogProbMetric: 27.4971 - val_loss: 27.7481 - val_MinusLogProbMetric: 27.7481 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 934/1000
2023-10-27 03:18:16.294 
Epoch 934/1000 
	 loss: 27.5027, MinusLogProbMetric: 27.5027, val_loss: 27.7588, val_MinusLogProbMetric: 27.7588

Epoch 934: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.5027 - MinusLogProbMetric: 27.5027 - val_loss: 27.7588 - val_MinusLogProbMetric: 27.7588 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 935/1000
2023-10-27 03:18:46.134 
Epoch 935/1000 
	 loss: 27.4991, MinusLogProbMetric: 27.4991, val_loss: 27.7570, val_MinusLogProbMetric: 27.7570

Epoch 935: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.4991 - MinusLogProbMetric: 27.4991 - val_loss: 27.7570 - val_MinusLogProbMetric: 27.7570 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 936/1000
2023-10-27 03:19:20.115 
Epoch 936/1000 
	 loss: 27.5067, MinusLogProbMetric: 27.5067, val_loss: 27.7399, val_MinusLogProbMetric: 27.7399

Epoch 936: val_loss did not improve from 27.73215
196/196 - 34s - loss: 27.5067 - MinusLogProbMetric: 27.5067 - val_loss: 27.7399 - val_MinusLogProbMetric: 27.7399 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 937/1000
2023-10-27 03:19:51.143 
Epoch 937/1000 
	 loss: 27.4984, MinusLogProbMetric: 27.4984, val_loss: 27.7486, val_MinusLogProbMetric: 27.7486

Epoch 937: val_loss did not improve from 27.73215
196/196 - 31s - loss: 27.4984 - MinusLogProbMetric: 27.4984 - val_loss: 27.7486 - val_MinusLogProbMetric: 27.7486 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 938/1000
2023-10-27 03:20:20.714 
Epoch 938/1000 
	 loss: 27.5007, MinusLogProbMetric: 27.5007, val_loss: 27.7466, val_MinusLogProbMetric: 27.7466

Epoch 938: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.5007 - MinusLogProbMetric: 27.5007 - val_loss: 27.7466 - val_MinusLogProbMetric: 27.7466 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 939/1000
2023-10-27 03:20:50.798 
Epoch 939/1000 
	 loss: 27.4971, MinusLogProbMetric: 27.4971, val_loss: 27.7489, val_MinusLogProbMetric: 27.7489

Epoch 939: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.4971 - MinusLogProbMetric: 27.4971 - val_loss: 27.7489 - val_MinusLogProbMetric: 27.7489 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 940/1000
2023-10-27 03:21:25.125 
Epoch 940/1000 
	 loss: 27.5012, MinusLogProbMetric: 27.5012, val_loss: 27.7863, val_MinusLogProbMetric: 27.7863

Epoch 940: val_loss did not improve from 27.73215
196/196 - 34s - loss: 27.5012 - MinusLogProbMetric: 27.5012 - val_loss: 27.7863 - val_MinusLogProbMetric: 27.7863 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 941/1000
2023-10-27 03:21:58.388 
Epoch 941/1000 
	 loss: 27.5029, MinusLogProbMetric: 27.5029, val_loss: 27.7420, val_MinusLogProbMetric: 27.7420

Epoch 941: val_loss did not improve from 27.73215
196/196 - 33s - loss: 27.5029 - MinusLogProbMetric: 27.5029 - val_loss: 27.7420 - val_MinusLogProbMetric: 27.7420 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 942/1000
2023-10-27 03:22:28.061 
Epoch 942/1000 
	 loss: 27.4965, MinusLogProbMetric: 27.4965, val_loss: 27.7362, val_MinusLogProbMetric: 27.7362

Epoch 942: val_loss did not improve from 27.73215
196/196 - 30s - loss: 27.4965 - MinusLogProbMetric: 27.4965 - val_loss: 27.7362 - val_MinusLogProbMetric: 27.7362 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 943/1000
2023-10-27 03:22:58.779 
Epoch 943/1000 
	 loss: 27.4987, MinusLogProbMetric: 27.4987, val_loss: 27.7594, val_MinusLogProbMetric: 27.7594

Epoch 943: val_loss did not improve from 27.73215
196/196 - 31s - loss: 27.4987 - MinusLogProbMetric: 27.4987 - val_loss: 27.7594 - val_MinusLogProbMetric: 27.7594 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 944/1000
2023-10-27 03:23:28.932 
Epoch 944/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 27.7271, val_MinusLogProbMetric: 27.7271

Epoch 944: val_loss improved from 27.73215 to 27.72709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 31s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 27.7271 - val_MinusLogProbMetric: 27.7271 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 945/1000
2023-10-27 03:24:02.628 
Epoch 945/1000 
	 loss: 27.4792, MinusLogProbMetric: 27.4792, val_loss: 27.7307, val_MinusLogProbMetric: 27.7307

Epoch 945: val_loss did not improve from 27.72709
196/196 - 33s - loss: 27.4792 - MinusLogProbMetric: 27.4792 - val_loss: 27.7307 - val_MinusLogProbMetric: 27.7307 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 946/1000
2023-10-27 03:24:37.389 
Epoch 946/1000 
	 loss: 27.4817, MinusLogProbMetric: 27.4817, val_loss: 27.7506, val_MinusLogProbMetric: 27.7506

Epoch 946: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4817 - MinusLogProbMetric: 27.4817 - val_loss: 27.7506 - val_MinusLogProbMetric: 27.7506 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 947/1000
2023-10-27 03:25:11.199 
Epoch 947/1000 
	 loss: 27.4842, MinusLogProbMetric: 27.4842, val_loss: 27.7401, val_MinusLogProbMetric: 27.7401

Epoch 947: val_loss did not improve from 27.72709
196/196 - 34s - loss: 27.4842 - MinusLogProbMetric: 27.4842 - val_loss: 27.7401 - val_MinusLogProbMetric: 27.7401 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 948/1000
2023-10-27 03:25:45.544 
Epoch 948/1000 
	 loss: 27.4815, MinusLogProbMetric: 27.4815, val_loss: 27.7322, val_MinusLogProbMetric: 27.7322

Epoch 948: val_loss did not improve from 27.72709
196/196 - 34s - loss: 27.4815 - MinusLogProbMetric: 27.4815 - val_loss: 27.7322 - val_MinusLogProbMetric: 27.7322 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 949/1000
2023-10-27 03:26:20.157 
Epoch 949/1000 
	 loss: 27.4828, MinusLogProbMetric: 27.4828, val_loss: 27.7335, val_MinusLogProbMetric: 27.7335

Epoch 949: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4828 - MinusLogProbMetric: 27.4828 - val_loss: 27.7335 - val_MinusLogProbMetric: 27.7335 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 950/1000
2023-10-27 03:26:50.645 
Epoch 950/1000 
	 loss: 27.4823, MinusLogProbMetric: 27.4823, val_loss: 27.7340, val_MinusLogProbMetric: 27.7340

Epoch 950: val_loss did not improve from 27.72709
196/196 - 30s - loss: 27.4823 - MinusLogProbMetric: 27.4823 - val_loss: 27.7340 - val_MinusLogProbMetric: 27.7340 - lr: 3.1250e-05 - 30s/epoch - 156ms/step
Epoch 951/1000
2023-10-27 03:27:20.398 
Epoch 951/1000 
	 loss: 27.4822, MinusLogProbMetric: 27.4822, val_loss: 27.7318, val_MinusLogProbMetric: 27.7318

Epoch 951: val_loss did not improve from 27.72709
196/196 - 30s - loss: 27.4822 - MinusLogProbMetric: 27.4822 - val_loss: 27.7318 - val_MinusLogProbMetric: 27.7318 - lr: 3.1250e-05 - 30s/epoch - 152ms/step
Epoch 952/1000
2023-10-27 03:27:51.400 
Epoch 952/1000 
	 loss: 27.4805, MinusLogProbMetric: 27.4805, val_loss: 27.7323, val_MinusLogProbMetric: 27.7323

Epoch 952: val_loss did not improve from 27.72709
196/196 - 31s - loss: 27.4805 - MinusLogProbMetric: 27.4805 - val_loss: 27.7323 - val_MinusLogProbMetric: 27.7323 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 953/1000
2023-10-27 03:28:26.476 
Epoch 953/1000 
	 loss: 27.4815, MinusLogProbMetric: 27.4815, val_loss: 27.7339, val_MinusLogProbMetric: 27.7339

Epoch 953: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4815 - MinusLogProbMetric: 27.4815 - val_loss: 27.7339 - val_MinusLogProbMetric: 27.7339 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 954/1000
2023-10-27 03:29:00.888 
Epoch 954/1000 
	 loss: 27.4829, MinusLogProbMetric: 27.4829, val_loss: 27.7334, val_MinusLogProbMetric: 27.7334

Epoch 954: val_loss did not improve from 27.72709
196/196 - 34s - loss: 27.4829 - MinusLogProbMetric: 27.4829 - val_loss: 27.7334 - val_MinusLogProbMetric: 27.7334 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 955/1000
2023-10-27 03:29:35.865 
Epoch 955/1000 
	 loss: 27.4830, MinusLogProbMetric: 27.4830, val_loss: 27.7281, val_MinusLogProbMetric: 27.7281

Epoch 955: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4830 - MinusLogProbMetric: 27.4830 - val_loss: 27.7281 - val_MinusLogProbMetric: 27.7281 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 956/1000
2023-10-27 03:30:10.533 
Epoch 956/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 27.7356, val_MinusLogProbMetric: 27.7356

Epoch 956: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 27.7356 - val_MinusLogProbMetric: 27.7356 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 957/1000
2023-10-27 03:30:45.197 
Epoch 957/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 27.7314, val_MinusLogProbMetric: 27.7314

Epoch 957: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 27.7314 - val_MinusLogProbMetric: 27.7314 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 958/1000
2023-10-27 03:31:20.467 
Epoch 958/1000 
	 loss: 27.4865, MinusLogProbMetric: 27.4865, val_loss: 27.7591, val_MinusLogProbMetric: 27.7591

Epoch 958: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4865 - MinusLogProbMetric: 27.4865 - val_loss: 27.7591 - val_MinusLogProbMetric: 27.7591 - lr: 3.1250e-05 - 35s/epoch - 180ms/step
Epoch 959/1000
2023-10-27 03:31:54.249 
Epoch 959/1000 
	 loss: 27.4860, MinusLogProbMetric: 27.4860, val_loss: 27.7327, val_MinusLogProbMetric: 27.7327

Epoch 959: val_loss did not improve from 27.72709
196/196 - 34s - loss: 27.4860 - MinusLogProbMetric: 27.4860 - val_loss: 27.7327 - val_MinusLogProbMetric: 27.7327 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 960/1000
2023-10-27 03:32:29.062 
Epoch 960/1000 
	 loss: 27.4824, MinusLogProbMetric: 27.4824, val_loss: 27.7326, val_MinusLogProbMetric: 27.7326

Epoch 960: val_loss did not improve from 27.72709
196/196 - 35s - loss: 27.4824 - MinusLogProbMetric: 27.4824 - val_loss: 27.7326 - val_MinusLogProbMetric: 27.7326 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 961/1000
2023-10-27 03:33:03.425 
Epoch 961/1000 
	 loss: 27.4870, MinusLogProbMetric: 27.4870, val_loss: 27.7286, val_MinusLogProbMetric: 27.7286

Epoch 961: val_loss did not improve from 27.72709
196/196 - 34s - loss: 27.4870 - MinusLogProbMetric: 27.4870 - val_loss: 27.7286 - val_MinusLogProbMetric: 27.7286 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 962/1000
2023-10-27 03:33:37.812 
Epoch 962/1000 
	 loss: 27.4857, MinusLogProbMetric: 27.4857, val_loss: 27.7410, val_MinusLogProbMetric: 27.7410

Epoch 962: val_loss did not improve from 27.72709
196/196 - 34s - loss: 27.4857 - MinusLogProbMetric: 27.4857 - val_loss: 27.7410 - val_MinusLogProbMetric: 27.7410 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 963/1000
2023-10-27 03:34:12.091 
Epoch 963/1000 
	 loss: 27.4833, MinusLogProbMetric: 27.4833, val_loss: 27.7259, val_MinusLogProbMetric: 27.7259

Epoch 963: val_loss improved from 27.72709 to 27.72594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_393/weights/best_weights.h5
196/196 - 35s - loss: 27.4833 - MinusLogProbMetric: 27.4833 - val_loss: 27.7259 - val_MinusLogProbMetric: 27.7259 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 964/1000
2023-10-27 03:34:47.196 
Epoch 964/1000 
	 loss: 27.4817, MinusLogProbMetric: 27.4817, val_loss: 27.7290, val_MinusLogProbMetric: 27.7290

Epoch 964: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4817 - MinusLogProbMetric: 27.4817 - val_loss: 27.7290 - val_MinusLogProbMetric: 27.7290 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 965/1000
2023-10-27 03:35:21.633 
Epoch 965/1000 
	 loss: 27.4847, MinusLogProbMetric: 27.4847, val_loss: 27.7452, val_MinusLogProbMetric: 27.7452

Epoch 965: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4847 - MinusLogProbMetric: 27.4847 - val_loss: 27.7452 - val_MinusLogProbMetric: 27.7452 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 966/1000
2023-10-27 03:35:55.697 
Epoch 966/1000 
	 loss: 27.4827, MinusLogProbMetric: 27.4827, val_loss: 27.7407, val_MinusLogProbMetric: 27.7407

Epoch 966: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4827 - MinusLogProbMetric: 27.4827 - val_loss: 27.7407 - val_MinusLogProbMetric: 27.7407 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 967/1000
2023-10-27 03:36:30.501 
Epoch 967/1000 
	 loss: 27.4829, MinusLogProbMetric: 27.4829, val_loss: 27.7513, val_MinusLogProbMetric: 27.7513

Epoch 967: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4829 - MinusLogProbMetric: 27.4829 - val_loss: 27.7513 - val_MinusLogProbMetric: 27.7513 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 968/1000
2023-10-27 03:37:05.299 
Epoch 968/1000 
	 loss: 27.4824, MinusLogProbMetric: 27.4824, val_loss: 27.7345, val_MinusLogProbMetric: 27.7345

Epoch 968: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4824 - MinusLogProbMetric: 27.4824 - val_loss: 27.7345 - val_MinusLogProbMetric: 27.7345 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 969/1000
2023-10-27 03:37:39.988 
Epoch 969/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 27.7608, val_MinusLogProbMetric: 27.7608

Epoch 969: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 27.7608 - val_MinusLogProbMetric: 27.7608 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 970/1000
2023-10-27 03:38:14.073 
Epoch 970/1000 
	 loss: 27.4864, MinusLogProbMetric: 27.4864, val_loss: 27.7369, val_MinusLogProbMetric: 27.7369

Epoch 970: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4864 - MinusLogProbMetric: 27.4864 - val_loss: 27.7369 - val_MinusLogProbMetric: 27.7369 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 971/1000
2023-10-27 03:38:48.412 
Epoch 971/1000 
	 loss: 27.4850, MinusLogProbMetric: 27.4850, val_loss: 27.7484, val_MinusLogProbMetric: 27.7484

Epoch 971: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4850 - MinusLogProbMetric: 27.4850 - val_loss: 27.7484 - val_MinusLogProbMetric: 27.7484 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 972/1000
2023-10-27 03:39:23.065 
Epoch 972/1000 
	 loss: 27.4852, MinusLogProbMetric: 27.4852, val_loss: 27.7341, val_MinusLogProbMetric: 27.7341

Epoch 972: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4852 - MinusLogProbMetric: 27.4852 - val_loss: 27.7341 - val_MinusLogProbMetric: 27.7341 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 973/1000
2023-10-27 03:39:57.808 
Epoch 973/1000 
	 loss: 27.4838, MinusLogProbMetric: 27.4838, val_loss: 27.7345, val_MinusLogProbMetric: 27.7345

Epoch 973: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4838 - MinusLogProbMetric: 27.4838 - val_loss: 27.7345 - val_MinusLogProbMetric: 27.7345 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 974/1000
2023-10-27 03:40:32.118 
Epoch 974/1000 
	 loss: 27.4820, MinusLogProbMetric: 27.4820, val_loss: 27.7310, val_MinusLogProbMetric: 27.7310

Epoch 974: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4820 - MinusLogProbMetric: 27.4820 - val_loss: 27.7310 - val_MinusLogProbMetric: 27.7310 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 975/1000
2023-10-27 03:41:06.887 
Epoch 975/1000 
	 loss: 27.4848, MinusLogProbMetric: 27.4848, val_loss: 27.7405, val_MinusLogProbMetric: 27.7405

Epoch 975: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4848 - MinusLogProbMetric: 27.4848 - val_loss: 27.7405 - val_MinusLogProbMetric: 27.7405 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 976/1000
2023-10-27 03:41:41.306 
Epoch 976/1000 
	 loss: 27.4829, MinusLogProbMetric: 27.4829, val_loss: 27.7332, val_MinusLogProbMetric: 27.7332

Epoch 976: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4829 - MinusLogProbMetric: 27.4829 - val_loss: 27.7332 - val_MinusLogProbMetric: 27.7332 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 977/1000
2023-10-27 03:42:15.697 
Epoch 977/1000 
	 loss: 27.4826, MinusLogProbMetric: 27.4826, val_loss: 27.7377, val_MinusLogProbMetric: 27.7377

Epoch 977: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4826 - MinusLogProbMetric: 27.4826 - val_loss: 27.7377 - val_MinusLogProbMetric: 27.7377 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 978/1000
2023-10-27 03:42:50.446 
Epoch 978/1000 
	 loss: 27.4823, MinusLogProbMetric: 27.4823, val_loss: 27.7583, val_MinusLogProbMetric: 27.7583

Epoch 978: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4823 - MinusLogProbMetric: 27.4823 - val_loss: 27.7583 - val_MinusLogProbMetric: 27.7583 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 979/1000
2023-10-27 03:43:25.082 
Epoch 979/1000 
	 loss: 27.4839, MinusLogProbMetric: 27.4839, val_loss: 27.7470, val_MinusLogProbMetric: 27.7470

Epoch 979: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4839 - MinusLogProbMetric: 27.4839 - val_loss: 27.7470 - val_MinusLogProbMetric: 27.7470 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 980/1000
2023-10-27 03:43:59.580 
Epoch 980/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 27.7514, val_MinusLogProbMetric: 27.7514

Epoch 980: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 27.7514 - val_MinusLogProbMetric: 27.7514 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 981/1000
2023-10-27 03:44:34.372 
Epoch 981/1000 
	 loss: 27.4828, MinusLogProbMetric: 27.4828, val_loss: 27.7480, val_MinusLogProbMetric: 27.7480

Epoch 981: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4828 - MinusLogProbMetric: 27.4828 - val_loss: 27.7480 - val_MinusLogProbMetric: 27.7480 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 982/1000
2023-10-27 03:45:08.908 
Epoch 982/1000 
	 loss: 27.4842, MinusLogProbMetric: 27.4842, val_loss: 27.7704, val_MinusLogProbMetric: 27.7704

Epoch 982: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4842 - MinusLogProbMetric: 27.4842 - val_loss: 27.7704 - val_MinusLogProbMetric: 27.7704 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 983/1000
2023-10-27 03:45:43.482 
Epoch 983/1000 
	 loss: 27.4858, MinusLogProbMetric: 27.4858, val_loss: 27.7392, val_MinusLogProbMetric: 27.7392

Epoch 983: val_loss did not improve from 27.72594
196/196 - 35s - loss: 27.4858 - MinusLogProbMetric: 27.4858 - val_loss: 27.7392 - val_MinusLogProbMetric: 27.7392 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 984/1000
2023-10-27 03:46:17.504 
Epoch 984/1000 
	 loss: 27.4842, MinusLogProbMetric: 27.4842, val_loss: 27.7359, val_MinusLogProbMetric: 27.7359

Epoch 984: val_loss did not improve from 27.72594
196/196 - 34s - loss: 27.4842 - MinusLogProbMetric: 27.4842 - val_loss: 27.7359 - val_MinusLogProbMetric: 27.7359 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 985/1000
2023-10-27 03:46:46.401 
Epoch 985/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 27.7323, val_MinusLogProbMetric: 27.7323

Epoch 985: val_loss did not improve from 27.72594
196/196 - 29s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 27.7323 - val_MinusLogProbMetric: 27.7323 - lr: 3.1250e-05 - 29s/epoch - 147ms/step
Epoch 986/1000
2023-10-27 03:47:13.865 
Epoch 986/1000 
	 loss: 27.4821, MinusLogProbMetric: 27.4821, val_loss: 27.7313, val_MinusLogProbMetric: 27.7313

Epoch 986: val_loss did not improve from 27.72594
196/196 - 27s - loss: 27.4821 - MinusLogProbMetric: 27.4821 - val_loss: 27.7313 - val_MinusLogProbMetric: 27.7313 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 987/1000
2023-10-27 03:47:40.903 
Epoch 987/1000 
	 loss: 27.4819, MinusLogProbMetric: 27.4819, val_loss: 27.7343, val_MinusLogProbMetric: 27.7343

Epoch 987: val_loss did not improve from 27.72594
196/196 - 27s - loss: 27.4819 - MinusLogProbMetric: 27.4819 - val_loss: 27.7343 - val_MinusLogProbMetric: 27.7343 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Epoch 988/1000
2023-10-27 03:48:08.246 
Epoch 988/1000 
	 loss: 27.4822, MinusLogProbMetric: 27.4822, val_loss: 27.7359, val_MinusLogProbMetric: 27.7359

Epoch 988: val_loss did not improve from 27.72594
196/196 - 27s - loss: 27.4822 - MinusLogProbMetric: 27.4822 - val_loss: 27.7359 - val_MinusLogProbMetric: 27.7359 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 989/1000
2023-10-27 03:48:39.307 
Epoch 989/1000 
	 loss: 27.4842, MinusLogProbMetric: 27.4842, val_loss: 27.7331, val_MinusLogProbMetric: 27.7331

Epoch 989: val_loss did not improve from 27.72594
196/196 - 31s - loss: 27.4842 - MinusLogProbMetric: 27.4842 - val_loss: 27.7331 - val_MinusLogProbMetric: 27.7331 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 990/1000
2023-10-27 03:49:12.186 
Epoch 990/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 27.7295, val_MinusLogProbMetric: 27.7295

Epoch 990: val_loss did not improve from 27.72594
196/196 - 33s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 27.7295 - val_MinusLogProbMetric: 27.7295 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 991/1000
2023-10-27 03:49:41.882 
Epoch 991/1000 
	 loss: 27.4870, MinusLogProbMetric: 27.4870, val_loss: 27.7340, val_MinusLogProbMetric: 27.7340

Epoch 991: val_loss did not improve from 27.72594
196/196 - 30s - loss: 27.4870 - MinusLogProbMetric: 27.4870 - val_loss: 27.7340 - val_MinusLogProbMetric: 27.7340 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 992/1000
2023-10-27 03:50:09.043 
Epoch 992/1000 
	 loss: 27.4831, MinusLogProbMetric: 27.4831, val_loss: 27.7322, val_MinusLogProbMetric: 27.7322

Epoch 992: val_loss did not improve from 27.72594
196/196 - 27s - loss: 27.4831 - MinusLogProbMetric: 27.4831 - val_loss: 27.7322 - val_MinusLogProbMetric: 27.7322 - lr: 3.1250e-05 - 27s/epoch - 139ms/step
Epoch 993/1000
2023-10-27 03:50:35.828 
Epoch 993/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 27.7420, val_MinusLogProbMetric: 27.7420

Epoch 993: val_loss did not improve from 27.72594
196/196 - 27s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 27.7420 - val_MinusLogProbMetric: 27.7420 - lr: 3.1250e-05 - 27s/epoch - 137ms/step
Epoch 994/1000
2023-10-27 03:51:03.172 
Epoch 994/1000 
	 loss: 27.4820, MinusLogProbMetric: 27.4820, val_loss: 27.7383, val_MinusLogProbMetric: 27.7383

Epoch 994: val_loss did not improve from 27.72594
196/196 - 27s - loss: 27.4820 - MinusLogProbMetric: 27.4820 - val_loss: 27.7383 - val_MinusLogProbMetric: 27.7383 - lr: 3.1250e-05 - 27s/epoch - 140ms/step
Epoch 995/1000
2023-10-27 03:51:34.250 
Epoch 995/1000 
	 loss: 27.4807, MinusLogProbMetric: 27.4807, val_loss: 27.7398, val_MinusLogProbMetric: 27.7398

Epoch 995: val_loss did not improve from 27.72594
196/196 - 31s - loss: 27.4807 - MinusLogProbMetric: 27.4807 - val_loss: 27.7398 - val_MinusLogProbMetric: 27.7398 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 996/1000
2023-10-27 03:52:07.401 
Epoch 996/1000 
	 loss: 27.4824, MinusLogProbMetric: 27.4824, val_loss: 27.7440, val_MinusLogProbMetric: 27.7440

Epoch 996: val_loss did not improve from 27.72594
196/196 - 33s - loss: 27.4824 - MinusLogProbMetric: 27.4824 - val_loss: 27.7440 - val_MinusLogProbMetric: 27.7440 - lr: 3.1250e-05 - 33s/epoch - 169ms/step
Epoch 997/1000
2023-10-27 03:52:38.543 
Epoch 997/1000 
	 loss: 27.4839, MinusLogProbMetric: 27.4839, val_loss: 27.7410, val_MinusLogProbMetric: 27.7410

Epoch 997: val_loss did not improve from 27.72594
196/196 - 31s - loss: 27.4839 - MinusLogProbMetric: 27.4839 - val_loss: 27.7410 - val_MinusLogProbMetric: 27.7410 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 998/1000
2023-10-27 03:53:08.009 
Epoch 998/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 27.7379, val_MinusLogProbMetric: 27.7379

Epoch 998: val_loss did not improve from 27.72594
196/196 - 29s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 27.7379 - val_MinusLogProbMetric: 27.7379 - lr: 3.1250e-05 - 29s/epoch - 150ms/step
Epoch 999/1000
2023-10-27 03:53:36.745 
Epoch 999/1000 
	 loss: 27.4822, MinusLogProbMetric: 27.4822, val_loss: 27.7362, val_MinusLogProbMetric: 27.7362

Epoch 999: val_loss did not improve from 27.72594
196/196 - 29s - loss: 27.4822 - MinusLogProbMetric: 27.4822 - val_loss: 27.7362 - val_MinusLogProbMetric: 27.7362 - lr: 3.1250e-05 - 29s/epoch - 147ms/step
Epoch 1000/1000
2023-10-27 03:54:03.791 
Epoch 1000/1000 
	 loss: 27.4823, MinusLogProbMetric: 27.4823, val_loss: 27.7272, val_MinusLogProbMetric: 27.7272

Epoch 1000: val_loss did not improve from 27.72594
196/196 - 27s - loss: 27.4823 - MinusLogProbMetric: 27.4823 - val_loss: 27.7272 - val_MinusLogProbMetric: 27.7272 - lr: 3.1250e-05 - 27s/epoch - 138ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 933.
Model trained in 34668.28 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.62 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.82 s.
===========
Run 393/720 done in 34673.87 s.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

===========
Generating train data for run 400.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_400
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_221"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_222 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f0d268f7c40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d26e57670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d26e57670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d25df59c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d25c5f3d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d25c5f940>, <keras.callbacks.ModelCheckpoint object at 0x7f0d25c5fa00>, <keras.callbacks.EarlyStopping object at 0x7f0d25c5fc70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d25c5fca0>, <keras.callbacks.TerminateOnNaN object at 0x7f0d25c5f8e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_400/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 400/720 with hyperparameters:
timestamp = 2023-10-27 03:54:13.283452
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 5: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:56:59.299 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6450.4170, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 166s - loss: nan - MinusLogProbMetric: 6450.4170 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 166s/epoch - 845ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 400.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_400
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_232"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_233 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f0d86692fb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c04372920>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c04372920>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c0c297280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d8663f190>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d8663f700>, <keras.callbacks.ModelCheckpoint object at 0x7f0d8663f7c0>, <keras.callbacks.EarlyStopping object at 0x7f0d8663fa30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d8663fa60>, <keras.callbacks.TerminateOnNaN object at 0x7f0d8663f6a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_400/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 400/720 with hyperparameters:
timestamp = 2023-10-27 03:57:10.159046
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 67: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 04:00:10.141 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 3940.9749, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 180s - loss: nan - MinusLogProbMetric: 3940.9749 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 180s/epoch - 917ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 400.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_400
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_243"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_244 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f0c68260700>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b8483fc40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b8483fc40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d4b0e2b90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d4a037eb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0cd8324460>, <keras.callbacks.ModelCheckpoint object at 0x7f0cd8324520>, <keras.callbacks.EarlyStopping object at 0x7f0cd8324790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0cd83247c0>, <keras.callbacks.TerminateOnNaN object at 0x7f0cd8324400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_400/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 400/720 with hyperparameters:
timestamp = 2023-10-27 04:00:19.161535
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-27 04:04:01.811 
Epoch 1/1000 
	 loss: 3558.8262, MinusLogProbMetric: 3558.8262, val_loss: 1923.3481, val_MinusLogProbMetric: 1923.3481

Epoch 1: val_loss improved from inf to 1923.34814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 223s - loss: 3558.8262 - MinusLogProbMetric: 3558.8262 - val_loss: 1923.3481 - val_MinusLogProbMetric: 1923.3481 - lr: 1.1111e-04 - 223s/epoch - 1s/step
Epoch 2/1000
2023-10-27 04:05:04.188 
Epoch 2/1000 
	 loss: 1490.7523, MinusLogProbMetric: 1490.7523, val_loss: 1164.3540, val_MinusLogProbMetric: 1164.3540

Epoch 2: val_loss improved from 1923.34814 to 1164.35400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 62s - loss: 1490.7523 - MinusLogProbMetric: 1490.7523 - val_loss: 1164.3540 - val_MinusLogProbMetric: 1164.3540 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 3/1000
2023-10-27 04:06:09.663 
Epoch 3/1000 
	 loss: 1071.2612, MinusLogProbMetric: 1071.2612, val_loss: 1138.1639, val_MinusLogProbMetric: 1138.1639

Epoch 3: val_loss improved from 1164.35400 to 1138.16394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 65s - loss: 1071.2612 - MinusLogProbMetric: 1071.2612 - val_loss: 1138.1639 - val_MinusLogProbMetric: 1138.1639 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 4/1000
2023-10-27 04:07:12.493 
Epoch 4/1000 
	 loss: 924.6144, MinusLogProbMetric: 924.6144, val_loss: 757.8345, val_MinusLogProbMetric: 757.8345

Epoch 4: val_loss improved from 1138.16394 to 757.83453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 63s - loss: 924.6144 - MinusLogProbMetric: 924.6144 - val_loss: 757.8345 - val_MinusLogProbMetric: 757.8345 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 5/1000
2023-10-27 04:08:15.182 
Epoch 5/1000 
	 loss: 711.7906, MinusLogProbMetric: 711.7906, val_loss: 712.7795, val_MinusLogProbMetric: 712.7795

Epoch 5: val_loss improved from 757.83453 to 712.77954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 63s - loss: 711.7906 - MinusLogProbMetric: 711.7906 - val_loss: 712.7795 - val_MinusLogProbMetric: 712.7795 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 6/1000
2023-10-27 04:09:19.736 
Epoch 6/1000 
	 loss: 674.0281, MinusLogProbMetric: 674.0281, val_loss: 628.3295, val_MinusLogProbMetric: 628.3295

Epoch 6: val_loss improved from 712.77954 to 628.32953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 65s - loss: 674.0281 - MinusLogProbMetric: 674.0281 - val_loss: 628.3295 - val_MinusLogProbMetric: 628.3295 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 7/1000
2023-10-27 04:10:30.224 
Epoch 7/1000 
	 loss: 606.0860, MinusLogProbMetric: 606.0860, val_loss: 589.2685, val_MinusLogProbMetric: 589.2685

Epoch 7: val_loss improved from 628.32953 to 589.26849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 70s - loss: 606.0860 - MinusLogProbMetric: 606.0860 - val_loss: 589.2685 - val_MinusLogProbMetric: 589.2685 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 8/1000
2023-10-27 04:11:35.085 
Epoch 8/1000 
	 loss: 571.1119, MinusLogProbMetric: 571.1119, val_loss: 549.1909, val_MinusLogProbMetric: 549.1909

Epoch 8: val_loss improved from 589.26849 to 549.19092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 65s - loss: 571.1119 - MinusLogProbMetric: 571.1119 - val_loss: 549.1909 - val_MinusLogProbMetric: 549.1909 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 9/1000
2023-10-27 04:12:50.212 
Epoch 9/1000 
	 loss: 538.6962, MinusLogProbMetric: 538.6962, val_loss: 525.4482, val_MinusLogProbMetric: 525.4482

Epoch 9: val_loss improved from 549.19092 to 525.44824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 75s - loss: 538.6962 - MinusLogProbMetric: 538.6962 - val_loss: 525.4482 - val_MinusLogProbMetric: 525.4482 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 10/1000
2023-10-27 04:13:54.275 
Epoch 10/1000 
	 loss: 512.0961, MinusLogProbMetric: 512.0961, val_loss: 502.9640, val_MinusLogProbMetric: 502.9640

Epoch 10: val_loss improved from 525.44824 to 502.96402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 64s - loss: 512.0961 - MinusLogProbMetric: 512.0961 - val_loss: 502.9640 - val_MinusLogProbMetric: 502.9640 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 11/1000
2023-10-27 04:15:00.916 
Epoch 11/1000 
	 loss: 484.8752, MinusLogProbMetric: 484.8752, val_loss: 471.1951, val_MinusLogProbMetric: 471.1951

Epoch 11: val_loss improved from 502.96402 to 471.19510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 67s - loss: 484.8752 - MinusLogProbMetric: 484.8752 - val_loss: 471.1951 - val_MinusLogProbMetric: 471.1951 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 12/1000
2023-10-27 04:16:10.340 
Epoch 12/1000 
	 loss: 459.3047, MinusLogProbMetric: 459.3047, val_loss: 451.8547, val_MinusLogProbMetric: 451.8547

Epoch 12: val_loss improved from 471.19510 to 451.85474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 69s - loss: 459.3047 - MinusLogProbMetric: 459.3047 - val_loss: 451.8547 - val_MinusLogProbMetric: 451.8547 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 13/1000
2023-10-27 04:17:15.998 
Epoch 13/1000 
	 loss: 455.1114, MinusLogProbMetric: 455.1114, val_loss: 437.6235, val_MinusLogProbMetric: 437.6235

Epoch 13: val_loss improved from 451.85474 to 437.62347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 66s - loss: 455.1114 - MinusLogProbMetric: 455.1114 - val_loss: 437.6235 - val_MinusLogProbMetric: 437.6235 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 14/1000
2023-10-27 04:18:28.678 
Epoch 14/1000 
	 loss: 428.2270, MinusLogProbMetric: 428.2270, val_loss: 424.0496, val_MinusLogProbMetric: 424.0496

Epoch 14: val_loss improved from 437.62347 to 424.04956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 73s - loss: 428.2270 - MinusLogProbMetric: 428.2270 - val_loss: 424.0496 - val_MinusLogProbMetric: 424.0496 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 15/1000
2023-10-27 04:19:36.619 
Epoch 15/1000 
	 loss: 418.1700, MinusLogProbMetric: 418.1700, val_loss: 430.3377, val_MinusLogProbMetric: 430.3377

Epoch 15: val_loss did not improve from 424.04956
196/196 - 67s - loss: 418.1700 - MinusLogProbMetric: 418.1700 - val_loss: 430.3377 - val_MinusLogProbMetric: 430.3377 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 16/1000
2023-10-27 04:20:43.156 
Epoch 16/1000 
	 loss: 403.0991, MinusLogProbMetric: 403.0991, val_loss: 401.3317, val_MinusLogProbMetric: 401.3317

Epoch 16: val_loss improved from 424.04956 to 401.33173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 68s - loss: 403.0991 - MinusLogProbMetric: 403.0991 - val_loss: 401.3317 - val_MinusLogProbMetric: 401.3317 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 17/1000
2023-10-27 04:21:55.445 
Epoch 17/1000 
	 loss: 391.9833, MinusLogProbMetric: 391.9833, val_loss: 388.0055, val_MinusLogProbMetric: 388.0055

Epoch 17: val_loss improved from 401.33173 to 388.00552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 72s - loss: 391.9833 - MinusLogProbMetric: 391.9833 - val_loss: 388.0055 - val_MinusLogProbMetric: 388.0055 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 18/1000
2023-10-27 04:23:03.184 
Epoch 18/1000 
	 loss: 393.5583, MinusLogProbMetric: 393.5583, val_loss: 498.1150, val_MinusLogProbMetric: 498.1150

Epoch 18: val_loss did not improve from 388.00552
196/196 - 67s - loss: 393.5583 - MinusLogProbMetric: 393.5583 - val_loss: 498.1150 - val_MinusLogProbMetric: 498.1150 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 19/1000
2023-10-27 04:24:10.775 
Epoch 19/1000 
	 loss: 463.2790, MinusLogProbMetric: 463.2790, val_loss: 491.5177, val_MinusLogProbMetric: 491.5177

Epoch 19: val_loss did not improve from 388.00552
196/196 - 68s - loss: 463.2790 - MinusLogProbMetric: 463.2790 - val_loss: 491.5177 - val_MinusLogProbMetric: 491.5177 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 20/1000
2023-10-27 04:25:16.710 
Epoch 20/1000 
	 loss: 411.0126, MinusLogProbMetric: 411.0126, val_loss: 388.6626, val_MinusLogProbMetric: 388.6626

Epoch 20: val_loss did not improve from 388.00552
196/196 - 66s - loss: 411.0126 - MinusLogProbMetric: 411.0126 - val_loss: 388.6626 - val_MinusLogProbMetric: 388.6626 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 21/1000
2023-10-27 04:26:26.300 
Epoch 21/1000 
	 loss: 378.1645, MinusLogProbMetric: 378.1645, val_loss: 374.6713, val_MinusLogProbMetric: 374.6713

Epoch 21: val_loss improved from 388.00552 to 374.67133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 71s - loss: 378.1645 - MinusLogProbMetric: 378.1645 - val_loss: 374.6713 - val_MinusLogProbMetric: 374.6713 - lr: 1.1111e-04 - 71s/epoch - 361ms/step
Epoch 22/1000
2023-10-27 04:27:37.454 
Epoch 22/1000 
	 loss: 460.5799, MinusLogProbMetric: 460.5799, val_loss: 509.9751, val_MinusLogProbMetric: 509.9751

Epoch 22: val_loss did not improve from 374.67133
196/196 - 70s - loss: 460.5799 - MinusLogProbMetric: 460.5799 - val_loss: 509.9751 - val_MinusLogProbMetric: 509.9751 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 23/1000
2023-10-27 04:28:46.542 
Epoch 23/1000 
	 loss: 449.8898, MinusLogProbMetric: 449.8898, val_loss: 420.3610, val_MinusLogProbMetric: 420.3610

Epoch 23: val_loss did not improve from 374.67133
196/196 - 69s - loss: 449.8898 - MinusLogProbMetric: 449.8898 - val_loss: 420.3610 - val_MinusLogProbMetric: 420.3610 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 24/1000
2023-10-27 04:29:58.118 
Epoch 24/1000 
	 loss: 388.3434, MinusLogProbMetric: 388.3434, val_loss: 374.1264, val_MinusLogProbMetric: 374.1264

Epoch 24: val_loss improved from 374.67133 to 374.12640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 73s - loss: 388.3434 - MinusLogProbMetric: 388.3434 - val_loss: 374.1264 - val_MinusLogProbMetric: 374.1264 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 25/1000
2023-10-27 04:31:08.490 
Epoch 25/1000 
	 loss: 362.8596, MinusLogProbMetric: 362.8596, val_loss: 353.6550, val_MinusLogProbMetric: 353.6550

Epoch 25: val_loss improved from 374.12640 to 353.65500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 70s - loss: 362.8596 - MinusLogProbMetric: 362.8596 - val_loss: 353.6550 - val_MinusLogProbMetric: 353.6550 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 26/1000
2023-10-27 04:32:21.694 
Epoch 26/1000 
	 loss: 346.7961, MinusLogProbMetric: 346.7961, val_loss: 339.2292, val_MinusLogProbMetric: 339.2292

Epoch 26: val_loss improved from 353.65500 to 339.22919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 73s - loss: 346.7961 - MinusLogProbMetric: 346.7961 - val_loss: 339.2292 - val_MinusLogProbMetric: 339.2292 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 27/1000
2023-10-27 04:33:28.390 
Epoch 27/1000 
	 loss: 370.6337, MinusLogProbMetric: 370.6337, val_loss: 371.7704, val_MinusLogProbMetric: 371.7704

Epoch 27: val_loss did not improve from 339.22919
196/196 - 66s - loss: 370.6337 - MinusLogProbMetric: 370.6337 - val_loss: 371.7704 - val_MinusLogProbMetric: 371.7704 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 28/1000
2023-10-27 04:34:32.532 
Epoch 28/1000 
	 loss: 347.5103, MinusLogProbMetric: 347.5103, val_loss: 331.4432, val_MinusLogProbMetric: 331.4432

Epoch 28: val_loss improved from 339.22919 to 331.44318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 65s - loss: 347.5103 - MinusLogProbMetric: 347.5103 - val_loss: 331.4432 - val_MinusLogProbMetric: 331.4432 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 29/1000
2023-10-27 04:35:42.345 
Epoch 29/1000 
	 loss: 320.5809, MinusLogProbMetric: 320.5809, val_loss: 312.2036, val_MinusLogProbMetric: 312.2036

Epoch 29: val_loss improved from 331.44318 to 312.20364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 70s - loss: 320.5809 - MinusLogProbMetric: 320.5809 - val_loss: 312.2036 - val_MinusLogProbMetric: 312.2036 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 30/1000
2023-10-27 04:36:53.222 
Epoch 30/1000 
	 loss: 307.0910, MinusLogProbMetric: 307.0910, val_loss: 302.2014, val_MinusLogProbMetric: 302.2014

Epoch 30: val_loss improved from 312.20364 to 302.20135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 71s - loss: 307.0910 - MinusLogProbMetric: 307.0910 - val_loss: 302.2014 - val_MinusLogProbMetric: 302.2014 - lr: 1.1111e-04 - 71s/epoch - 362ms/step
Epoch 31/1000
2023-10-27 04:37:59.145 
Epoch 31/1000 
	 loss: 296.4698, MinusLogProbMetric: 296.4698, val_loss: 290.7619, val_MinusLogProbMetric: 290.7619

Epoch 31: val_loss improved from 302.20135 to 290.76187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 66s - loss: 296.4698 - MinusLogProbMetric: 296.4698 - val_loss: 290.7619 - val_MinusLogProbMetric: 290.7619 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 32/1000
2023-10-27 04:39:17.668 
Epoch 32/1000 
	 loss: 286.5346, MinusLogProbMetric: 286.5346, val_loss: 283.2864, val_MinusLogProbMetric: 283.2864

Epoch 32: val_loss improved from 290.76187 to 283.28638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 78s - loss: 286.5346 - MinusLogProbMetric: 286.5346 - val_loss: 283.2864 - val_MinusLogProbMetric: 283.2864 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 33/1000
2023-10-27 04:40:29.957 
Epoch 33/1000 
	 loss: 278.7437, MinusLogProbMetric: 278.7437, val_loss: 274.2934, val_MinusLogProbMetric: 274.2934

Epoch 33: val_loss improved from 283.28638 to 274.29343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 73s - loss: 278.7437 - MinusLogProbMetric: 278.7437 - val_loss: 274.2934 - val_MinusLogProbMetric: 274.2934 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 34/1000
2023-10-27 04:41:51.250 
Epoch 34/1000 
	 loss: 271.4223, MinusLogProbMetric: 271.4223, val_loss: 267.1712, val_MinusLogProbMetric: 267.1712

Epoch 34: val_loss improved from 274.29343 to 267.17117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 271.4223 - MinusLogProbMetric: 271.4223 - val_loss: 267.1712 - val_MinusLogProbMetric: 267.1712 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 35/1000
2023-10-27 04:43:12.969 
Epoch 35/1000 
	 loss: 270.3935, MinusLogProbMetric: 270.3935, val_loss: 268.6953, val_MinusLogProbMetric: 268.6953

Epoch 35: val_loss did not improve from 267.17117
196/196 - 80s - loss: 270.3935 - MinusLogProbMetric: 270.3935 - val_loss: 268.6953 - val_MinusLogProbMetric: 268.6953 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 36/1000
2023-10-27 04:44:33.634 
Epoch 36/1000 
	 loss: 325.0656, MinusLogProbMetric: 325.0656, val_loss: 290.8864, val_MinusLogProbMetric: 290.8864

Epoch 36: val_loss did not improve from 267.17117
196/196 - 81s - loss: 325.0656 - MinusLogProbMetric: 325.0656 - val_loss: 290.8864 - val_MinusLogProbMetric: 290.8864 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 37/1000
2023-10-27 04:45:53.366 
Epoch 37/1000 
	 loss: 282.6864, MinusLogProbMetric: 282.6864, val_loss: 274.0186, val_MinusLogProbMetric: 274.0186

Epoch 37: val_loss did not improve from 267.17117
196/196 - 80s - loss: 282.6864 - MinusLogProbMetric: 282.6864 - val_loss: 274.0186 - val_MinusLogProbMetric: 274.0186 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 38/1000
2023-10-27 04:47:13.344 
Epoch 38/1000 
	 loss: 265.5804, MinusLogProbMetric: 265.5804, val_loss: 259.4199, val_MinusLogProbMetric: 259.4199

Epoch 38: val_loss improved from 267.17117 to 259.41986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 265.5804 - MinusLogProbMetric: 265.5804 - val_loss: 259.4199 - val_MinusLogProbMetric: 259.4199 - lr: 1.1111e-04 - 82s/epoch - 416ms/step
Epoch 39/1000
2023-10-27 04:48:34.599 
Epoch 39/1000 
	 loss: 255.8581, MinusLogProbMetric: 255.8581, val_loss: 251.3547, val_MinusLogProbMetric: 251.3547

Epoch 39: val_loss improved from 259.41986 to 251.35471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 255.8581 - MinusLogProbMetric: 255.8581 - val_loss: 251.3547 - val_MinusLogProbMetric: 251.3547 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 40/1000
2023-10-27 04:49:55.311 
Epoch 40/1000 
	 loss: 249.9250, MinusLogProbMetric: 249.9250, val_loss: 245.7118, val_MinusLogProbMetric: 245.7118

Epoch 40: val_loss improved from 251.35471 to 245.71182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 249.9250 - MinusLogProbMetric: 249.9250 - val_loss: 245.7118 - val_MinusLogProbMetric: 245.7118 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 41/1000
2023-10-27 04:51:16.541 
Epoch 41/1000 
	 loss: 243.4413, MinusLogProbMetric: 243.4413, val_loss: 239.2048, val_MinusLogProbMetric: 239.2048

Epoch 41: val_loss improved from 245.71182 to 239.20485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 243.4413 - MinusLogProbMetric: 243.4413 - val_loss: 239.2048 - val_MinusLogProbMetric: 239.2048 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 42/1000
2023-10-27 04:52:37.064 
Epoch 42/1000 
	 loss: 242.5889, MinusLogProbMetric: 242.5889, val_loss: 236.0497, val_MinusLogProbMetric: 236.0497

Epoch 42: val_loss improved from 239.20485 to 236.04974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 80s - loss: 242.5889 - MinusLogProbMetric: 242.5889 - val_loss: 236.0497 - val_MinusLogProbMetric: 236.0497 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 43/1000
2023-10-27 04:53:59.518 
Epoch 43/1000 
	 loss: 237.4927, MinusLogProbMetric: 237.4927, val_loss: 237.6850, val_MinusLogProbMetric: 237.6850

Epoch 43: val_loss did not improve from 236.04974
196/196 - 81s - loss: 237.4927 - MinusLogProbMetric: 237.4927 - val_loss: 237.6850 - val_MinusLogProbMetric: 237.6850 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 44/1000
2023-10-27 04:55:20.493 
Epoch 44/1000 
	 loss: 230.7955, MinusLogProbMetric: 230.7955, val_loss: 241.1519, val_MinusLogProbMetric: 241.1519

Epoch 44: val_loss did not improve from 236.04974
196/196 - 81s - loss: 230.7955 - MinusLogProbMetric: 230.7955 - val_loss: 241.1519 - val_MinusLogProbMetric: 241.1519 - lr: 1.1111e-04 - 81s/epoch - 413ms/step
Epoch 45/1000
2023-10-27 04:56:41.737 
Epoch 45/1000 
	 loss: 260.9489, MinusLogProbMetric: 260.9489, val_loss: 305.3458, val_MinusLogProbMetric: 305.3458

Epoch 45: val_loss did not improve from 236.04974
196/196 - 81s - loss: 260.9489 - MinusLogProbMetric: 260.9489 - val_loss: 305.3458 - val_MinusLogProbMetric: 305.3458 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 46/1000
2023-10-27 04:58:03.253 
Epoch 46/1000 
	 loss: 422.4438, MinusLogProbMetric: 422.4438, val_loss: 303.1042, val_MinusLogProbMetric: 303.1042

Epoch 46: val_loss did not improve from 236.04974
196/196 - 82s - loss: 422.4438 - MinusLogProbMetric: 422.4438 - val_loss: 303.1042 - val_MinusLogProbMetric: 303.1042 - lr: 1.1111e-04 - 82s/epoch - 416ms/step
Epoch 47/1000
2023-10-27 04:59:23.366 
Epoch 47/1000 
	 loss: 310.1030, MinusLogProbMetric: 310.1030, val_loss: 349.6571, val_MinusLogProbMetric: 349.6571

Epoch 47: val_loss did not improve from 236.04974
196/196 - 80s - loss: 310.1030 - MinusLogProbMetric: 310.1030 - val_loss: 349.6571 - val_MinusLogProbMetric: 349.6571 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 48/1000
2023-10-27 05:00:43.221 
Epoch 48/1000 
	 loss: 321.6300, MinusLogProbMetric: 321.6300, val_loss: 281.1574, val_MinusLogProbMetric: 281.1574

Epoch 48: val_loss did not improve from 236.04974
196/196 - 80s - loss: 321.6300 - MinusLogProbMetric: 321.6300 - val_loss: 281.1574 - val_MinusLogProbMetric: 281.1574 - lr: 1.1111e-04 - 80s/epoch - 407ms/step
Epoch 49/1000
2023-10-27 05:02:04.348 
Epoch 49/1000 
	 loss: 293.1437, MinusLogProbMetric: 293.1437, val_loss: 280.1289, val_MinusLogProbMetric: 280.1289

Epoch 49: val_loss did not improve from 236.04974
196/196 - 81s - loss: 293.1437 - MinusLogProbMetric: 293.1437 - val_loss: 280.1289 - val_MinusLogProbMetric: 280.1289 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 50/1000
2023-10-27 05:03:26.287 
Epoch 50/1000 
	 loss: 270.8117, MinusLogProbMetric: 270.8117, val_loss: 259.2698, val_MinusLogProbMetric: 259.2698

Epoch 50: val_loss did not improve from 236.04974
196/196 - 82s - loss: 270.8117 - MinusLogProbMetric: 270.8117 - val_loss: 259.2698 - val_MinusLogProbMetric: 259.2698 - lr: 1.1111e-04 - 82s/epoch - 418ms/step
Epoch 51/1000
2023-10-27 05:04:45.681 
Epoch 51/1000 
	 loss: 249.5970, MinusLogProbMetric: 249.5970, val_loss: 241.1160, val_MinusLogProbMetric: 241.1160

Epoch 51: val_loss did not improve from 236.04974
196/196 - 79s - loss: 249.5970 - MinusLogProbMetric: 249.5970 - val_loss: 241.1160 - val_MinusLogProbMetric: 241.1160 - lr: 1.1111e-04 - 79s/epoch - 405ms/step
Epoch 52/1000
2023-10-27 05:06:07.000 
Epoch 52/1000 
	 loss: 235.8901, MinusLogProbMetric: 235.8901, val_loss: 230.5693, val_MinusLogProbMetric: 230.5693

Epoch 52: val_loss improved from 236.04974 to 230.56931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 235.8901 - MinusLogProbMetric: 235.8901 - val_loss: 230.5693 - val_MinusLogProbMetric: 230.5693 - lr: 1.1111e-04 - 83s/epoch - 421ms/step
Epoch 53/1000
2023-10-27 05:07:28.831 
Epoch 53/1000 
	 loss: 225.6738, MinusLogProbMetric: 225.6738, val_loss: 221.5533, val_MinusLogProbMetric: 221.5533

Epoch 53: val_loss improved from 230.56931 to 221.55333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 225.6738 - MinusLogProbMetric: 225.6738 - val_loss: 221.5533 - val_MinusLogProbMetric: 221.5533 - lr: 1.1111e-04 - 82s/epoch - 418ms/step
Epoch 54/1000
2023-10-27 05:08:50.598 
Epoch 54/1000 
	 loss: 217.2121, MinusLogProbMetric: 217.2121, val_loss: 213.0527, val_MinusLogProbMetric: 213.0527

Epoch 54: val_loss improved from 221.55333 to 213.05273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 217.2121 - MinusLogProbMetric: 217.2121 - val_loss: 213.0527 - val_MinusLogProbMetric: 213.0527 - lr: 1.1111e-04 - 82s/epoch - 417ms/step
Epoch 55/1000
2023-10-27 05:10:13.153 
Epoch 55/1000 
	 loss: 209.6526, MinusLogProbMetric: 209.6526, val_loss: 206.1572, val_MinusLogProbMetric: 206.1572

Epoch 55: val_loss improved from 213.05273 to 206.15723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 209.6526 - MinusLogProbMetric: 209.6526 - val_loss: 206.1572 - val_MinusLogProbMetric: 206.1572 - lr: 1.1111e-04 - 83s/epoch - 421ms/step
Epoch 56/1000
2023-10-27 05:11:36.122 
Epoch 56/1000 
	 loss: 287.5465, MinusLogProbMetric: 287.5465, val_loss: 327.6963, val_MinusLogProbMetric: 327.6963

Epoch 56: val_loss did not improve from 206.15723
196/196 - 82s - loss: 287.5465 - MinusLogProbMetric: 287.5465 - val_loss: 327.6963 - val_MinusLogProbMetric: 327.6963 - lr: 1.1111e-04 - 82s/epoch - 416ms/step
Epoch 57/1000
2023-10-27 05:12:57.564 
Epoch 57/1000 
	 loss: 283.1695, MinusLogProbMetric: 283.1695, val_loss: 262.0377, val_MinusLogProbMetric: 262.0377

Epoch 57: val_loss did not improve from 206.15723
196/196 - 81s - loss: 283.1695 - MinusLogProbMetric: 283.1695 - val_loss: 262.0377 - val_MinusLogProbMetric: 262.0377 - lr: 1.1111e-04 - 81s/epoch - 416ms/step
Epoch 58/1000
2023-10-27 05:14:18.915 
Epoch 58/1000 
	 loss: 255.4422, MinusLogProbMetric: 255.4422, val_loss: 250.9398, val_MinusLogProbMetric: 250.9398

Epoch 58: val_loss did not improve from 206.15723
196/196 - 81s - loss: 255.4422 - MinusLogProbMetric: 255.4422 - val_loss: 250.9398 - val_MinusLogProbMetric: 250.9398 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 59/1000
2023-10-27 05:15:40.318 
Epoch 59/1000 
	 loss: 247.4966, MinusLogProbMetric: 247.4966, val_loss: 244.1108, val_MinusLogProbMetric: 244.1108

Epoch 59: val_loss did not improve from 206.15723
196/196 - 81s - loss: 247.4966 - MinusLogProbMetric: 247.4966 - val_loss: 244.1108 - val_MinusLogProbMetric: 244.1108 - lr: 1.1111e-04 - 81s/epoch - 415ms/step
Epoch 60/1000
2023-10-27 05:17:00.855 
Epoch 60/1000 
	 loss: 242.9642, MinusLogProbMetric: 242.9642, val_loss: 242.0549, val_MinusLogProbMetric: 242.0549

Epoch 60: val_loss did not improve from 206.15723
196/196 - 81s - loss: 242.9642 - MinusLogProbMetric: 242.9642 - val_loss: 242.0549 - val_MinusLogProbMetric: 242.0549 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 61/1000
2023-10-27 05:18:21.234 
Epoch 61/1000 
	 loss: 239.0309, MinusLogProbMetric: 239.0309, val_loss: 236.4659, val_MinusLogProbMetric: 236.4659

Epoch 61: val_loss did not improve from 206.15723
196/196 - 80s - loss: 239.0309 - MinusLogProbMetric: 239.0309 - val_loss: 236.4659 - val_MinusLogProbMetric: 236.4659 - lr: 1.1111e-04 - 80s/epoch - 410ms/step
Epoch 62/1000
2023-10-27 05:19:42.332 
Epoch 62/1000 
	 loss: 233.2477, MinusLogProbMetric: 233.2477, val_loss: 230.2970, val_MinusLogProbMetric: 230.2970

Epoch 62: val_loss did not improve from 206.15723
196/196 - 81s - loss: 233.2477 - MinusLogProbMetric: 233.2477 - val_loss: 230.2970 - val_MinusLogProbMetric: 230.2970 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 63/1000
2023-10-27 05:21:02.690 
Epoch 63/1000 
	 loss: 201.5545, MinusLogProbMetric: 201.5545, val_loss: 181.7314, val_MinusLogProbMetric: 181.7314

Epoch 63: val_loss improved from 206.15723 to 181.73138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 201.5545 - MinusLogProbMetric: 201.5545 - val_loss: 181.7314 - val_MinusLogProbMetric: 181.7314 - lr: 1.1111e-04 - 82s/epoch - 418ms/step
Epoch 64/1000
2023-10-27 05:22:25.255 
Epoch 64/1000 
	 loss: 180.4908, MinusLogProbMetric: 180.4908, val_loss: 176.0834, val_MinusLogProbMetric: 176.0834

Epoch 64: val_loss improved from 181.73138 to 176.08342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 180.4908 - MinusLogProbMetric: 180.4908 - val_loss: 176.0834 - val_MinusLogProbMetric: 176.0834 - lr: 1.1111e-04 - 82s/epoch - 421ms/step
Epoch 65/1000
2023-10-27 05:23:47.969 
Epoch 65/1000 
	 loss: 173.5095, MinusLogProbMetric: 173.5095, val_loss: 170.9070, val_MinusLogProbMetric: 170.9070

Epoch 65: val_loss improved from 176.08342 to 170.90695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 173.5095 - MinusLogProbMetric: 173.5095 - val_loss: 170.9070 - val_MinusLogProbMetric: 170.9070 - lr: 1.1111e-04 - 83s/epoch - 421ms/step
Epoch 66/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 189: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 05:25:07.943 
Epoch 66/1000 
	 loss: nan, MinusLogProbMetric: 169.0003, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 66: val_loss did not improve from 170.90695
196/196 - 79s - loss: nan - MinusLogProbMetric: 169.0003 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 79s/epoch - 401ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 3.703703703703703e-05.
===========
Generating train data for run 400.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_400
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_254"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_255 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f0d6553fdf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d84750af0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d84750af0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bc98475e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c3448c250>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c3448c7c0>, <keras.callbacks.ModelCheckpoint object at 0x7f0c3448c880>, <keras.callbacks.EarlyStopping object at 0x7f0c3448caf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c3448cb20>, <keras.callbacks.TerminateOnNaN object at 0x7f0c3448c760>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 400/720 with hyperparameters:
timestamp = 2023-10-27 05:25:21.708656
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-27 05:29:32.168 
Epoch 1/1000 
	 loss: 237.1802, MinusLogProbMetric: 237.1802, val_loss: 372.4121, val_MinusLogProbMetric: 372.4121

Epoch 1: val_loss improved from inf to 372.41211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 251s - loss: 237.1802 - MinusLogProbMetric: 237.1802 - val_loss: 372.4121 - val_MinusLogProbMetric: 372.4121 - lr: 3.7037e-05 - 251s/epoch - 1s/step
Epoch 2/1000
2023-10-27 05:30:57.616 
Epoch 2/1000 
	 loss: 230.8281, MinusLogProbMetric: 230.8281, val_loss: 180.2729, val_MinusLogProbMetric: 180.2729

Epoch 2: val_loss improved from 372.41211 to 180.27295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 230.8281 - MinusLogProbMetric: 230.8281 - val_loss: 180.2729 - val_MinusLogProbMetric: 180.2729 - lr: 3.7037e-05 - 85s/epoch - 435ms/step
Epoch 3/1000
2023-10-27 05:32:21.142 
Epoch 3/1000 
	 loss: 167.0060, MinusLogProbMetric: 167.0060, val_loss: 156.3682, val_MinusLogProbMetric: 156.3682

Epoch 3: val_loss improved from 180.27295 to 156.36819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 167.0060 - MinusLogProbMetric: 167.0060 - val_loss: 156.3682 - val_MinusLogProbMetric: 156.3682 - lr: 3.7037e-05 - 83s/epoch - 425ms/step
Epoch 4/1000
2023-10-27 05:33:43.258 
Epoch 4/1000 
	 loss: 145.9440, MinusLogProbMetric: 145.9440, val_loss: 136.0325, val_MinusLogProbMetric: 136.0325

Epoch 4: val_loss improved from 156.36819 to 136.03247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 145.9440 - MinusLogProbMetric: 145.9440 - val_loss: 136.0325 - val_MinusLogProbMetric: 136.0325 - lr: 3.7037e-05 - 82s/epoch - 419ms/step
Epoch 5/1000
2023-10-27 05:35:06.070 
Epoch 5/1000 
	 loss: 141.7598, MinusLogProbMetric: 141.7598, val_loss: 139.2470, val_MinusLogProbMetric: 139.2470

Epoch 5: val_loss did not improve from 136.03247
196/196 - 81s - loss: 141.7598 - MinusLogProbMetric: 141.7598 - val_loss: 139.2470 - val_MinusLogProbMetric: 139.2470 - lr: 3.7037e-05 - 81s/epoch - 416ms/step
Epoch 6/1000
2023-10-27 05:36:26.872 
Epoch 6/1000 
	 loss: 124.0046, MinusLogProbMetric: 124.0046, val_loss: 116.4364, val_MinusLogProbMetric: 116.4364

Epoch 6: val_loss improved from 136.03247 to 116.43643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 124.0046 - MinusLogProbMetric: 124.0046 - val_loss: 116.4364 - val_MinusLogProbMetric: 116.4364 - lr: 3.7037e-05 - 82s/epoch - 420ms/step
Epoch 7/1000
2023-10-27 05:37:49.110 
Epoch 7/1000 
	 loss: 116.2702, MinusLogProbMetric: 116.2702, val_loss: 106.9923, val_MinusLogProbMetric: 106.9923

Epoch 7: val_loss improved from 116.43643 to 106.99230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 116.2702 - MinusLogProbMetric: 116.2702 - val_loss: 106.9923 - val_MinusLogProbMetric: 106.9923 - lr: 3.7037e-05 - 82s/epoch - 419ms/step
Epoch 8/1000
2023-10-27 05:39:11.258 
Epoch 8/1000 
	 loss: 124.8758, MinusLogProbMetric: 124.8758, val_loss: 104.2393, val_MinusLogProbMetric: 104.2393

Epoch 8: val_loss improved from 106.99230 to 104.23925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 124.8758 - MinusLogProbMetric: 124.8758 - val_loss: 104.2393 - val_MinusLogProbMetric: 104.2393 - lr: 3.7037e-05 - 82s/epoch - 418ms/step
Epoch 9/1000
2023-10-27 05:40:34.424 
Epoch 9/1000 
	 loss: 113.3462, MinusLogProbMetric: 113.3462, val_loss: 103.6962, val_MinusLogProbMetric: 103.6962

Epoch 9: val_loss improved from 104.23925 to 103.69615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 113.3462 - MinusLogProbMetric: 113.3462 - val_loss: 103.6962 - val_MinusLogProbMetric: 103.6962 - lr: 3.7037e-05 - 83s/epoch - 424ms/step
Epoch 10/1000
2023-10-27 05:41:56.805 
Epoch 10/1000 
	 loss: 104.4051, MinusLogProbMetric: 104.4051, val_loss: 94.7122, val_MinusLogProbMetric: 94.7122

Epoch 10: val_loss improved from 103.69615 to 94.71215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 104.4051 - MinusLogProbMetric: 104.4051 - val_loss: 94.7122 - val_MinusLogProbMetric: 94.7122 - lr: 3.7037e-05 - 82s/epoch - 421ms/step
Epoch 11/1000
2023-10-27 05:43:18.591 
Epoch 11/1000 
	 loss: 106.1363, MinusLogProbMetric: 106.1363, val_loss: 99.5306, val_MinusLogProbMetric: 99.5306

Epoch 11: val_loss did not improve from 94.71215
196/196 - 81s - loss: 106.1363 - MinusLogProbMetric: 106.1363 - val_loss: 99.5306 - val_MinusLogProbMetric: 99.5306 - lr: 3.7037e-05 - 81s/epoch - 411ms/step
Epoch 12/1000
2023-10-27 05:44:39.085 
Epoch 12/1000 
	 loss: 87.3104, MinusLogProbMetric: 87.3104, val_loss: 83.3252, val_MinusLogProbMetric: 83.3252

Epoch 12: val_loss improved from 94.71215 to 83.32517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 87.3104 - MinusLogProbMetric: 87.3104 - val_loss: 83.3252 - val_MinusLogProbMetric: 83.3252 - lr: 3.7037e-05 - 82s/epoch - 417ms/step
Epoch 13/1000
2023-10-27 05:46:01.617 
Epoch 13/1000 
	 loss: 98.9995, MinusLogProbMetric: 98.9995, val_loss: 110.3498, val_MinusLogProbMetric: 110.3498

Epoch 13: val_loss did not improve from 83.32517
196/196 - 81s - loss: 98.9995 - MinusLogProbMetric: 98.9995 - val_loss: 110.3498 - val_MinusLogProbMetric: 110.3498 - lr: 3.7037e-05 - 81s/epoch - 414ms/step
Epoch 14/1000
2023-10-27 05:47:23.071 
Epoch 14/1000 
	 loss: 96.3520, MinusLogProbMetric: 96.3520, val_loss: 90.7064, val_MinusLogProbMetric: 90.7064

Epoch 14: val_loss did not improve from 83.32517
196/196 - 81s - loss: 96.3520 - MinusLogProbMetric: 96.3520 - val_loss: 90.7064 - val_MinusLogProbMetric: 90.7064 - lr: 3.7037e-05 - 81s/epoch - 416ms/step
Epoch 15/1000
2023-10-27 05:48:44.048 
Epoch 15/1000 
	 loss: 91.8309, MinusLogProbMetric: 91.8309, val_loss: 80.8073, val_MinusLogProbMetric: 80.8073

Epoch 15: val_loss improved from 83.32517 to 80.80731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 91.8309 - MinusLogProbMetric: 91.8309 - val_loss: 80.8073 - val_MinusLogProbMetric: 80.8073 - lr: 3.7037e-05 - 82s/epoch - 420ms/step
Epoch 16/1000
2023-10-27 05:50:07.915 
Epoch 16/1000 
	 loss: 77.9641, MinusLogProbMetric: 77.9641, val_loss: 75.2803, val_MinusLogProbMetric: 75.2803

Epoch 16: val_loss improved from 80.80731 to 75.28033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 77.9641 - MinusLogProbMetric: 77.9641 - val_loss: 75.2803 - val_MinusLogProbMetric: 75.2803 - lr: 3.7037e-05 - 84s/epoch - 428ms/step
Epoch 17/1000
2023-10-27 05:51:31.423 
Epoch 17/1000 
	 loss: 287.0904, MinusLogProbMetric: 287.0904, val_loss: 657.4342, val_MinusLogProbMetric: 657.4342

Epoch 17: val_loss did not improve from 75.28033
196/196 - 82s - loss: 287.0904 - MinusLogProbMetric: 287.0904 - val_loss: 657.4342 - val_MinusLogProbMetric: 657.4342 - lr: 3.7037e-05 - 82s/epoch - 419ms/step
Epoch 18/1000
2023-10-27 05:52:52.821 
Epoch 18/1000 
	 loss: 625.5580, MinusLogProbMetric: 625.5580, val_loss: 467.7930, val_MinusLogProbMetric: 467.7930

Epoch 18: val_loss did not improve from 75.28033
196/196 - 81s - loss: 625.5580 - MinusLogProbMetric: 625.5580 - val_loss: 467.7930 - val_MinusLogProbMetric: 467.7930 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 19/1000
2023-10-27 05:54:18.530 
Epoch 19/1000 
	 loss: 461.7971, MinusLogProbMetric: 461.7971, val_loss: 345.3609, val_MinusLogProbMetric: 345.3609

Epoch 19: val_loss did not improve from 75.28033
196/196 - 86s - loss: 461.7971 - MinusLogProbMetric: 461.7971 - val_loss: 345.3609 - val_MinusLogProbMetric: 345.3609 - lr: 3.7037e-05 - 86s/epoch - 437ms/step
Epoch 20/1000
2023-10-27 05:55:40.279 
Epoch 20/1000 
	 loss: 333.4542, MinusLogProbMetric: 333.4542, val_loss: 333.4561, val_MinusLogProbMetric: 333.4561

Epoch 20: val_loss did not improve from 75.28033
196/196 - 82s - loss: 333.4542 - MinusLogProbMetric: 333.4542 - val_loss: 333.4561 - val_MinusLogProbMetric: 333.4561 - lr: 3.7037e-05 - 82s/epoch - 417ms/step
Epoch 21/1000
2023-10-27 05:57:01.881 
Epoch 21/1000 
	 loss: 274.7442, MinusLogProbMetric: 274.7442, val_loss: 237.9142, val_MinusLogProbMetric: 237.9142

Epoch 21: val_loss did not improve from 75.28033
196/196 - 82s - loss: 274.7442 - MinusLogProbMetric: 274.7442 - val_loss: 237.9142 - val_MinusLogProbMetric: 237.9142 - lr: 3.7037e-05 - 82s/epoch - 416ms/step
Epoch 22/1000
2023-10-27 05:58:23.730 
Epoch 22/1000 
	 loss: 219.4533, MinusLogProbMetric: 219.4533, val_loss: 202.6885, val_MinusLogProbMetric: 202.6885

Epoch 22: val_loss did not improve from 75.28033
196/196 - 82s - loss: 219.4533 - MinusLogProbMetric: 219.4533 - val_loss: 202.6885 - val_MinusLogProbMetric: 202.6885 - lr: 3.7037e-05 - 82s/epoch - 418ms/step
Epoch 23/1000
2023-10-27 05:59:45.751 
Epoch 23/1000 
	 loss: 190.2777, MinusLogProbMetric: 190.2777, val_loss: 179.5691, val_MinusLogProbMetric: 179.5691

Epoch 23: val_loss did not improve from 75.28033
196/196 - 82s - loss: 190.2777 - MinusLogProbMetric: 190.2777 - val_loss: 179.5691 - val_MinusLogProbMetric: 179.5691 - lr: 3.7037e-05 - 82s/epoch - 418ms/step
Epoch 24/1000
2023-10-27 06:01:09.049 
Epoch 24/1000 
	 loss: 174.6561, MinusLogProbMetric: 174.6561, val_loss: 189.2749, val_MinusLogProbMetric: 189.2749

Epoch 24: val_loss did not improve from 75.28033
196/196 - 83s - loss: 174.6561 - MinusLogProbMetric: 174.6561 - val_loss: 189.2749 - val_MinusLogProbMetric: 189.2749 - lr: 3.7037e-05 - 83s/epoch - 425ms/step
Epoch 25/1000
2023-10-27 06:02:32.201 
Epoch 25/1000 
	 loss: 166.2220, MinusLogProbMetric: 166.2220, val_loss: 158.7170, val_MinusLogProbMetric: 158.7170

Epoch 25: val_loss did not improve from 75.28033
196/196 - 83s - loss: 166.2220 - MinusLogProbMetric: 166.2220 - val_loss: 158.7170 - val_MinusLogProbMetric: 158.7170 - lr: 3.7037e-05 - 83s/epoch - 424ms/step
Epoch 26/1000
2023-10-27 06:03:53.508 
Epoch 26/1000 
	 loss: 150.4874, MinusLogProbMetric: 150.4874, val_loss: 144.8999, val_MinusLogProbMetric: 144.8999

Epoch 26: val_loss did not improve from 75.28033
196/196 - 81s - loss: 150.4874 - MinusLogProbMetric: 150.4874 - val_loss: 144.8999 - val_MinusLogProbMetric: 144.8999 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 27/1000
2023-10-27 06:05:15.349 
Epoch 27/1000 
	 loss: 186.0561, MinusLogProbMetric: 186.0561, val_loss: 167.8886, val_MinusLogProbMetric: 167.8886

Epoch 27: val_loss did not improve from 75.28033
196/196 - 82s - loss: 186.0561 - MinusLogProbMetric: 186.0561 - val_loss: 167.8886 - val_MinusLogProbMetric: 167.8886 - lr: 3.7037e-05 - 82s/epoch - 418ms/step
Epoch 28/1000
2023-10-27 06:06:37.024 
Epoch 28/1000 
	 loss: 152.2206, MinusLogProbMetric: 152.2206, val_loss: 143.8918, val_MinusLogProbMetric: 143.8918

Epoch 28: val_loss did not improve from 75.28033
196/196 - 82s - loss: 152.2206 - MinusLogProbMetric: 152.2206 - val_loss: 143.8918 - val_MinusLogProbMetric: 143.8918 - lr: 3.7037e-05 - 82s/epoch - 417ms/step
Epoch 29/1000
2023-10-27 06:07:57.182 
Epoch 29/1000 
	 loss: 137.4950, MinusLogProbMetric: 137.4950, val_loss: 133.5341, val_MinusLogProbMetric: 133.5341

Epoch 29: val_loss did not improve from 75.28033
196/196 - 80s - loss: 137.4950 - MinusLogProbMetric: 137.4950 - val_loss: 133.5341 - val_MinusLogProbMetric: 133.5341 - lr: 3.7037e-05 - 80s/epoch - 409ms/step
Epoch 30/1000
2023-10-27 06:09:17.067 
Epoch 30/1000 
	 loss: 160.1203, MinusLogProbMetric: 160.1203, val_loss: 139.6564, val_MinusLogProbMetric: 139.6564

Epoch 30: val_loss did not improve from 75.28033
196/196 - 80s - loss: 160.1203 - MinusLogProbMetric: 160.1203 - val_loss: 139.6564 - val_MinusLogProbMetric: 139.6564 - lr: 3.7037e-05 - 80s/epoch - 408ms/step
Epoch 31/1000
2023-10-27 06:10:37.210 
Epoch 31/1000 
	 loss: 132.3457, MinusLogProbMetric: 132.3457, val_loss: 126.6254, val_MinusLogProbMetric: 126.6254

Epoch 31: val_loss did not improve from 75.28033
196/196 - 80s - loss: 132.3457 - MinusLogProbMetric: 132.3457 - val_loss: 126.6254 - val_MinusLogProbMetric: 126.6254 - lr: 3.7037e-05 - 80s/epoch - 409ms/step
Epoch 32/1000
2023-10-27 06:11:57.788 
Epoch 32/1000 
	 loss: 122.7609, MinusLogProbMetric: 122.7609, val_loss: 119.7196, val_MinusLogProbMetric: 119.7196

Epoch 32: val_loss did not improve from 75.28033
196/196 - 81s - loss: 122.7609 - MinusLogProbMetric: 122.7609 - val_loss: 119.7196 - val_MinusLogProbMetric: 119.7196 - lr: 3.7037e-05 - 81s/epoch - 411ms/step
Epoch 33/1000
2023-10-27 06:13:19.313 
Epoch 33/1000 
	 loss: 117.4758, MinusLogProbMetric: 117.4758, val_loss: 114.8922, val_MinusLogProbMetric: 114.8922

Epoch 33: val_loss did not improve from 75.28033
196/196 - 82s - loss: 117.4758 - MinusLogProbMetric: 117.4758 - val_loss: 114.8922 - val_MinusLogProbMetric: 114.8922 - lr: 3.7037e-05 - 82s/epoch - 416ms/step
Epoch 34/1000
2023-10-27 06:14:40.870 
Epoch 34/1000 
	 loss: 112.5589, MinusLogProbMetric: 112.5589, val_loss: 110.3214, val_MinusLogProbMetric: 110.3214

Epoch 34: val_loss did not improve from 75.28033
196/196 - 82s - loss: 112.5589 - MinusLogProbMetric: 112.5589 - val_loss: 110.3214 - val_MinusLogProbMetric: 110.3214 - lr: 3.7037e-05 - 82s/epoch - 416ms/step
Epoch 35/1000
2023-10-27 06:16:00.321 
Epoch 35/1000 
	 loss: 122.1232, MinusLogProbMetric: 122.1232, val_loss: 216.6437, val_MinusLogProbMetric: 216.6437

Epoch 35: val_loss did not improve from 75.28033
196/196 - 79s - loss: 122.1232 - MinusLogProbMetric: 122.1232 - val_loss: 216.6437 - val_MinusLogProbMetric: 216.6437 - lr: 3.7037e-05 - 79s/epoch - 405ms/step
Epoch 36/1000
2023-10-27 06:17:20.712 
Epoch 36/1000 
	 loss: 131.2503, MinusLogProbMetric: 131.2503, val_loss: 114.9267, val_MinusLogProbMetric: 114.9267

Epoch 36: val_loss did not improve from 75.28033
196/196 - 80s - loss: 131.2503 - MinusLogProbMetric: 131.2503 - val_loss: 114.9267 - val_MinusLogProbMetric: 114.9267 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 37/1000
2023-10-27 06:18:42.083 
Epoch 37/1000 
	 loss: 109.6596, MinusLogProbMetric: 109.6596, val_loss: 106.5198, val_MinusLogProbMetric: 106.5198

Epoch 37: val_loss did not improve from 75.28033
196/196 - 81s - loss: 109.6596 - MinusLogProbMetric: 109.6596 - val_loss: 106.5198 - val_MinusLogProbMetric: 106.5198 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 38/1000
2023-10-27 06:20:02.947 
Epoch 38/1000 
	 loss: 103.6233, MinusLogProbMetric: 103.6233, val_loss: 100.8090, val_MinusLogProbMetric: 100.8090

Epoch 38: val_loss did not improve from 75.28033
196/196 - 81s - loss: 103.6233 - MinusLogProbMetric: 103.6233 - val_loss: 100.8090 - val_MinusLogProbMetric: 100.8090 - lr: 3.7037e-05 - 81s/epoch - 413ms/step
Epoch 39/1000
2023-10-27 06:21:23.152 
Epoch 39/1000 
	 loss: 99.3467, MinusLogProbMetric: 99.3467, val_loss: 97.6488, val_MinusLogProbMetric: 97.6488

Epoch 39: val_loss did not improve from 75.28033
196/196 - 80s - loss: 99.3467 - MinusLogProbMetric: 99.3467 - val_loss: 97.6488 - val_MinusLogProbMetric: 97.6488 - lr: 3.7037e-05 - 80s/epoch - 409ms/step
Epoch 40/1000
2023-10-27 06:22:43.828 
Epoch 40/1000 
	 loss: 96.6326, MinusLogProbMetric: 96.6326, val_loss: 95.1826, val_MinusLogProbMetric: 95.1826

Epoch 40: val_loss did not improve from 75.28033
196/196 - 81s - loss: 96.6326 - MinusLogProbMetric: 96.6326 - val_loss: 95.1826 - val_MinusLogProbMetric: 95.1826 - lr: 3.7037e-05 - 81s/epoch - 412ms/step
Epoch 41/1000
2023-10-27 06:24:05.918 
Epoch 41/1000 
	 loss: 94.0915, MinusLogProbMetric: 94.0915, val_loss: 92.7295, val_MinusLogProbMetric: 92.7295

Epoch 41: val_loss did not improve from 75.28033
196/196 - 82s - loss: 94.0915 - MinusLogProbMetric: 94.0915 - val_loss: 92.7295 - val_MinusLogProbMetric: 92.7295 - lr: 3.7037e-05 - 82s/epoch - 419ms/step
Epoch 42/1000
2023-10-27 06:25:26.974 
Epoch 42/1000 
	 loss: 92.2181, MinusLogProbMetric: 92.2181, val_loss: 91.2006, val_MinusLogProbMetric: 91.2006

Epoch 42: val_loss did not improve from 75.28033
196/196 - 81s - loss: 92.2181 - MinusLogProbMetric: 92.2181 - val_loss: 91.2006 - val_MinusLogProbMetric: 91.2006 - lr: 3.7037e-05 - 81s/epoch - 414ms/step
Epoch 43/1000
2023-10-27 06:26:48.374 
Epoch 43/1000 
	 loss: 89.9875, MinusLogProbMetric: 89.9875, val_loss: 88.8561, val_MinusLogProbMetric: 88.8561

Epoch 43: val_loss did not improve from 75.28033
196/196 - 81s - loss: 89.9875 - MinusLogProbMetric: 89.9875 - val_loss: 88.8561 - val_MinusLogProbMetric: 88.8561 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 44/1000
2023-10-27 06:28:08.747 
Epoch 44/1000 
	 loss: 88.0730, MinusLogProbMetric: 88.0730, val_loss: 87.1876, val_MinusLogProbMetric: 87.1876

Epoch 44: val_loss did not improve from 75.28033
196/196 - 80s - loss: 88.0730 - MinusLogProbMetric: 88.0730 - val_loss: 87.1876 - val_MinusLogProbMetric: 87.1876 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 45/1000
2023-10-27 06:29:16.732 
Epoch 45/1000 
	 loss: 86.8194, MinusLogProbMetric: 86.8194, val_loss: 86.0061, val_MinusLogProbMetric: 86.0061

Epoch 45: val_loss did not improve from 75.28033
196/196 - 68s - loss: 86.8194 - MinusLogProbMetric: 86.8194 - val_loss: 86.0061 - val_MinusLogProbMetric: 86.0061 - lr: 3.7037e-05 - 68s/epoch - 347ms/step
Epoch 46/1000
2023-10-27 06:30:25.273 
Epoch 46/1000 
	 loss: 85.0549, MinusLogProbMetric: 85.0549, val_loss: 84.2447, val_MinusLogProbMetric: 84.2447

Epoch 46: val_loss did not improve from 75.28033
196/196 - 69s - loss: 85.0549 - MinusLogProbMetric: 85.0549 - val_loss: 84.2447 - val_MinusLogProbMetric: 84.2447 - lr: 3.7037e-05 - 69s/epoch - 350ms/step
Epoch 47/1000
2023-10-27 06:31:44.057 
Epoch 47/1000 
	 loss: 104.4741, MinusLogProbMetric: 104.4741, val_loss: 87.1677, val_MinusLogProbMetric: 87.1677

Epoch 47: val_loss did not improve from 75.28033
196/196 - 79s - loss: 104.4741 - MinusLogProbMetric: 104.4741 - val_loss: 87.1677 - val_MinusLogProbMetric: 87.1677 - lr: 3.7037e-05 - 79s/epoch - 402ms/step
Epoch 48/1000
2023-10-27 06:33:04.848 
Epoch 48/1000 
	 loss: 83.9028, MinusLogProbMetric: 83.9028, val_loss: 82.2945, val_MinusLogProbMetric: 82.2945

Epoch 48: val_loss did not improve from 75.28033
196/196 - 81s - loss: 83.9028 - MinusLogProbMetric: 83.9028 - val_loss: 82.2945 - val_MinusLogProbMetric: 82.2945 - lr: 3.7037e-05 - 81s/epoch - 412ms/step
Epoch 49/1000
2023-10-27 06:34:24.712 
Epoch 49/1000 
	 loss: 81.1183, MinusLogProbMetric: 81.1183, val_loss: 80.8792, val_MinusLogProbMetric: 80.8792

Epoch 49: val_loss did not improve from 75.28033
196/196 - 80s - loss: 81.1183 - MinusLogProbMetric: 81.1183 - val_loss: 80.8792 - val_MinusLogProbMetric: 80.8792 - lr: 3.7037e-05 - 80s/epoch - 407ms/step
Epoch 50/1000
2023-10-27 06:35:45.120 
Epoch 50/1000 
	 loss: 79.6375, MinusLogProbMetric: 79.6375, val_loss: 79.6042, val_MinusLogProbMetric: 79.6042

Epoch 50: val_loss did not improve from 75.28033
196/196 - 80s - loss: 79.6375 - MinusLogProbMetric: 79.6375 - val_loss: 79.6042 - val_MinusLogProbMetric: 79.6042 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 51/1000
2023-10-27 06:37:05.896 
Epoch 51/1000 
	 loss: 78.0913, MinusLogProbMetric: 78.0913, val_loss: 77.5928, val_MinusLogProbMetric: 77.5928

Epoch 51: val_loss did not improve from 75.28033
196/196 - 81s - loss: 78.0913 - MinusLogProbMetric: 78.0913 - val_loss: 77.5928 - val_MinusLogProbMetric: 77.5928 - lr: 3.7037e-05 - 81s/epoch - 412ms/step
Epoch 52/1000
2023-10-27 06:38:27.234 
Epoch 52/1000 
	 loss: 76.9501, MinusLogProbMetric: 76.9501, val_loss: 76.3057, val_MinusLogProbMetric: 76.3057

Epoch 52: val_loss did not improve from 75.28033
196/196 - 81s - loss: 76.9501 - MinusLogProbMetric: 76.9501 - val_loss: 76.3057 - val_MinusLogProbMetric: 76.3057 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 53/1000
2023-10-27 06:39:47.583 
Epoch 53/1000 
	 loss: 122.0741, MinusLogProbMetric: 122.0741, val_loss: 100.0554, val_MinusLogProbMetric: 100.0554

Epoch 53: val_loss did not improve from 75.28033
196/196 - 80s - loss: 122.0741 - MinusLogProbMetric: 122.0741 - val_loss: 100.0554 - val_MinusLogProbMetric: 100.0554 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 54/1000
2023-10-27 06:41:08.812 
Epoch 54/1000 
	 loss: 91.7821, MinusLogProbMetric: 91.7821, val_loss: 90.3113, val_MinusLogProbMetric: 90.3113

Epoch 54: val_loss did not improve from 75.28033
196/196 - 81s - loss: 91.7821 - MinusLogProbMetric: 91.7821 - val_loss: 90.3113 - val_MinusLogProbMetric: 90.3113 - lr: 3.7037e-05 - 81s/epoch - 414ms/step
Epoch 55/1000
2023-10-27 06:42:29.425 
Epoch 55/1000 
	 loss: 85.1964, MinusLogProbMetric: 85.1964, val_loss: 82.4878, val_MinusLogProbMetric: 82.4878

Epoch 55: val_loss did not improve from 75.28033
196/196 - 81s - loss: 85.1964 - MinusLogProbMetric: 85.1964 - val_loss: 82.4878 - val_MinusLogProbMetric: 82.4878 - lr: 3.7037e-05 - 81s/epoch - 411ms/step
Epoch 56/1000
2023-10-27 06:43:50.380 
Epoch 56/1000 
	 loss: 82.0193, MinusLogProbMetric: 82.0193, val_loss: 79.8922, val_MinusLogProbMetric: 79.8922

Epoch 56: val_loss did not improve from 75.28033
196/196 - 81s - loss: 82.0193 - MinusLogProbMetric: 82.0193 - val_loss: 79.8922 - val_MinusLogProbMetric: 79.8922 - lr: 3.7037e-05 - 81s/epoch - 413ms/step
Epoch 57/1000
2023-10-27 06:45:11.769 
Epoch 57/1000 
	 loss: 79.0568, MinusLogProbMetric: 79.0568, val_loss: 77.6962, val_MinusLogProbMetric: 77.6962

Epoch 57: val_loss did not improve from 75.28033
196/196 - 81s - loss: 79.0568 - MinusLogProbMetric: 79.0568 - val_loss: 77.6962 - val_MinusLogProbMetric: 77.6962 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 58/1000
2023-10-27 06:46:33.121 
Epoch 58/1000 
	 loss: 85.2176, MinusLogProbMetric: 85.2176, val_loss: 82.6537, val_MinusLogProbMetric: 82.6537

Epoch 58: val_loss did not improve from 75.28033
196/196 - 81s - loss: 85.2176 - MinusLogProbMetric: 85.2176 - val_loss: 82.6537 - val_MinusLogProbMetric: 82.6537 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 59/1000
2023-10-27 06:47:55.655 
Epoch 59/1000 
	 loss: 77.8793, MinusLogProbMetric: 77.8793, val_loss: 75.4543, val_MinusLogProbMetric: 75.4543

Epoch 59: val_loss did not improve from 75.28033
196/196 - 83s - loss: 77.8793 - MinusLogProbMetric: 77.8793 - val_loss: 75.4543 - val_MinusLogProbMetric: 75.4543 - lr: 3.7037e-05 - 83s/epoch - 421ms/step
Epoch 60/1000
2023-10-27 06:49:17.053 
Epoch 60/1000 
	 loss: 74.4460, MinusLogProbMetric: 74.4460, val_loss: 73.5819, val_MinusLogProbMetric: 73.5819

Epoch 60: val_loss improved from 75.28033 to 73.58186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 74.4460 - MinusLogProbMetric: 74.4460 - val_loss: 73.5819 - val_MinusLogProbMetric: 73.5819 - lr: 3.7037e-05 - 83s/epoch - 422ms/step
Epoch 61/1000
2023-10-27 06:50:39.013 
Epoch 61/1000 
	 loss: 72.8963, MinusLogProbMetric: 72.8963, val_loss: 72.3732, val_MinusLogProbMetric: 72.3732

Epoch 61: val_loss improved from 73.58186 to 72.37318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 72.8963 - MinusLogProbMetric: 72.8963 - val_loss: 72.3732 - val_MinusLogProbMetric: 72.3732 - lr: 3.7037e-05 - 82s/epoch - 417ms/step
Epoch 62/1000
2023-10-27 06:52:00.492 
Epoch 62/1000 
	 loss: 71.8060, MinusLogProbMetric: 71.8060, val_loss: 71.1161, val_MinusLogProbMetric: 71.1161

Epoch 62: val_loss improved from 72.37318 to 71.11605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 71.8060 - MinusLogProbMetric: 71.8060 - val_loss: 71.1161 - val_MinusLogProbMetric: 71.1161 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 63/1000
2023-10-27 06:53:20.734 
Epoch 63/1000 
	 loss: 70.5881, MinusLogProbMetric: 70.5881, val_loss: 70.0232, val_MinusLogProbMetric: 70.0232

Epoch 63: val_loss improved from 71.11605 to 70.02320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 70.5881 - MinusLogProbMetric: 70.5881 - val_loss: 70.0232 - val_MinusLogProbMetric: 70.0232 - lr: 3.7037e-05 - 81s/epoch - 411ms/step
Epoch 64/1000
2023-10-27 06:54:39.988 
Epoch 64/1000 
	 loss: 69.5793, MinusLogProbMetric: 69.5793, val_loss: 69.1278, val_MinusLogProbMetric: 69.1278

Epoch 64: val_loss improved from 70.02320 to 69.12777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 79s - loss: 69.5793 - MinusLogProbMetric: 69.5793 - val_loss: 69.1278 - val_MinusLogProbMetric: 69.1278 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 65/1000
2023-10-27 06:56:00.463 
Epoch 65/1000 
	 loss: 68.6229, MinusLogProbMetric: 68.6229, val_loss: 68.3962, val_MinusLogProbMetric: 68.3962

Epoch 65: val_loss improved from 69.12777 to 68.39616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 68.6229 - MinusLogProbMetric: 68.6229 - val_loss: 68.3962 - val_MinusLogProbMetric: 68.3962 - lr: 3.7037e-05 - 81s/epoch - 413ms/step
Epoch 66/1000
2023-10-27 06:57:22.213 
Epoch 66/1000 
	 loss: 68.7146, MinusLogProbMetric: 68.7146, val_loss: 67.4554, val_MinusLogProbMetric: 67.4554

Epoch 66: val_loss improved from 68.39616 to 67.45541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 68.7146 - MinusLogProbMetric: 68.7146 - val_loss: 67.4554 - val_MinusLogProbMetric: 67.4554 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 67/1000
2023-10-27 06:58:43.513 
Epoch 67/1000 
	 loss: 67.1759, MinusLogProbMetric: 67.1759, val_loss: 67.4532, val_MinusLogProbMetric: 67.4532

Epoch 67: val_loss improved from 67.45541 to 67.45322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 67.1759 - MinusLogProbMetric: 67.1759 - val_loss: 67.4532 - val_MinusLogProbMetric: 67.4532 - lr: 3.7037e-05 - 82s/epoch - 418ms/step
Epoch 68/1000
2023-10-27 07:00:05.813 
Epoch 68/1000 
	 loss: 66.4759, MinusLogProbMetric: 66.4759, val_loss: 67.4592, val_MinusLogProbMetric: 67.4592

Epoch 68: val_loss did not improve from 67.45322
196/196 - 81s - loss: 66.4759 - MinusLogProbMetric: 66.4759 - val_loss: 67.4592 - val_MinusLogProbMetric: 67.4592 - lr: 3.7037e-05 - 81s/epoch - 412ms/step
Epoch 69/1000
2023-10-27 07:01:27.173 
Epoch 69/1000 
	 loss: 79.9965, MinusLogProbMetric: 79.9965, val_loss: 156.5497, val_MinusLogProbMetric: 156.5497

Epoch 69: val_loss did not improve from 67.45322
196/196 - 81s - loss: 79.9965 - MinusLogProbMetric: 79.9965 - val_loss: 156.5497 - val_MinusLogProbMetric: 156.5497 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 70/1000
2023-10-27 07:02:49.000 
Epoch 70/1000 
	 loss: 78.5336, MinusLogProbMetric: 78.5336, val_loss: 68.1207, val_MinusLogProbMetric: 68.1207

Epoch 70: val_loss did not improve from 67.45322
196/196 - 82s - loss: 78.5336 - MinusLogProbMetric: 78.5336 - val_loss: 68.1207 - val_MinusLogProbMetric: 68.1207 - lr: 3.7037e-05 - 82s/epoch - 417ms/step
Epoch 71/1000
2023-10-27 07:04:10.059 
Epoch 71/1000 
	 loss: 67.4097, MinusLogProbMetric: 67.4097, val_loss: 65.3907, val_MinusLogProbMetric: 65.3907

Epoch 71: val_loss improved from 67.45322 to 65.39074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 67.4097 - MinusLogProbMetric: 67.4097 - val_loss: 65.3907 - val_MinusLogProbMetric: 65.3907 - lr: 3.7037e-05 - 83s/epoch - 423ms/step
Epoch 72/1000
2023-10-27 07:05:31.999 
Epoch 72/1000 
	 loss: 65.0319, MinusLogProbMetric: 65.0319, val_loss: 64.4449, val_MinusLogProbMetric: 64.4449

Epoch 72: val_loss improved from 65.39074 to 64.44493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 65.0319 - MinusLogProbMetric: 65.0319 - val_loss: 64.4449 - val_MinusLogProbMetric: 64.4449 - lr: 3.7037e-05 - 81s/epoch - 415ms/step
Epoch 73/1000
2023-10-27 07:06:47.345 
Epoch 73/1000 
	 loss: 63.7994, MinusLogProbMetric: 63.7994, val_loss: 63.4032, val_MinusLogProbMetric: 63.4032

Epoch 73: val_loss improved from 64.44493 to 63.40321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 75s - loss: 63.7994 - MinusLogProbMetric: 63.7994 - val_loss: 63.4032 - val_MinusLogProbMetric: 63.4032 - lr: 3.7037e-05 - 75s/epoch - 384ms/step
Epoch 74/1000
2023-10-27 07:07:57.062 
Epoch 74/1000 
	 loss: 70.1538, MinusLogProbMetric: 70.1538, val_loss: 83.4334, val_MinusLogProbMetric: 83.4334

Epoch 74: val_loss did not improve from 63.40321
196/196 - 68s - loss: 70.1538 - MinusLogProbMetric: 70.1538 - val_loss: 83.4334 - val_MinusLogProbMetric: 83.4334 - lr: 3.7037e-05 - 68s/epoch - 349ms/step
Epoch 75/1000
2023-10-27 07:09:17.372 
Epoch 75/1000 
	 loss: 98.8904, MinusLogProbMetric: 98.8904, val_loss: 69.2157, val_MinusLogProbMetric: 69.2157

Epoch 75: val_loss did not improve from 63.40321
196/196 - 80s - loss: 98.8904 - MinusLogProbMetric: 98.8904 - val_loss: 69.2157 - val_MinusLogProbMetric: 69.2157 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 76/1000
2023-10-27 07:10:37.319 
Epoch 76/1000 
	 loss: 65.7857, MinusLogProbMetric: 65.7857, val_loss: 63.9539, val_MinusLogProbMetric: 63.9539

Epoch 76: val_loss did not improve from 63.40321
196/196 - 80s - loss: 65.7857 - MinusLogProbMetric: 65.7857 - val_loss: 63.9539 - val_MinusLogProbMetric: 63.9539 - lr: 3.7037e-05 - 80s/epoch - 408ms/step
Epoch 77/1000
2023-10-27 07:11:57.970 
Epoch 77/1000 
	 loss: 64.3414, MinusLogProbMetric: 64.3414, val_loss: 63.8953, val_MinusLogProbMetric: 63.8953

Epoch 77: val_loss did not improve from 63.40321
196/196 - 81s - loss: 64.3414 - MinusLogProbMetric: 64.3414 - val_loss: 63.8953 - val_MinusLogProbMetric: 63.8953 - lr: 3.7037e-05 - 81s/epoch - 411ms/step
Epoch 78/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 153: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 07:13:02.722 
Epoch 78/1000 
	 loss: nan, MinusLogProbMetric: 66.7892, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 78: val_loss did not improve from 63.40321
196/196 - 65s - loss: nan - MinusLogProbMetric: 66.7892 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 65s/epoch - 330ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.2345679012345677e-05.
===========
Generating train data for run 400.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_400/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_400
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_265"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_266 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f0d65273e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b84972350>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b84972350>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c045a3040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ce01a5300>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ce01a49d0>, <keras.callbacks.ModelCheckpoint object at 0x7f0ce01a4be0>, <keras.callbacks.EarlyStopping object at 0x7f0ce01a40d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ce01a43a0>, <keras.callbacks.TerminateOnNaN object at 0x7f0ce01a4ca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 400/720 with hyperparameters:
timestamp = 2023-10-27 07:13:15.878469
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-27 07:17:32.484 
Epoch 1/1000 
	 loss: 66.5014, MinusLogProbMetric: 66.5014, val_loss: 62.2142, val_MinusLogProbMetric: 62.2142

Epoch 1: val_loss improved from inf to 62.21424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 258s - loss: 66.5014 - MinusLogProbMetric: 66.5014 - val_loss: 62.2142 - val_MinusLogProbMetric: 62.2142 - lr: 1.2346e-05 - 258s/epoch - 1s/step
Epoch 2/1000
2023-10-27 07:18:56.539 
Epoch 2/1000 
	 loss: 61.0706, MinusLogProbMetric: 61.0706, val_loss: 60.4769, val_MinusLogProbMetric: 60.4769

Epoch 2: val_loss improved from 62.21424 to 60.47691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 61.0706 - MinusLogProbMetric: 61.0706 - val_loss: 60.4769 - val_MinusLogProbMetric: 60.4769 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 3/1000
2023-10-27 07:20:20.150 
Epoch 3/1000 
	 loss: 59.3701, MinusLogProbMetric: 59.3701, val_loss: 59.8868, val_MinusLogProbMetric: 59.8868

Epoch 3: val_loss improved from 60.47691 to 59.88678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 59.3701 - MinusLogProbMetric: 59.3701 - val_loss: 59.8868 - val_MinusLogProbMetric: 59.8868 - lr: 1.2346e-05 - 84s/epoch - 426ms/step
Epoch 4/1000
2023-10-27 07:21:44.473 
Epoch 4/1000 
	 loss: 67.3560, MinusLogProbMetric: 67.3560, val_loss: 71.5674, val_MinusLogProbMetric: 71.5674

Epoch 4: val_loss did not improve from 59.88678
196/196 - 83s - loss: 67.3560 - MinusLogProbMetric: 67.3560 - val_loss: 71.5674 - val_MinusLogProbMetric: 71.5674 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 5/1000
2023-10-27 07:23:07.349 
Epoch 5/1000 
	 loss: 58.1292, MinusLogProbMetric: 58.1292, val_loss: 56.9802, val_MinusLogProbMetric: 56.9802

Epoch 5: val_loss improved from 59.88678 to 56.98019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 58.1292 - MinusLogProbMetric: 58.1292 - val_loss: 56.9802 - val_MinusLogProbMetric: 56.9802 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 6/1000
2023-10-27 07:24:32.842 
Epoch 6/1000 
	 loss: 55.9386, MinusLogProbMetric: 55.9386, val_loss: 55.2249, val_MinusLogProbMetric: 55.2249

Epoch 6: val_loss improved from 56.98019 to 55.22489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 55.9386 - MinusLogProbMetric: 55.9386 - val_loss: 55.2249 - val_MinusLogProbMetric: 55.2249 - lr: 1.2346e-05 - 86s/epoch - 437ms/step
Epoch 7/1000
2023-10-27 07:25:58.318 
Epoch 7/1000 
	 loss: 58.7069, MinusLogProbMetric: 58.7069, val_loss: 56.2009, val_MinusLogProbMetric: 56.2009

Epoch 7: val_loss did not improve from 55.22489
196/196 - 84s - loss: 58.7069 - MinusLogProbMetric: 58.7069 - val_loss: 56.2009 - val_MinusLogProbMetric: 56.2009 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 8/1000
2023-10-27 07:27:22.481 
Epoch 8/1000 
	 loss: 54.5955, MinusLogProbMetric: 54.5955, val_loss: 53.4385, val_MinusLogProbMetric: 53.4385

Epoch 8: val_loss improved from 55.22489 to 53.43847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 54.5955 - MinusLogProbMetric: 54.5955 - val_loss: 53.4385 - val_MinusLogProbMetric: 53.4385 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 9/1000
2023-10-27 07:28:48.061 
Epoch 9/1000 
	 loss: 60.1107, MinusLogProbMetric: 60.1107, val_loss: 191.4648, val_MinusLogProbMetric: 191.4648

Epoch 9: val_loss did not improve from 53.43847
196/196 - 84s - loss: 60.1107 - MinusLogProbMetric: 60.1107 - val_loss: 191.4648 - val_MinusLogProbMetric: 191.4648 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 10/1000
2023-10-27 07:30:10.378 
Epoch 10/1000 
	 loss: 70.3364, MinusLogProbMetric: 70.3364, val_loss: 54.2340, val_MinusLogProbMetric: 54.2340

Epoch 10: val_loss did not improve from 53.43847
196/196 - 82s - loss: 70.3364 - MinusLogProbMetric: 70.3364 - val_loss: 54.2340 - val_MinusLogProbMetric: 54.2340 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 11/1000
2023-10-27 07:31:33.181 
Epoch 11/1000 
	 loss: 52.5481, MinusLogProbMetric: 52.5481, val_loss: 51.4906, val_MinusLogProbMetric: 51.4906

Epoch 11: val_loss improved from 53.43847 to 51.49063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 52.5481 - MinusLogProbMetric: 52.5481 - val_loss: 51.4906 - val_MinusLogProbMetric: 51.4906 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 12/1000
2023-10-27 07:32:58.131 
Epoch 12/1000 
	 loss: 52.0146, MinusLogProbMetric: 52.0146, val_loss: 50.8373, val_MinusLogProbMetric: 50.8373

Epoch 12: val_loss improved from 51.49063 to 50.83732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 52.0146 - MinusLogProbMetric: 52.0146 - val_loss: 50.8373 - val_MinusLogProbMetric: 50.8373 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 13/1000
2023-10-27 07:34:22.798 
Epoch 13/1000 
	 loss: 50.4194, MinusLogProbMetric: 50.4194, val_loss: 49.7650, val_MinusLogProbMetric: 49.7650

Epoch 13: val_loss improved from 50.83732 to 49.76502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 50.4194 - MinusLogProbMetric: 50.4194 - val_loss: 49.7650 - val_MinusLogProbMetric: 49.7650 - lr: 1.2346e-05 - 85s/epoch - 431ms/step
Epoch 14/1000
2023-10-27 07:35:47.810 
Epoch 14/1000 
	 loss: 49.4290, MinusLogProbMetric: 49.4290, val_loss: 49.4979, val_MinusLogProbMetric: 49.4979

Epoch 14: val_loss improved from 49.76502 to 49.49791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 49.4290 - MinusLogProbMetric: 49.4290 - val_loss: 49.4979 - val_MinusLogProbMetric: 49.4979 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 15/1000
2023-10-27 07:37:12.818 
Epoch 15/1000 
	 loss: 48.7546, MinusLogProbMetric: 48.7546, val_loss: 48.6649, val_MinusLogProbMetric: 48.6649

Epoch 15: val_loss improved from 49.49791 to 48.66495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 48.7546 - MinusLogProbMetric: 48.7546 - val_loss: 48.6649 - val_MinusLogProbMetric: 48.6649 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 16/1000
2023-10-27 07:38:35.358 
Epoch 16/1000 
	 loss: 48.0682, MinusLogProbMetric: 48.0682, val_loss: 47.9600, val_MinusLogProbMetric: 47.9600

Epoch 16: val_loss improved from 48.66495 to 47.96003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 48.0682 - MinusLogProbMetric: 48.0682 - val_loss: 47.9600 - val_MinusLogProbMetric: 47.9600 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 17/1000
2023-10-27 07:39:46.023 
Epoch 17/1000 
	 loss: 49.2066, MinusLogProbMetric: 49.2066, val_loss: 47.2063, val_MinusLogProbMetric: 47.2063

Epoch 17: val_loss improved from 47.96003 to 47.20629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 71s - loss: 49.2066 - MinusLogProbMetric: 49.2066 - val_loss: 47.2063 - val_MinusLogProbMetric: 47.2063 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 18/1000
2023-10-27 07:41:06.735 
Epoch 18/1000 
	 loss: 46.9723, MinusLogProbMetric: 46.9723, val_loss: 46.8036, val_MinusLogProbMetric: 46.8036

Epoch 18: val_loss improved from 47.20629 to 46.80358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 46.9723 - MinusLogProbMetric: 46.9723 - val_loss: 46.8036 - val_MinusLogProbMetric: 46.8036 - lr: 1.2346e-05 - 81s/epoch - 413ms/step
Epoch 19/1000
2023-10-27 07:42:32.242 
Epoch 19/1000 
	 loss: 46.5191, MinusLogProbMetric: 46.5191, val_loss: 46.5255, val_MinusLogProbMetric: 46.5255

Epoch 19: val_loss improved from 46.80358 to 46.52551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 46.5191 - MinusLogProbMetric: 46.5191 - val_loss: 46.5255 - val_MinusLogProbMetric: 46.5255 - lr: 1.2346e-05 - 86s/epoch - 437ms/step
Epoch 20/1000
2023-10-27 07:43:58.482 
Epoch 20/1000 
	 loss: 45.9290, MinusLogProbMetric: 45.9290, val_loss: 45.9426, val_MinusLogProbMetric: 45.9426

Epoch 20: val_loss improved from 46.52551 to 45.94264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 45.9290 - MinusLogProbMetric: 45.9290 - val_loss: 45.9426 - val_MinusLogProbMetric: 45.9426 - lr: 1.2346e-05 - 86s/epoch - 439ms/step
Epoch 21/1000
2023-10-27 07:45:24.687 
Epoch 21/1000 
	 loss: 45.5469, MinusLogProbMetric: 45.5469, val_loss: 45.3641, val_MinusLogProbMetric: 45.3641

Epoch 21: val_loss improved from 45.94264 to 45.36412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 45.5469 - MinusLogProbMetric: 45.5469 - val_loss: 45.3641 - val_MinusLogProbMetric: 45.3641 - lr: 1.2346e-05 - 86s/epoch - 439ms/step
Epoch 22/1000
2023-10-27 07:46:50.516 
Epoch 22/1000 
	 loss: 45.4607, MinusLogProbMetric: 45.4607, val_loss: 45.1356, val_MinusLogProbMetric: 45.1356

Epoch 22: val_loss improved from 45.36412 to 45.13564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 45.4607 - MinusLogProbMetric: 45.4607 - val_loss: 45.1356 - val_MinusLogProbMetric: 45.1356 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 23/1000
2023-10-27 07:48:16.318 
Epoch 23/1000 
	 loss: 44.6432, MinusLogProbMetric: 44.6432, val_loss: 44.4615, val_MinusLogProbMetric: 44.4615

Epoch 23: val_loss improved from 45.13564 to 44.46150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 44.6432 - MinusLogProbMetric: 44.6432 - val_loss: 44.4615 - val_MinusLogProbMetric: 44.4615 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 24/1000
2023-10-27 07:49:41.875 
Epoch 24/1000 
	 loss: 44.1947, MinusLogProbMetric: 44.1947, val_loss: 44.0904, val_MinusLogProbMetric: 44.0904

Epoch 24: val_loss improved from 44.46150 to 44.09037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 44.1947 - MinusLogProbMetric: 44.1947 - val_loss: 44.0904 - val_MinusLogProbMetric: 44.0904 - lr: 1.2346e-05 - 86s/epoch - 437ms/step
Epoch 25/1000
2023-10-27 07:51:06.973 
Epoch 25/1000 
	 loss: 44.7432, MinusLogProbMetric: 44.7432, val_loss: 43.9448, val_MinusLogProbMetric: 43.9448

Epoch 25: val_loss improved from 44.09037 to 43.94483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 44.7432 - MinusLogProbMetric: 44.7432 - val_loss: 43.9448 - val_MinusLogProbMetric: 43.9448 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 26/1000
2023-10-27 07:52:32.117 
Epoch 26/1000 
	 loss: 43.5955, MinusLogProbMetric: 43.5955, val_loss: 43.3334, val_MinusLogProbMetric: 43.3334

Epoch 26: val_loss improved from 43.94483 to 43.33344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 43.5955 - MinusLogProbMetric: 43.5955 - val_loss: 43.3334 - val_MinusLogProbMetric: 43.3334 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 27/1000
2023-10-27 07:53:58.983 
Epoch 27/1000 
	 loss: 43.1227, MinusLogProbMetric: 43.1227, val_loss: 43.5901, val_MinusLogProbMetric: 43.5901

Epoch 27: val_loss did not improve from 43.33344
196/196 - 85s - loss: 43.1227 - MinusLogProbMetric: 43.1227 - val_loss: 43.5901 - val_MinusLogProbMetric: 43.5901 - lr: 1.2346e-05 - 85s/epoch - 436ms/step
Epoch 28/1000
2023-10-27 07:55:21.378 
Epoch 28/1000 
	 loss: 42.8253, MinusLogProbMetric: 42.8253, val_loss: 43.1907, val_MinusLogProbMetric: 43.1907

Epoch 28: val_loss improved from 43.33344 to 43.19070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 42.8253 - MinusLogProbMetric: 42.8253 - val_loss: 43.1907 - val_MinusLogProbMetric: 43.1907 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 29/1000
2023-10-27 07:56:39.648 
Epoch 29/1000 
	 loss: 51.0602, MinusLogProbMetric: 51.0602, val_loss: 45.3395, val_MinusLogProbMetric: 45.3395

Epoch 29: val_loss did not improve from 43.19070
196/196 - 77s - loss: 51.0602 - MinusLogProbMetric: 51.0602 - val_loss: 45.3395 - val_MinusLogProbMetric: 45.3395 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 30/1000
2023-10-27 07:57:52.391 
Epoch 30/1000 
	 loss: 43.2203, MinusLogProbMetric: 43.2203, val_loss: 42.5420, val_MinusLogProbMetric: 42.5420

Epoch 30: val_loss improved from 43.19070 to 42.54196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 74s - loss: 43.2203 - MinusLogProbMetric: 43.2203 - val_loss: 42.5420 - val_MinusLogProbMetric: 42.5420 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 31/1000
2023-10-27 07:59:11.164 
Epoch 31/1000 
	 loss: 50.3614, MinusLogProbMetric: 50.3614, val_loss: 50.8561, val_MinusLogProbMetric: 50.8561

Epoch 31: val_loss did not improve from 42.54196
196/196 - 77s - loss: 50.3614 - MinusLogProbMetric: 50.3614 - val_loss: 50.8561 - val_MinusLogProbMetric: 50.8561 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 32/1000
2023-10-27 08:00:25.717 
Epoch 32/1000 
	 loss: 44.6211, MinusLogProbMetric: 44.6211, val_loss: 43.1377, val_MinusLogProbMetric: 43.1377

Epoch 32: val_loss did not improve from 42.54196
196/196 - 75s - loss: 44.6211 - MinusLogProbMetric: 44.6211 - val_loss: 43.1377 - val_MinusLogProbMetric: 43.1377 - lr: 1.2346e-05 - 75s/epoch - 380ms/step
Epoch 33/1000
2023-10-27 08:01:38.096 
Epoch 33/1000 
	 loss: 42.5351, MinusLogProbMetric: 42.5351, val_loss: 42.6158, val_MinusLogProbMetric: 42.6158

Epoch 33: val_loss did not improve from 42.54196
196/196 - 72s - loss: 42.5351 - MinusLogProbMetric: 42.5351 - val_loss: 42.6158 - val_MinusLogProbMetric: 42.6158 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 34/1000
2023-10-27 08:02:53.595 
Epoch 34/1000 
	 loss: 41.7750, MinusLogProbMetric: 41.7750, val_loss: 41.4274, val_MinusLogProbMetric: 41.4274

Epoch 34: val_loss improved from 42.54196 to 41.42744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 77s - loss: 41.7750 - MinusLogProbMetric: 41.7750 - val_loss: 41.4274 - val_MinusLogProbMetric: 41.4274 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 35/1000
2023-10-27 08:04:12.126 
Epoch 35/1000 
	 loss: 64.4816, MinusLogProbMetric: 64.4816, val_loss: 51.5431, val_MinusLogProbMetric: 51.5431

Epoch 35: val_loss did not improve from 41.42744
196/196 - 77s - loss: 64.4816 - MinusLogProbMetric: 64.4816 - val_loss: 51.5431 - val_MinusLogProbMetric: 51.5431 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 36/1000
2023-10-27 08:05:23.136 
Epoch 36/1000 
	 loss: 47.5886, MinusLogProbMetric: 47.5886, val_loss: 45.3901, val_MinusLogProbMetric: 45.3901

Epoch 36: val_loss did not improve from 41.42744
196/196 - 71s - loss: 47.5886 - MinusLogProbMetric: 47.5886 - val_loss: 45.3901 - val_MinusLogProbMetric: 45.3901 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 37/1000
2023-10-27 08:06:41.084 
Epoch 37/1000 
	 loss: 43.9684, MinusLogProbMetric: 43.9684, val_loss: 43.0491, val_MinusLogProbMetric: 43.0491

Epoch 37: val_loss did not improve from 41.42744
196/196 - 78s - loss: 43.9684 - MinusLogProbMetric: 43.9684 - val_loss: 43.0491 - val_MinusLogProbMetric: 43.0491 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 38/1000
2023-10-27 08:07:54.983 
Epoch 38/1000 
	 loss: 42.5081, MinusLogProbMetric: 42.5081, val_loss: 41.9643, val_MinusLogProbMetric: 41.9643

Epoch 38: val_loss did not improve from 41.42744
196/196 - 74s - loss: 42.5081 - MinusLogProbMetric: 42.5081 - val_loss: 41.9643 - val_MinusLogProbMetric: 41.9643 - lr: 1.2346e-05 - 74s/epoch - 377ms/step
Epoch 39/1000
2023-10-27 08:09:15.344 
Epoch 39/1000 
	 loss: 41.7380, MinusLogProbMetric: 41.7380, val_loss: 41.6105, val_MinusLogProbMetric: 41.6105

Epoch 39: val_loss did not improve from 41.42744
196/196 - 80s - loss: 41.7380 - MinusLogProbMetric: 41.7380 - val_loss: 41.6105 - val_MinusLogProbMetric: 41.6105 - lr: 1.2346e-05 - 80s/epoch - 410ms/step
Epoch 40/1000
2023-10-27 08:10:29.014 
Epoch 40/1000 
	 loss: 41.3694, MinusLogProbMetric: 41.3694, val_loss: 41.2201, val_MinusLogProbMetric: 41.2201

Epoch 40: val_loss improved from 41.42744 to 41.22009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 75s - loss: 41.3694 - MinusLogProbMetric: 41.3694 - val_loss: 41.2201 - val_MinusLogProbMetric: 41.2201 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 41/1000
2023-10-27 08:11:49.499 
Epoch 41/1000 
	 loss: 40.9893, MinusLogProbMetric: 40.9893, val_loss: 41.0965, val_MinusLogProbMetric: 41.0965

Epoch 41: val_loss improved from 41.22009 to 41.09650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 80s - loss: 40.9893 - MinusLogProbMetric: 40.9893 - val_loss: 41.0965 - val_MinusLogProbMetric: 41.0965 - lr: 1.2346e-05 - 80s/epoch - 410ms/step
Epoch 42/1000
2023-10-27 08:13:07.407 
Epoch 42/1000 
	 loss: 40.6089, MinusLogProbMetric: 40.6089, val_loss: 41.0638, val_MinusLogProbMetric: 41.0638

Epoch 42: val_loss improved from 41.09650 to 41.06377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 78s - loss: 40.6089 - MinusLogProbMetric: 40.6089 - val_loss: 41.0638 - val_MinusLogProbMetric: 41.0638 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 43/1000
2023-10-27 08:14:27.346 
Epoch 43/1000 
	 loss: 40.3781, MinusLogProbMetric: 40.3781, val_loss: 40.6433, val_MinusLogProbMetric: 40.6433

Epoch 43: val_loss improved from 41.06377 to 40.64325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 80s - loss: 40.3781 - MinusLogProbMetric: 40.3781 - val_loss: 40.6433 - val_MinusLogProbMetric: 40.6433 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 44/1000
2023-10-27 08:15:40.709 
Epoch 44/1000 
	 loss: 41.6422, MinusLogProbMetric: 41.6422, val_loss: 43.2945, val_MinusLogProbMetric: 43.2945

Epoch 44: val_loss did not improve from 40.64325
196/196 - 72s - loss: 41.6422 - MinusLogProbMetric: 41.6422 - val_loss: 43.2945 - val_MinusLogProbMetric: 43.2945 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 45/1000
2023-10-27 08:16:59.206 
Epoch 45/1000 
	 loss: 40.9853, MinusLogProbMetric: 40.9853, val_loss: 40.0253, val_MinusLogProbMetric: 40.0253

Epoch 45: val_loss improved from 40.64325 to 40.02530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 80s - loss: 40.9853 - MinusLogProbMetric: 40.9853 - val_loss: 40.0253 - val_MinusLogProbMetric: 40.0253 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 46/1000
2023-10-27 08:18:16.494 
Epoch 46/1000 
	 loss: 39.8255, MinusLogProbMetric: 39.8255, val_loss: 39.9321, val_MinusLogProbMetric: 39.9321

Epoch 46: val_loss improved from 40.02530 to 39.93213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 77s - loss: 39.8255 - MinusLogProbMetric: 39.8255 - val_loss: 39.9321 - val_MinusLogProbMetric: 39.9321 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 47/1000
2023-10-27 08:19:40.372 
Epoch 47/1000 
	 loss: 39.4982, MinusLogProbMetric: 39.4982, val_loss: 39.5429, val_MinusLogProbMetric: 39.5429

Epoch 47: val_loss improved from 39.93213 to 39.54295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 39.4982 - MinusLogProbMetric: 39.4982 - val_loss: 39.5429 - val_MinusLogProbMetric: 39.5429 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 48/1000
2023-10-27 08:21:00.329 
Epoch 48/1000 
	 loss: 39.4431, MinusLogProbMetric: 39.4431, val_loss: 39.6519, val_MinusLogProbMetric: 39.6519

Epoch 48: val_loss did not improve from 39.54295
196/196 - 78s - loss: 39.4431 - MinusLogProbMetric: 39.4431 - val_loss: 39.6519 - val_MinusLogProbMetric: 39.6519 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 49/1000
2023-10-27 08:22:21.604 
Epoch 49/1000 
	 loss: 41.3035, MinusLogProbMetric: 41.3035, val_loss: 39.4051, val_MinusLogProbMetric: 39.4051

Epoch 49: val_loss improved from 39.54295 to 39.40506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 83s - loss: 41.3035 - MinusLogProbMetric: 41.3035 - val_loss: 39.4051 - val_MinusLogProbMetric: 39.4051 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 50/1000
2023-10-27 08:23:39.912 
Epoch 50/1000 
	 loss: 39.0829, MinusLogProbMetric: 39.0829, val_loss: 38.9049, val_MinusLogProbMetric: 38.9049

Epoch 50: val_loss improved from 39.40506 to 38.90494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 78s - loss: 39.0829 - MinusLogProbMetric: 39.0829 - val_loss: 38.9049 - val_MinusLogProbMetric: 38.9049 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 51/1000
2023-10-27 08:25:02.332 
Epoch 51/1000 
	 loss: 38.9715, MinusLogProbMetric: 38.9715, val_loss: 39.3176, val_MinusLogProbMetric: 39.3176

Epoch 51: val_loss did not improve from 38.90494
196/196 - 81s - loss: 38.9715 - MinusLogProbMetric: 38.9715 - val_loss: 39.3176 - val_MinusLogProbMetric: 39.3176 - lr: 1.2346e-05 - 81s/epoch - 414ms/step
Epoch 52/1000
2023-10-27 08:26:20.037 
Epoch 52/1000 
	 loss: 38.8626, MinusLogProbMetric: 38.8626, val_loss: 39.0951, val_MinusLogProbMetric: 39.0951

Epoch 52: val_loss did not improve from 38.90494
196/196 - 78s - loss: 38.8626 - MinusLogProbMetric: 38.8626 - val_loss: 39.0951 - val_MinusLogProbMetric: 39.0951 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 53/1000
2023-10-27 08:27:42.248 
Epoch 53/1000 
	 loss: 38.8391, MinusLogProbMetric: 38.8391, val_loss: 38.7160, val_MinusLogProbMetric: 38.7160

Epoch 53: val_loss improved from 38.90494 to 38.71600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 38.8391 - MinusLogProbMetric: 38.8391 - val_loss: 38.7160 - val_MinusLogProbMetric: 38.7160 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 54/1000
2023-10-27 08:29:04.830 
Epoch 54/1000 
	 loss: 38.4674, MinusLogProbMetric: 38.4674, val_loss: 38.6091, val_MinusLogProbMetric: 38.6091

Epoch 54: val_loss improved from 38.71600 to 38.60909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 38.4674 - MinusLogProbMetric: 38.4674 - val_loss: 38.6091 - val_MinusLogProbMetric: 38.6091 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 55/1000
2023-10-27 08:30:25.796 
Epoch 55/1000 
	 loss: 38.1906, MinusLogProbMetric: 38.1906, val_loss: 38.3475, val_MinusLogProbMetric: 38.3475

Epoch 55: val_loss improved from 38.60909 to 38.34745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 38.1906 - MinusLogProbMetric: 38.1906 - val_loss: 38.3475 - val_MinusLogProbMetric: 38.3475 - lr: 1.2346e-05 - 81s/epoch - 414ms/step
Epoch 56/1000
2023-10-27 08:31:45.962 
Epoch 56/1000 
	 loss: 38.1456, MinusLogProbMetric: 38.1456, val_loss: 39.0149, val_MinusLogProbMetric: 39.0149

Epoch 56: val_loss did not improve from 38.34745
196/196 - 79s - loss: 38.1456 - MinusLogProbMetric: 38.1456 - val_loss: 39.0149 - val_MinusLogProbMetric: 39.0149 - lr: 1.2346e-05 - 79s/epoch - 401ms/step
Epoch 57/1000
2023-10-27 08:32:55.728 
Epoch 57/1000 
	 loss: 38.1845, MinusLogProbMetric: 38.1845, val_loss: 38.0437, val_MinusLogProbMetric: 38.0437

Epoch 57: val_loss improved from 38.34745 to 38.04366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 71s - loss: 38.1845 - MinusLogProbMetric: 38.1845 - val_loss: 38.0437 - val_MinusLogProbMetric: 38.0437 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 58/1000
2023-10-27 08:34:12.955 
Epoch 58/1000 
	 loss: 44.1956, MinusLogProbMetric: 44.1956, val_loss: 39.8803, val_MinusLogProbMetric: 39.8803

Epoch 58: val_loss did not improve from 38.04366
196/196 - 76s - loss: 44.1956 - MinusLogProbMetric: 44.1956 - val_loss: 39.8803 - val_MinusLogProbMetric: 39.8803 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 59/1000
2023-10-27 08:35:27.603 
Epoch 59/1000 
	 loss: 47.7876, MinusLogProbMetric: 47.7876, val_loss: 41.8020, val_MinusLogProbMetric: 41.8020

Epoch 59: val_loss did not improve from 38.04366
196/196 - 75s - loss: 47.7876 - MinusLogProbMetric: 47.7876 - val_loss: 41.8020 - val_MinusLogProbMetric: 41.8020 - lr: 1.2346e-05 - 75s/epoch - 381ms/step
Epoch 60/1000
2023-10-27 08:36:52.070 
Epoch 60/1000 
	 loss: 40.0207, MinusLogProbMetric: 40.0207, val_loss: 39.3826, val_MinusLogProbMetric: 39.3826

Epoch 60: val_loss did not improve from 38.04366
196/196 - 84s - loss: 40.0207 - MinusLogProbMetric: 40.0207 - val_loss: 39.3826 - val_MinusLogProbMetric: 39.3826 - lr: 1.2346e-05 - 84s/epoch - 431ms/step
Epoch 61/1000
2023-10-27 08:38:10.815 
Epoch 61/1000 
	 loss: 38.8314, MinusLogProbMetric: 38.8314, val_loss: 38.6028, val_MinusLogProbMetric: 38.6028

Epoch 61: val_loss did not improve from 38.04366
196/196 - 79s - loss: 38.8314 - MinusLogProbMetric: 38.8314 - val_loss: 38.6028 - val_MinusLogProbMetric: 38.6028 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 62/1000
2023-10-27 08:39:34.572 
Epoch 62/1000 
	 loss: 38.3519, MinusLogProbMetric: 38.3519, val_loss: 38.3735, val_MinusLogProbMetric: 38.3735

Epoch 62: val_loss did not improve from 38.04366
196/196 - 84s - loss: 38.3519 - MinusLogProbMetric: 38.3519 - val_loss: 38.3735 - val_MinusLogProbMetric: 38.3735 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 63/1000
2023-10-27 08:40:50.733 
Epoch 63/1000 
	 loss: 38.0949, MinusLogProbMetric: 38.0949, val_loss: 38.2687, val_MinusLogProbMetric: 38.2687

Epoch 63: val_loss did not improve from 38.04366
196/196 - 76s - loss: 38.0949 - MinusLogProbMetric: 38.0949 - val_loss: 38.2687 - val_MinusLogProbMetric: 38.2687 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 64/1000
2023-10-27 08:42:08.938 
Epoch 64/1000 
	 loss: 38.0938, MinusLogProbMetric: 38.0938, val_loss: 37.7953, val_MinusLogProbMetric: 37.7953

Epoch 64: val_loss improved from 38.04366 to 37.79530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 80s - loss: 38.0938 - MinusLogProbMetric: 38.0938 - val_loss: 37.7953 - val_MinusLogProbMetric: 37.7953 - lr: 1.2346e-05 - 80s/epoch - 406ms/step
Epoch 65/1000
2023-10-27 08:43:24.256 
Epoch 65/1000 
	 loss: 37.6684, MinusLogProbMetric: 37.6684, val_loss: 37.6476, val_MinusLogProbMetric: 37.6476

Epoch 65: val_loss improved from 37.79530 to 37.64757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 75s - loss: 37.6684 - MinusLogProbMetric: 37.6684 - val_loss: 37.6476 - val_MinusLogProbMetric: 37.6476 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 66/1000
2023-10-27 08:44:45.003 
Epoch 66/1000 
	 loss: 37.5449, MinusLogProbMetric: 37.5449, val_loss: 37.5874, val_MinusLogProbMetric: 37.5874

Epoch 66: val_loss improved from 37.64757 to 37.58741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 81s - loss: 37.5449 - MinusLogProbMetric: 37.5449 - val_loss: 37.5874 - val_MinusLogProbMetric: 37.5874 - lr: 1.2346e-05 - 81s/epoch - 412ms/step
Epoch 67/1000
2023-10-27 08:46:01.729 
Epoch 67/1000 
	 loss: 37.3881, MinusLogProbMetric: 37.3881, val_loss: 38.6470, val_MinusLogProbMetric: 38.6470

Epoch 67: val_loss did not improve from 37.58741
196/196 - 76s - loss: 37.3881 - MinusLogProbMetric: 37.3881 - val_loss: 38.6470 - val_MinusLogProbMetric: 38.6470 - lr: 1.2346e-05 - 76s/epoch - 385ms/step
Epoch 68/1000
2023-10-27 08:47:19.940 
Epoch 68/1000 
	 loss: 37.3693, MinusLogProbMetric: 37.3693, val_loss: 37.3553, val_MinusLogProbMetric: 37.3553

Epoch 68: val_loss improved from 37.58741 to 37.35527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 80s - loss: 37.3693 - MinusLogProbMetric: 37.3693 - val_loss: 37.3553 - val_MinusLogProbMetric: 37.3553 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 69/1000
2023-10-27 08:48:41.764 
Epoch 69/1000 
	 loss: 37.1453, MinusLogProbMetric: 37.1453, val_loss: 38.5867, val_MinusLogProbMetric: 38.5867

Epoch 69: val_loss did not improve from 37.35527
196/196 - 80s - loss: 37.1453 - MinusLogProbMetric: 37.1453 - val_loss: 38.5867 - val_MinusLogProbMetric: 38.5867 - lr: 1.2346e-05 - 80s/epoch - 409ms/step
Epoch 70/1000
2023-10-27 08:49:55.621 
Epoch 70/1000 
	 loss: 37.1124, MinusLogProbMetric: 37.1124, val_loss: 37.0844, val_MinusLogProbMetric: 37.0844

Epoch 70: val_loss improved from 37.35527 to 37.08438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 75s - loss: 37.1124 - MinusLogProbMetric: 37.1124 - val_loss: 37.0844 - val_MinusLogProbMetric: 37.0844 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 71/1000
2023-10-27 08:51:17.574 
Epoch 71/1000 
	 loss: 36.9336, MinusLogProbMetric: 36.9336, val_loss: 36.9386, val_MinusLogProbMetric: 36.9386

Epoch 71: val_loss improved from 37.08438 to 36.93865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 36.9336 - MinusLogProbMetric: 36.9336 - val_loss: 36.9386 - val_MinusLogProbMetric: 36.9386 - lr: 1.2346e-05 - 82s/epoch - 418ms/step
Epoch 72/1000
2023-10-27 08:52:45.627 
Epoch 72/1000 
	 loss: 36.9689, MinusLogProbMetric: 36.9689, val_loss: 44.4187, val_MinusLogProbMetric: 44.4187

Epoch 72: val_loss did not improve from 36.93865
196/196 - 87s - loss: 36.9689 - MinusLogProbMetric: 36.9689 - val_loss: 44.4187 - val_MinusLogProbMetric: 44.4187 - lr: 1.2346e-05 - 87s/epoch - 442ms/step
Epoch 73/1000
2023-10-27 08:54:11.554 
Epoch 73/1000 
	 loss: 37.1290, MinusLogProbMetric: 37.1290, val_loss: 36.9403, val_MinusLogProbMetric: 36.9403

Epoch 73: val_loss did not improve from 36.93865
196/196 - 86s - loss: 37.1290 - MinusLogProbMetric: 37.1290 - val_loss: 36.9403 - val_MinusLogProbMetric: 36.9403 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 74/1000
2023-10-27 08:55:35.770 
Epoch 74/1000 
	 loss: 36.5935, MinusLogProbMetric: 36.5935, val_loss: 37.2574, val_MinusLogProbMetric: 37.2574

Epoch 74: val_loss did not improve from 36.93865
196/196 - 84s - loss: 36.5935 - MinusLogProbMetric: 36.5935 - val_loss: 37.2574 - val_MinusLogProbMetric: 37.2574 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 75/1000
2023-10-27 08:57:00.260 
Epoch 75/1000 
	 loss: 36.4610, MinusLogProbMetric: 36.4610, val_loss: 36.4109, val_MinusLogProbMetric: 36.4109

Epoch 75: val_loss improved from 36.93865 to 36.41087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 36.4610 - MinusLogProbMetric: 36.4610 - val_loss: 36.4109 - val_MinusLogProbMetric: 36.4109 - lr: 1.2346e-05 - 86s/epoch - 440ms/step
Epoch 76/1000
2023-10-27 08:58:26.269 
Epoch 76/1000 
	 loss: 36.2883, MinusLogProbMetric: 36.2883, val_loss: 36.3883, val_MinusLogProbMetric: 36.3883

Epoch 76: val_loss improved from 36.41087 to 36.38828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 36.2883 - MinusLogProbMetric: 36.2883 - val_loss: 36.3883 - val_MinusLogProbMetric: 36.3883 - lr: 1.2346e-05 - 86s/epoch - 439ms/step
Epoch 77/1000
2023-10-27 08:59:52.635 
Epoch 77/1000 
	 loss: 36.1976, MinusLogProbMetric: 36.1976, val_loss: 36.5435, val_MinusLogProbMetric: 36.5435

Epoch 77: val_loss did not improve from 36.38828
196/196 - 85s - loss: 36.1976 - MinusLogProbMetric: 36.1976 - val_loss: 36.5435 - val_MinusLogProbMetric: 36.5435 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 78/1000
2023-10-27 09:01:18.529 
Epoch 78/1000 
	 loss: 36.5235, MinusLogProbMetric: 36.5235, val_loss: 36.2429, val_MinusLogProbMetric: 36.2429

Epoch 78: val_loss improved from 36.38828 to 36.24286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 87s - loss: 36.5235 - MinusLogProbMetric: 36.5235 - val_loss: 36.2429 - val_MinusLogProbMetric: 36.2429 - lr: 1.2346e-05 - 87s/epoch - 446ms/step
Epoch 79/1000
2023-10-27 09:02:44.514 
Epoch 79/1000 
	 loss: 36.0805, MinusLogProbMetric: 36.0805, val_loss: 36.0138, val_MinusLogProbMetric: 36.0138

Epoch 79: val_loss improved from 36.24286 to 36.01381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 36.0805 - MinusLogProbMetric: 36.0805 - val_loss: 36.0138 - val_MinusLogProbMetric: 36.0138 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 80/1000
2023-10-27 09:04:09.195 
Epoch 80/1000 
	 loss: 36.0546, MinusLogProbMetric: 36.0546, val_loss: 36.5871, val_MinusLogProbMetric: 36.5871

Epoch 80: val_loss did not improve from 36.01381
196/196 - 83s - loss: 36.0546 - MinusLogProbMetric: 36.0546 - val_loss: 36.5871 - val_MinusLogProbMetric: 36.5871 - lr: 1.2346e-05 - 83s/epoch - 425ms/step
Epoch 81/1000
2023-10-27 09:05:33.746 
Epoch 81/1000 
	 loss: 38.7487, MinusLogProbMetric: 38.7487, val_loss: 40.7391, val_MinusLogProbMetric: 40.7391

Epoch 81: val_loss did not improve from 36.01381
196/196 - 85s - loss: 38.7487 - MinusLogProbMetric: 38.7487 - val_loss: 40.7391 - val_MinusLogProbMetric: 40.7391 - lr: 1.2346e-05 - 85s/epoch - 431ms/step
Epoch 82/1000
2023-10-27 09:06:59.050 
Epoch 82/1000 
	 loss: 36.9039, MinusLogProbMetric: 36.9039, val_loss: 35.9316, val_MinusLogProbMetric: 35.9316

Epoch 82: val_loss improved from 36.01381 to 35.93158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 87s - loss: 36.9039 - MinusLogProbMetric: 36.9039 - val_loss: 35.9316 - val_MinusLogProbMetric: 35.9316 - lr: 1.2346e-05 - 87s/epoch - 442ms/step
Epoch 83/1000
2023-10-27 09:08:25.654 
Epoch 83/1000 
	 loss: 40.2263, MinusLogProbMetric: 40.2263, val_loss: 86.3055, val_MinusLogProbMetric: 86.3055

Epoch 83: val_loss did not improve from 35.93158
196/196 - 85s - loss: 40.2263 - MinusLogProbMetric: 40.2263 - val_loss: 86.3055 - val_MinusLogProbMetric: 86.3055 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 84/1000
2023-10-27 09:09:49.835 
Epoch 84/1000 
	 loss: 49.3914, MinusLogProbMetric: 49.3914, val_loss: 40.1098, val_MinusLogProbMetric: 40.1098

Epoch 84: val_loss did not improve from 35.93158
196/196 - 84s - loss: 49.3914 - MinusLogProbMetric: 49.3914 - val_loss: 40.1098 - val_MinusLogProbMetric: 40.1098 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 85/1000
2023-10-27 09:11:14.794 
Epoch 85/1000 
	 loss: 39.6214, MinusLogProbMetric: 39.6214, val_loss: 39.3630, val_MinusLogProbMetric: 39.3630

Epoch 85: val_loss did not improve from 35.93158
196/196 - 85s - loss: 39.6214 - MinusLogProbMetric: 39.6214 - val_loss: 39.3630 - val_MinusLogProbMetric: 39.3630 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 86/1000
2023-10-27 09:12:39.986 
Epoch 86/1000 
	 loss: 44.1532, MinusLogProbMetric: 44.1532, val_loss: 37.8667, val_MinusLogProbMetric: 37.8667

Epoch 86: val_loss did not improve from 35.93158
196/196 - 85s - loss: 44.1532 - MinusLogProbMetric: 44.1532 - val_loss: 37.8667 - val_MinusLogProbMetric: 37.8667 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 87/1000
2023-10-27 09:14:04.445 
Epoch 87/1000 
	 loss: 37.0451, MinusLogProbMetric: 37.0451, val_loss: 36.8823, val_MinusLogProbMetric: 36.8823

Epoch 87: val_loss did not improve from 35.93158
196/196 - 84s - loss: 37.0451 - MinusLogProbMetric: 37.0451 - val_loss: 36.8823 - val_MinusLogProbMetric: 36.8823 - lr: 1.2346e-05 - 84s/epoch - 431ms/step
Epoch 88/1000
2023-10-27 09:15:29.804 
Epoch 88/1000 
	 loss: 36.6481, MinusLogProbMetric: 36.6481, val_loss: 36.6209, val_MinusLogProbMetric: 36.6209

Epoch 88: val_loss did not improve from 35.93158
196/196 - 85s - loss: 36.6481 - MinusLogProbMetric: 36.6481 - val_loss: 36.6209 - val_MinusLogProbMetric: 36.6209 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 89/1000
2023-10-27 09:16:54.982 
Epoch 89/1000 
	 loss: 36.4720, MinusLogProbMetric: 36.4720, val_loss: 36.6885, val_MinusLogProbMetric: 36.6885

Epoch 89: val_loss did not improve from 35.93158
196/196 - 85s - loss: 36.4720 - MinusLogProbMetric: 36.4720 - val_loss: 36.6885 - val_MinusLogProbMetric: 36.6885 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 90/1000
2023-10-27 09:18:19.100 
Epoch 90/1000 
	 loss: 35.9070, MinusLogProbMetric: 35.9070, val_loss: 35.4495, val_MinusLogProbMetric: 35.4495

Epoch 90: val_loss improved from 35.93158 to 35.44947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 35.9070 - MinusLogProbMetric: 35.9070 - val_loss: 35.4495 - val_MinusLogProbMetric: 35.4495 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 91/1000
2023-10-27 09:19:43.733 
Epoch 91/1000 
	 loss: 35.4844, MinusLogProbMetric: 35.4844, val_loss: 35.5369, val_MinusLogProbMetric: 35.5369

Epoch 91: val_loss did not improve from 35.44947
196/196 - 84s - loss: 35.4844 - MinusLogProbMetric: 35.4844 - val_loss: 35.5369 - val_MinusLogProbMetric: 35.5369 - lr: 1.2346e-05 - 84s/epoch - 426ms/step
Epoch 92/1000
2023-10-27 09:21:08.097 
Epoch 92/1000 
	 loss: 35.3408, MinusLogProbMetric: 35.3408, val_loss: 35.4283, val_MinusLogProbMetric: 35.4283

Epoch 92: val_loss improved from 35.44947 to 35.42834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 35.3408 - MinusLogProbMetric: 35.3408 - val_loss: 35.4283 - val_MinusLogProbMetric: 35.4283 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 93/1000
2023-10-27 09:22:35.000 
Epoch 93/1000 
	 loss: 35.3693, MinusLogProbMetric: 35.3693, val_loss: 35.5379, val_MinusLogProbMetric: 35.5379

Epoch 93: val_loss did not improve from 35.42834
196/196 - 85s - loss: 35.3693 - MinusLogProbMetric: 35.3693 - val_loss: 35.5379 - val_MinusLogProbMetric: 35.5379 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 94/1000
2023-10-27 09:23:59.058 
Epoch 94/1000 
	 loss: 35.3070, MinusLogProbMetric: 35.3070, val_loss: 35.6596, val_MinusLogProbMetric: 35.6596

Epoch 94: val_loss did not improve from 35.42834
196/196 - 84s - loss: 35.3070 - MinusLogProbMetric: 35.3070 - val_loss: 35.6596 - val_MinusLogProbMetric: 35.6596 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 95/1000
2023-10-27 09:25:21.435 
Epoch 95/1000 
	 loss: 43.8347, MinusLogProbMetric: 43.8347, val_loss: 40.9704, val_MinusLogProbMetric: 40.9704

Epoch 95: val_loss did not improve from 35.42834
196/196 - 82s - loss: 43.8347 - MinusLogProbMetric: 43.8347 - val_loss: 40.9704 - val_MinusLogProbMetric: 40.9704 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 96/1000
2023-10-27 09:26:45.371 
Epoch 96/1000 
	 loss: 37.5265, MinusLogProbMetric: 37.5265, val_loss: 36.9912, val_MinusLogProbMetric: 36.9912

Epoch 96: val_loss did not improve from 35.42834
196/196 - 84s - loss: 37.5265 - MinusLogProbMetric: 37.5265 - val_loss: 36.9912 - val_MinusLogProbMetric: 36.9912 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 97/1000
2023-10-27 09:28:09.467 
Epoch 97/1000 
	 loss: 36.6296, MinusLogProbMetric: 36.6296, val_loss: 36.4035, val_MinusLogProbMetric: 36.4035

Epoch 97: val_loss did not improve from 35.42834
196/196 - 84s - loss: 36.6296 - MinusLogProbMetric: 36.6296 - val_loss: 36.4035 - val_MinusLogProbMetric: 36.4035 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 98/1000
2023-10-27 09:29:33.849 
Epoch 98/1000 
	 loss: 37.0733, MinusLogProbMetric: 37.0733, val_loss: 36.2182, val_MinusLogProbMetric: 36.2182

Epoch 98: val_loss did not improve from 35.42834
196/196 - 84s - loss: 37.0733 - MinusLogProbMetric: 37.0733 - val_loss: 36.2182 - val_MinusLogProbMetric: 36.2182 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 99/1000
2023-10-27 09:31:00.614 
Epoch 99/1000 
	 loss: 36.0880, MinusLogProbMetric: 36.0880, val_loss: 35.8178, val_MinusLogProbMetric: 35.8178

Epoch 99: val_loss did not improve from 35.42834
196/196 - 87s - loss: 36.0880 - MinusLogProbMetric: 36.0880 - val_loss: 35.8178 - val_MinusLogProbMetric: 35.8178 - lr: 1.2346e-05 - 87s/epoch - 443ms/step
Epoch 100/1000
2023-10-27 09:32:26.393 
Epoch 100/1000 
	 loss: 35.8768, MinusLogProbMetric: 35.8768, val_loss: 36.2181, val_MinusLogProbMetric: 36.2181

Epoch 100: val_loss did not improve from 35.42834
196/196 - 86s - loss: 35.8768 - MinusLogProbMetric: 35.8768 - val_loss: 36.2181 - val_MinusLogProbMetric: 36.2181 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 101/1000
2023-10-27 09:33:50.631 
Epoch 101/1000 
	 loss: 35.8770, MinusLogProbMetric: 35.8770, val_loss: 35.6140, val_MinusLogProbMetric: 35.6140

Epoch 101: val_loss did not improve from 35.42834
196/196 - 84s - loss: 35.8770 - MinusLogProbMetric: 35.8770 - val_loss: 35.6140 - val_MinusLogProbMetric: 35.6140 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 102/1000
2023-10-27 09:35:15.528 
Epoch 102/1000 
	 loss: 35.5711, MinusLogProbMetric: 35.5711, val_loss: 35.5855, val_MinusLogProbMetric: 35.5855

Epoch 102: val_loss did not improve from 35.42834
196/196 - 85s - loss: 35.5711 - MinusLogProbMetric: 35.5711 - val_loss: 35.5855 - val_MinusLogProbMetric: 35.5855 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 103/1000
2023-10-27 09:36:41.701 
Epoch 103/1000 
	 loss: 35.3441, MinusLogProbMetric: 35.3441, val_loss: 35.4757, val_MinusLogProbMetric: 35.4757

Epoch 103: val_loss did not improve from 35.42834
196/196 - 86s - loss: 35.3441 - MinusLogProbMetric: 35.3441 - val_loss: 35.4757 - val_MinusLogProbMetric: 35.4757 - lr: 1.2346e-05 - 86s/epoch - 440ms/step
Epoch 104/1000
2023-10-27 09:38:06.390 
Epoch 104/1000 
	 loss: 35.1998, MinusLogProbMetric: 35.1998, val_loss: 35.1951, val_MinusLogProbMetric: 35.1951

Epoch 104: val_loss improved from 35.42834 to 35.19513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 35.1998 - MinusLogProbMetric: 35.1998 - val_loss: 35.1951 - val_MinusLogProbMetric: 35.1951 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 105/1000
2023-10-27 09:39:32.287 
Epoch 105/1000 
	 loss: 35.3901, MinusLogProbMetric: 35.3901, val_loss: 36.5687, val_MinusLogProbMetric: 36.5687

Epoch 105: val_loss did not improve from 35.19513
196/196 - 85s - loss: 35.3901 - MinusLogProbMetric: 35.3901 - val_loss: 36.5687 - val_MinusLogProbMetric: 36.5687 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 106/1000
2023-10-27 09:40:55.501 
Epoch 106/1000 
	 loss: 35.2318, MinusLogProbMetric: 35.2318, val_loss: 35.2814, val_MinusLogProbMetric: 35.2814

Epoch 106: val_loss did not improve from 35.19513
196/196 - 83s - loss: 35.2318 - MinusLogProbMetric: 35.2318 - val_loss: 35.2814 - val_MinusLogProbMetric: 35.2814 - lr: 1.2346e-05 - 83s/epoch - 425ms/step
Epoch 107/1000
2023-10-27 09:42:20.490 
Epoch 107/1000 
	 loss: 35.0054, MinusLogProbMetric: 35.0054, val_loss: 35.0537, val_MinusLogProbMetric: 35.0537

Epoch 107: val_loss improved from 35.19513 to 35.05367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 35.0054 - MinusLogProbMetric: 35.0054 - val_loss: 35.0537 - val_MinusLogProbMetric: 35.0537 - lr: 1.2346e-05 - 86s/epoch - 441ms/step
Epoch 108/1000
2023-10-27 09:43:46.058 
Epoch 108/1000 
	 loss: 35.0521, MinusLogProbMetric: 35.0521, val_loss: 34.9756, val_MinusLogProbMetric: 34.9756

Epoch 108: val_loss improved from 35.05367 to 34.97557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 85s - loss: 35.0521 - MinusLogProbMetric: 35.0521 - val_loss: 34.9756 - val_MinusLogProbMetric: 34.9756 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 109/1000
2023-10-27 09:45:12.721 
Epoch 109/1000 
	 loss: 34.9049, MinusLogProbMetric: 34.9049, val_loss: 34.9414, val_MinusLogProbMetric: 34.9414

Epoch 109: val_loss improved from 34.97557 to 34.94136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 87s - loss: 34.9049 - MinusLogProbMetric: 34.9049 - val_loss: 34.9414 - val_MinusLogProbMetric: 34.9414 - lr: 1.2346e-05 - 87s/epoch - 442ms/step
Epoch 110/1000
2023-10-27 09:46:37.899 
Epoch 110/1000 
	 loss: 34.7592, MinusLogProbMetric: 34.7592, val_loss: 34.9937, val_MinusLogProbMetric: 34.9937

Epoch 110: val_loss did not improve from 34.94136
196/196 - 84s - loss: 34.7592 - MinusLogProbMetric: 34.7592 - val_loss: 34.9937 - val_MinusLogProbMetric: 34.9937 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 111/1000
2023-10-27 09:48:00.727 
Epoch 111/1000 
	 loss: 35.0049, MinusLogProbMetric: 35.0049, val_loss: 34.9349, val_MinusLogProbMetric: 34.9349

Epoch 111: val_loss improved from 34.94136 to 34.93491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 84s - loss: 35.0049 - MinusLogProbMetric: 35.0049 - val_loss: 34.9349 - val_MinusLogProbMetric: 34.9349 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 112/1000
2023-10-27 09:49:24.861 
Epoch 112/1000 
	 loss: 34.7012, MinusLogProbMetric: 34.7012, val_loss: 35.1876, val_MinusLogProbMetric: 35.1876

Epoch 112: val_loss did not improve from 34.93491
196/196 - 83s - loss: 34.7012 - MinusLogProbMetric: 34.7012 - val_loss: 35.1876 - val_MinusLogProbMetric: 35.1876 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 113/1000
2023-10-27 09:50:50.452 
Epoch 113/1000 
	 loss: 34.9346, MinusLogProbMetric: 34.9346, val_loss: 42.8678, val_MinusLogProbMetric: 42.8678

Epoch 113: val_loss did not improve from 34.93491
196/196 - 86s - loss: 34.9346 - MinusLogProbMetric: 34.9346 - val_loss: 42.8678 - val_MinusLogProbMetric: 42.8678 - lr: 1.2346e-05 - 86s/epoch - 437ms/step
Epoch 114/1000
2023-10-27 09:52:11.333 
Epoch 114/1000 
	 loss: 35.0869, MinusLogProbMetric: 35.0869, val_loss: 34.6861, val_MinusLogProbMetric: 34.6861

Epoch 114: val_loss improved from 34.93491 to 34.68615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 82s - loss: 35.0869 - MinusLogProbMetric: 35.0869 - val_loss: 34.6861 - val_MinusLogProbMetric: 34.6861 - lr: 1.2346e-05 - 82s/epoch - 417ms/step
Epoch 115/1000
2023-10-27 09:53:33.445 
Epoch 115/1000 
	 loss: 34.5058, MinusLogProbMetric: 34.5058, val_loss: 34.9700, val_MinusLogProbMetric: 34.9700

Epoch 115: val_loss did not improve from 34.68615
196/196 - 81s - loss: 34.5058 - MinusLogProbMetric: 34.5058 - val_loss: 34.9700 - val_MinusLogProbMetric: 34.9700 - lr: 1.2346e-05 - 81s/epoch - 414ms/step
Epoch 116/1000
2023-10-27 09:54:58.551 
Epoch 116/1000 
	 loss: 98.2718, MinusLogProbMetric: 98.2718, val_loss: 64.3921, val_MinusLogProbMetric: 64.3921

Epoch 116: val_loss did not improve from 34.68615
196/196 - 85s - loss: 98.2718 - MinusLogProbMetric: 98.2718 - val_loss: 64.3921 - val_MinusLogProbMetric: 64.3921 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 117/1000
2023-10-27 09:56:21.031 
Epoch 117/1000 
	 loss: 53.6439, MinusLogProbMetric: 53.6439, val_loss: 47.7171, val_MinusLogProbMetric: 47.7171

Epoch 117: val_loss did not improve from 34.68615
196/196 - 82s - loss: 53.6439 - MinusLogProbMetric: 53.6439 - val_loss: 47.7171 - val_MinusLogProbMetric: 47.7171 - lr: 1.2346e-05 - 82s/epoch - 421ms/step
Epoch 118/1000
2023-10-27 09:57:46.507 
Epoch 118/1000 
	 loss: 45.1690, MinusLogProbMetric: 45.1690, val_loss: 43.2870, val_MinusLogProbMetric: 43.2870

Epoch 118: val_loss did not improve from 34.68615
196/196 - 85s - loss: 45.1690 - MinusLogProbMetric: 45.1690 - val_loss: 43.2870 - val_MinusLogProbMetric: 43.2870 - lr: 1.2346e-05 - 85s/epoch - 436ms/step
Epoch 119/1000
2023-10-27 09:59:11.281 
Epoch 119/1000 
	 loss: 41.8387, MinusLogProbMetric: 41.8387, val_loss: 41.0773, val_MinusLogProbMetric: 41.0773

Epoch 119: val_loss did not improve from 34.68615
196/196 - 85s - loss: 41.8387 - MinusLogProbMetric: 41.8387 - val_loss: 41.0773 - val_MinusLogProbMetric: 41.0773 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 120/1000
2023-10-27 10:00:34.998 
Epoch 120/1000 
	 loss: 39.8511, MinusLogProbMetric: 39.8511, val_loss: 39.7626, val_MinusLogProbMetric: 39.7626

Epoch 120: val_loss did not improve from 34.68615
196/196 - 84s - loss: 39.8511 - MinusLogProbMetric: 39.8511 - val_loss: 39.7626 - val_MinusLogProbMetric: 39.7626 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 121/1000
2023-10-27 10:01:58.128 
Epoch 121/1000 
	 loss: 39.5357, MinusLogProbMetric: 39.5357, val_loss: 38.8094, val_MinusLogProbMetric: 38.8094

Epoch 121: val_loss did not improve from 34.68615
196/196 - 83s - loss: 39.5357 - MinusLogProbMetric: 39.5357 - val_loss: 38.8094 - val_MinusLogProbMetric: 38.8094 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 122/1000
2023-10-27 10:03:21.768 
Epoch 122/1000 
	 loss: 38.0621, MinusLogProbMetric: 38.0621, val_loss: 37.7694, val_MinusLogProbMetric: 37.7694

Epoch 122: val_loss did not improve from 34.68615
196/196 - 84s - loss: 38.0621 - MinusLogProbMetric: 38.0621 - val_loss: 37.7694 - val_MinusLogProbMetric: 37.7694 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 123/1000
2023-10-27 10:04:47.659 
Epoch 123/1000 
	 loss: 37.3380, MinusLogProbMetric: 37.3380, val_loss: 37.1154, val_MinusLogProbMetric: 37.1154

Epoch 123: val_loss did not improve from 34.68615
196/196 - 86s - loss: 37.3380 - MinusLogProbMetric: 37.3380 - val_loss: 37.1154 - val_MinusLogProbMetric: 37.1154 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 124/1000
2023-10-27 10:06:11.060 
Epoch 124/1000 
	 loss: 36.7215, MinusLogProbMetric: 36.7215, val_loss: 36.5669, val_MinusLogProbMetric: 36.5669

Epoch 124: val_loss did not improve from 34.68615
196/196 - 83s - loss: 36.7215 - MinusLogProbMetric: 36.7215 - val_loss: 36.5669 - val_MinusLogProbMetric: 36.5669 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 125/1000
2023-10-27 10:07:35.420 
Epoch 125/1000 
	 loss: 36.3487, MinusLogProbMetric: 36.3487, val_loss: 36.2640, val_MinusLogProbMetric: 36.2640

Epoch 125: val_loss did not improve from 34.68615
196/196 - 84s - loss: 36.3487 - MinusLogProbMetric: 36.3487 - val_loss: 36.2640 - val_MinusLogProbMetric: 36.2640 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 126/1000
2023-10-27 10:09:00.083 
Epoch 126/1000 
	 loss: 35.9228, MinusLogProbMetric: 35.9228, val_loss: 35.7890, val_MinusLogProbMetric: 35.7890

Epoch 126: val_loss did not improve from 34.68615
196/196 - 85s - loss: 35.9228 - MinusLogProbMetric: 35.9228 - val_loss: 35.7890 - val_MinusLogProbMetric: 35.7890 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 127/1000
2023-10-27 10:10:25.296 
Epoch 127/1000 
	 loss: 35.7211, MinusLogProbMetric: 35.7211, val_loss: 35.5844, val_MinusLogProbMetric: 35.5844

Epoch 127: val_loss did not improve from 34.68615
196/196 - 85s - loss: 35.7211 - MinusLogProbMetric: 35.7211 - val_loss: 35.5844 - val_MinusLogProbMetric: 35.5844 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 128/1000
2023-10-27 10:11:51.186 
Epoch 128/1000 
	 loss: 35.4312, MinusLogProbMetric: 35.4312, val_loss: 38.3195, val_MinusLogProbMetric: 38.3195

Epoch 128: val_loss did not improve from 34.68615
196/196 - 86s - loss: 35.4312 - MinusLogProbMetric: 35.4312 - val_loss: 38.3195 - val_MinusLogProbMetric: 38.3195 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 129/1000
2023-10-27 10:13:17.250 
Epoch 129/1000 
	 loss: 35.3823, MinusLogProbMetric: 35.3823, val_loss: 35.2465, val_MinusLogProbMetric: 35.2465

Epoch 129: val_loss did not improve from 34.68615
196/196 - 86s - loss: 35.3823 - MinusLogProbMetric: 35.3823 - val_loss: 35.2465 - val_MinusLogProbMetric: 35.2465 - lr: 1.2346e-05 - 86s/epoch - 439ms/step
Epoch 130/1000
2023-10-27 10:14:43.352 
Epoch 130/1000 
	 loss: 35.4741, MinusLogProbMetric: 35.4741, val_loss: 35.2990, val_MinusLogProbMetric: 35.2990

Epoch 130: val_loss did not improve from 34.68615
196/196 - 86s - loss: 35.4741 - MinusLogProbMetric: 35.4741 - val_loss: 35.2990 - val_MinusLogProbMetric: 35.2990 - lr: 1.2346e-05 - 86s/epoch - 439ms/step
Epoch 131/1000
2023-10-27 10:16:08.448 
Epoch 131/1000 
	 loss: 35.3330, MinusLogProbMetric: 35.3330, val_loss: 35.2397, val_MinusLogProbMetric: 35.2397

Epoch 131: val_loss did not improve from 34.68615
196/196 - 85s - loss: 35.3330 - MinusLogProbMetric: 35.3330 - val_loss: 35.2397 - val_MinusLogProbMetric: 35.2397 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 132/1000
2023-10-27 10:17:31.861 
Epoch 132/1000 
	 loss: 35.0022, MinusLogProbMetric: 35.0022, val_loss: 35.1847, val_MinusLogProbMetric: 35.1847

Epoch 132: val_loss did not improve from 34.68615
196/196 - 83s - loss: 35.0022 - MinusLogProbMetric: 35.0022 - val_loss: 35.1847 - val_MinusLogProbMetric: 35.1847 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 133/1000
2023-10-27 10:18:55.224 
Epoch 133/1000 
	 loss: 34.7601, MinusLogProbMetric: 34.7601, val_loss: 35.7152, val_MinusLogProbMetric: 35.7152

Epoch 133: val_loss did not improve from 34.68615
196/196 - 83s - loss: 34.7601 - MinusLogProbMetric: 34.7601 - val_loss: 35.7152 - val_MinusLogProbMetric: 35.7152 - lr: 1.2346e-05 - 83s/epoch - 425ms/step
Epoch 134/1000
2023-10-27 10:20:18.681 
Epoch 134/1000 
	 loss: 34.9084, MinusLogProbMetric: 34.9084, val_loss: 34.6915, val_MinusLogProbMetric: 34.6915

Epoch 134: val_loss did not improve from 34.68615
196/196 - 83s - loss: 34.9084 - MinusLogProbMetric: 34.9084 - val_loss: 34.6915 - val_MinusLogProbMetric: 34.6915 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 135/1000
2023-10-27 10:21:42.229 
Epoch 135/1000 
	 loss: 34.5617, MinusLogProbMetric: 34.5617, val_loss: 35.3241, val_MinusLogProbMetric: 35.3241

Epoch 135: val_loss did not improve from 34.68615
196/196 - 84s - loss: 34.5617 - MinusLogProbMetric: 34.5617 - val_loss: 35.3241 - val_MinusLogProbMetric: 35.3241 - lr: 1.2346e-05 - 84s/epoch - 426ms/step
Epoch 136/1000
2023-10-27 10:23:06.687 
Epoch 136/1000 
	 loss: 38.7502, MinusLogProbMetric: 38.7502, val_loss: 38.2866, val_MinusLogProbMetric: 38.2866

Epoch 136: val_loss did not improve from 34.68615
196/196 - 84s - loss: 38.7502 - MinusLogProbMetric: 38.7502 - val_loss: 38.2866 - val_MinusLogProbMetric: 38.2866 - lr: 1.2346e-05 - 84s/epoch - 431ms/step
Epoch 137/1000
2023-10-27 10:24:31.265 
Epoch 137/1000 
	 loss: 35.3915, MinusLogProbMetric: 35.3915, val_loss: 34.7934, val_MinusLogProbMetric: 34.7934

Epoch 137: val_loss did not improve from 34.68615
196/196 - 85s - loss: 35.3915 - MinusLogProbMetric: 35.3915 - val_loss: 34.7934 - val_MinusLogProbMetric: 34.7934 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 138/1000
2023-10-27 10:25:55.466 
Epoch 138/1000 
	 loss: 34.3903, MinusLogProbMetric: 34.3903, val_loss: 34.5610, val_MinusLogProbMetric: 34.5610

Epoch 138: val_loss improved from 34.68615 to 34.56100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 34.3903 - MinusLogProbMetric: 34.3903 - val_loss: 34.5610 - val_MinusLogProbMetric: 34.5610 - lr: 1.2346e-05 - 86s/epoch - 437ms/step
Epoch 139/1000
2023-10-27 10:27:22.997 
Epoch 139/1000 
	 loss: 34.2982, MinusLogProbMetric: 34.2982, val_loss: 34.1774, val_MinusLogProbMetric: 34.1774

Epoch 139: val_loss improved from 34.56100 to 34.17744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 87s - loss: 34.2982 - MinusLogProbMetric: 34.2982 - val_loss: 34.1774 - val_MinusLogProbMetric: 34.1774 - lr: 1.2346e-05 - 87s/epoch - 446ms/step
Epoch 140/1000
2023-10-27 10:28:48.826 
Epoch 140/1000 
	 loss: 34.2696, MinusLogProbMetric: 34.2696, val_loss: 34.5783, val_MinusLogProbMetric: 34.5783

Epoch 140: val_loss did not improve from 34.17744
196/196 - 84s - loss: 34.2696 - MinusLogProbMetric: 34.2696 - val_loss: 34.5783 - val_MinusLogProbMetric: 34.5783 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 141/1000
2023-10-27 10:30:13.978 
Epoch 141/1000 
	 loss: 46.5708, MinusLogProbMetric: 46.5708, val_loss: 81.3108, val_MinusLogProbMetric: 81.3108

Epoch 141: val_loss did not improve from 34.17744
196/196 - 85s - loss: 46.5708 - MinusLogProbMetric: 46.5708 - val_loss: 81.3108 - val_MinusLogProbMetric: 81.3108 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 142/1000
2023-10-27 10:31:37.119 
Epoch 142/1000 
	 loss: 46.5439, MinusLogProbMetric: 46.5439, val_loss: 36.4415, val_MinusLogProbMetric: 36.4415

Epoch 142: val_loss did not improve from 34.17744
196/196 - 83s - loss: 46.5439 - MinusLogProbMetric: 46.5439 - val_loss: 36.4415 - val_MinusLogProbMetric: 36.4415 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 143/1000
2023-10-27 10:32:59.725 
Epoch 143/1000 
	 loss: 35.9000, MinusLogProbMetric: 35.9000, val_loss: 36.0419, val_MinusLogProbMetric: 36.0419

Epoch 143: val_loss did not improve from 34.17744
196/196 - 83s - loss: 35.9000 - MinusLogProbMetric: 35.9000 - val_loss: 36.0419 - val_MinusLogProbMetric: 36.0419 - lr: 1.2346e-05 - 83s/epoch - 421ms/step
Epoch 144/1000
2023-10-27 10:34:23.623 
Epoch 144/1000 
	 loss: 35.3259, MinusLogProbMetric: 35.3259, val_loss: 34.9613, val_MinusLogProbMetric: 34.9613

Epoch 144: val_loss did not improve from 34.17744
196/196 - 84s - loss: 35.3259 - MinusLogProbMetric: 35.3259 - val_loss: 34.9613 - val_MinusLogProbMetric: 34.9613 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 145/1000
2023-10-27 10:35:47.228 
Epoch 145/1000 
	 loss: 34.8744, MinusLogProbMetric: 34.8744, val_loss: 34.8711, val_MinusLogProbMetric: 34.8711

Epoch 145: val_loss did not improve from 34.17744
196/196 - 84s - loss: 34.8744 - MinusLogProbMetric: 34.8744 - val_loss: 34.8711 - val_MinusLogProbMetric: 34.8711 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 146/1000
2023-10-27 10:37:11.901 
Epoch 146/1000 
	 loss: 35.0105, MinusLogProbMetric: 35.0105, val_loss: 34.7084, val_MinusLogProbMetric: 34.7084

Epoch 146: val_loss did not improve from 34.17744
196/196 - 85s - loss: 35.0105 - MinusLogProbMetric: 35.0105 - val_loss: 34.7084 - val_MinusLogProbMetric: 34.7084 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 147/1000
2023-10-27 10:38:34.880 
Epoch 147/1000 
	 loss: 34.6610, MinusLogProbMetric: 34.6610, val_loss: 34.6856, val_MinusLogProbMetric: 34.6856

Epoch 147: val_loss did not improve from 34.17744
196/196 - 83s - loss: 34.6610 - MinusLogProbMetric: 34.6610 - val_loss: 34.6856 - val_MinusLogProbMetric: 34.6856 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 148/1000
2023-10-27 10:39:59.040 
Epoch 148/1000 
	 loss: 34.7637, MinusLogProbMetric: 34.7637, val_loss: 34.6810, val_MinusLogProbMetric: 34.6810

Epoch 148: val_loss did not improve from 34.17744
196/196 - 84s - loss: 34.7637 - MinusLogProbMetric: 34.7637 - val_loss: 34.6810 - val_MinusLogProbMetric: 34.6810 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 149/1000
2023-10-27 10:41:23.954 
Epoch 149/1000 
	 loss: 34.5305, MinusLogProbMetric: 34.5305, val_loss: 35.3219, val_MinusLogProbMetric: 35.3219

Epoch 149: val_loss did not improve from 34.17744
196/196 - 85s - loss: 34.5305 - MinusLogProbMetric: 34.5305 - val_loss: 35.3219 - val_MinusLogProbMetric: 35.3219 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 150/1000
2023-10-27 10:42:48.841 
Epoch 150/1000 
	 loss: 34.3360, MinusLogProbMetric: 34.3360, val_loss: 34.2854, val_MinusLogProbMetric: 34.2854

Epoch 150: val_loss did not improve from 34.17744
196/196 - 85s - loss: 34.3360 - MinusLogProbMetric: 34.3360 - val_loss: 34.2854 - val_MinusLogProbMetric: 34.2854 - lr: 1.2346e-05 - 85s/epoch - 433ms/step
Epoch 151/1000
2023-10-27 10:44:13.869 
Epoch 151/1000 
	 loss: 34.3066, MinusLogProbMetric: 34.3066, val_loss: 34.3205, val_MinusLogProbMetric: 34.3205

Epoch 151: val_loss did not improve from 34.17744
196/196 - 85s - loss: 34.3066 - MinusLogProbMetric: 34.3066 - val_loss: 34.3205 - val_MinusLogProbMetric: 34.3205 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 152/1000
2023-10-27 10:45:38.512 
Epoch 152/1000 
	 loss: 34.2003, MinusLogProbMetric: 34.2003, val_loss: 34.3639, val_MinusLogProbMetric: 34.3639

Epoch 152: val_loss did not improve from 34.17744
196/196 - 85s - loss: 34.2003 - MinusLogProbMetric: 34.2003 - val_loss: 34.3639 - val_MinusLogProbMetric: 34.3639 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 153/1000
2023-10-27 10:47:03.147 
Epoch 153/1000 
	 loss: 34.1531, MinusLogProbMetric: 34.1531, val_loss: 34.0816, val_MinusLogProbMetric: 34.0816

Epoch 153: val_loss improved from 34.17744 to 34.08165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 34.1531 - MinusLogProbMetric: 34.1531 - val_loss: 34.0816 - val_MinusLogProbMetric: 34.0816 - lr: 1.2346e-05 - 86s/epoch - 439ms/step
Epoch 154/1000
2023-10-27 10:48:29.829 
Epoch 154/1000 
	 loss: 34.7168, MinusLogProbMetric: 34.7168, val_loss: 34.0954, val_MinusLogProbMetric: 34.0954

Epoch 154: val_loss did not improve from 34.08165
196/196 - 85s - loss: 34.7168 - MinusLogProbMetric: 34.7168 - val_loss: 34.0954 - val_MinusLogProbMetric: 34.0954 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 155/1000
2023-10-27 10:49:56.259 
Epoch 155/1000 
	 loss: 34.0002, MinusLogProbMetric: 34.0002, val_loss: 34.0671, val_MinusLogProbMetric: 34.0671

Epoch 155: val_loss improved from 34.08165 to 34.06712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 88s - loss: 34.0002 - MinusLogProbMetric: 34.0002 - val_loss: 34.0671 - val_MinusLogProbMetric: 34.0671 - lr: 1.2346e-05 - 88s/epoch - 448ms/step
Epoch 156/1000
2023-10-27 10:51:21.149 
Epoch 156/1000 
	 loss: 76.0065, MinusLogProbMetric: 76.0065, val_loss: 146.8660, val_MinusLogProbMetric: 146.8660

Epoch 156: val_loss did not improve from 34.06712
196/196 - 83s - loss: 76.0065 - MinusLogProbMetric: 76.0065 - val_loss: 146.8660 - val_MinusLogProbMetric: 146.8660 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 157/1000
2023-10-27 10:52:46.172 
Epoch 157/1000 
	 loss: 91.0139, MinusLogProbMetric: 91.0139, val_loss: 70.7318, val_MinusLogProbMetric: 70.7318

Epoch 157: val_loss did not improve from 34.06712
196/196 - 85s - loss: 91.0139 - MinusLogProbMetric: 91.0139 - val_loss: 70.7318 - val_MinusLogProbMetric: 70.7318 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 158/1000
2023-10-27 10:54:09.913 
Epoch 158/1000 
	 loss: 64.8606, MinusLogProbMetric: 64.8606, val_loss: 60.1564, val_MinusLogProbMetric: 60.1564

Epoch 158: val_loss did not improve from 34.06712
196/196 - 84s - loss: 64.8606 - MinusLogProbMetric: 64.8606 - val_loss: 60.1564 - val_MinusLogProbMetric: 60.1564 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 159/1000
2023-10-27 10:55:33.738 
Epoch 159/1000 
	 loss: 57.1373, MinusLogProbMetric: 57.1373, val_loss: 53.9812, val_MinusLogProbMetric: 53.9812

Epoch 159: val_loss did not improve from 34.06712
196/196 - 84s - loss: 57.1373 - MinusLogProbMetric: 57.1373 - val_loss: 53.9812 - val_MinusLogProbMetric: 53.9812 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 160/1000
2023-10-27 10:56:57.931 
Epoch 160/1000 
	 loss: 51.1233, MinusLogProbMetric: 51.1233, val_loss: 48.9081, val_MinusLogProbMetric: 48.9081

Epoch 160: val_loss did not improve from 34.06712
196/196 - 84s - loss: 51.1233 - MinusLogProbMetric: 51.1233 - val_loss: 48.9081 - val_MinusLogProbMetric: 48.9081 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 161/1000
2023-10-27 10:58:21.809 
Epoch 161/1000 
	 loss: 47.3015, MinusLogProbMetric: 47.3015, val_loss: 45.9283, val_MinusLogProbMetric: 45.9283

Epoch 161: val_loss did not improve from 34.06712
196/196 - 84s - loss: 47.3015 - MinusLogProbMetric: 47.3015 - val_loss: 45.9283 - val_MinusLogProbMetric: 45.9283 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 162/1000
2023-10-27 10:59:44.952 
Epoch 162/1000 
	 loss: 45.2276, MinusLogProbMetric: 45.2276, val_loss: 44.4997, val_MinusLogProbMetric: 44.4997

Epoch 162: val_loss did not improve from 34.06712
196/196 - 83s - loss: 45.2276 - MinusLogProbMetric: 45.2276 - val_loss: 44.4997 - val_MinusLogProbMetric: 44.4997 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 163/1000
2023-10-27 11:01:09.421 
Epoch 163/1000 
	 loss: 43.8761, MinusLogProbMetric: 43.8761, val_loss: 43.3048, val_MinusLogProbMetric: 43.3048

Epoch 163: val_loss did not improve from 34.06712
196/196 - 84s - loss: 43.8761 - MinusLogProbMetric: 43.8761 - val_loss: 43.3048 - val_MinusLogProbMetric: 43.3048 - lr: 1.2346e-05 - 84s/epoch - 431ms/step
Epoch 164/1000
2023-10-27 11:02:35.078 
Epoch 164/1000 
	 loss: 42.8480, MinusLogProbMetric: 42.8480, val_loss: 42.3705, val_MinusLogProbMetric: 42.3705

Epoch 164: val_loss did not improve from 34.06712
196/196 - 86s - loss: 42.8480 - MinusLogProbMetric: 42.8480 - val_loss: 42.3705 - val_MinusLogProbMetric: 42.3705 - lr: 1.2346e-05 - 86s/epoch - 437ms/step
Epoch 165/1000
2023-10-27 11:04:01.304 
Epoch 165/1000 
	 loss: 42.0525, MinusLogProbMetric: 42.0525, val_loss: 41.6179, val_MinusLogProbMetric: 41.6179

Epoch 165: val_loss did not improve from 34.06712
196/196 - 86s - loss: 42.0525 - MinusLogProbMetric: 42.0525 - val_loss: 41.6179 - val_MinusLogProbMetric: 41.6179 - lr: 1.2346e-05 - 86s/epoch - 440ms/step
Epoch 166/1000
2023-10-27 11:05:25.434 
Epoch 166/1000 
	 loss: 41.5084, MinusLogProbMetric: 41.5084, val_loss: 41.0796, val_MinusLogProbMetric: 41.0796

Epoch 166: val_loss did not improve from 34.06712
196/196 - 84s - loss: 41.5084 - MinusLogProbMetric: 41.5084 - val_loss: 41.0796 - val_MinusLogProbMetric: 41.0796 - lr: 1.2346e-05 - 84s/epoch - 429ms/step
Epoch 167/1000
2023-10-27 11:06:49.804 
Epoch 167/1000 
	 loss: 40.7905, MinusLogProbMetric: 40.7905, val_loss: 40.5690, val_MinusLogProbMetric: 40.5690

Epoch 167: val_loss did not improve from 34.06712
196/196 - 84s - loss: 40.7905 - MinusLogProbMetric: 40.7905 - val_loss: 40.5690 - val_MinusLogProbMetric: 40.5690 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 168/1000
2023-10-27 11:08:16.121 
Epoch 168/1000 
	 loss: 40.2509, MinusLogProbMetric: 40.2509, val_loss: 40.2056, val_MinusLogProbMetric: 40.2056

Epoch 168: val_loss did not improve from 34.06712
196/196 - 86s - loss: 40.2509 - MinusLogProbMetric: 40.2509 - val_loss: 40.2056 - val_MinusLogProbMetric: 40.2056 - lr: 1.2346e-05 - 86s/epoch - 440ms/step
Epoch 169/1000
2023-10-27 11:09:39.791 
Epoch 169/1000 
	 loss: 41.6543, MinusLogProbMetric: 41.6543, val_loss: 46.7225, val_MinusLogProbMetric: 46.7225

Epoch 169: val_loss did not improve from 34.06712
196/196 - 84s - loss: 41.6543 - MinusLogProbMetric: 41.6543 - val_loss: 46.7225 - val_MinusLogProbMetric: 46.7225 - lr: 1.2346e-05 - 84s/epoch - 427ms/step
Epoch 170/1000
2023-10-27 11:11:04.865 
Epoch 170/1000 
	 loss: 40.2029, MinusLogProbMetric: 40.2029, val_loss: 39.1789, val_MinusLogProbMetric: 39.1789

Epoch 170: val_loss did not improve from 34.06712
196/196 - 85s - loss: 40.2029 - MinusLogProbMetric: 40.2029 - val_loss: 39.1789 - val_MinusLogProbMetric: 39.1789 - lr: 1.2346e-05 - 85s/epoch - 434ms/step
Epoch 171/1000
2023-10-27 11:12:31.664 
Epoch 171/1000 
	 loss: 38.8706, MinusLogProbMetric: 38.8706, val_loss: 38.6238, val_MinusLogProbMetric: 38.6238

Epoch 171: val_loss did not improve from 34.06712
196/196 - 87s - loss: 38.8706 - MinusLogProbMetric: 38.8706 - val_loss: 38.6238 - val_MinusLogProbMetric: 38.6238 - lr: 1.2346e-05 - 87s/epoch - 443ms/step
Epoch 172/1000
2023-10-27 11:13:57.608 
Epoch 172/1000 
	 loss: 38.4113, MinusLogProbMetric: 38.4113, val_loss: 38.2146, val_MinusLogProbMetric: 38.2146

Epoch 172: val_loss did not improve from 34.06712
196/196 - 86s - loss: 38.4113 - MinusLogProbMetric: 38.4113 - val_loss: 38.2146 - val_MinusLogProbMetric: 38.2146 - lr: 1.2346e-05 - 86s/epoch - 438ms/step
Epoch 173/1000
2023-10-27 11:15:20.988 
Epoch 173/1000 
	 loss: 38.1265, MinusLogProbMetric: 38.1265, val_loss: 38.0421, val_MinusLogProbMetric: 38.0421

Epoch 173: val_loss did not improve from 34.06712
196/196 - 83s - loss: 38.1265 - MinusLogProbMetric: 38.1265 - val_loss: 38.0421 - val_MinusLogProbMetric: 38.0421 - lr: 1.2346e-05 - 83s/epoch - 425ms/step
Epoch 174/1000
2023-10-27 11:16:44.177 
Epoch 174/1000 
	 loss: 38.0628, MinusLogProbMetric: 38.0628, val_loss: 37.6378, val_MinusLogProbMetric: 37.6378

Epoch 174: val_loss did not improve from 34.06712
196/196 - 83s - loss: 38.0628 - MinusLogProbMetric: 38.0628 - val_loss: 37.6378 - val_MinusLogProbMetric: 37.6378 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 175/1000
2023-10-27 11:18:09.362 
Epoch 175/1000 
	 loss: 37.4343, MinusLogProbMetric: 37.4343, val_loss: 37.2315, val_MinusLogProbMetric: 37.2315

Epoch 175: val_loss did not improve from 34.06712
196/196 - 85s - loss: 37.4343 - MinusLogProbMetric: 37.4343 - val_loss: 37.2315 - val_MinusLogProbMetric: 37.2315 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 176/1000
2023-10-27 11:19:31.435 
Epoch 176/1000 
	 loss: 37.0585, MinusLogProbMetric: 37.0585, val_loss: 36.8861, val_MinusLogProbMetric: 36.8861

Epoch 176: val_loss did not improve from 34.06712
196/196 - 82s - loss: 37.0585 - MinusLogProbMetric: 37.0585 - val_loss: 36.8861 - val_MinusLogProbMetric: 36.8861 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 177/1000
2023-10-27 11:20:55.256 
Epoch 177/1000 
	 loss: 36.8036, MinusLogProbMetric: 36.8036, val_loss: 36.5929, val_MinusLogProbMetric: 36.5929

Epoch 177: val_loss did not improve from 34.06712
196/196 - 84s - loss: 36.8036 - MinusLogProbMetric: 36.8036 - val_loss: 36.5929 - val_MinusLogProbMetric: 36.5929 - lr: 1.2346e-05 - 84s/epoch - 428ms/step
Epoch 178/1000
2023-10-27 11:22:18.187 
Epoch 178/1000 
	 loss: 36.7037, MinusLogProbMetric: 36.7037, val_loss: 36.3598, val_MinusLogProbMetric: 36.3598

Epoch 178: val_loss did not improve from 34.06712
196/196 - 83s - loss: 36.7037 - MinusLogProbMetric: 36.7037 - val_loss: 36.3598 - val_MinusLogProbMetric: 36.3598 - lr: 1.2346e-05 - 83s/epoch - 423ms/step
Epoch 179/1000
2023-10-27 11:23:42.541 
Epoch 179/1000 
	 loss: 36.4539, MinusLogProbMetric: 36.4539, val_loss: 36.2432, val_MinusLogProbMetric: 36.2432

Epoch 179: val_loss did not improve from 34.06712
196/196 - 84s - loss: 36.4539 - MinusLogProbMetric: 36.4539 - val_loss: 36.2432 - val_MinusLogProbMetric: 36.2432 - lr: 1.2346e-05 - 84s/epoch - 430ms/step
Epoch 180/1000
2023-10-27 11:25:05.697 
Epoch 180/1000 
	 loss: 36.4862, MinusLogProbMetric: 36.4862, val_loss: 37.0275, val_MinusLogProbMetric: 37.0275

Epoch 180: val_loss did not improve from 34.06712
196/196 - 83s - loss: 36.4862 - MinusLogProbMetric: 36.4862 - val_loss: 37.0275 - val_MinusLogProbMetric: 37.0275 - lr: 1.2346e-05 - 83s/epoch - 424ms/step
Epoch 181/1000
2023-10-27 11:26:30.349 
Epoch 181/1000 
	 loss: 36.1144, MinusLogProbMetric: 36.1144, val_loss: 35.9475, val_MinusLogProbMetric: 35.9475

Epoch 181: val_loss did not improve from 34.06712
196/196 - 85s - loss: 36.1144 - MinusLogProbMetric: 36.1144 - val_loss: 35.9475 - val_MinusLogProbMetric: 35.9475 - lr: 1.2346e-05 - 85s/epoch - 432ms/step
Epoch 182/1000
2023-10-27 11:27:53.815 
Epoch 182/1000 
	 loss: 35.9109, MinusLogProbMetric: 35.9109, val_loss: 35.8649, val_MinusLogProbMetric: 35.8649

Epoch 182: val_loss did not improve from 34.06712
196/196 - 83s - loss: 35.9109 - MinusLogProbMetric: 35.9109 - val_loss: 35.8649 - val_MinusLogProbMetric: 35.8649 - lr: 1.2346e-05 - 83s/epoch - 426ms/step
Epoch 183/1000
2023-10-27 11:29:19.198 
Epoch 183/1000 
	 loss: 35.8924, MinusLogProbMetric: 35.8924, val_loss: 36.0038, val_MinusLogProbMetric: 36.0038

Epoch 183: val_loss did not improve from 34.06712
196/196 - 85s - loss: 35.8924 - MinusLogProbMetric: 35.8924 - val_loss: 36.0038 - val_MinusLogProbMetric: 36.0038 - lr: 1.2346e-05 - 85s/epoch - 436ms/step
Epoch 184/1000
2023-10-27 11:30:44.789 
Epoch 184/1000 
	 loss: 35.6510, MinusLogProbMetric: 35.6510, val_loss: 36.0380, val_MinusLogProbMetric: 36.0380

Epoch 184: val_loss did not improve from 34.06712
196/196 - 86s - loss: 35.6510 - MinusLogProbMetric: 35.6510 - val_loss: 36.0380 - val_MinusLogProbMetric: 36.0380 - lr: 1.2346e-05 - 86s/epoch - 437ms/step
Epoch 185/1000
2023-10-27 11:32:10.081 
Epoch 185/1000 
	 loss: 35.4495, MinusLogProbMetric: 35.4495, val_loss: 35.3628, val_MinusLogProbMetric: 35.3628

Epoch 185: val_loss did not improve from 34.06712
196/196 - 85s - loss: 35.4495 - MinusLogProbMetric: 35.4495 - val_loss: 35.3628 - val_MinusLogProbMetric: 35.3628 - lr: 1.2346e-05 - 85s/epoch - 435ms/step
Epoch 186/1000
2023-10-27 11:33:25.595 
Epoch 186/1000 
	 loss: 35.2462, MinusLogProbMetric: 35.2462, val_loss: 35.2907, val_MinusLogProbMetric: 35.2907

Epoch 186: val_loss did not improve from 34.06712
196/196 - 76s - loss: 35.2462 - MinusLogProbMetric: 35.2462 - val_loss: 35.2907 - val_MinusLogProbMetric: 35.2907 - lr: 1.2346e-05 - 76s/epoch - 385ms/step
Epoch 187/1000
2023-10-27 11:34:32.238 
Epoch 187/1000 
	 loss: 35.2036, MinusLogProbMetric: 35.2036, val_loss: 35.1298, val_MinusLogProbMetric: 35.1298

Epoch 187: val_loss did not improve from 34.06712
196/196 - 67s - loss: 35.2036 - MinusLogProbMetric: 35.2036 - val_loss: 35.1298 - val_MinusLogProbMetric: 35.1298 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 188/1000
2023-10-27 11:35:43.343 
Epoch 188/1000 
	 loss: 35.0412, MinusLogProbMetric: 35.0412, val_loss: 34.9872, val_MinusLogProbMetric: 34.9872

Epoch 188: val_loss did not improve from 34.06712
196/196 - 71s - loss: 35.0412 - MinusLogProbMetric: 35.0412 - val_loss: 34.9872 - val_MinusLogProbMetric: 34.9872 - lr: 1.2346e-05 - 71s/epoch - 363ms/step
Epoch 189/1000
2023-10-27 11:36:48.588 
Epoch 189/1000 
	 loss: 34.9627, MinusLogProbMetric: 34.9627, val_loss: 34.9425, val_MinusLogProbMetric: 34.9425

Epoch 189: val_loss did not improve from 34.06712
196/196 - 65s - loss: 34.9627 - MinusLogProbMetric: 34.9627 - val_loss: 34.9425 - val_MinusLogProbMetric: 34.9425 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 190/1000
2023-10-27 11:37:57.412 
Epoch 190/1000 
	 loss: 34.7823, MinusLogProbMetric: 34.7823, val_loss: 34.6353, val_MinusLogProbMetric: 34.6353

Epoch 190: val_loss did not improve from 34.06712
196/196 - 69s - loss: 34.7823 - MinusLogProbMetric: 34.7823 - val_loss: 34.6353 - val_MinusLogProbMetric: 34.6353 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 191/1000
2023-10-27 11:39:06.723 
Epoch 191/1000 
	 loss: 36.4598, MinusLogProbMetric: 36.4598, val_loss: 39.2720, val_MinusLogProbMetric: 39.2720

Epoch 191: val_loss did not improve from 34.06712
196/196 - 69s - loss: 36.4598 - MinusLogProbMetric: 36.4598 - val_loss: 39.2720 - val_MinusLogProbMetric: 39.2720 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 192/1000
2023-10-27 11:40:12.663 
Epoch 192/1000 
	 loss: 38.6341, MinusLogProbMetric: 38.6341, val_loss: 37.8072, val_MinusLogProbMetric: 37.8072

Epoch 192: val_loss did not improve from 34.06712
196/196 - 66s - loss: 38.6341 - MinusLogProbMetric: 38.6341 - val_loss: 37.8072 - val_MinusLogProbMetric: 37.8072 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 193/1000
2023-10-27 11:41:28.354 
Epoch 193/1000 
	 loss: 38.7807, MinusLogProbMetric: 38.7807, val_loss: 37.3259, val_MinusLogProbMetric: 37.3259

Epoch 193: val_loss did not improve from 34.06712
196/196 - 76s - loss: 38.7807 - MinusLogProbMetric: 38.7807 - val_loss: 37.3259 - val_MinusLogProbMetric: 37.3259 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 194/1000
2023-10-27 11:42:33.213 
Epoch 194/1000 
	 loss: 36.9149, MinusLogProbMetric: 36.9149, val_loss: 37.3508, val_MinusLogProbMetric: 37.3508

Epoch 194: val_loss did not improve from 34.06712
196/196 - 65s - loss: 36.9149 - MinusLogProbMetric: 36.9149 - val_loss: 37.3508 - val_MinusLogProbMetric: 37.3508 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 195/1000
2023-10-27 11:43:44.812 
Epoch 195/1000 
	 loss: 36.7040, MinusLogProbMetric: 36.7040, val_loss: 36.7477, val_MinusLogProbMetric: 36.7477

Epoch 195: val_loss did not improve from 34.06712
196/196 - 72s - loss: 36.7040 - MinusLogProbMetric: 36.7040 - val_loss: 36.7477 - val_MinusLogProbMetric: 36.7477 - lr: 1.2346e-05 - 72s/epoch - 365ms/step
Epoch 196/1000
2023-10-27 11:44:51.480 
Epoch 196/1000 
	 loss: 36.4803, MinusLogProbMetric: 36.4803, val_loss: 36.5321, val_MinusLogProbMetric: 36.5321

Epoch 196: val_loss did not improve from 34.06712
196/196 - 67s - loss: 36.4803 - MinusLogProbMetric: 36.4803 - val_loss: 36.5321 - val_MinusLogProbMetric: 36.5321 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 197/1000
2023-10-27 11:45:56.408 
Epoch 197/1000 
	 loss: 36.3085, MinusLogProbMetric: 36.3085, val_loss: 36.2229, val_MinusLogProbMetric: 36.2229

Epoch 197: val_loss did not improve from 34.06712
196/196 - 65s - loss: 36.3085 - MinusLogProbMetric: 36.3085 - val_loss: 36.2229 - val_MinusLogProbMetric: 36.2229 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 198/1000
2023-10-27 11:47:09.609 
Epoch 198/1000 
	 loss: 36.0884, MinusLogProbMetric: 36.0884, val_loss: 36.6445, val_MinusLogProbMetric: 36.6445

Epoch 198: val_loss did not improve from 34.06712
196/196 - 73s - loss: 36.0884 - MinusLogProbMetric: 36.0884 - val_loss: 36.6445 - val_MinusLogProbMetric: 36.6445 - lr: 1.2346e-05 - 73s/epoch - 373ms/step
Epoch 199/1000
2023-10-27 11:48:14.291 
Epoch 199/1000 
	 loss: 36.0064, MinusLogProbMetric: 36.0064, val_loss: 35.9592, val_MinusLogProbMetric: 35.9592

Epoch 199: val_loss did not improve from 34.06712
196/196 - 65s - loss: 36.0064 - MinusLogProbMetric: 36.0064 - val_loss: 35.9592 - val_MinusLogProbMetric: 35.9592 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 200/1000
2023-10-27 11:49:22.272 
Epoch 200/1000 
	 loss: 35.8860, MinusLogProbMetric: 35.8860, val_loss: 35.7994, val_MinusLogProbMetric: 35.7994

Epoch 200: val_loss did not improve from 34.06712
196/196 - 68s - loss: 35.8860 - MinusLogProbMetric: 35.8860 - val_loss: 35.7994 - val_MinusLogProbMetric: 35.7994 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 201/1000
2023-10-27 11:50:29.414 
Epoch 201/1000 
	 loss: 35.7112, MinusLogProbMetric: 35.7112, val_loss: 35.6232, val_MinusLogProbMetric: 35.6232

Epoch 201: val_loss did not improve from 34.06712
196/196 - 67s - loss: 35.7112 - MinusLogProbMetric: 35.7112 - val_loss: 35.6232 - val_MinusLogProbMetric: 35.6232 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 202/1000
2023-10-27 11:51:35.305 
Epoch 202/1000 
	 loss: 35.5698, MinusLogProbMetric: 35.5698, val_loss: 35.6804, val_MinusLogProbMetric: 35.6804

Epoch 202: val_loss did not improve from 34.06712
196/196 - 66s - loss: 35.5698 - MinusLogProbMetric: 35.5698 - val_loss: 35.6804 - val_MinusLogProbMetric: 35.6804 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 203/1000
2023-10-27 11:52:48.295 
Epoch 203/1000 
	 loss: 35.5269, MinusLogProbMetric: 35.5269, val_loss: 35.4861, val_MinusLogProbMetric: 35.4861

Epoch 203: val_loss did not improve from 34.06712
196/196 - 73s - loss: 35.5269 - MinusLogProbMetric: 35.5269 - val_loss: 35.4861 - val_MinusLogProbMetric: 35.4861 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 204/1000
2023-10-27 11:53:54.049 
Epoch 204/1000 
	 loss: 35.8981, MinusLogProbMetric: 35.8981, val_loss: 35.5776, val_MinusLogProbMetric: 35.5776

Epoch 204: val_loss did not improve from 34.06712
196/196 - 66s - loss: 35.8981 - MinusLogProbMetric: 35.8981 - val_loss: 35.5776 - val_MinusLogProbMetric: 35.5776 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 205/1000
2023-10-27 11:55:07.001 
Epoch 205/1000 
	 loss: 35.3467, MinusLogProbMetric: 35.3467, val_loss: 35.4663, val_MinusLogProbMetric: 35.4663

Epoch 205: val_loss did not improve from 34.06712
196/196 - 73s - loss: 35.3467 - MinusLogProbMetric: 35.3467 - val_loss: 35.4663 - val_MinusLogProbMetric: 35.4663 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 206/1000
2023-10-27 11:56:14.619 
Epoch 206/1000 
	 loss: 35.1155, MinusLogProbMetric: 35.1155, val_loss: 35.1192, val_MinusLogProbMetric: 35.1192

Epoch 206: val_loss did not improve from 34.06712
196/196 - 68s - loss: 35.1155 - MinusLogProbMetric: 35.1155 - val_loss: 35.1192 - val_MinusLogProbMetric: 35.1192 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 207/1000
2023-10-27 11:57:23.120 
Epoch 207/1000 
	 loss: 35.1666, MinusLogProbMetric: 35.1666, val_loss: 35.1322, val_MinusLogProbMetric: 35.1322

Epoch 207: val_loss did not improve from 34.06712
196/196 - 68s - loss: 35.1666 - MinusLogProbMetric: 35.1666 - val_loss: 35.1322 - val_MinusLogProbMetric: 35.1322 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 208/1000
2023-10-27 11:58:33.702 
Epoch 208/1000 
	 loss: 35.0813, MinusLogProbMetric: 35.0813, val_loss: 35.1912, val_MinusLogProbMetric: 35.1912

Epoch 208: val_loss did not improve from 34.06712
196/196 - 71s - loss: 35.0813 - MinusLogProbMetric: 35.0813 - val_loss: 35.1912 - val_MinusLogProbMetric: 35.1912 - lr: 6.1728e-06 - 71s/epoch - 360ms/step
Epoch 209/1000
2023-10-27 11:59:40.846 
Epoch 209/1000 
	 loss: 35.0782, MinusLogProbMetric: 35.0782, val_loss: 35.2087, val_MinusLogProbMetric: 35.2087

Epoch 209: val_loss did not improve from 34.06712
196/196 - 67s - loss: 35.0782 - MinusLogProbMetric: 35.0782 - val_loss: 35.2087 - val_MinusLogProbMetric: 35.2087 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 210/1000
2023-10-27 12:00:52.779 
Epoch 210/1000 
	 loss: 38.3655, MinusLogProbMetric: 38.3655, val_loss: 38.2685, val_MinusLogProbMetric: 38.2685

Epoch 210: val_loss did not improve from 34.06712
196/196 - 72s - loss: 38.3655 - MinusLogProbMetric: 38.3655 - val_loss: 38.2685 - val_MinusLogProbMetric: 38.2685 - lr: 6.1728e-06 - 72s/epoch - 367ms/step
Epoch 211/1000
2023-10-27 12:01:59.086 
Epoch 211/1000 
	 loss: 37.3659, MinusLogProbMetric: 37.3659, val_loss: 40.0395, val_MinusLogProbMetric: 40.0395

Epoch 211: val_loss did not improve from 34.06712
196/196 - 66s - loss: 37.3659 - MinusLogProbMetric: 37.3659 - val_loss: 40.0395 - val_MinusLogProbMetric: 40.0395 - lr: 6.1728e-06 - 66s/epoch - 338ms/step
Epoch 212/1000
2023-10-27 12:03:09.791 
Epoch 212/1000 
	 loss: 37.7774, MinusLogProbMetric: 37.7774, val_loss: 39.9940, val_MinusLogProbMetric: 39.9940

Epoch 212: val_loss did not improve from 34.06712
196/196 - 71s - loss: 37.7774 - MinusLogProbMetric: 37.7774 - val_loss: 39.9940 - val_MinusLogProbMetric: 39.9940 - lr: 6.1728e-06 - 71s/epoch - 361ms/step
Epoch 213/1000
2023-10-27 12:04:20.562 
Epoch 213/1000 
	 loss: 36.8985, MinusLogProbMetric: 36.8985, val_loss: 36.6020, val_MinusLogProbMetric: 36.6020

Epoch 213: val_loss did not improve from 34.06712
196/196 - 71s - loss: 36.8985 - MinusLogProbMetric: 36.8985 - val_loss: 36.6020 - val_MinusLogProbMetric: 36.6020 - lr: 6.1728e-06 - 71s/epoch - 361ms/step
Epoch 214/1000
2023-10-27 12:05:26.661 
Epoch 214/1000 
	 loss: 46.5569, MinusLogProbMetric: 46.5569, val_loss: 40.9459, val_MinusLogProbMetric: 40.9459

Epoch 214: val_loss did not improve from 34.06712
196/196 - 66s - loss: 46.5569 - MinusLogProbMetric: 46.5569 - val_loss: 40.9459 - val_MinusLogProbMetric: 40.9459 - lr: 6.1728e-06 - 66s/epoch - 337ms/step
Epoch 215/1000
2023-10-27 12:06:43.736 
Epoch 215/1000 
	 loss: 39.3310, MinusLogProbMetric: 39.3310, val_loss: 37.4301, val_MinusLogProbMetric: 37.4301

Epoch 215: val_loss did not improve from 34.06712
196/196 - 77s - loss: 39.3310 - MinusLogProbMetric: 39.3310 - val_loss: 37.4301 - val_MinusLogProbMetric: 37.4301 - lr: 6.1728e-06 - 77s/epoch - 393ms/step
Epoch 216/1000
2023-10-27 12:07:49.622 
Epoch 216/1000 
	 loss: 36.9991, MinusLogProbMetric: 36.9991, val_loss: 36.0480, val_MinusLogProbMetric: 36.0480

Epoch 216: val_loss did not improve from 34.06712
196/196 - 66s - loss: 36.9991 - MinusLogProbMetric: 36.9991 - val_loss: 36.0480 - val_MinusLogProbMetric: 36.0480 - lr: 6.1728e-06 - 66s/epoch - 336ms/step
Epoch 217/1000
2023-10-27 12:09:03.253 
Epoch 217/1000 
	 loss: 35.6409, MinusLogProbMetric: 35.6409, val_loss: 35.4813, val_MinusLogProbMetric: 35.4813

Epoch 217: val_loss did not improve from 34.06712
196/196 - 74s - loss: 35.6409 - MinusLogProbMetric: 35.6409 - val_loss: 35.4813 - val_MinusLogProbMetric: 35.4813 - lr: 6.1728e-06 - 74s/epoch - 376ms/step
Epoch 218/1000
2023-10-27 12:10:10.959 
Epoch 218/1000 
	 loss: 35.5399, MinusLogProbMetric: 35.5399, val_loss: 36.7777, val_MinusLogProbMetric: 36.7777

Epoch 218: val_loss did not improve from 34.06712
196/196 - 68s - loss: 35.5399 - MinusLogProbMetric: 35.5399 - val_loss: 36.7777 - val_MinusLogProbMetric: 36.7777 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 219/1000
2023-10-27 12:11:17.620 
Epoch 219/1000 
	 loss: 35.8629, MinusLogProbMetric: 35.8629, val_loss: 35.5687, val_MinusLogProbMetric: 35.5687

Epoch 219: val_loss did not improve from 34.06712
196/196 - 67s - loss: 35.8629 - MinusLogProbMetric: 35.8629 - val_loss: 35.5687 - val_MinusLogProbMetric: 35.5687 - lr: 6.1728e-06 - 67s/epoch - 340ms/step
Epoch 220/1000
2023-10-27 12:12:29.885 
Epoch 220/1000 
	 loss: 35.3258, MinusLogProbMetric: 35.3258, val_loss: 35.0960, val_MinusLogProbMetric: 35.0960

Epoch 220: val_loss did not improve from 34.06712
196/196 - 72s - loss: 35.3258 - MinusLogProbMetric: 35.3258 - val_loss: 35.0960 - val_MinusLogProbMetric: 35.0960 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 221/1000
2023-10-27 12:13:34.909 
Epoch 221/1000 
	 loss: 34.6884, MinusLogProbMetric: 34.6884, val_loss: 34.4451, val_MinusLogProbMetric: 34.4451

Epoch 221: val_loss did not improve from 34.06712
196/196 - 65s - loss: 34.6884 - MinusLogProbMetric: 34.6884 - val_loss: 34.4451 - val_MinusLogProbMetric: 34.4451 - lr: 6.1728e-06 - 65s/epoch - 332ms/step
Epoch 222/1000
2023-10-27 12:14:50.267 
Epoch 222/1000 
	 loss: 34.3215, MinusLogProbMetric: 34.3215, val_loss: 34.3033, val_MinusLogProbMetric: 34.3033

Epoch 222: val_loss did not improve from 34.06712
196/196 - 75s - loss: 34.3215 - MinusLogProbMetric: 34.3215 - val_loss: 34.3033 - val_MinusLogProbMetric: 34.3033 - lr: 6.1728e-06 - 75s/epoch - 384ms/step
Epoch 223/1000
2023-10-27 12:15:58.269 
Epoch 223/1000 
	 loss: 34.1892, MinusLogProbMetric: 34.1892, val_loss: 34.1582, val_MinusLogProbMetric: 34.1582

Epoch 223: val_loss did not improve from 34.06712
196/196 - 68s - loss: 34.1892 - MinusLogProbMetric: 34.1892 - val_loss: 34.1582 - val_MinusLogProbMetric: 34.1582 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 224/1000
2023-10-27 12:17:06.260 
Epoch 224/1000 
	 loss: 34.0700, MinusLogProbMetric: 34.0700, val_loss: 34.0935, val_MinusLogProbMetric: 34.0935

Epoch 224: val_loss did not improve from 34.06712
196/196 - 68s - loss: 34.0700 - MinusLogProbMetric: 34.0700 - val_loss: 34.0935 - val_MinusLogProbMetric: 34.0935 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 225/1000
2023-10-27 12:18:15.826 
Epoch 225/1000 
	 loss: 33.9335, MinusLogProbMetric: 33.9335, val_loss: 33.9089, val_MinusLogProbMetric: 33.9089

Epoch 225: val_loss improved from 34.06712 to 33.90891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 71s - loss: 33.9335 - MinusLogProbMetric: 33.9335 - val_loss: 33.9089 - val_MinusLogProbMetric: 33.9089 - lr: 6.1728e-06 - 71s/epoch - 361ms/step
Epoch 226/1000
2023-10-27 12:19:23.964 
Epoch 226/1000 
	 loss: 33.8548, MinusLogProbMetric: 33.8548, val_loss: 33.8550, val_MinusLogProbMetric: 33.8550

Epoch 226: val_loss improved from 33.90891 to 33.85501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 68s - loss: 33.8548 - MinusLogProbMetric: 33.8548 - val_loss: 33.8550 - val_MinusLogProbMetric: 33.8550 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 227/1000
2023-10-27 12:20:39.295 
Epoch 227/1000 
	 loss: 33.8984, MinusLogProbMetric: 33.8984, val_loss: 33.9580, val_MinusLogProbMetric: 33.9580

Epoch 227: val_loss did not improve from 33.85501
196/196 - 74s - loss: 33.8984 - MinusLogProbMetric: 33.8984 - val_loss: 33.9580 - val_MinusLogProbMetric: 33.9580 - lr: 6.1728e-06 - 74s/epoch - 379ms/step
Epoch 228/1000
2023-10-27 12:21:43.927 
Epoch 228/1000 
	 loss: 33.7140, MinusLogProbMetric: 33.7140, val_loss: 33.6793, val_MinusLogProbMetric: 33.6793

Epoch 228: val_loss improved from 33.85501 to 33.67927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 66s - loss: 33.7140 - MinusLogProbMetric: 33.7140 - val_loss: 33.6793 - val_MinusLogProbMetric: 33.6793 - lr: 6.1728e-06 - 66s/epoch - 336ms/step
Epoch 229/1000
2023-10-27 12:22:53.980 
Epoch 229/1000 
	 loss: 33.6675, MinusLogProbMetric: 33.6675, val_loss: 33.7718, val_MinusLogProbMetric: 33.7718

Epoch 229: val_loss did not improve from 33.67927
196/196 - 69s - loss: 33.6675 - MinusLogProbMetric: 33.6675 - val_loss: 33.7718 - val_MinusLogProbMetric: 33.7718 - lr: 6.1728e-06 - 69s/epoch - 351ms/step
Epoch 230/1000
2023-10-27 12:24:03.091 
Epoch 230/1000 
	 loss: 33.6581, MinusLogProbMetric: 33.6581, val_loss: 33.9239, val_MinusLogProbMetric: 33.9239

Epoch 230: val_loss did not improve from 33.67927
196/196 - 69s - loss: 33.6581 - MinusLogProbMetric: 33.6581 - val_loss: 33.9239 - val_MinusLogProbMetric: 33.9239 - lr: 6.1728e-06 - 69s/epoch - 353ms/step
Epoch 231/1000
2023-10-27 12:25:07.429 
Epoch 231/1000 
	 loss: 33.5792, MinusLogProbMetric: 33.5792, val_loss: 33.5963, val_MinusLogProbMetric: 33.5963

Epoch 231: val_loss improved from 33.67927 to 33.59626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 65s - loss: 33.5792 - MinusLogProbMetric: 33.5792 - val_loss: 33.5963 - val_MinusLogProbMetric: 33.5963 - lr: 6.1728e-06 - 65s/epoch - 334ms/step
Epoch 232/1000
2023-10-27 12:26:21.198 
Epoch 232/1000 
	 loss: 33.5784, MinusLogProbMetric: 33.5784, val_loss: 33.6962, val_MinusLogProbMetric: 33.6962

Epoch 232: val_loss did not improve from 33.59626
196/196 - 73s - loss: 33.5784 - MinusLogProbMetric: 33.5784 - val_loss: 33.6962 - val_MinusLogProbMetric: 33.6962 - lr: 6.1728e-06 - 73s/epoch - 371ms/step
Epoch 233/1000
2023-10-27 12:27:25.276 
Epoch 233/1000 
	 loss: 33.5756, MinusLogProbMetric: 33.5756, val_loss: 33.5529, val_MinusLogProbMetric: 33.5529

Epoch 233: val_loss improved from 33.59626 to 33.55289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 65s - loss: 33.5756 - MinusLogProbMetric: 33.5756 - val_loss: 33.5529 - val_MinusLogProbMetric: 33.5529 - lr: 6.1728e-06 - 65s/epoch - 332ms/step
Epoch 234/1000
2023-10-27 12:28:32.970 
Epoch 234/1000 
	 loss: 33.4879, MinusLogProbMetric: 33.4879, val_loss: 33.5307, val_MinusLogProbMetric: 33.5307

Epoch 234: val_loss improved from 33.55289 to 33.53065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 68s - loss: 33.4879 - MinusLogProbMetric: 33.4879 - val_loss: 33.5307 - val_MinusLogProbMetric: 33.5307 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 235/1000
2023-10-27 12:29:43.418 
Epoch 235/1000 
	 loss: 33.4653, MinusLogProbMetric: 33.4653, val_loss: 33.5346, val_MinusLogProbMetric: 33.5346

Epoch 235: val_loss did not improve from 33.53065
196/196 - 69s - loss: 33.4653 - MinusLogProbMetric: 33.4653 - val_loss: 33.5346 - val_MinusLogProbMetric: 33.5346 - lr: 6.1728e-06 - 69s/epoch - 352ms/step
Epoch 236/1000
2023-10-27 12:30:48.387 
Epoch 236/1000 
	 loss: 33.4471, MinusLogProbMetric: 33.4471, val_loss: 33.4628, val_MinusLogProbMetric: 33.4628

Epoch 236: val_loss improved from 33.53065 to 33.46280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 67s - loss: 33.4471 - MinusLogProbMetric: 33.4471 - val_loss: 33.4628 - val_MinusLogProbMetric: 33.4628 - lr: 6.1728e-06 - 67s/epoch - 340ms/step
Epoch 237/1000
2023-10-27 12:32:01.858 
Epoch 237/1000 
	 loss: 33.4797, MinusLogProbMetric: 33.4797, val_loss: 33.5380, val_MinusLogProbMetric: 33.5380

Epoch 237: val_loss did not improve from 33.46280
196/196 - 72s - loss: 33.4797 - MinusLogProbMetric: 33.4797 - val_loss: 33.5380 - val_MinusLogProbMetric: 33.5380 - lr: 6.1728e-06 - 72s/epoch - 366ms/step
Epoch 238/1000
2023-10-27 12:33:07.993 
Epoch 238/1000 
	 loss: 33.4792, MinusLogProbMetric: 33.4792, val_loss: 33.4220, val_MinusLogProbMetric: 33.4220

Epoch 238: val_loss improved from 33.46280 to 33.42195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 67s - loss: 33.4792 - MinusLogProbMetric: 33.4792 - val_loss: 33.4220 - val_MinusLogProbMetric: 33.4220 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 239/1000
2023-10-27 12:34:17.069 
Epoch 239/1000 
	 loss: 33.4877, MinusLogProbMetric: 33.4877, val_loss: 33.5902, val_MinusLogProbMetric: 33.5902

Epoch 239: val_loss did not improve from 33.42195
196/196 - 68s - loss: 33.4877 - MinusLogProbMetric: 33.4877 - val_loss: 33.5902 - val_MinusLogProbMetric: 33.5902 - lr: 6.1728e-06 - 68s/epoch - 346ms/step
Epoch 240/1000
2023-10-27 12:35:24.201 
Epoch 240/1000 
	 loss: 33.4425, MinusLogProbMetric: 33.4425, val_loss: 33.3535, val_MinusLogProbMetric: 33.3535

Epoch 240: val_loss improved from 33.42195 to 33.35347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 68s - loss: 33.4425 - MinusLogProbMetric: 33.4425 - val_loss: 33.3535 - val_MinusLogProbMetric: 33.3535 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 241/1000
2023-10-27 12:36:32.262 
Epoch 241/1000 
	 loss: 33.2806, MinusLogProbMetric: 33.2806, val_loss: 33.3359, val_MinusLogProbMetric: 33.3359

Epoch 241: val_loss improved from 33.35347 to 33.33586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 68s - loss: 33.2806 - MinusLogProbMetric: 33.2806 - val_loss: 33.3359 - val_MinusLogProbMetric: 33.3359 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 242/1000
2023-10-27 12:37:43.039 
Epoch 242/1000 
	 loss: 33.3050, MinusLogProbMetric: 33.3050, val_loss: 33.3762, val_MinusLogProbMetric: 33.3762

Epoch 242: val_loss did not improve from 33.33586
196/196 - 70s - loss: 33.3050 - MinusLogProbMetric: 33.3050 - val_loss: 33.3762 - val_MinusLogProbMetric: 33.3762 - lr: 6.1728e-06 - 70s/epoch - 356ms/step
Epoch 243/1000
2023-10-27 12:38:54.370 
Epoch 243/1000 
	 loss: 33.2513, MinusLogProbMetric: 33.2513, val_loss: 33.3110, val_MinusLogProbMetric: 33.3110

Epoch 243: val_loss improved from 33.33586 to 33.31102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 72s - loss: 33.2513 - MinusLogProbMetric: 33.2513 - val_loss: 33.3110 - val_MinusLogProbMetric: 33.3110 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 244/1000
2023-10-27 12:40:03.229 
Epoch 244/1000 
	 loss: 33.3290, MinusLogProbMetric: 33.3290, val_loss: 33.2800, val_MinusLogProbMetric: 33.2800

Epoch 244: val_loss improved from 33.31102 to 33.28000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 69s - loss: 33.3290 - MinusLogProbMetric: 33.3290 - val_loss: 33.2800 - val_MinusLogProbMetric: 33.2800 - lr: 6.1728e-06 - 69s/epoch - 351ms/step
Epoch 245/1000
2023-10-27 12:41:12.370 
Epoch 245/1000 
	 loss: 33.2126, MinusLogProbMetric: 33.2126, val_loss: 33.2479, val_MinusLogProbMetric: 33.2479

Epoch 245: val_loss improved from 33.28000 to 33.24793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 69s - loss: 33.2126 - MinusLogProbMetric: 33.2126 - val_loss: 33.2479 - val_MinusLogProbMetric: 33.2479 - lr: 6.1728e-06 - 69s/epoch - 354ms/step
Epoch 246/1000
2023-10-27 12:42:21.209 
Epoch 246/1000 
	 loss: 33.1959, MinusLogProbMetric: 33.1959, val_loss: 33.2031, val_MinusLogProbMetric: 33.2031

Epoch 246: val_loss improved from 33.24793 to 33.20312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 69s - loss: 33.1959 - MinusLogProbMetric: 33.1959 - val_loss: 33.2031 - val_MinusLogProbMetric: 33.2031 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 247/1000
2023-10-27 12:43:29.372 
Epoch 247/1000 
	 loss: 33.1619, MinusLogProbMetric: 33.1619, val_loss: 33.2993, val_MinusLogProbMetric: 33.2993

Epoch 247: val_loss did not improve from 33.20312
196/196 - 67s - loss: 33.1619 - MinusLogProbMetric: 33.1619 - val_loss: 33.2993 - val_MinusLogProbMetric: 33.2993 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 248/1000
2023-10-27 12:44:38.530 
Epoch 248/1000 
	 loss: 33.2042, MinusLogProbMetric: 33.2042, val_loss: 33.2273, val_MinusLogProbMetric: 33.2273

Epoch 248: val_loss did not improve from 33.20312
196/196 - 69s - loss: 33.2042 - MinusLogProbMetric: 33.2042 - val_loss: 33.2273 - val_MinusLogProbMetric: 33.2273 - lr: 6.1728e-06 - 69s/epoch - 353ms/step
Epoch 249/1000
2023-10-27 12:45:49.312 
Epoch 249/1000 
	 loss: 33.1790, MinusLogProbMetric: 33.1790, val_loss: 33.2411, val_MinusLogProbMetric: 33.2411

Epoch 249: val_loss did not improve from 33.20312
196/196 - 71s - loss: 33.1790 - MinusLogProbMetric: 33.1790 - val_loss: 33.2411 - val_MinusLogProbMetric: 33.2411 - lr: 6.1728e-06 - 71s/epoch - 361ms/step
Epoch 250/1000
2023-10-27 12:46:56.333 
Epoch 250/1000 
	 loss: 33.1187, MinusLogProbMetric: 33.1187, val_loss: 33.1914, val_MinusLogProbMetric: 33.1914

Epoch 250: val_loss improved from 33.20312 to 33.19138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 68s - loss: 33.1187 - MinusLogProbMetric: 33.1187 - val_loss: 33.1914 - val_MinusLogProbMetric: 33.1914 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 251/1000
2023-10-27 12:48:13.244 
Epoch 251/1000 
	 loss: 33.1913, MinusLogProbMetric: 33.1913, val_loss: 33.2054, val_MinusLogProbMetric: 33.2054

Epoch 251: val_loss did not improve from 33.19138
196/196 - 76s - loss: 33.1913 - MinusLogProbMetric: 33.1913 - val_loss: 33.2054 - val_MinusLogProbMetric: 33.2054 - lr: 6.1728e-06 - 76s/epoch - 386ms/step
Epoch 252/1000
2023-10-27 12:49:29.090 
Epoch 252/1000 
	 loss: 33.0959, MinusLogProbMetric: 33.0959, val_loss: 33.1613, val_MinusLogProbMetric: 33.1613

Epoch 252: val_loss improved from 33.19138 to 33.16131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 77s - loss: 33.0959 - MinusLogProbMetric: 33.0959 - val_loss: 33.1613 - val_MinusLogProbMetric: 33.1613 - lr: 6.1728e-06 - 77s/epoch - 394ms/step
Epoch 253/1000
2023-10-27 12:50:54.780 
Epoch 253/1000 
	 loss: 33.0511, MinusLogProbMetric: 33.0511, val_loss: 33.1329, val_MinusLogProbMetric: 33.1329

Epoch 253: val_loss improved from 33.16131 to 33.13290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 86s - loss: 33.0511 - MinusLogProbMetric: 33.0511 - val_loss: 33.1329 - val_MinusLogProbMetric: 33.1329 - lr: 6.1728e-06 - 86s/epoch - 437ms/step
Epoch 254/1000
2023-10-27 12:52:20.794 
Epoch 254/1000 
	 loss: 33.0613, MinusLogProbMetric: 33.0613, val_loss: 33.2364, val_MinusLogProbMetric: 33.2364

Epoch 254: val_loss did not improve from 33.13290
196/196 - 85s - loss: 33.0613 - MinusLogProbMetric: 33.0613 - val_loss: 33.2364 - val_MinusLogProbMetric: 33.2364 - lr: 6.1728e-06 - 85s/epoch - 431ms/step
Epoch 255/1000
2023-10-27 12:53:46.957 
Epoch 255/1000 
	 loss: 33.0452, MinusLogProbMetric: 33.0452, val_loss: 33.1241, val_MinusLogProbMetric: 33.1241

Epoch 255: val_loss improved from 33.13290 to 33.12411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 88s - loss: 33.0452 - MinusLogProbMetric: 33.0452 - val_loss: 33.1241 - val_MinusLogProbMetric: 33.1241 - lr: 6.1728e-06 - 88s/epoch - 447ms/step
Epoch 256/1000
2023-10-27 12:55:13.566 
Epoch 256/1000 
	 loss: 33.0771, MinusLogProbMetric: 33.0771, val_loss: 33.1111, val_MinusLogProbMetric: 33.1111

Epoch 256: val_loss improved from 33.12411 to 33.11107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_400/weights/best_weights.h5
196/196 - 87s - loss: 33.0771 - MinusLogProbMetric: 33.0771 - val_loss: 33.1111 - val_MinusLogProbMetric: 33.1111 - lr: 6.1728e-06 - 87s/epoch - 442ms/step
Epoch 257/1000
2023-10-27 12:56:29.206 
Epoch 257/1000 
	 loss: 33.0592, MinusLogProbMetric: 33.0592, val_loss: 33.1611, val_MinusLogProbMetric: 33.1611

Epoch 257: val_loss did not improve from 33.11107
196/196 - 74s - loss: 33.0592 - MinusLogProbMetric: 33.0592 - val_loss: 33.1611 - val_MinusLogProbMetric: 33.1611 - lr: 6.1728e-06 - 74s/epoch - 378ms/step
Epoch 258/1000
2023-10-27 12:57:40.557 
Epoch 258/1000 
	 loss: 75.3435, MinusLogProbMetric: 75.3435, val_loss: 45.5802, val_MinusLogProbMetric: 45.5802

Epoch 258: val_loss did not improve from 33.11107
196/196 - 71s - loss: 75.3435 - MinusLogProbMetric: 75.3435 - val_loss: 45.5802 - val_MinusLogProbMetric: 45.5802 - lr: 6.1728e-06 - 71s/epoch - 364ms/step
Epoch 259/1000
2023-10-27 12:58:49.219 
Epoch 259/1000 
	 loss: 41.9671, MinusLogProbMetric: 41.9671, val_loss: 39.9326, val_MinusLogProbMetric: 39.9326

Epoch 259: val_loss did not improve from 33.11107
196/196 - 69s - loss: 41.9671 - MinusLogProbMetric: 41.9671 - val_loss: 39.9326 - val_MinusLogProbMetric: 39.9326 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 260/1000
2023-10-27 13:00:07.258 
Epoch 260/1000 
	 loss: 39.5549, MinusLogProbMetric: 39.5549, val_loss: 39.0381, val_MinusLogProbMetric: 39.0381

Epoch 260: val_loss did not improve from 33.11107
196/196 - 78s - loss: 39.5549 - MinusLogProbMetric: 39.5549 - val_loss: 39.0381 - val_MinusLogProbMetric: 39.0381 - lr: 6.1728e-06 - 78s/epoch - 398ms/step
Epoch 261/1000
2023-10-27 13:01:17.672 
Epoch 261/1000 
	 loss: 38.3878, MinusLogProbMetric: 38.3878, val_loss: 38.0885, val_MinusLogProbMetric: 38.0885

Epoch 261: val_loss did not improve from 33.11107
196/196 - 70s - loss: 38.3878 - MinusLogProbMetric: 38.3878 - val_loss: 38.0885 - val_MinusLogProbMetric: 38.0885 - lr: 6.1728e-06 - 70s/epoch - 359ms/step
Epoch 262/1000
2023-10-27 13:02:33.368 
Epoch 262/1000 
	 loss: 37.6599, MinusLogProbMetric: 37.6599, val_loss: 37.4634, val_MinusLogProbMetric: 37.4634

Epoch 262: val_loss did not improve from 33.11107
196/196 - 76s - loss: 37.6599 - MinusLogProbMetric: 37.6599 - val_loss: 37.4634 - val_MinusLogProbMetric: 37.4634 - lr: 6.1728e-06 - 76s/epoch - 386ms/step
Epoch 263/1000
2023-10-27 13:03:41.329 
Epoch 263/1000 
	 loss: 37.2629, MinusLogProbMetric: 37.2629, val_loss: 39.1954, val_MinusLogProbMetric: 39.1954

Epoch 263: val_loss did not improve from 33.11107
196/196 - 68s - loss: 37.2629 - MinusLogProbMetric: 37.2629 - val_loss: 39.1954 - val_MinusLogProbMetric: 39.1954 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 264/1000
2023-10-27 13:04:50.132 
Epoch 264/1000 
	 loss: 37.1776, MinusLogProbMetric: 37.1776, val_loss: 36.9888, val_MinusLogProbMetric: 36.9888

Epoch 264: val_loss did not improve from 33.11107
196/196 - 69s - loss: 37.1776 - MinusLogProbMetric: 37.1776 - val_loss: 36.9888 - val_MinusLogProbMetric: 36.9888 - lr: 6.1728e-06 - 69s/epoch - 351ms/step
Epoch 265/1000
2023-10-27 13:05:57.247 
Epoch 265/1000 
	 loss: 37.2710, MinusLogProbMetric: 37.2710, val_loss: 37.0573, val_MinusLogProbMetric: 37.0573

Epoch 265: val_loss did not improve from 33.11107
196/196 - 67s - loss: 37.2710 - MinusLogProbMetric: 37.2710 - val_loss: 37.0573 - val_MinusLogProbMetric: 37.0573 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 266/1000
2023-10-27 13:07:03.641 
Epoch 266/1000 
	 loss: 36.4066, MinusLogProbMetric: 36.4066, val_loss: 36.3535, val_MinusLogProbMetric: 36.3535

Epoch 266: val_loss did not improve from 33.11107
196/196 - 66s - loss: 36.4066 - MinusLogProbMetric: 36.4066 - val_loss: 36.3535 - val_MinusLogProbMetric: 36.3535 - lr: 6.1728e-06 - 66s/epoch - 339ms/step
Epoch 267/1000
2023-10-27 13:08:08.272 
Epoch 267/1000 
	 loss: 37.0424, MinusLogProbMetric: 37.0424, val_loss: 36.0310, val_MinusLogProbMetric: 36.0310

Epoch 267: val_loss did not improve from 33.11107
196/196 - 65s - loss: 37.0424 - MinusLogProbMetric: 37.0424 - val_loss: 36.0310 - val_MinusLogProbMetric: 36.0310 - lr: 6.1728e-06 - 65s/epoch - 330ms/step
Epoch 268/1000
2023-10-27 13:09:14.033 
Epoch 268/1000 
	 loss: 35.8024, MinusLogProbMetric: 35.8024, val_loss: 35.7073, val_MinusLogProbMetric: 35.7073

Epoch 268: val_loss did not improve from 33.11107
196/196 - 66s - loss: 35.8024 - MinusLogProbMetric: 35.8024 - val_loss: 35.7073 - val_MinusLogProbMetric: 35.7073 - lr: 6.1728e-06 - 66s/epoch - 335ms/step
Epoch 269/1000
2023-10-27 13:10:19.851 
Epoch 269/1000 
	 loss: 36.0618, MinusLogProbMetric: 36.0618, val_loss: 36.6277, val_MinusLogProbMetric: 36.6277

Epoch 269: val_loss did not improve from 33.11107
196/196 - 66s - loss: 36.0618 - MinusLogProbMetric: 36.0618 - val_loss: 36.6277 - val_MinusLogProbMetric: 36.6277 - lr: 6.1728e-06 - 66s/epoch - 336ms/step
Epoch 270/1000
2023-10-27 13:11:25.456 
Epoch 270/1000 
	 loss: 36.7283, MinusLogProbMetric: 36.7283, val_loss: 36.8283, val_MinusLogProbMetric: 36.8283

Epoch 270: val_loss did not improve from 33.11107
196/196 - 66s - loss: 36.7283 - MinusLogProbMetric: 36.7283 - val_loss: 36.8283 - val_MinusLogProbMetric: 36.8283 - lr: 6.1728e-06 - 66s/epoch - 335ms/step
Epoch 271/1000
2023-10-27 13:12:31.135 
Epoch 271/1000 
	 loss: 35.7064, MinusLogProbMetric: 35.7064, val_loss: 35.3612, val_MinusLogProbMetric: 35.3612

Epoch 271: val_loss did not improve from 33.11107
196/196 - 66s - loss: 35.7064 - MinusLogProbMetric: 35.7064 - val_loss: 35.3612 - val_MinusLogProbMetric: 35.3612 - lr: 6.1728e-06 - 66s/epoch - 335ms/step
Epoch 272/1000
2023-10-27 13:13:36.670 
Epoch 272/1000 
	 loss: 35.3054, MinusLogProbMetric: 35.3054, val_loss: 35.4332, val_MinusLogProbMetric: 35.4332

Epoch 272: val_loss did not improve from 33.11107
196/196 - 66s - loss: 35.3054 - MinusLogProbMetric: 35.3054 - val_loss: 35.4332 - val_MinusLogProbMetric: 35.4332 - lr: 6.1728e-06 - 66s/epoch - 334ms/step
Epoch 273/1000
2023-10-27 13:14:40.774 
Epoch 273/1000 
	 loss: 35.1137, MinusLogProbMetric: 35.1137, val_loss: 35.2289, val_MinusLogProbMetric: 35.2289

Epoch 273: val_loss did not improve from 33.11107
196/196 - 64s - loss: 35.1137 - MinusLogProbMetric: 35.1137 - val_loss: 35.2289 - val_MinusLogProbMetric: 35.2289 - lr: 6.1728e-06 - 64s/epoch - 327ms/step
Epoch 274/1000
2023-10-27 13:15:45.795 
Epoch 274/1000 
	 loss: 35.1824, MinusLogProbMetric: 35.1824, val_loss: 35.0878, val_MinusLogProbMetric: 35.0878

Epoch 274: val_loss did not improve from 33.11107
196/196 - 65s - loss: 35.1824 - MinusLogProbMetric: 35.1824 - val_loss: 35.0878 - val_MinusLogProbMetric: 35.0878 - lr: 6.1728e-06 - 65s/epoch - 332ms/step
Epoch 275/1000
2023-10-27 13:16:50.215 
Epoch 275/1000 
	 loss: 34.9308, MinusLogProbMetric: 34.9308, val_loss: 35.0545, val_MinusLogProbMetric: 35.0545

Epoch 275: val_loss did not improve from 33.11107
196/196 - 64s - loss: 34.9308 - MinusLogProbMetric: 34.9308 - val_loss: 35.0545 - val_MinusLogProbMetric: 35.0545 - lr: 6.1728e-06 - 64s/epoch - 329ms/step
Epoch 276/1000
2023-10-27 13:17:55.301 
Epoch 276/1000 
	 loss: 35.1406, MinusLogProbMetric: 35.1406, val_loss: 34.9948, val_MinusLogProbMetric: 34.9948

Epoch 276: val_loss did not improve from 33.11107
196/196 - 65s - loss: 35.1406 - MinusLogProbMetric: 35.1406 - val_loss: 34.9948 - val_MinusLogProbMetric: 34.9948 - lr: 6.1728e-06 - 65s/epoch - 332ms/step
Epoch 277/1000
2023-10-27 13:19:00.291 
Epoch 277/1000 
	 loss: 35.8611, MinusLogProbMetric: 35.8611, val_loss: 37.7052, val_MinusLogProbMetric: 37.7052

Epoch 277: val_loss did not improve from 33.11107
196/196 - 65s - loss: 35.8611 - MinusLogProbMetric: 35.8611 - val_loss: 37.7052 - val_MinusLogProbMetric: 37.7052 - lr: 6.1728e-06 - 65s/epoch - 332ms/step
Epoch 278/1000
2023-10-27 13:20:08.237 
Epoch 278/1000 
	 loss: 36.0243, MinusLogProbMetric: 36.0243, val_loss: 35.5774, val_MinusLogProbMetric: 35.5774

Epoch 278: val_loss did not improve from 33.11107
196/196 - 68s - loss: 36.0243 - MinusLogProbMetric: 36.0243 - val_loss: 35.5774 - val_MinusLogProbMetric: 35.5774 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 279/1000
2023-10-27 13:21:12.855 
Epoch 279/1000 
	 loss: 35.3377, MinusLogProbMetric: 35.3377, val_loss: 35.1655, val_MinusLogProbMetric: 35.1655

Epoch 279: val_loss did not improve from 33.11107
196/196 - 65s - loss: 35.3377 - MinusLogProbMetric: 35.3377 - val_loss: 35.1655 - val_MinusLogProbMetric: 35.1655 - lr: 6.1728e-06 - 65s/epoch - 330ms/step
Epoch 280/1000
2023-10-27 13:22:19.444 
Epoch 280/1000 
	 loss: 35.2784, MinusLogProbMetric: 35.2784, val_loss: 34.7454, val_MinusLogProbMetric: 34.7454

Epoch 280: val_loss did not improve from 33.11107
196/196 - 67s - loss: 35.2784 - MinusLogProbMetric: 35.2784 - val_loss: 34.7454 - val_MinusLogProbMetric: 34.7454 - lr: 6.1728e-06 - 67s/epoch - 340ms/step
Epoch 281/1000
2023-10-27 13:23:24.280 
Epoch 281/1000 
	 loss: 35.0666, MinusLogProbMetric: 35.0666, val_loss: 34.9398, val_MinusLogProbMetric: 34.9398

Epoch 281: val_loss did not improve from 33.11107
196/196 - 65s - loss: 35.0666 - MinusLogProbMetric: 35.0666 - val_loss: 34.9398 - val_MinusLogProbMetric: 34.9398 - lr: 6.1728e-06 - 65s/epoch - 331ms/step
Epoch 282/1000
2023-10-27 13:24:29.417 
Epoch 282/1000 
	 loss: 34.6622, MinusLogProbMetric: 34.6622, val_loss: 34.5205, val_MinusLogProbMetric: 34.5205

Epoch 282: val_loss did not improve from 33.11107
196/196 - 65s - loss: 34.6622 - MinusLogProbMetric: 34.6622 - val_loss: 34.5205 - val_MinusLogProbMetric: 34.5205 - lr: 6.1728e-06 - 65s/epoch - 332ms/step
Epoch 283/1000
2023-10-27 13:25:34.459 
Epoch 283/1000 
	 loss: 40.2798, MinusLogProbMetric: 40.2798, val_loss: 38.8319, val_MinusLogProbMetric: 38.8319

Epoch 283: val_loss did not improve from 33.11107
196/196 - 65s - loss: 40.2798 - MinusLogProbMetric: 40.2798 - val_loss: 38.8319 - val_MinusLogProbMetric: 38.8319 - lr: 6.1728e-06 - 65s/epoch - 331ms/step
Epoch 284/1000
2023-10-27 13:26:42.981 
Epoch 284/1000 
	 loss: 37.1836, MinusLogProbMetric: 37.1836, val_loss: 36.5186, val_MinusLogProbMetric: 36.5186

Epoch 284: val_loss did not improve from 33.11107
196/196 - 69s - loss: 37.1836 - MinusLogProbMetric: 37.1836 - val_loss: 36.5186 - val_MinusLogProbMetric: 36.5186 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 285/1000
2023-10-27 13:27:47.261 
Epoch 285/1000 
	 loss: 40.2226, MinusLogProbMetric: 40.2226, val_loss: 39.6679, val_MinusLogProbMetric: 39.6679

Epoch 285: val_loss did not improve from 33.11107
196/196 - 64s - loss: 40.2226 - MinusLogProbMetric: 40.2226 - val_loss: 39.6679 - val_MinusLogProbMetric: 39.6679 - lr: 6.1728e-06 - 64s/epoch - 328ms/step
Epoch 286/1000
2023-10-27 13:28:51.686 
Epoch 286/1000 
	 loss: 36.1917, MinusLogProbMetric: 36.1917, val_loss: 35.2593, val_MinusLogProbMetric: 35.2593

Epoch 286: val_loss did not improve from 33.11107
196/196 - 64s - loss: 36.1917 - MinusLogProbMetric: 36.1917 - val_loss: 35.2593 - val_MinusLogProbMetric: 35.2593 - lr: 6.1728e-06 - 64s/epoch - 329ms/step
Epoch 287/1000
2023-10-27 13:29:59.962 
Epoch 287/1000 
	 loss: 34.8413, MinusLogProbMetric: 34.8413, val_loss: 34.7404, val_MinusLogProbMetric: 34.7404

Epoch 287: val_loss did not improve from 33.11107
196/196 - 68s - loss: 34.8413 - MinusLogProbMetric: 34.8413 - val_loss: 34.7404 - val_MinusLogProbMetric: 34.7404 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 288/1000
2023-10-27 13:31:03.519 
Epoch 288/1000 
	 loss: 34.5275, MinusLogProbMetric: 34.5275, val_loss: 34.4993, val_MinusLogProbMetric: 34.4993

Epoch 288: val_loss did not improve from 33.11107
196/196 - 64s - loss: 34.5275 - MinusLogProbMetric: 34.5275 - val_loss: 34.4993 - val_MinusLogProbMetric: 34.4993 - lr: 6.1728e-06 - 64s/epoch - 324ms/step
Epoch 289/1000
2023-10-27 13:32:14.202 
Epoch 289/1000 
	 loss: 34.3555, MinusLogProbMetric: 34.3555, val_loss: 34.3634, val_MinusLogProbMetric: 34.3634

Epoch 289: val_loss did not improve from 33.11107
196/196 - 71s - loss: 34.3555 - MinusLogProbMetric: 34.3555 - val_loss: 34.3634 - val_MinusLogProbMetric: 34.3634 - lr: 6.1728e-06 - 71s/epoch - 361ms/step
Epoch 290/1000
2023-10-27 13:33:20.066 
Epoch 290/1000 
	 loss: 36.3977, MinusLogProbMetric: 36.3977, val_loss: 43.2720, val_MinusLogProbMetric: 43.2720

Epoch 290: val_loss did not improve from 33.11107
196/196 - 66s - loss: 36.3977 - MinusLogProbMetric: 36.3977 - val_loss: 43.2720 - val_MinusLogProbMetric: 43.2720 - lr: 6.1728e-06 - 66s/epoch - 336ms/step
Epoch 291/1000
2023-10-27 13:34:22.666 
Epoch 291/1000 
	 loss: 38.4196, MinusLogProbMetric: 38.4196, val_loss: 36.8905, val_MinusLogProbMetric: 36.8905

Epoch 291: val_loss did not improve from 33.11107
196/196 - 63s - loss: 38.4196 - MinusLogProbMetric: 38.4196 - val_loss: 36.8905 - val_MinusLogProbMetric: 36.8905 - lr: 6.1728e-06 - 63s/epoch - 319ms/step
Epoch 292/1000
2023-10-27 13:35:33.877 
Epoch 292/1000 
	 loss: 36.6076, MinusLogProbMetric: 36.6076, val_loss: 36.4580, val_MinusLogProbMetric: 36.4580

Epoch 292: val_loss did not improve from 33.11107
196/196 - 71s - loss: 36.6076 - MinusLogProbMetric: 36.6076 - val_loss: 36.4580 - val_MinusLogProbMetric: 36.4580 - lr: 6.1728e-06 - 71s/epoch - 363ms/step
Epoch 293/1000
2023-10-27 13:36:37.858 
Epoch 293/1000 
	 loss: 36.2459, MinusLogProbMetric: 36.2459, val_loss: 36.1894, val_MinusLogProbMetric: 36.1894

Epoch 293: val_loss did not improve from 33.11107
196/196 - 64s - loss: 36.2459 - MinusLogProbMetric: 36.2459 - val_loss: 36.1894 - val_MinusLogProbMetric: 36.1894 - lr: 6.1728e-06 - 64s/epoch - 326ms/step
Epoch 294/1000
2023-10-27 13:37:45.523 
Epoch 294/1000 
	 loss: 36.0050, MinusLogProbMetric: 36.0050, val_loss: 35.9576, val_MinusLogProbMetric: 35.9576

Epoch 294: val_loss did not improve from 33.11107
196/196 - 68s - loss: 36.0050 - MinusLogProbMetric: 36.0050 - val_loss: 35.9576 - val_MinusLogProbMetric: 35.9576 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 295/1000
2023-10-27 13:38:56.182 
Epoch 295/1000 
	 loss: 35.7757, MinusLogProbMetric: 35.7757, val_loss: 35.9100, val_MinusLogProbMetric: 35.9100

Epoch 295: val_loss did not improve from 33.11107
196/196 - 71s - loss: 35.7757 - MinusLogProbMetric: 35.7757 - val_loss: 35.9100 - val_MinusLogProbMetric: 35.9100 - lr: 6.1728e-06 - 71s/epoch - 360ms/step
Epoch 296/1000
2023-10-27 13:39:59.295 
Epoch 296/1000 
	 loss: 35.6599, MinusLogProbMetric: 35.6599, val_loss: 35.5675, val_MinusLogProbMetric: 35.5675

Epoch 296: val_loss did not improve from 33.11107
196/196 - 63s - loss: 35.6599 - MinusLogProbMetric: 35.6599 - val_loss: 35.5675 - val_MinusLogProbMetric: 35.5675 - lr: 6.1728e-06 - 63s/epoch - 322ms/step
Epoch 297/1000
2023-10-27 13:41:11.651 
Epoch 297/1000 
	 loss: 50.2642, MinusLogProbMetric: 50.2642, val_loss: 40.4172, val_MinusLogProbMetric: 40.4172

Epoch 297: val_loss did not improve from 33.11107
196/196 - 72s - loss: 50.2642 - MinusLogProbMetric: 50.2642 - val_loss: 40.4172 - val_MinusLogProbMetric: 40.4172 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 298/1000
2023-10-27 13:42:15.895 
Epoch 298/1000 
	 loss: 38.4700, MinusLogProbMetric: 38.4700, val_loss: 37.4410, val_MinusLogProbMetric: 37.4410

Epoch 298: val_loss did not improve from 33.11107
196/196 - 64s - loss: 38.4700 - MinusLogProbMetric: 38.4700 - val_loss: 37.4410 - val_MinusLogProbMetric: 37.4410 - lr: 6.1728e-06 - 64s/epoch - 328ms/step
Epoch 299/1000
2023-10-27 13:43:19.700 
Epoch 299/1000 
	 loss: 36.9588, MinusLogProbMetric: 36.9588, val_loss: 36.5849, val_MinusLogProbMetric: 36.5849

Epoch 299: val_loss did not improve from 33.11107
196/196 - 64s - loss: 36.9588 - MinusLogProbMetric: 36.9588 - val_loss: 36.5849 - val_MinusLogProbMetric: 36.5849 - lr: 6.1728e-06 - 64s/epoch - 326ms/step
Epoch 300/1000
2023-10-27 13:44:32.045 
Epoch 300/1000 
	 loss: 36.2656, MinusLogProbMetric: 36.2656, val_loss: 36.1204, val_MinusLogProbMetric: 36.1204

Epoch 300: val_loss did not improve from 33.11107
196/196 - 72s - loss: 36.2656 - MinusLogProbMetric: 36.2656 - val_loss: 36.1204 - val_MinusLogProbMetric: 36.1204 - lr: 6.1728e-06 - 72s/epoch - 369ms/step
Epoch 301/1000
2023-10-27 13:45:39.526 
Epoch 301/1000 
	 loss: 35.9318, MinusLogProbMetric: 35.9318, val_loss: 35.8328, val_MinusLogProbMetric: 35.8328

Epoch 301: val_loss did not improve from 33.11107
196/196 - 67s - loss: 35.9318 - MinusLogProbMetric: 35.9318 - val_loss: 35.8328 - val_MinusLogProbMetric: 35.8328 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 302/1000
2023-10-27 13:46:47.180 
Epoch 302/1000 
	 loss: 36.7229, MinusLogProbMetric: 36.7229, val_loss: 35.7523, val_MinusLogProbMetric: 35.7523

Epoch 302: val_loss did not improve from 33.11107
196/196 - 68s - loss: 36.7229 - MinusLogProbMetric: 36.7229 - val_loss: 35.7523 - val_MinusLogProbMetric: 35.7523 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 303/1000
2023-10-27 13:47:55.965 
Epoch 303/1000 
	 loss: 35.3393, MinusLogProbMetric: 35.3393, val_loss: 35.0727, val_MinusLogProbMetric: 35.0727

Epoch 303: val_loss did not improve from 33.11107
196/196 - 69s - loss: 35.3393 - MinusLogProbMetric: 35.3393 - val_loss: 35.0727 - val_MinusLogProbMetric: 35.0727 - lr: 6.1728e-06 - 69s/epoch - 351ms/step
Epoch 304/1000
2023-10-27 13:49:05.020 
Epoch 304/1000 
	 loss: 34.8704, MinusLogProbMetric: 34.8704, val_loss: 34.8195, val_MinusLogProbMetric: 34.8195

Epoch 304: val_loss did not improve from 33.11107
196/196 - 69s - loss: 34.8704 - MinusLogProbMetric: 34.8704 - val_loss: 34.8195 - val_MinusLogProbMetric: 34.8195 - lr: 6.1728e-06 - 69s/epoch - 352ms/step
Epoch 305/1000
2023-10-27 13:50:13.502 
Epoch 305/1000 
	 loss: 34.6333, MinusLogProbMetric: 34.6333, val_loss: 34.5778, val_MinusLogProbMetric: 34.5778

Epoch 305: val_loss did not improve from 33.11107
196/196 - 68s - loss: 34.6333 - MinusLogProbMetric: 34.6333 - val_loss: 34.5778 - val_MinusLogProbMetric: 34.5778 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 306/1000
2023-10-27 13:51:22.906 
Epoch 306/1000 
	 loss: 34.4713, MinusLogProbMetric: 34.4713, val_loss: 34.6528, val_MinusLogProbMetric: 34.6528

Epoch 306: val_loss did not improve from 33.11107
196/196 - 69s - loss: 34.4713 - MinusLogProbMetric: 34.4713 - val_loss: 34.6528 - val_MinusLogProbMetric: 34.6528 - lr: 6.1728e-06 - 69s/epoch - 354ms/step
Epoch 307/1000
2023-10-27 13:52:31.584 
Epoch 307/1000 
	 loss: 34.6939, MinusLogProbMetric: 34.6939, val_loss: 34.3356, val_MinusLogProbMetric: 34.3356

Epoch 307: val_loss did not improve from 33.11107
196/196 - 69s - loss: 34.6939 - MinusLogProbMetric: 34.6939 - val_loss: 34.3356 - val_MinusLogProbMetric: 34.3356 - lr: 3.0864e-06 - 69s/epoch - 350ms/step
Epoch 308/1000
2023-10-27 13:53:38.156 
Epoch 308/1000 
	 loss: 34.2309, MinusLogProbMetric: 34.2309, val_loss: 34.2838, val_MinusLogProbMetric: 34.2838

Epoch 308: val_loss did not improve from 33.11107
196/196 - 67s - loss: 34.2309 - MinusLogProbMetric: 34.2309 - val_loss: 34.2838 - val_MinusLogProbMetric: 34.2838 - lr: 3.0864e-06 - 67s/epoch - 340ms/step
Epoch 309/1000
2023-10-27 13:54:46.880 
Epoch 309/1000 
	 loss: 34.1760, MinusLogProbMetric: 34.1760, val_loss: 34.2106, val_MinusLogProbMetric: 34.2106

Epoch 309: val_loss did not improve from 33.11107
196/196 - 69s - loss: 34.1760 - MinusLogProbMetric: 34.1760 - val_loss: 34.2106 - val_MinusLogProbMetric: 34.2106 - lr: 3.0864e-06 - 69s/epoch - 351ms/step
Epoch 310/1000
2023-10-27 13:55:56.493 
Epoch 310/1000 
	 loss: 34.1180, MinusLogProbMetric: 34.1180, val_loss: 34.1505, val_MinusLogProbMetric: 34.1505

Epoch 310: val_loss did not improve from 33.11107
196/196 - 70s - loss: 34.1180 - MinusLogProbMetric: 34.1180 - val_loss: 34.1505 - val_MinusLogProbMetric: 34.1505 - lr: 3.0864e-06 - 70s/epoch - 355ms/step
Epoch 311/1000
2023-10-27 13:57:02.684 
Epoch 311/1000 
	 loss: 34.0894, MinusLogProbMetric: 34.0894, val_loss: 34.1166, val_MinusLogProbMetric: 34.1166

Epoch 311: val_loss did not improve from 33.11107
196/196 - 66s - loss: 34.0894 - MinusLogProbMetric: 34.0894 - val_loss: 34.1166 - val_MinusLogProbMetric: 34.1166 - lr: 3.0864e-06 - 66s/epoch - 338ms/step
Epoch 312/1000
2023-10-27 13:58:09.637 
Epoch 312/1000 
	 loss: 34.0295, MinusLogProbMetric: 34.0295, val_loss: 34.0562, val_MinusLogProbMetric: 34.0562

Epoch 312: val_loss did not improve from 33.11107
196/196 - 67s - loss: 34.0295 - MinusLogProbMetric: 34.0295 - val_loss: 34.0562 - val_MinusLogProbMetric: 34.0562 - lr: 3.0864e-06 - 67s/epoch - 342ms/step
Epoch 313/1000
2023-10-27 13:59:16.297 
Epoch 313/1000 
	 loss: 33.9772, MinusLogProbMetric: 33.9772, val_loss: 34.0152, val_MinusLogProbMetric: 34.0152

Epoch 313: val_loss did not improve from 33.11107
196/196 - 67s - loss: 33.9772 - MinusLogProbMetric: 33.9772 - val_loss: 34.0152 - val_MinusLogProbMetric: 34.0152 - lr: 3.0864e-06 - 67s/epoch - 340ms/step
Epoch 314/1000
2023-10-27 14:00:23.093 
Epoch 314/1000 
	 loss: 33.9369, MinusLogProbMetric: 33.9369, val_loss: 33.9748, val_MinusLogProbMetric: 33.9748

Epoch 314: val_loss did not improve from 33.11107
196/196 - 67s - loss: 33.9369 - MinusLogProbMetric: 33.9369 - val_loss: 33.9748 - val_MinusLogProbMetric: 33.9748 - lr: 3.0864e-06 - 67s/epoch - 341ms/step
Epoch 315/1000
2023-10-27 14:01:34.590 
Epoch 315/1000 
	 loss: 33.8979, MinusLogProbMetric: 33.8979, val_loss: 33.9681, val_MinusLogProbMetric: 33.9681

Epoch 315: val_loss did not improve from 33.11107
196/196 - 71s - loss: 33.8979 - MinusLogProbMetric: 33.8979 - val_loss: 33.9681 - val_MinusLogProbMetric: 33.9681 - lr: 3.0864e-06 - 71s/epoch - 365ms/step
Epoch 316/1000
2023-10-27 14:02:45.592 
Epoch 316/1000 
	 loss: 33.8938, MinusLogProbMetric: 33.8938, val_loss: 33.9769, val_MinusLogProbMetric: 33.9769

Epoch 316: val_loss did not improve from 33.11107
196/196 - 71s - loss: 33.8938 - MinusLogProbMetric: 33.8938 - val_loss: 33.9769 - val_MinusLogProbMetric: 33.9769 - lr: 3.0864e-06 - 71s/epoch - 362ms/step
Epoch 317/1000
2023-10-27 14:04:06.625 
Epoch 317/1000 
	 loss: 33.8565, MinusLogProbMetric: 33.8565, val_loss: 33.8675, val_MinusLogProbMetric: 33.8675

Epoch 317: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.8565 - MinusLogProbMetric: 33.8565 - val_loss: 33.8675 - val_MinusLogProbMetric: 33.8675 - lr: 3.0864e-06 - 81s/epoch - 413ms/step
Epoch 318/1000
2023-10-27 14:05:27.291 
Epoch 318/1000 
	 loss: 33.7879, MinusLogProbMetric: 33.7879, val_loss: 33.8203, val_MinusLogProbMetric: 33.8203

Epoch 318: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.7879 - MinusLogProbMetric: 33.7879 - val_loss: 33.8203 - val_MinusLogProbMetric: 33.8203 - lr: 3.0864e-06 - 81s/epoch - 412ms/step
Epoch 319/1000
2023-10-27 14:06:48.722 
Epoch 319/1000 
	 loss: 33.7749, MinusLogProbMetric: 33.7749, val_loss: 33.8213, val_MinusLogProbMetric: 33.8213

Epoch 319: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.7749 - MinusLogProbMetric: 33.7749 - val_loss: 33.8213 - val_MinusLogProbMetric: 33.8213 - lr: 3.0864e-06 - 81s/epoch - 415ms/step
Epoch 320/1000
2023-10-27 14:08:09.188 
Epoch 320/1000 
	 loss: 33.7340, MinusLogProbMetric: 33.7340, val_loss: 33.7963, val_MinusLogProbMetric: 33.7963

Epoch 320: val_loss did not improve from 33.11107
196/196 - 80s - loss: 33.7340 - MinusLogProbMetric: 33.7340 - val_loss: 33.7963 - val_MinusLogProbMetric: 33.7963 - lr: 3.0864e-06 - 80s/epoch - 411ms/step
Epoch 321/1000
2023-10-27 14:09:30.679 
Epoch 321/1000 
	 loss: 33.7098, MinusLogProbMetric: 33.7098, val_loss: 33.7448, val_MinusLogProbMetric: 33.7448

Epoch 321: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.7098 - MinusLogProbMetric: 33.7098 - val_loss: 33.7448 - val_MinusLogProbMetric: 33.7448 - lr: 3.0864e-06 - 81s/epoch - 416ms/step
Epoch 322/1000
2023-10-27 14:10:51.315 
Epoch 322/1000 
	 loss: 34.1348, MinusLogProbMetric: 34.1348, val_loss: 36.7686, val_MinusLogProbMetric: 36.7686

Epoch 322: val_loss did not improve from 33.11107
196/196 - 81s - loss: 34.1348 - MinusLogProbMetric: 34.1348 - val_loss: 36.7686 - val_MinusLogProbMetric: 36.7686 - lr: 3.0864e-06 - 81s/epoch - 411ms/step
Epoch 323/1000
2023-10-27 14:12:11.423 
Epoch 323/1000 
	 loss: 34.1872, MinusLogProbMetric: 34.1872, val_loss: 34.0028, val_MinusLogProbMetric: 34.0028

Epoch 323: val_loss did not improve from 33.11107
196/196 - 80s - loss: 34.1872 - MinusLogProbMetric: 34.1872 - val_loss: 34.0028 - val_MinusLogProbMetric: 34.0028 - lr: 3.0864e-06 - 80s/epoch - 409ms/step
Epoch 324/1000
2023-10-27 14:13:33.704 
Epoch 324/1000 
	 loss: 33.8425, MinusLogProbMetric: 33.8425, val_loss: 33.8854, val_MinusLogProbMetric: 33.8854

Epoch 324: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.8425 - MinusLogProbMetric: 33.8425 - val_loss: 33.8854 - val_MinusLogProbMetric: 33.8854 - lr: 3.0864e-06 - 82s/epoch - 420ms/step
Epoch 325/1000
2023-10-27 14:14:55.581 
Epoch 325/1000 
	 loss: 33.7648, MinusLogProbMetric: 33.7648, val_loss: 33.8164, val_MinusLogProbMetric: 33.8164

Epoch 325: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.7648 - MinusLogProbMetric: 33.7648 - val_loss: 33.8164 - val_MinusLogProbMetric: 33.8164 - lr: 3.0864e-06 - 82s/epoch - 418ms/step
Epoch 326/1000
2023-10-27 14:16:17.629 
Epoch 326/1000 
	 loss: 33.7243, MinusLogProbMetric: 33.7243, val_loss: 33.7752, val_MinusLogProbMetric: 33.7752

Epoch 326: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.7243 - MinusLogProbMetric: 33.7243 - val_loss: 33.7752 - val_MinusLogProbMetric: 33.7752 - lr: 3.0864e-06 - 82s/epoch - 419ms/step
Epoch 327/1000
2023-10-27 14:17:40.458 
Epoch 327/1000 
	 loss: 33.6707, MinusLogProbMetric: 33.6707, val_loss: 33.7218, val_MinusLogProbMetric: 33.7218

Epoch 327: val_loss did not improve from 33.11107
196/196 - 83s - loss: 33.6707 - MinusLogProbMetric: 33.6707 - val_loss: 33.7218 - val_MinusLogProbMetric: 33.7218 - lr: 3.0864e-06 - 83s/epoch - 423ms/step
Epoch 328/1000
2023-10-27 14:19:02.392 
Epoch 328/1000 
	 loss: 33.6353, MinusLogProbMetric: 33.6353, val_loss: 33.6938, val_MinusLogProbMetric: 33.6938

Epoch 328: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.6353 - MinusLogProbMetric: 33.6353 - val_loss: 33.6938 - val_MinusLogProbMetric: 33.6938 - lr: 3.0864e-06 - 82s/epoch - 418ms/step
Epoch 329/1000
2023-10-27 14:20:24.663 
Epoch 329/1000 
	 loss: 33.6252, MinusLogProbMetric: 33.6252, val_loss: 33.6611, val_MinusLogProbMetric: 33.6611

Epoch 329: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.6252 - MinusLogProbMetric: 33.6252 - val_loss: 33.6611 - val_MinusLogProbMetric: 33.6611 - lr: 3.0864e-06 - 82s/epoch - 420ms/step
Epoch 330/1000
2023-10-27 14:21:46.204 
Epoch 330/1000 
	 loss: 33.6920, MinusLogProbMetric: 33.6920, val_loss: 33.6750, val_MinusLogProbMetric: 33.6750

Epoch 330: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.6920 - MinusLogProbMetric: 33.6920 - val_loss: 33.6750 - val_MinusLogProbMetric: 33.6750 - lr: 3.0864e-06 - 82s/epoch - 416ms/step
Epoch 331/1000
2023-10-27 14:23:07.575 
Epoch 331/1000 
	 loss: 33.5589, MinusLogProbMetric: 33.5589, val_loss: 33.6045, val_MinusLogProbMetric: 33.6045

Epoch 331: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.5589 - MinusLogProbMetric: 33.5589 - val_loss: 33.6045 - val_MinusLogProbMetric: 33.6045 - lr: 3.0864e-06 - 81s/epoch - 415ms/step
Epoch 332/1000
2023-10-27 14:24:28.682 
Epoch 332/1000 
	 loss: 33.7187, MinusLogProbMetric: 33.7187, val_loss: 33.6097, val_MinusLogProbMetric: 33.6097

Epoch 332: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.7187 - MinusLogProbMetric: 33.7187 - val_loss: 33.6097 - val_MinusLogProbMetric: 33.6097 - lr: 3.0864e-06 - 81s/epoch - 414ms/step
Epoch 333/1000
2023-10-27 14:25:50.945 
Epoch 333/1000 
	 loss: 33.5329, MinusLogProbMetric: 33.5329, val_loss: 33.5802, val_MinusLogProbMetric: 33.5802

Epoch 333: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.5329 - MinusLogProbMetric: 33.5329 - val_loss: 33.5802 - val_MinusLogProbMetric: 33.5802 - lr: 3.0864e-06 - 82s/epoch - 420ms/step
Epoch 334/1000
2023-10-27 14:27:10.724 
Epoch 334/1000 
	 loss: 33.5062, MinusLogProbMetric: 33.5062, val_loss: 33.5809, val_MinusLogProbMetric: 33.5809

Epoch 334: val_loss did not improve from 33.11107
196/196 - 80s - loss: 33.5062 - MinusLogProbMetric: 33.5062 - val_loss: 33.5809 - val_MinusLogProbMetric: 33.5809 - lr: 3.0864e-06 - 80s/epoch - 407ms/step
Epoch 335/1000
2023-10-27 14:28:31.539 
Epoch 335/1000 
	 loss: 33.6352, MinusLogProbMetric: 33.6352, val_loss: 33.7859, val_MinusLogProbMetric: 33.7859

Epoch 335: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.6352 - MinusLogProbMetric: 33.6352 - val_loss: 33.7859 - val_MinusLogProbMetric: 33.7859 - lr: 3.0864e-06 - 81s/epoch - 412ms/step
Epoch 336/1000
2023-10-27 14:29:52.743 
Epoch 336/1000 
	 loss: 33.5214, MinusLogProbMetric: 33.5214, val_loss: 33.5808, val_MinusLogProbMetric: 33.5808

Epoch 336: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.5214 - MinusLogProbMetric: 33.5214 - val_loss: 33.5808 - val_MinusLogProbMetric: 33.5808 - lr: 3.0864e-06 - 81s/epoch - 414ms/step
Epoch 337/1000
2023-10-27 14:31:11.817 
Epoch 337/1000 
	 loss: 33.4956, MinusLogProbMetric: 33.4956, val_loss: 33.5346, val_MinusLogProbMetric: 33.5346

Epoch 337: val_loss did not improve from 33.11107
196/196 - 79s - loss: 33.4956 - MinusLogProbMetric: 33.4956 - val_loss: 33.5346 - val_MinusLogProbMetric: 33.5346 - lr: 3.0864e-06 - 79s/epoch - 403ms/step
Epoch 338/1000
2023-10-27 14:32:30.148 
Epoch 338/1000 
	 loss: 33.4672, MinusLogProbMetric: 33.4672, val_loss: 33.5058, val_MinusLogProbMetric: 33.5058

Epoch 338: val_loss did not improve from 33.11107
196/196 - 78s - loss: 33.4672 - MinusLogProbMetric: 33.4672 - val_loss: 33.5058 - val_MinusLogProbMetric: 33.5058 - lr: 3.0864e-06 - 78s/epoch - 400ms/step
Epoch 339/1000
2023-10-27 14:33:50.058 
Epoch 339/1000 
	 loss: 33.4300, MinusLogProbMetric: 33.4300, val_loss: 33.5005, val_MinusLogProbMetric: 33.5005

Epoch 339: val_loss did not improve from 33.11107
196/196 - 80s - loss: 33.4300 - MinusLogProbMetric: 33.4300 - val_loss: 33.5005 - val_MinusLogProbMetric: 33.5005 - lr: 3.0864e-06 - 80s/epoch - 408ms/step
Epoch 340/1000
2023-10-27 14:35:11.843 
Epoch 340/1000 
	 loss: 34.1325, MinusLogProbMetric: 34.1325, val_loss: 33.9144, val_MinusLogProbMetric: 33.9144

Epoch 340: val_loss did not improve from 33.11107
196/196 - 82s - loss: 34.1325 - MinusLogProbMetric: 34.1325 - val_loss: 33.9144 - val_MinusLogProbMetric: 33.9144 - lr: 3.0864e-06 - 82s/epoch - 417ms/step
Epoch 341/1000
2023-10-27 14:36:33.401 
Epoch 341/1000 
	 loss: 33.6444, MinusLogProbMetric: 33.6444, val_loss: 33.6056, val_MinusLogProbMetric: 33.6056

Epoch 341: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.6444 - MinusLogProbMetric: 33.6444 - val_loss: 33.6056 - val_MinusLogProbMetric: 33.6056 - lr: 3.0864e-06 - 82s/epoch - 416ms/step
Epoch 342/1000
2023-10-27 14:37:50.478 
Epoch 342/1000 
	 loss: 33.4762, MinusLogProbMetric: 33.4762, val_loss: 33.5157, val_MinusLogProbMetric: 33.5157

Epoch 342: val_loss did not improve from 33.11107
196/196 - 77s - loss: 33.4762 - MinusLogProbMetric: 33.4762 - val_loss: 33.5157 - val_MinusLogProbMetric: 33.5157 - lr: 3.0864e-06 - 77s/epoch - 393ms/step
Epoch 343/1000
2023-10-27 14:38:59.597 
Epoch 343/1000 
	 loss: 33.4215, MinusLogProbMetric: 33.4215, val_loss: 33.4744, val_MinusLogProbMetric: 33.4744

Epoch 343: val_loss did not improve from 33.11107
196/196 - 69s - loss: 33.4215 - MinusLogProbMetric: 33.4215 - val_loss: 33.4744 - val_MinusLogProbMetric: 33.4744 - lr: 3.0864e-06 - 69s/epoch - 353ms/step
Epoch 344/1000
2023-10-27 14:40:18.991 
Epoch 344/1000 
	 loss: 33.4034, MinusLogProbMetric: 33.4034, val_loss: 33.4991, val_MinusLogProbMetric: 33.4991

Epoch 344: val_loss did not improve from 33.11107
196/196 - 79s - loss: 33.4034 - MinusLogProbMetric: 33.4034 - val_loss: 33.4991 - val_MinusLogProbMetric: 33.4991 - lr: 3.0864e-06 - 79s/epoch - 405ms/step
Epoch 345/1000
2023-10-27 14:41:40.472 
Epoch 345/1000 
	 loss: 33.3641, MinusLogProbMetric: 33.3641, val_loss: 33.4222, val_MinusLogProbMetric: 33.4222

Epoch 345: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.3641 - MinusLogProbMetric: 33.3641 - val_loss: 33.4222 - val_MinusLogProbMetric: 33.4222 - lr: 3.0864e-06 - 81s/epoch - 416ms/step
Epoch 346/1000
2023-10-27 14:43:00.897 
Epoch 346/1000 
	 loss: 33.3849, MinusLogProbMetric: 33.3849, val_loss: 33.6417, val_MinusLogProbMetric: 33.6417

Epoch 346: val_loss did not improve from 33.11107
196/196 - 80s - loss: 33.3849 - MinusLogProbMetric: 33.3849 - val_loss: 33.6417 - val_MinusLogProbMetric: 33.6417 - lr: 3.0864e-06 - 80s/epoch - 410ms/step
Epoch 347/1000
2023-10-27 14:44:20.189 
Epoch 347/1000 
	 loss: 33.8461, MinusLogProbMetric: 33.8461, val_loss: 33.4354, val_MinusLogProbMetric: 33.4354

Epoch 347: val_loss did not improve from 33.11107
196/196 - 79s - loss: 33.8461 - MinusLogProbMetric: 33.8461 - val_loss: 33.4354 - val_MinusLogProbMetric: 33.4354 - lr: 3.0864e-06 - 79s/epoch - 405ms/step
Epoch 348/1000
2023-10-27 14:45:40.112 
Epoch 348/1000 
	 loss: 33.3337, MinusLogProbMetric: 33.3337, val_loss: 33.4270, val_MinusLogProbMetric: 33.4270

Epoch 348: val_loss did not improve from 33.11107
196/196 - 80s - loss: 33.3337 - MinusLogProbMetric: 33.3337 - val_loss: 33.4270 - val_MinusLogProbMetric: 33.4270 - lr: 3.0864e-06 - 80s/epoch - 408ms/step
Epoch 349/1000
2023-10-27 14:47:01.430 
Epoch 349/1000 
	 loss: 33.3480, MinusLogProbMetric: 33.3480, val_loss: 33.3954, val_MinusLogProbMetric: 33.3954

Epoch 349: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.3480 - MinusLogProbMetric: 33.3480 - val_loss: 33.3954 - val_MinusLogProbMetric: 33.3954 - lr: 3.0864e-06 - 81s/epoch - 415ms/step
Epoch 350/1000
2023-10-27 14:48:21.705 
Epoch 350/1000 
	 loss: 33.3804, MinusLogProbMetric: 33.3804, val_loss: 33.3788, val_MinusLogProbMetric: 33.3788

Epoch 350: val_loss did not improve from 33.11107
196/196 - 80s - loss: 33.3804 - MinusLogProbMetric: 33.3804 - val_loss: 33.3788 - val_MinusLogProbMetric: 33.3788 - lr: 3.0864e-06 - 80s/epoch - 410ms/step
Epoch 351/1000
2023-10-27 14:49:43.488 
Epoch 351/1000 
	 loss: 33.2899, MinusLogProbMetric: 33.2899, val_loss: 33.4213, val_MinusLogProbMetric: 33.4213

Epoch 351: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.2899 - MinusLogProbMetric: 33.2899 - val_loss: 33.4213 - val_MinusLogProbMetric: 33.4213 - lr: 3.0864e-06 - 82s/epoch - 417ms/step
Epoch 352/1000
2023-10-27 14:51:05.828 
Epoch 352/1000 
	 loss: 33.7683, MinusLogProbMetric: 33.7683, val_loss: 35.4382, val_MinusLogProbMetric: 35.4382

Epoch 352: val_loss did not improve from 33.11107
196/196 - 82s - loss: 33.7683 - MinusLogProbMetric: 33.7683 - val_loss: 35.4382 - val_MinusLogProbMetric: 35.4382 - lr: 3.0864e-06 - 82s/epoch - 420ms/step
Epoch 353/1000
2023-10-27 14:52:26.854 
Epoch 353/1000 
	 loss: 34.0540, MinusLogProbMetric: 34.0540, val_loss: 33.6757, val_MinusLogProbMetric: 33.6757

Epoch 353: val_loss did not improve from 33.11107
196/196 - 81s - loss: 34.0540 - MinusLogProbMetric: 34.0540 - val_loss: 33.6757 - val_MinusLogProbMetric: 33.6757 - lr: 3.0864e-06 - 81s/epoch - 413ms/step
Epoch 354/1000
2023-10-27 14:53:48.198 
Epoch 354/1000 
	 loss: 33.3704, MinusLogProbMetric: 33.3704, val_loss: 33.3465, val_MinusLogProbMetric: 33.3465

Epoch 354: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.3704 - MinusLogProbMetric: 33.3704 - val_loss: 33.3465 - val_MinusLogProbMetric: 33.3465 - lr: 3.0864e-06 - 81s/epoch - 415ms/step
Epoch 355/1000
2023-10-27 14:55:08.708 
Epoch 355/1000 
	 loss: 33.2616, MinusLogProbMetric: 33.2616, val_loss: 33.3328, val_MinusLogProbMetric: 33.3328

Epoch 355: val_loss did not improve from 33.11107
196/196 - 81s - loss: 33.2616 - MinusLogProbMetric: 33.2616 - val_loss: 33.3328 - val_MinusLogProbMetric: 33.3328 - lr: 3.0864e-06 - 81s/epoch - 411ms/step
Epoch 356/1000
2023-10-27 14:56:29.279 
Epoch 356/1000 
	 loss: 33.4631, MinusLogProbMetric: 33.4631, val_loss: 33.3911, val_MinusLogProbMetric: 33.3911

Epoch 356: val_loss did not improve from 33.11107
Restoring model weights from the end of the best epoch: 256.
196/196 - 81s - loss: 33.4631 - MinusLogProbMetric: 33.4631 - val_loss: 33.3911 - val_MinusLogProbMetric: 33.3911 - lr: 3.0864e-06 - 81s/epoch - 414ms/step
Epoch 356: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 933.
Model trained in 27794.19 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 2.05 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 2.38 s.
===========
Run 400/720 done in 39747.75 s.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_408/ already exists.
Skipping it.
===========
Run 408/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_409/ already exists.
Skipping it.
===========
Run 409/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_414/ already exists.
Skipping it.
===========
Run 414/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_415/ already exists.
Skipping it.
===========
Run 415/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_416/ already exists.
Skipping it.
===========
Run 416/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_417/ already exists.
Skipping it.
===========
Run 417/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_418/ already exists.
Skipping it.
===========
Run 418/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_419/ already exists.
Skipping it.
===========
Run 419/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_420/ already exists.
Skipping it.
===========
Run 420/720 already exists. Skipping it.
===========

===========
Generating train data for run 421.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_276"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_277 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f0d8512ded0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d8716a800>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d8716a800>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d047b2ec0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0db262b400>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0db262a950>, <keras.callbacks.ModelCheckpoint object at 0x7f0db2628ac0>, <keras.callbacks.EarlyStopping object at 0x7f0db262ba00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0db2629330>, <keras.callbacks.TerminateOnNaN object at 0x7f0db26289a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 14:56:42.592851
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 14:59:05.241 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 143s/epoch - 727ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 421.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_287"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_288 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f144f3818a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d65a8b700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d65a8b700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f09e1e3e620>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f144f02d270>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f144f02d7e0>, <keras.callbacks.ModelCheckpoint object at 0x7f144f02d8a0>, <keras.callbacks.EarlyStopping object at 0x7f144f02db10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f144f02db40>, <keras.callbacks.TerminateOnNaN object at 0x7f144f02d780>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 14:59:15.074394
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:01:56.083 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 161s/epoch - 820ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 421.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_298"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_299 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f0b006aa3e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b083f0bb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b083f0bb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bc9c64f40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d860d31f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d860d3760>, <keras.callbacks.ModelCheckpoint object at 0x7f0d860d3820>, <keras.callbacks.EarlyStopping object at 0x7f0d860d3a90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d860d3ac0>, <keras.callbacks.TerminateOnNaN object at 0x7f0d860d3700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:02:05.787595
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:04:53.637 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 168s/epoch - 855ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 421.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_309"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_310 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f0b7db0ab00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4adea500>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4adea500>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0ac81452d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0bc100bee0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0bc10f8490>, <keras.callbacks.ModelCheckpoint object at 0x7f0bc10f8550>, <keras.callbacks.EarlyStopping object at 0x7f0bc10f87c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0bc10f87f0>, <keras.callbacks.TerminateOnNaN object at 0x7f0bc10f8430>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:05:04.308925
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:07:39.627 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 155s/epoch - 792ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 421.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_320"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_321 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f144e997f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd9e30760>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd9e30760>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0ce1e26d40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f143de4fdc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f143de88370>, <keras.callbacks.ModelCheckpoint object at 0x7f143de88430>, <keras.callbacks.EarlyStopping object at 0x7f143de886a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f143de886d0>, <keras.callbacks.TerminateOnNaN object at 0x7f143de88310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:07:50.379825
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:10:26.246 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 156s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 156s/epoch - 795ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 421.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_331"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_332 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f0d656b6320>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d0755d090>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d0755d090>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d2491a710>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0bc929c760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0bc929ccd0>, <keras.callbacks.ModelCheckpoint object at 0x7f0bc929cd90>, <keras.callbacks.EarlyStopping object at 0x7f0bc929d000>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0bc929d030>, <keras.callbacks.TerminateOnNaN object at 0x7f0bc929cc70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:10:36.714433
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:13:22.228 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 165s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 165s/epoch - 843ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 421.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_342"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_343 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f0bc100bcd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b00534a30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b00534a30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b7d472650>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ce1bd5030>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ce1bd7af0>, <keras.callbacks.ModelCheckpoint object at 0x7f0ce1bd70a0>, <keras.callbacks.EarlyStopping object at 0x7f0ce1bd6fb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ce1bd5cc0>, <keras.callbacks.TerminateOnNaN object at 0x7f0ce1bd6aa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:13:32.579165
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:16:01.556 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 149s/epoch - 759ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 421.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_353"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_354 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f141d961060>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce1bd5ff0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce1bd5ff0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d055be0e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f140d143e50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f140cf80400>, <keras.callbacks.ModelCheckpoint object at 0x7f140cf804c0>, <keras.callbacks.EarlyStopping object at 0x7f140cf80730>, <keras.callbacks.ReduceLROnPlateau object at 0x7f140cf80760>, <keras.callbacks.TerminateOnNaN object at 0x7f140cf803a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:16:11.876553
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:18:53.354 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 161s/epoch - 823ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 421.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_364"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_365 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f0b00a7beb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b00a22680>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b00a22680>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bec73bbe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b00a06680>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b00a06bf0>, <keras.callbacks.ModelCheckpoint object at 0x7f0b00a06cb0>, <keras.callbacks.EarlyStopping object at 0x7f0b00a06f20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b00a06f50>, <keras.callbacks.TerminateOnNaN object at 0x7f0b00a06b90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:19:03.246624
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:21:21.038 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 138s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 138s/epoch - 702ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 421.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_375"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_376 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f0cda6ad540>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b7d686110>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b7d686110>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b840d2170>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b00ff94e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b00ff9a50>, <keras.callbacks.ModelCheckpoint object at 0x7f0b00ff9b10>, <keras.callbacks.EarlyStopping object at 0x7f0b00ff9d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b00ff9db0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b00ff99f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:21:31.490255
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:24:05.413 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 154s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 154s/epoch - 784ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 421.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_421/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_421
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_386"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_387 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f0c343b8910>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0a98c177f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0a98c177f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b74961270>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b00590940>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_421/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b00590eb0>, <keras.callbacks.ModelCheckpoint object at 0x7f0b00590f70>, <keras.callbacks.EarlyStopping object at 0x7f0b005911e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b00591210>, <keras.callbacks.TerminateOnNaN object at 0x7f0b00590e50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_421/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 421/720 with hyperparameters:
timestamp = 2023-10-27 15:24:15.104214
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:26:36.078 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12230.9463, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 12230.9463 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 141s/epoch - 718ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 421/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 422.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_397"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_398 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f0cb1146770>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0cb2547d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0cb2547d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b00591060>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c85157370>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c851578e0>, <keras.callbacks.ModelCheckpoint object at 0x7f0c851579a0>, <keras.callbacks.EarlyStopping object at 0x7f0c85157c10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c85157c40>, <keras.callbacks.TerminateOnNaN object at 0x7f0c85157880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:26:46.255773
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:29:48.158 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11153.9062, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 182s - loss: nan - MinusLogProbMetric: 11153.9062 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 182s/epoch - 928ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 422.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_408"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_409 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f0e2c3f1390>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f10d4636e90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f10d4636e90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f106c5647f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ecc2f38b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ecc2f3e20>, <keras.callbacks.ModelCheckpoint object at 0x7f0ecc2f3ee0>, <keras.callbacks.EarlyStopping object at 0x7f0ecc2f3fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ecc2f3dc0>, <keras.callbacks.TerminateOnNaN object at 0x7f0ecc2f3df0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:29:56.161186
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:32:22.961 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11125.4609, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 11125.4609 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 147s/epoch - 748ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 422.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_419"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_420 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f0b7596d150>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce2449ff0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce2449ff0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f143dd9efe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ce3e2e860>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ce3e2edd0>, <keras.callbacks.ModelCheckpoint object at 0x7f0ce3e2ee90>, <keras.callbacks.EarlyStopping object at 0x7f0ce3e2f100>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ce3e2f130>, <keras.callbacks.TerminateOnNaN object at 0x7f0ce3e2ed70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:32:32.807804
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 39: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:35:40.587 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10654.9619, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 188s - loss: nan - MinusLogProbMetric: 10654.9619 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 188s/epoch - 957ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 422.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_430"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_431 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f0eb47a8100>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c6bf97160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c6bf97160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b030c3b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0bc8ff1d50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0bc8ff22c0>, <keras.callbacks.ModelCheckpoint object at 0x7f0bc8ff2380>, <keras.callbacks.EarlyStopping object at 0x7f0bc8ff25f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0bc8ff2620>, <keras.callbacks.TerminateOnNaN object at 0x7f0bc8ff2260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:35:50.685753
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:38:52.414 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11391.6953, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 182s - loss: nan - MinusLogProbMetric: 11391.6953 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 182s/epoch - 926ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 422.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_441"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_442 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f106c2c0eb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0f6ef6cfd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0f6ef6cfd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0f882a7280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0e244de5f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0e945b3490>, <keras.callbacks.ModelCheckpoint object at 0x7f0e945b2200>, <keras.callbacks.EarlyStopping object at 0x7f0e945b3610>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0e945b38b0>, <keras.callbacks.TerminateOnNaN object at 0x7f0e945b2260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:39:04.037070
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 31: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:41:44.408 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11325.7832, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 160s - loss: nan - MinusLogProbMetric: 11325.7832 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 160s/epoch - 817ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 422.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_452"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_453 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f140cb631c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b01afcc10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b01afcc10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f140c824be0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f140c2b8310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f140c2b8880>, <keras.callbacks.ModelCheckpoint object at 0x7f140c2b8940>, <keras.callbacks.EarlyStopping object at 0x7f140c2b8bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f140c2b8be0>, <keras.callbacks.TerminateOnNaN object at 0x7f140c2b8820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:41:54.270170
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:45:02.532 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11438.4785, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 188s - loss: nan - MinusLogProbMetric: 11438.4785 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 188s/epoch - 959ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 422.
===========
Train data generated in 0.44 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_463"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_464 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f0db2c0e0b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d86759a80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d86759a80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f144fbe62f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0f6ef13df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0f6ef133d0>, <keras.callbacks.ModelCheckpoint object at 0x7f0f6ef127a0>, <keras.callbacks.EarlyStopping object at 0x7f0f6ef11d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0f6ef12b90>, <keras.callbacks.TerminateOnNaN object at 0x7f0f6ef110c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:45:13.341589
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:47:57.090 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11432.7725, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 11432.7725 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 164s/epoch - 834ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 422.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_474"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_475 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f0edc41ba30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0cdbe9cc70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0cdbe9cc70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b7528a320>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ea43724a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ea4372a10>, <keras.callbacks.ModelCheckpoint object at 0x7f0ea4372ad0>, <keras.callbacks.EarlyStopping object at 0x7f0ea4372d40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ea4372d70>, <keras.callbacks.TerminateOnNaN object at 0x7f0ea43729b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:48:06.837908
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:51:26.039 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11438.3320, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 199s - loss: nan - MinusLogProbMetric: 11438.3320 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 199s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 422.
===========
Train data generated in 0.44 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_485"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_486 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f0ee479b7f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0f3c4e2b30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0f3c4e2b30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0a9b2834c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0bc192cc70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0bc192f3a0>, <keras.callbacks.ModelCheckpoint object at 0x7f0bc192d4b0>, <keras.callbacks.EarlyStopping object at 0x7f0bc192da80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0bc192d120>, <keras.callbacks.TerminateOnNaN object at 0x7f0bc192ce80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:51:37.098185
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:54:01.658 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11438.3721, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 11438.3721 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 144s/epoch - 736ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 422.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_496"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_497 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f0b09ce9480>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c4e22cdf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c4e22cdf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c4edc0460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c49bf3d60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c49bac310>, <keras.callbacks.ModelCheckpoint object at 0x7f0c49bac3d0>, <keras.callbacks.EarlyStopping object at 0x7f0c49bac640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c49bac670>, <keras.callbacks.TerminateOnNaN object at 0x7f0c49bac2b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:54:10.856399
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:56:53.479 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11434.5029, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 11434.5029 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 163s/epoch - 830ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 422.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_422/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_422
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_507"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_508 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f0e2c590430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4b8f8070>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d4b8f8070>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0ce2529150>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d648bdd20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_422/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d648bcb80>, <keras.callbacks.ModelCheckpoint object at 0x7f0d648be2c0>, <keras.callbacks.EarlyStopping object at 0x7f0d648bc820>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d648be5f0>, <keras.callbacks.TerminateOnNaN object at 0x7f0d648befe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_422/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 422/720 with hyperparameters:
timestamp = 2023-10-27 15:57:04.055739
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 15:59:21.430 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11443.2529, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 11443.2529 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 137s/epoch - 700ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 422/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 423.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_518"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_519 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f0cda53faf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0cda5ef820>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0cda5ef820>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1000436dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0cda549e10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0cda54a380>, <keras.callbacks.ModelCheckpoint object at 0x7f0cda54a440>, <keras.callbacks.EarlyStopping object at 0x7f0cda54a6b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0cda54a6e0>, <keras.callbacks.TerminateOnNaN object at 0x7f0cda54a320>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 15:59:32.452762
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:02:40.600 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 188s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 188s/epoch - 958ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 423.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_529"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_530 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f0b757fe080>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0a9b6d6b90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0a9b6d6b90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1461c7d840>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f09e19f2860>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f09e19f22f0>, <keras.callbacks.ModelCheckpoint object at 0x7f09e19f2260>, <keras.callbacks.EarlyStopping object at 0x7f09e19f1f90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f09e19f1fc0>, <keras.callbacks.TerminateOnNaN object at 0x7f09e19f2350>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:02:52.939682
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:05:49.596 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 176s/epoch - 900ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 423.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_540"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_541 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f14034fb340>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0cb1de2500>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0cb1de2500>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1403467b80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1403531a80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1403531ff0>, <keras.callbacks.ModelCheckpoint object at 0x7f14035320b0>, <keras.callbacks.EarlyStopping object at 0x7f1403532320>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1403532350>, <keras.callbacks.TerminateOnNaN object at 0x7f1403531f90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:05:59.869008
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:09:30.776 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 211s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 211s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 423.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_551"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_552 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f0b754e6f20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0cb2005630>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0cb2005630>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c44431f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c0504d660>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c0504dbd0>, <keras.callbacks.ModelCheckpoint object at 0x7f0c0504dc90>, <keras.callbacks.EarlyStopping object at 0x7f0c0504df00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c0504df30>, <keras.callbacks.TerminateOnNaN object at 0x7f0c0504db70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:09:42.742600
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:12:52.538 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 189s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 189s/epoch - 967ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 423.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_562"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_563 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f0c6b57f520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c0578d3c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c0578d3c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c6b875b10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0beccb6230>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0beccb6dd0>, <keras.callbacks.ModelCheckpoint object at 0x7f0beccb6e30>, <keras.callbacks.EarlyStopping object at 0x7f0beccb6fe0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0beccb7d30>, <keras.callbacks.TerminateOnNaN object at 0x7f0beccb6d10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:13:05.800843
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:16:35.719 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 210s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 210s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 423.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_573"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_574 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f0d27c627d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0ac9495600>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0ac9495600>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c346cf700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c4968c490>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c4968ca00>, <keras.callbacks.ModelCheckpoint object at 0x7f0c4968cac0>, <keras.callbacks.EarlyStopping object at 0x7f0c4968cd30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c4968cd60>, <keras.callbacks.TerminateOnNaN object at 0x7f0c4968c9a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:16:46.316616
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:19:40.975 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 174s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 174s/epoch - 889ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 423.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_584"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_585 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f0c6b3d7e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce1f95300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce1f95300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b08fbf4f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c2446bb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c24468cd0>, <keras.callbacks.ModelCheckpoint object at 0x7f0c24469c60>, <keras.callbacks.EarlyStopping object at 0x7f0c2446ab60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c2446b3a0>, <keras.callbacks.TerminateOnNaN object at 0x7f0c2446b580>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:20:05.069121
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:23:22.250 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 197s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 197s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 423.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_595"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_596 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f13d986be80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f13e244d300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f13e244d300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f13c9407100>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13c9298a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13c9298fa0>, <keras.callbacks.ModelCheckpoint object at 0x7f13c9299060>, <keras.callbacks.EarlyStopping object at 0x7f13c92992d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13c9299300>, <keras.callbacks.TerminateOnNaN object at 0x7f13c9298f40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:23:32.308458
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 16:27:00.566 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12989.9580, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 208s - loss: nan - MinusLogProbMetric: 12989.9580 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 208s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 423.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_423/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_423
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.377221    7.127741    6.2438807  ...  0.41386062  8.333076
  -0.20018621]
 [ 6.990193    3.6036158   7.2023716  ...  2.5000641   0.24059269
   4.0499587 ]
 [ 4.625933    7.1490397   5.677378   ... -0.07451734  8.288161
   0.11478856]
 ...
 [ 3.0931716   6.40883     3.5263834  ...  9.596216   10.054146
   9.589475  ]
 [ 3.3032775   7.705798    4.4765306  ...  9.215089    9.65445
   9.671152  ]
 [ 5.3625393   6.8024626   6.3470583  ...  0.19077146  8.306975
   0.49249855]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_606"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_607 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f09e87d0cd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f09e8770af0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f09e8770af0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bf91b9840>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f09e1a3f8b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_423/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f09e1a3e440>, <keras.callbacks.ModelCheckpoint object at 0x7f09e1a3d420>, <keras.callbacks.EarlyStopping object at 0x7f09e1a3c430>, <keras.callbacks.ReduceLROnPlateau object at 0x7f09e1a3f0d0>, <keras.callbacks.TerminateOnNaN object at 0x7f09e1a3fac0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 3.2146864 ,  6.631287  ,  3.8488867 , ...,  8.701448  ,
         9.995869  ,  8.467634  ],
       [ 2.8034534 ,  6.2523446 ,  6.587882  , ...,  0.2610864 ,
         8.289457  ,  0.40014002],
       [ 3.1083388 ,  6.904127  ,  3.8563085 , ...,  9.2451105 ,
         9.438324  ,  9.252335  ],
       ...,
       [ 3.2707572 ,  7.137167  ,  4.15955   , ...,  8.417309  ,
         9.719597  ,  9.971758  ],
       [ 2.9350822 ,  7.2153897 ,  4.6962466 , ...,  8.324518  ,
         9.470934  , 10.1611395 ],
       [ 2.8749945 ,  7.372215  ,  3.7357469 , ...,  8.341062  ,
         9.43408   ,  9.775034  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_423/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 423/720 with hyperparameters:
timestamp = 2023-10-27 16:27:12.539785
ndims = 100
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.377221    7.127741    6.2438807   5.435673    4.0165205   6.219049
  4.1100683   8.8912945   8.92213     3.839687    7.2213755   5.307855
  5.647077    9.321208    0.88903034  1.1936541   0.02009015  8.125818
  8.583153    9.655658    9.708242    8.047664    4.635592    7.956708
  0.69603527  6.3117905   0.84046364  9.506726    5.873458    3.812477
  2.5029004   8.513819    4.5791717   5.4547668   0.38966614  5.245923
  6.1506085   6.674606    9.7018795   6.636814    3.3463945   4.644469
  7.275876    0.607445    7.351542    6.6946445   2.4746628   1.2882023
  3.4071944   5.330654    6.1714883   4.4290667   9.648411   -0.15112126
  2.9771137   0.42422986  6.6482043   2.43198     4.204295    4.4230494
  1.6423705   1.2461631   7.064476    1.7268543   3.0895011   4.317311
  8.377744    1.4959692   8.227504    0.34852362  9.020381    4.5677514
 10.634903    5.1945934   7.095311    0.5202157   2.788597    1.1506679
  2.1603026   0.45417947  3.3186061   4.6653333  -0.767903    7.405855
  6.0683556   0.04347444  4.8182964   1.1355917   6.122411    9.17348
  3.1331472   6.9240985   1.4263403   7.782923    2.9285986   1.2252684
  6.0443587   0.41386062  8.333076   -0.20018621]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
LLVM ERROR: Unable to allocate section memory!
